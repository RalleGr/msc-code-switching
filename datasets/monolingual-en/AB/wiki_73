<doc id="20423" url="https://en.wikipedia.org/wiki?curid=20423" title="Malaria">
Malaria

Malaria is a mosquito-borne infectious disease that affects humans and other animals. Malaria causes symptoms that typically include fever, tiredness, vomiting, and headaches. In severe cases it can cause yellow skin, seizures, coma, or death. Symptoms usually begin ten to fifteen days after being bitten by an infected mosquito. If not properly treated, people may have recurrences of the disease months later. In those who have recently survived an infection, reinfection usually causes milder symptoms. This partial resistance disappears over months to years if the person has no continuing exposure to malaria.
Malaria is caused by single-celled microorganisms of the "Plasmodium" group. The disease is most commonly spread by an infected female "Anopheles" mosquito. The mosquito bite introduces the parasites from the mosquito's saliva into a person's blood. The parasites travel to the liver where they mature and reproduce. Five species of "Plasmodium" can infect and be spread by humans. Most deaths are caused by "P. falciparum", whereas "P. vivax", "P. ovale", and "P. malariae" generally cause a milder form of malaria. The species "P. knowlesi" rarely causes disease in humans. Malaria is typically diagnosed by the microscopic examination of blood using blood films, or with antigen-based rapid diagnostic tests. Methods that use the polymerase chain reaction to detect the parasite's DNA have been developed, but are not widely used in areas where malaria is common due to their cost and complexity.
The risk of disease can be reduced by preventing mosquito bites through the use of mosquito nets and insect repellents, or with mosquito control measures such as spraying insecticides and draining standing water. Several medications are available to prevent malaria in travellers to areas where the disease is common. Occasional doses of the combination medication sulfadoxine/pyrimethamine are recommended in infants and after the first trimester of pregnancy in areas with high rates of malaria. As of 2020, there is one vaccine which has been shown to reduce the risk of malaria by about 40% in children in Africa. Efforts to develop more effective vaccines are ongoing. The recommended treatment for malaria is a combination of antimalarial medications that includes an artemisinin. The second medication may be either mefloquine, lumefantrine, or sulfadoxine/pyrimethamine. Quinine along with doxycycline may be used if an artemisinin is not available. It is recommended that in areas where the disease is common, malaria is confirmed if possible before treatment is started due to concerns of increasing drug resistance. Resistance among the parasites has developed to several antimalarial medications; for example, chloroquine-resistant "P. falciparum" has spread to most malarial areas, and resistance to artemisinin has become a problem in some parts of Southeast Asia.
The disease is widespread in the tropical and subtropical regions that exist in a broad band around the equator. This includes much of sub-Saharan Africa, Asia, and Latin America. In 2018 there were 228 million cases of malaria worldwide resulting in an estimated 405,000 deaths. Approximately 93% of the cases and 94% of deaths occurred in Africa. Rates of disease have decreased from 2010 to 2014, but increased from 2015 to 2017, during which there were 231 million cases. Malaria is commonly associated with poverty and has a major negative effect on economic development. In Africa, it is estimated to result in losses of US$12 billion a year due to increased healthcare costs, lost ability to work, and negative effects on tourism.

The signs and symptoms of malaria typically begin 8–25 days following infection, but may occur later in those who have taken antimalarial medications as prevention. Initial manifestations of the disease—common to all malaria species—are similar to flu-like symptoms, and can resemble other conditions such as sepsis, gastroenteritis, and viral diseases. The presentation may include headache, fever, shivering, joint pain, vomiting, hemolytic anemia, jaundice, hemoglobin in the urine, retinal damage, and convulsions.

The classic symptom of malaria is paroxysm—a cyclical occurrence of sudden coldness followed by shivering and then fever and sweating, occurring every two days (tertian fever) in "P. vivax" and "P. ovale" infections, and every three days (quartan fever) for "P. malariae". "P. falciparum" infection can cause recurrent fever every 36–48 hours, or a less pronounced and almost continuous fever.

Severe malaria is usually caused by "P. falciparum" (often referred to as falciparum malaria). Symptoms of falciparum malaria arise 9–30 days after infection. Individuals with cerebral malaria frequently exhibit neurological symptoms, including abnormal posturing, nystagmus, conjugate gaze palsy (failure of the eyes to turn together in the same direction), opisthotonus, seizures, or coma.

Malaria has several serious complications. Among these is the development of respiratory distress, which occurs in up to 25% of adults and 40% of children with severe "P. falciparum" malaria. Possible causes include respiratory compensation of metabolic acidosis, noncardiogenic pulmonary oedema, concomitant pneumonia, and severe anaemia. Although rare in young children with severe malaria, acute respiratory distress syndrome occurs in 5–25% of adults and up to 29% of pregnant women. Coinfection of HIV with malaria increases mortality. Kidney failure is a feature of blackwater fever, where haemoglobin from lysed red blood cells leaks into the urine.

Infection with "P. falciparum" may result in cerebral malaria, a form of severe malaria that involves encephalopathy. It is associated with retinal whitening, which may be a useful clinical sign in distinguishing malaria from other causes of fever. An enlarged spleen, enlarged liver or both of these, severe headache, low blood sugar, and haemoglobin in the urine with kidney failure may occur. Complications may include spontaneous bleeding, coagulopathy, and shock.

Malaria in pregnant women is an important cause of stillbirths, infant mortality, abortion and low birth weight, particularly in "P. falciparum" infection, but also with "P. vivax".

Malaria parasites belong to the genus "Plasmodium" (phylum Apicomplexa). In humans, malaria is caused by "P. falciparum", "P. malariae", "P. ovale", "P. vivax" and "P. knowlesi". Among those infected, "P. falciparum" is the most common species identified (~75%) followed by "P. vivax" (~20%). Although "P. falciparum" traditionally accounts for the majority of deaths, recent evidence suggests that "P. vivax" malaria is associated with potentially life-threatening conditions about as often as with a diagnosis of "P. falciparum" infection. "P. vivax" proportionally is more common outside Africa. There have been documented human infections with several species of "Plasmodium" from higher apes; however, except for "P. knowlesi"—a zoonotic species that causes malaria in macaques—these are mostly of limited public health importance.

In the life cycle of "Plasmodium", a female "Anopheles" mosquito (the definitive host) transmits a motile infective form (called the sporozoite) to a vertebrate host such as a human (the secondary host), thus acting as a transmission vector. A sporozoite travels through the blood vessels to liver cells (hepatocytes), where it reproduces asexually (tissue schizogony), producing thousands of merozoites. These infect new red blood cells and initiate a series of asexual multiplication cycles (blood schizogony) that produce 8 to 24 new infective merozoites, at which point the cells burst and the infective cycle begins anew.

Other merozoites develop into immature gametocytes, which are the precursors of male and female gametes. When a fertilised mosquito bites an infected person, gametocytes are taken up with the blood and mature in the mosquito gut. The male and female gametocytes fuse and form an ookinete—a fertilised, motile zygote. Ookinetes develop into new sporozoites that migrate to the insect's salivary glands, ready to infect a new vertebrate host. The sporozoites are injected into the skin, in the saliva, when the mosquito takes a subsequent blood meal.

Only female mosquitoes feed on blood; male mosquitoes feed on plant nectar and do not transmit the disease. Females of the mosquito genus "Anopheles" prefer to feed at night. They usually start searching for a meal at dusk, and continue through the night until they succeed. Malaria parasites can also be transmitted by blood transfusions, although this is rare.

Symptoms of malaria can recur after varying symptom-free periods. Depending upon the cause, recurrence can be classified as either recrudescence, relapse, or reinfection. Recrudescence is when symptoms return after a symptom-free period. It is caused by parasites surviving in the blood as a result of inadequate or ineffective treatment. Relapse is when symptoms reappear after the parasites have been eliminated from the blood but persist as dormant hypnozoites in liver cells. Relapse commonly occurs between 8–24 weeks and is often seen in "P. vivax" and "P. ovale" infections. However, relapse-like "P. vivax" recurrences are probably being over-attributed to hypnozoite activation. Some of them might have an extra-vascular merozoite origin, making these recurrences recrudescences, not relapses. One newly recognised, non-hypnozoite, possible contributing source to recurrent peripheral "P. vivax" parasitemia is erythrocytic forms in bone marrow. "P. vivax" malaria cases in temperate areas often involve overwintering by hypnozoites, with relapses beginning the year after the mosquito bite. Reinfection means the parasite that caused the past infection was eliminated from the body but a new parasite was introduced. Reinfection cannot readily be distinguished from recrudescence, although recurrence of infection within two weeks of treatment for the initial infection is typically attributed to treatment failure. People may develop some immunity when exposed to frequent infections.

Global climate change is likely to affect malaria transmission, but the degree of effect and the areas affected is uncertain. Greater rainfall in certain areas of India, and following an El Niño event is associated with increased mosquito numbers.

Malaria infection develops via two phases: one that involves the liver (exoerythrocytic phase), and one that involves red blood cells, or erythrocytes (erythrocytic phase). When an infected mosquito pierces a person's skin to take a blood meal, sporozoites in the mosquito's saliva enter the bloodstream and migrate to the liver where they infect hepatocytes, multiplying asexually and asymptomatically for a period of 8–30 days.

After a potential dormant period in the liver, these organisms differentiate to yield thousands of merozoites, which, following rupture of their host cells, escape into the blood and infect red blood cells to begin the erythrocytic stage of the life cycle. The parasite escapes from the liver undetected by wrapping itself in the cell membrane of the infected host liver cell.

Within the red blood cells, the parasites multiply further, again asexually, periodically breaking out of their host cells to invade fresh red blood cells. Several such amplification cycles occur. Thus, classical descriptions of waves of fever arise from simultaneous waves of merozoites escaping and infecting red blood cells.

Some "P. vivax" sporozoites do not immediately develop into exoerythrocytic-phase merozoites, but instead, produce hypnozoites that remain dormant for periods ranging from several months (7–10 months is typical) to several years. After a period of dormancy, they reactivate and produce merozoites. Hypnozoites are responsible for long incubation and late relapses in "P. vivax" infections, although their existence in "P. ovale" is uncertain.

The parasite is relatively protected from attack by the body's immune system because for most of its human life cycle it resides within the liver and blood cells and is relatively invisible to immune surveillance. However, circulating infected blood cells are destroyed in the spleen. To avoid this fate, the "P. falciparum" parasite displays adhesive proteins on the surface of the infected blood cells, causing the blood cells to stick to the walls of small blood vessels, thereby sequestering the parasite from passage through the general circulation and the spleen. The blockage of the microvasculature causes symptoms such as those in placental malaria. Sequestered red blood cells can breach the blood–brain barrier and cause cerebral malaria.

According to a 2005 review, due to the high levels of mortality and morbidity caused by malaria—especially the "P. falciparum" species—it has placed the greatest selective pressure on the human genome in recent history. Several genetic factors provide some resistance to it including sickle cell trait, thalassaemia traits, glucose-6-phosphate dehydrogenase deficiency, and the absence of Duffy antigens on red blood cells.

The impact of sickle cell trait on malaria immunity illustrates some evolutionary trade-offs that have occurred because of endemic malaria. Sickle cell trait causes a change in the haemoglobin molecule in the blood. Normally, red blood cells have a very flexible, biconcave shape that allows them to move through narrow capillaries; however, when the modified haemoglobin S molecules are exposed to low amounts of oxygen, or crowd together due to dehydration, they can stick together forming strands that cause the cell to distort into a curved sickle shape. In these strands, the molecule is not as effective in taking or releasing oxygen, and the cell is not flexible enough to circulate freely. In the early stages of malaria, the parasite can cause infected red cells to sickle, and so they are removed from circulation sooner. This reduces the frequency with which malaria parasites complete their life cycle in the cell. Individuals who are homozygous (with two copies of the abnormal haemoglobin beta allele) have sickle-cell anaemia, while those who are heterozygous (with one abnormal allele and one normal allele) experience resistance to malaria without severe anaemia. Although the shorter life expectancy for those with the homozygous condition would tend to disfavour the trait's survival, the trait is preserved in malaria-prone regions because of the benefits provided by the heterozygous form.

Liver dysfunction as a result of malaria is uncommon and usually only occurs in those with another liver condition such as viral hepatitis or chronic liver disease. The syndrome is sometimes called "malarial hepatitis". While it has been considered a rare occurrence, malarial hepatopathy has seen an increase, particularly in Southeast Asia and India. Liver compromise in people with malaria correlates with a greater likelihood of complications and death.

Owing to the non-specific nature of the presentation of symptoms, diagnosis of malaria in non-endemic areas requires a high degree of suspicion, which might be elicited by any of the following: recent travel history, enlarged spleen, fever, low number of platelets in the blood, and higher-than-normal levels of bilirubin in the blood combined with a normal level of white blood cells. Reports in 2016 and 2017 from countries where malaria is common suggest high levels of over diagnosis due to insufficient or inaccurate laboratory testing.

Malaria is usually confirmed by the microscopic examination of blood films or by antigen-based rapid diagnostic tests (RDT). In some areas, RDTs must be able to distinguish whether the malaria symptoms are caused by "Plasmodium falciparum" or by other species of parasites since treatment strategies could differ for non-"P. falciparum" infections. Microscopy is the most commonly used method to detect the malarial parasite—about 165 million blood films were examined for malaria in 2010. Despite its widespread usage, diagnosis by microscopy suffers from two main drawbacks: many settings (especially rural) are not equipped to perform the test, and the accuracy of the results depends on both the skill of the person examining the blood film and the levels of the parasite in the blood. The sensitivity of blood films ranges from 75–90% in optimum conditions, to as low as 50%. Commercially available RDTs are often more accurate than blood films at predicting the presence of malaria parasites, but they are widely variable in diagnostic sensitivity and specificity depending on manufacturer, and are unable to tell how many parasites are present. However, incorporating RDTs into the diagnosis of malaria can reduce antimalarial prescription. Although RDT does not improve the health outcomes of those infected with malaria, it also does not lead to worse outcomes when compared to presumptive antimalarial treatment.

In regions where laboratory tests are readily available, malaria should be suspected, and tested for, in any unwell person who has been in an area where malaria is endemic. In areas that cannot afford laboratory diagnostic tests, it has become common to use only a history of fever as the indication to treat for malaria—thus the common teaching "fever equals malaria unless proven otherwise". A drawback of this practice is overdiagnosis of malaria and mismanagement of non-malarial fever, which wastes limited resources, erodes confidence in the health care system, and contributes to drug resistance. Although polymerase chain reaction-based tests have been developed, they are not widely used in areas where malaria is common as of 2012, due to their complexity.

Malaria is classified into either "severe" or "uncomplicated" by the World Health Organization (WHO). It is deemed severe when "any" of the following criteria are present, otherwise it is considered uncomplicated.

Cerebral malaria is defined as a severe "P. falciparum"-malaria presenting with neurological symptoms, including coma (with a Glasgow coma scale less than 11, or a Blantyre coma scale less than 3), or with a coma that lasts longer than 30 minutes after a seizure.

Various types of malaria have been called by the names below:

Methods used to prevent malaria include medications, mosquito elimination and the prevention of bites. As of 2020, there is one vaccine for malaria (known as RTS,S) which is licensed for use. The presence of malaria in an area requires a combination of high human population density, high anopheles mosquito population density and high rates of transmission from humans to mosquitoes and from mosquitoes to humans. If any of these is lowered sufficiently, the parasite eventually disappears from that area, as happened in North America, Europe, and parts of the Middle East. However, unless the parasite is eliminated from the whole world, it could re-establish if conditions revert to a combination that favors the parasite's reproduction. Furthermore, the cost per person of eliminating anopheles mosquitoes rises with decreasing population density, making it economically unfeasible in some areas.

Prevention of malaria may be more cost-effective than treatment of the disease in the long run, but the initial costs required are out of reach of many of the world's poorest people. There is a wide difference in the costs of control (i.e. maintenance of low endemicity) and elimination programs between countries. For example, in China—whose government in 2010 announced a strategy to pursue malaria elimination in the Chinese provinces—the required investment is a small proportion of public expenditure on health. In contrast, a similar programme in Tanzania would cost an estimated one-fifth of the public health budget.

In areas where malaria is common, children under five years old often have anaemia, which is sometimes due to malaria. Giving children with anaemia in these areas preventive antimalarial medication improves red blood cell levels slightly but does not affect the risk of death or need for hospitalisation.

Vector control refers to methods used to decrease malaria by reducing the levels of transmission by mosquitoes. For individual protection, the most effective insect repellents are based on DEET or picaridin. However, there is insufficient evidence that mosquito repellants can prevent malaria infection. Insecticide-treated mosquito nets (ITNs) and indoor residual spraying (IRS) have been shown highly effective in preventing malaria among children in areas where malaria is common. Prompt treatment of confirmed cases with artemisinin-based combination therapies (ACTs) may also reduce transmission.

Mosquito nets help keep mosquitoes away from people and reduce infection rates and transmission of malaria. Nets are not a perfect barrier and are often treated with an insecticide designed to kill the mosquito before it has time to find a way past the net. Insecticide-treated nets are estimated to be twice as effective as untreated nets and offer greater than 70% protection compared with no net. Between 2000 and 2008, the use of ITNs saved the lives of an estimated 250,000 infants in Sub-Saharan Africa. About 13% of households in Sub-Saharan countries owned ITNs in 2007 and 31% of African households were estimated to own at least one ITN in 2008. In 2000, 1.7 million (1.8%) African children living in areas of the world where malaria is common were protected by an ITN. That number increased to 20.3 million (18.5%) African children using ITNs in 2007, leaving 89.6 million children unprotected and to 68% African children using mosquito nets in 2015. Most nets are impregnated with pyrethroids, a class of insecticides with low toxicity. They are most effective when used from dusk to dawn. It is recommended to hang a large "bed net" above the center of a bed and either tuck the edges under the mattress or make sure it is large enough such that it touches the ground. ITN is beneficial towards pregnancy outcomes in malaria-endemic regions in Africa but more data is needed in Asia and Latin America.

In areas of high malaria resistance, Piperonyl Butoxide combined with Pyrethroids in ITN is effective in reducing malaria infection rates.

Indoor residual spraying is the spraying of insecticides on the walls inside a home. After feeding, many mosquitoes rest on a nearby surface while digesting the bloodmeal, so if the walls of houses have been coated with insecticides, the resting mosquitoes can be killed before they can bite another person and transfer the malaria parasite. As of 2006, the World Health Organization recommends 12 insecticides in IRS operations, including DDT and the pyrethroids cyfluthrin and deltamethrin. This public health use of small amounts of DDT is permitted under the Stockholm Convention, which prohibits its agricultural use. One problem with all forms of IRS is insecticide resistance. Mosquitoes affected by IRS tend to rest and live indoors, and due to the irritation caused by spraying, their descendants tend to rest and live outdoors, meaning that they are less affected by the IRS. It is uncertain whether the use of IRS together with ITN is effective in reducing malaria cases due to wide geographical variety of malaria distribution, malaria transmission, and insecticide resistance.

People have tried a number of other methods to reduce mosquito bites and slow the spread of malaria. Efforts to decrease mosquito larvae by decreasing the availability of open water where they develop, or by adding substances to decrease their development, are effective in some locations. Electronic mosquito repellent devices, which make very high-frequency sounds that are supposed to keep female mosquitoes away, have no supporting evidence of effectiveness. There is a low certainty evidence that fogging may have an effect on malaria transmission. Larviciding by hand delivery of chemical or microbial insecticides into water bodies containing low larval distribution may reduce malarial transmission. There is insufficient evidence to determine whether larvivorous fish can decrease mosquito density and transmission in the area.

There are a number of medications that can help prevent or interrupt malaria in travellers to places where infection is common. Many of these medications are also used in treatment. In places where "Plasmodium" is resistant to one or more medications, three medications—mefloquine, doxycycline, or the combination of atovaquone/proguanil ("Malarone")—are frequently used for prevention. Doxycycline and the atovaquone/proguanil are better tolerated while mefloquine is taken once a week. Areas of the world with chloroquine-sensitive malaria are uncommon. Antimalarial mass drug administration to an entire population at the same time may reduce the risk of contracting malaria in the population.

The protective effect does not begin immediately, and people visiting areas where malaria exists usually start taking the drugs one to two weeks before they arrive, and continue taking them for four weeks after leaving (except for atovaquone/proguanil, which only needs to be started two days before and continued for seven days afterward). The use of preventative drugs is often not practical for those who live in areas where malaria exists, and their use is usually given only to pregnant women and short-term visitors. This is due to the cost of the drugs, side effects from long-term use, and the difficulty in obtaining antimalarial drugs outside of wealthy nations. During pregnancy, medication to prevent malaria has been found to improve the weight of the baby at birth and decrease the risk of anaemia in the mother. The use of preventative drugs where malaria-bearing mosquitoes are present may encourage the development of partial resistance.

Giving antimalarial drugs to infants through intermittent preventive therapy can reduce the risk of having malaria infection, hospital admission, and anaemia.

Mefloquine is more effective than sulfadoxine-pyrimethamine in preventing malaria for HIV-negative pregnant women. Cotrimoxazole is effective in preventing malaria infection and reduce the risk of getting anaemia in HIV-positive women. Giving sulfadoxine-pyrimethamine for three or more doses as intermittent preventive therapy is superior than two doses for HIV-positive women living in malaria-endemic areas.

Community participation and health education strategies promoting awareness of malaria and the importance of control measures have been successfully used to reduce the incidence of malaria in some areas of the developing world. Recognising the disease in the early stages can prevent it from becoming fatal. Education can also inform people to cover over areas of stagnant, still water, such as water tanks that are ideal breeding grounds for the parasite and mosquito, thus cutting down the risk of the transmission between people. This is generally used in urban areas where there are large centers of population in a confined space and transmission would be most likely in these areas. Intermittent preventive therapy is another intervention that has been used successfully to control malaria in pregnant women and infants, and in preschool children where transmission is seasonal.

Malaria is treated with antimalarial medications; the ones used depends on the type and severity of the disease. While medications against fever are commonly used, their effects on outcomes are not clear. Providing free antimalarial drugs to households may reduce childhood deaths when used appropriately. Programmes which presumptively treat all causes of fever with antimalarial drugs may lead to overuse of antimalarials and undertreat other causes of fever. Nevertheless, the use of malaria rapid-diagnostic kits can help to reduce over-usage of antimalarials.

Simple or uncomplicated malaria may be treated with oral medications. Arteminisin drugs are effective and safe in treating uncomplicated malaria. Arteminisim in combination with other antimalarials (known as artemisinin-combination therapy, or ACT) is about 90% effective when used to treat uncomplicated malaria. The most effective treatment for "P. falciparum" infection is the use of ACT, which decreases resistance to any single drug component. Artemether-lumefantrine (six-dose regimen) is more effective than the artemether-lumefantrine (four-dose regimen) or other regimens not containing artemisinin derivatives in treating falciparum malaria. Another recommended combination is dihydroartemisinin and piperaquine. Artemisinin-naphthoquine combination therapy showed promising results in treating falciparum malaria. However, more research need to establish its efficacy as a reliable treatment. Artesunate plus mefloquine performs better than mefloquine alone in treating uncomplicated falciparum malaria in low transmission settings. There is limited data to show atovaquone-proguanil is more effective than chloroquine, amodiaquine, and mefloquine in treating falciparum malaria. Azithromycin monotherapy or combination therapy has not shown effectiveness in treating plasmodium or vivax malaria. Amodiaquine plus sulfadoxine-pyrimethamine may achieve less treatment failures when compared to sulfadoxine-pyrimethamine alone in uncomplicated falciparum malaria. There is insufficient data on chlorproguanil-dapsone in treating uncomplicated falciparum malaria. The addition of primaquine with artemisinin-based combination therapy for falciparum malaria reduces its transmission at day 3-4 and day 8 of infection. Sulfadoxine-pyrimethamine plus artesunate is better than sulfadoxine-pyrimethamine plus amodiaquine in controlling treatment failure at day 28. However, the latter is better than the former in reducing gametocytes in blood at day 7.

Infection with "P. vivax", "P. ovale" or "P. malariae" usually does not require hospitalisation. Treatment of "P. vivax" requires both treatment of blood stages (with chloroquine or ACT) and clearance of liver forms with primaquine. Arteminisin-based combination therapy is as effective as chloroquine in treating uncomplicated "P. vivax" malaria. Treatment with tafenoquine prevents relapses after confirmed "P. vivax" malaria. However, for those treated with chloroquine for blood stage infection, 14 days of primaquine treatment is required to prevent relapse. Shorter primaquine regimens may lead to higher relapse rates. There is no difference in effectiveness between primaquine given for seven or 14 days for prevention of relapse in vivax malaria. The shorter regimen may be useful for those with treatment compliance problems.

To treat malaria during pregnancy, the WHO recommends the use of quinine plus clindamycin early in the pregnancy (1st trimester), and ACT in later stages (2nd and 3rd trimesters). There is limited safety data on the antimalarial drugs in pregnancy.

Cases of severe and complicated malaria are almost always caused by infection with "P. falciparum". The other species usually cause only febrile disease. Severe and complicated malaria cases are medical emergencies since mortality rates are high (10% to 50%).

Recommended treatment for severe malaria is the intravenous use of antimalarial drugs. For severe malaria, parenteral artesunate was superior to quinine in both children and adults. In another systematic review, artemisinin derivatives (artemether and arteether) were as efficacious as quinine in the treatment of cerebral malaria in children. Treatment of severe malaria involves supportive measures that are best done in a critical care unit. This includes the management of high fevers and the seizures that may result from it. It also includes monitoring for poor breathing effort, low blood sugar, and low blood potassium. Artemisinin derivatives have the same or better efficacy than quinolones in preventing deaths in severe or complicated malaria. Quinine loading dose helps to shorten the duration of fever and increases parasite clearance from the body. There is no difference in effectiveness when using intrarectal quinine compared to intravenous or intramuscular quinine in treating uncomplicated/complicated falciparum malaria. There is insufficient evidence for intramuscular arteether to treat severe malaria. The provision of rectal artesunate before transfer to hospital may reduce the rate of death for children with severe malaria.

Cerebral malaria is the form of severe and complicated malaria with the worst neurological symptoms. There is insufficient data on whether osmotic agents such as mannitol or urea are effective in treating cerebral malaria. Routine phenobarbitone in cerebral malaria is associated with fewer convulsions but possibly more deaths. There is no evidence that steroids would bring treatment benefits for cerebral malaria.

There is insufficient evidence to show that blood transfusion is useful in either reducing deaths for children with severe anaemia or in improving their haematocrit in one month. There is insufficient evidence that iron chelating agents such as deferoxamine and deferiprone improve outcomes of those with malaria falciparum infection.

Drug resistance poses a growing problem in 21st-century malaria treatment. In the 2000s (decade), malaria with partial resistance to artemisins emerged in Southeast Asia. Resistance is now common against all classes of antimalarial drugs apart from artemisinins. Treatment of resistant strains became increasingly dependent on this class of drugs. The cost of artemisinins limits their use in the developing world. Malaria strains found on the Cambodia–Thailand border are resistant to combination therapies that include artemisinins, and may, therefore, be untreatable. Exposure of the parasite population to artemisinin monotherapies in subtherapeutic doses for over 30 years and the availability of substandard artemisinins likely drove the selection of the resistant phenotype. Resistance to artemisinin has been detected in Cambodia, Myanmar, Thailand, and Vietnam, and there has been emerging resistance in Laos. Resistance to the combination of artemisinin and piperaquine was first detected in 2013 in Cambodia, and by 2019 had spread across Cambodia and into Laos, Thailand and Vietnam (with up to 80 percent of malaria parasites resistant in some regions).

There is insufficient evidence in unit packaged antimalarial drugs in preventing treatment failures of malaria infection. However, if supported by training of healthcare providers and patient information, there is improvement in compliance of those receiving treatment.

When properly treated, people with malaria can usually expect a complete recovery. However, severe malaria can progress extremely rapidly and cause death within hours or days. In the most severe cases of the disease, fatality rates can reach 20%, even with intensive care and treatment. Over the longer term, developmental impairments have been documented in children who have suffered episodes of severe malaria. Chronic infection without severe disease can occur in an immune-deficiency syndrome associated with a decreased responsiveness to "Salmonella" bacteria and the Epstein–Barr virus.

During childhood, malaria causes anaemia during a period of rapid brain development, and also direct brain damage resulting from cerebral malaria. Some survivors of cerebral malaria have an increased risk of neurological and cognitive deficits, behavioural disorders, and epilepsy. Malaria prophylaxis was shown to improve cognitive function and school performance in clinical trials when compared to placebo groups.

The WHO estimates that in 2018 there were 228 million new cases of malaria resulting in 405,000 deaths. Children under 5 years old are the most affected, accounting for 67% (272,000) of malaria deaths worldwide in 2018. About 125 million pregnant women are at risk of infection each year; in Sub-Saharan Africa, maternal malaria is associated with up to 200,000 estimated infant deaths yearly. There are about 10,000 malaria cases per year in Western Europe, and 1300–1500 in the United States. The United States eradicated malaria in 1951. About 900 people died from the disease in Europe between 1993 and 2003. Both the global incidence of disease and resulting mortality have declined in recent years. According to the WHO and UNICEF, deaths attributable to malaria in 2015 were reduced by 60% from a 2000 estimate of 985,000, largely due to the widespread use of insecticide-treated nets and artemisinin-based combination therapies. In 2012, there were 207 million cases of malaria. That year, the disease is estimated to have killed between 473,000 and 789,000 people, many of whom were children in Africa. Efforts at decreasing the disease in Africa since 2000 have been partially effective, with rates of the disease dropping by an estimated forty percent on the continent.

Malaria is presently endemic in a broad band around the equator, in areas of the Americas, many parts of Asia, and much of Africa; in Sub-Saharan Africa, 85–90% of malaria fatalities occur. An estimate for 2009 reported that countries with the highest death rate per 100,000 of population were Ivory Coast (86.15), Angola (56.93) and Burkina Faso (50.66). A 2010 estimate indicated the deadliest countries per population were Burkina Faso, Mozambique and Mali. The Malaria Atlas Project aims to map global levels of malaria, providing a way to determine the global spatial limits of the disease and to assess disease burden. This effort led to the publication of a map of "P. falciparum" endemicity in 2010 and an update in 2019. As of 2010, about 100 countries have endemic malaria. Every year, 125 million international travellers visit these countries, and more than 30,000 contract the disease.

The geographic distribution of malaria within large regions is complex, and malaria-afflicted and malaria-free areas are often found close to each other. Malaria is prevalent in tropical and subtropical regions because of rainfall, consistent high temperatures and high humidity, along with stagnant waters where mosquito larvae readily mature, providing them with the environment they need for continuous breeding. In drier areas, outbreaks of malaria have been predicted with reasonable accuracy by mapping rainfall. Malaria is more common in rural areas than in cities. For example, several cities in the Greater Mekong Subregion of Southeast Asia are essentially malaria-free, but the disease is prevalent in many rural regions, including along international borders and forest fringes. In contrast, malaria in Africa is present in both rural and urban areas, though the risk is lower in the larger cities.

Although the parasite responsible for "P. falciparum" malaria has been in existence for 50,000–100,000 years, the population size of the parasite did not increase until about 10,000 years ago, concurrently with advances in agriculture and the development of human settlements. Close relatives of the human malaria parasites remain common in chimpanzees. Some evidence suggests that the "P. falciparum" malaria may have originated in gorillas.

References to the unique periodic fevers of malaria are found throughout history. Hippocrates described periodic fevers, labelling them tertian, quartan, subtertian and quotidian. The Roman Columella associated the disease with insects from swamps. Malaria may have contributed to the decline of the Roman Empire, and was so pervasive in Rome that it was known as the "Roman fever". Several regions in ancient Rome were considered at-risk for the disease because of the favourable conditions present for malaria vectors. This included areas such as southern Italy, the island of Sardinia, the Pontine Marshes, the lower regions of coastal Etruria and the city of Rome along the Tiber. The presence of stagnant water in these places was preferred by mosquitoes for breeding grounds. Irrigated gardens, swamp-like grounds, run-off from agriculture, and drainage problems from road construction led to the increase of standing water. In Medieval West Africa, the people of Djenné successfully identified the mosquito as the vector and cause of malaria.

The term malaria originates from Mediaeval —"bad air"; the disease was formerly called "ague" or "marsh fever" due to its association with swamps and marshland. The term first appeared in the English literature about 1829. Malaria was once common in most of Europe and North America, where it is no longer endemic, though imported cases do occur.

Scientific studies on malaria made their first significant advance in 1880, when Charles Louis Alphonse Laveran—a French army doctor working in the military hospital of Constantine in Algeria—observed parasites inside the red blood cells of infected people for the first time. He, therefore, proposed that malaria is caused by this organism, the first time a protist was identified as causing disease. For this and later discoveries, he was awarded the 1907 Nobel Prize for Physiology or Medicine. A year later, Carlos Finlay, a Cuban doctor treating people with yellow fever in Havana, provided strong evidence that mosquitoes were transmitting disease to and from humans. This work followed earlier suggestions by Josiah C. Nott, and work by Sir Patrick Manson, the "father of tropical medicine", on the transmission of filariasis.

In April 1894, a Scottish physician, Sir Ronald Ross, visited Sir Patrick Manson at his house on Queen Anne Street, London. This visit was the start of four years of collaboration and fervent research that culminated in 1897 when Ross, who was working in the Presidency General Hospital in Calcutta, proved the complete life-cycle of the malaria parasite in mosquitoes. He thus proved that the mosquito was the vector for malaria in humans by showing that certain mosquito species transmit malaria to birds. He isolated malaria parasites from the salivary glands of mosquitoes that had fed on infected birds. For this work, Ross received the 1902 Nobel Prize in Medicine. After resigning from the Indian Medical Service, Ross worked at the newly established Liverpool School of Tropical Medicine and directed malaria-control efforts in Egypt, Panama, Greece and Mauritius. The findings of Finlay and Ross were later confirmed by a medical board headed by Walter Reed in 1900. Its recommendations were implemented by William C. Gorgas in the health measures undertaken during construction of the Panama Canal. This public-health work saved the lives of thousands of workers and helped develop the methods used in future public-health campaigns against the disease.

In 1896, Amico Bignami discussed the role of mosquitoes in malaria. In 1898, Bignami, Giovanni Battista Grassi and Giuseppe Bastianelli succeeded in showing experimentally the transmission of malaria in humans, using infected mosquitoes to contract malaria themselves which they presented in November 1898 to the Accademia dei Lincei.

The first effective treatment for malaria came from the bark of cinchona tree, which contains quinine. This tree grows on the slopes of the Andes, mainly in Peru. The indigenous peoples of Peru made a tincture of cinchona to control fever. Its effectiveness against malaria was found and the Jesuits introduced the treatment to Europe around 1640; by 1677, it was included in the London Pharmacopoeia as an antimalarial treatment. It was not until 1820 that the active ingredient, quinine, was extracted from the bark, isolated and named by the French chemists Pierre Joseph Pelletier and Joseph Bienaimé Caventou.

Quinine was the predominant malarial medication until the 1920s when other medications began to appear. In the 1940s, chloroquine replaced quinine as the treatment of both uncomplicated and severe malaria until resistance supervened, first in Southeast Asia and South America in the 1950s and then globally in the 1980s.

The medicinal value of "Artemisia annua" has been used by Chinese herbalists in traditional Chinese medicines for 2,000 years. In 1596, Li Shizhen recommended tea made from qinghao specifically to treat malaria symptoms in his "Compendium of Materia Medica". Artemisinins, discovered by Chinese scientist Tu Youyou and colleagues in the 1970s from the plant "Artemisia annua", became the recommended treatment for "P. falciparum" malaria, administered in severe cases in combination with other antimalarials. Tu says she was influenced by a traditional Chinese herbal medicine source, "The Handbook of Prescriptions for Emergency Treatments", written in 340 by Ge Hong. For her work on malaria, Tu Youyou received the 2015 Nobel Prize in Physiology or Medicine.

"Plasmodium vivax" was used between 1917 and the 1940s for malariotherapy—deliberate injection of malaria parasites to induce a fever to combat certain diseases such as tertiary syphilis. In 1927, the inventor of this technique, Julius Wagner-Jauregg, received the Nobel Prize in Physiology or Medicine for his discoveries. The technique was dangerous, killing about 15% of patients, so it is no longer in use.

The first pesticide used for indoor residual spraying was DDT. Although it was initially used exclusively to combat malaria, its use quickly spread to agriculture. In time, pest control, rather than disease control, came to dominate DDT use, and this large-scale agricultural use led to the evolution of pesticide-resistant mosquitoes in many regions. The DDT resistance shown by "Anopheles" mosquitoes can be compared to antibiotic resistance shown by bacteria. During the 1960s, awareness of the negative consequences of its indiscriminate use increased, ultimately leading to bans on agricultural applications of DDT in many countries in the 1970s. Before DDT, malaria was successfully eliminated or controlled in tropical areas like Brazil and Egypt by removing or poisoning the breeding grounds of the mosquitoes or the aquatic habitats of the larval stages, for example by applying the highly toxic arsenic compound Paris Green to places with standing water.

Malaria vaccines have been an elusive goal of research. The first promising studies demonstrating the potential for a malaria vaccine were performed in 1967 by immunising mice with live, radiation-attenuated sporozoites, which provided significant protection to the mice upon subsequent injection with normal, viable sporozoites. Since the 1970s, there has been a considerable effort to develop similar vaccination strategies for humans. The first vaccine, called RTS,S, was approved by European regulators in 2015.

Malaria is not just a disease commonly associated with poverty: some evidence suggests that it is also a cause of poverty and a major hindrance to economic development. Although tropical regions are most affected, malaria's furthest influence reaches into some temperate zones that have extreme seasonal changes. The disease has been associated with major negative economic effects on regions where it is widespread. During the late 19th and early 20th centuries, it was a major factor in the slow economic development of the American southern states.

A comparison of average per capita GDP in 1995, adjusted for parity of purchasing power, between countries with malaria and countries without malaria gives a fivefold difference (US$1,526 versus US$8,268). In the period 1965 to 1990, countries where malaria was common had an average per capita GDP that increased only 0.4% per year, compared to 2.4% per year in other countries.

Poverty can increase the risk of malaria since those in poverty do not have the financial capacities to prevent or treat the disease. In its entirety, the economic impact of malaria has been estimated to cost Africa US$12 billion every year. The economic impact includes costs of health care, working days lost due to sickness, days lost in education, decreased productivity due to brain damage from cerebral malaria, and loss of investment and tourism. The disease has a heavy burden in some countries, where it may be responsible for 30–50% of hospital admissions, up to 50% of outpatient visits, and up to 40% of public health spending.
Cerebral malaria is one of the leading causes of neurological disabilities in African children. Studies comparing cognitive functions before and after treatment for severe malarial illness continued to show significantly impaired school performance and cognitive abilities even after recovery. Consequently, severe and cerebral malaria have far-reaching socioeconomic consequences that extend beyond the immediate effects of the disease.

Sophisticated counterfeits have been found in several Asian countries such as Cambodia, China, Indonesia, Laos, Thailand, and Vietnam, and are an important cause of avoidable death in those countries. The WHO said that studies indicate that up to 40% of artesunate-based malaria medications are counterfeit, especially in the Greater Mekong region. They have established a rapid alert system to rapidly report information about counterfeit drugs to relevant authorities in participating countries. There is no reliable way for doctors or lay people to detect counterfeit drugs without help from a laboratory. Companies are attempting to combat the persistence of counterfeit drugs by using new technology to provide security from source to distribution.

Another clinical and public health concern is the proliferation of substandard antimalarial medicines resulting from inappropriate concentration of ingredients, contamination with other drugs or toxic impurities, poor quality ingredients, poor stability and inadequate packaging. A 2012 study demonstrated that roughly one-third of antimalarial medications in Southeast Asia and Sub-Saharan Africa failed chemical analysis, packaging analysis, or were falsified.

Throughout history, the contraction of malaria has played a prominent role in the fates of government rulers, nation-states, military personnel, and military actions. In 1910, Nobel Prize in Medicine-winner Ronald Ross (himself a malaria survivor), published a book titled "The Prevention of Malaria" that included a chapter titled "The Prevention of Malaria in War." The chapter's author, Colonel C. H. Melville, Professor of Hygiene at Royal Army Medical College in London, addressed the prominent role that malaria has historically played during wars: "The history of malaria in war might almost be taken to be the history of war itself, certainly the history of war in the Christian era. ... It is probably the case that many of the so-called camp fevers, and probably also a considerable proportion of the camp dysentery, of the wars of the sixteenth, seventeenth and eighteenth centuries were malarial in origin." In British-occupied India the cocktail gin and tonic may have come about as a way of taking quinine, known for its antimalarial properties.

Malaria was the most significant health hazard encountered by U.S. troops in the South Pacific during World War II, where about 500,000 men were infected. According to Joseph Patrick Byrne, "Sixty thousand American soldiers died of malaria during the African and South Pacific campaigns."

Significant financial investments have been made to procure existing and create new antimalarial agents. During World War I and World War II, inconsistent supplies of the natural antimalaria drugs cinchona bark and quinine prompted substantial funding into research and development of other drugs and vaccines. American military organisations conducting such research initiatives include the Navy Medical Research Center, Walter Reed Army Institute of Research, and the U.S. Army Medical Research Institute of Infectious Diseases of the US Armed Forces.

Additionally, initiatives have been founded such as Malaria Control in War Areas (MCWA), established in 1942, and its successor, the Communicable Disease Center (now known as the Centers for Disease Control and Prevention, or CDC) established in 1946. According to the CDC, MCWA "was established to control malaria around military training bases in the southern United States and its territories, where malaria was still problematic".

Several notable attempts are being made to eliminate the parasite from sections of the world, or to eradicate it worldwide. In 2006, the organisation Malaria No More set a public goal of eliminating malaria from Africa by 2015, and the organization claimed they planned to dissolve if that goal was accomplished. In 2007, World Malaria Day was established by the 60th session of World Health Assembly. As of 2018 they are still functioning. One malaria vaccine is currently licensed for use and several others are in clinical trials, which are intended to provide protection for children in endemic areas and reduce the speed of transmission of the disease. , The Global Fund to Fight AIDS, Tuberculosis and Malaria has distributed 230 million insecticide-treated nets intended to stop mosquito-borne transmission of malaria. The U.S.-based Clinton Foundation has worked to manage demand and stabilise prices in the artemisinin market. Other efforts, such as the Malaria Atlas Project, focus on analysing climate and weather information required to accurately predict the spread of malaria based on the availability of habitat of malaria-carrying parasites. The Malaria Policy Advisory Committee (MPAC) of the World Health Organization (WHO) was formed in 2012, "to provide strategic advice and technical input to WHO on all aspects of malaria control and elimination". In November 2013, WHO and the malaria vaccine funders group set a goal to develop vaccines designed to interrupt malaria transmission with the long-term goal of malaria eradication.

Malaria has been successfully eliminated or greatly reduced in certain areas. Malaria was once common in the United States and southern Europe, but vector control programs, in conjunction with the monitoring and treatment of infected humans, eliminated it from those regions. Several factors contributed, such as the draining of wetland breeding grounds for agriculture and other changes in water management practices, and advances in sanitation, including greater use of glass windows and screens in dwellings. Malaria was eliminated from most parts of the United States in the early 20th century by such methods, and the use of the pesticide DDT and other means eliminated it from the remaining pockets in the South in the 1950s as part of the National Malaria Eradication Program.

In 2015 the WHO targeted a 90% reduction in deaths from malaria by 2030 and Bill Gates said in 2016 that he thought global eradication would be possible by 2040.

In 2018, WHO announced that Paraguay was free of malaria, after an eradication effort that began in 1950.

As of 2019, the eradication process is ongoing, but with the current approaches and tools, it will be very hard to achieve a world free of malaria. Approaches may require investing more in research and greater universal health care. Continuing surveillance will also be important to prevent return of malaria in countries where the disease has been eliminated.

The Malaria Eradication Research Agenda (malERA) initiative was a consultative process to identify which areas of research and development (R&D) must be addressed for worldwide eradication of malaria.

A vaccine against malaria called RTS,S/AS01 (RTS,S) was approved by European regulators in 2015. As of 2019 it is undergoing pilot trials in 3 sub-Saharan African countries – Ghana, Kenya and Malawi – as part of the WHO's Malaria Vaccine Implementation Programme (MVIP).
Immunity (or, more accurately, tolerance) to "P. falciparum" malaria does occur naturally, but only in response to years of repeated infection. An individual can be protected from a "P. falciparum" infection if they receive about a thousand bites from mosquitoes that carry a version of the parasite rendered non-infective by a dose of X-ray irradiation. The highly polymorphic nature of many "P. falciparum" proteins results in significant challenges to vaccine design. Vaccine candidates that target antigens on gametes, zygotes, or ookinetes in the mosquito midgut aim to block the transmission of malaria. These transmission-blocking vaccines induce antibodies in the human blood; when a mosquito takes a blood meal from a protected individual, these antibodies prevent the parasite from completing its development in the mosquito. Other vaccine candidates, targeting the blood-stage of the parasite's life cycle, have been inadequate on their own. For example, SPf66 was tested extensively in areas where the disease was common in the 1990s, but trials showed it to be insufficiently effective.

Malaria parasites contain apicoplasts, organelles usually found in plants, complete with their own genomes. These apicoplasts are thought to have originated through the endosymbiosis of algae and play a crucial role in various aspects of parasite metabolism, such as fatty acid biosynthesis. Over 400 proteins have been found to be produced by apicoplasts and these are now being investigated as possible targets for novel antimalarial drugs.

With the onset of drug-resistant "Plasmodium" parasites, new strategies are being developed to combat the widespread disease. One such approach lies in the introduction of synthetic pyridoxal-amino acid adducts, which are taken up by the parasite and ultimately interfere with its ability to create several essential B vitamins. Antimalarial drugs using synthetic metal-based complexes are attracting research interest.

A non-chemical vector control strategy involves genetic manipulation of malaria mosquitoes. Advances in genetic engineering technologies make it possible to introduce foreign DNA into the mosquito genome and either decrease the lifespan of the mosquito, or make it more resistant to the malaria parasite. Sterile insect technique is a genetic control method whereby large numbers of sterile male mosquitoes are reared and released. Mating with wild females reduces the wild population in the subsequent generation; repeated releases eventually eliminate the target population.

Genomics is central to malaria research. With the sequencing of "P. falciparum", one of its vectors "Anopheles gambiae", and the human genome, the genetics of all three organisms in the malaria lifecycle can be studied. Another new application of genetic technology is the ability to produce genetically modified mosquitoes that do not transmit malaria, potentially allowing biological control of malaria transmission.

In one study, a genetically-modified strain of "Anopheles stephensi" was created that no longer supported malaria transmission, and this resistance was passed down to mosquito offspring.

Gene drive is a technique for changing wild populations, for instance to combat or eliminate insects so they cannot transmit diseases (in particular mosquitoes in the cases of malaria, zika, dengue and yellow fever).

Nearly 200 parasitic "Plasmodium" species have been identified that infect birds, reptiles, and other mammals, and about 30 species naturally infect non-human primates. Some malaria parasites that affect non-human primates (NHP) serve as model organisms for human malarial parasites, such as "P. coatneyi" (a model for "P. falciparum") and "P. cynomolgi" ("P. vivax"). Diagnostic techniques used to detect parasites in NHP are similar to those employed for humans. Malaria parasites that infect rodents are widely used as models in research, such as "P. berghei". Avian malaria primarily affects species of the order Passeriformes, and poses a substantial threat to birds of Hawaii, the Galapagos, and other archipelagoes. The parasite "P. relictum" is known to play a role in limiting the distribution and abundance of endemic Hawaiian birds. Global warming is expected to increase the prevalence and global distribution of avian malaria, as elevated temperatures provide optimal conditions for parasite reproduction.



</doc>
<doc id="20424" url="https://en.wikipedia.org/wiki?curid=20424" title="Lunar phase">
Lunar phase

The lunar phase or Moon phase is the shape of the directly sunlit portion of the Moon as viewed from Earth. The lunar phases gradually change over the period of a synodic month (about 29.53 days), as the orbital positions of the Moon around Earth and of Earth around the Sun shift.

The Moon's rotation is tidally locked by Earth's gravity; therefore, most of the same lunar side always faces Earth. This near side is variously sunlit, depending on the position of the Moon in its orbit. Thus, the sunlit portion of this face can vary from 0% (at new moon) to 100% (at full moon). The lunar terminator is the boundary between the illuminated and darkened hemispheres.

Each of the four "intermediate" lunar phases (see below) is around 7.4 days, but this varies slightly due to the elliptical shape of the Moon's orbit. Aside from some craters near the lunar poles, such as Shoemaker, all parts of the Moon see around 13.77 days of daylight, followed by 13.77 days of "night". (The side of the Moon facing away from Earth is sometimes called the "dark side of the Moon", although that is a misnomer).

There are four "principal" lunar phases: new moon, first quarter, full moon, and last quarter (also known as third or final quarter), when the Moon's ecliptic longitude is at an angle to the Sun (as viewed from Earth) of 0°, 90°, 180°, and 270°, respectively. Each of these phases appear at slightly different times at different locations on Earth. During the intervals between principal phases are "intermediate" phases, during which the Moon's apparent shape is either crescent or gibbous. The intermediate phases last one-quarter of a synodic month, or 7.38 days, on average. The descriptor "waxing" is used for an intermediate phase when the Moon's apparent shape is thickening, from new to full moon, and "waning" when the shape is thinning. The longest duration between full moon to new moon (or new moon to full moon) lasts about 15 days and 14.5 hours, while the shortest duration between full moon to new moon (or new moon to full moon) lasts only about 13 days and 22.5 hours.









Non-Western cultures may use a different number of lunar phases; for example, traditional Hawaiian culture has a total of 30 phases (one per day).

When the Sun and Moon are aligned on the same side of the Earth, the Moon is "new", and the side of the Moon facing Earth is not illuminated by the Sun. As the Moon "waxes" (the amount of illuminated surface as seen from Earth is increasing), the lunar phases progress through new moon, crescent moon, first-quarter moon, gibbous moon, and full moon. The Moon is then said to "wane" as it passes through the gibbous moon, third-quarter moon, crescent moon, and back to new moon. The terms "old moon" and "new moon" are not interchangeable. The "old moon" is a waning sliver (which eventually becomes undetectable to the naked eye) until the moment it aligns with the Sun and begins to wax, at which point it becomes new again. "Half moon" is often used to mean the first- and third-quarter moons, while the term "quarter" refers to the extent of the Moon's cycle around the Earth, not its shape.

When an illuminated hemisphere is viewed from a certain angle, the portion of the illuminated area that is visible will have a two-dimensional shape as defined by the intersection of an ellipse and circle (in which the ellipse's major axis coincides with the circle's diameter). If the half-ellipse is convex with respect to the half-circle, then the shape will be gibbous (bulging outwards), whereas if the half-ellipse is concave with respect to the half-circle, then the shape will be a crescent. When a crescent moon occurs, the phenomenon of earthshine may be apparent, where the night side of the Moon dimly reflects indirect sunlight reflected from Earth.

In the Northern Hemisphere, if the left (east) side of the Moon is dark, then the bright part is thickening, and the Moon is described as waxing (shifting toward full moon). If the right (west) side of the Moon is dark, then the bright part is thinning, and the Moon is described as waning (past full and shifting toward new moon). Assuming that the viewer is in the Northern Hemisphere, the right side of the Moon is the part that is always waxing. (That is, if the right side is dark, the Moon is becoming darker; if the right side is lit, the Moon is getting brighter.)

In the Southern Hemisphere, the Moon is observed from a perspective inverted, or rotated 180°, to that of the Northern and to all of the images in this article, so that the opposite sides appear to wax or wane.

Closer to the Equator, the lunar terminator will appear horizontal during the morning and evening. Since the above descriptions of the lunar phases only apply at middle or high latitudes, observers moving towards the tropics from northern or southern latitudes will see the Moon rotated anti-clockwise or clockwise with respect to the images in this article.

The lunar crescent can open upward or downward, with the "horns" of the crescent pointing up or down, respectively. When the Sun appears above the Moon in the sky, the crescent opens downward; when the Moon is above the Sun, the crescent opens upward. The crescent Moon is most clearly and brightly visible when the Sun is below the horizon, which implies that the Moon must be above the Sun, and the crescent must open upward. This is therefore the orientation in which the crescent Moon is most often seen from the tropics. The waxing and waning crescents look very similar. The waxing crescent appears in the western sky in the evening, and the waning crescent in the eastern sky in the morning.

When the Moon as seen from Earth is a thin crescent, Earth as viewed from the Moon is almost fully lit by the Sun. Often, the dark side of the Moon is dimly illuminated by indirect sunlight reflected from Earth, but is bright enough to be easily visible from Earth. This phenomenon is called earthshine and sometimes picturesquely described as "the old moon in the new moon's arms" or "the new moon in the old moon's arms".

The Gregorian calendar month, which is of a tropical year, is about 30.44 days, while the cycle of lunar phases (the Moon's synodic period) repeats every 29.53 days on average. Therefore, the timing of the lunar phases shifts by an average of almost one day for each successive month. (A lunar year lasts about 354 days.)

Photographing the Moon's phase every day for a month (starting in the evening after sunset, and repeating roughly 24 hours and 50 minutes later, and ending in the morning before sunrise) and arranging the series of photos on a calendar would create a composite image like the example calendar (May 8 – June 6, 2005) shown on the left. May 20 is blank because a picture would be taken before midnight on May 19 and the next after midnight on May 21.

Similarly, on a calendar listing moonrise or moonset times, some days will appear to be skipped. When moonrise precedes midnight one night, the next moonrise will follow midnight on the next night (so too with moonset). The "skipped day" is just a feature of the Moon's eastward movement in relation to the Sun, which at most latitudes, causes the Moon to rise later each day. The Moon follows a predictable orbit every month.

Each of the four intermediate phases lasts approximately seven days (7.38 days on average), but varies slightly due to lunar apogee and perigee.

The number of days counted from the time of the new moon is the Moon's "age". Each complete cycle of phases is called a "lunation".

The approximate age of the Moon, and hence the approximate phase, can be calculated for any date by calculating the number of days since a known new moon (such as January 1, 1900 or August 11, 1999) and reducing this modulo 29.530588853 (the length of a synodic month). The difference between two dates can be calculated by subtracting the Julian day number of one from that of the other, or there are simpler formulae giving (for instance) the number of days since December 31, 1899. However, this calculation assumes a perfectly circular orbit and makes no allowance for the time of day at which the new moon occurred and therefore may be incorrect by several hours. (It also becomes less accurate the larger the difference between the required date and the reference date). It is accurate enough to use in a novelty clock application showing lunar phase, but specialist usage taking account of lunar apogee and perigee requires a more elaborate calculation.

The Earth subtends an angle of about two degrees when seen from the Moon. This means that an observer on Earth who sees the Moon when it is close to the eastern horizon sees it from an angle that is about 2 degrees different from the line of sight of an observer who sees the Moon on the western horizon. The Moon moves about 12 degrees around its orbit per day, so, if these observers were stationary, they would see the phases of the Moon at times that differ by about one-sixth of a day, or 4 hours. But in reality, the observers are on the surface of the rotating Earth, so someone who sees the Moon on the eastern horizon at one moment sees it on the western horizon about 12 hours later. This adds an oscillation to the apparent progression of the lunar phases. They appear to occur more slowly when the Moon is high in the sky than when it is below the horizon. The Moon appears to move jerkily, and the phases do the same. The amplitude of this oscillation is never more than about four hours, which is a small fraction of a month. It does not have any obvious effect on the appearance of the Moon. However, it does affect accurate calculations of the times of lunar phases.

It might be expected that once every month, when the Moon passes between Earth and the Sun during a new moon, its shadow would fall on Earth causing a solar eclipse, but this does not happen every month. Nor is it true that during every full moon, the Earth's shadow falls on the Moon, causing a lunar eclipse. Solar and lunar eclipses are not observed "every" month because the plane of the Moon's orbit around the Earth is tilted by about 5° with respect to the plane of Earth's orbit around the Sun (the plane of the ecliptic). Thus, when new and full moons occur, the Moon usually lies to the north or south of a direct line through the Earth and Sun. Although an eclipse can only occur when the Moon is either new (solar) or full (lunar), it must also be positioned very near the intersection of Earth's orbital plane about the Sun and the Moon's orbital plane about the Earth (that is, at one of its nodes). This happens about twice per year, and so there are between four and seven eclipses in a calendar year. Most of these eclipses are partial; total eclipses of the Moon or Sun are less frequent.






</doc>
<doc id="20426" url="https://en.wikipedia.org/wiki?curid=20426" title="Metonic cycle">
Metonic cycle

The Metonic cycle or enneadecaeteris (from 
(enneakaidekaeteris), "nineteen") is a period of approximately 19 years after which the phases of the moon recur on the same day of the year. The recurrence is not perfect, and by precise observation the Metonic cycle is defined as 235 synodic lunar months, a period which is just 1h27m33s longer than 19 tropical years. Using these integer numbers facilitates the construction of a luni-solar calendar.

A tropical year is longer than 12 lunar months and shorter than 13 of them. The arithmetical equality "12x12 + 7x13 = 235" allows it to be seen that a combination of 12 'shorter' (12 months) years and 7 'longer' (13 months) years will be equal to 19=12+7 solar years.

Traditionally, for the Babylonian and Hebrew lunisolar calendars, the years 3, 6, 8, 11, 14, 17, and 19 are the long (13-month) years of the Metonic cycle. This cycle forms the basis of the Greek and Hebrew calendars, and is used for the computation of the date of Easter each year.

The Babylonians applied the 19-year cycle since the late sixth century BC. As they measured the moon's motion against the stars, the 235:19 relationship may originally have referred to sidereal years, instead of tropical years as it has been used for various calendars.

According to Livy, the king of Rome Numa Pompilius (753–673 BC) inserted intercalary months in such a way that "in the twentieth year the days should fall in with the same position of the sun from which they had started." As "the twentieth year" takes place nineteen years after "the first year", this seems to indicate that the Metonic cycle was applied to Numa's calendar.

Diodorus Siculus reports that Apollo is said to have visited the Hyperboreans once every 19 years.

The Metonic cycle has been implemented in the Antikythera mechanism which offers unexpected evidence for the popularity of the calendar based on it.
Meton of Athens approximated the cycle to a whole number (6,940) of days, obtained by 125 long months of 30 days and 110 short months of 29 days. During the next century, Callippus developed the Callippic cycle of four 19-year periods for a 76-year cycle with a mean year of exactly 365.25 days.

The (19-year) Metonic cycle is a lunisolar cycle, as is the (76-year) Callippic cycle. An important example of an application of the Metonic cycle in the Julian calendar is the 19-year lunar cycle insofar as provided with a Metonic structure. Around AD 260 the Alexandrian computist Anatolius, who became bishop of Laodicea in AD 268, was the first to construct a version of this efficient computistical instrument for determining the date of Easter Sunday. However, it was some later, somewhat different, version of the Metonic 19-year lunar cycle which ultimately, as the basic structure of Dionysius Exiguus’ and also of Bede’s Easter table, would prevail throughout Christendom for a long time, at least until in the year 1582, when the Julian calendar was replaced with the Gregorian calendar.

The Runic calendar is a perpetual calendar based on the 19-year-long Metonic cycle. Also known as a Rune staff or Runic Almanac, it appears to have been a medieval Swedish invention. This calendar does not rely on knowledge of the duration of the tropical year or of the occurrence of leap years. It is set at the beginning of each year by observing the first full moon after the winter solstice. The oldest one known, and the only one from the Middle Ages, is the Nyköping staff, which is believed to date from the 13th century.

The Bahá'í calendar, established during the middle of the 19th century, is also based on cycles of 19 years.

In China, the traditional Chinese calendar used the Metonic cycle ever since the first known ancient China calendar. The cycle was continually used until the 5th century when it was replaced by a more accurate cycle.

The importance of the tropical year for agriculture came to be realized much later than the adoption of lunar months for time keeping. However, it was recognized that the two cannot be easily coordinated over a short time span, so longer intervals were considered and the Metonic cycle was discovered as rather good, but not perfect, schema. The currently accepted values are:

The difference is 0.086 days for a cycle which mean that after a dozen returns there will be a full day of delay between the astronomical data and calculations. The error is actually one day every 219 years, or 12.4 parts per million. However, the Metonic cycle turned out to be very close to other periods:

Being close (to somewhat more than half a day) to 255 draconic months, the Metonic cycle is also an eclipse cycle, which lasts only for about 4 or 5 recurrences of eclipses. The Octon is of a Metonic cycle (47 synodic months, 3.8 years), and it recurs about 20 to 25 cycles.

This cycle seems to be a coincidence. The periods of the Moon's orbit around the Earth and the Earth's orbit around the Sun are believed to be independent, and not to have any known physical resonance. An example of a non-coincidental cycle is the orbit of Mercury, with its 3:2 spin-orbit resonance.

A lunar year of 12 synodic months is about 354 days, approximately 11 days short of the "365-day" solar year. Therefore, for a lunisolar calendar, every 2 to 3 years there is a difference of more than a full lunar month between the lunar and solar years, and an extra ("embolismic") month needs to be inserted (intercalation). The Athenians initially seem not to have had a regular means of intercalating a 13th month; instead, the question of when to add a month was decided by an official. Meton's discovery made it possible to propose a regular intercalation scheme. The Babylonians seem to have introduced this scheme around 500 BC, thus well before Meton.

The Metonic cycle is related to two less accurate subcycles:


By combining appropriate numbers of 11-year and 19-year periods, it is possible to generate ever more accurate cycles. For example, simple arithmetic shows that:


This gives an error of only about half an hour in 687 years (2.5 seconds a year), although this is subject to secular variation in the length of the tropical year and the lunation.

At the time of Meton, axial precession had not yet been discovered, and he could not distinguish between sidereal years (currently: 365.256363 days) and tropical years (currently: 365.242190 days). Most calendars, like the commonly used Gregorian calendar, are based on the tropical year and maintain the seasons at the same calendar times each year.





</doc>
<doc id="20427" url="https://en.wikipedia.org/wiki?curid=20427" title="March 26">
March 26





</doc>
<doc id="20428" url="https://en.wikipedia.org/wiki?curid=20428" title="Marcello Malpighi">
Marcello Malpighi

Marcello Malpighi (10 March 1628 – 29 November 1694) was an Italian biologist and physician, who is referred to as the "Founder of microscopical anatomy, histology & Father of physiology and embryology". Malpighi's name is borne by several physiological features related to the biological excretory system, such as the Malpighian corpuscles and Malpighian pyramids of the kidneys and the Malpighian tubule system of insects. The splenic lymphoid nodules are often called the "Malpighian bodies of the spleen" or Malpighian corpuscles. The botanical family Malpighiaceae is also named after him. He was the first person to see capillaries in animals, and he discovered the link between arteries and veins that had eluded William Harvey. Malpighi was one of the earliest people to observe red blood cells under a microscope, after Jan Swammerdam. His treatise "De polypo cordis" (1666) was important for understanding blood composition, as well as how blood clots. In it, Malpighi described how the form of a blood clot differed in the right against the left sides of the heart.

The use of the microscope enabled Malpighi to discover that invertebrates do not use lungs to breathe, but small holes in their skin called tracheae. Malpighi also studied the anatomy of the brain and concluded this organ is a gland. In terms of modern endocrinology, this deduction is correct because the hypothalamus of the brain has long been recognized for its hormone-secreting capacity.

Because Malpighi had a wide knowledge of both plants and animals, he made contributions to the scientific study of both. The Royal Society of London published two volumes of his botanical and zoological works in 1675 and 1679. Another edition followed in 1687, and a supplementary volume in 1697. In his autobiography, Malpighi speaks of his "Anatome Plantarum", decorated with the engravings of Robert White, as "the most elegant format in the whole literate world."

His study of plants led him to conclude that plants had tubules similar to those he saw in insects like the silk worm (using his microscope, he probably saw the stomata, through which plants exchange carbon dioxide with oxygen). Malpighi observed that when a ring-like portion of bark was removed on a trunk a swelling occurred in the tissues above the ring, and he correctly interpreted this as growth stimulated by food coming down from the leaves, and being blocked above the ring.

Malpighi was born on 10 March 1628 at Crevalcore near Bologna, Italy. The son of well-to-do parents, Malpighi was educated in his native city, entering the University of Bologna at the age of 17. In a posthumous work delivered and dedicated to the Royal Society in London in 1697, Malpighi says he completed his grammatical studies in 1645, at which point he began to apply himself to the study of peripatetic philosophy. He completed these studies about 1649, where at the persuasion of his mother Frances Natalis, he began to study physics. When his parents and grandmother became ill, he returned to his family home near Bologna to care for them.
Malpighi studied Aristotelian philosophy at the University of Bologna while he was very young. 
Despite opposition from the university authorities because he was non-Bolognese by birth, in 1653 he was granted doctorates in both medicine and philosophy. He later graduated as a medical doctor at the age of 25. Subsequently, he was appointed as a teacher, whereupon he immediately dedicated himself to further study in anatomy and medicine. For most of his career, Malpighi combined an intense interest in scientific research with a fond love of teaching. He was invited to correspond with the Royal Society in 1667 by Henry Oldenburg, and became a fellow of the society the next year.

In 1656, Ferdinand II of Tuscany invited him to the professorship of theoretical medicine at the University of Pisa. There Malpighi began his lifelong friendship with Giovanni Borelli, mathematician and naturalist, who was a prominent supporter of the Accademia del Cimento, one of the first scientific societies. Malpighi questioned the prevailing medical teachings at Pisa, tried experiments on colour changes in blood, and attempted to recast anatomical, physiological, and medical problems of the day. Family responsibilities and poor health prompted Malpighi's return in 1659 to the University of Bologna, where he continued to teach and do research with his microscopes. In 1661 he identified and described the pulmonary and capillary network connecting small arteries with small veins. Malpighi's views evoked increasing controversy and dissent, mainly from envy and lack of understanding on the part of his colleagues.

In 1653, his father, mother, and grandmother being dead, Malpighi left his family villa and returned to the University of Bologna to study anatomy. In 1656, he was made a reader at Bologna, and then a professor of physics at Pisa, where he began to abandon the disputative method of learning and apply himself to a more experimental method of research. Based on this research, he wrote some "Dialogues against the Peripatetics and Galenists" (those who followed the precepts of Galen), which were destroyed when his house burned down. Weary of philosophical disputation, in 1660, Malpighi returned to Bologna and dedicated himself to the study of anatomy. He subsequently discovered a new structure of the lungs which led him to several disputes with the learned medical men of the times. In 1662, he was made a professor of Physics at the Academy of Messina.

Retiring from university life to his villa in the country near Bologna in 1663, he worked as a physician while continuing to conduct experiments on the plants and insects he found on his estate. There he made discoveries of the structure of plants which he published in his "Observations". At the end of 1666, Malpighi was invited to return to the public academy at Messina, which he did in 1667. Although he accepted temporary chairs at the universities of Pisa and Messina, throughout his life he continuously returned to Bologna to practice medicine, a city that repaid him by erecting a monument in his memory after his death.

In 1668, Malpighi received a letter from Mr. Oldenburg of the Royal Society in London, inviting him to correspond. Malpighi wrote his history of the silkworm in 1668, and sent the manuscript to Mr. Oldenburg. As a result, Malpighi was made a member of the Royal Society in 1669. In 1671, Malpighi's "Anatomy of Plants" was published in London by the Royal Society, and he simultaneously wrote to Mr. Oldenburg, telling him of his recent discoveries regarding the lungs, fibers of the spleen and testicles, and several other discoveries involving the brain and sensory organs. He also shared more information regarding his research on plants. At that time, he related his disputes with some younger physicians who were strenuous supporters of the Galenic principles and opposed to all new discoveries. Following many other discoveries and publications, in 1691, Malpighi was invited to Rome by Pope Innocent XII to become papal physician and professor of medicine at the Papal Medical School. He remained in Rome until his death.

Marcello Malpighi is buried in the church of the Santi Gregorio e Siro, in Bologna, where nowadays can be seen a marble monument to the scientist with an inscription in Latin remembering – among other things – his "SUMMUM INGENIUM / INTEGERRIMAM VITAM / FORTEM STRENUAMQUE MENTEM / AUDACEM SALUTARIS ARTIS AMOREM" (great genius, honest life, strong and tough mind, daring love for the medical art).

Around the age of 38, and with a remarkable academic career behind him, Malpighi decided to dedicate his free time to anatomical studies. Although he conducted some of his studies using vivisection and others through the dissection of corpses, his most illustrative efforts appear to have been based on the use of the microscope. Because of this work, many microscopic anatomical structures are named after Malpighi, including a skin layer (Malpighi layer) and two different Malpighian corpuscles in the kidneys and the spleen, as well as the Malpighian tubules in the excretory system of insects.

Although a Dutch spectacle maker created the compound lens and inserted it in a microscope around the turn of the 17th century, and Galileo had applied the principle of the compound lens to the making of his microscope patented in 1609, its possibilities as a microscope had remained unexploited for half a century, until Robert Hooke improved the instrument. Following this, Marcello Malpighi, Hooke, and two other early investigators associated with the Royal Society, Nehemiah Grew and Antoine van Leeuwenhoek were fortunate to have a virtually untried tool in their hands as they began their investigations.

In 1661, Malpighi observed capillary structures in frog lungs. Extrapolating to humans, he offered an explanation for how air and blood mix in the lungs. Malpighi also used the microscope for his studies of the skin, kidneys, and liver. For example, after he dissected a black male, Malpighi made some groundbreaking headway into the discovery of the origin of black skin. He found that the black pigment was associated with a layer of mucus just beneath the skin.

A talented sketch artist, Malpighi seems to have been the first author to have made detailed drawings of individual organs of flowers. In his "Anatome plantarum" is a longitudinal section of a flower of "Nigella" (his Melanthi, literally honey-flower) with details of the nectariferous organs. He adds that it is strange that nature has produced on the leaves of the flower shell-like organs in which honey is produced.

Malpighi had success in tracing the ontogeny of plant organs, and the serial development of the shoot owing to his instinct shaped in the sphere of animal embryology. He specialized in seedling development, and in 1679, he published a volume containing a series of exquisitely drawn and engraved images of the stages of development of Leguminosae (beans) and Cucurbitaceae (squash, melons). Later, he published material depicting the development of the date palm. The great Swedish botanist Linnaeus named the genus "Malpighia" in honor of Malpighi's work with plants; "Malpighia" is the type genus for the Malpighiaceae, a family of tropical and subtropical flowering plants.

Because Malpighi was concerned with teratology (the scientific study of the visible conditions caused by the interruption or alteration of normal development) he expressed grave misgivings about the view of his contemporaries that the galls of trees and herbs gave birth to insects. He conjectured (correctly) that the creatures in question arose from eggs previously laid in the plant tissue.

Malpighi's investigations of the lifecycle of plants and animals led him into the topic of reproduction. He created detailed drawings of his studies of chick embryo development, seed development in plants (such as the lemon tree), and the transformation of caterpillars into insects. His discoveries helped to illuminate philosophical arguments surrounding the topics of "emboîtment", pre-existence, preformation, epigenesis, and metamorphosis.

In 1691 Pope Innocent XII invited him to Rome as papal physician. He taught medicine in the Papal Medical School and wrote a long treatise about his studies which he donated to the Royal Society of London.

Marcello Malpighi died of apoplexy (an old-fashioned term for a stroke or stroke-like symptoms) in Rome on 29 September 1694, at the age of 66. In accordance with his wishes, an autopsy was performed. The Royal Society published his studies in 1696. Asteroid 11121 Malpighi is named in his honor.

Malpighi is buried in the church of the Santi Gregorio e Siro, in Bologna, where nowadays can be seen a marble monument to the scientist with an inscription in Latin remembering – among other things – his "SUMMUM INGENIUM / INTEGERRIMAM VITAM / FORTEM STRENUAMQUE MENTEM / AUDACEM SALUTARIS ARTIS AMOREM" (great genius, honest life, strong and tough mind, daring love for the medical art).




</doc>
<doc id="20431" url="https://en.wikipedia.org/wiki?curid=20431" title="Momentum">
Momentum

In Newtonian mechanics, linear momentum, translational momentum, or simply momentum (pl. momenta) is the product of the mass and velocity of an object. It is a vector quantity, possessing a magnitude and a direction. If is an object's mass and is its velocity (also a vector quantity), then the object's momentum is:<br>
formula_1<br>
In SI units, momentum is measured in kilogram meters per second (kg⋅m/s).

Newton's second law of motion states that the rate of change of a body's momentum is equal to the net force acting on it. Momentum depends on the frame of reference, but in any inertial frame it is a "conserved" quantity, meaning that if a closed system is not affected by external forces, its total linear momentum does not change. Momentum is also conserved in special relativity (with a modified formula) and, in a modified form, in electrodynamics, quantum mechanics, quantum field theory, and general relativity. It is an expression of one of the fundamental symmetries of space and time: translational symmetry.

Advanced formulations of classical mechanics, Lagrangian and Hamiltonian mechanics, allow one to choose coordinate systems that incorporate symmetries and constraints. In these systems the conserved quantity is generalized momentum, and in general this is different from the kinetic momentum defined above. The concept of generalized momentum is carried over into quantum mechanics, where it becomes an operator on a wave function. The momentum and position operators are related by the Heisenberg uncertainty principle.

In continuous systems such as electromagnetic fields, fluid dynamics and deformable bodies, a momentum density can be defined, and a continuum version of the conservation of momentum leads to equations such as the Navier–Stokes equations for fluids or the Cauchy momentum equation for deformable solids or fluids.

Momentum is a vector quantity: it has both magnitude and direction. Since momentum has a direction, it can be used to predict the resulting direction and speed of motion of objects after they collide. Below, the basic properties of momentum are described in one dimension. The vector equations are almost identical to the scalar equations (see multiple dimensions).

The momentum of a particle is conventionally represented by the letter . It is the product of two quantities, the particle's mass (represented by the letter ) and its velocity ():

The unit of momentum is the product of the units of mass and velocity. In SI units, if the mass is in kilograms and the velocity is in meters per second then the momentum is in kilogram meters per second (kg⋅m/s). In cgs units, if the mass is in grams and the velocity in centimeters per second, then the momentum is in gram centimeters per second (g⋅cm/s).

Being a vector, momentum has magnitude and direction. For example, a 1 kg model airplane, traveling due north at 1 m/s in straight and level flight, has a momentum of 1 kg⋅m/s due north measured with reference to the ground.

The momentum of a system of particles is the vector sum of their momenta. If two particles have respective masses and , and velocities and , the total momentum is
The momenta of more than two particles can be added more generally with the following:

A system of particles has a center of mass, a point determined by the weighted sum of their positions:

If one or more of the particles is moving, the center of mass of the system will generally be moving as well (unless the system is in pure rotation around it). If the total mass of the particles is formula_6, and the center of mass is moving at velocity , the momentum of the system is:
This is known as Euler's first law.

If the net force applied to a particle is constant, and is applied for a time interval , the momentum of the particle changes by an amount

In differential form, this is Newton's second law; the rate of change of the momentum of a particle is equal to the instantaneous force acting on it,

If the net force experienced by a particle changes as a function of time, , the change in momentum (or impulse ) between times and is

Impulse is measured in the derived units of the newton second (1 N⋅s = 1 kg⋅m/s) or dyne second (1 dyne⋅s = 1 g⋅cm/s)

Under the assumption of constant mass , it is equivalent to write
hence the net force is equal to the mass of the particle times its acceleration.

"Example": A model airplane of mass 1 kg accelerates from rest to a velocity of 6 m/s due north in 2 s. The net force required to produce this acceleration is 3 newtons due north. The change in momentum is 6 kg⋅m/s due north. The rate of change of momentum is 3 (kg⋅m/s)/s due north which is numerically equivalent to 3 newtons.

In a closed system (one that does not exchange any matter with its surroundings and is not acted on by external forces) the total momentum is constant. This fact, known as the "law of conservation of momentum", is implied by Newton's laws of motion. Suppose, for example, that two particles interact. Because of the third law, the forces between them are equal and opposite. If the particles are numbered 1 and 2, the second law states that and . Therefore,
with the negative sign indicating that the forces oppose. Equivalently,

If the velocities of the particles are and before the interaction, and afterwards they are and , then

This law holds no matter how complicated the force is between particles. Similarly, if there are several particles, the momentum exchanged between each pair of particles adds up to zero, so the total change in momentum is zero. This conservation law applies to all interactions, including collisions and separations caused by explosive forces. It can also be generalized to situations where Newton's laws do not hold, for example in the theory of relativity and in electrodynamics.

Momentum is a measurable quantity, and the measurement depends on the motion of the observer. For example: if an apple is sitting in a glass elevator that is descending, an outside observer, looking into the elevator, sees the apple moving, so, to that observer, the apple has a non-zero momentum. To someone inside the elevator, the apple does not move, so, it has zero momentum. The two observers each have a frame of reference, in which, they observe motions, and, if the elevator is descending steadily, they will see behavior that is consistent with those same physical laws.

Suppose a particle has position in a stationary frame of reference. From the point of view of another frame of reference, moving at a uniform speed , the position (represented by a primed coordinate) changes with time as
This is called a Galilean transformation. If the particle is moving at speed in the first frame of reference, in the second, it is moving at speed
Since does not change, the accelerations are the same:
Thus, momentum is conserved in both reference frames. Moreover, as long as the force has the same form, in both frames, Newton's second law is unchanged. Forces such as Newtonian gravity, which depend only on the scalar distance between objects, satisfy this criterion. This independence of reference frame is called Newtonian relativity or Galilean invariance.

A change of reference frame, can, often, simplify calculations of motion. For example, in a collision of two particles, a reference frame can be chosen, where, one particle begins at rest. Another, commonly used reference frame, is the center of mass frame – one that is moving with the center of mass. In this frame,
the total momentum is zero.

By itself, the law of conservation of momentum is not enough to determine the motion of particles after a collision. Another property of the motion, kinetic energy, must be known. This is not necessarily conserved. If it is conserved, the collision is called an "elastic collision"; if not, it is an "inelastic collision".

An elastic collision is one in which no kinetic energy is absorbed in the collision. Perfectly elastic "collisions" can occur when the objects do not touch each other, as for example in atomic or nuclear scattering where electric repulsion keeps them apart. A slingshot maneuver of a satellite around a planet can also be viewed as a perfectly elastic collision. A collision between two pool balls is a good example of an "almost" totally elastic collision, due to their high rigidity, but when bodies come in contact there is always some dissipation.

A head-on elastic collision between two bodies can be represented by velocities in one dimension, along a line passing through the bodies. If the velocities are and before the collision and and after, the equations expressing conservation of momentum and kinetic energy are:

A change of reference frame can simplify analysis of a collision. For example, suppose there are two bodies of equal mass , one stationary and one approaching the other at a speed (as in the figure). The center of mass is moving at speed and both bodies are moving towards it at speed . Because of the symmetry, after the collision both must be moving away from the center of mass at the same speed. Adding the speed of the center of mass to both, we find that the body that was moving is now stopped and the other is moving away at speed . The bodies have exchanged their velocities. Regardless of the velocities of the bodies, a switch to the center of mass frame leads us to the same conclusion. Therefore, the final velocities are given by

In general, when the initial velocities are known, the final velocities are given by
If one body has much greater mass than the other, its velocity will be little affected by a collision while the other body will experience a large change.

In an inelastic collision, some of the kinetic energy of the colliding bodies is converted into other forms of energy (such as heat or sound). Examples include traffic collisions, in which the effect of loss of kinetic energy can be seen in the damage to the vehicles; electrons losing some of their energy to atoms (as in the Franck–Hertz experiment); and particle accelerators in which the kinetic energy is converted into mass in the form of new particles.

In a perfectly inelastic collision (such as a bug hitting a windshield), both bodies have the same motion afterwards. A head-on inelastic collision between two bodies can be represented by velocities in one dimension, along a line passing through the bodies. If the velocities are and before the collision then in a perfectly inelastic collision both bodies will be travelling with velocity after the collision. The equation expressing conservation of momentum is:
If one body is motionless to begin with (e.g. formula_23), the equation for conservation of momentum is
so
In a different situation, if the frame of reference is moving at the final velocity such that formula_26, the objects would be brought to rest by a perfectly inelastic collision and 100% of the kinetic energy is converted to other forms of energy. In this instance the initial velocities of the bodies would be non-zero, or the bodies would have to be massless.

One measure of the inelasticity of the collision is the coefficient of restitution , defined as the ratio of relative velocity of separation to relative velocity of approach. In applying this measure to a ball bouncing from a solid surface, this can be easily measured using the following formula:

The momentum and energy equations also apply to the motions of objects that begin together and then move apart. For example, an explosion is the result of a chain reaction that transforms potential energy stored in chemical, mechanical, or nuclear form into kinetic energy, acoustic energy, and electromagnetic radiation. Rockets also make use of conservation of momentum: propellant is thrust outward, gaining momentum, and an equal and opposite momentum is imparted to the rocket.

Real motion has both direction and velocity and must be represented by a vector. In a coordinate system with axes, velocity has components in the -direction, in the -direction, in the -direction. The vector is represented by a boldface symbol:
Similarly, the momentum is a vector quantity and is represented by a boldface symbol:

The equations in the previous sections, work in vector form if the scalars and are replaced by vectors and . Each vector equation represents three scalar equations. For example,
represents three equations:

The kinetic energy equations are exceptions to the above replacement rule. The equations are still one-dimensional, but each scalar represents the magnitude of the vector, for example,
Each vector equation represents three scalar equations. Often coordinates can be chosen so that only two components are needed, as in the figure. Each component can be obtained separately and the results combined to produce a vector result.

A simple construction involving the center of mass frame can be used to show that if a stationary elastic sphere is struck by a moving sphere, the two will head off at right angles after the collision (as in the figure).

The concept of momentum plays a fundamental role in explaining the behavior of variable-mass objects such as a rocket ejecting fuel or a star accreting gas. In analyzing such an object, one treats the object's mass as a function that varies with time: . The momentum of the object at time is therefore . One might then try to invoke Newton's second law of motion by saying that the external force on the object is related to its momentum by , but this is incorrect, as is the related expression found by applying the product rule to :

This equation does not correctly describe the motion of variable-mass objects. The correct equation is
where is the velocity of the ejected/accreted mass "as seen in the object's rest frame". This is distinct from , which is the velocity of the object itself as seen in an inertial frame.

This equation is derived by keeping track of both the momentum of the object as well as the momentum of the ejected/accreted mass ("dm"). When considered together, the object and the mass ("dm") constitute a closed system in which total momentum is conserved.

Newtonian physics assumes that absolute time and space exist outside of any observer; this gives rise to Galilean invariance. It also results in a prediction that the speed of light can vary from one reference frame to another. This is contrary to observation. In the special theory of relativity, Einstein keeps the postulate that the equations of motion do not depend on the reference frame, but assumes that the speed of light is invariant. As a result, position and time in two reference frames are related by the Lorentz transformation instead of the Galilean transformation.

Consider, for example, one reference frame moving relative to another at velocity in the direction. The Galilean transformation gives the coordinates of the moving frame as
while the Lorentz transformation gives
where is the Lorentz factor:

Newton's second law, with mass fixed, is not invariant under a Lorentz transformation. However, it can be made invariant by making the "inertial mass" of an object a function of velocity:

The modified momentum,
obeys Newton's second law:

Within the domain of classical mechanics, relativistic momentum closely approximates Newtonian momentum: at low velocity, is approximately equal to , the Newtonian expression for momentum.

In the theory of special relativity, physical quantities are expressed in terms of four-vectors that include time as a fourth coordinate along with the three space coordinates. These vectors are generally represented by capital letters, for example for position. The expression for the "four-momentum" depends on how the coordinates are expressed. Time may be given in its normal units or multiplied by the speed of light so that all the components of the four-vector have dimensions of length. If the latter scaling is used, an interval of proper time, , defined by
is invariant under Lorentz transformations (in this expression and in what follows the metric signature has been used, different authors use different conventions). Mathematically this invariance can be ensured in one of two ways: by treating the four-vectors as Euclidean vectors and multiplying time by ; or by keeping time a real quantity and embedding the vectors in a Minkowski space. In a Minkowski space, the scalar product of two four-vectors and is defined as

In all the coordinate systems, the (contravariant) relativistic four-velocity is defined by
and the (contravariant) four-momentum is
where is the invariant mass. If (in Minkowski space), then
Using Einstein's mass-energy equivalence, , this can be rewritten as
Thus, conservation of four-momentum is Lorentz-invariant and implies conservation of both mass and energy.

The magnitude of the momentum four-vector is equal to :
and is invariant across all reference frames.

The relativistic energy–momentum relationship holds even for massless particles such as photons; by setting it follows that

In a game of relativistic "billiards", if a stationary particle is hit by a moving particle in an elastic collision, the paths formed by the two afterwards will form an acute angle. This is unlike the non-relativistic case where they travel at right angles.

The four-momentum of a planar wave can be related to a wave four-vector
For a particle, the relationship between temporal components, , is the Planck–Einstein relation, and the relation between spatial components, , describes a de Broglie matter wave.

Newton's laws can be difficult to apply to many kinds of motion because the motion is limited by "constraints". For example, a bead on an abacus is constrained to move along its wire and a pendulum bob is constrained to swing at a fixed distance from the pivot. Many such constraints can be incorporated by changing the normal Cartesian coordinates to a set of "generalized coordinates" that may be fewer in number. Refined mathematical methods have been developed for solving mechanics problems in generalized coordinates. They introduce a "generalized momentum", also known as the "canonical" or "conjugate momentum", that extends the concepts of both linear momentum and angular momentum. To distinguish it from generalized momentum, the product of mass and velocity is also referred to as "mechanical", "kinetic" or "kinematic momentum". The two main methods are described below.

In Lagrangian mechanics, a Lagrangian is defined as the difference between the kinetic energy and the potential energy :

If the generalized coordinates are represented as a vector and time differentiation is represented by a dot over the variable, then the equations of motion (known as the Lagrange or Euler–Lagrange equations) are a set of equations:
If a coordinate is not a Cartesian coordinate, the associated generalized momentum component does not necessarily have the dimensions of linear momentum. Even if is a Cartesian coordinate, will not be the same as the mechanical momentum if the potential depends on velocity. Some sources represent the kinematic momentum by the symbol .

In this mathematical framework, a generalized momentum is associated with the generalized coordinates. Its components are defined as
Each component is said to be the "conjugate momentum" for the coordinate .

Now if a given coordinate does not appear in the Lagrangian (although its time derivative might appear), then
This is the generalization of the conservation of momentum.

Even if the generalized coordinates are just the ordinary spatial coordinates, the conjugate momenta are not necessarily the ordinary momentum coordinates. An example is found in the section on electromagnetism.

In Hamiltonian mechanics, the Lagrangian (a function of generalized coordinates and their derivatives) is replaced by a Hamiltonian that is a function of generalized coordinates and momentum. The Hamiltonian is defined as
where the momentum is obtained by differentiating the Lagrangian as above. The Hamiltonian equations of motion are
As in Lagrangian mechanics, if a generalized coordinate does not appear in the Hamiltonian, its conjugate momentum component is conserved.

Conservation of momentum is a mathematical consequence of the homogeneity (shift symmetry) of space (position in space is the canonical conjugate quantity to momentum). That is, conservation of momentum is a consequence of the fact that the laws of physics do not depend on position; this is a special case of Noether's theorem.

In Maxwell's equations, the forces between particles are mediated by electric and magnetic fields. The electromagnetic force ("Lorentz force") on a particle with charge due to a combination of electric field and magnetic field is
(in SI units).
It has an electric potential and magnetic vector potential .
In the non-relativistic regime, its generalized momentum is

while in relativistic mechanics this becomes

formula_59

The quantity formula_60 is sometimes called the "potential momentum". It is the momentum due to the interaction of the particle with the electromagnetic fields. The name is an analogy with the potential energy formula_61, which is the energy due to the interaction of the particle with the electromagnetic fields. These quantities form a four-vector, so the analogy is consistent; besides, the concept of potential momentum is important in explaining the so-called hidden-momentum of the electromagnetic fields

In Newtonian mechanics, the law of conservation of momentum can be derived from the law of action and reaction, which states that every force has a reciprocating equal and opposite force. Under some circumstances, moving charged particles can exert forces on each other in non-opposite directions. Nevertheless, the combined momentum of the particles and the electromagnetic field is conserved.

The Lorentz force imparts a momentum to the particle, so by Newton's second law the particle must impart a momentum to the electromagnetic fields.

In a vacuum, the momentum per unit volume is
where is the vacuum permeability and is the speed of light. The momentum density is proportional to the Poynting vector which gives the directional rate of energy transfer per unit area:

If momentum is to be conserved over the volume over a region , changes in the momentum of matter through the Lorentz force must be balanced by changes in the momentum of the electromagnetic field and outflow of momentum. If is the momentum of all the particles in , and the particles are treated as a continuum, then Newton's second law gives
The electromagnetic momentum is
and the equation for conservation of each component of the momentum is
The term on the right is an integral over the surface area of the surface representing momentum flow into and out of the volume, and is a component of the surface normal of . The quantity is called the Maxwell stress tensor, defined as

The above results are for the "microscopic" Maxwell equations, applicable to electromagnetic forces in a vacuum (or on a very small scale in media). It is more difficult to define momentum density in media because the division into electromagnetic and mechanical is arbitrary. The definition of electromagnetic momentum density is modified to
where the H-field is related to the B-field and the magnetization by
The electromagnetic stress tensor depends on the properties of the media.

In quantum mechanics, momentum is defined as a self-adjoint operator on the wave function. The Heisenberg uncertainty principle defines limits on how accurately the momentum and position of a single observable system can be known at once. In quantum mechanics, position and momentum are conjugate variables.

For a single particle described in the position basis the momentum operator can be written as

where is the gradient operator, is the reduced Planck constant, and is the imaginary unit. This is a commonly encountered form of the momentum operator, though the momentum operator in other bases can take other forms. For example, in momentum space the momentum operator is represented as

where the operator acting on a wave function yields that wave function multiplied by the value , in an analogous fashion to the way that the position operator acting on a wave function yields that wave function multiplied by the value "x".

For both massive and massless objects, relativistic momentum is related to the phase constant formula_72 by
Electromagnetic radiation (including visible light, ultraviolet light, and radio waves) is carried by photons. Even though photons (the particle aspect of light) have no mass, they still carry momentum. This leads to applications such as the solar sail. The calculation of the momentum of light within dielectric media is somewhat controversial (see Abraham–Minkowski controversy).

In fields such as fluid dynamics and solid mechanics, it is not feasible to follow the motion of individual atoms or molecules. Instead, the materials must be approximated by a continuum in which there is a particle or fluid parcel at each point that is assigned the average of the properties of atoms in a small region nearby. In particular, it has a density and velocity that depend on time and position . The momentum per unit volume is .

Consider a column of water in hydrostatic equilibrium. All the forces on the water are in balance and the water is motionless. On any given drop of water, two forces are balanced. The first is gravity, which acts directly on each atom and molecule inside. The gravitational force per unit volume is , where is the gravitational acceleration. The second force is the sum of all the forces exerted on its surface by the surrounding water. The force from below is greater than the force from above by just the amount needed to balance gravity. The normal force per unit area is the pressure . The average force per unit volume inside the droplet is the gradient of the pressure, so the force balance equation is

If the forces are not balanced, the droplet accelerates. This acceleration is not simply the partial derivative because the fluid in a given volume changes with time. Instead, the material derivative is needed:
Applied to any physical quantity, the material derivative includes the rate of change at a point and the changes due to advection as fluid is carried past the point. Per unit volume, the rate of change in momentum is equal to . This is equal to the net force on the droplet.

Forces that can change the momentum of a droplet include the gradient of the pressure and gravity, as above. In addition, surface forces can deform the droplet. In the simplest case, a shear stress , exerted by a force parallel to the surface of the droplet, is proportional to the rate of deformation or strain rate. Such a shear stress occurs if the fluid has a velocity gradient because the fluid is moving faster on one side than another. If the speed in the direction varies with , the tangential force in direction per unit area normal to the direction is
where is the viscosity. This is also a flux, or flow per unit area, of x-momentum through the surface.

Including the effect of viscosity, the momentum balance equations for the incompressible flow of a Newtonian fluid are
These are known as the Navier–Stokes equations.

The momentum balance equations can be extended to more general materials, including solids. For each surface with normal in direction and force in direction , there is a stress component . The nine components make up the Cauchy stress tensor , which includes both pressure and shear. The local conservation of momentum is expressed by the Cauchy momentum equation:
where is the body force.

The Cauchy momentum equation is broadly applicable to deformations of solids and liquids. The relationship between the stresses and the strain rate depends on the properties of the material (see Types of viscosity).

A disturbance in a medium gives rise to oscillations, or waves, that propagate away from their source. In a fluid, small changes in pressure can often be described by the acoustic wave equation:
where is the speed of sound. In a solid, similar equations can be obtained for propagation of pressure (P-waves) and shear (S-waves).

The flux, or transport per unit area, of a momentum component by a velocity is equal to . In the linear approximation that leads to the above acoustic equation, the time average of this flux is zero. However, nonlinear effects can give rise to a nonzero average. It is possible for momentum flux to occur even though the wave itself does not have a mean momentum.

In about 530 AD, working in Alexandria, Byzantine philosopher John Philoponus developed a concept of momentum in his commentary to Aristotle's "Physics". Aristotle claimed that everything that is moving must be kept moving by something. For example, a thrown ball must be kept moving by motions of the air. Most writers continued to accept Aristotle's theory until the time of Galileo, but a few were skeptical. Philoponus pointed out the absurdity in Aristotle's claim that motion of an object is promoted by the same air that is resisting its passage. He proposed instead that an impetus was imparted to the object in the act of throwing it. Ibn Sīnā (also known by his Latinized name Avicenna) read Philoponus and published his own theory of motion in "The Book of Healing" in 1020. He agreed that an impetus is imparted to a projectile by the thrower; but unlike Philoponus, who believed that it was a temporary virtue that would decline even in a vacuum, he viewed it as a persistent, requiring external forces such as air resistance to dissipate it.
The work of Philoponus, and possibly that of Ibn Sīnā, was read and refined by the European philosophers Peter Olivi and Jean Buridan. Buridan, who in about 1350 was made rector of the University of Paris, referred to impetus being proportional to the weight times the speed. Moreover, Buridan's theory was different from his predecessor's in that he did not consider impetus to be self-dissipating, asserting that a body would be arrested by the forces of air resistance and gravity which might be opposing its impetus.

René Descartes believed that the total "quantity of motion" () in the universe is conserved, where the quantity of motion is understood as the product of size and speed. This should not be read as a statement of the modern law of momentum, since he had no concept of mass as distinct from weight and size, and more important, he believed that it is speed rather than velocity that is conserved. So for Descartes if a moving object were to bounce off a surface, changing its direction but not its speed, there would be no change in its quantity of motion. Galileo, in his "Two New Sciences", used the Italian word "impeto" to similarly describe Descartes' quantity of motion.

Leibniz, in his "Discourse on Metaphysics", gave an argument against Descartes' construction of the conservation of the "quantity of motion" using an example of dropping blocks of different sizes different distances. He points out that force is conserved but quantity of motion, construed as the product of size and speed of an object, is not conserved.

Christiaan Huygens concluded quite early that Descartes's laws for the elastic collision of two bodies must be wrong, and he formulated the correct laws. An important step was his recognition of the Galilean invariance of the problems. His views then took many years to be circulated. He passed them on in person to William Brouncker and Christopher Wren in London, in 1661. What Spinoza wrote to Henry Oldenburg about them, in 1666 which was during the Second Anglo-Dutch War, was guarded. Huygens had actually worked them out in a manuscript "De motu corporum ex percussione" in the period 1652–6. The war ended in 1667, and Huygens announced his results to the Royal Society in 1668. He published them in the "Journal des sçavans" in 1669.

The first correct statement of the law of conservation of momentum was by English mathematician John Wallis in his 1670 work, "Mechanica sive De Motu, Tractatus Geometricus": "the initial state of the body, either of rest or of motion, will persist" and "If the force is greater than the resistance, motion will result". Wallis used "momentum" for quantity of motion, and "vis" for force. Newton's "Philosophiæ Naturalis Principia Mathematica", when it was first published in 1687, showed a similar casting around for words to use for the mathematical momentum. His Definition II defines "quantitas motus", "quantity of motion", as "arising from the velocity and quantity of matter conjointly", which identifies it as momentum. Thus when in Law II he refers to "mutatio motus", "change of motion", being proportional to the force impressed, he is generally taken to mean momentum and not motion. It remained only to assign a standard term to the quantity of motion. The first use of "momentum" in its proper mathematical sense is not clear but by the time of Jennings's "Miscellanea" in 1721, five years before the final edition of Newton's "Principia Mathematica", momentum or "quantity of motion" was being defined for students as "a rectangle", the product of and , where is "quantity of material" and is "velocity", .




</doc>
<doc id="20432" url="https://en.wikipedia.org/wiki?curid=20432" title="Mood stabilizer">
Mood stabilizer

A mood stabilizer, (or mood stabiliser), is a psychiatric medication used to treat mood disorders characterized by intense and sustained mood shifts, such as bipolar disorder and the bipolar type of schizoaffective disorder.

Mood stabilizers are best known for the treatment of bipolar disorder, preventing mood shifts to mania (or hypomania) and depression. Mood stabilizers are also used in schizoaffective disorder when it is the bipolar type.

The term "mood stabilizer" does not describe a mechanism, but rather an effect. More precise terminology based on pharmacology is used to further classify these agents. Drugs commonly classed as mood stabilizers include:


Many agents described as "mood stabilizers" are also categorized as anticonvulsants. The term "anticonvulsant mood stabilizers" is sometimes used to describe these as a class. Although this group is also defined by effect rather than mechanism, there is at least a preliminary understanding of the mechanism of most of the anticonvulsants used in the treatment of mood disorders.


There is insufficient evidence to support the use of various other anticonvulsants, such as gabapentin and topiramate, as mood stabilizers.



In routine practice, monotherapy is often not sufficiently effective for acute and/or maintenance therapy and thus most patients are given combination therapies. Combination therapy (atypical antipsychotic with lithium or valproate) shows better efficacy over monotherapy in the manic phase in terms of efficacy and prevention of relapse. However, side effects are more frequent and discontinuation rates due to adverse events are higher with combination therapy than with monotherapy.

Most mood stabilizers are primarily antimanic agents, meaning that they are effective at treating mania and mood cycling and shifting, but are not effective at treating acute depression. The principal exceptions to that rule, because they treat both manic and depressive symptoms, are lamotrigine, lithium carbonate, olanzapine and quetiapine.

Nevertheless, antidepressants are still often prescribed in addition to mood stabilizers during depressive phases. This brings some risks, however, as antidepressants can induce mania, psychosis, and other disturbing problems in people with bipolar disorder—in particular, when taken alone. The risk of antidepressant-induced mania when given to patients concomitantly on antimanic agents is not known for certain but may still exist. The majority of antidepressants appear ineffective in treating bipolar depression.

Antidepressants cause several risks when given to bipolar patients. They are ineffective in treating acute bipolar depression, preventing relapse, and can cause rapid cycling. Studies have shown that antidepressants have no benefit versus a placebo or other treatment. Antidepressants can also lead to a higher rate of non-lethal suicidal behavior. Relapse can also be related to treatment with antidepressants. This is less likely to occur if a mood stabilizer is combined with an antidepressant, rather than an antidepressant being used alone. Evidence from previous studies shows that rapid cycling is linked to use of antidepressants. Rapid cycling is defined as the presence of four or more mood episodes within a year's time. Evidence suggests that rapid cycling and mixed symptoms have become more common since antidepressant medication has come into widespread use. There is a need for caution when treating bipolar patients with antidepressant medication due to the risks that they pose. 

Use of mood stabilizers and anticonvulsants such as lamotrigine, carbamazapine, valproate and others may lead to chronic folate deficiency, potentiating depression. Also, "Folate deficiency may increase the risk of depression and reduce the action of antidepressants." L-methylfolate (also formally known as 5-MTHF or levomefolic acid), a centrally acting trimonoamine modulator, boosts the synthesis of three CNS neurotransmitters: dopamine, norepinephrine and serotonin. Mood stabilizers and anticonvulsants may interfere with folic acid absorption and L-methylfolate formation. Augmentation with the medical food L-methylfolate may improve antidepressant effects of these medicines, including lithium and antidepressants themselves, by boosting the synthesis of antidepressant neurotransmitters. However, the U.S. National Institutes of Health issued a warning caution about the use of L-methylfolate for patients with bipolar disease.

The precise mechanism of action of lithium is still unknown, and it is suspected that it acts at various points of the neuron between the nucleus and the synapse. Lithium is known to inhibit the enzyme GSK-3B. This improves the functioning of the circadian clock—which is thought to be often malfunctioning in people with bipolar disorder—and positively modulates gene transcription of brain-derived neurotrophic factor (BDNF). The resulting increase in neural plasticity may be central to lithium's therapeutic effects. Lithium may also increase the synthesis of serotonin.

All of the anticonvulsants routinely used to treat bipolar disorder are blockers of voltage-gated sodium channels, affecting the brain's glutamate system. For valproic acid, carbamazepine and oxcarbazepine, however, their mood-stabilizing effects may be more related to effects on the GABAergic system. Lamotrigine is known to decrease the patient's cortisol response to stress.

One possible downstream target of several mood stabilizers such as lithium, valproate, and carbamazepine is the arachidonic acid cascade.



</doc>
<doc id="20433" url="https://en.wikipedia.org/wiki?curid=20433" title="Mere Christianity">
Mere Christianity

Mere Christianity is a 1952 theological book by C. S. Lewis, adapted from a series of BBC radio talks made between 1941 and 1944, while Lewis was at Oxford during the Second World War. Considered a classic of Christian apologetics, the transcripts of the broadcasts originally appeared in print as three separate pamphlets: "The Case for Christianity" ("Broadcast Talks" in the UK) (1942), "Christian Behaviour" (1943), and "Beyond Personality" (1944). Lewis was invited to give the talks by James Welch, the BBC Director of Religious Broadcasting, who had read his 1940 book, "The Problem of Pain".

James Welch, the Director of Religious Broadcasting for the BBC, read Lewis's "The Problem of Pain" and subsequently wrote to him the following:I write to ask whether you would be willing to help us in our work of religious broadcasting ... The microphone is a limiting, and rather irritating, instrument, but the quality of thinking and depth of conviction which I find in your book ought sure to be shared with a great many other people. Welch suggested two potential subjects. Lewis responded with thanks and observed that modern literature, the first potential subject, did not suit him, thereby electing the latter option, the Christian Faith as Lewis saw it... Ultimately, this was the course he set upon. In the radio talks and the derived book, "Mere Christianity", he articulated the congruous tenets of Christian faith.

In the preface to later editions of the book, Lewis described his intentions of avoiding contended theological doctrine, focusing instead on foundational principles and applicable derivations. Along with his use of pithy and succinct language, Lewis was able to more nearly pertain and better appeal his subject to the commonly-educated man, who comprised the vast majority of his audience. Although, he still kept the work plausibly erudite for the intellectuals of his generation, for whom the jargon of formal Christian theology did not retain its original meaning.

The core of the first section centers on an argument from morality, the basis of which is the "law of human nature", a "rule about right and wrong," which, Lewis maintained, is commonly available and known to all human beings. He cites, as an example, the case of Nazi Germany, writing: "This law was called the Law of nature because people thought that every one knew it by nature and did not need to be taught it. They did not mean, of course, that you might not find an odd individual here and there who did not know it, just as you find a few people who are colour-blind or have no ear for a tune. But taking the race as a whole, they thought that the human idea of decent behaviour was obvious to every one. And I believe they were right. If they were not, then all the things we said about the war were nonsense. What was the sense in saying the enemy were in the wrong unless Right is a real thing which the Nazis at bottom knew as well as we did and ought to have practised? If they had had no notion of what we mean by right, then, though we might still have had to fight them, we could no more have blamed them for that than for the colour of their hair."

On a more mundane level, it is generally accepted that stealing is a violation of this moral law. Lewis argues that the moral law is like scientific laws (e.g. gravity) or mathematics in that it was not contrived by humans. However, it is unlike scientific laws in that it can be broken or ignored, and it is known intuitively, rather than through experimentation. After introducing the moral law, Lewis argues that thirst reflects the fact that people naturally need water, and there is no other substance which satisfies that need. Lewis points out that earthly experience does not satisfy the human craving for "joy" and that only God could fit the bill; humans cannot know to yearn for something if it does not exist.

After providing reasons for his conversion to theism, Lewis goes over rival conceptions of God to Christianity. Pantheism, he argues, is incoherent, and atheism too simple. Eventually he arrives at Jesus Christ, and invokes a well-known argument now known as "Lewis's trilemma". Lewis, arguing that Jesus was claiming to be God, uses logic to advance three possibilities: either he really was God, was deliberately lying, or was not God but thought himself to be (which would make him delusional and likely insane). The book goes on to say that the latter two possibilities are not consistent with Jesus' character and it was most likely that he was being truthful.

The next third of the book explores the ethics resulting from Christian belief. He cites the four cardinal virtues: prudence, justice, temperance, and fortitude. After touching on these, he goes into the three theological virtues: hope, faith, and charity. Lewis also explains morality as being composed of three "layers": relationships between man and man, the motivations and attitudes of the man himself, and contrasting worldviews.

Lewis also covers such topics as social relations and forgiveness, sexual ethics and the tenets of Christian marriage, and the relationship between morality and psychoanalysis. He also writes about "the great sin": pride, which he argues to be the root cause of all evil and rebellion.

His most important point is that Christianity mandates that one "love your neighbour as yourself." He points out that all persons unconditionally love themselves. Even if one does not "like" oneself, one would still love oneself. Christians, he writes, must also apply this attitude to others, even if they do not like them. Lewis calls this one of the "great secrets": when one acts as if he loves others, he will presently come to love them.

Lewis' voice became nearly as recognizable as that of Winston Churchill during World War II, when the talks were given. The book has since become among the most popular evangelical works in existence. In 2006, "Mere Christianity" was placed third in "Christianity Today" list of the most influential books amongst evangelicals since 1945. The title has influenced "Touchstone Magazine: A Journal of Mere Christianity" and William Dembski's book "Mere Creation". Charles Colson's conversion to Christianity resulted from his reading this book, as did the conversions of Francis Collins, Jonathan Aitken, Josh Caterer, and the philosopher C. E. M. Joad.

A passage in the book also influenced the name of contemporary Christian Texan Grammy-nominated pop/rock group Sixpence None the Richer. The phrase, "the hammering process" was used by the Christian metal band Living Sacrifice for the name of their album "The Hammering Process". The metalcore band Norma Jean derived the title of their song "No Passenger: No Parasite" from the section in the book in which Lewis describes a fully Christian society as having "No passengers or parasites".



</doc>
<doc id="20434" url="https://en.wikipedia.org/wiki?curid=20434" title="Mathematical game">
Mathematical game

A mathematical game is a game whose rules, strategies, and outcomes are defined by clear mathematical parameters. Often, such games have simple rules and match procedures, such as Tic-tac-toe and Dots and Boxes. Generally, mathematical games need not be conceptually intricate to involve deeper computational underpinnings. For example, even though the rules of Mancala are relatively basic, the game can be rigorously analyzed through the lens of combinatorial game theory.

Mathematical games differ sharply from mathematical puzzles in that mathematical puzzles require specific mathematical expertise to complete, whereas mathematical games do not require a deep knowledge of mathematics to play. Often, the arithmetic core of mathematical games is not readily apparent to players untrained to note the statistical or mathematical aspects.

Some mathematical games are of deep interest in the field of recreational mathematics. 

When studying a game's core mathematics, arithmetic theory is generally of higher utility than actively playing or observing the game itself. To analyze a game numerically, it is particularly useful to study the rules of the game insofar as they can yield equations or relevant formulas. This is frequently done to determine winning strategies or to distinguish if the game has a solution.

Sometimes it is not immediately obvious that a particular game involves chance. Often a card game is described as "pure strategy" and such, but a game with any sort of random shuffling or face-down dealing of cards should not be considered to be "no chance". Several abstract strategy games are listed below:






</doc>
<doc id="20435" url="https://en.wikipedia.org/wiki?curid=20435" title="Martin Gardner">
Martin Gardner

Martin Gardner (October 21, 1914May 22, 2010) was an American popular mathematics and popular science writer, with interests also encompassing scientific skepticism, micromagic, philosophy, religion, and literature—especially the writings of Lewis Carroll, L. Frank Baum, and G. K. Chesterton. He was also a leading authority on Lewis Carroll. "The Annotated Alice", which incorporated the text of Carroll's two Alice books, was his most successful work and sold over a million copies. He had a lifelong interest in magic and illusion and was regarded as one of the most important magicians of the twentieth century. He was considered the doyen of American puzzlers. He was a prolific and versatile author, publishing more than 100 books.

Gardner was best known for creating and sustaining interest in recreational mathematics—and by extension, mathematics in general—throughout the latter half of the 20th century, principally through his "Mathematical Games" columns. These appeared for twenty-five years in "Scientific American", and his subsequent books collecting them.

Gardner was one of the foremost anti-pseudoscience polemicists of the 20th century. His 1957 book "Fads and Fallacies in the Name of Science" became a classic and seminal work of the skeptical movement. In 1976 he joined with fellow skeptics to found CSICOP, an organization promoting scientific inquiry and the use of reason in examining extraordinary claims.

Martin Gardner was born into a prosperous family in Tulsa, Oklahoma, to James Henry Gardner, a prominent petroleum geologist, and his wife, Willie Wilkerson Spiers, a Montessori-trained teacher. His mother taught Martin to read before he started school, reading him "The Wizard of Oz", and this began a lifelong interest in the Oz books of L. Frank Baum. His fascination with puzzles started in his boyhood when his father gave him a copy of Sam Loyd's "Cyclopedia of 5000 Puzzles, Tricks and Conundrums". 

He attended the University of Chicago, where he earned his bachelor's degree in philosophy in 1936. Early jobs included reporter on the "Tulsa Tribune", writer at the University of Chicago Office of Press Relations, and case worker in Chicago's Black Belt for the city's Relief Administration. During World War II, he served for four years in the U.S. Navy as a yeoman on board the destroyer escort USS "Pope" in the Atlantic. His ship was still in the Atlantic when the war came to an end with the surrender of Japan in August 1945.

After the war, Gardner returned to the University of Chicago. He attended graduate school for a year there, but he did not earn an advanced degree.

In 1950 he wrote an article in the "Antioch Review" entitled "The Hermit Scientist". It was one of Gardner's earliest articles about junk science, and in 1952 a much-expanded version became his first published book: "In the Name of Science: An Entertaining Survey of the High Priests and Cultists of Science, Past and Present".

In the late 1940s, Gardner moved to New York City and became a writer and editor at "Humpty Dumpty" magazine, where for eight years he wrote features and stories for it and several other children's magazines. His paper-folding puzzles at that magazine led to his first work at "Scientific American." For many decades, Gardner, his wife Charlotte, and their two sons, Jim and Tom, lived in Hastings-on-Hudson, New York, where he earned his living as a freelance author, publishing books with several different publishers, and also publishing hundreds of magazine and newspaper articles. The year 1960 saw the original edition of the best-selling book of his career, "The Annotated Alice".

In 1979, Gardner retired from Scientific American and he and his wife Charlotte moved to Hendersonville, North Carolina. Gardner never really retired as an author, but continued to write math articles, sending them to The Mathematical Intelligencer, Math Horizons, The College Mathematics Journal, and Scientific American. He also revised some of his older books such as "Origami, Eleusis, and the Soma Cube". Charlotte died in 2000 and in 2004 Gardner returned to Oklahoma, where his son, James Gardner, was a professor of education at the University of Oklahoma in Norman. He died there on May 22, 2010. An autobiography — "Undiluted Hocus-Pocus: The Autobiography of Martin Gardner" — was published posthumously.

Martin Gardner had a major impact on mathematics in the second half of the 20th century. His column was called "Mathematical Games" but it was much more than that. His writing introduced many readers to real mathematics for the first time in their lives. The column lasted for 25 years and was read avidly by the generation of mathematicians and physicists who grew up in the years 1956 to 1981. It was the original inspiration for many of them to become mathematicians or scientists themselves.

David Auerbach wrote: "A case can be made, in purely practical terms, for Martin Gardner as one of the most influential writers of the 20th century. His popularizations of science and mathematical games in Scientific American, over the 25 years he wrote for them, might have helped create more young mathematicians and computer scientists than any other single factor prior to the advent of the personal computer."

His admirers included such diverse people as W. H. Auden, Arthur C. Clarke, Carl Sagan, Isaac Asimov, Richard Dawkins, Stephen Jay Gould, and the entire French literary group known as the Oulipo. Salvador Dali once sought him out to discuss four-dimensional hypercubes. Gardner wrote to M.C. Escher in 1961 to ask permission to use his Horseman tessellation in an upcoming column about H.S.M. Coxeter. Escher replied, saying that he knew Gardner as author of "The Annotated Alice", which had been sent to Escher by Coxeter. The correspondence led to Gardner introducing the previously unknown Escher's art to the world. His writing was both broad and deep. Noam Chomsky once wrote, "Martin Gardner's contribution to contemporary intellectual culture is unique—in its range, its insight, and understanding of hard questions that matter." Gardner repeatedly alerted the public (and other mathematicians) to recent discoveries in mathematics–recreational and otherwise. In addition to introducing many first-rate puzzles and topics such as Penrose tiles and Conway's Game of Life, he was equally adept at writing captivating columns about traditional mathematical topics such as knot theory, Fibonacci numbers, Pascal's triangle, the Möbius strip, transfinite numbers, four-dimensional space, Zeno's paradoxes, Fermat's last theorem, and the four-color problem.

Gardner set a new high standard for writing about mathematics. In a 2004 interview he said, "I go up to calculus, and beyond that I don't understand any of the papers that are being written. I consider that that was an advantage for the type of column I was doing because I had to understand what I was writing about, and that enabled me to write in such a way that an average reader could understand what I was saying. If you are writing popularly about math, I think it's good not to know too much math." And he was fearsomely bright. John Horton Conway called him "the most learned man I have ever met." Colm Mulcahy spoke for many when he said, "Gardner was without doubt the best friend mathematics ever had."

For over a quarter century Gardner wrote a monthly column on the subject of recreational mathematics for "Scientific American". It all began with his free-standing article on hexaflexagons which ran in the December 1956 issue. Flexagons became a bit of a fad and soon people all over New York City were making them. Gerry Piel, the "SA" publisher at the time, asked Gardner, "Is there enough similar material to this to make a regular feature?" Gardner said he thought so. The January 1957 issue contained his first column, entitled "Mathematical Games". Almost 300 more columns were to follow.

The "Mathematical Games" column became the most popular feature of the magazine and was the first thing that many readers turned to. In September 1977 Scientific American acknowledged the prestige and popularity of Gardner's column by moving it from the back to the very front of the magazine. It ran from 1956 to 1981 with sporadic columns afterwards and was the first introduction of many subjects to a wider audience, notably:

Ironically, Gardner had problems learning calculus and never took a mathematics course after high school. While editing "Humpty Dumpty's Magazine" he constructed many paper folding puzzles, and this led to his interest in the flexagons invented by British mathematician Arthur H Stone. The subsequent article he wrote on hexaflexagons led directly to the column.

Gardner's son Jim once asked him what was his favorite puzzle, and Gardner answered almost immediately: "The monkey and the coconuts". It had been the subject of his April 1958 Games column and in 2001 he chose to make it the first chapter of his "best of" collection, "The Colossal Book of Mathematics".

In the 1980s "Mathematical Games" began to appear only irregularly. Other authors began to share the column, and the June 1986 issue saw the final installment under that title. In 1981, on Gardner's retirement from "Scientific American", the column was replaced by Douglas Hofstadter's "Metamagical Themas", a name that is an anagram of "Mathematical Games".

Virtually all of the games columns were collected in book form starting in 1959 with "The Scientific American Book of Mathematical Puzzles & Diversions". Over the next four decades fourteen more books followed. Donald Knuth called them the .

There is a group of people who lie at the intersection of mathematics, philosophy, magic, and scientific skepticism who all knew and worked with Martin Gardner. They are united by a striking originality in their work and they sometimes owe much of their fame to being featured by Gardner in his column. The fact that Gardner had an unerring eye for spotting and promoting such people is one of the reasons his column was so influential. Indeed, Gardner filled a role a little bit like the 17th century French polymath Marin Mersenne in that he maintained this network and made these people aware of each other–and this led to further fruitful collaborations. For example, if it were not for Gardner, mathematicians Conway, Berlekamp, and Guy probably would have never gotten together to write "Winning Ways for your Mathematical Plays", a foundational book in combinatorial game theory. Gardner also introduced Conway to Benoit Mandelbrot because he knew they both were interested in Penrose tiles. If it were not for Gardner, Doris Schattschneider and Marjorie Rice would not have gotten together to document the newly discovered pentagon tilings.

Late in his life Gardner once said, "When I first started the column, I was not in touch with any mathematicians, and gradually mathematicians who were creative in the field found out about the column and began corresponding with me. So my most interesting columns were columns based on the material I got from them, so I owe them a big debt of gratitude." The games column was not just Gardner. It was this group of people he collected, nurtured, and acted as a conduit for—a group that came to be known as "Martin Gardner’s mathematical grapevine."

Gardner prepared each of his columns in a painstaking and scholarly fashion and conducted copious correspondence to be sure that everything was fact-checked for mathematical accuracy. But this grapevine, with Gardner at the center, was also a rich source of ideas, mathematical and otherwise, with gossip flowing in many directions. Communications was often by postcard or telephone and Gardner kept meticulous notes of everything, typically on index cards. Archives of just some of his correspondence stored at Stanford University occupy some 63 linear feet of shelf space. This correspondence led to columns about the rep-tiles and pentominos of Solomon W. Golomb; the space filling curves of Bill Gosper; the aperiodic tiles of Roger Penrose; the Game of Life invented by John H. Conway; the superellipse and the Soma cube of Piet Hein; the trapdoor functions of Diffie, Hellman, and Merkle; the flexigons of Stone, Tuckerman, Feynman, and Tukey; the geometrical delights in a book by H. S. M. Coxeter; the game of Hex invented by John Nash; Tutte's account of squaring the square; and many other topics.

The wide array of mathematicians, physicists, computer scientists, philosophers, magicians, artists, writers, and other influential thinkers who can be counted as part of Gardner's mathematical grapevine includes:
Doris Schattschneider sometimes refers to this circle of collaborators as MG.

Gardner died in 2010 but the grapevine lives on, and is even adding new members from time to time. Many of the people in the Gardner circle continue to meet every two years at Gathering 4 Gardner (G4G) founded in 1993 by Berlekamp, Tom Rodgers, and other admirers of Martin Gardner. A keynote speaker at G4G13 was Fields medalist Manjul Bhargava.

Gardner was an uncompromising critic of fringe science. His book "Fads and Fallacies in the Name of Science" (1952, revised 1957) launched the modern skeptical movement. It debunked dubious movements and theories including Fletcherism, Lamarckism, food faddism, Dowsing Rods, Charles Fort, Rudolf Steiner, Dianetics, the Bates method for improving eyesight, Einstein deniers, the Flat Earth theory, the lost continents of Atlantis and Lemuria, Immanuel Velikovsky's Worlds in Collision, the reincarnation of Bridey Murphy, Wilhelm Reich's orgone theory, the spontaneous generation of life, extra-sensory perception and psychokinesis, homeopathy, phrenology, palmistry, graphology, and numerology. This book and his subsequent efforts ("Science: Good, Bad and Bogus", 1981; "Order and Surprise", 1983, "Gardner's Whys & Wherefores", 1989, etc.) provoked a lot of criticism from the advocates of alternative science and New Age philosophy; he kept up running dialogues (both public and private) with many of them for decades. 

In a review of "Science: Good, Bad and Bogus", Stephen Jay Gould called Gardner "The Quack Detector", a writer who "expunge[d] nonsense" and in so doing had "become a priceless national resource."

In 1976 Gardner joined with fellow skeptics philosopher Paul Kurtz, psychologist Ray Hyman, sociologist Marcello Truzzi, and stage magician James Randi to found the Committee for the Scientific Investigation of Claims of the Paranormal (now called the Committee for Skeptical Inquiry). Luminaries such as astronomer Carl Sagan, author and biochemist Isaac Asimov, psychologist B. F. Skinner, and journalist Philip J. Klass became fellows of the program. From 1983 to 2002 he wrote a monthly column called "Notes of a Fringe Watcher" (originally "Notes of a Psi-Watcher") for "Skeptical Inquirer", that organization's monthly magazine. These columns have been collected in five books starting with "The New Age: Notes of a Fringe Watcher" in 1988.

Gardner was a relentless critic of self-proclaimed Israeli psychic Uri Geller and wrote two satirical booklets about him in the 1970s using the pen name "Uriah Fuller" in which he explained how such purported psychics do their seemingly impossible feats such as mentally bending spoons and reading minds.

Martin Gardner continued to criticize junk science throughout his life–and he was fearless. His targets included not just safe subjects like astrology and UFO sightings, but topics such as chiropractic, vegetarianism, Madame Blavatsky, creationism, Scientology, the Laffer curve, Christian Science, and the Hutchins-Adler Great Books Movement. The last thing he wrote in the spring of 2010 (a month before his death) was an article excoriating the "dubious medical opinions and bogus science" of Oprah Winfrey—particularly her support for the thoroughly discredited theory that vaccinations cause autism; it went on to bemoan the "needless deaths of children" that such notions are likely to cause.

"Skeptical Inquirer" named him one of the Ten Outstanding Skeptics of the Twentieth Century. In 2010 he was posthumously honored with an award for his contributions in the skeptical field from the Independent Investigations Group. In 1982 the Committee for Skeptical Inquiry awarded Gardner its "In Praise of Reason Award" for his "heroic efforts in defense of reason and the dignity of the skeptical attitude", and in 2011 it added Gardner to its Pantheon of Skeptics.

Martin Gardner's father once showed him a magic trick when he was a little boy. Young Martin was fascinated to see physical laws seemingly violated and this led to a lifelong passion for magic and illusion. He wrote for a magic magazine in high school and worked in a department store demonstrating magic tricks while he was at the University of Chicago. The very first thing that Martin Gardner ever published (at the age of fifteen) was a magic trick in "The Sphinx", the official magazine of the Society of American Magicians. He focused mainly on micromagic (table or close-up magic) and, from the 1930s on, published a significant number of original contributions to this secretive field. Magician Joe M. Turner said, "The Encyclopedia of Impromptu Magic", which Gardner wrote in 1985, "is guaranteed to show up in any poll of magicians' favorite magic books." His first magic book for the general public, "Mathematics, Magic and Mystery" (Dover, 1956), is still considered a classic in the field. He was well known for his innovative tapping and spelling effects, with and without playing cards, and was most proud of the effect he called the "Wink Change".

Many of Gardner's lifelong friends were magicians. These included William Simon who introduced Gardner to Charlotte Greenwald, whom he married in 1952, fellow CSICOP founder and pseudoscience fighter James Randi, Dai Vernon, Jerry Andrus, statistician Persi Diaconis, and polymath Raymond Smullyan. Diaconis and Smullyan like Gardner straddled the two worlds of mathematics and magic. Mathematics and magic were frequently intertwined in Gardner's work. One of his earliest books, "Mathematics, Magic and Mystery" (1956), was about mathematically based magic tricks. Mathematical magic tricks were often featured in his "Mathematical Games" column–for example, his August 1962 column was titled "A variety of diverting tricks collected at a fictitious convention of magicians." From 1998 to 2002 he wrote a monthly column on magic tricks called "Trick of the Month" in The Physics Teacher, a journal published by the American Association of Physics Teachers.
In 1999 "Magic magazine" named Gardner one of the "100 Most Influential Magicians of the Twentieth Century". In 2005 he received a 'Lifetime Achievement Fellowship' from the Academy of Magical Arts. The last work to be published during his lifetime was a magic trick in the May 2010 issue of "".

Gardner was raised as a Methodist (his mother was very religious) but rejected established religion as an adult. He considered himself a philosophical theist and a fideist. He believed in a personal God, in an afterlife, and prayer, but rejected established religion. Nevertheless, he had abiding fascination with religious belief. In his autobiography, he stated: "When many of my fans discovered that I believed in God and even hoped for an afterlife, they were shocked and dismayed ... I do not mean the God of the Bible, especially the God of the Old Testament, or any other book that claims to be divinely inspired. For me God is a "Wholly Other" transcendent intelligence, impossible for us to understand. He or she is somehow responsible for our universe and capable of providing, how I have no inkling, an afterlife."

Gardner described his own belief as philosophical theism inspired by the works of philosopher Miguel de Unamuno. While eschewing systematic religious doctrine, he retained a belief in God, asserting that this belief cannot be confirmed or disconfirmed by reason or science. At the same time, he was skeptical of claims that any god has communicated with human beings through spoken or telepathic revelation or through miracles in the natural world. Gardner has been quoted as saying that he regarded parapsychology and other research into the paranormal as tantamount to "tempting God" and seeking "signs and wonders". He stated that while he would expect tests on the efficacy of prayers to be negative, he would not rule out "a priori" the possibility that as yet unknown paranormal forces may allow prayers to influence the physical world.

Gardner wrote repeatedly about what public figures such as Robert Maynard Hutchins, Mortimer Adler, and William F. Buckley, Jr. believed and whether their beliefs were logically consistent. In some cases, he attacked prominent religious figures such as Mary Baker Eddy on the grounds that their claims are unsupportable. His semi-autobiographical novel "The Flight of Peter Fromm" depicts a traditionally Protestant Christian man struggling with his faith, examining 20th century scholarship and intellectual movements and ultimately rejecting Christianity while remaining a theist.

Gardner said that he suspected that the fundamental nature of human consciousness may not be knowable or discoverable, unless perhaps a physics more profound than ("underlying") quantum mechanics is some day developed. In this regard, he said, he was an adherent of the "New Mysterianism".

Gardner was considered a leading authority on Lewis Carroll. His annotated version of "Alice's Adventures in Wonderland" and "Through the Looking Glass", explaining the many mathematical riddles, wordplay, and literary references found in the Alice books, was first published as "The Annotated Alice" (Clarkson Potter, 1960). Sequels were published with new annotations as "More Annotated Alice" (Random House, 1990), and finally as "The Annotated Alice: The Definitive Edition" (Norton, 1999), combining notes from the earlier editions and new material. The original book arose when Gardner found the Alice books "sort of frightening" when he was young, but found them fascinating as an adult. He felt that someone ought to annotate them, and suggested to a publisher that Bertrand Russell be asked; when the publisher was unable to get past Russell's secretary, Gardner was asked to take on the project himself.

There had long been annotated books written by scholars for other scholars, but Gardner was the first to write such a work for the general public, and soon many other writers followed his lead. Gardner himself went on to produce annotated editions of G. K. Chesterton's "The Innocence Of Father Brown" and "The Man Who Was Thursday", as well as of celebrated poems including "The Rime of the Ancient Mariner", "Casey at the Bat", "The Night Before Christmas", and "The Hunting of the Snark".

Gardner wrote two novels. He was a perennial fan of the Oz books written by L. Frank Baum, and in 1988 he published "Visitors from Oz", based on the characters in Baum's various Oz books. Gardner was a founding member of the International Wizard of Oz Club, and winner of its 1971 L. Frank Baum Memorial Award. His other novel was "The Flight of Peter Fromm" (1973), which reflected his lifelong fascination with religious belief and the problem of faith.

His short stories were collected in "The No-Sided Professor and Other Tales of Fantasy, Humor, Mystery, and Philosophy" (1987).

At the age of 95 Gardner wrote "Undiluted Hocus-Pocus: The Autobiography of Martin Gardner". He was living in a one-room apartment in Norman, Oklahoma and, as was his custom, wrote it on a typewriter and edited it using scissors and rubber cement. He took the title from a poem, a so-called grook, by his good friend Piet Hein, which perfectly expresses Gardner's abiding sense of mystery and wonder about existence.
Gardner's interest in wordplay led him to conceive of a magazine on recreational linguistics. In 1967 he pitched the idea to Greenwood Periodicals and nominated Dmitri Borgmann as editor. The resulting journal, "Word Ways", carried many of his articles; it was still publishing his submissions posthumously. He also wrote a "Puzzle Tale" column for "Asimov's Science Fiction" magazine from 1977 to 1986. Gardner was a member of the all-male literary banqueting club, the Trap Door Spiders, which served as the basis of Isaac Asimov's fictional group of mystery solvers, the Black Widowers.

Gardner often used pen names. In 1952, while working for the children's magazine "Humpty Dumpty", he contributed stories written by "Humpty Dumpty Jnr". For several years starting in 1953 he was a managing editor of "Polly Pigtails", a magazine for young girls, and also wrote under that name. His "Annotated Casey at the Bat" (1967) included a parody of the poem, attributed to "Nitram Rendrag" (his name spelled backwards). Using the pen name "Uriah Fuller", he wrote two books attacking the alleged psychic Uri Geller. In later years, Gardner often wrote parodies of his favorite poems under the name "Armand T. Ringer", an anagram of his name. In 1983 one George Groth panned Gardner's book "The Whys of a Philosophical Scrivener" in the "New York Review of Books". Only in the last line of the review was it revealed that George Groth was Martin Gardner himself.

In his January 1960 "Mathematical Games" column, Gardner introduced the fictitious "Dr. Matrix" and wrote about him often over the next two decades. Dr. Matrix was not exactly a pen name, although Gardner did pretend that everything in these columns came from the fertile mind of the good doctor. Then in 1979 Dr. Matrix himself published an article in the quite respectable "Two-Year College Mathematics Journal". It was called "Martin Gardner: Defending the Honor of the Human Mind" and contained a biography of Gardner and a history of his "Mathematical Games" column. It would be a further decade before Martin published an article in such a mathematics journal under his own name.

Gardner was known for his sometimes controversial philosophy of mathematics. He wrote negative reviews of "The Mathematical Experience" by Philip J. Davis and Reuben Hersh and "What Is Mathematics, Really?" by Hersh, both of which were critical of aspects of mathematical Platonism, and the first of which was well received by the mathematical community. While Gardner was often perceived as a hard-core Platonist, his reviews demonstrated some formalist tendencies. Gardner maintained that his views are widespread among mathematicians, but Hersh has countered that in his experience as a professional mathematician and speaker, this is not the case.

Over the years Gardner held forth on many contemporary issues, arguing for his points of view in fields from general semantics to fuzzy logic to watching TV (he once wrote a negative review of Jerry Mander's book "Four Arguments for the Elimination of Television"). He was a frequent contributor to "The New York Review of Books". His philosophical views are described and defended in his book "The Whys of a Philosophical Scrivener" (1983, revised 1999).

The numerous awards Gardner received include:

The Mathematical Associating of America has established a Martin Gardner Lecture to be given each year on the last day of MAA MathFest, the summer meeting of the MAA. The first annual lecture, "Recreational Mathematics and Computer Science: Martin Gardner's Influence on Research", was given by Erik Demaine of the Massachusetts Institute of Technology on Saturday, August 3, 2019, at MathFest in Cincinnati.

There are eight bricks honoring Gardner in the Paul R. Halmos Commemorative Walk, installed by The Mathematical Association of America (MAA) at their Conference Center in Washington, D.C. Gardner has an Erdös number of 1.

Martin Gardner continued to write up until his death in 2010, and his community of fans grew to span several generations. Moreover, his influence was so broad that many of his fans had little or no contact with each other. This led Atlanta entrepreneur and puzzle collector Tom Rodgers to the idea of hosting a weekend gathering celebrating Gardner's contributions to recreational mathematics, rationality, magic, puzzles, literature, and philosophy. Although Gardner was famously shy, and would usually decline an honor if it required him to make a personal appearance, Rogers persuaded him to attend the first such "Gathering 4 Gardner" (G4G), held in Atlanta in January 1993.

A second such get-together was held in 1996, again with Gardner in attendance, and this led Rodgers and his friends to make the gathering a regular, bi-annual event. Participants over the years have ranged from long-time Gardner friends such as John Horton Conway, Elwyn Berlekamp, Ronald Graham, Donald Coxeter, and Richard K. Guy, to newcomers like mathematician and mathematical artist Erik Demaine and mathematical video maker Vi Hart.

The program at the "G4G" meetings presents topics which Gardner had written about. The first gathering in 1993 was G4G1 and the 1996 event was G4G2. Since then it has been in even-numbered years, so far always in Atlanta. The 2018 event was G4G13.

In a publishing career spanning 80 years (1930-2010), Gardner authored or edited over 100 books and countless articles, columns and reviews.

All Gardner's works were non-fiction except for two novels — "The Flight of Peter Fromm" (1973) and "Visitors from Oz" (1998) — and two collections of short pieces — "The Magic Numbers of Dr. Matrix" (1967, 1985) and "The No-Sided Professor" (1987).




</doc>
<doc id="20436" url="https://en.wikipedia.org/wiki?curid=20436" title="MIDI timecode">
MIDI timecode

MIDI time code (MTC) embeds the same timing information as standard SMPTE timecode as a series of small 'quarter-frame' MIDI messages. There is no provision for the user bits in the standard MIDI time code messages, and messages are used to carry this information instead. The quarter-frame messages are transmitted in a sequence of eight messages, thus a complete timecode value is specified every two frames. If the MIDI data stream is running close to capacity, the MTC data may arrive a little behind schedule which has the effect of introducing a small amount of jitter. In order to avoid this it is ideal to use a completely separate MIDI port for MTC data. Larger full-frame messages, which encapsulate a frame worth of timecode in a single message, are used to locate to a time while timecode is not running.

Unlike standard SMPTE timecode, MIDI timecode's quarter-frame and full-frame messages carry a two-bit flag value that identifies the rate of the timecode, specifying it as either:

MTC distinguishes between film speed and video speed only by the rate at which timecode advances, not by the information contained in the timecode messages; thus, 29.97 frame/s dropframe is represented as 30 frame/s dropframe at 0.1% pulldown.

MTC allows the synchronisation of a sequencer or DAW with other devices that can synchronise to MTC or for these devices to 'slave' to a tape machine that is striped with SMPTE. For this to happen a SMPTE to MTC converter needs to be employed. It is possible for a tape machine to synchronise to an MTC signal (if converted to SMPTE), if the tape machine is able to 'slave' to incoming timecode via motor control, which is a rare feature.

The MIDI time code is 32 bits long, of which 24 are used, while 8 bits are unused and always zero. Because the full-time code messages requires that the most significant bits of each byte are zero (valid MIDI data bytes), there are really only 28 available bits and 4 spare bits.

Like most audiovisual timecodes such as SMPTE time code, it encodes only time of day, repeating each 24 hours. Time is given in units of hours, minutes, seconds, and frames. There may be 24, 25, or 30 frames per second.

Unlike most other timecodes, the components are encoded in straight binary, not binary-coded decimal.

Each component is assigned one byte:

When there is a jump in the time code, a single full-time code is sent to synchronize attached equipment. This takes the form of a special global system exclusive message:
The manufacturer ID of codice_10 indicates a real-time universal message, the channel of codice_10 indicates it is a global broadcast. The following ID of codice_12 identifies this is a time code type message, and the second codice_12 indicates it is a full-time code message. The 4 bytes of time code follow. Although MIDI is generally little-endian, the 4 time code bytes follow in big-endian order, followed by a codice_14 "end of exclusive" byte.

After a jump, the time clock stops until the first following quarter-frame message is received.

When the time is running continuously, the 32-bit time code is broken into 8 4-bit pieces, and one piece is transmitted each quarter frame. I.e. 96—120 times per second, depending on the frame rate. Since it takes eight quarter frames for a complete time code message, the complete SMPTE time is updated every two frames. A quarter-frame messages consists of a status byte of 0xF1, followed by a single 7-bit data value: 3 bits to identify the piece, and 4 bits of partial time code. When time is running forward, the piece numbers increment from 0–7; with the time that piece 0 is transmitted is the coded instant, and the remaining pieces are transmitted later.

If the MIDI data stream is being rewound, the piece numbers count backward. Again, piece 0 is transmitted at the coded moment.

The time code is divided little-endian as follows:



</doc>
<doc id="20437" url="https://en.wikipedia.org/wiki?curid=20437" title="Mass transfer">
Mass transfer

Mass transfer is the net movement of mass from one location, usually meaning stream, phase, fraction or component, to another. Mass transfer occurs in many processes, such as absorption, evaporation, drying, precipitation, membrane filtration, and distillation. Mass transfer is used by different scientific disciplines for different processes and mechanisms. The phrase is commonly used in engineering for physical processes that involve diffusive and convective transport of chemical species within physical systems.

Some common examples of mass transfer processes are the evaporation of water from a pond to the atmosphere, the purification of blood in the kidneys and liver, and the distillation of alcohol. In industrial processes, mass transfer operations include separation of chemical components in distillation columns, absorbers such as scrubbers or stripping, adsorbers such as activated carbon beds, and liquid-liquid extraction. Mass transfer is often coupled to additional transport processes, for instance in industrial cooling towers. These towers couple heat transfer to mass transfer by allowing hot water to flow in contact with air. The water is cooled by expelling some of its content in the form of water vapour.

In astrophysics, mass transfer is the process by which matter gravitationally bound to a body, usually a star, fills its Roche lobe and becomes gravitationally bound to a second body, usually a compact object (white dwarf, neutron star or black hole), and is eventually accreted onto it. It is a common phenomenon in binary systems, and may play an important role in some types of supernovae and pulsars.

Mass transfer finds extensive application in chemical engineering problems. It is used in reaction engineering, separations engineering, heat transfer engineering, and many other sub-disciplines of chemical engineering like electrochemical engineering.

The driving force for mass transfer is usually a difference in chemical potential, when it can be defined, though other thermodynamic gradients may couple to the flow of mass and drive it as well. A chemical species moves from areas of high chemical potential to areas of low chemical potential. Thus, the maximum theoretical extent of a given mass transfer is typically determined by the point at which the chemical potential is uniform. For single phase-systems, this usually translates to uniform concentration throughout the phase, while for multiphase systems chemical species will often prefer one phase over the others and reach a uniform chemical potential only when most of the chemical species has been absorbed into the preferred phase, as in liquid-liquid extraction.

While thermodynamic equilibrium determines the theoretical extent of a given mass transfer operation, the actual rate of mass transfer will depend on additional factors including the flow patterns within the system and the diffusivities of the species in each phase. This rate can be quantified through the calculation and application of mass transfer coefficients for an overall process. These mass transfer coefficients are typically published in terms of dimensionless numbers, often including Péclet numbers, Reynolds numbers, Sherwood numbers and Schmidt numbers, among others.

There are notable similarities in the commonly used approximate differential equations for momentum, heat, and mass transfer. The molecular transfer equations of Newton's law for fluid momentum at low Reynolds number (Stokes flow), Fourier's law for heat, and Fick's law for mass are very similar, since they are all linear approximations to transport of conserved quantities in a flow field. 
At higher Reynolds number, the analogy between mass and heat transfer and momentum transfer becomes less useful due to the nonlinearity of the Navier-Stokes equation (or more fundamentally, the general momentum conservation equation), but the analogy between heat and mass transfer remains good. A great deal of effort has been devoted to developing analogies among these three transport processes so as to allow prediction of one from any of the others.



</doc>
<doc id="20448" url="https://en.wikipedia.org/wiki?curid=20448" title="Museum of Jurassic Technology">
Museum of Jurassic Technology

The Museum of Jurassic Technology at 9341 Venice Boulevard in the Palms district of Los Angeles, California, was founded by David Hildebrand Wilson and Diana Drake Wilson in 1988. It calls itself "an educational institution dedicated to the advancement of knowledge and the public appreciation of the Lower Jurassic", the relevance of the term "Lower Jurassic" to the museum's collections being left uncertain and unexplained.

The museum's collection includes a mixture of artistic, scientific, ethnographic, and historic items, as well as some unclassifiable exhibits; the diversity evokes the cabinets of curiosities that were the 16th-century predecessors of modern natural-history museums. The factual claims of many of the museum's exhibits strain credibility, provoking an array of interpretations.

David Hildebrand Wilson received a MacArthur Foundation fellowship in 2001.

The museum contains an unusual collection of exhibits and objects with varying and uncertain degrees of authenticity. "The New York Times" critic Edward Rothstein described it as a "museum about museums", "where the persistent question is: what kind of place is this?" "Smithsonian" magazine called it "a witty, self-conscious homage to private museums of yore . . . when natural history was only barely charted by science, and museums were closer to Renaissance cabinets of curiosity." In a similar vein, "The Economist" said the museum "captures a time chronicled in Richard Holmes's recent book "The Age of Wonder", when science mingled with poetry in its pursuit of answers to life's mysterious questions."

Lawrence Weschler's book, "Mr. Wilson's Cabinet of Wonder: Pronged Ants, Horned Humans, Mice on Toast, And Other Marvels of Jurassic Technology", attempts to explain the mystery of the Museum of Jurassic Technology. Weschler deeply explores the museum through conversations with its founder, David Wilson, and through outside research on several exhibitions. His investigations into the history of certain exhibits led to varying results of authenticity; some exhibits seem to have been created by Wilson's imagination while other exhibits might be suitable for display in a natural history museum. The Museum of Jurassic Technology at its heart, according to Wilson, is "a museum interested in presenting phenomena that other natural history museums are unwilling to present."

The museum's introductory slideshow recounts that "In its original sense, the term, 'museum' meant 'a spot dedicated to the Muses, a place where man's mind could attain a mood of aloofness above everyday affairs'". In this spirit, the dimly lit atmosphere, wood and glass vitrines, and labyrinthine floorplan lead visitors through an eclectic range of exhibits on art, natural history, history of science, philosophy, and anthropology, with a special focus on the history of museums and the variety of paths to knowledge. The museum attracts approximately 25,000 visitors per year.

The museum maintains more than thirty permanent exhibits, including:

From 1992 to 2006, the museum's Foundation Collection was on display in its Tochtermuseum at the Karl Ernst Osthaus-Museum in Hagen, Germany. This exhibition was part of the Museum of Museums wing at the KEOM, which came into being under the stewardship of director Michael Fehr.

In 2005, the museum opened its Tula Tea Room, a Russian-style tea room where Georgian tea is served. This room is a miniature reconstruction of the study of Tsar Nicolas II from the Winter Palace in St. Petersburg, Russia. The Borzoi Kabinet Theater screens a series of poetic documentaries produced by the Museum of Jurassic Technology in collaboration with the St. Petersburg–based arts and science collective Kabinet. The series of films, entitled "A Chain of Flowers", draws its name from the quotation by Charles Willson Peale: "The Learner must be led always from familiar objects toward the unfamiliar, guided along, as it were, a chain of flowers into the mysteries of life". The titles of the films are "Levsha: The Cross-eyed Lefty from Tula and the Steel Flea" (2001), "Obshee Delo: The Common Task" (2005), "Bol'shoe Sovietskaia Zatmenie: The Great Soviet Eclipse" (2008), "The Book of Wisdom and Lies" (2011), and "Language of the Birds" (2012).

The museum was the subject of a 1995 book by Lawrence Weschler entitled "Mr. Wilson's Cabinet of Wonder: Pronged Ants, Horned Humans, Mice on Toast, and Other Marvels of Jurassic Technology", which describes in detail many of its exhibits. The museum is mentioned in the novel "The Museum of Innocence", by Turkish Nobel-laureate Orhan Pamuk.



</doc>
<doc id="20451" url="https://en.wikipedia.org/wiki?curid=20451" title="Men at Work">
Men at Work

Men at Work are an Australian rock band formed in 1979 and best known for hits such as "Who Can It Be Now?" and "Down Under". Its founding member was Colin Hay on lead vocals and guitar. After playing as an acoustic duo with Ron Strykert during 1978–79, he formed the group with Strykert playing bass guitar, and Jerry Speiser on drums. They were soon joined by Greg Ham on flute, saxophone, and keyboards and John Rees on bass guitar, with Strykert then switching to lead guitar. The group was managed by Russell Depeller, a friend of Hay, whom he met at Latrobe University. This line-up achieved national and international success in the early 1980s. In January 1983, they were the first Australian artists to have a simultaneous No. 1 album and No. 1 single in the United States "Billboard" charts: "Business as Usual" (released on 9 November 1981) and "Down Under" (1981), respectively. With the same works, they achieved the distinction of a simultaneous No. 1 album and No. 1 single on the Australian, New Zealand, and United Kingdom charts. Their second album "Cargo" (2 May 1983) was also No. 1 in Australia, No. 2 in New Zealand, No. 3 in the US, and No. 8 in the UK. Their third album "Two Hearts" (3 April 1985) reached the top 20 in Australia and top 50 in the US.

They won the Grammy Award for Best New Artist in 1983, they were inducted into the ARIA Hall of Fame in 1994, and they have sold over 30 million albums worldwide. In May 2001, "Down Under" was listed at No. 4 on the APRA Top 30 Australian songs and "Business as Usual" appeared in the book "100 Best Australian Albums" (October 2010).

The original band line-up split in two in 1984, with Speiser and Rees being asked to leave the group. This left Hay, Ham and Strykert. During the recording of the "Two Hearts" album, Strykert decided to leave. Soon after the release of "Two Hearts", Ham left also, leaving Hay as the sole remaining member. Hay and Ham toured the world as Men at Work from 1996 until 2002. On 19 April 2012, Ham was found dead at his home from an apparent heart attack. In 2019, Hay revived the Men at Work moniker and began touring with the assistance of a backing band featuring none of the other original members.

The nucleus of Men at Work formed in Melbourne around June 1979 with Colin Hay on lead vocals and guitar, Ron Strykert on bass guitar, and Jerry Speiser on drums. They were soon joined by Greg Ham on flute, sax and keyboards, and then John Rees on bass guitar, with Ron switching to lead guitar. Hay had emigrated to Australia in 1967 from Scotland with his family. In 1978, he had formed an acoustic duo with Strykert, which expanded by mid-1979 with the addition of Speiser. Around this time as a side project, keyboardist Greg Sneddon (ex-Alroy Band). a former bandmate of Jerry Speiser, together with Speiser, Hay and Strykert performed and recorded the music to 'Riff Raff", a low budget stage musical, upon which Sneddon had worked.

Hay had asked Greg Ham to join the group, but Greg had hesitated, as he was finishing his music degree. Ultimately, he decided to join the band in October 1979. John Rees, a friend of Jerry, joined soon after. The name Men At Work was thrown into the hat by Colin Hay, and was seconded by Ron Strykert, when a name was required to put on the blackboard outside The Cricketer's Arms Hotel, Richmond. The band built a "grass roots" reputation as a pub rock band. In 1980, the group issued their debut single, "Keypunch Operator" backed by "Down Under", with both tracks co-written by Hay and Strykert. It was "self-financed" and appeared on their own independent, M. A. W. label. Australian musicologist, Ian McFarlane, felt the A-side was "a fast-paced country-styled rocker with a clean sound and quirky rhythm". Despite not appearing in the top 100 on the Australian Kent Music Report Singles Chart, by the end of that year the group had "grown in stature to become the most in-demand and highly paid, unsigned band of the year".

Early in 1981 Men at Work signed with CBS Records, the Australian branch of CBS Records International, (which became Sony Music) on the recommendation of Peter Karpin, the label's A&R person. The group's first single with CBS Records in Australia "Who Can It Be Now?", was released in June 1981 which reached No. 2 and remained in the chart for 24 weeks. It had been produced by United States-based Peter McIan, who was also working on their debut album, "Business as Usual".

McIan, together with the band worked on the arrangements for all the songs that appeared on "Business As Usual". Their next single was a re-arranged and "popified" version of "Down Under". It appeared in October that year and reached No. 1 in November, where it remained for six weeks. "Business as Usual" was also released in October and went to No. 1 on the Australian Kent Music Report Albums Chart, spending a total of nine weeks at the top spot. "The Canberra Times" Garry Raffaele opined that it "generally stays at a high level, tight and jerky ... There is a delicacy about this music — and that is not a thing you can say about too many rock groups. The flute and reeds of Greg Ham do much to further that". McFarlane noted that "[a]side from the strength of the music, part of the album's appeal was its economy. The production sound was low-key, but clean and uncluttered. Indeed, the songs stood by themselves with little embellishment save for a bright, melodic, singalong quality".

By February the following year both "Down Under" and "Business as Usual" had reached No. 1 on the respective Official New Zealand Music Charts – the latter was the first Australian album to reach that peak in New Zealand. Despite its strong Australian and New Zealand showing, and having an American producer (McIan), "Business as Usual" was twice rejected by Columbia's US parent company. Thanks to the persistence of Russell Depeller and Karpin, the album was finally released in the US and the United Kingdom in April 1982 – six months after its Australian release. Their next single, "Be Good Johnny", was issued in Australia in April 1982 and reached No. 8 in Australia, and No. 3 in New Zealand.

Men at Work initially broke through to North American audiences in the western provinces of Canada with "Who Can It Be Now?" hitting the top 10 on radio stations in Winnipeg by May 1982. It peaked at No. 8 on the Canadian "RPM" Top Singles Chart in July. In August the group toured Canada and the US to promote the album and related singles, supporting Fleetwood Mac. The band became more popular on Canadian radio in the following months and also started receiving top 40 US airplay by August. In October "Who Can It Be Now?" reached No. 1 on the US "Billboard" Hot 100, while Canada was one single ahead with "Down Under" topping the Canadian charts that same month. In the following month "Business as Usual" began a 15-week run at No. 1 on the "Billboard" 200.

While "Who Can It Be Now?" was still in the top ten in the US, "Down Under" was finally released in that market. It entered the US charts at No. 79 and ten weeks later, it was No. 1. By January 1983 Men at Work had the top album and single in both the US and the UK – never previously achieved by an Australian act. "Be Good Johnny" received moderate airplay in the US; it reached the top 20 in Canada.

"Down Under" gained international media exposure in September 1983 through television coverage of the Australian challenge for the America's Cup yacht trophy in September 1983 when it was adopted as the theme song by the crew of the successful "Australia II".

The band released their second album, "Cargo", in April 1983, which also peaked at No. 1 – for two weeks – on the Australian charts. In New Zealand it reached No. 2. It had been finished in mid-1982 with McIan producing again, but was held back due to the success of their debut album on the international market, where "Business as Usual" was still riding high. "Cargo" appeared at No. 3 on the "Billboard" 200, and No. 8 in the UK. The lead single, "Overkill", was issued in Australia ahead of the album in October 1982 and reached No. 6, it peaked at No. 3 in the US. "Dr. Heckyll & Mr. Jive" followed in March 1983 made it to No. 5 in Australia, and No. 28 in the US. "It's a Mistake" reached No. 6 in the US. The band toured the world extensively in 1983.

In 1984, long standing tensions between Hay and Speiser led to a split in the band. Both Rees and Speiser were told they were "not required", as Hay, Ham and Strykert used session musicians to record their third album, "Two Hearts" (23 April 1985). Studio musicians included Jeremy Alsop on bass guitar (ex-Ram Band, Pyramid, Broderick Smith Band); and Mark Kennedy on drums (Spectrum, Ayers Rock, Marcia Hines Band). "Two Hearts" was produced by Hay and Ham. It was a critical and commercial failure compared to their previous albums and only peaked at No. 16 in Australia, and No. 50 on the US chart. Strykert had left during its production.

Four tracks were released as singles, "Everything I Need" (May 1985), "Man with Two Hearts", "Maria" (August), and "Hard Luck Story" (October); only the lead single charted in Australia (No. 37) and the US (No. 47). The album relied heavily on drum machines and synthesisers, and reduced the presence of Ham's saxophone, giving it a different feel compared to its predecessors. Hay and Ham hired new bandmates, to tour in support of "Two Hearts", with Alsop and Kennedy joined by James Black on guitar and keyboards (Mondo Rock, The Black Sorrows). Soon after a third guitarist, Colin Bayley (Mi-Sex), was added and Kennedy was replaced on drums by Chad Wackerman (Frank Zappa). Australian singers Kate Ceberano and Renée Geyer had also worked on the album and performed live as guest vocalists.

On 13 July 1985 Men at Work performed three tracks for the Oz for Africa concert (part of the global Live Aid program)—"Maria", "Overkill", and an unreleased one, "The Longest Night". They were broadcast in Australia (on both Seven Network and Nine Network) and on MTV in the US. "Maria" and "Overkill" were also broadcast by American Broadcasting Company (ABC) during their Live Aid telecast. Ham left during the band's time touring behind the album. The final Men at Work performances during 1985 had jazz saxophonist Paul Williamson (The Black Sorrows), replacing Ham. By early 1986 the band was defunct and Hay started recording his first solo album, "Looking for Jack" (January 1987), which had Alsop and Wackerman as session musicians.

By mid-1996, after a ten-year absence, Hay and Ham reformed Men at Work to tour South America. They had enjoyed strong fan support there during their earlier career and demands for a reunion had persisted. The 1996 line up had Stephen Hadley on bass guitar and backing vocals (ex-The Black Sorrows, Paul Kelly Band); Simon Hosford on guitar and backing vocals (Colin Hay backing band); and John Watson on drums (The Black Sorrows). The tour culminated in a performance in São Paulo, which was recorded for the Brazilian release of a live album, "Brazil '96", in 1997, which was co-produced by Hay and Ham for Sony Records. It was re-released worldwide in 1998 as "Brazil" with a bonus track, "The Longest Night", the first new studio track since "Two Hearts".

In 1997 drummer Tony Floyd replaced Watson but by 1998 the lineup was Hay, Ham, James Ryan (guitar, backing vocals), Rick Grossman (of the Hoodoo Gurus) on bass and Peter Maslen (ex-Boom Crash Opera) on drums. In 1999 Ryan, Grossman and Maslen were out and Hosford and Floyd were back in, along with bassist Stuart Speed. Rodrigo Aravena was brought in on bass in 2000, along with Heta Moses on drums. Moses was replaced by Warren Trout in 2001 as Stephen Hadley returned on bass.

The band toured Australia, South America, Europe and the US from 1998 to 2000. Men at Work performed "Down Under" at the closing ceremony of the 2000 Summer Olympics in Sydney, alongside Paul Hogan of ""Crocodile" Dundee" (1986).

One of their European tours for mid-2000 was cancelled and the group had disbanded by 2002, although Hay and Ham periodically reunited Men at Work with guest musicians (including an appearance in February 2009, when they performed "Down Under" as a duo at the Australia Unites Victorian Bushfire Appeal Telethon).

In February 2010 Larrikin Music Publishing won a case against Hay and Strykert, their record label (Sony BMG Music Entertainment) and music publishing company (EMI Songs Australia) arising from the uncredited appropriation of "Kookaburra", originally written in 1932 by Marion Sinclair and for which Larrikin owned the publishing rights, as the flute line in the Men at Work song, "Down Under". Back in early 2009 the Australian music-themed TV quiz, "Spicks and Specks", had posed a question which suggested that "Down Under" contained elements of "Kookaburra".

Larrikin, then headed by Norman Lurie, filed suit after Larrikin was sold to another company and had demanded between 40% and 60% of the previous six years of earnings from the song. In February 2010 the judge ruled that "Down Under" did contain a flute riff based on "Kookaburra" but stipulated that neither was it necessarily the hook nor a substantial part of the hit song (Hay and Strykert had written the track years before the flute riff was added by Ham). In July 2010 a judge ruled that Larrikin should be paid 5% of past (since 2002) and future profits. Ham took the verdict particularly hard, feeling responsible for having performed the flute riff at the centre of the lawsuit and worried that he would only be remembered for copying someone else's music, resulting in depression and anxiety. Ham's body was found in his Carlton North home on April 19, 2012 after he suffered a fatal heart attack at age 58.

In June 2019, Hay toured Europe with a group of Los Angeles-based session musicians under the name Men at Work, despite the band featuring no other original members of the band.

Hay maintained a solo career and played with Ringo Starr & His All-Starr Band. Strykert relocated to Hobart in 2009 from Los Angeles, and continued to play music and released his first solo album, "Paradise", in September that year. He expressed resentment towards Hay, mainly over royalties. Ham remained musically active and played sax with the Melbourne-based group The Nudist Funk Orchestra until his death. Rees was a music teacher in Melbourne and also played the violin and bass guitar for the band Beggs 2 Differ. Speiser played drums for the band, The Afterburner.

The group won the 1983 Grammy Award for Best New Artist; the other nominees were Asia, Jennifer Holliday, The Human League and Stray Cats. In August 1983 they were given a Crystal Globe Award for $100 million worth of record business by their US label. That same year in Canada they were awarded a Juno Award for "International LP of the Year". Men at Work have sold over 30 million albums worldwide.

At the ARIA Music Awards of 1994 they were inducted into the related Hall of Fame. On 28 May 2001 "Down Under" was listed at No. 4 on the APRA Top 30 Australian songs. In October 2010, "Business as Usual" was listed in the book, "100 Best Australian Albums".

Colin Hay has been the only constant member in all configurations.





</doc>
<doc id="20452" url="https://en.wikipedia.org/wiki?curid=20452" title="Meconium aspiration syndrome">
Meconium aspiration syndrome

Meconium aspiration syndrome (MAS) also known as neonatal aspiration of meconium is a medical condition affecting newborn infants. It describes the spectrum of disorders and pathophysiology of newborns born in meconium-stained amniotic fluid (MSAF) and have meconium within their lungs. Therefore, MAS has a wide range of severity depending on what conditions and complications develop after parturition. Furthermore, the pathophysiology of MAS is multifactorial and extremely complex which is why it is the leading cause of morbidity and mortality in term infants.

The word "meconium" is derived from the Greek word "mēkōnion" meaning "juice from the opium poppy" as the sedative effects it had on the foetus were observed by Aristotle. 

Meconium is a sticky dark-green substance which contains gastrointestinal secretions, amniotic fluid, bile acids, bile, blood, mucus, cholesterol, pancreatic secretions, lanugo, vernix caseosa and cellular debris. Meconium accumulates in the foetal gastrointestinal tract throughout the third trimester of pregnancy and it is the first intestinal discharge released within the first 48 hours after birth. Notably, since meconium and the whole content of the gastrointestinal tract is located ‘extracorporeally,’ its constituents are hidden and normally not recognised by the foetal immune system.

For the meconium within the amniotic fluid to successfully cause MAS, it has to enter the respiratory system during the period when the fluid-filled lungs transition into an air-filled organ capable of gas exchange.

The main theories of meconium passage into amniotic fluid are caused by fetal maturity or from foetal stress as a result of hypoxia or infection. Other factors that promote the passage of meconium "in utero" include placental insufficiency, maternal hypertension, pre-eclampsia and maternal drug use of tobacco and cocaine. However, the exact mechanism for meconium passage into the amniotic fluid is not completely understood and it may be a combination of several factors. 

There may be an important association between foetal distress and hypoxia with MSAF. It is believed that foetal distress develops into foetal hypoxia causing the foetus to defecate meconium resulting in MSAF and then perhaps MAS. Other stressors which causes foetal distress, and therefore meconium passage, includes when umbilical vein oxygen saturation is below 30%.

Foetal hypoxic stress during parturition can stimulate colonic activity, by enhancing intestinal peristalsis and relaxing the anal sphincter, which results in the passage of meconium. Then, because of intrauterine gasping or from the first few breaths after delivery, MAS may develop. Furthermore, aspiration of thick meconium leads to obstruction of airways resulting in a more severe hypoxia.

It is important to note that the association between foetal distress and meconium passage is not a definite cause-effect relationship as over ¾ of infants with MSAF are vigorous at birth and do not have any distress or hypoxia. Additionally, foetal distress occurs frequently without the passage of meconium as well.

Although meconium is present in the gastrointestinal tract early in development, MSAF rarely occurs before 34 weeks gestation.

Peristalsis of the foetal intestines is present as early as 8 weeks gestation and the anal sphincter develops at about 20–22 weeks. The early control mechanisms of the anal sphincter are not well understood, however there is evidence that the foetus does defecate routinely into the amniotic cavity even in the absence of distress. The presence of fetal intestinal enzymes have been found in the amniotic fluid of women who are as early as 14–22 weeks pregnant. Thus, suggesting there is free passage of the intestinal contents into the amniotic fluid. 

Motilin is found in higher concentrations in post-term than pre-term foetal gastrointestinal tracts. Similarly, intestinal parasympathetic innervation and myelination also increases in later gestations. Therefore, the increased incidence of MAS in post-term pregnancies may reflect the maturation and development of the peristalsis within the gastrointestinal tract in the newborn.

As MAS describes a spectrum of disorders of newborns born through MSAF, without any congenital respiratory disorders or other underlying pathology, there are numerous hypothesised mechanisms and causes for the onset of this syndrome. Long-term consequences may arise from these disorders, for example, infants that develop MAS have higher rates of developing neurodevelopmental defects due to poor respiration. 

In the first 15 minutes of meconium aspiration, there is obstruction of larger airways which causes increased lung resistance, decreased lung compliance, acute hypoxemia, hypercapnia, atelectasis and respiratory acidosis. After 60 minutes of exposure, the meconium travels further down into the smaller airways. Once within the terminal bronchioles and alveoli, the meconium triggers inflammation, pulmonary edema, vasoconstriction, bronchoconstriction, collapse of airways and inactivation of surfactant. 

The lung areas which do not or only partially participate in ventilation, because of obstruction and/or destruction, will become hypoxic and an inflammatory response may consequently occur. Partial obstruction will lead to air trapping and hyperinflation of certain lung areas and pneumothorax may follow. Chronic hypoxia will lead to an increase in pulmonary vascular smooth muscle tone and persistent pulmonary hypertension causing respiratory and circulatory failure. 

Microorganisms, most commonly Gram-negative rods, and endotoxins are found in samples of MSAF at a higher rate than in clear amniotic fluid, for example 46.9% of patients with MSAF also had endotoxins present. A microbial invasion of the amniotic cavity (MIAC) is more common in patients with MSAF and this could ultimately lead to an intra-amniotic inflammatory response. MIAC is associated with high concentrations of cytokines (such as IL-6), chemokines (such as IL-8 and monocyte chemoattractant protein-1), complement, phospholipase A and matrix-degrading enzymes. Therefore, these aforementioned mediators within the amniotic fluid during MIAC and intra-amniotic infection could, when aspirated "in" "utero", induce lung inflammation within the foetus. 

Meconium has a complex chemical composition, so it is difficult to identify a single agent responsible for the several diseases that arise. As meconium is stored inside the intestines, and is partly unexposed to the immune system, when it becomes aspirated the innate immune system recognises as a foreign and dangerous substance. The immune system, which is present at birth, responds within minutes with a low specificity and no memory in order to try to eliminate microbes. Meconium perhaps leads to chemical pneumonitis as it is a potent activator of inflammatory mediators which include cytokines, complement, prostaglandins and reactive oxygen species.

Meconium is a source of pro-inflammatory cytokines, including tumour necrosis factor (TNF) and interleukins (IL-1, IL-6, IL-8), and mediators produced by neutrophils, macrophages and epithelial cells that may injure the lung tissue directly or indirectly. For example, proteolytic enzymes are released from neutrophilic granules and these may damage the lung membrane and surfactant proteins. Additionally, activated leukocytes and cytokines generate reactive nitrogen and oxygen species which have cytotoxic effects. Oxidative stress results in vasoconstriction, bronchoconstriction, platelet aggregation and accelerated cellular apoptosis. Recently, it has been hypothesised that meconium is a potent activator of toll-like receptor (TLRs) and complement, key mediators in inflammation, and may thus contribute to the inflammatory response in MAS. 

Meconium contains high amounts of phospholipase A (PLA), a potent proinflammatory enzyme, which may directly (or through the stimulation of arachidonic acid) lead to surfactant dysfunction, lung epithelium destruction, tissue necrosis and an increase in apoptosis. Meconium can also activate the coagulation cascade, production of platelet-activating factor (PAF) and other vasoactive substances that may lead to destruction of capillary endothelium and basement membranes. Injury to the alveolocapillary membrane results in leakage of liquid, plasma proteins, and cells into the interstitium and alveolar spaces.

Surfactant is synthesised by type II alveolar cells and is made of a complex of phospholipids, proteins and saccharides. It functions to lower surface tension (to allow for lung expansion during inspiration), stabilise alveoli at the end of expiration (to prevent alveolar collapse) and prevents lung oedema. Surfactant also contributes to lung protection and defence as it is also an anti-inflammatory agent. Surfactant enhances the removal of inhaled particles and senescent cells away from the alveolar structure. 

The extent of surfactant inhibition depends on both the concentration of surfactant and meconium. If the surfactant concentration is low, even very highly diluted meconium can inhibit surfactant function whereas, in high surfactant concentrations, the effects of meconium are limited. Meconium may impact surfactant mechanisms by preventing surfactant from spreading over the alveolar surface, decreasing the concentration of surfactant proteins (SP-A and SP-B), and by changing the viscosity and structure of surfactant. Several morphological changes occur after meconium exposure, the most notable being the detachment of airway epithelium from stroma and the shedding of epithelial cells into the airway. These indicate a direct detrimental effect on lung alveolar cells because of the introduction of meconium into the lungs.

Persistent pulmonary hypertension (PPHN) is the failure of the foetal circulation to adapt to extra-uterine conditions after birth. PPHN is associated with various respiratory diseases, including MAS (as 15-20% of infants with MAS develop PPHN), but also pneumonia and sepsis. A combination of hypoxia, pulmonary vasoconstriction and ventilation/perfusion mismatch can trigger PPHN, depending on the concentration of meconium within the respiratory tract. PPHN in newborns is the leading cause of death in MAS. 

Apoptosis is an important mechanism in the clearance of injured cells and in tissue repair, however too much apoptosis may cause harm, such as acute lung injury. Meconium induces apoptosis and DNA cleavage of lung airway epithelial cells, this is detected by the presence of fragmented DNA within the airways and in alveolar epithelial nuclei. Meconium induces an inflammatory reaction within the lungs as there is an increase of autophagocytic cells and levels of caspase 3 after exposure. After 8 hours of meconium exposure, in rabbit foetuses, the total amount of apoptotic cells is 54%. Therefore, the majority of meconium-induced lung damage may be due to the apoptosis of lung epithelium.

Respiratory distress in an infant born through the darkly coloured MSAF as well as meconium obstructing the airways is usually sufficient enough to diagnose MAS. Additionally, newborns with MAS can have other types of respiratory distress such as tachypnea and hypercapnia. Sometimes it is hard to diagnose MAS as it can be confused with other diseases that also cause respiratory distress, such as pneumonia. Additionally, X-rays and lung ultrasounds can be quick, easy and cheap imaging techniques to diagnose lung diseases like MAS.

In general, the incidence of MAS has been significantly reduced over the past two decades as the number of post-term deliveries has minimised. Currently, labour is induced in women who have been pregnant for longer than 41 weeks gestation.

Prevention during pregnancy may include amnioinfusion and antibiotics but the effectiveness of these treatments are questionable. 

As previously mentioned, oropharyngeal and nasopharyngeal suctioning is not an ideal preventative treatment for both vigorous and depressed (not breathing) infants.

Most infants born through MSAF do not require any treatments (other than routine postnatal care) as they show no signs of respiratory distress, as only approximately 5% of infants born through MSAF develop MAS. However, infants which do develop MAS need to be admitted to a neonatal unit where they will be closely observed and provided any treatments needed. Observations include monitoring heart rate, respiratory rate, oxygen saturation and blood glucose (to detect worsening respiratory acidosis or the development of hypoglycemia). In general, treatment of MAS is more supportive in nature.

To clear the airways of meconium, tracheal suctioning can be used however, the efficacy of this method is in question and it can cause harm. 

In cases of MAS, there is a need for supplemental oxygen for at least 12 hours in order to maintain oxygen saturation of haemoglobin at 92% or more. The severity of respiratory distress can vary significantly between newborns with MAS, as some require minimal or no supplemental oxygen requirement and, in severe cases, mechanical ventilation may be needed. The desired oxygen saturation is between 90-95% and PaO may be as high as 90mmHg. In cases where there is thick meconium deep within the lungs, mechanical ventilation may be required. In extreme cases, extracorporeal membrane oxygenation (ECMO) may be utilised in infants who fail to respond to ventilation therapy. While on ECMO, the body can have time to absorb the meconium and for all the associated disorders to resolve. There has been an excellent response to this treatment, as the survival rate of MAS while on ECMO is more than 94%.

Ventilation of infants with MAS can be challenging and, as MAS can affect each individual differently, ventilation administration may need to be customised. Some newborns with MAS can have homogenous lung changes and others can have inconsistent and patchy changes to their lungs. It is common for sedation and muscle relaxants to be used to optimise ventilation and minimise the risk of pneumothorax associated with dyssynchronous breathing.

Inhaled nitric oxide (iNO) acts on vascular smooth muscle causing selective pulmonary vasodilation. This is ideal in the treatment of PPHN as it causes vasodilation within ventilated areas of the lung thus, decreasing the ventilation-perfusion mismatch and thereby, improves oxygenation. Treatment utilising iNO decreases the need for ECMO and mortality in newborns with hypoxic respiratory failure and PPHN as a result of MAS. However, approximately 30-50% of infants with PPHN do not respond to iNO therapy. 

As inflammation is such a huge issue in MAS, treatment has consisted of anti-inflammatories. 

Glucocorticoids (GCs) have a strong anti-inflammatory activity and works to reduce the migration and activation of neutrophils, eosinophils, mononuclears and other cells. GCs reduce the migration of neutrophils into the lungs ergo, decreasing their adherence to the endothelium. Thus, there is a reduction in the action of mediators released from these cells and therefore, a reduced inflammatory response.

GCs also possess a genomic mechanism of action in which, once bound to a glucocorticoid receptor, the activated complex moves into the nucleus and inhibits transcription of mRNA. Ultimately, effecting whether various proteins get produced or not. Inhibiting the transcription of nuclear factor (NF-κB) and protein activator (AP-1) attenuates the expression of pro-inflammatory cytokines (IL-1, IL-6, IL-8 and TNF etc.), enzymes (PLA, COX-2, iNOs etc.) and other biologically active substances. The anti-inflammatory effect of GCs is also demonstrated by enhancing the activity of lipocortines which inhibit the activity of PLA and therefore, decrease the production of arachidonic acid and mediators of lipoxygenase and cyclooxygenase pathways. 

Anti-inflammatories need to be administered as quickly as possible as the effect of these drugs can diminish even just an hour after meconium aspiration. For example, early administration of dexamethasone significantly enhanced gas exchange, reduced ventilatory pressures, decreased the number of neutrophils in the bronchoalveolar area, reduced oedema formation and oxidative lung injury.However, GCs may increase the risk of infection and this risk increases with the dose and duration of glucocorticoid treatment. Other issues can arise, such as aggravation of diabetes mellitus, osteoporosis, skin atrophy and growth retardation in children. 

Phosphodiesterases (PDE) degrades cAMP and cGMP and, within the respiratory system of a newborn with MAS, various isoforms of PDE may be involved due to their pro-inflammatory and smooth muscle contractile activity. Therefore, non-selective and selective inhibitors of PDE could potentially be used in MAS therapy. However, the use of PDE inhibitors can cause cardiovascular side effects. Non-selective PDE inhibitors, such as methylxanthines, increase concentrations of cAMP and cGMP in the cells leading to bronchodilation and vasodilation. Additionally, methylxanthines decreases the concentrations of calcium, acetylcholine and monoamines, this controls the release of various mediators of inflammation and bronchoconstriction, including prostaglandins. Selective PDE inhibitors target one subtype of phosphodiesterase and in MAS the activities of PDE-3, PDE-4, PDE-5 and PDE-7 may become enhanced. For example, Milrinone (a selective PDE3 inhibitor) improved oxygenation and survival of neonates with MAS.

Arachidonic acid is metabolised, via cyclooxygenase (COX) and lipoxygenase, to various substances including prostaglandins and leukotrienes, which exhibit potent pro-inflammatory and vasoactive effects. By inhibiting COX, and more specifically COX-2, (either through selective or non-selective drugs) inflammation and oedema can be reduced. However, COX inhibitors may induce peptic ulcers and cause hyperkalemia and hypernatremia. Additionally, COX inhibitors have not shown any great response in the treatment of MAS.

Meconium is typically sterile however, it can contain various cultures of bacteria so appropriate antibiotics may need to be prescribed. 

Lung lavage with diluted surfactant is a new treatment with potentially beneficial results depending on how early it is administered in newborns with MAS. This treatment shows promise as it has a significant effect on air leaks, pneumothorax, the need for ECMO and death. Early intervention and using it on newborns with mild MAS is more effective. However, there are risks as a large volume of fluid instillation to the lung of a newborn can be dangerous (particularly in cases of severe MAS with pulmonary hypertension) as it can exacerbate hypoxia and lead to mortality.

Originally, it was believed that MAS developed as a result of the meconium being a physical blockage of the airways. Thus, to prevent newborns, who were born through MSAF, from developing MAS, suctioning of the oropharyngeal and nasopharyngeal area before delivery of the shoulders followed by tracheal aspiration was utilised for 20 years. This treatment was believed to be effective as it was reported to significantly decrease the incidence of MAS compared to those newborns born through MSAF who were not treated. This claim was later disproved and future studies concluded that oropharyngeal and nasopharyngeal suctioning, before delivery of the shoulders in infants born through MSAF, does not prevent MAS or its complications. In fact, it can cause more issues and damage (e.g. mucosal damage), thus it is not a recommended preventative treatment. Suctioning may not significantly reduce the incidence of MAS as meconium passage and aspiration may occur "in-utero." Thereby making the suctioning redundant and useless as the meconium may already be deep within the lungs at the time of birth. 

Historically, amnioinfusion has been used when MSAF was present, which involves a transcervical infusion of fluid during labour. The idea was to dilute the thick meconium to reduce its potential pathophysiology and reduce cases of MAS, since MAS is more prevalent in cases of thick meconium. However, there are associated risks, such as umbilical cord prolapse and prolongation of labour. The UK National Institute of Health and Clinical Excellence (NICE) Guidelines recommend against the use of amnioinfusion in women with MSAF.

1 in every 7 pregnancies have MSAF and, of these cases, approximately 5% of these infants develop MAS. MSAF is observed 23-52% in pregnancies at 42 weeks therefore, the frequency of MAS increases as the length of gestation increases, such that the prevalence is greatest in post-term pregnancies. Conversely, preterm births are not frequently associated with MSAF (only approximately 5% in total contain MSAF). The rate of MAS declines in populations where labour is induced in women that have pregnancies exceeding 41 weeks. There are many suspected pre-disposing factors that are thought to increase the risk of MAS. For example, the risk of MSAF is higher in African American, African and Pacific Islander mothers, compared to mothers from other ethnic groups.

Research is being focused on developing both a successful method for preventing MAS as well as an effective treatment. For example, investigations are being made in the efficiency of anti-inflammatory agents, surfactant replacement therapy and antibiotic therapy. More research needs to be conducted on the pharmacological properties of, for example, glucocorticoids, including dosages, administration, timing or any drug interactions. Additionally, there is still research being conducted on whether intubation and suctioning of meconium in newborns with MAS is beneficial, harmful or is simply a redundant and outdated treatment. In general, there is still no generally accepted therapeutic protocol and effective treatment plan for MAS.




</doc>
<doc id="20453" url="https://en.wikipedia.org/wiki?curid=20453" title="Meconium">
Meconium

Meconium is the earliest stool of a mammalian infant. Unlike later feces, meconium is composed of materials ingested during the time the infant spends in the uterus: intestinal epithelial cells, lanugo, mucus, amniotic fluid, bile and water. Meconium, unlike later feces, is viscous and sticky like tar, its color usually being a very dark olive green; it is almost odorless. When diluted in amniotic fluid, it may appear in various shades of green, brown, or yellow. It should be completely passed by the end of the first few days after birth, with the stools progressing toward yellow (digested milk).

Meconium is normally retained in the infant's bowel until after birth, but sometimes it is expelled into the amniotic fluid (also called "amniotic liquor") prior to birth or during labor and delivery. The stained amniotic fluid (called "meconium liquor" or "meconium-stained liquor") is recognized by medical staff as a possible sign of fetal distress. Some post-dates pregnancies (when they are more than 40 weeks pregnant) may also have meconium-stained liquor without fetal distress. Medical staff may aspirate the meconium from the nose and mouth of a newborn immediately after delivery in the event the baby shows signs of respiratory distress to decrease the risk of meconium aspiration syndrome, which can occur in meconium-stained amniotic fluid.

Most of the time that the amniotic fluid is stained with meconium, it will be homogeneously distributed throughout the fluid, making it brown. This indicates that the fetus passed the meconium some time ago such that sufficient mixing occurred as to establish the homogeneous mixture. Terminal meconium occurs when the fetus passes the meconium a short enough time before birth/cesarean section that the amniotic fluid remains clear, but individual clumps of meconium are in the fluid.

The failure to pass meconium is a symptom of several diseases including Hirschsprung's disease and cystic fibrosis.

The meconium sometimes becomes thickened and congested in the intestines, a condition known as meconium ileus. Meconium ileus is often the first sign of cystic fibrosis. In cystic fibrosis, the meconium can form a bituminous black-green mechanical obstruction in a segment of the ileum. Beyond this, there may be a few separate grey-white globular pellets. Below this level, the bowel is a narrow and empty micro-colon. Above the level of the obstruction, there are several loops of hypertrophied bowel distended with fluid. No meconium is passed, and abdominal distension and vomiting appear soon after birth. About 20% of cases of cystic fibrosis present with meconium ileus, while approximately 20% of one series of cases of meconium ileus did not have cystic fibrosis. The presence of meconium ileus is not related to the severity of the cystic fibrosis. The obstruction can be relieved in a number of different ways.

Meconium ileus should be distinguished from meconium plug syndrome, in which a tenacious mass of mucus prevents the meconium from passing and there is no risk of intestinal perforation. Meconium ileus has a significant risk of intestinal perforation. In a barium enema, meconium plug syndrome shows a normal or dilated colon as compared to micro-colon in meconium ileus.

Meconium can be tested for various drugs, to check for "in utero" exposure. Using meconium, a Canadian research group showed that by measuring a by-product of alcohol, fatty acid ethyl esters (FAEE) they could objectively detect excessive maternal drinking of alcohol during pregnancy. In the US, the results of meconium testing may be used by child protective services and other law enforcement agencies to determine the eligibility of the parents to keep the newborn. Meconium can also be analyzed to detect the tobacco use of mothers during their pregnancy, which is commonly under-reported.

The issue of whether meconium is sterile remains debated and is an area of ongoing research. Although some researchers have reported evidence of bacteria in meconium, this has not been consistently confirmed. Other researchers have raised questions about whether these findings may be due to contamination after sample collection and that meconium is, in fact, sterile until after birth. Further researchers have hypothesized that there may be bacteria in the womb, but these are a normal part of pregnancy and could have an important role in shaping the developing immune system and are not harmful to the baby.

The Latin term "meconium" derives from the Greek , "mēkōnion", a diminutive of , "mēkōn", i.e. poppy, in reference either to its tar-like appearance that may resemble some raw opium preparations or to Aristotle's belief that it induces sleep in the fetus.



</doc>
<doc id="20454" url="https://en.wikipedia.org/wiki?curid=20454" title="Montreux Convention Regarding the Regime of the Straits">
Montreux Convention Regarding the Regime of the Straits

The Montreux Convention Regarding the Regime of the Straits is a 1936 agreement that gives Turkey control over the Turkish Straits (the Bosporus and Dardanelles straits) and regulates the transit of naval warships. The Convention guarantees the free passage of civilian vessels in peacetime, and restricts the passage of naval ships not belonging to Black Sea states. The terms of the Convention have been a source of controversy over the years, most notably about the Soviet Union's military access to the Mediterranean Sea.

Signed on 20 July 1936 at the Montreux Palace in Switzerland, the Convention permitted Turkey to remilitarise the Straits. It went into effect on 9 November 1936 and was registered in the "League of Nations Treaty Series" on 11 December 1936. It remains in force.

The proposed 21st-century Kanal Istanbul project may be a possible bypass to the Montreux Convention and allow greater Turkish autonomy with respect to the passage of military ships (which are limited in number, tonnage, and weaponry) from the Black Sea to the Sea of Marmara.

The convention was one of a series of agreements in the 19th and 20th centuries that sought to address the long-running "Straits Question" of who should control the strategically vital link between the Black Sea and Mediterranean Sea. In 1923, the Treaty of Lausanne had demilitarised the Dardanelles and opened the Straits to unrestricted civilian and military traffic, under the supervision of the International Straits Commission of the League of Nations.

By the late 1930s, the strategic situation in the Mediterranean had altered with the rise of Fascist Italy, which controlled the Greek-inhabited Dodecanese islands off the west coast of Turkey and had constructed fortifications on Rhodes, Leros and Kos. The Turks feared that Italy would seek to exploit access to the Straits to expand its power into Anatolia and the Black Sea region. There were also fears of Bulgarian rearmament. Although Turkey was not permitted to refortify the Straits, it nonetheless did so secretly.

In April 1935, the Turkish government dispatched a lengthy diplomatic note to the signatories of the Treaty of Lausanne proposing a conference on the agreement of a new regime for the Straits and requested that the League of Nations authorise the reconstruction of the Dardanelles forts. In the note, Turkish foreign minister Tevfik Rüştü Aras explained that the international situation had changed greatly since 1923. At that time, Europe had been moving towards disarmament and an international guarantee to defend the Straits. The Abyssinia Crisis of 1934–35, the denunciation by Germany of the Treaty of Versailles and international moves towards rearmament meant that "the only guarantee intended to guard against the total insecurity of the Straits has just disappeared in its turn." Indeed, Aras said, "the Powers most closely concerned are proclaiming the existence of a threat of general conflagration." The key weaknesses of the present regime were that the machinery for collective guarantees were too slow and ineffective, there was no contingency for a general threat of war and no provision for Turkey to defend itself. Turkey was therefore prepared

The response to the note was generally favourable, and Australia, Bulgaria, France, Germany, Greece, Japan, Romania, the Soviet Union, Turkey, the United Kingdom and Yugoslavia agreed to attend negotiations at Montreux in Switzerland, which began on 22 June 1936. Two major powers were not represented: Italy, whose aggressively expansionist policies had prompted the conference in the first place, refused to attend and the United States declined even to send an observer.

Turkey, the UK and the Soviet Union each put forward their own set of proposals, aimed chiefly at protecting their own interests. The British favoured the continuation of a relatively restrictive approach, while the Turks sought a more liberal regime that reasserted their own control over the Straits and the Soviets proposed a regime that would guarantee absolute freedom of passage. The British, supported by France, sought to exclude the Soviet fleet from the Mediterranean Sea, where it might have threatened the vital shipping lanes to India, Egypt and the Far East. In the end, the British conceded some of their requests while the Soviets succeeded in ensuring that the Black Sea countries – including the USSR – were given some exemptions from the military restrictions imposed on non-Black Sea nations. The agreement was ratified by all of the conference attendees with the exception of Germany, which had not been a signatory to the Treaty of Lausanne, and with reservations by Japan, and came into force on 9 November 1936.

Britain's willingness to make concessions has been attributed to a desire to avoid Turkey being driven to ally itself with, or fall under the influence of, Adolf Hitler or Benito Mussolini. It was thus the first in a series of steps by Britain and France to ensure that Turkey would either remain neutral or tilt towards the Western Allies in the event of any future conflict with the Axis.

The Convention consists of 29 Articles, four annexes and one protocol. Articles 2–7 consider the passage of merchant ships. Articles 8–22 consider the passage of war vessels. The key principle of freedom of passage and navigation is stated in articles 1 and 2. Article 1 provides, "The High Contracting Parties recognise and affirm the principle of freedom of passage and navigation by sea in the Straits". Article 2 states, "In time of peace, merchant vessels shall enjoy complete freedom of passage and navigation in the Straits, by day and by night, under any flag with any kind of cargo."

The International Straits Commission was abolished, authorising the full resumption of Turkish military control over the Straits and the refortification of the Dardanelles. Turkey was authorised to close the Straits to all foreign warships in wartime or when it was threatened by aggression. Also, it was authorised to refuse transit from merchant ships belonging to countries at war with Turkey.

A number of highly-specific restrictions were imposed on what type of warships are allowed passage. Non-Black-Sea powers willing to send a vessel must notify Turkey 8 days prior of their sought passing. Also, no more than nine foreign warships, with a total aggregate tonnage of 15,000 tons, may pass at any one time. Furthermore, no single ship heavier than 10.000t can pass. An aggregate tonnage of all non-Black Sea warships in the Black Sea must be no more than 30,000 tons (or 45,000 tons under special conditions), and they are permitted to stay in the Black Sea for no longer than twenty-one days. Only Black Sea states may transit capital ships of any tonnage, escorted by no more than two destroyers.

Under Article 12, Black Sea states are also allowed to send submarines through the Straits, with prior notice, as long as the vessels have been constructed, purchased or sent for repair outside the Black Sea. The less restrictive rules applicable to Black Sea states were agreed as, effectively, a concession to the Soviet Union, the only Black Sea state other than Turkey with any significant number of capital ships or submarines. The passage of civil aircraft between the Mediterranean and Black Seas is permitted but only along routes authorised by the Turkish government.

The terms of the Convention were largely a reflection of the international situation in the mid-1930s. They largely served Turkish and Soviet interests, enabling Turkey to regain military control of the Straits and assuring Soviet dominance of the Black Sea. Although the Convention restricted the Soviets' ability to send naval forces into the Mediterranean Sea, thereby satisfying British concerns about Soviet intrusion into what was considered a British sphere of influence, it also ensured that outside powers could not exploit the Straits to threaten the Soviet Union. That was to have significant repercussions during World War II when the Montreux regime prevented the Axis powers from sending naval forces through the Straits to attack the Soviet Union. The Axis powers were thus severely limited in naval capability in their Black Sea campaigns, relying principally on small vessels that had been transported overland by rail and canal networks.

Auxiliary vessels and armed merchant ships occupied a grey area, however, and the transit of such vessels through the straits led to friction between the Allies and Turkey. Repeated protests from Moscow and London led to the Turkish government banning the movements of "suspicious" Axis ships with effect from June 1944 after a number of German auxiliary ships had been permitted to transit the Straits.

Although the Montreux Convention is cited by the Turkish government as prohibiting aircraft carriers from transiting the straits, the treaty actually contains no explicit prohibition on aircraft carriers. However, modern aircraft carriers are heavier than the 15,000 ton limit imposed on warships, making it impossible for non-Black Sea powers to transit modern aircraft carriers through the Straits.

Under Article 11, Black Sea states are permitted to transit capital ships of any tonnage through the straits, but Annex II specifically excludes aircraft carriers from the definition of capital ship. In 1936, it was common for battleships to carry observation aircraft. Therefore, aircraft carriers were defined as ships that were "designed or adapted primarily for the purpose of carrying and operating aircraft at sea." The inclusion of aircraft on any other ship does not classify it as an aircraft carrier.

The Soviet Union designated its "Kiev"-class and "Kuznetsov"-class ships as "aircraft-carrying cruisers" because these ships were armed with P-500 and P-700 cruise missiles, which also form the main armament of the "Slava"-class cruiser and the "Kirov"-class battlecruiser. The result was that the Soviet Navy could send its aircraft-carrying cruisers through the Straits in compliance with the Convention, but at the same time the Convention denied access to NATO aircraft carriers, which exceeded the 15,000 ton limit.

Turkey chose to accept the designation of the Soviet aircraft carrying cruisers as aircraft cruisers, as any revision of the Convention could leave Turkey with less control over the Turkish Straits, and the UN Convention on the Law of the Sea had already established more liberal passage through other straits. By allowing the Soviet aircraft carrying cruisers to transit the Straits, Turkey could leave the more restrictive Montreux Convention in place.

The Convention remains in force, but not without dispute. It was repeatedly challenged by the Soviet Union during World War II and the Cold War. As early as 1939, Joseph Stalin sought to reopen the Straits Question and proposed joint Turkish and Soviet control of the Straits, complaining that "a small state [i.e. Turkey] supported by Great Britain held a great state by the throat and gave it no outlet". After the Molotov–Ribbentrop Pact was signed by the Soviet Union and Nazi Germany, the Soviet Foreign Minister Vyacheslav Molotov informed his German counterparts that the Soviet Union wished to take military control of the Straits and establish its own military base there. The Soviets returned to the issue in 1945 and 1946, demanding a revision of the Montreux Convention at a conference excluding most of the Montreux signatories, a permanent Soviet military presence and joint control of the Straits. That was firmly rejected by Turkey, despite an ongoing Soviet "strategy of tension". For several years after World War II, the Soviets exploited the restriction on the number of foreign warships by ensuring that one of theirs was always in the Straits, thus effectively blocking any state other than Turkey from sending warships through the Straits. Soviet pressure expanded into full demands to revise the Montreux Convention, which led to the Turkish Straits crisis of 1946, which led to Turkey abandoning its policy of neutrality. In 1947, it became the recipient of US military and economic assistance under the Truman Doctrine of containment and joined the NATO alliance, along with Greece, in 1952.

The passage of US warships through the Straits also raised controversy, as the convention forbids the transit of non-Black Sea nations' warships with guns of a calibre larger than eight inches (203 mm). In the 1960s, the US sent warships carrying 420 mm calibre ASROC missiles through the Straits, prompting Soviet protests. The Turkish government rejected the Soviet complaints, pointing out that guided missiles were not guns and that since such weapons had not existed at the time of the Convention, they were not restricted.

According to Antiwar.com news editor Jason Ditz, the Montreux Convention is an obstacle to US Naval buildup in the Black Sea because of its stipulations regulating warship traffic by nations not sharing Black Sea coastline. The US thinktank Stratfor has written that these stipulations place Turkey's relationship to the US and its obligations as a NATO alliance member in conflict with Russia and the regulations of the Montreux Convention. The US Navy has stated its forces will continue to operate in the Black Sea in accordance with the convention.

The United Nations Convention on the Law of the Sea (UNCLOS), which entered into force in November 1994, has prompted calls for the Montreux Convention to be revised and adapted to make it compatible with UNCLOS's regime governing straits used for international navigation. However, Turkey's longstanding refusal to sign UNCLOS has meant that Montreux remains in force without further amendments.

The safety of vessels passing through the Bosporus has become a major concern in recent years as the volume of traffic has increased greatly since the Convention was signed: from 4,500 in 1934 to 49,304 by 1998. As well as obvious environmental concerns, the Straits bisect the city of Istanbul, with over 14 million people living on its shores and so maritime incidents in the Straits pose a considerable risk to public safety. The Convention does not, however, make any provision for the regulation of shipping for the purposes of safety and environmental protection. In January 1994, the Turkish government adopted new "Maritime Traffic Regulations for the Turkish Straits and the Marmara Region". That introduced a new regulatory regime "to ensure the safety of navigation, life and property and to protect the environment in the region" but without violating the Montreux principle of free passage. The new regulations provoked some controversy when Russia, Greece, Cyprus, Romania, Ukraine and Bulgaria raised objections. However, they were approved by the International Maritime Organization on the grounds that they were not intended to prejudice "the rights of any ship using the Straits under international law". The regulations were revised in November 1998 to address Russian concerns.



</doc>
<doc id="20455" url="https://en.wikipedia.org/wiki?curid=20455" title="Michael Jordan">
Michael Jordan

Michael Jeffrey Jordan (born February 17, 1963), also known by his initials MJ, is an American former professional basketball player who is the principal owner of the Charlotte Hornets of the National Basketball Association (NBA). He played 15 seasons in the NBA, winning six championships with the Chicago Bulls. His biography on the official NBA website states: "By acclamation, Michael Jordan is the greatest basketball player of all time." He was integral in helping to popularize the NBA around the world in the 1980s and 1990s, becoming an American and global cultural icon in the process.

Jordan played college basketball for three seasons under coach Dean Smith with the North Carolina Tar Heels. As a freshman, he was a member of the Tar Heels' national championship team in 1982. Jordan joined the Bulls in 1984 as the third overall draft pick, and quickly emerged as a league star, entertaining crowds with his prolific scoring while gaining a reputation as one of the game's best defensive players. His leaping ability, demonstrated by performing slam dunks from the free throw line in Slam Dunk Contests, earned him the nicknames "Air Jordan" and "His Airness". Jordan won his first NBA championship with the Bulls in 1991, and followed that achievement with titles in 1992 and 1993, securing a "three-peat". Jordan abruptly retired from basketball before the 1993–94 NBA season to play Minor League Baseball, but returned to the Bulls in March 1995 and led them to three more championships in 1996, 1997, and 1998, as well as a then-record 72 regular-season wins in the 1995–96 NBA season. He retired for a second time in January 1999 but returned for two more NBA seasons from 2001 to 2003 as a member of the Washington Wizards.

Jordan's individual accolades and accomplishments include six NBA Finals Most Valuable Player (MVP) Awards, ten scoring titles (both all-time records), five MVP Awards, ten All-NBA First Team designations, nine All-Defensive First Team honors, fourteen NBA All-Star Game selections, three All-Star Game MVP Awards, three steals titles, and the 1988 NBA Defensive Player of the Year Award. He holds the NBA records for career regular season scoring average (30.12 points per game) and career playoff scoring average (33.45 points per game). In 1999, he was named the 20th century's greatest North American athlete by ESPN, and was second to Babe Ruth on the Associated Press' list of athletes of the century. Jordan was twice inducted into the Naismith Memorial Basketball Hall of Fame, once in 2009 for his individual career and again in 2010 as part of the 1992 United States men's Olympic basketball team ("The Dream Team"). He became a member of the FIBA Hall of Fame in 2015.

One of the most effectively marketed athletes of his generation, Jordan is also known for his product endorsements. He fueled the success of Nike's Air Jordan sneakers which were introduced in 1984 and remain popular today. Jordan also starred as himself in the 1996 film "Space Jam". He became part-owner and head of basketball operations for the Charlotte Bobcats (now Hornets) in 2006, and bought a controlling interest in 2010. In 2014, Jordan became the first billionaire player in NBA history. With a net worth of $2.1 billion, he is the fourth-richest African American, behind Robert F. Smith, David Steward, and Oprah Winfrey.

Michael Jeffrey Jordan was born at Cumberland Hospital in the Fort Greene neighborhood of New York City's Brooklyn borough on February 17, 1963, the son of bank employee Deloris (née Peoples) and equipment supervisor James R. Jordan Sr. In 1968, he moved with his family to Wilmington, North Carolina. Jordan attended Emsley A. Laney High School in Wilmington, where he highlighted his athletic career by playing basketball, baseball, and football. He tried out for the varsity basketball team during his sophomore year but, at 5'11" (1.80 m), he was deemed too short to play at that level. His taller friend, Harvest Leroy Smith, was the only sophomore to make the team.

Motivated to prove his worth, Jordan became the star of Laney's junior varsity team, and tallied several 40-point games. The following summer, he grew four inches (10 cm) and trained rigorously. Upon earning a spot on the varsity roster, Jordan averaged more than 25 points per game (ppg) over his final two seasons of high school play. As a senior, he was selected to play in the 1981 McDonald's All-American Game and scored 30 points, after averaging 27 points, 12 rebounds, and 6 assists per game for the season. Jordan was recruited by numerous college basketball programs, including Duke, North Carolina, South Carolina, Syracuse, and Virginia. In 1981, he accepted a basketball scholarship to the University of North Carolina at Chapel Hill, where he majored in cultural geography.

As a freshman in coach Dean Smith's team-oriented system, he was named ACC Freshman of the Year after he averaged 13.4 ppg on 53.4% shooting (field goal percentage). He made the game-winning jump shot in the 1982 NCAA Championship game against Georgetown, which was led by future NBA rival Patrick Ewing. Jordan later described this shot as the major turning point in his basketball career. During his three seasons with the Tar Heels, he averaged 17.7 ppg on 54.0% shooting, and added 5.0 rpg. He was selected by consensus to the NCAA All-American First Team in both his sophomore (1983) and junior (1984) seasons. After winning the Naismith and the Wooden College Player of the Year awards in 1984, Jordan left North Carolina one year before his scheduled graduation to enter the 1984 NBA draft.

Jordan returned to North Carolina to complete his degree in 1986, when he graduated with a Bachelor of Arts degree in geography.

The Chicago Bulls selected Jordan with the third overall pick of the 1984 NBA draft, after Hakeem Olajuwon (Houston Rockets) and Sam Bowie (Portland Trail Blazers). One of the primary reasons why Jordan was not drafted sooner was because the first two teams were in need of a center. However, Trail Blazers general manager Stu Inman contended that it was not a matter of drafting a center, but more a matter of taking Sam Bowie over Jordan, in part because Portland already had Clyde Drexler, who was a guard with similar skills to Jordan. ESPN, citing Bowie's injury-laden college career, named the Blazers' choice of Bowie as the worst draft pick in North American professional sports history.

During his rookie season with the Bulls, Jordan averaged 28.2 ppg on 51.5% shooting, and helped make a team that had won 35% of games in the previous three seasons playoff contenders. He quickly became a fan favorite even in opposing arenas. Roy S. Johnson of "The New York Times" described him as "the phenomenal rookie of the Bulls" in November, and Jordan appeared on the cover of "Sports Illustrated" with the heading "A Star Is Born" in December. The fans also voted in Jordan as an All-Star starter during his rookie season. Controversy arose before the All-Star game when word surfaced that several veteran players—led by Isiah Thomas—were upset by the amount of attention Jordan was receiving. This led to a so-called "freeze-out" on Jordan, where players refused to pass the ball to him throughout the game. The controversy left Jordan relatively unaffected when he returned to regular season play, and he would go on to be voted Rookie of the Year. The Bulls finished the season 38–44 and lost to the Milwaukee Bucks in four games in the first round of the playoffs.

Jordan's second season was cut short when he broke his foot in the third game of the year, causing him to miss 64 games. The Bulls made the playoffs despite Jordan's injury and a 30–52 record, at the time the fifth worst record of any team to qualify for the playoffs in NBA history. Jordan recovered in time to participate in the postseason and performed well upon his return. Against a 1985–86 Boston Celtics team that is often considered one of the greatest in NBA history, Jordan set the still-unbroken record for points in a playoff game with 63 in Game 2. The Celtics, however, managed to sweep the series.

Jordan had completely recovered in time for the 1986–87 season, and he had one of the most prolific scoring seasons in NBA history. He became the only player other than Wilt Chamberlain to score 3,000 points in a season, averaging a league high 37.1 points on 48.2% shooting. In addition, Jordan demonstrated his defensive prowess, as he became the first player in NBA history to record 200 steals and 100 blocked shots in a season. Despite Jordan's success, Magic Johnson won the league's Most Valuable Player Award. The Bulls reached 40 wins, and advanced to the playoffs for the third consecutive year. However, they were again swept by the Celtics.

Jordan again led the league in scoring during the 1987–88 season, averaging 35.0 ppg on 53.5% shooting and won his first league MVP Award. He was also named the Defensive Player of the Year, as he had averaged 1.6 blocks and a league high 3.16 steals per game. The Bulls finished 50–32, and made it out of the first round of the playoffs for the first time in Jordan's career, as they defeated the Cleveland Cavaliers in five games. However, the Bulls then lost in five games to the more experienced Detroit Pistons, who were led by Isiah Thomas and a group of physical players known as the "".

In the 1988–89 season, Jordan again led the league in scoring, averaging 32.5 ppg on 53.8% shooting from the field, along with 8 rpg and 8 apg. The Bulls finished with a 47–35 record, and advanced to the Eastern Conference Finals, defeating the Cavaliers and New York Knicks along the way. The Cavaliers series included a career highlight for Jordan when he hit "The Shot" over Craig Ehlo at the buzzer in the fifth and final game of the series. However, the Pistons again defeated the Bulls, this time in six games, by utilizing their "Jordan Rules" method of guarding Jordan, which consisted of double and triple teaming him every time he touched the ball.

The Bulls entered the 1989–90 season as a team on the rise, with their core group of Jordan and young improving players like Scottie Pippen and Horace Grant, and under the guidance of new coach Phil Jackson. On March 28, 1990, Jordan scored a career-high 69 points in a 117–113 road win over the Cavaliers. He averaged a league leading 33.6 ppg on 52.6% shooting, to go with 6.9 rpg and 6.3 apg in leading the Bulls to a 55–27 record. They again advanced to the Eastern Conference Finals after beating the Bucks and Philadelphia 76ers. However, despite pushing the series to seven games, the Bulls lost to the Pistons for the third consecutive season.

In the 1990–91 season, Jordan won his second MVP award after averaging 31.5 ppg on 53.9% shooting, 6.0 rpg, and 5.5 apg for the regular season. The Bulls finished in first place in their division for the first time in 16 years and set a franchise record with 61 wins in the regular season. With Scottie Pippen developing into an All-Star, the Bulls had elevated their play. The Bulls defeated the New York Knicks and the Philadelphia 76ers in the opening two rounds of the playoffs. They advanced to the Eastern Conference Finals where their rival, the Detroit Pistons, awaited them. However, this time the Bulls beat the Pistons in a four-game sweep.

The Bulls advanced to the NBA Finals for the first time in franchise history to face the Los Angeles Lakers, who had Magic Johnson and James Worthy, two formidable opponents. The Bulls won the series four games to one, and compiled a 15–2 playoff record along the way. Perhaps the best known moment of the series came in Game 2 when, attempting a dunk, Jordan avoided a potential Sam Perkins block by switching the ball from his right hand to his left in mid-air to lay the shot into the basket. In his first Finals appearance, Jordan posted per game averages of 31.2 points on 56% shooting from the field, 11.4 assists, 6.6 rebounds, 2.8 steals, and 1.4 blocks. Jordan won his first NBA Finals MVP award, and he cried while holding the NBA Finals trophy.

Jordan and the Bulls continued their dominance in the 1991–92 season, establishing a 67–15 record, topping their franchise record from 1990–91. Jordan won his second consecutive MVP award with averages of 30.1 points, 6.4 rebounds and 6.1 assists per game on 52% shooting. After winning a physical 7-game series over the New York Knicks in the second round of the playoffs and finishing off the Cleveland Cavaliers in the Conference Finals in 6 games, the Bulls met Clyde Drexler and the Portland Trail Blazers in the Finals. The media, hoping to recreate a Magic–Bird rivalry, highlighted the similarities between "Air" Jordan and Clyde "The Glide" during the pre-Finals hype. In the first game, Jordan scored a Finals-record 35 points in the first half, including a record-setting six three-point field goals. After the sixth three-pointer, he jogged down the court shrugging as he looked courtside. Marv Albert, who broadcast the game, later stated that it was as if Jordan was saying, "I can't believe I'm doing this." The Bulls went on to win Game 1, and defeat the Blazers in six games. Jordan was named Finals MVP for the second year in a row and finished the series averaging 35.8 ppg, 4.8 rpg, and 6.5 apg, while shooting 53% from the floor.

In the 1992–93 season, despite a 32.6 ppg, 6.7 rpg, and 5.5 apg campaign and a second-place finish in Defensive Player of the Year voting, Jordan's streak of consecutive MVP seasons ended as he lost the award to his friend Charles Barkley. Coincidentally, Jordan and the Bulls met Barkley and his Phoenix Suns in the 1993 NBA Finals. The Bulls won their third NBA championship on a game-winning shot by John Paxson and a last-second block by Horace Grant, but Jordan was once again Chicago's leader. He averaged a Finals-record 41.0 ppg during the six-game series, and became the first player in NBA history to win three straight Finals MVP awards. He scored more than 30 points in every game of the series, including 40 or more points in 4 consecutive games. With his third Finals triumph, Jordan capped off a seven-year run where he attained seven scoring titles and three championships, but there were signs that Jordan was tiring of his massive celebrity and all of the non-basketball hassles in his life.

During the Bulls' playoff run in 1993, Jordan was seen gambling in Atlantic City, New Jersey, the night before a game against the New York Knicks. The previous year, he admitted that he had to cover $57,000 in gambling losses, and author Richard Esquinas wrote a book in 1993 claiming he had won $1.25 million from Jordan on the golf course. NBA Commissioner David Stern denied in 1995 and 2006 that Jordan's 1993 retirement was a secret suspension by the league for gambling, but the rumor spread widely.

In 2005, Jordan discussed his gambling with Ed Bradley of "60 Minutes" and admitted that he made reckless decisions. Jordan stated, "Yeah, I've gotten myself into situations where I would not walk away and I've pushed the envelope. Is that compulsive? Yeah, it depends on how you look at it. If you're willing to jeopardize your livelihood and your family, then yeah". When Bradley asked him if his gambling ever got to the level where it jeopardized his livelihood or family, Jordan replied, "No". In 2010 Ron Shelton, director of "Jordan Rides the Bus", said that he began working on the documentary believing that the NBA had suspended him, but that research "convinced [him it] was nonsense".

On October 6, 1993, Jordan announced his retirement, saying that he had lost his desire to play basketball. Jordan later said that the death of his father three months earlier had helped shape his decision. James Jordan was murdered on July 23, 1993, at a highway rest area in Lumberton, North Carolina, by two teenagers, Daniel Green and Larry Martin Demery, who carjacked his Lexus bearing the license plate "UNC 0023". His body, dumped in a South Carolina swamp, was not discovered until August 3. Green and Demery were found after they made calls on James Jordan's cell phone, convicted at trial, and sentenced to life in prison. Jordan was close to his father; as a child he had imitated the way his father stuck out his tongue while absorbed in work. He later adopted it as his own signature, often displaying it as he drove to the basket. In 1996, he founded a Chicago-area Boys & Girls Club and dedicated it to his father.

In his 1998 autobiography "For the Love of the Game", Jordan wrote that he had been preparing for retirement as early as the summer of 1992. The added exhaustion due to the Dream Team run in the 1992 Olympics solidified Jordan's feelings about the game and his ever-growing celebrity status. Jordan's announcement sent shock waves throughout the NBA and appeared on the front pages of newspapers around the world.

Jordan further surprised the sports world by signing a Minor League Baseball contract with the Chicago White Sox on February 7, 1994. He reported to spring training in Sarasota, Florida, and was assigned to the team's minor league system on March 31, 1994. Jordan has said this decision was made to pursue the dream of his late father, who had always envisioned his son as a Major League Baseball player. The White Sox were owned by Bulls owner Jerry Reinsdorf, who continued to honor Jordan's basketball contract during the years he played baseball.

In 1994, Jordan played for the Birmingham Barons, a Double-A minor league affiliate of the Chicago White Sox, batting .202 with three home runs, 51 runs batted in, 30 stolen bases, 114 strikeouts, 51 bases on balls, and 11 errors. He also appeared for the Scottsdale Scorpions in the 1994 Arizona Fall League, batting .252 against the top prospects in baseball. On November 1, 1994, his number 23 was retired by the Bulls in a ceremony that included the erection of a permanent sculpture known as "The Spirit" outside the new United Center.

In the 1993–94 season, the Bulls achieved a 55–27 record without Jordan in the lineup, and lost to the New York Knicks in the second round of the playoffs. The 1994–95 Bulls were a shell of the championship team of just two years earlier. Struggling at mid-season to ensure a spot in the playoffs, Chicago was 31–31 at one point in mid-March. The team received help, however, when Jordan decided to return to the Bulls.

In March 1995, Jordan decided to quit baseball because he feared he might become a replacement player during the Major League Baseball strike. On March 18, 1995, Jordan announced his return to the NBA through a two-word press release: "I'm back." The next day, Jordan took to the court with the Bulls to face the Indiana Pacers in Indianapolis, scoring 19 points. The game had the highest Nielsen rating of any regular season NBA game since 1975. Although he could have worn his normal number even though the Bulls retired it, Jordan wore number 45, his baseball number.

Despite his 18-month hiatus from the NBA, Jordan played well, making a game-winning jump shot against Atlanta in his fourth game back. He scored 55 points in his next game, against the Knicks at Madison Square Garden on March 28, 1995. Boosted by Jordan's comeback, the Bulls went 13–4 to make the playoffs and advanced to the Eastern Conference Semifinals against the Orlando Magic. At the end of Game 1, Orlando's Nick Anderson stripped Jordan from behind, leading to the game-winning basket for the Magic; he would later comment that Jordan "didn't look like the old Michael Jordan" and that "No. 45 doesn't explode like No. 23 used to."

Jordan responded by scoring 38 points in the next game, which Chicago won. Before the game, Jordan decided that he would immediately resume wearing his former number, 23. The Bulls were fined $25,000 for failing to report the impromptu number change to the NBA. Jordan was fined an additional $5,000 for opting to wear white sneakers when the rest of the Bulls wore black. He averaged 31 points per game in the series, but Orlando won the series in six games.

Jordan was freshly motivated by the playoff defeat, and he trained aggressively for the 1995–96 season. The Bulls were strengthened by the addition of rebound specialist Dennis Rodman, and the team dominated the league, starting the season at 41–3. The Bulls eventually finished with the best regular season record in NBA history, 72–10, a mark broken two decades later by the 2015–16 Golden State Warriors. Jordan led the league in scoring with 30.4 ppg and won the league's regular-season and All-Star Game MVP awards.

In the playoffs, the Bulls lost only three games in four series (Miami Heat 3–0, New York Knicks 4–1, Orlando Magic 4–0). They defeated the Seattle SuperSonics 4–2 in the NBA Finals to win their fourth championship. Jordan was named Finals MVP for a record fourth time, surpassing Magic Johnson's three Finals MVP awards. He also achieved only the second sweep of the MVP Awards in the All-Star Game, regular season, and NBA Finals, after Willis Reed in the 1969–70 season. Upon winning the championship, his first since his father's murder, Jordan reacted emotionally, clutching the game ball and crying on the locker room floor.

In the 1996–97 season, the Bulls started out 69–11, but missed out on a second consecutive 70-win season by losing their final two games to finish 69–13. However, Jordan was beaten for the NBA MVP Award by Karl Malone. The Bulls again advanced to the Finals, where they faced Malone's Utah Jazz. The series against the Jazz featured two of the more memorable clutch moments of Jordan's career. He won Game 1 for the Bulls with a buzzer-beating jump shot. In Game 5, with the series tied at 2, Jordan played despite being feverish and dehydrated from a stomach virus. In what is known as the "Flu Game", Jordan scored 38 points, including the game-deciding 3-pointer with 25 seconds remaining. The Bulls won 90–88 and went on to win the series in six games. For the fifth time in as many Finals appearances, Jordan received the Finals MVP award. During the 1997 NBA All-Star Game, Jordan posted the first triple double in All-Star Game history in a victorious effort; however, he did not receive the MVP award.

Jordan and the Bulls compiled a 62–20 record in the 1997–98 season. Jordan led the league with 28.7 points per game, securing his fifth regular-season MVP award, plus honors for All-NBA First Team, First Defensive Team and the All-Star Game MVP. The Bulls won the Eastern Conference Championship for a third straight season, including surviving a seven-game series with the Indiana Pacers in the Eastern Conference Finals; it was the first time Jordan had played in a Game 7 since the 1992 Eastern Conference Semifinals with the Knicks. After winning, they moved on for a rematch with the Jazz in the Finals.

The Bulls returned to the Delta Center for Game 6 on June 14, 1998, leading the series 3–2. Jordan executed a series of plays, considered to be one of the greatest clutch performances in NBA Finals history. With 41.9 seconds remaining and the Bulls trailing 86–83, Phil Jackson called a timeout. When play resumed, Jordan received the inbound pass, drove to the basket, and sank a shot over several Jazz defenders, cutting Utah's lead to 86–85. The Jazz brought the ball upcourt and passed the ball to Malone, who was set up in the low post and was being guarded by Rodman. Malone jostled with Rodman and caught the pass, but Jordan cut behind him and stole the ball out of his hands. Jordan then dribbled down the court and paused, eyeing his defender, Jazz guard Bryon Russell. With 10 seconds remaining, Jordan started to dribble right, then crossed over to his left, possibly pushing off Russell, although the officials did not call a foul. With 5.2 seconds left, Jordan made the climactic shot of his Bulls career. He gave Chicago an 87–86 lead with a jumper over Russell. Afterwards, the Jazz' John Stockton narrowly missed a game-winning three-pointer. The buzzer sounded, and Jordan and the Bulls won their sixth NBA championship, achieving a second three-peat in the decade. Once again, Jordan was voted Finals MVP, having led all scorers by averaging 33.5 points per game, including 45 in the deciding Game 6. Jordan's six Finals MVPs is a record; Shaquille O'Neal, Magic Johnson, LeBron James, and Tim Duncan are tied for second place with three apiece. The 1998 Finals holds the highest television rating of any Finals series in history. Game 6 also holds the highest television rating of any game in NBA history.

With Phil Jackson's contract expiring, the pending departures of Scottie Pippen and Dennis Rodman looming, and being in the latter stages of an owner-induced lockout of NBA players, Jordan retired for the second time on January 13, 1999. On January 19, 2000, Jordan returned to the NBA not as a player, but as part owner and president of basketball operations for the Washington Wizards. Jordan's responsibilities with the Wizards were comprehensive. He controlled all aspects of the Wizards' basketball operations, and had the final say in all personnel matters. Opinions of Jordan as a basketball executive were mixed. He managed to purge the team of several highly paid, unpopular players (such as forward Juwan Howard and point guard Rod Strickland), but used the first pick in the 2001 NBA draft to select high schooler Kwame Brown, who did not live up to expectations and was traded away after four seasons.

Despite his January 1999 claim that he was "99.9% certain" that he would never play another NBA game, in the summer of 2001 Jordan expressed interest in making another comeback, this time with his new team. Inspired by the NHL comeback of his friend Mario Lemieux the previous winter, Jordan spent much of the spring and summer of 2001 in training, holding several invitation-only camps for NBA players in Chicago. In addition, Jordan hired his old Chicago Bulls head coach, Doug Collins, as Washington's coach for the upcoming season, a decision that many saw as foreshadowing another Jordan return.

On September 25, 2001, Jordan announced his return to the NBA to play for the Washington Wizards, indicating his intention to donate his salary as a player to a relief effort for the victims of the September 11 attacks. In an injury-plagued 2001–02 season, he led the team in scoring (22.9 ppg), assists (5.2 apg), and steals (1.42 spg). However, torn cartilage in his right knee ended Jordan's season after only 60 games, the fewest he had played in a regular season since playing 17 games after returning from his first retirement during the 1994–95 season. Jordan started 53 of his 60 games for the season, averaging 24.3 points, 5.4 assists, and 6.0 rebounds, and shooting 41.9% from the field in his 53 starts. His last seven appearances were in a reserve role, in which he averaged just over 20 minutes per game.

Playing in his 14th and final NBA All-Star Game in 2003, Jordan passed Kareem Abdul-Jabbar as the all-time leading scorer in All-Star Game history (a record since broken by LeBron James and Kobe Bryant). That year, Jordan was the only Washington player to play in all 82 games, starting in 67 of them. He averaged 20.0 points, 6.1 rebounds, 3.8 assists, and 1.5 steals per game. He also shot 45% from the field, and 82% from the free throw line. Even though he turned 40 during the season, he scored 20 or more points 42 times, 30 or more points nine times, and 40 or more points three times. On February 21, 2003, Jordan became the first 40-year-old to tally 43 points in an NBA game. During his stint with the Wizards, all of Jordan's home games at the MCI Center were sold out, and the Wizards were the second most-watched team in the NBA, averaging 20,172 fans a game at home and 19,311 on the road. However, neither of Jordan's final two seasons resulted in a playoff appearance for the Wizards, and Jordan was often unsatisfied with the play of those around him. At several points he openly criticized his teammates to the media, citing their lack of focus and intensity, notably that of the number one draft pick in the 2001 NBA draft, Kwame Brown.

With the recognition that 2002–03 would be Jordan's final season, tributes were paid to him throughout the NBA. In his final game at the United Center in Chicago, which was his old home court, Jordan received a four-minute standing ovation. The Miami Heat retired the number 23 jersey on April 11, 2003, even though Jordan never played for the team. At the 2003 All-Star Game, Jordan was offered a starting spot from Tracy McGrady and Allen Iverson, but refused both. In the end, he accepted the spot of Vince Carter.

Jordan played in his final NBA game on April 16, 2003, in Philadelphia. After scoring only 13 points in the game, Jordan went to the bench with 4 minutes and 13 seconds remaining in the third quarter and his team trailing the Philadelphia 76ers, 75–56. Just after the start of the fourth quarter, the First Union Center crowd began chanting "We want Mike!" After much encouragement from coach Doug Collins, Jordan finally rose from the bench and re-entered the game, replacing Larry Hughes with 2:35 remaining. At 1:45, Jordan was intentionally fouled by the 76ers' Eric Snow, and stepped to the line to make both free throws. After the second foul shot, the 76ers in-bounded the ball to rookie John Salmons, who in turn was intentionally fouled by Bobby Simmons one second later, stopping time so that Jordan could return to the bench. Jordan received a three-minute standing ovation from his teammates, his opponents, the officials, and the crowd of 21,257 fans.

Jordan played on two Olympic gold medal-winning American basketball teams. He won a gold medal as a college player in the 1984 Summer Olympics. The team was coached by Bob Knight and featured players such as Patrick Ewing, Sam Perkins, Chris Mullin, Steve Alford, and Wayman Tisdale. Jordan led the team in scoring, averaging 17.1 ppg for the tournament.

In the 1992 Summer Olympics, he was a member of the star-studded squad that included Magic Johnson and Larry Bird that was dubbed the "Dream Team". Jordan was the only player to start all eight games in the Olympics. Playing limited minutes due to the frequent blowouts, Jordan averaged 14.9 ppg, finishing second on the team in scoring. Jordan and fellow Dream Team members Ewing and Mullin are the only American men's basketball players to win Olympic gold medals as amateurs and professionals.

After his third retirement, Jordan assumed that he would be able to return to his front office position as Director of Basketball Operations with the Wizards. However, his previous tenure in the Wizards' front office had produced the aforementioned mixed results and may have also influenced the trade of Richard "Rip" Hamilton for Jerry Stackhouse (although Jordan was not technically Director of Basketball Operations in 2002). On May 7, 2003, Wizards owner Abe Pollin fired Jordan as the team's president of basketball operations. Jordan later stated that he felt betrayed, and that if he had known he would be fired upon retiring he never would have come back to play for the Wizards.

Jordan kept busy over the next few years. He stayed in shape, played golf in celebrity charity tournaments, and spent time with his family in Chicago. He also promoted his Jordan Brand clothing line and rode motorcycles. Since 2004, Jordan has owned Michael Jordan Motorsports, a professional closed-course motorcycle road racing team that competed with two Suzukis in the premier Superbike championship sanctioned by the American Motorcyclist Association (AMA) until the end of the 2013 season.

On June 15, 2006, Jordan bought a minority stake in the Charlotte Bobcats (now known as the Hornets), becoming the team's second-largest shareholder behind majority owner Robert L. Johnson. As part of the deal, Jordan took full control over the basketball side of the operation, with the title "Managing Member of Basketball Operations". Despite Jordan's previous success as an endorser, he has made an effort not to be included in Charlotte's marketing campaigns. A decade earlier, Jordan had made a bid to become part-owner of Charlotte's original NBA team, the Charlotte Hornets, but talks collapsed when owner George Shinn refused to give Jordan complete control of basketball operations.

In February 2010, it was reported that Jordan was seeking majority ownership of the Bobcats. As February wore on, it became apparent that Jordan and former Houston Rockets president George Postolos were the leading contenders for ownership of the team. On February 27, the Bobcats announced that Johnson had reached an agreement with Jordan and his group, MJ Basketball Holdings, to buy the team pending NBA approval. On March 17, the NBA Board of Governors unanimously approved Jordan's purchase, making him the first former player to become the majority owner of an NBA team. It also made him the league's only African-American majority owner of an NBA team.

During the 2011 NBA lockout, "The New York Times" wrote that Jordan led a group of 10 to 14 hardline owners who wanted to cap the players' share of basketball-related income at 50 percent and as low as 47. Journalists observed that, during the labor dispute in 1998, Jordan had told Washington Wizards then-owner Abe Pollin, "If you can't make a profit, you should sell your team." Jason Whitlock of FoxSports.com called Jordan a "sellout" wanting "current players to pay for his incompetence." He cited Jordan's executive decisions to draft disappointing players Kwame Brown and Adam Morrison.

During the 2011–12 NBA season that was shortened to 66 games by the lockout, the Bobcats posted a 7–59 record. The team closed out the season with a 23-game losing streak. Their .106 winning percentage was the worst in NBA history. "I'm not real happy about the record book scenario last year. It's very, very frustrating," Jordan said before the next season.

During the 2019 NBA offseason, Jordan sold a minority piece of the Hornets to Gabe Plotkin and Daniel Sundheim, retaining the majority of the team for himself.

Jordan was a shooting guard who could also play as a small forward (the position he would primarily play during his second return to professional basketball with the Washington Wizards), and as a point guard. Jordan was known throughout his career as a strong clutch performer. With the Bulls, he decided 25 games with field goals or free throws in the last 30 seconds, including two NBA Finals games and five other playoff contests. His competitiveness was visible in his prolific trash-talk and well-known work ethic. Jordan often used perceived slights to fuel his performances. Sportswriter Wright Thompson described him as "a killer, in the Darwinian sense of the word, immediately sensing and attacking someone's weakest spot." As the Bulls organization built the franchise around Jordan, management had to trade away players who were not "tough enough" to compete with him in practice. To help improve his defense, he spent extra hours studying film of opponents. On offense, he relied more upon instinct and improvisation at game time.

Noted as a durable player, Jordan did not miss four or more games while active for a full season from 1986–87 to 2001–02, when he injured his right knee. Of the 15 seasons Jordan was in the NBA, he played all 82 regular season games nine times. Jordan has frequently cited David Thompson, Walter Davis, and Jerry West as influences. Confirmed at the start of his career, and possibly later on, Jordan had a special "Love of the Game Clause" written into his contract (unusual at the time) which allowed him to play basketball against anyone at any time, anywhere.

Jordan had a versatile offensive game. He was capable of aggressively driving to the basket, as well as drawing fouls from his opponents at a high rate; his 8,772 free throw attempts are the 11th-highest total in NBA history. As his career progressed, Jordan also developed the ability to post up his opponents and score with his trademark fadeaway jump shot, using his leaping ability to "fade away" from block attempts. According to Hubie Brown, this move alone made him nearly unstoppable. Despite media criticism as a "selfish" player early in his career, Jordan's 5.3 assists per game also indicate his willingness to defer to his teammates. After shooting under 30% from three-point range in his first five seasons in the NBA, including a career-low 13% in the season, Jordan improved to a career-high 50% in the season. The three-point shot became more of a focus of his game from 1994–95 to 1996–97, when the NBA shortened its three-point line from to . His three-point field-goal percentages ranged from 35% to 43% in seasons in which he attempted at least 230 three-pointers between 1989–90 and 1996–97. For a guard, Jordan was also a good rebounder (6.2 per game).

In 1988, Jordan was honored with the NBA's Defensive Player of the Year Award and became the first NBA player to win both the Defensive Player of the Year and Most Valuable Player awards in a career (since equaled by Hakeem Olajuwon, David Robinson, and Kevin Garnett; Olajuwon is the only player other than Jordan to win both during the same season). In addition, he set both seasonal and career records for blocked shots by a guard, and combined this with his ball-thieving ability to become a standout defensive player. He ranks third in NBA history in total steals with 2,514, trailing John Stockton and Jason Kidd. Jerry West often stated that he was more impressed with Jordan's defensive contributions than his offensive ones. He was also known to have strong eyesight; broadcaster Al Michaels said that he was able to read baseball box scores on a 27-inch (69 cm) television clearly from about 50 feet (15 m) away. During the 2001 NBA Finals, Phil Jackson compared Jordan's dominance to Shaquille O'Neal, stating "Michael would get fouled on every play and still have to play through it and just clear himself for shots instead and would rise to that occasion."

Source:

Jordan's talent was clear from his first NBA season; by November he was being compared to Julius Erving. Larry Bird said of the rookie Jordan that he was the best player he had ever seen and that Jordan was "one of a kind" and comparable to Wayne Gretzky as an athlete. In his first game in Madison Square Garden against the New York Knicks, Jordan received a standing ovation of almost one minute. After he scored a playoff record 63 points against the Boston Celtics on April 20, 1986, Bird described him as "God disguised as Michael Jordan".

Jordan led the NBA in scoring in 10 seasons (NBA record) and tied Wilt Chamberlain's record of seven consecutive scoring titles. He was also a fixture on the NBA All-Defensive First Team, making the roster nine times (NBA record shared with Gary Payton, Kevin Garnett and Kobe Bryant). Jordan also holds the top career regular season and playoff scoring averages of 30.1 and 33.4 points per game, respectively. By 1998, the season of his Finals-winning shot against the Jazz, he was well known throughout the league as a clutch performer. In the regular season, Jordan was the Bulls' primary threat in the final seconds of a close game and in the playoffs; he would always ask for the ball at crunch time. Jordan's total of 5,987 points in the playoffs is the second-highest in NBA history. He retired with 32,292 points in regular season play, placing him fifth on the NBA's all-time scoring list behind Kareem Abdul-Jabbar, Karl Malone, Kobe Bryant, and LeBron James.

With five regular-season MVPs (tied for second place with Bill Russell—only Kareem Abdul-Jabbar has won more, with six), six Finals MVPs (NBA record), and three All-Star Game MVPs, Jordan is the most decorated player in NBA history. Jordan finished among the top three in regular-season MVP voting 10 times, and was named one of the 50 Greatest Players in NBA History in 1996. He is one of only seven players in history to win an NCAA championship, an NBA championship, and an Olympic gold medal (doing so twice with the 1984 and 1992 U.S. men's basketball teams). Since 1976, the year of the NBA's merger with the American Basketball Association, Jordan and Pippen are the only two players to win six NBA Finals playing for one team. In the All-Star Game fan ballot, Jordan received the most votes nine times, more than any other player.
Many of Jordan's contemporaries have said that Jordan is the greatest basketball player of all time. In 1999, an ESPN survey of journalists, athletes and other sports figures ranked Jordan the greatest North American athlete of the 20th century, above such luminaries as Babe Ruth and Muhammad Ali. Jordan placed second to Babe Ruth in the Associated Press' December 1999 list of 20th century athletes. In addition, the Associated Press voted him the greatest basketball player of the 20th century. Jordan has also appeared on the front cover of "Sports Illustrated" a record 50 times. In the September 1996 issue of "Sport", which was the publication's 50th-anniversary issue, Jordan was named the greatest athlete of the past 50 years.

Jordan's athletic leaping ability, highlighted in his back-to-back Slam Dunk Contest championships in 1987 and 1988, is credited by many people with having influenced a generation of young players. Several current NBA players—including LeBron James and Dwyane Wade—have stated that they considered Jordan their role model while they were growing up. In addition, commentators have dubbed a number of next-generation players "the next Michael Jordan" upon their entry to the NBA, including Penny Hardaway, Grant Hill, Allen Iverson, Bryant, James, Vince Carter, and Dwyane Wade. Although Jordan was a well-rounded player, his "Air Jordan" image is also often credited with inadvertently decreasing the jump shooting skills, defense, and fundamentals of young players, a fact Jordan himself has lamented. 

During his heyday, Jordan did much to increase the status of the game. Television ratings increased only during his time in the league. The popularity of the NBA in the U.S. declined after his last title. As late as 2015, Finals ratings had not returned to the level reached during his last championship-winning season.

In August 2009, the Naismith Memorial Basketball Hall of Fame in Springfield, Massachusetts, opened a Michael Jordan exhibit that contained items from his college and NBA careers, as well as from the 1992 "Dream Team". The exhibit also has a batting glove to signify Jordan's short career in Minor League Baseball. After Jordan received word of his acceptance into the Hall of Fame, he selected Class of 1996 member David Thompson to present him. As Jordan would later explain during his induction speech in September 2009, when he was growing up in North Carolina, he was not a fan of the Tar Heels and greatly admired Thompson, who played at rival North Carolina State. In September, he was inducted into the Hall with several former Bulls teammates in attendance, including Scottie Pippen, Dennis Rodman, Charles Oakley, Ron Harper, Steve Kerr, and Toni Kukoč. Two of Jordan's former coaches, Dean Smith and Doug Collins, were also among those present. His emotional reaction during his speech—when he began to cry—was captured by Associated Press photographer Stephan Savoia and would later go viral on social media as the Crying Jordan Internet meme. In 2016, President Barack Obama honored Jordan with the Presidential Medal of Freedom.

Jordan is the fourth of five children. He has two older brothers, Larry Jordan and James R. Jordan Jr., one older sister, Deloris, and one younger sister, Roslyn. James retired in 2006 as the Command Sergeant Major of the 35th Signal Brigade of the XVIII Airborne Corps in the U.S. Army. Jordan's nephew through Larry, Justin, played Division I basketball at UNC Greensboro and is a scout for the Charlotte Hornets.

Jordan married Juanita Vanoy in September 1989. They had two sons, Jeffrey and Marcus, and a daughter, Jasmine. The Jordans filed for divorce on January 4, 2002, citing irreconcilable differences, but reconciled shortly thereafter. They again filed for divorce and were granted a final decree of dissolution of marriage on December 29, 2006, commenting that the decision was made "mutually and amicably". It is reported that Juanita received a $168 million settlement (equivalent to $ million in ), making it the largest celebrity divorce settlement on public record at the time.

In 1991, Jordan purchased a lot in Highland Park, Illinois, on which he planned to build a 56,000 square-foot (5,200 m) mansion. It was completed in 1995. He listed the mansion for sale in 2012. His two sons attended Loyola Academy, a private Catholic school in Wilmette, Illinois. Jeffrey graduated in 2007 and played his first collegiate basketball game for the University of Illinois on November 11, 2007. After two seasons, he left the Illinois basketball team in 2009. He later rejoined the team for a third season, then received a release to transfer to the University of Central Florida, where Marcus was attending. Marcus transferred to Whitney Young High School after his sophomore year at Loyola Academy and graduated in 2009. He began attending UCF in the fall of 2009, and played three seasons of basketball for the school. 

On July 21, 2006, a judge in Cook County, Illinois, determined that Jordan did not owe his alleged former lover Karla Knafel $5 million in a breach of contract claim. Jordan had allegedly paid Knafel $250,000 to keep their relationship a secret. Knafel claimed Jordan promised her $5 million for remaining silent and agreeing not to file a paternity suit after Knafel learned she was pregnant in 1991. A DNA test showed Jordan was not the father of the child.

Jordan proposed to his longtime girlfriend, Cuban-American model Yvette Prieto, on Christmas 2011, and they were married on April 27, 2013, at Bethesda-by-the-Sea Episcopal Church. It was announced on November 30, 2013, that the two were expecting their first child together. On February 11, 2014, Prieto gave birth to identical twin daughters named Victoria and Ysabel. Jordan became a grandfather in 2019 when his daughter Jasmine gave birth to a son, whose father is professional basketball player Rakeem Christmas.

Jordan is one of the most marketed sports figures in history. He has been a major spokesman for such brands as Nike, Coca-Cola, Chevrolet, Gatorade, McDonald's, Ball Park Franks, Rayovac, Wheaties, Hanes, and MCI. Jordan has had a long relationship with Gatorade, appearing in over 20 commercials for the company since 1991, including the "Be Like Mike" commercials in which a song was sung by children wishing to be like Jordan.

Nike created a signature shoe for Jordan, called the Air Jordan, in 1984. One of Jordan's more popular commercials for the shoe involved Spike Lee playing the part of Mars Blackmon. In the commercials Lee, as Blackmon, attempted to find the source of Jordan's abilities and became convinced that "it's gotta be the shoes". The hype and demand for the shoes even brought on a spate of "shoe-jackings" where people were robbed of their sneakers at gunpoint. Subsequently, Nike spun off the Jordan line into its own division named the "Jordan Brand". The company features an impressive list of athletes and celebrities as endorsers. The brand has also sponsored college sports programs such as those of North Carolina, California, Georgetown, and Marquette.

Jordan also has been associated with the Looney Tunes cartoon characters. A Nike commercial shown during 1992's Super Bowl XXVI featured Jordan and Bugs Bunny playing basketball. The Super Bowl commercial inspired the 1996 live action/animated film "Space Jam", which starred Jordan and Bugs in a fictional story set during the former's first retirement from basketball. They have subsequently appeared together in several commercials for MCI. Jordan also made an appearance in the music video for Michael Jackson's "Jam" (1992).

Jordan's yearly income from the endorsements is estimated to be over $40 million. In addition, when Jordan's power at the ticket gates was at its highest point, the Bulls regularly sold out both their home and road games. Due to this, Jordan set records in player salary by signing annual contracts worth in excess of US$30 million per season. An academic study found that Jordan's first NBA comeback resulted in an increase in the market capitalization of his client firms of more than $1 billion.

Most of Jordan's endorsement deals, including his first deal with Nike, were engineered by his agent, David Falk. Jordan has described Falk as "the best at what he does" and that "marketing-wise, he's great. He's the one who came up with the concept of 'Air Jordan.'"

In June 2010, Jordan was ranked by "Forbes" magazine as the 20th-most powerful celebrity in the world with $55 million earned between June 2009 and June 2010. According to the "Forbes" article, Jordan Brand generates $1 billion in sales for Nike. In June 2014, Jordan was named the first NBA player to become a billionaire, after he increased his stake in the Charlotte Hornets from 80% to 89.5%. On January 20, 2015, Jordan was honored with the "Charlotte Business Journal"'s Business Person of the Year for 2014. In 2017, he became a part owner of the Miami Marlins of Major League Baseball.

"Forbes" designated Jordan as the athlete with the highest career earnings in 2017. From his Jordan Brand income and endorsements, Jordan's 2015 income was an estimated $110 million, the most of any retired athlete. , his net worth is estimated at $2.1 billion by "Forbes", making him the fourth-richest African-American, behind Robert F. Smith, David Steward, and Oprah Winfrey.

Jordan co-owns an automotive group which bears his name. The company has a Nissan dealership in Durham, North Carolina, acquired in 1990, and formerly had a Lincoln–Mercury dealership from 1995 until its closure in June 2009. The company also owned a Nissan franchise in Glen Burnie, Maryland. The restaurant industry is another business interest of Jordan's. His restaurants include a steakhouse in New York City's Grand Central Terminal, among others. Jordan is the majority investor in a golf course, Grove XXIII, under construction in Hobe Sound, Florida.

From 2001 to 2014, Jordan hosted an annual golf tournament, the Michael Jordan Celebrity Invitational, that raised money for various charities. In 2006, Jordan and his wife Juanita pledged $5 million to Chicago's Hales Franciscan High School. The Jordan Brand has made donations to Habitat for Humanity and a Louisiana branch of the Boys & Girls Clubs of America.

The Make-A-Wish Foundation named Jordan its Chief Wish Ambassador in 2008. Five years later, he granted his 200th wish for the organization. As of 2019, he has raised more than $5 million for the Make-A-Wish Foundation.

In 2015, Jordan donated a settlement of undisclosed size from a lawsuit against supermarkets that had used his name without permission to 23 different Chicago charities. Jordan funded two Novant Health Michael Jordan Family Clinics in Charlotte, North Carolina, in 2017 by giving $7 million, the biggest donation he had made at the time. One year later, after Hurricane Florence damaged parts of North Carolina, including his former hometown of Wilmington, Jordan donated $2 million to relief efforts. He gave $1 million to aid the Bahamas' recovery following Hurricane Dorian in 2019.

On June 5, 2020, in the wake of the protests following the killing of George Floyd, Jordan and his brand announced in a joint statement that they will be donating $100 million over the next 10 years to organizations dedicated to "ensuring racial equality, social justice and greater access to education."

Jordan played himself in the 1996 comedy film "Space Jam". The film was a box office success, making $230 million worldwide, although the reviews were mixed.

In 2000, Jordan was the subject of an IMAX documentary about his career with the Chicago Bulls, especially the 1998 championship season, entitled "Michael Jordan to the Max". Two decades later, the same period of Jordan's life was covered in much greater and more personal detail by "The Last Dance", a 10-part TV documentary which debuted on ESPN in April and May 2020. "The Last Dance" relied heavily on about 500 hours of candid film of Jordan's and his teammates' off-court activities which an NBA Entertainment crew had shot over the course of the 1997–98 NBA season for use in a documentary. The project was delayed for many years because Jordan had not yet given his permission for the footage to be used.

Jordan has authored several books focusing on his life, basketball career, and world view.






</doc>
<doc id="20458" url="https://en.wikipedia.org/wiki?curid=20458" title="Musicology">
Musicology

Musicology (from Greek 'μουσική' (mousikē) for 'music' and 'λογος' (logos) for 'domain of study') is the scholarly analysis and research-based study of music. Musicology departments traditionally belong to the humanities, although some music research is scientific in focus (psychological, sociological, acoustical, neurological, computational). A scholar who participates in musical research is a musicologist.

Musicology traditionally is divided in three main branches: historical musicology, systematic musicology and ethnomusicology. Historical musicologists mostly study the history of the so-called Western classical tradition, though the study of music history need not be limited to that. Ethnomusicologists draw from anthropology (particularly field research) to understand how and why people make music. Systematic musicology includes music theory, aesthetics, pedagogy, musical acoustics, the science and technology of musical instruments, and the musical implications of physiology, psychology, sociology, philosophy and computing. Cognitive musicology is the set of phenomena surrounding the cognitive modeling of music. When musicologists carry out research using computers, their research often falls under the field of computational musicology. Music therapy is a specialized form of applied musicology which is sometimes considered more closely affiliated with health fields, and other times regarded as part of musicology proper.

The 19th century philosophical trends that led to the re-establishment of formal musicology education in German and Austrian universities had combined methods of systematization with evolution. These models were established not only in the field of physical anthropology, but also cultural anthropology. This was influenced by Hegel's ideas on ordering "phenomena" from the simple to complex as the stages of evolution are classified from primitive to developed, and stages of history from ancient to modern. Comparative methods became more widespread in diverse disciplines from anatomy to Indo-European linguistics, and beginning around 1880, also in comparative musicology.

The parent disciplines of musicology include:

Musicology also has two central, practically oriented sub-disciplines with no parent discipline: performance practice and research (sometimes viewed as a form of artistic research), and the theory, analysis and composition of music. The disciplinary neighbors of musicology address other forms of art, performance, ritual and communication, including the history and theory of the visual and plastic arts and of architecture; linguistics, literature and theater; religion and theology; and sport. Musical knowledge is applied in medicine, education and music therapy—which, effectively, are parent disciplines of applied musicology.

Music history or historical musicology is concerned with the composition, performance, reception and criticism of music over time. Historical studies of music are for example concerned with a composer's life and works, the developments of styles and genres, "e.g.", baroque concertos, the social function of music for a particular group of people, "e.g.", court music, or modes of performance at a particular place and time, "e.g.", Johann Sebastian Bach's choir in Leipzig. Like the comparable field of art history, different branches and schools of historical musicology emphasize different types of musical works and approaches to music. There are also national differences in various definitions of historical musicology. In theory, "music history" could refer to the study of the history of any type or genre of music, "e.g.", the history of Indian music or the history of rock. In practice, these research topics are more often considered within ethnomusicology (see below) and "historical musicology" is typically assumed to imply Western Art music of the European tradition.

The methods of historical musicology include source studies (especially manuscript studies), paleography, philology (especially textual criticism), style criticism, historiography (the choice of historical method), musical analysis (analysis of music to find "inner coherence") and iconography. The application of musical analysis to further these goals is often a part of music history, though pure analysis or the development of new tools of music analysis is more likely to be seen in the field of music theory. Music historians create a number of written products, ranging from journal articles describing their current research, new editions of musical works, biographies of composers and other musicians, book-length studies or university textbook chapters or entire textbooks. Music historians may examine issues in a close focus, as in the case of scholars who examine the relationship between words and music for a given composer's art songs. On the other hand, some scholars take a broader view, and assess the place of a given type of music, such as the symphony in society using techniques drawn from other fields, such as economics, sociology or philosophy.

"New musicology" is a term applied since the late 1980s to a wide body of work emphasizing cultural study, analysis and criticism of music. Such work may be based on feminist, gender studies, queer theory or postcolonial theory, or the work of Theodor W. Adorno. Although New Musicology emerged from within historical musicology, the emphasis on cultural study within the Western art music tradition places New Musicology at the junction between historical, ethnological and sociological research in music.

New musicology was a reaction against traditional historical musicology, which according to Susan McClary, "fastidiously declares issues of musical signification off-limits to those engaged in legitimate scholarship." Charles Rosen, however, retorts that McClary, "sets up, like so many of the 'new musicologists', a straw man to knock down, the dogma that music has no meaning, and no political or social significance." Today, many musicologists no longer distinguish between musicology and new musicology since it has been recognized that many of the scholarly concerns once associated with new musicology already were mainstream in musicology, so that the term "new" no longer applies.

Ethnomusicology, formerly comparative musicology, is the study of music in its cultural context. It is often considered the anthropology or ethnography of music. Jeff Todd Titon has called it the study of "people making music". Although it is most often concerned with the study of non-Western musics, it also includes the study of Western music from an anthropological or sociological perspective, cultural studies and sociology as well as other disciplines in the social sciences and humanities. Some ethnomusicologists primarily conduct historical studies, but the majority are involved in long-term participant observation or combine ethnographic, musicological, and historical approaches in their fieldwork. Therefore, ethnomusiological scholarship can be characterized as featuring a substantial, intensive fieldwork component, often involving long-term residence within the community studied.
Closely related to ethnomusiology is the emerging branch of sociomusicology. For instance, Ko (2011) proposed the hypothesis of "Biliterate and Trimusical" in Hong Kong sociomusicology.

Popular music studies, known, "misleadingly", as "popular musicology", emerged in the 1980s as an increasing number of musicologists, ethnomusicologists and other varieties of historians of American and European culture began to write about popular musics past and present. The first journal focusing on popular music studies was Popular Music, which began publication in 1981. The same year an academic society solely devoted to the topic was formed, the International Association for the Study of Popular Music. The Association's founding was partly motivated by the interdisciplinary agenda of popular musicology though the group has been characterized by a polarized 'musicological' and 'sociological' approach also typical of popular musicology.

Music theory is a field of study that describes the elements of music and includes the development and application of methods for composing and for analyzing music through both notation and, on occasion, musical sound itself. Broadly, theory may include any statement, belief or conception of or about music (Boretz, 1995). A person who studies or practices music theory is a music theorist.

Some music theorists attempt to explain the techniques composers use by establishing rules and patterns. Others model the experience of listening to or performing music. Though extremely diverse in their interests and commitments, many Western music theorists are united in their belief that the acts of composing, performing and listening to music may be explicated to a high degree of detail (this, as opposed to a conception of musical expression as fundamentally ineffable except in musical sounds). Generally, works of music theory are both descriptive and prescriptive, attempting both to define practice and to influence later practice.

Musicians study music theory to understand the structural relationships in the (nearly always notated) music. Composers study music theory to understand how to produce effects and structure their own works. Composers may study music theory to guide their precompositional and compositional decisions. Broadly speaking, music theory in the Western tradition focuses on harmony and counterpoint, and then uses these to explain large scale structure and the creation of melody.

Music psychology applies the content and methods of all subdisciplines of psychology (perception, cognition, motivation, etc.) to understand how music is created, perceived, responded to, and incorporated into individuals' and societies' daily lives. Its primary branches include cognitive musicology, which emphasizes the use of computational models for human musical abilities and cognition, and the cognitive neuroscience of music, which studies the way that music perception and production manifests in the brain using the methodologies of cognitive neuroscience. While aspects of the field can be highly theoretical, much of modern music psychology seeks to optimize the practices and professions of music performance, composition, education and therapy.

Performance practice draws on many of the tools of historical musicology to answer the specific question of how music was performed in various places at various times in the past. Although previously confined to early music, recent research in performance practice has embraced questions such as how the early history of recording affected the use of vibrato in classical music or instruments in Klezmer.

Within the rubric of musicology, performance practice tends to emphasize the collection and synthesis of evidence about how music should be performed. The important other side, learning how to sing authentically or perform a historical instrument is usually part of conservatory or other performance training. However, many top researchers in performance practice are also excellent musicians.

Music performance research (or music performance science) is strongly associated with music psychology. It aims to document and explain the psychological, physiological, sociological and cultural details of how music is actually performed (rather than how it should be performed). The approach to research tends to be systematic and empirical and to involve the collection and analysis of both quantitative and qualitative data. The findings of music performance research can often be applied in music education.

Musicologists in tenure track professor positions typically hold a Ph.D in musicology. In the 1960s and 1970s, some musicologists obtained professor positions with an M.A. as their highest degree, but in the 2010s, the Ph.D is the standard minimum credential for tenure track professor positions. As part of their initial training, musicologists typically complete a B.Mus or a B.A. in music (or a related field such as history) and in many cases an M.A. in musicology. Some individuals apply directly from a bachelor's degree to a Ph.D, and in these cases, they may not receive an M.A. In the 2010s, given the increasingly interdisciplinary nature of university graduate programs, some applicants for musicology Ph.D programs may have academic training both in music and outside of music (e.g., a student may apply with a B.Mus and an M.A. in psychology). In music education, individuals may hold an M.Ed and an Ed.D.

Most musicologists work as instructors, lecturers or professors in colleges, universities or conservatories. The job market for tenure track professor positions is very competitive. Entry-level applicants must hold a completed Ph.D or the equivalent degree and applicants to more senior professor positions must have a strong record of publishing in peer-reviewed journals. Some Ph.D-holding musicologists are only able to find insecure positions as sessional lecturers. The job tasks of a musicologist are the same as those of a professor in any other humanities discipline: teaching undergraduate and/or graduate classes in their area of specialization and, in many cases some general courses (such as Music Appreciation or Introduction to Music History); conducting research in their area of expertise, publishing articles about their research in peer-reviewed journals, authors book chapters, books or textbooks; traveling to conferences to give talks on their research and learn about research in their field; and, if their program includes a graduate school, supervising M.A. and Ph.D students, giving them guidance on the preparation of their theses and dissertations. Some musicology professors may take on senior administrative positions in their institution, such as Dean or Chair of the School of Music.

The vast majority of major musicologists and music historians from past generations have been men, as in the 19th century and early 20th century; women's involvement in teaching music was mainly in elementary and secondary music teaching. Nevertheless, some women musicologists have reached the top ranks of the profession. Carolyn Abbate (born 1956) is an American musicologist who did her PhD at Princeton University. She has been described by the "Harvard Gazette" as "one of the world's most accomplished and admired music historians".

Susan McClary (born 1946) is a musicologist associated with new musicology who incorporates feminist music criticism in her work. McClary holds a PhD from Harvard University. One of her best known works is "Feminine Endings" (1991), which covers musical constructions of gender and sexuality, gendered aspects of traditional music theory, gendered sexuality in musical narrative, music as a gendered discourse and issues affecting women musicians.

Other notable women scholars include:



Many musicology journals are only available in print or through pay-for-access portals. This list, however, contains a sample of peer reviewed and open-access journals in various subfields as examples of musicological writings:
A list of open-access European journals in the domains of music theory and/or analysis is available on the website of the European Network for Theory & Analysis of Music. A more complete list of open-access journals in theory and analysis can be found on the website of the Société Belge d'Analyse Musicale (in French).


</doc>
<doc id="20460" url="https://en.wikipedia.org/wiki?curid=20460" title="Film promotion">
Film promotion

Film promotion is the practice of promotion specifically in the film industry, and usually occurs in coordination with the process of film distribution. Sometimes called the press junket or film junket, film promotion generally includes press releases, advertising campaigns, merchandising, franchising, media and interviews with the key people involved with the making of the film, like actors and directors. As with all business, it is an important part of any release because of the inherent high financial risk; film studios will invest in expensive marketing campaigns to maximize revenue early in the release cycle. Marketing budgets tend to equal about half the production budget. Publicity is generally handled by the distributor and exhibitors.

Trailers are a mainstay of film promotion, because they are delivered directly to movie-goers. They screen in theatres before movie showings. Generally they tell the story of the movie in a highly condensed fashion compressing maximum appeal into two and half minutes.





Film actors, directors, and producers appear for television, cable, radio, print, and online media interviews, which can be conducted in person or remotely. During film production, these can take place "on set". After the film's premiere, key personnel make appearances in major market cities or participate remotely via satellite videoconference or telephone. The purpose of interviews is to encourage journalists to publish stories about their "exclusive interviews" with the film's stars, thereby creating "marketing buzz" around the film and stimulating audience interest in watching the film.

When it comes to feature films picked up by a major film studio for international distribution, promotional tours are notoriously grueling. Key cast and crew are often contracted to travel to several major cities around the world to promote the film and sit for dozens of interviews. In every interview they are supposed to stay "on message" by energetically expressing their enthusiasm for the film in a way that appears candid, fun, and fresh, even though it may be their fifth or sixth interview that day. They are expected to disclose just enough juicy "behind-the-scenes" information about the filmmaking process or the filmmakers' artistic vision to make each journalist feel like he or she got a nice scoop, while at the same time tactfully avoiding disclosure of anything embarrassing, humiliating or truly negative that may be detrimental to the film's box office gross and profit or influence a critic's review as well as the public's opinion.

There are seven distinct types of research conducted by film distributors in connection with domestic theatrical releases, according to "Marketing to Moviegoers: Second Edition." Such audience research can cost $1 million per film, especially when scores of TV advertisements are tested and re-tested. The bulk of research is done by major studios for the roughly 170 major releases they mount each year that are supported by tens of millions of advertising buys for each film. Independent film distributors, which typically spend less than $10 million in media buys per film, don’t have the budget or breadth of advertising materials to analyze, so they spend little or nothing on pre-release audience research.
When audience research is conducted for domestic theatrical release, it involves these areas:

Marketing can play a big role in whether or not a film gets the green light. Audience research is a strong factor in determining the ability of a film to sell in theaters, which is ultimately how films make their money. As part of a movie's Marketing strategy, audience research comes into account as producers create promotional materials. These promotional materials consistently change and evolve as a direct consequence of audience research up until the film opens in theaters.


</doc>
<doc id="20463" url="https://en.wikipedia.org/wiki?curid=20463" title="Miltiades (disambiguation)">
Miltiades (disambiguation)

Miltiades the Younger (c. 550 – 489 BC) was tyrant of the Thracian Chersonese and the Athenian commanding general in the Battle of Marathon.

Miltiades may also refer to:




</doc>
<doc id="20468" url="https://en.wikipedia.org/wiki?curid=20468" title="Maggie Out">
Maggie Out

"Maggie Out" was a chant popular during the Miners' Strike, student grant protests, Poll Tax protests and other public demonstrations that fell within the time when Margaret Thatcher was the Prime Minister of the United Kingdom.

The chant called for her to be removed from that role. It was referred to, in that context, during a parliamentary session in 1982. 
When Margaret Thatcher felt compelled to resign some people had memories of chanting it for thirteen years. People were passionate about this group activity and associated it with varied political struggles from that time.

It is a variant of the "Oggy Oggy Oggy, Oi Oi Oi" chant. When used in that format, the lyrics were:

<poem>
Maggie, Maggie, Maggie!
Out! Out! Out!

Maggie, Maggie, Maggie!
Out! Out! Out!

Maggie!
Out!

Maggie!
Out!

Maggie, Maggie, Maggie!
Out! Out! Out!
</poem>

The Larks produced a track called "Maggie, Maggie, Maggie (Out, Out, Out)" which was included on the Miners' Benefit LP "Here We Go" on Sterile Records.

Upon Thatcher's resignation, groups of opponents gathered at Downing Street, chanting a variation - replacing the word "out" with "gone".

Following the death of Thatcher on 8 April 2013, this chant was revived in the format of "Maggie, Maggie Maggie (Dead, Dead, Dead)" at celebratory parties held in Glasgow, London and Reading.


</doc>
<doc id="20469" url="https://en.wikipedia.org/wiki?curid=20469" title="M25 motorway">
M25 motorway

The M25 or London Orbital Motorway is a major road encircling almost all of Greater London, England. The Dartford Crossing (A282) is part of the orbital route but is not part of the motorway. The M25 is one of the most important roads in Britain and one of the busiest. The final section was opened by the Prime Minister Margaret Thatcher in 1986; on opening it was the longest ring road in Europe at .

Patrick Abercrombie had proposed an orbital motorway around London in the 1944 "Greater London Plan", which evolved into the London Ringways project in the early 1960s. By 1966, planning had started on two projects, Ringway 3 to the north and Ringway 4 to the south. By the time the first sections opened in 1975, it was decided the ringways would be combined into a single orbital motorway. The M25 was one of the first motorway projects to consider environmental concerns and almost 40 public inquiries took place. The road was built as planned despite some protest that included the section over the North Downs and around Epping Forest which required an extension of the Bell Common Tunnel.

Although the M25 was popular during construction, it quickly became apparent that there was insufficient traffic capacity. Because of the public inquiries, several junctions merely served local roads where office and retail developments were built, attracting even more traffic onto the M25 than it was designed for. The congestion has led to traffic management schemes that include variable speed limit and smart motorway. Since opening, the M25 has been progressively widened, particularly near Heathrow Airport.

In some cases, such as the Communications Act 2003, it is used as a de-facto reference to Greater London.

The M25 almost completely encircles Greater London and passes briefly through it to the east. Junctions 1A–5 are in Kent, 6–14 are in Surrey, 15–16 are in Buckinghamshire, 17–25 are in Hertfordshire, and 26–31 are in Essex. Policing of the road is carried out by an integrated group made up of the Metropolitan, Thames Valley, Essex, Kent, Hertfordshire and Surrey forces. Primary destinations signed ahead on the motorway include the Dartford Crossing, Sevenoaks, Gatwick Airport, Heathrow Airport, Watford, Stansted Airport and Brentwood.

To the east of London the two ends of the M25 are joined to complete a loop by the non-motorway A282 Dartford Crossing of the River Thames between Thurrock and Dartford. The crossing consists of twin two-lane tunnels and the four-lane QE2 (Queen Elizabeth II) bridge. with a main span of . Passage across the bridge or through the tunnels is subject to a charge between 6 am and 10 pm, its level depending on the kind of vehicle. The road is not under motorway regulations so that other traffic can cross the Thames east of the Woolwich Ferry; the only crossing further to the east is a passenger ferry between Gravesend, Kent, and Tilbury, Essex.

At Junction 5, the clockwise carriageway of the M25 is routed off the main north–south dual carriageway onto the main east–west dual carriageway with the main north–south carriageway becoming the A21. In the opposite direction, to the east of the point where the M25 diverges from the main east–west carriageway, that carriageway becomes the M26 motorway. From here to Junction 8, the M25 follows the edge of the North Downs close to several historic buildings such as Chevening, Titsey Place, Hever Castle and Chartwell. The interchange with the M23 motorway near Reigate is a four-level stack; one of only a few examples in Britain. Past this, the M25 runs close to the Surrey Hills AONB.

To the west, the M25 passes close to the edge of Heathrow, and within sight of Windsor Castle. North of this, it crosses the Chiltern Main Line under the Chalfont Viaduct, a 19th-century railway bridge. Red kites can often be seen overhead to the north of this, up to Junction 21. The northern section of the M25 passes close to All Saints Pastoral Centre near London Colney, Waltham Abbey and Copped Hall. This section also features two cut-and-cover tunnels, including the Bell Common Tunnel. The north-eastern section of the motorway passes close to North Ockendon, the only settlement of Greater London situated outside the M25. It then runs close to the Rainham Marshes Nature Reserve before reaching the northern end of the Dartford Crossing.

In 2004, following an opinion poll, the London Assembly proposed aligning the Greater London boundary with the M25. "Inside the M25" and "outside/beyond the M25" are colloquial, looser alternatives to "Greater London" sometimes used in haulage. The Communications Act 2003 explicitly uses the M25 as the boundary in requiring a proportion of television programmes to be made outside the London area; it states a requirement of "a suitable proportion of the programmes made in the United Kingdom" to be made "in the United Kingdom outside the M25 area", defined in Section 362 as "the area the outer boundary of which is represented by the London Orbital Motorway (M25)".

Sections of the M25 form part of two long-distance E-roads, designated by the United Nations Economic Commission for Europe. The E15, which runs from Inverness to Algeciras, follows the M25 and A282 clockwise from the A1(M) at junction 23 to the M20 at junction 3; while the E30 Cork to Omsk route runs from the M4 at junction 15, clockwise to the A12 at junction 28. The United Kingdom is formally part of the E-roads network but, unlike in other countries, these routes are not marked on any road signs.

The M25 was originally built mostly as a dual three-lane motorway. Much of this has since been widened to dual four lanes for almost half, to a dual five-lanes section between Junctions 12 and 14 and a dual six-lane section between Junctions 14 and 15. Further widening is in progress of minor sections with plans for smart motorways in many others.

Two motorway service areas are on the M25, and two others are directly accessible from it. Those on the M25 are Clacket Lane between Junctions 5 and 6 (in the south-east) and Cobham between Junctions 9 and 10 (in the south-west). Those directly accessible from it are South Mimms off Junction 23 (to the north of London) and Thurrock off Junction 31 (to the east of London).

As is common with other motorways, the M25 is equipped with emergency ("SOS") telephones. These connect to two Highways England operated control centres at Godstone (for Junctions 1 to 15 inclusive) and South Mimms (for 16–31). The Dartford Crossing has a dedicated control centre. There is an extensive network of closed circuit television (CCTV) on the motorway so incidents can be easily identified and located. A number of 4×4 vehicles patrol the motorway, attempting to keep traffic moving where possible, and assisting the local police. They can act as a rolling roadblock when there are obstacles on the road.

When completed, the M25 only had street lighting for of its length. Originally, low pressure sodium (SOX) lighting was the most prominent technology used, but this has been gradually replaced with high-pressure sodium (SON) lighting. the motorway has more than 10,000 streetlights. The M25 has a number of pollution control valves along its length, which can shut off drainage in the event of a chemical or fuel spill.

The idea of a general bypass around London was first proposed early in the 20th century. An outer orbital route around the capital had been suggested in 1913, and was re-examined as a motorway route in Sir Charles Bressey's and Sir Edwin Lutyens' "The Highway Development Survey, 1937". Sir Patrick Abercrombie's "County of London Plan, 1943" and "Greater London Plan, 1944" proposed a series of five roads encircling the capital. The northern sections of the M25 follow a similar route to the Outer London Defence Ring, a concentric series of anti-tank defences and pillboxes designed to slow down a potential German invasion of the capital during World War II. This was marked as the D Ring on Abercombie's plans. Following the war, 11 separate county councils told the Ministry of Transport that an orbital route was "first priority" for London.

Plans stalled because the route was planned to pass through several urban areas, which attracted criticism. The original D Ring through north-west London was intended to be a simple upgrade of streets. In 1951, Middlesex County Council planned a route for the orbital road through the county, passing through Eastcote and west of Bushey, connecting with the proposed M1 motorway, but it was rejected by the Ministry two years later. An alternative route via Harrow and Ealing was proposed, but this was abandoned after the council revealed the extent of property demolition required.

In 1964, the London County Council announced the London Ringways plan, to consist of four concentric motorway rings around London. The following year, the transport minister Barbara Castle announced that the D Ring would be essential to build. The component parts of what became the M25 came from Ringway 3 / M16 motorway in the north and Ringway 4 in the south.

The Ringways plan was controversial owing to the destruction required for the inner two ring roads, (Ringway 1 and Ringway 2). Parts of Ringway 1 were constructed (including the West Cross Route), despite stiff opposition, before the overall plan was postponed in February 1972. In April 1973, the Greater London Council elections resulted in a Labour Party victory; the party then formally announced the cancellation of the Ringways running inside Greater London. This did not affect the routes that would become the M25, because they were planned as central government projects from the outset.

There was no individual public inquiry into the M25 as a whole. Each section was presented to planning authorities in its own right and was individually justified, with 39 separate public inquiries relating to sections of the route. The need for the ministry to negotiate with local councils meant that more junctions with local traffic were built than originally proposed. A report in 1981 showed that the M25 had the potential to attract office and retail development along its route, negating the proposed traffic improvements and making Central London a less desirable place to work. None of the motorway was prevented from being built by objections at the public inquiries. However, as a consequence of the backlash against the Ringways, and criticism at the public inquiries, the motorway was built with environmental concerns in mind. New features included additional earth mounds, cuttings and fences that reduced noise, and over two million trees and shrubs to hide the view of the road.

Construction of parts of the two outer ring roads, Ringways 3 and 4, began in 1973. The first section, between South Mimms and Potters Bar in Hertfordshire (Junctions 23 to 24) opened in September 1975. It was provisionally known as the M16 and was given the temporary general-purpose road designation A1178. A section of the North Orbital Road between Rickmansworth and Hunton Bridge was proposed in 1966, with detailed planning in 1971. The road was constructed to motorway standards and opened in October 1976. It eventually became part of the M25's route. The section to the south, from Heathrow Airport to Rickmansworth had five separate routes proposed when a public inquiry was launched in 1974. The Department of Transport sent out 15,000 questionnaires about the preferred route, with 5,000 replies. A route was fixed in 1978, with objections delaying the start of construction in 1982.

The southern section of what became the M25 through Surrey and Kent was first conceived to be an east–west road south of London to relieve the A25, and running parallel to it, with its eastern end following the route of what is now the M26. It was originally proposed as an all-purpose route, but was upgraded to motorway standard in 1966. It was the first section of the route announced as M25 from the beginning. The first section from Godstone to Reigate (Junctions 6 to 8) was first planned in 1966 and opened in February 1976. A section of Ringway 3 south of the river between Dartford and Swanley (Junctions 1 to 3) was constructed between May 1974 and April 1977.
In 1975, following extensive opposition to some parts of Ringway 3 through Middlesex and South London, the transport minister John Gilbert announced that the north section of Ringway 3 already planned would be combined with the southern section of Ringway 4, forming a single orbital motorway to be known as the M25, and the M16 designation was dropped. This scheme required two additional sections to join what were two different schemes, from Swanley to Sevenoaks in the south-east and Hunton Bridge to Potters Bar in the north-west. The section of Ringway 3 west of South Mimms anti-clockwise around London to Swanley in Kent was cancelled.

The section from Potters Bar to the Dartford Tunnel was constructed in stages from June 1979 onwards, with the final section between Waltham Cross (Junction 25) to Theydon Garnon (Junction 27) opening in January 1984. This section, running through Epping Forest, attracted opposition and protests. In 1973, local residents had parked combine harvesters in Parliament Square in protest against the road, draped with large banners reading "Not Epping Likely". As a consequence of this, the Bell Common Tunnel that runs in this area is twice as long as originally proposed.

The most controversial section of the M25 was that between Swanley and Sevenoaks (Junctions 3 to 5) in Kent across the Darenth Valley, Badgers Mount and the North Downs. An 1800-member group named Defend Darenth Valley and the North Downs Action Group (DANDAG) argued that the link was unnecessary, it would damage an Area of Outstanding Natural Beauty and it would be primarily used by local traffic as a bypass for the old A21 road between Farnborough and Sevenoaks. After a length inquiry process, chaired by George Dobry QC, the transport minister Kenneth Clarke announced the motorway would be built as proposed.

The section from the M40 motorway to the 1970s North Orbital Road construction (Junctions 16 to 17) opened in January 1985. The route under the Chalfont Viaduct meant the motorway was restricted to a width of three lanes in each direction.

The Prime Minister Margaret Thatcher officially opened the M25 on 29 October 1986, with a ceremony in the section between Junctions 22 to 23 (London Colney and South Mimms). To avoid the threat of road protesters, the ceremony was held a quarter of a mile from the nearest bridge. The total estimated cost of the motorway was around £1 billion. It required of concrete, of asphalt and involved the removal of of spoil. Upon completion, it was the longest orbital motorway in the world at . At the opening ceremony, Thatcher announced that 98 miles had been constructed while the Conservatives were in office, calling it "a splendid achievement for Britain". A 58-page brochure was published, commemorating the completion of the motorway.

The M25 was initially popular with the public. In the 1987 general election, the Conservatives won in every constituency that the motorway passed through, in particular gaining Thurrock from Labour. Coach tours were organised for a trip around the new road. However, it quickly became apparent that the M25 suffered from chronic congestion. A report in "The Economist" said it "had taken 70 years to plan [the motorway], 12 to build it and just one to find it was inadequate". Thatcher rebuked the negative response, calling it "carping and criticism".

Traffic levels quickly exceeded the maximum design capacity. Two months before opening, the government admitted that the three-lane section between Junctions 11 and 13 was inadequate, and that it would have to be widened to four. In 1990 the Secretary of State for Transport announced plans to widen the whole of the M25 to four lanes. By 1993 the motorway, designed for a maximum of 88,000 vehicles per day, was carrying 200,000. At this time, the M25 carried 15% of UK motorway traffic and there were plans to add six lanes to the section from Junctions 12 to 15 as well as widening the rest of the motorway to four lanes.

In parts, particularly the western third, this plan went ahead, due to consistent congestion. Again, however, plans to widen further sections to eight lanes (four each way) were scaled back in 2009 in response to rising costs. The plans were reinstated in the agreed Highways Agency 2013–14 business plan.

In June 1992, the Department for Transport (DfT) announced a proposal to widen the section close to Heathrow Airport to fourteen lanes by way of three additional link roads. This attracted fierce opposition from road protesters opposing the Newbury Bypass and other schemes, but also from local authorities. Surrey County Council led a formal objection to the widening scheme. It was cancelled shortly afterwards. In 1994, the Standing Advisory Committee on Trunk Road Appraisal published a report saying that "the M25 experience most probably does ... serve as an example of a case where roads generate traffic" and that further improvements to the motorway were counterproductive. In April 1995, the Transport Minister Brian Mawhinney announced that the Heathrow link roads would be scrapped.
In 1995 a contract was awarded to widen the section between Junctions 8 and 10 from six to eight lanes for a cost of £93.4 million, and a Motorway Incident Detection and Automatic Signalling (MIDAS) system was introduced to the M25 from Junction 10 to Junction 15 at a cost of £13.5m in 1995. This was then extended to Junction 16 at a cost of £11.7m in 2002. This consists of a distributed network of traffic and weather sensors, speed cameras and variable-speed signs that control traffic speeds with little human supervision, and has improved traffic flow slightly, reducing the amount of start-stop driving.

After Labour won the 1997 election, the road budget was cut from £6 billion to £1.4 billion. However, the DfT announced new proposals to widen the section between Junction 12 (M3) and Junction 15 (M4) to twelve lanes. At the Heathrow Terminal 5 public inquiry, a Highways Agency official said that the widening was needed to accommodate traffic to the proposed new terminal; however, the transport minister said that no such evidence had been given. Environmental groups objected to the decision to go ahead with a scheme that would create the widest motorways in the UK without holding a public inquiry. Friends of the Earth claimed the real reason for the widening was to support Terminal 5. The decision was again deferred. A ten-lane scheme was announced in 1998 and the £148 million 'M25 Jct 12 to 15 Widening' contract was awarded to Balfour Beatty in 2003. The scheme was completed in 2005 as dual-five lanes between Junctions 12 and 14 and dual-six lanes from Junctions 14 to 15.

In 2007, Junction 25 (A10/Waltham Cross) was remodelled to increase capacity. The nearby Holmesdale Tunnel was widened to three lanes in an easterly direction, and an additional left-turn lane added from the A10 onto the motorway. The total cost was £75 million.

Work to widen the exit slip-roads in both directions at Junction 28 (A12 / A1023) was completed in 2008. It was designed to reduce the amount of traffic queueing on the slip roads at busy periods, particularly traffic from the clockwise M25 joining the northbound A12. In 2018, a new scheme was proposed as the junction had reached capacity at over 7,500 vehicles per hour. This would involve building a two-lane link road between the M25 and the A12. The work is expected to be completed around 2021/22.

In 2006, the Highways Agency proposed widening of the M25 from six to eight lanes, between Junctions 5 and 6, and 16 to 30, as part of a Design, Build, Finance and Operate (DBFO) project. A shortlist of contractors was announced in October 2006 for the project, which was expected to cost £4.5 billion. Contractors were asked to resubmit their bids in January 2008, and in June 2009 the new transport minister indicated that the cost had risen to £5.5 billion and the benefit to cost ratio had dropped considerably. In January 2009 the government announced that plans to widen the sections from Junctions 5 to 7 and 23 to 27 had been 'scrapped' and that hard shoulder running would be introduced instead. However, widening to four lanes was reinstated in the 2013–14 Highways Agency Business Plan.

In 2009, a £6.2 billion M25 DBFO private finance initiative contract was awarded to Connect Plus to widen the sections between Junctions 16 to 23 and 27 to 30, and maintain the M25 and the Dartford Crossing for a 30-year period.

Work to widen the section between Junctions 16 (M40) and 23 (A1(M)) to dual four lanes started in July 2009 at an estimated cost of £580 million. The Junction 16 to 21 (M1) section was completed by July 2011 and the Junction 21 to 23 by June 2012. Works to widen the Junctions 27 (M11) to 30 (A13) section to dual four lanes also started in July 2009. The Junction 27 to 28 (A12) section was completed in July 2010, and the Junction 28 to 29 (A127) in June 2011, and finally the Junction 29 to 30 (A13) section opened in May 2012.

Work to introduce smart motorway technology and permanent hard shoulder running on two sections of the M25 began in 2013. The first section between Junctions 5 (A21/M26) and 7 (M23) started construction in May 2013 with the scheme being completed and opened in April 2014. The second section, between Junctions 23 (A1/A1(M)) and 27 (M11), began construction in February 2013 and was completed and opened in November 2014.

In December 2016, Highways England completed the capacity project at Junction 30 (Thurrock) as part of the Thames Gateway Delivery Plan. The £100m scheme included widening the M25 to four lanes, adding additional link roads, and improvements to drainage.

Work is due to begin in 2020 or 2021 to expand Junction 10, where the M25 meets the A3. There are concerns about the loss of woodland required.

The M25 is one of Europe's busiest motorways. In 2003, a maximum of 196,000 vehicles a day were recorded on the motorway just south of Heathrow between junctions 13 and 14. The stretch between Junctions 14 and 15 nearby consistently records the highest daily traffic counts on the British strategic road network, with the average flow in 2018 of 219,492 counts (lower than the record peak measured in 2014 of 262,842 counts).

Traffic on the M25 is monitored by Connect Plus Services on behalf of Highways England. The company operates a series of transportable CCTV cameras that can be easily moved into congestion hotspots. This allows operators to see a clear view of the motorway and what can be done to tackle individual areas of congestion. Prior to its liquidation, Carillion was subcontracted to manage traffic on the M25, delivering live alerts from body-worn cameras via 3G, 4G and Wi-Fi.

Since 1995, sections of the M25 have been equipped with variable speed limits. These purposefully slow traffic down in the event of congestion or an obstruction and help manage the traffic flow. The scheme was originally trialled between Junctions 10 and 16, and was made a permanent fixture in 1997.

The Dartford Crossing is the only fixed vehicle crossing of the Thames east of Greater London. It is also the busiest crossing in the United Kingdom, and consequently puts pressure on M25 traffic. Users of the crossing do not pay a toll, but rather a congestion charge; the signs at the crossing are the same deployed over the London congestion charge zone. In 2009 the Department for Transport published options for a new Lower Thames Crossing to add capacity to the Dartford Crossing or create a new road and crossing linking to the M2 and M20 motorways. Plans for this stalled, and were cancelled by the Mayor of London, Boris Johnson in 2013, to be replaced by the Gallions Reach Crossing. Initially a straight ferry replacement for the Woolwich Ferry, this was later changed to be a possible bridge or tunnel.

On 11 December 1984, nine people died and ten were injured in a multiple-vehicle collision between junctions 5 and 6. Dense fog had descended suddenly, and 26 vehicles were involved.

On 16 December 1988, several vehicles were stolen and used as getaway for acts of murder and robbery, using the M25 to quickly move between targets. Three men, including Raphael Rowe were tried and sentenced to life imprisonment in 1990, but maintained their innocence. They became known as the M25 Three. Rowe studied journalism while in prison and following release became investigative journalist for the BBC.

In 1996, Kenneth Noye murdered Stephen Cameron in a road rage incident while stopped at traffic lights on an M25 junction. He was convicted in 2000 and sentenced to life imprisonment. He was released in June 2019.

In November 2014, during overnight roadworks, a piece of road surface near Junction 9 at Leatherhead failed to set correctly because of rain. This created a pothole in the road and caused a tailback. The Minister for Transport John Hayes criticised the work and the resulting traffic problems.

The M25 has had problems with animals or birds on the carriageway. In 2009, the Highways Agency reported that they were called out several times a week to remove a swan from the motorway around Junction 13. There have been several reported accidents resulting from horses running onto the main carriageway.

The orbital nature of the motorway, in common with racetracks, lent itself to unofficial, and illegal, motor racing. At the end of the 1980s, before the advent of speed enforcement devices, owners of supercars would meet at night at service stations such as South Mimms and conduct time trials. Times below 1 hour were achieved – an average speed of over 117 mph (188 km/h), which included coming to a halt at the Dartford Tunnel road user charge payment booths. The winner received champagne rather than money. The "Enfield Gazette" referred to an "M25 club", and posters appeared near the M25 advertising the "First London Cannonball Run". The racing had mostly disappeared by the end of the 1980s and could not be done after speed cameras were introduced on the M25.

The video game, "M25 Racer", was produced by Davilex Games in 1997, simulating the racing phenomenon. A similar game, "TOCA Race Driver 3" by Codemasters, was released in 2006. It was criticised by road safety experts for its realistic portrayal of a V8 supercar on the M25 at , completing a lap in 42 minutes.

The M25 and the Dartford Crossing are known for frequent traffic jams. This was noticed before the entire road had been completed; at the official opening ceremony Margaret Thatcher complained about "those who carp and criticise". The jams have inspired derogatory names, such as "Britain's Biggest Car Park" and songs (e.g., Chris Rea's "The Road to Hell"). Nevertheless, coach tours around the M25 have continued to run into the 21st century.

The M25 plays a role in the comedy-fantasy novel "Good Omens", as "evidence for the hidden hand of Satan in the affairs of Man". The demon character, Crowley, had manipulated the design of the M25 to resemble a Satanic sigil, and tried to ensure it would anger as many people as possible to drive them off the path of good. The lengthy series of public inquiries for motorways throughout the 1970s, particularly the M25, influenced the opening of "The Hitchhiker's Guide to the Galaxy", where the Earth is destroyed to make way for a hyperspace bypass.

The M25 enjoyed a more positive reputation among ravers in the late 1980s, when this new orbital motorway became a popular route to the parties that took place around the outskirts of London. The use of the M25 for these raves inspired the name of the electronic duo Orbital.
Iain Sinclair's 2002 book and film "London Orbital" is based on a year-long journey around the M25 on foot.
A piece of graffiti on the Chalfont Viaduct, clearly visible from the M25 and reading "" (parodying John Lennon's "Give Peace A Chance") became popular with the public, attracting its own Facebook group. The message originally read "Peas", supposedly the tag of a London graffiti artist; the rest of the wording is reported to have referred to his frequent clashes with the law. In September 2018, after almost 20 years, the graffiti was vandalised and then removed and replaced with the message "give Helch a break". A spokesman for Network Rail sympathised with the requests to restore the "much-loved graffiti", but said they do not condone people putting their lives at risk by trespassing.

Data from driver location signs provide carriageway identifier information. The numbers on the signs are kilometres from a point on the north side of the Dartford Crossing, while the letter is "A" for the clockwise carriageway and "B" for the anticlockwise. They are spaced every .

The M25 has been criticised for having too many junctions; 14 of them serve only local roads. In 2016, Edmund King, president of the Automobile Association, attributed congestion on the M25 to excessive junctions. This leads to "junction hoppers" who only use the motorway for a short distance before exiting; their difference in speed when entering and leaving the main carriageway causes a domino effect, resulting in all vehicles slowing down.

The M25 originally opened without any service areas. The first, at South Mimms, was opened by Margaret Thatcher in June 1987, a week before the election. Thatcher admired the practical and no-frills architecture of Charles Forte and praised him in her opening speech. The second, Clacket Lane, was opened by Robert Key, Minister for Roads and Traffic, on 21 July 1993. Construction was delayed as the remains of a Roman villa were found on the site, requiring archaeological research. The other service area between junctions is Cobham, which opened on 13 September 2012.

!scope=col| miles
!scope=col| km
!scope=col| Clockwise exits (A carriageway)
!scope=col| Junction
!scope=col| Anti-clockwise exits (B carriageway)
!scope=col| Opening date
! scope=row| RiverThames
! scope=row| J1A
! scope=row| J1B
! scope=row| J2
! scope=row| J3
! scope=row| J4
! scope=row| J5
! scope=row| Services
! scope=row| J6
! scope=row| J7
! scope=row| J8
! scope=row| J9
! scope=row| Services
! scope=row| J10
! scope=row| J11
! scope=row| J12
! scope=row| J13
! scope=row| J14
! scope=row| J15
! scope=row| J16
! scope=row| J17
! scope=row| J18
! scope=row| J19
! scope=row| J20
! scope=row| J21
! scope=row| J21A
! scope=row| J22
! scope=row| J23
! scope=row| J24
! scope=row| J25
! scope=row| J26
! scope=row| J27
! scope=row| J28
! scope=row| J29
! scope=row| J30
! scope=row| J31
! scope=row| RiverThames

Notes
Citations
Sources




</doc>
<doc id="20474" url="https://en.wikipedia.org/wiki?curid=20474" title="Mohs scale of mineral hardness">
Mohs scale of mineral hardness

The Mohs scale of mineral hardness () is a qualitative ordinal scale characterizing scratch resistance of various minerals through the ability of harder material to scratch softer material. Created in 1812 by German geologist and mineralogist Friedrich Mohs, it is one of several definitions of hardness in materials science, some of which are more quantitative. The method of comparing hardness by observing which minerals can scratch others is of great antiquity, having been mentioned by Theophrastus in his treatise "On Stones", , followed by Pliny the Elder in his "Naturalis Historia", . While greatly facilitating the identification of minerals in the field, the Mohs scale does not show how well hard materials perform in an industrial setting.

Despite its lack of precision, the Mohs scale is relevant for field geologists, who use the scale to roughly identify minerals using scratch kits. The Mohs scale hardness of minerals can be commonly found in reference sheets.

Mohs hardness is useful in milling. It allows assessment of which kind of mill will best reduce a given product whose hardness is known. The scale is used at electronic manufacturers for testing the resilience of flat panel display components (such as cover glass for LCDs or encapsulation for OLEDs).

The Mohs scale has been used to evaluate the hardness of smartphone screens. Most modern smartphone displays use Gorilla Glass that scratches at level 6 with deeper grooves at level 7 on the Mohs scale of hardness.

The Mohs scale of mineral hardness is based on the ability of one natural sample of mineral to scratch another mineral visibly. The samples of matter used by Mohs are all different minerals. Minerals are chemically pure solids found in nature. Rocks are made up of one or more minerals. As the hardest known naturally occurring substance when the scale was designed, diamonds are at the top of the scale. The hardness of a material is measured against the scale by finding the hardest material that the given material can scratch, or the softest material that can scratch the given material. For example, if some material is scratched by apatite but not by fluorite, its hardness on the Mohs scale would fall between 4 and 5. "Scratching" a material for the purposes of the Mohs scale means creating non-elastic dislocations visible to the naked eye. Frequently, materials that are lower on the Mohs scale can create microscopic, non-elastic dislocations on materials that have a higher Mohs number. While these microscopic dislocations are permanent and sometimes detrimental to the harder material's structural integrity, they are not considered "scratches" for the determination of a Mohs scale number.

The Mohs scale is a purely ordinal scale. For example, corundum (9) is twice as hard as topaz (8), but diamond (10) is four times as hard as corundum. The table below shows the comparison with the absolute hardness measured by a sclerometer, with pictorial examples.

On the Mohs scale, a streak plate (unglazed porcelain) has a hardness of approximately 7.0. Using these ordinary materials of known hardness can be a simple way to approximate the position of a mineral on the scale.

The table below incorporates additional substances that may fall between levels:

Comparison between Mohs hardness and Vickers hardness:



</doc>
<doc id="20476" url="https://en.wikipedia.org/wiki?curid=20476" title="Murray Gell-Mann">
Murray Gell-Mann

Murray Gell-Mann (; September 15, 1929 – May 24, 2019) was an American physicist who received the 1969 Nobel Prize in Physics for his work on the theory of elementary particles. He was the Robert Andrews Millikan Professor of Theoretical Physics Emeritus at the California Institute of Technology, a distinguished fellow and one of the co-founders of the Santa Fe Institute, a professor of physics at the University of New Mexico, and the Presidential Professor of Physics and Medicine at the University of Southern California.

Gell-Mann spent several periods at CERN, a nuclear research facility in Switzerland, among others as a John Simon Guggenheim Memorial Foundation fellow in 1972.

Gell-Mann was born in lower Manhattan into a family of Jewish immigrants from the Austro-Hungarian Empire, specifically from Chernivtsi (historical name: Czernowitz) in present-day Ukraine. His parents were Pauline (née Reichstein) and Arthur Isidore Gell-Mann, who taught English as a second language (ESL).

Propelled by an intense boyhood curiosity and love for nature and mathematics, he graduated valedictorian from the Columbia Grammar & Preparatory School aged 14 and subsequently entered Yale College as a member of Jonathan Edwards College. At Yale, he participated in the William Lowell Putnam Mathematical Competition and was on the team representing Yale University (along with Murray Gerstenhaber and Henry O. Pollak) that won the second prize in 1947. 

Gell-Mann graduated from Yale with a bachelor's degree in physics in 1948 and intended to pursue graduate studies in physics. He sought to remain in the Ivy League for his graduate education and applied to Princeton University as well as Harvard University. He was rejected by Princeton and accepted by Harvard, but the latter institution was unable to offer him any of the financial assistance that he needed. He was accepted by the Massachusetts Institute of Technology (MIT) and received a letter from Victor Weisskopf urging him to attend MIT and become Weisskopf's research assistant, which would provide Gell-Mann with the financial assistance he needed. Unaware of MIT's eminent status in physics research, Gell-Mann was "miserable" with the fact that he would not be able to attend Princeton or Harvard and considered suicide. He stated that he realized he could try to first enter MIT and commit suicide afterwards if he found it to be truly terrible. However, he couldn't first choose suicide and then attend MIT; the two were not "commutable", as Gell-Mann said.

Gell-Mann received his Ph.D. in physics from MIT in 1951 after completing a doctoral dissertation, titled "Coupling strength and nuclear reactions", under the supervision of Victor Weisskopf. 

Gell-Mann was a postdoctoral fellow at the Institute for Advanced Study in 1951, and a visiting research professor at the University of Illinois at Urbana–Champaign from 1952 to 1953. He was a visiting associate professor at Columbia University and an associate professor at the University of Chicago in 1954–1955 before moving to the California Institute of Technology, where he taught from 1955 until he retired in 1993.

In 1958, Gell-Mann in collaboration with Richard Feynman, in parallel with the independent team of E. C. George Sudarshan and Robert Marshak, discovered the chiral structures of the weak interaction of physics and developed the V-A theory (vector minus axial vector theory). This work followed the experimental discovery of the violation of parity by Chien-Shiung Wu, as suggested by Chen-Ning Yang and Tsung-Dao Lee, theoretically.

Gell-Mann's work in the 1950s involved recently discovered cosmic ray particles that came to be called kaons and hyperons. Classifying these particles led him to propose that a quantum number called strangeness would be conserved by the strong and the electromagnetic interactions, but not by the weak interactions. (Kazuhiko Nishijima arrived at this idea independently, calling the quantity formula_1-charge after the eta meson.) Another of Gell-Mann's ideas is the Gell-Mann–Okubo formula, which was, initially, a formula based on empirical results, but was later explained by his quark model. Gell-Mann and Abraham Pais were involved in explaining the puzzling aspect of the neutral kaon mixing.

Murray Gell-Mann's fortunate encounter with mathematician Richard Earl Block at Caltech, in the fall of 1960, "enlightened" him to introduce a novel classification scheme, in 1961, for hadrons, elementary particles that participate in the strong interaction. A similar scheme had been independently proposed by Yuval Ne'eman, and is now explained by the quark model. Gell-Mann referred to the scheme as the "eightfold way", because of the "octets" of particles in the classification (the term is a reference to the Eightfold Path of Buddhism).

Gell-Mann, along with Maurice Lévy, developed the sigma model of pions, which describes low-energy pion interactions.

In 1964, Gell-Mann and, independently, George Zweig went on to postulate the existence of quarks, particles of which the hadrons of this scheme are composed. The name was coined by Gell-Mann and is a reference to the novel "Finnegans Wake", by James Joyce ("Three quarks for Muster Mark!" book 2, episode 4). Zweig had referred to the particles as "aces", but Gell-Mann's name caught on. Quarks, antiquarks, and gluons were soon established as the underlying elementary objects in the study of the structure of hadrons. He was awarded a Nobel Prize in Physics in 1969 for his contributions and discoveries concerning the classification of elementary particles and their interactions.

In the 1960s, he introduced current algebra as a method of systematically exploiting symmetries to extract predictions from quark models, in the absence of reliable dynamical theory. This method led to model-independent sum rules confirmed by experiment and provided starting points underpinning the development of the Standard Model (SM), the widely accepted theory of elementary particles.

In 1972 he and Harald Fritzsch introduced the conserved quantum number "color charge", and later, together with Heinrich Leutwyler, they coined the term quantum chromodynamics (QCD) as the gauge theory of the strong interaction. The quark model is a part of QCD, and it has been robust enough to accommodate in a natural fashion the discovery of new "flavors" of quarks, which superseded the eightfold way scheme.

Gell-Mann was responsible, together with Pierre Ramond and Richard Slansky, and independently of Peter Minkowski, Rabindra Mohapatra, Goran Senjanović, Sheldon Lee Glashow, and Tsutomu Yanagida, for the seesaw theory of neutrino masses, that produces masses at the large scale in any theory with a right-handed neutrino. He is also known to have played a role in keeping string theory alive through the 1970s and early 1980s, supporting that line of research at a time when it was a topic of niche interest.

At the time of his death, Gell-Mann was the Robert Andrews Millikan Professor of Theoretical Physics Emeritus at California Institute of Technology as well as a University Professor in the Physics and Astronomy Department of the University of New Mexico in Albuquerque, New Mexico, and the Presidential Professor of Physics and Medicine at the University of Southern California. He was a member of the editorial board of the "Encyclopædia Britannica". In 1984 Gell-Mann was one of several co-founders of the Santa Fe Institute—a non-profit theoretical research institute in Santa Fe, New Mexico intended to study complex systems and disseminate the notion of a separate interdisciplinary study of complexity theory.

He wrote a popular science book about physics and complexity science, "The Quark and the Jaguar: Adventures in the Simple and the Complex" (1994). The title of the book is taken from a line of a poem by Arthur Sze: "The world of the quark has everything to do with a jaguar circling in the night".

The author George Johnson has written a biography of Gell-Mann, "Strange Beauty: Murray Gell-Mann, and the Revolution in 20th-Century Physics" (1999), which was shortlisted for the Royal Society Book Prize. Gell-Mann himself criticized "Strange Beauty" for some inaccuracies, with one interviewer reporting him wincing at the mention of it. In a review in the Caltech magazine "Engineering & Science," Gell-Mann's colleague, the physicist David Goodstein, wrote: "I don't envy Murray the weird experience of reading so penetrating and perceptive a biography of himself. . . . George Johnson has written a fine biography of this important and complex man." Physicist and Nobel laureate Philip Anderson, called the book "a masterpiece of scientific explication for the layman" and a "must read" in a review for the "Times Higher Education Supplement" and in his chapter on Gell-Mann from a 2011 book. Sheldon Lee Glashow, another Nobel laureate, gave "Strange Beauty" a generally positive review while noting some inaccuracies, and physicist and science historian Silvan S. Schweber called the book "an elegant biography of one of the outstanding theorists of the twentieth century" though he noted that Johnson did not go into depth about Gell-Mann's work with military-industrial organizations like the Institute for Defense Analyses. Johnson has written that Gell-Mann was a perfectionist and that "The Quark and the Jaguar" was consequently submitted late and incomplete. In an item on Edge.org, Johnson described the back story of his relationship with Gell-Mann and noted that an errata sheet appears on the biography's webpage. Gell-Mann's one-time Caltech associate Stephen Wolfram called Johnson's book "a very good biography of Murray, which Murray hated". Wolfram also wrote that Gell-Mann thought the writing of "The Quark and the Jaguar" to be responsible for a heart attack he (Gell-Mann) had had.
In 2012 Gell-Mann and his companion Mary McFadden published the book "Mary McFadden: A Lifetime of Design, Collecting, and Adventure".

Gell-Mann was a proponent of the consistent histories approach to understanding quantum mechanics, which he advocated in papers with James Hartle.

Gell-Mann married J. Margaret Dow (d. 1981) in 1955; they had a daughter and a son. Margaret died in 1981, and in 1992 he married Marcia Southwick, with whom he had a stepson.

Gell-Mann's interests outside of physics included archaeology, birdwatching and linguistics. Along with S. A. Starostin, he established the "Evolution of Human Languages project" at the Santa Fe Institute. As a humanist and an agnostic, Gell-Mann was a Humanist Laureate in the International Academy of Humanism.

Gell-Mann died on May 24, 2019, at his home in Santa Fe, New Mexico. He was remembered by, among others, novelist Cormac McCarthy, who saw Murray as a polymath who "knew more things about more things than anyone I've ever met." "Losing Murray is like losing the Encyclopædia Britannica."

Gell-Mann won numerous awards and honours including the following:


Universities that gave Gell-Mann honorary doctorates include Cambridge, Columbia, the University of Chicago, Oxford and Yale.





</doc>
<doc id="20478" url="https://en.wikipedia.org/wiki?curid=20478" title="Magnetopause">
Magnetopause

The magnetopause is the abrupt boundary between a magnetosphere and the surrounding plasma. For planetary science, the magnetopause is the boundary between the planet's magnetic field and the solar wind. The location of the magnetopause is determined by the balance between the pressure of the dynamic planetary magnetic field and the dynamic pressure of the solar wind. As the solar wind pressure increases and decreases, the magnetopause moves inward and outward in response. Waves (ripples and flapping motion) along the magnetopause move in the direction of the solar wind flow in response to small-scale variations in the solar wind pressure and to Kelvin–Helmholtz instability.

The solar wind is supersonic and passes through a bow shock where the direction of flow is changed so that most of the solar wind plasma is deflected to either side of the magnetopause, much like water is deflected before the bow of a ship. The zone of shocked solar wind plasma is the magnetosheath. At Earth and all the other planets with intrinsic magnetic fields, some solar wind plasma succeeds in entering and becoming trapped within the magnetosphere. At Earth, the solar wind plasma which enters the magnetosphere forms the plasma sheet. The amount of solar wind plasma and energy that enters the magnetosphere is regulated by the orientation of the interplanetary magnetic field, which is embedded in the solar wind.

The Sun and other stars with magnetic fields and stellar winds have a solar magnetopause or heliopause where the stellar environment is bounded by the interstellar environment.

Prior to the age of space exploration, interplanetary space was considered to be a vacuum. The coincidence of the Carrington super flare and the super geomagnetic event of 1859 was evidence that plasma was ejected from the Sun during a flare event. Chapman and Ferraro proposed that a plasma was emitted by the Sun in a burst as part of a flare event which disturbed the planet's magnetic field in a manner known as a geomagnetic storm. The collision frequency of particles in the plasma in the interplanetary medium is very low and the electrical conductivity is so high that it could be approximated to an infinite conductor. A magnetic field in a vacuum cannot penetrate a volume with infinite conductivity. Chapman and Bartels (1940) illustrated this concept by postulating a plate with infinite conductivity placed on the dayside of a planet's dipole as shown in the schematic. The field lines on the dayside are bent. At low latitudes, the magnetic field lines are pushed inward. At high latitudes, the magnetic field lines are pushed backwards and over the polar regions. The boundary between the region dominated by the planet's magnetic field (i.e., the magnetosphere) and the plasma in the interplanetary medium is the magnetopause. The configuration equivalent to a flat, infinitely conductive plate is achieved by placing an image dipole (green arrow at left of schematic) at twice the distance from the planet's dipole to the magnetopause along the planet-Sun line. Since the solar wind is continuously flowing outward, the magnetopause above, below and to the sides of the planet are swept backward into the geomagnetic tail as shown in the artist's concept. The region (shown in pink in the schematic) which separates field lines from the planet which are pushed inward from those which are pushed backward over the poles is an area of weak magnetic field or day-side cusp. Solar wind particles can enter the planet's magnetosphere through the cusp region. Because the solar wind exists at all times and not just times of solar flares, the magnetopause is a permanent feature of the space near any planet with a magnetic field.

The magnetic field lines of the planet's magnetic field are not stationary. They are continuously joining or merging with magnetic field lines of the interplanetary magnetic field. The joined field lines are swept back over the poles into the planetary magnetic tail. In the tail, the field lines from the planet's magnetic field are re-joined and start moving toward night-side of the planet. The physics of this process was first explained by Dungey (1961).

If one assumed that magnetopause was just a boundary between a magnetic field in a vacuum and a plasma with a weak magnetic field embedded in it, then the magnetopause would be defined by electrons and ions penetrating one gyroradius into the magnetic field domain. Since the gyro-motion of electrons and ions is in opposite directions, an electric current flows along the boundary. The actual magnetopause is much more complex.

If the pressure from particles within the magnetosphere is neglected, it is possible to estimate the distance to the part of the magnetosphere that faces the Sun. The condition governing this position is that the dynamic ram pressure from the solar wind is equal to the magnetic pressure from the Earth's magnetic field:
where formula_2 and formula_3 are the density and velocity of the solar wind, and
"B"("r") is the magnetic field strength of the planet in SI units ("B" in T, μ in H/m).

Since the dipole magnetic field strength varies with distance as formula_4 the magnetic field strength can be written as formula_5, where formula_6 is the planet's magnetic moment, expressed in formula_7.
Solving this equation for r leads to an estimate of the distance

The distance from Earth to the subsolar magnetopause varies over time due to solar activity, but typical distances range from 6–15 Rformula_10. Empirical models using real-time solar wind data can provide a real-time estimate of the magnetopause location. A bow shock stands upstream from the magnetopause. It serves to decelerate and deflect the solar wind flow before it reaches the magnetopause.

Research on the magnetopause is conducted using the LMN coordinate system (which is set of axes like XYZ). N points normal to the magnetopause outward to the magnetosheath, L lies along the projection of the dipole axis onto the magnetopause (positive northward), and M completes the triad by pointing dawnward.

Venus and Mars do not have a planetary magnetic field and do not have a magnetopause. The solar wind interacts with the planet's atmosphere and a void is created behind the planet. In the case of the Earth's moon and other bodies without a magnetic field or atmosphere, the body's surface interacts with the solar wind and a void is created behind the body.



</doc>
<doc id="20479" url="https://en.wikipedia.org/wiki?curid=20479" title="Magnetosphere">
Magnetosphere

A magnetosphere is a region of space surrounding an astronomical object in which charged particles are affected by that object's magnetic field. It is created by a star or planet with an active interior dynamo.

In the space environment close to a planetary body, the magnetic field resembles a magnetic dipole. Farther out, field lines can be significantly distorted by the flow of electrically conducting plasma, as emitted from the Sun or a nearby star. e.g. the solar wind. Planets having active magnetospheres, like the Earth, are capable of mitigating or blocking the effects of solar radiation or cosmic radiation, that also protects all living organisms from potentially detrimental and dangerous consequences. This is studied under the specialized scientific subjects of plasma physics, space physics and aeronomy.

Study of Earth's magnetosphere began in 1600, when William Gilbert discovered that the magnetic field on the surface of Earth resembled that of a terrella, a small, magnetized sphere. In the 1940s, Walter M. Elsasser proposed the model of dynamo theory, which attributes Earth's magnetic field to the motion of Earth's iron outer core. Through the use of magnetometers, scientists were able to study the variations in Earth's magnetic field as functions of both time and latitude and longitude.

Beginning in the late 1940s, rockets were used to study cosmic rays. In 1958, Explorer 1, the first of the Explorer series of space missions, was launched to study the intensity of cosmic rays above the atmosphere and measure the fluctuations in this activity. This mission observed the existence of the Van Allen radiation belt (located in the inner region of Earth's magnetosphere), with the follow up Explorer 3 later that year definitively proving its existence. Also during 1958, Eugene Parker proposed the idea of the solar wind, with the term 'magnetosphere' being proposed by Thomas Gold in 1959 to explain how the solar wind interacted with the Earth's magnetic field. The later mission of Explorer 12 in 1961 led by the Cahill and Amazeen observation in 1963 of a sudden decrease in magnetic field strength near the noon-time meridian, later was named the magnetopause. By 1983, the International Cometary Explorer observed the magnetotail, or the distant magnetic field.

Magnetospheres are dependent on several variables: the type of astronomical object, the nature of sources of plasma and momentum, the period of the object's spin, the nature of the axis about which the object spins, the axis of the magnetic dipole, and the magnitude and direction of the flow of solar wind.

The planetary distance where the magnetosphere can withstand the solar wind pressure is called the Chapman–Ferraro distance. This is usefully modeled by the formula wherein formula_1 represents the radius of the planet, formula_2 represents the magnetic field on the surface of the planet at the equator, and formula_3 represents the velocity of the solar wind:

A magnetosphere is classified as "intrinsic" when formula_5, or when the primary opposition to the flow of solar wind is the magnetic field of the object. Mercury, Earth, Jupiter, Ganymede, Saturn, Uranus, and Neptune, for example, exhibit intrinsic magnetospheres. A magnetosphere is classified as "induced" when formula_6, or when the solar wind is not opposed by the object's magnetic field. In this case, the solar wind interacts with the atmosphere or ionosphere of the planet (or surface of the planet, if the planet has no atmosphere). Venus has an induced magnetic field, which means that because Venus appears to have no internal dynamo effect, the only magnetic field present is that formed by the solar wind's wrapping around the physical obstacle of Venus (see also Venus' induced magnetosphere). When formula_7, the planet itself and its magnetic field both contribute. It is possible that Mars is of this type.

The bow shock forms the outermost layer of the magnetosphere; the boundary between the magnetosphere and the ambient medium. For stars, this is usually the boundary between the stellar wind and interstellar medium; for planets, the speed of the solar wind there decreases as it approaches the magnetopause.

The magnetosheath is the region of the magnetosphere between the bow shock and the magnetopause. It is formed mainly from shocked solar wind, though it contains a small amount of plasma from the magnetosphere. It is an area exhibiting high particle energy flux, where the direction and magnitude of the magnetic field varies erratically. This is caused by the collection of solar wind gas that has effectively undergone thermalization. It acts as a cushion that transmits the pressure from the flow of the solar wind and the barrier of the magnetic field from the object.

The magnetopause is the area of the magnetosphere wherein the pressure from the planetary magnetic field is balanced with the pressure from the solar wind. It is the convergence of the shocked solar wind from the magnetosheath with the magnetic field of the object and plasma from the magnetosphere. Because both sides of this convergence contain magnetized plasma, the interactions between them are complex. The structure of the magnetopause depends upon the Mach number and beta of the plasma, as well as the magnetic field. The magnetopause changes size and shape as the pressure from the solar wind fluctuates.

Opposite the compressed magnetic field is the magnetotail, where the magnetosphere extends far beyond the astronomical object. It contains two lobes, referred to as the northern and southern tail lobes. Magnetic field lines in the northern tail lobe point towards the object while those in the southern tail lobe point away. The tail lobes are almost empty, with few charged particles opposing the flow of the solar wind. The two lobes are separated by a plasma sheet, an area where the magnetic field is weaker, and the density of charged particles is higher.

Over Earth's equator, the magnetic field lines become almost horizontal, then return to reconnect at high latitudes. However, at high altitudes, the magnetic field is significantly distorted by the solar wind and its solar magnetic field. On the dayside of Earth, the magnetic field is significantly compressed by the solar wind to a distance of approximately . Earth's bow shock is about thick and located about from Earth. The magnetopause exists at a distance of several hundred kilometers above Earth's surface. Earth's magnetopause has been compared to a sieve because it allows solar wind particles to enter. Kelvin–Helmholtz instabilities occur when large swirls of plasma travel along the edge of the magnetosphere at a different velocity from the magnetosphere, causing the plasma to slip past. This results in magnetic reconnection, and as the magnetic field lines break and reconnect, solar wind particles are able to enter the magnetosphere. On Earth's nightside, the magnetic field extends in the magnetotail, which lengthwise exceeds . Earth's magnetotail is the primary source of the polar aurora. Also, NASA scientists have suggested that Earth's magnetotail might cause "dust storms" on the Moon by creating a potential difference between the day side and the night side.

Many astronomical objects generate and maintain magnetospheres. In the Solar System this includes the Sun, Mercury, Jupiter, Saturn, Uranus, Neptune, and Ganymede. The magnetosphere of Jupiter is the largest planetary magnetosphere in the Solar System, extending up to on the dayside and almost to the orbit of Saturn on the nightside. Jupiter's magnetosphere is stronger than Earth's by an order of magnitude, and its magnetic moment is approximately 18,000 times larger.
Venus, Mars, and Pluto, on the other hand, have no magnetic field. This may have had significant effects on their geological history. It is theorized that Venus and Mars may have lost their primordial water to photodissociation and the solar wind. A strong magnetosphere greatly slows this process.



</doc>
<doc id="20481" url="https://en.wikipedia.org/wiki?curid=20481" title="Manama">
Manama

Manama ( ""   ) is the capital and largest city of Bahrain, with an approximate population of 157,000 people. Long an important trading center in the Persian Gulf, Manama is home to a very diverse population. After periods of Portuguese and Persian control and invasions from the ruling dynasties of Saudi Arabia and Oman, Bahrain established itself as an independent nation during the 19th century period of British hegemony.

Although the current twin cities of Manama and Muharraq appear to have been founded simultaneously in the 1800s, Muharraq took prominence due to its defensive location and was thus the capital of Bahrain until 1923. Manama became the mercantile capital and was the gateway to the main Bahrain Island. In the 20th century, Bahrain's oil wealth helped spur fast growth and in the 1990s a concerted diversification effort led to expansion in other industries and helped transform Manama into an important financial hub in the Middle East. Manama was designated as the 2012 capital of Arab culture by the Arab League, and a beta global city by the Globalization and World Cities Research Network in 2018.

The name is derived from the Arabic word المنامة (transliterated:"al-manãma") meaning "the place of rest" or "the place of dreams".

There is evidence of human settlement on the northern coastline of Bahrain dating back to the Bronze Age. The Dilmun civilisation inhabited the area in 3000 BC, serving as a key regional trading hub between Mesopotamia, Magan and the Indus Valley civilisation. Approximately 100,000 Dilmun burial mounds were found across the north and central regions of the country, some originating 5,000 years ago. Despite the discovery of the mounds, there is no significant evidence to suggest heavy urbanisation took place during the Dilmun era. It is believed that the majority of the population lived in rural areas, numbering several thousand. Evidence of an ancient large rural population was confirmed by one of Alexander the Great's ship captains, during voyages in the Persian Gulf. A vast system of aqueducts in northern Bahrain helped facilitate ancient horticulture and agriculture. 

The commercial network of Dilmun lasted for almost 2,000 years, after which the Assyrians took control of the island in 700 BC for more than a century. This was followed by Babylonian and Achaemenid rule, which later gave way to Greek influence during the time of Alexander the Great's conquests. In the first century AD, the Roman writer Pliny the Elder wrote of Tylos, the Hellenic name of Bahrain in the classical era, and its pearls and cotton fields. The island came under the control of the Parthian and Sassanid empires respectively, by which time Nestorian Christianity started to spread in Bahrain. By 410–420 AD, a Nestorian bishopric and monastery was established in Al Dair, on the neighbouring island of Muharraq. Following the conversion of Bahrain to Islam in 628 AD, work on one of the earliest mosques in the region, the Khamis Mosque, began as early as the seventh century AD. During this time, Bahrain was engaged in long distance marine trading, evident from the discovery of Chinese coins dating between 600–1200 AD, in Manama.

In 1330, under the Jarwanid dynasty, the island became a tributary of the Kingdom of Hormuz. The town of Manama was mentioned by name for the first time in a manuscript dating to 1345 AD. Bahrain, particularly Manama and the nearby settlement of Bilad Al Qadeem, became a centre of Shia scholarship and training for the ulema, it would remain so for centuries. The ulema would help fund pearling expeditions and finance grain production in the rural areas surrounding the city. In 1521, Bahrain fell to the expanding Portuguese Empire in the Persian Gulf, having already defeated Hormuz. The Portuguese consolidated their hold on the island by constructing the Bahrain Fort, on the outskirts of Manama. After numerous revolts and an expanding Safavid empire in Persia, the Portuguese were expelled from Bahrain and the Safavids took control in 1602.

The Safavids, sidelining Manama, designated the nearby town of Bilad Al Qadeem as the provincial capital. The town was also the seat of the Persian governor and the Shaikh al-Islam of the islands. The position of Shaikh al-Islam lay under the jurisdiction of the central Safavid government and as such, candidates were carefully vetted by the Isfahan courts. During the Safavid era, the islands continued to be a centre for Twelver Shi'ism scholarship, producing clerics for use in mainland Persia. Additionally, the rich agricultural northern region of Bahrain continued to flourish due to an abundance of date palm farms and orchards. The Portuguese traveler Pedro Teixeira commented on the extensive cultivation of crops like barley and wheat. The opening of Persian markets to Bahraini exports, especially pearls, boosted the islands' export economy. The yearly income of exported Bahraini pearls was 600,000 ducats, collected by around 2,000 pearling dhows. Another factor that contributed to Bahrain's agricultural wealth was the migration of Shia cultivators from Ottoman-occupied Qatif and al-Hasa, fearing religious persecution, in 1537. Sometime after 1736, Nader Shah constructed a fort on the southern outskirts of Manama (likely the Diwan Fort).

Persian control over the Persian Gulf waned during the later half of the 18th century. At this time, Bahrain archipelago was a dependency of the emirate of Bushehr, itself a part of Persia. In 1783, the Bani Utbah tribal confederation invaded Bahrain and expelled the resident governor Nasr Al-Madhkur. As a result, the Al Khalifa family became the rulers of the country, and all political relations with Bushehr and Persia/Iran were terminated. Ahmed ibn Muhammad ibn Khalifa (later called Ahmed al-Fateh, lit. "Ahmed the conqueror") become the dynasty's first Hakim of Bahrain. Political instability in the 19th century had disastrous effects on Manama's economy; Invasions by the Omanis in 1800 and by the Wahhabis in 1810–11, in addition to a civil war in 1842 between Bahrain's co-rulers saw the town being a major battleground. The instability paralysed commercial trade in Manama; the town's port was closed, most merchants fled abroad to Kuwait and the Persian coast until hostilities ceased. The English scholar William Gifford Palgrave, on a visit to Manama in 1862, described the town as having a few ruined stone buildings, with a landscape dominated with the huts of poor fishermen and pearl-divers.

The Pax Britannica of the 19th century resulted in British consolidation of trade routes, particularly those close to the British Raj. In response to piracy in the Persian Gulf region, the British deployed warships and forced much of the Persian Gulf States at the time (including Bahrain) to sign the General Maritime Treaty of 1820, which prohibited piracy and slavery. In 1861, the Perpetual Truce of Peace and Friendship was signed between Britain and Bahrain, which placed the British in charge of defending Bahrain in exchange for British control over Bahraini foreign affairs. With the ascension of Isa ibn Ali Al Khalifa as the Hakim of Bahrain in 1869, Manama became the centre of British activity in the Persian Gulf, though its interests were initially strictly commercial. Trading recovered fully by 1873 and the country's earnings from pearl exports increased by sevenfold between 1873 and 1900. Representing the British were native agents, usually from minorities such as Persians or Huwala who regularly reported back to British India and the British political residency in Bushehr. The position of native agent was later replaced by a British political agent, following the construction of the British political residency (locally referred to in ) in 1900, which further solidified Britain's position in Manama.

Following the outbreak of World War I in 1914, the British Raj used Manama as a military base of operations during the Mesopotamian campaign. Prompted by the presence of oil in the region, the British political agency in Bushire concluded an oil agreement with the Hakim to prohibit the exploration and exploitation of oil for a five-year period. In 1919, Bahrain was officially integrated into the British empire as an overseas imperial territory following the Bahrain order-in-council decree, issued in 1913. The decree gave the resident political agent greater powers and placed Bahrain under the residency of Bushire and therefore under the governance of the British Raj. The British pressured a series of administrative reforms in Bahrain during the 1920s (a move met with opposition from tribal leaders), during which the aging Hakim Isa ibn Ali Al Khalifa was forced to abdicate in favour of his reform-minded son Hamad ibn Isa Al Khalifa. A municipal government was established in Manama in 1919, the Customs office was reorganised in 1923 and placed under the supervision of an English businessman, the pearling industry was later reformed in 1924. Earnings from the customs office would be kept in the newly created state treasury. Civil courts were established for the first time in 1923, followed by the establishment of the Department of Land Registration in 1924. Charles Belgrave, from the Colonial office, was appointed in 1926 by the British to carry on further reforms and manage administration as a financial advisor to the King. He later organised the State Police and was in charge of the Finance and Land departments of the government. 

In 1927, the country's pearling economy collapsed due to the introduction of Japanese cultured pearls in the world market. It is estimated that between 1929 and 1931, pearling entrepreneurs lost more than two-thirds of their income. Further aggravated by the Great Depression, many leading Bahraini businessmen, shopkeepers, and pearl-divers fell into debt. With the discovery of oil in 1932 and the subsequent production of oil exports in 1934, the country gained a greater significance in geopolitics. The security of oil supplies in the Middle East was a priority of the British, especially in the run-up to the Second World War. The discovery of oil led to gradual employment of bankrupt divers from the pearling industry in the 1930s, eventually causing the pearling industry to disappear. During the war, the country served as a strategic airbase between Britain and India as well as hosting RAF Muharraq and a naval base in Juffair. Bahrain was bombed by the Italian Air Force in 1940. In 1947, following the end of the war and subsequent Indian independence, the British residency of the Persian Gulf moved to Manama from Bushire.
Following the rise of Arab nationalism across the Middle East and sparked by the Suez Crisis in 1956, anti-British unrest broke out in Manama, organised by the National Union Committee. Though the NUC advocated peaceful demonstrations, buildings and enterprises belonging to Europeans (the British in particular) as well as the main Catholic church in the city and petrol stations, were targeted and set ablaze. Demonstrations held in front of the British political residency called for the dismissal of Charles Belgrave, who was later dismissed by the direct intervention of the Foreign Office the following year. A subsequent crackdown on the NUC led to the dissolution of the body. Another anti-British uprising erupted in March 1965, though predominately led by students aspiring for independence rather than by Arab nationalists. In 1968, the British announced their withdrawal from Bahrain by 1971. The newly independent State of Bahrain designated Manama as the capital city.
Post-independence Manama was characterised by the rapid urbanisation of the city and the swallowing-up of neighboring villages and hamlets into a single urbanised area, incorporating new neighbourhoods such as Adliya and Salmaniya. The construction boom attracted large numbers of foreigners from the Indian subcontinent and by 1981, foreigners outnumbered Bahrainis two-to-one. The construction of the Diplomatic Area district in the city's northeast helped facilitate diversification of the country's economy from oil by exploiting the lucrative financial industry. Financial institutions in the district numbered 187 by 1986. The scarcity of land suitable for construction led to land reclamation. Religious activism migrated from Manama to the suburban districts of Bani Jamra, Diraz and Bilad Al Qadeem, hotspots of unrest in the 1990s uprising that called for the reinstatement of an elected parliament. In 2001, the National Action Charter, presented by King Hamad bin Isa al-Khalifa was approved by Bahrainis. The charter led to the first parliamentary and municipal elections in decades. Further elections in 2006 and 2010 led to the election of Islamist parties, Al Wefaq, Al Menbar, and Al Asalah, as well as independent candidates. In 2011, a month-long uprising led to the intervention of GCC forces and the proclamation of a three-month state of emergency. The Bahrain Independent Commission of Inquiry published a 500-page report on the events of 2011.

Historically, Manama has been restricted to what is now known as the Manama Souq and the Manama Fort (now the Ministry of Interior) to its south. However the city has now grown to include a number of newer suburban developments as well as older neighboring villages that have been engulfed by the growth of the city. The districts that make up Manama today include:
Manama is part of the Capital Governorate, one of five Governorates of Bahrain. Until 2002 it was part of the municipality of Al-Manamah. Councils exist within the governorates; eight constituencies are voted upon within Capital Governorate in 2006.

Manama is the focal point of the Bahraini economy. While petroleum has decreased in importance in recent years due to depleting reserves and growth in other industries, it is still the mainstay of the economy. Heavy industry (e.g. aluminium smelting, ship repair), banking and finance, and tourism are among the industries which have experienced recent growth. Several multinationals have facilities and offices in and around Manama. The primary industry in Manama itself is financial services, with over two hundred financial institutions and banks based in the CBD and the Diplomatic Area. Manama is a financial hub for the Persian Gulf region and a center of Islamic banking. There is also a large retail sector in the shopping malls around Seef, while the center of Manama is dominated by small workshops and traders.

Manama's economy in the early 20th century relied heavily on pearling; in 1907, the pearling industry was estimated to include 917 boats providing employment for up to 18,000 people. Shipbuilding also employed several hundred in both Manama and Muharraq. The estimated income earned from pearling in 1926 and subsequent years prior to the Great Depression was £1.5 million annually. Custom duties and tariffs served as the prime source of revenue for the government. With the onset of the Great Depression, the collapse of the pearling industry and the discovery of oil in 1932, the country's economy began to shift towards oil.

Historically, the ports at Manama were of poor reputation. The British described the ports importing systems as being "very bad – goods were exposed to the weather and there were long delays in delivery", in 1911. Indians began maintaining the ports and new resources were built on site, improving the situation. As of 1920, Manama was one of the main exporters of Bahrain pearls, attracting steamships from India. During this time, they also imported goods from India and from other regional countries. They imported rice, textiles, ghee, coffee, dates, tea, tobacco, fuel, and livestock. They exported less of a variety, with a focus on pearls, oysters, and sailcloth. For the year of 1911–12, Manama was visited by 52 steamships, the majority being British and the rest Turkish-Arabian.

The role of Manama as a regional port city in the Persian Gulf made it a hub for migrant workers in search of a better living. As a result, Manama has often been described, both in the pre-oil and post-oil era, as a cosmopolitan city. In 1904, it was estimated that Manama's population numbered 25,000, out of which half were believed to have been foreigners from Basra, Najd, al-Hasa and Iran, as well as from India and Europe.

The two main branches of Islam, Shia Islam and Sunni Islam, coexisted in Manama for centuries and are represented by distinct ethnic groups. The Shia community is represented by the native Arab Baharna, the Hasawis and Qatifis of mainland Arabia and the Persian Ajam. The Sunni community is represented by Arab Bedouin tribes who migrated in the eighteenth century along with the Bani Utbah and the Huwala, Arabic-speaking Persians. There is also a sizable native Bahraini Christian population in the country, numbering more than a thousand, in addition to immigrant Hindus and a small native Jewish community numbering 37.

Manama is the main hub of the country's road network. At the moment the city's road network is undergoing substantial development to ameliorate the situation of traffic in the city. Due to the fact that it is the capital and the main city in the country, where most of the government and the commercial offices and facilities are established, along with the entertainment centers, and the country's fast growth, vehicle population is increasing rapidly.

The widening of roads in the old districts of Manama and the development of a national network linking the capital to other settlements commenced as early as the arrival of the first car in 1914. The continuous increase in the number of cars from 395 in 1944, to 3,379 in 1954 and to 18,372 cars in 1970 caused urban development to primarily focus on expanding the road network, widening carriageways and the establishment of more parking spaces. Many tracks previously laid in the pre-oil era (prior to the 1930s) were resurfaced and widened, turning them into 'road arteries'. Initial widening of the roads started in the Manama Souq district, widening its main roads by demolishing encroaching houses.

A series of ring roads were constructed (Isa al Kabeer avenue in the 1930s, Exhibition avenue in the 1960s and Al Fateh highway in the 1980s), to push back the coastline and extend the city area in belt-like forms. To the north, the foreshore used to be around "Government Avenue" in the 1920s but it shifted to a new road, "King Faisal Road", in the early 1930s which became the coastal road. To the east, a bridge connected Manama to Muharraq since 1929, a new causeway was built in 1941 which replaced the old wooden bridge. Transits between the two islands peaked after the construction of the Bahrain International Airport in 1932.

To the south of Manama, roads connected groves, lagoons and marshes of Hoora, Adliya, Gudaibiya and Juffair. Villages such as Mahooz, Ghuraifa, Seqaya served as the end of these roads. To the west, a major highway was built that linked Manama to the isolated village port of Budaiya, this highway crossed through the 'green belt' villages of Sanabis, Jidhafs and Duraz. To the south, a road was built that connected Manama to Riffa. The discovery of oil accelerated the growth of the city's road network.

The four main islands and all the towns and villages are linked by well-constructed roads. There were of roadways in 2002, of which were paved. A causeway stretching over , connect Manama with Muharraq Island, and another bridge joins Sitra to the main island. A four-lane highway atop a causeway, linking Bahrain with the Saudi Arabian mainland via the island of Umm an-Nasan was completed in December 1986, and financed by Saudi Arabia. In 2000, there were 172,684 passenger vehicles and 41,820 commercial vehicles.

Bahrain's port of Mina Salman can accommodate 16 oceangoing vessels drawing up to . In 2001, Bahrain had a merchant fleet of eight ships of 1,000 GRT or over, totaling 270,784 GRT. Private vehicles and taxis are the primary means of transportation in the city.

Manama has a recently reformed comprehensive bus service that launched on 1 April 2015, with a fleet of 141 MAN buses. Regulated by the Ministry of Transportation, bus routes extend across Bahrain and around Manama with fares of a minimum 200 Fils (BD0.200) (around $0.50(USD); £0.30).

Bahrain International Airport is located on the nearby Muharraq Island, approximately from the CBD. It is a premier hub airport in the Middle East. Strategically located in the Northern Persian Gulf between the major markets of Saudi Arabia and Iran, the airport has one of the widest range and highest frequency of regional services with connections to major international destinations in Europe, Asia, Africa, and North America.
Bahrain also has a military airbase, the Isa Air Base, located in the south at Sakhir. This is the base of the Bahrain Defence Force, or BDF.

Quranic schools were the only source of education in Bahrain prior to the 20th century; such schools were primarily dedicated to the study of the Qur'an. The first modern school to open in the country was a missionary elementary school set up in 1892 (according to one account) in Manama by the Reformed Church in America, with the school's syllabus comprising English, Mathematics and the study of Christianity. Leading merchants in the country sent their children to the school until it was closed down in 1933 due to financial difficulties. The school reopened some years later under the name of Al Raja School where it operates till the present day. In addition to the American Mission School, another foreign private school was opened in 1910; Al-Ittihad school, funded by the Persian community of Bahrain.

Following the end of the First World War, Western ideas became more widespread in the country, culminating in the opening of the first public school of Bahrain, Al-Hidaya Al-Khalifia Boys school, in the island of Muharraq in 1919. The school was founded by prominent citizens of Muharraq and was endorsed by the Bahraini royal family. The country's first Education Committee was established by several leading Bahraini merchants, headed by Shaikh Abdulla bin Isa Al-Khalifa, the son of the then-ruler of Bahrain Isa ibn Ali Al Khalifa, who acted as the de facto Minister of Education. The Education Committee was also responsible for managing the Al-Hidaya Boys school. The school was, in fact, the brainchild of Shaikh Abdulla, who suggested the idea after returning from post-World War I celebrations in England.

In 1926, a second public school for boys opened up in Manama called the Jafaria School. Two years later, in 1928, the first public school for girls was established. Due to financial constraints suffered by the Education Committee, the Bahraini government took control of the schools in 1930.

Presently, Manama has a wide range of private and public universities and colleges such as Ahlia University, Applied Science University, Arab Open University, Arabian Gulf University, Bahrain Institute of Banking and Finance, and the College of Health and Sport Sciences. Other notable primary and secondary schools situated in the city include the Bahrain School, the Indian School, Al Raja School amongst others.

The city is located in the north-eastern corner of Bahrain on a small peninsula. As in the rest of Bahrain, the land is generally flat (or gently rolling) and arid.

Manama has an arid climate. In common with the rest of Bahrain, Manama experiences extreme climatic conditions, with summer temperatures up to , and winter as low as with even hail on rare occasions. Average temperatures of the summer and winter seasons are generally from about 17 °C (63 °F) to about 34 °C (93 °F). The most pleasant time in Bahrain is autumn when sunshine is comparatively low, coupled with warm temperatures tempered by soft breezes.
The country attracts a large number of foreigners and foreign influences, with just under one-third of the population hailing from abroad. Alcohol is legal in the country, with bars and nightclubs operating in the city. Bahrain gave women the right to vote in elections for the first time in 2002. Football is the most popular sport in Manama (and the rest of the country), with three teams from Manama participating in the Bahraini Premier League.

Notable cultural sites within Manama include the Bab Al Bahrain and the adjacent souq area. In the 2010s, the historic core of Manama underwent revitalisation efforts alongside the Manama souq, which are due to be completed in 2020. The central areas of Manama are also the main location for Muharram processions in the country, attracting hundreds of thousands of people annually from Bahrain and across the Gulf.


 These student protests were led by intellectuals and poets such as Qassim Haddad.





</doc>
<doc id="20484" url="https://en.wikipedia.org/wiki?curid=20484" title="Mance Lipscomb">
Mance Lipscomb

Mance Lipscomb (April 9, 1895 – January 30, 1976) was an American blues singer, guitarist and songster. He was born Beau De Glen Lipscomb near Navasota, Texas. As a youth he took the name Mance (short for "emancipation") from a friend of his oldest brother, Charlie.

Lipscomb was born April 9, 1895. His father was an ex-slave from Alabama; his mother was half African American. For most of his life, Lipscomb supported himself as a tenant farmer in Texas. He had started playing guitar at an early age and became an accomplished musician.

He was discovered and recorded by Mack McCormick and Chris Strachwitz in 1960, during a revival of interest in the country blues. He recorded many albums of blues, ragtime, Tin Pan Alley, and folk music (most of them released by Strachwitz's Arhoolie Records), singing and accompanying himself on acoustic guitar. Lipscomb had a "dead-thumb" finger-picking guitar technique and an expressive voice. He honed his skills by playing in nearby Brenham, Texas, with a blind musician, Sam Rogers. 

His first release was the album "Texas Songster" (1960). Lipscomb performed songs in a wide range of genres, from old songs such as "Sugar Babe" (the first he ever learned), to pop numbers like "Shine On, Harvest Moon" and "It's a Long Way to Tipperary".

In 1961 he recorded the album "Trouble in Mind", released by Reprise Records. In May 1963, he appeared at the first Monterey Folk Festival, (which later became the Monterey Pop Festival) alongside other folk artists such as Bob Dylan, and Peter, Paul and Mary in California.

Unlike many of his contemporaries, Lipscomb did not record in the early blues era. His life is well documented thanks to his autobiography, "I Say Me for a Parable: The Oral Autobiography of Mance Lipscomb, Texas Bluesman", narrated to Glen Alyn (published posthumously). He was the subject of a short 1971 documentary film by Les Blank, called "A Well Spent Life".

He began playing the guitar at an early age and played regularly for years at local gatherings, mostly what he called "Saturday night suppers" hosted by someone in the area. He and his wife regularly hosted such gatherings for a while. Until around 1960, most of his musical activity took place within what he called his "precinct", the area around Navasota.

Following his discovery by McCormick and Strachwitz, Lipscomb became an important figure in the American folk music revival of the 1960s. He was a regular performer at folk festivals and folk-blues clubs around the United States, notably the Ash Grove in Los Angeles, California.

He died in Navasota in 1976, two years after suffering a stroke.


An annual Navasota Blues Festival is held in his honor. On August 12, 2011, a bronze sculpture of him was unveiled in Mance Lipscomb Park in Navasota. The statue was sculpted by the artist Sid Henderson of California and weighs almost 300 pounds. It portrays Lipscomb playing his guitar whilst seated on a bench, with room for fans to sit beside him and play their own guitars at his side.




</doc>
<doc id="20485" url="https://en.wikipedia.org/wiki?curid=20485" title="Melbourne Cup">
Melbourne Cup

The Melbourne Cup is Australia's most famous annual Thoroughbred horse race. It is a 3200-metre race for three-year-olds and over, conducted by the Victoria Racing Club on the Flemington Racecourse in Melbourne, Victoria as part of the Melbourne Spring Racing Carnival. It is the richest "two-mile" handicap in the world and one of the richest turf races. The event starts at 3:00 pm on the first Tuesday of November and is known locally as "the race that stops the nation".

The Melbourne Cup has a long tradition, with the first race held in 1861. It was originally over but was shortened to in 1972 when Australia adopted the metric system. This reduced the distance by , and Rain Lover's 1968 race record of 3:19.1 was accordingly adjusted to 3:17.9. The present record holder is the 1990 winner Kingston Rule with a time of 3:16.3.

The race is a quality handicap for horses three years old and over, run over a distance of 3200 metres, on the first Tuesday in November at Flemington Racecourse. The minimum handicap weight is 50 kg. There is no maximum weight, but the top allocated weight must not be less than 57 kg. The weight allocated to each horse is declared by the VRC Handicapper in early September.

The Melbourne Cup race is a handicap contest in which the weight of the jockey and riding gear is adjusted with ballast to a nominated figure. Older horses carry more weight than younger ones and weights are adjusted further according to the horse's previous results.

Weights were theoretically calculated to give each horse an equal winning chance in the past, but in recent years the rules were adjusted to a "quality handicap" formula where superior horses are given less severe weight penalties than under pure handicap rules.

After the declaration of weights for the Melbourne Cup, the winner of any handicap flat race of the advertised value of A$55,000 or over to the winner, or an internationally recognised Listed, Group, or Graded handicap flat race, shall carry such additional weight (if any), for each win, as the VRC Handicapper shall determine.

Entries for the Melbourne Cup usually close during the first week of August. The initial entry fee is $600 per horse. Around 300 to 400 horses are nominated each year, but the final field is limited to 24 starters. Following the allocation of weights, the owner of each horse must on the four occasions before the race in November, declare the horse as an acceptor and pay a fee. First acceptance is $960, second acceptance is $1,450 and third acceptance is $2,420. The final acceptance fee, on the Saturday prior to the race, is $45,375. Should a horse be balloted out of the final field, the final declaration fee is refunded.

The race directors may exclude any horse from the race or exempt any horse from the ballot on the race, but in order to reduce the field to the safety limit of 24, horses are balloted out based on a number of factors which include prize money earned in the previous two years, wins or placings in certain lead-up races and allocated handicap weight

The winner of the following races are exempt from any ballot:


The limitation of 24 starters is stated explicitly to be for safety reasons. However, in the past far larger numbers were allowed - the largest field ever raced was 39 runners in 1890.

International horses (New Zealand not included) entered for the Melbourne Cup must undergo quarantine in an approved premises in their own country for a minimum period of 14 days before travelling to Australia. The premises must meet the Australian Government Standards. The Werribee International Horse Centre at Werribee racecourse is the Victorian quarantine station for international horses competing in the Melbourne Spring Racing Carnival. The facility has stabling for up to 24 horses in five separate stable complexes and is located 32 km from the Melbourne CBD.

The total prize money for the 2019 race is A$8,000,000, plus trophies valued at $250,000. The first 12 past the post receive prize money, with the winner being paid $4.4 million, second $1.1 million, third $550,000, fourth $350,000, fifth $230,000, with sixth through to twelfth place earning $160,000. Prize money is distributed to the connections of each horse in the ratio of 85 percent to the owner, 10 percent to the trainer and 5 percent to the jockey.

The 1985 Melbourne Cup, won by "What a Nuisance", was the first race run in Australia with prize money of $1 million.

The Cup currently has a $500,000 bonus for the owner of the winner if it has also won the group one Irish St. Leger run the previous September.

The winner of the first Melbourne Cup in 1861 received a gold watch. The first Melbourne Cup trophy was awarded in 1865 and was an elaborate silver bowl on a stand that had been manufactured in England. The first existing and un-altered Melbourne Cup is from 1866, presented to the owners of The Barb; as of 2013, it is in the National Museum of Australia. The silver trophy presented in 1867, now also in the National Museum of Australia, was also made in England but jewellers in Victoria complained to the Victorian Racing Club that the trophy should have been made locally. They believed the work of Melbournian, William Edwards, to be superior in both design and workmanship to the English made trophy. No trophy was awarded to the Melbourne Cup winner for the next eight years.

In 1876 Edward Fischer, an immigrant from Austria produced the first Australian-made trophy. It was an Etruscan shape with two handles. One side depicted a horse race with the grandstand and hill of Flemington in the background. The opposite side had the words "Melbourne Cup, 1876" and the name of the winning horse. A silver-plated base sporting three silver horses was added in 1888, but in 1891 the prize changed to being a , trophy showing a Victory figure offering an olive wreath to a jockey. From 1899 the trophy was in the form of silver galloping horse embossed on a plaque, although it was said to look like a greyhound by some people.

The last Melbourne Cup trophy manufactured in England was made for the 1914 event. It was a chalice centred on a long base which had a horse at each end. The trophy awarded in 1916, the first gold trophy, was a three-legged, three-armed rose bowl. The three-handled loving cup design was first awarded in 1919. In that year the Victorian Racing Club had commissioned James Steeth to design a trophy that would be in keeping with the prestige of the race, little realising that it would become the iconic Melbourne Cup still presented today. In the Second World War years (1942, 1943 and 1944) the winning owner received war bonds valued at 200 pounds.

A new trophy is struck each year and becomes the property of the winning owner. In the event of a dead heat, a second cup is on hand. The present trophy is hand spun from 1.65 kg of 18-carat gold. The winning trainer and jockey also receive a miniature replica of the cup (since 1973) and the strapper is awarded the Tommy Woodcock Trophy, named after the strapper of Phar Lap.

In 2003 an annual tour of the Melbourne Cup trophy was initiated to provide communities across Australia and New Zealand with an opportunity to view the Cup trophy and highlight the contribution the Melbourne Cup has made to Australia's social, sporting and racing culture. Each year, communities in Australia and New Zealand apply for the cup to tour their community and the tour also takes in cities around the world as part of the Victoria Racing Club's strategy to promote the Melbourne Cup and the Melbourne Cup Carnival internationally.

The Tour has visited schools and aged-care and hospital facilities, and participated in community events and celebrations including race days across Australia and New Zealand.

Frederick Standish, member of the Victorian Turf Club and steward on the day of the first Cup, was credited with forming the idea to hold a horse race and calling it the "Melbourne Cup".

Seventeen horses contested the first Melbourne Cup on Thursday 7 November 1861, racing for the modest prize of 710 gold sovereigns (£710) cash and a hand-beaten gold watch, winner takes all. The prize was not, as some have suggested, the largest purse up to that time. A large crowd of 4,000 men and women watched the race, although it has been suggested this was less than expected because of news reaching Melbourne of the death of explorers Burke and Wills five days earlier on 2 November. Nevertheless, the attendance was the largest at Flemington on any day for the past two years, with the exception of the recently run Two Thousand Guinea Stakes.

The winner of this first Melbourne Cup race was a 16.3 hand bay stallion by the name of Archer in a time of 3.52.00, ridden by John Cutts, trained by Etienne de Mestre, and leased (and consequently raced in his own name) by de Mestre. As a lessee de Mestre "owned" and was fully responsible for Archer during the lease. Archer was leased from the "Exeter Farm" of Jembaicumbene near Braidwood, New South Wales. His owners were Thomas John "Tom" Roberts (a good school-friend of de Mestre's), Rowland H. Hassall (Roberts' brother-in-law), and Edmund Molyneux Royds and William Edward Royds (Roberts' nephews).

The inaugural Melbourne Cup of 1861 was an eventful affair when one horse bolted before the start, and three of the seventeen starters fell during the race, two of which died. Archer, a Sydney "outsider" who drew scant favour in the betting, spread-eagled the field and defeated the favourite, and Victorian champion, Mormon by six lengths. Dismissed by the bookies, Archer took a lot of money away from Melbourne, 'refuelling interstate rivalry' and adding to the excitement of the Cup. The next day, Archer was raced in and won another 2-mile long distance race, the Melbourne Town Plate.

It has become legend that Archer walked over 800 km (over 500 miles) to Flemington from de Mestre's stable at "Terara" near Nowra, New South Wales. However, newspaper archives of the day reveal that he had travelled south from Sydney to Melbourne on the steamboat "City of Melbourne", together with de Mestre, and two of de Mestre's other horses Exeter and Inheritor. Before being winched aboard the steamboat for the trip to Melbourne, the horses had arrived in Sydney in September 1861.

Archer traveled to Melbourne by steamboat again the following year (1862) to run in the second Melbourne Cup. This time he won 810 gold sovereigns (£810) cash and a gold watch before a crowd of 7,000, nearly twice the size of the previous years large crowd in a time of 3.47.00, taking to two the number of Melbourne Cup wins by this horse. Archer had already won the 1862 AJC Queen Elizabeth Stakes in Randwick, Sydney, and returned to win his second Melbourne Cup carrying 10 stone 2 pounds. He defeated a field of twenty starters by eight lengths, a record that has never been beaten, and that was not matched for over 100 years. Mormon again running second. Winning the Melbourne Cup twice was a feat not repeated until more than seventy years later when Peter Pan won the race in 1932 and 1934, and winning the Melbourne Cup two years in a row was a feat not repeated until more than 30 years later when Rain Lover won in 1968 and 1969.

Archer traveled to Melbourne by steamboat yet again the next year (1863). Despite his weight of 11 stone 4 pounds, Archer would have contested the third cup in 1863, but due to a Victorian public holiday trainer Etienne de Mestre's telegraphed acceptance form arrived late, and Archer was scratched on a technicality. In protest of this decision and in a show of solidarity, many of de Mestre's owners boycotted the third race and scratched their horses in sympathy. As a result, the Melbourne Cup of that year ran with only 7 starters, the smallest number in the history of the Cup.

In 1865, Adam Lindsay Gordon wrote a verse in which the Melbourne Cup winner was called Tim Whiffler. Two years later in 1867 two horses with the name Tim Whiffler ran in the Melbourne Cup. (The year before in 1866 two horses with the same name, Falcon, also ran in the Melbourne Cup.) To distinguish between the two Tim Whifflers they were called "Sydney" Tim Whiffler and "Melbourne" Tim Whiffler. "Sydney" Tim Whiffler actually won the Cup. He was trained by Etienne de Mestre, and like Archer before him raced in de Mestre's name but was leased from the "Exeter Farm".

As early as 1865, Cup day was a half-holiday in Melbourne for public servants and bank officials. Various businesses also closed at lunchtime.

It took some years before the purpose of the declared holiday was acknowledged in the Victoria Government Gazette. The Gazette of 31 October 1873 announced that the following Thursday (Cup Day) be observed as a bank and civil (public) service holiday.

The Melbourne Cup was first run on a Tuesday in 1875, the first Tuesday in that month.

On 7 November 1876, the three-year-old filly, Briseis, owned and trained by James Wilson Snr., won in a time of 3.36.25. Briseis then went on to create a record that is never likely to be equalled, winning the VRC Derby, the Melbourne Cup and the VRC Oaks in the space of six days. She was ridden in the Melbourne Cup by the tiny featherweight figure of jockey Peter St. Albans. In 1876 at the recorded age thirteen (he was actually twelve, being 8 days short of his thirteenth birthday), Peter St. Albans is also the youngest person ever to win a Melbourne Cup. Before 75,000 at Flemington Briseis, with St Albans in the saddle, comfortably won by 1 length in the biggest field of all time. "At 4 o'clock the starter released the 33 runners and they swept down the long Flemington straight in a thundering rush. Briseis, ridden by what one writer termed a mere child, (in the Cup) captured a rare double, the Victoria Race Club Derby and the Melbourne Cup. Shouts and hurrahs were heard, hats were thrown in the air and one excited individual fell on his back in the attempt to do a somersault. The boy who rode the winner was carried around the pack and is the hero of the day," reported the "Australasian Sketcher" in 1876. Both Peter St. Albans and Briseis have now become racing legends, and Briseis is regarded as one of the greatest mares foaled in Australia.

Briseis wasn't the only sensation surrounding the 1876 Melbourne Cup. Two months before the event, on Saturday 9 September, the "City of Melbourne" sailed for Melbourne from Sydney with a cargo including 13 racehorses, many of whom were considered serious contenders for the Melbourne Cup. The following day the ship ran into a savage storm and was hit by several rogue waves, with Nemesis (the winner of the 1876 AJC Metropolitan Handicap in Randwick, Sydney and favourite for the Cup, owned by John Moffat) and Robin Hood (another favourite, owned by Etienne de Mestre) being among the 11 horses that were killed. Betting on the big race was paralysed. To the dismay and anger of the public, bookmakers, showing no feeling, presented a purse (loaded with coins) to the captain as token of their appreciation for his part in saving them many thousands of pounds in bets already laid on the favourites who had perished. Perhaps they should have kept their money, however. The outsider Briseis comfortably won by 1 length in the biggest field of all time and is an extremely good time, so it is unlikely that the horses who perished could have beaten her.

1877 is also the year that the trainer Etienne de Mestre won his fourth Melbourne Cup with Chester owned by Hon. James White. In 1878, as in previous years, De Mestre fielded more than one horse. He entered the favourite Firebell (owned by W.S. Cox) who finished last, Chester (owned by Hon. James White) the previous year's winner who fell, and Calamia (owned by de Mestre) who, though less fancied, won easily by two lengths. First prize was £1,790, the crowd was 80,000 and there were 30 starters. De Mestre's 1878 win with Calamia brought to 5 the number of Melbourne Cups he had won. This record was not to be matched for nearly 100 years when the trainer Bart Cummings won his fifth Melbourne Cup in 1975. Bart Cummings, regarded as the best Australian horse trainer of all time, went on to win 12 Melbourne Cups to 2008.

In 1883, the hardy New Zealand bred, Martini-Henry won the VRC Derby, the Melbourne Cup and on the following Monday retained his undefeated record by winning Mares' Produce Stakes.

Phar Lap, the most famous horse in the world of his day, won the 1930 Melbourne Cup at 11/8 odds on, the shortest-priced favourite in the history of the race. He had to be hidden away at Geelong before the race after an attempt was made to shoot him and only emerged an hour before the race time of the Cup. Phar Lap also competed in 1929 and 1931, but came 3rd and 8th respectively, despite heavy favouritism in both years.

There are a few legends of the first Aboriginal jockey to ride in a Melbourne Cup. It was believed to be John Cutts who won the first and second cups in 1861 and 1862 riding Archer. He was reputedly an Aboriginal stockman born in the area where Archer was trained but was actually John 'Cutts' Dillon, the son of a Sydney clerk, a jockey who rode for many trainers in his long career, and who was one of the best known, best-liked and most respected jockeys in New South Wales. It is thought that Peter St. Albans was the first Aboriginal jockey to win the cup, on Briseis in 1876. Because St. Albans was not quite 13 years old, the jockey was too young to ride in the cup. Thus, to allow him to race Briseis in the Cup, it was argued his birthdate and parents were unknown, and from this, the legend of him being Aboriginal grew. Both these legends, however, can definitely be disproved, and history had to wait nearly another 100 years. The first jockey of Indigenous heritage to ride a Melbourne Cup winner was Frank Reys in 1973 on Gala Supreme, who had a Filipino father and a half-Aboriginal mother.

The race has undergone several alterations in recent years, the most visible being the entry of many foreign-trained horses. Most have failed to cope with the conditions; the three successful "foreign raids" include two by Irish trainer Dermot K. Weld successful in 1993 and 2002, and one in 2006 by Katsumi Yoshida of Japan's renowned Yoshida racing and breeding family. The attraction for foreigners to compete was, primarily, the low-profile change to the new "quality handicap" weighting system.
The 1910 Melbourne Cup was won by Comedy King, the first foreign bred horse to do so. Subsequent foreign bred horses to win Cup were Backwood 1924; Phar Lap 1930; Wotan 1936; Beldale Ball 1980; At Talaq 1986; Kingston Rule 1990; Vintage Crop 1993; Jeune 1994; Media Puzzle 2002; Makybe Diva 2003, 2004, 2005; Americain 2010 and Dunaden 2011.

The 1938 Melbourne Cup was won by trainer Mrs. Allan McDonald, who conditioned Catalogue. Mrs McDonald was a successful trainer in New Zealand, however, at the time women were not allowed to compete as trainers in Australia so her husband's name was officially recorded as the winning trainer. The 2001 edition was won by New Zealand mare Ethereal, trained by Sheila Laxon, the first woman to formally train a Melbourne Cup winner. She also won the Caulfield Cup, a 2,400-metre race also held in Melbourne, and therefore has won the "Cups Double".
Maree Lyndon became the first female to ride in the Melbourne Cup, when she partnered Argonaut Style in 1987, in which she ran second last in the 21 horse field.

In 2004, Makybe Diva became the first mare to win two cups, and also the first horse to win with different trainers, after David Hall moved to Hong Kong and transferred her to the Lee Freedman stables.

The 2005 Melbourne Cup was held before a crowd of 106,479. Makybe Diva made history by becoming the only horse to win the race three times. Trainer Lee Freedman said after the race, "Go and find the youngest child on the course because that's the only person here who will have a chance of seeing this happen again in their lifetime."

Due to the 2007 Australian equine influenza outbreak, believed to have been started by a horse brought into Australia from Japan, neither Delta Blues nor Pop Rock participated in the 2007 Melbourne Cup. Both horses had been stabled in Japan. Corowa, NSW trained "Leica Falcon" also was not be permitted to race in Victoria, despite Corowa being close to the Victorian border. Leica Falcon was ordained as the new staying star of Australian racing in 2005 when he ran fourth in both the Caulfield Cup and in Makybe Diva's famous third Melbourne Cup victory. But serious leg injuries saw the horse not race for another 20 months. Efficient, the previous year's VRC Derby winner, won the race.

In 2013, Damien Oliver returned from an eight-month ban, after betting against his own mount at a previous race meet, to win his 3rd Melbourne cup.

The 2019 Melbourne Cup was overshadowed by recent news of the ill-treatment of horses in the Australian racing industry, and by the pulling out of notable celebrities including Taylor Swift, Megan Gale, and X-Men actress Lana Condor.



Melbourne Cup day is a public holiday for all working within metropolitan Melbourne and some parts of regional Victoria, but not for some country Victorian cities and towns which hold their own spring carnivals. For federal public servants it is also observed as a holiday in the entire state of Victoria, and from 2007 to 2009 also in the Australian Capital Territory known as Family and Community Day replacing Picnic Day. The Melbourne cup captures the public's imagination to the extent that people, whether at work, home, school, or out and about, usually stop to watch or listen to the race. Many people from outside of Melbourne take a half or full day off work to celebrate the occasion. Many people feel that the day should be a national public holiday as sick leave is said to increase on the day and productivity wanes.

As early as 1865, Cup Day was a half-holiday in Melbourne for public servants and bank officials. Various businesses also closed at lunchtime.

It took some years before the purpose of the declared holiday was acknowledged in the Victoria Government Gazette. The Gazette of 31 October 1873 announced that the following Thursday (Cup Day) be observed as a bank and civil (public) service holiday.





The event is one of the most popular spectator events in Australia, with sometimes over 110,000 people, some dressed in traditional formal raceday wear and others in all manner of exotic and amusing costumes, attending the race. The record crowd was 122,736 in 2003. The 1926 running of the Cup was the first time the 100,000 mark had been passed. Today the record at Flemington is held by the 2006 Victoria Derby when almost 130,000 attended.

In 2007, a limit was placed on the Spring Carnival attendance at Flemington Racecourse and race-goers are now required to pre-purchase tickets. Every year more and more people travel to Flemington Racecourse, in 2016 there was a 7.8 per cent increase in the number of out-of-state individuals (80,472) attending the Melbourne Cup Carnival;

'Fashions on the Field' is a major focus of the day, with substantial prizes awarded for the best-dressed man and woman. The requirement for elegant hats, and more recently the alternative of a fascinator, almost single-handedly keeps Melbourne's milliners in business. Raceday fashion has occasionally drawn almost as much attention as the race itself, The miniskirt received worldwide publicity when model Jean Shrimpton wore a white shift version of one on Derby Day during Melbourne Cup week in 1965.

Flowers, especially roses are an important component of the week's racing at Flemington. The racecourse has around 12,000 roses within its large expanse. Over 200 varieties of the fragrant flower are nurtured by a team of up to 12 gardeners. Each of the major racedays at Flemington has an official flower. Victoria Derby Day has the Corn Flower, Melbourne Cup Day is for the Yellow Rose, Oaks Day highlights the Pink Rose and Stakes Day goes to the Red Rose.

In the Melbourne metropolitan area, the race day has been a gazetted public holiday since 1877, but around both Australia and New Zealand a majority of people watch the race on television and gamble, either through direct betting or participating in workplace cup "sweeps". In 2000, a betting agency claimed that 80 percent of the adult Australian population placed a bet on the race that year. In 2010 it was predicted that $183 million would be spent by 83,000 tourists during the Spring Racing Carnival. In New Zealand, the Melbourne Cup is the country's single biggest betting event, with carnival race-days held at several of the country's top tracks showing the cup live on big screens.

It is commonly billed as "The race that stops a nation", but it is more accurately "The race that stops two nations", as many people in New Zealand, as well as Australia, pause to watch the race.






</doc>
<doc id="20486" url="https://en.wikipedia.org/wiki?curid=20486" title="Messerschmitt Me 163 Komet">
Messerschmitt Me 163 Komet

The Messerschmitt Me 163 Komet was a German rocket-powered interceptor aircraft. Designed by Alexander Lippisch, it is the only rocket-powered fighter aircraft ever to have been operational and the first piloted aircraft of any type to exceed 1000 km/h (621 mph) in level flight. Its performance and aspects of its design were unprecedented. German test pilot Heini Dittmar in early July 1944 reached , an unofficial flight airspeed record unmatched by turbojet-powered aircraft for almost a decade. Over 300 Komets were built, but the aircraft proved lackluster in its dedicated role as an interceptor and destroyed between 9 and 18 Allied aircraft against 10 losses. Aside from combat losses many pilots were killed during testing and training.

Work on the design started around 1937 under the aegis of the "Deutsche Forschungsanstalt für Segelflug" (DFS)—the German Institute for the study of sailplane flight. Their first design was a conversion of the earlier Lippisch Delta IV known as the DFS 39 and used purely as a glider testbed of the airframe. A larger follow-on version with a small propeller engine started as the DFS 194. This version used wingtip-mounted rudders, which Lippisch felt would cause problems at high speed. Lippisch changed the system of vertical stabilization for the DFS 194's airframe from the earlier DFS 39's wingtip rudders, to a conventional vertical stabilizer at the rear of the aircraft. The design included a number of features from its origins as a glider, notably a skid used for landings, which could be retracted into the aircraft's keel in flight. For takeoff, a pair of wheels, each mounted onto the ends of a specially designed cross-axle, were needed due to the weight of the fuel, but the wheels, forming a takeoff dolly under the landing skid, were released shortly after takeoff.

The designers planned to use the forthcoming Walter R-1-203 "cold engine" of thrust, which like the self-contained Walter HWK 109-500 "Starthilfe" RATO booster rocket unit, used a monopropellant consisting of stabilized HTP known by the name "T-Stoff". Heinkel had also been working with Hellmuth Walter on his rocket engines, mounting them in the He 112R's tail for testing – this was done in competition with Wernher von Braun's bi-propellant, alcohol/LOX-fed rocket motors, also with the He 112 as a test airframe – and with the Walter catalyzed HTP propulsion format for the first purpose-designed, liquid-fueled rocket aircraft, the He 176. Heinkel had also been selected to produce the fuselage for the DFS 194 when it entered production, as it was felt that the highly volatile monopropellant fuel's reactivity with organic matter would be too dangerous in a wooden fuselage structure. Work continued under the code name "Projekt X".

The division of work between DFS and Heinkel led to problems, notably that DFS seemed incapable of building even a prototype fuselage. Lippisch eventually asked to leave DFS and join Messerschmitt instead. On 2 January 1939, he moved with his team and the partly completed DFS 194 to the Messerschmitt works at Augsburg. The delays caused by this move allowed the engine development to catch up. Once at Messerschmitt, the team decided to abandon the propeller-powered version and move directly to rocket-power. The airframe was completed in Augsburg and in early 1940 was shipped to receive its engine at Peenemünde-West, one of the quartet of "Erprobungsstelle"-designated military aviation test facilities of the Reich. Although the engine proved to be extremely unreliable, the aircraft had excellent performance, reaching a speed of in one test.

In the Me 163B and -C subtypes, a ram-air turbine on the extreme nose of the fuselage, and the backup lead-acid battery inside the fuselage that it charged, provided the electrical power for the radio, the Revi16B, -C, or -D reflector gunsight, the direction finder, the compass, the firing circuits of the cannon, and some of the lighting in the cockpit instrumentation.

There was an onboard lead/acid battery, but its capacity was limited, as was its endurance, no more than 10 minutes, hence the fitted generator.

The airspeed indicator averaged readings from two sources: the pitot tube on the leading edge of the port wing, and a small pitot inlet in the nose, just above the top edge of the underskid channel. There was a further tapping-off of pressure-ducted air from the pitot tube which also provided the rate of climb indicator with its source.

The resistance group around the later executed Austrian priest Heinrich Maier had contacts with the Heinkelwerke in Jenbach in Tyrol, where important components for the Messerschmitt Me 163 were also produced. The group passed on relevant information to the Allies. With the location sketches of the production facilities, the Allied bombers were able to carry out targeted air strikes.

In early 1941 production of a prototype series, known as the "Me 163", began. Secrecy was such that the RLM's "GL/C" airframe number, "8-163", was actually that of the earlier Messerschmitt Bf 163. Three Bf 163-prototypes (V-1-V3) were built. It was thought that intelligence services would conclude any reference to the number "163" would be for that earlier design. In May 1941, the first prototype Me 163A, V4, was shipped to Peenemünde to receive the HWK RII-203 engine. By 2 October 1941, Me 163A V4, bearing the radio call sign letters, or "Stammkennzeichen", "KE+SW", set a new world speed record of , piloted by Heini Dittmar, with no apparent damage to the aircraft during the attempt. Some postwar aviation history publications stated that the Me 163A V3 was thought to have set the record.

The record figure was only officially surpassed after the war, by the American Douglas D-558-1 on 20 August 1947. Ten Me 163As (V4-V13) were built for pilot training and further tests
During testing of the prototype (A-series) aircraft, the jettisonable undercarriage presented a serious problem. The original dollies possessed well-sprung independent suspension for each wheel, and as the aircraft took off, the large springs rebounded and threw the dolly upward, striking the aircraft. The production (B-series) aircraft used much simpler, crossbeam-axled dollies, and relied on the landing skid’s oleo-pneumatic strut to absorb ground-running impacts during the takeoff run, as well as to absorb the shock of landing. If the hydraulic cylinder was malfunctioning, or the skid mistakenly left during a landing procedure in the "locked and lowered" position (as it had to be for takeoff), the impact of a hard touchdown on the skid could cause back injuries to the pilot.

Once on the ground, the aircraft had to be retrieved by a "Scheuch-Schlepper", a converted small agricultural vehicle, originally based on the concept of the two-wheel tractor, carrying a detachable third swiveling wheel at the extreme rear of its design for stability in normal use—this swiveling third wheel was replaced with a pivoting, special retrieval trailer that rolled on a pair of short, triple-wheeled continuous track setups (one per side) for military service wherever the "Komet" was based. This retrieval trailer usually possessed twin trailing lifting arms, that lifted the stationary aircraft off the ground from under each wing whenever it was not already on its twin-wheel dolly main gear, as when the aircraft had landed on its ventral skid and tailwheel after a mission. Another form of trailer, known also to have been trialled with the later B-series examples, was tried during the "Komet"s test phase, which used a pair of sausage-shaped air bags in place of the lifting arms and could also be towed by the "Scheuch-Schlepper" tractor, inflating the air bags to lift the aircraft. The three-wheeled "Scheuch-Schlepper" tractor used for the task was originally meant for farm use, but such a vehicle with a specialized trailer—which could also lift the Me 163's airframe completely clear of the ground to effect the recovery as a normal part of the Me 163's intended use—was required as the "Komet" was unpowered after exhausting its rocket propellants, and lacked main wheels after landing, from the jettisoning of its "dolly" main gear at takeoff. The slightly larger Sd Kfz 2 "Kettenkrad" half-track motorcycle, known to be used with the Me 262 jet fighter for ground handling needs, and documented as also being used with the Arado Ar 234B jet recon-bomber, was not known to have ever been used for ground handling operations with the "Komet" at any time.

During flight testing, the superior gliding capability of the "Komet" proved detrimental to safe landing. As the now un-powered aircraft completed its final descent, it could rise back into the air with the slightest updraft. Since the approach was unpowered, there was no opportunity to make another landing pass. For production models, a set of landing flaps allowed somewhat more controlled landings. This issue remained a problem throughout the program. Nevertheless, the overall performance was tremendous, and plans were made to put Me 163 squadrons all over Germany in around any potential target. Development of an operational version was given the highest priority.

In December 1941, work on an upgraded design began. A simplified construction format for the airframe was deemed necessary, as the Me 163A version was not truly optimized for large-scale production. The result was the Me 163B subtype, which had the desired, more mass-producible fuselage, wing panel, retractable landing skid and tailwheel designs with the previously mentioned unsprung dolly takeoff gear, and a generally one-piece conical nose for the forward fuselage which could incorporate a turbine for supplementary electrical power while in flight, as well as a one-piece, perimeter frame-only hinged canopy for ease of production.
Meanwhile, Walter had started work on the newer HWK 109-509 bipropellant "hot engine", which added a true fuel of hydrazine hydrate and methanol, designated "C-Stoff", that burned with the oxygen-rich exhaust from the "T-Stoff", used as the oxidizer, for added thrust (see: List of Stoffs). The new powerplant and numerous detail design changes meant to simplify production over the general A-series airframe design resulted in the significantly modified Me 163B of late 1941. Due to the "Reichsluftfahrtministerium" requirement that it should be possible to throttle the engine, the original power plant grew complicated and lost reliability.

The fuel system was particularly troublesome, as leaks incurred during hard landings easily caused fires and explosions. Metal fuel lines and fittings, which failed in unpredictable ways, were used as this was the best technology available. Both fuel and oxidizer were toxic and required extreme care when loading in the aircraft, yet there were occasions when "Komets" exploded on the tarmac from the propellants' hypergolic nature. Both propellants were clear fluids, and different tanker trucks were used for delivering each propellant to a particular "Komet" aircraft, usually the "C-Stoff" hydrazine/methanol-base fuel first. For safety purposes, it left the immediate area of the aircraft following its delivery and capping off of the "Komet"s fuel tanks from a rear located dorsal fuselage filling point just ahead of the "Komet"s vertical stabilizer. Then, the other tanker truck carrying the very reactive "T-Stoff" hydrogen peroxide oxidizer would deliver its load through a different filling point on the "Komet"s dorsal fuselage surface, located not far behind the rear edge of the canopy.

The corrosive nature of the liquids, especially for the "T-Stoff" oxidizer, required special protective gear for the pilots. To help prevent explosions, the engine and the propellant storage and delivery systems were frequently and thoroughly hosed down and flushed with water run through the propellant tanks and the rocket engine's propellant systems before and after flights, to clean out any remnants. The relative "closeness" to the pilot of some 120 litres (31.7 US gal) of the chemically active T-Stoff oxidizer, split between two auxiliary oxidizer tanks of equal volume to either side within the lower flanks of the cockpit area—besides the main oxidizer tank of some 1,040 litre (275 US gal) volume just behind the cockpit's rear wall, could present a serious or even fatal hazard to a pilot in a fuel-caused mishap.

Two prototypes were followed by 30 Me 163 B-0 pre-production aircraft armed with two 20 mm MG 151/20 cannon and some 400 Me 163 B-1 production aircraft armed with two 30 mm (1.18 inch) MK 108 cannons, but which were otherwise similar to the B-0. Early in the war, when German aircraft firms created versions of their aircraft for export purposes, the a was added to export ("ausland") variants (B-1a) or to foreign-built variants (Ba-1) but for the Me 163, there were neither export nor a foreign-built version. Later in the war, the "a" and successive letters were used for aircraft using different engine types: as Me 262 A-1a with Jumo engines, Me 262 A-1b with BMW engines. As the Me 163 was planned with an alternative BMW P3330A rocket engine, it is likely the "a" was used for this purpose on early examples. Only one Me 163, the V10, was tested with the BMW engine, so this designation suffix was soon dropped. The Me 163 B-1a did not have any wingtip "washout" built into it, and as a result, it had a much higher critical Mach number than the Me 163 B-1.

The Me 163B had very docile landing characteristics, mostly due to its integrated leading edge slots, located directly forward of the elevon control surfaces, and just behind and at the same angle as the wing's leading edge. It would neither stall nor spin. One could fly the "Komet" with the stick full back, and have it in a turn and then use the rudder to take it out of the turn, and not fear it snapping into a spin. It would also slip well. Because the Me 163B's airframe design was derived from glider design concepts, it had excellent gliding qualities, and the tendency to continue flying above the ground due to ground effect. On the other hand, making a too close turn from base onto final, the sink rate would increase, and one could quickly lose altitude and come in short. Another main difference from a propeller-driven aircraft is that there was no slipstream over the rudder. On takeoff, one had to attain the speed at which the aerodynamic controls become effective—about —and that was always a critical factor. Pilots accustomed to flying propeller-driven aircraft had to be careful that the control stick was not somewhere in the corner when the control surfaces began working. These, like many other specific Me 163 problems, would be resolved by specific training.

The performance of the Me 163 far exceeded that of contemporary piston engine fighters. At a speed of over the aircraft would take off, in a so-called "scharfer Start" ("sharp start", with "Start" being the German word for "take-off") from the ground, from its two-wheeled dolly. The aircraft would be kept at level flight at low altitude until the best climbing speed of around was reached, at which point it would jettison the dolly, retract its extendable skid using a knob-topped release lever just forward of the throttle (as both levers were located atop the cockpit's portside 120 litre "T-Stoff" oxidizer tank) that engaged the aforementioned pneumatic cylinder, and then pull up into a 70° angle of climb, to a bomber's altitude. It could go higher if required, reaching in an unheard-of three minutes. Once there, it would level off and quickly accelerate to around or faster, which no Allied fighter could match. The usable Mach number was similar to that of the Me 262, but because of the high thrust-to-drag ratio, it was much easier for the pilot to lose track of the onset of severe compressibility and risk loss of control. A Mach warning system was installed as a result. The aircraft was remarkably agile and docile to fly at high speed. According to Rudolf Opitz, chief test pilot of the Me 163, it could "fly circles around any other fighter of its time".

By this point, Messerschmitt was completely overloaded with production of the Messerschmitt Bf 109 and attempts to bring the Me 210 into service. Production in a dispersed network was handed over to Klemm, but quality control problems were such that the work was later given to Junkers, who were, at that time, underworked. As with many German designs of World War II's later years, parts of the airframe (especially the wings) were made of wood by furniture manufacturers. The older Me 163A and first Me 163B prototypes were used for training. It was planned to introduce the Me 163S, which removed the rocket engine and tank capacity and placed a second seat for the instructor above and behind the pilot, with his own canopy. The Me 163S would be used for glider landing training, which as explained above, was essential to operate the Me 163. It appears the 163Ss were converted from the earlier Me 163B series prototypes.

In service, the Me 163 turned out to be difficult to use against enemy aircraft. Its tremendous speed and climb rate meant a target was reached and passed in a matter of seconds. Although the Me 163 was a stable gun platform, it required excellent marksmanship to bring down an enemy bomber. The "Komet" was equipped with two 30 mm (1.18 inch) MK 108 cannons which had a relatively low muzzle velocity of 540 meters per second (1,772 feet/sec), and were accurate only at short range, making it almost impossible to hit a slow moving bomber. Four or five hits were typically needed to take down a B-17.

Innovative methods were employed to help pilots achieve kills. The most promising was a weapon called the "Sondergerät 500 Jägerfaust". This included 10 single-shot, short-barreled 50 mm (2 inch) guns pointing upwards, similar to "Schräge Musik". Five were mounted in the wing roots on each side of the aircraft. A photocell in the upper surface of the "Komet" triggered the weapons by detecting the change in brightness when the aircraft flew under a bomber. As each shell shot upwards, the disposable gun barrel that fired it was ejected downwards, thus making the weapon recoilless. It appears that this weapon was used in combat only once, resulting in the destruction of a Lancaster bomber on 10 April 1945.

The biggest concern about the design was the short flight time, which never met the projections made by Walter. With only seven and a half minutes of powered flight, the fighter truly was a dedicated point defense interceptor. To improve this, the Walter firm began developing two more advanced versions of the 509A rocket engine, the 509B and C, each with two separate combustion chambers of differing sizes, one above the other, for greater efficiency. The B-version possessed a main combustion chamber—usually termed in German as a "Hauptofen" on these dual-chamber subtypes—with an exterior shape much like that on the single chamber 509A version, with the C-version having a forward chamber shape of a more cylindrical nature, designed for a higher top thrust level of some 2,000 kg (4,410 lb) of thrust, while simultaneously dropping the use of the cubic-shape frame for the forward engine propellant flow/turbopump mechanisms as used by the earlier -A and -B versions. The 509B and 509C rocket motors' main combustion chambers were supported by the thrust tube exactly as the 509A motor's single chamber had been. They were tuned for high power for takeoff and climb. The added, smaller volume lower chamber on the two later models, nicknamed the "Marschofen" with approximately of thrust at its top performance level, was intended for more efficient, lower power cruise flight. These HWK 109–509B and C motors would improve endurance by as much as 50%. Two 163 Bs, models V6 and V18, were experimentally fitted with the lower-thrust B-version of the new twin-chamber engine (mandating twin combustion chamber pressure gauges on the instrument panel of any "Komet" equipped with them), a retractable tailwheel, and tested in spring 1944.

The main combustion chamber of the 509B engine used for the B V6 and V18 occupied the same location as the A-series' engine did, with the lower "Marschofen" cruise chamber housed within the retractable tailwheel's appropriately widened ventral tail fairing. On 6 July 1944, the Me 163B V18 (VA+SP), like the B V6 basically a standard production Me 163B airframe outfitted with the new, twin-chamber "cruiser" rocket motor with the aforementioned airframe modifications beneath the original rocket motor orifice to accept the extra combustion chamber, set a new unofficial world speed record of , piloted by Heini Dittmar, and landed with almost all of the vertical rudder surface broken away from flutter. This record was not broken in terms of absolute speed until 6 November 1947 by Chuck Yeager in flight number 58 that was part of the Bell X-1 test program, with a , or Mach 1.35 supersonic speed, recorded at an altitude of nearly . However, it is unclear if Dittmar's flight achieved sufficient altitude for its speed to be considered supersonic, as the X-1 did.

The X-1 never exceeded Dittmar's speed from a normal runway "scharfer Start" liftoff. Heini Dittmar had reached the performance, after a normal "hot start" ground takeoff, without an air drop from a mother ship. Neville Duke exceeded Heini Dittmar's record mark roughly 5-1/2 years after Yeager's achievement (and some 263 km/h short of it) on 31 August 1953 with the Hawker Hunter F Mk3 at a speed of , after a normal ground start. Postwar experimental aircraft of the aerodynamic configuration that the Me 163 used, were found to have serious stability problems when entering transonic flight, like the similarly configured, and turbojet powered, Northrop X-4 Bantam and de Havilland DH 108, which made the V18's record with the Walter 509B "cruiser" rocket motor more remarkable.

Waldemar Voigt of Messerschmitt's "Oberammergau" project and development offices started a redesign of the 163 to incorporate the new twin-chamber Walter rocket engine, as well as fix other problems. The resulting Me 163C design featured a larger wing through the addition of an insert at the wing root, an extended fuselage with extra tank capacity through the addition of a plug insert behind the wing, a ventral fairing whose aft section possessed a retractable tailwheel design closely resembling that pioneered on the Me 163B V6, and a new pressurized cockpit topped with a bubble canopy for improved visibility, on a fuselage that had dispensed with the earlier B-version's dorsal fairing. The additional tank capacity and cockpit pressurization allowed the maximum altitude to increase to , as well as improving powered time to about 12 minutes, almost doubling combat time (from about five minutes to nine). Three Me 163 C-1a prototypes were planned, but it appears only one was flown, but without its intended engine.

By this time the project was moved to Junkers. There, a new design effort under the direction of Heinrich Hertel at Dessau attempted to improve the "Komet". The Hertel team had to compete with the Lippisch team and their Me 163C. Hertel investigated the Me 163 and found it was not well suited for mass production and not optimized as a fighter aircraft, with the most glaring deficiency being the lack of retractable landing gear. To accommodate this, what would eventually become the Me 263 V1 prototype would be fitted with the desired tricycle gear, also accommodating the twin-chamber Walter rocket from the start—later it was assigned to the Ju 248 program.

The resulting "Junkers Ju 248" used a three-section fuselage to ease construction. The V1 prototype was completed for testing in August 1944, and was glider-tested behind a Junkers Ju 188. Some sources state that the Walter 109–509C engine was fitted in September, but it was probably never tested under power. At this point the RLM reassigned the project to Messerschmitt, where it became the Messerschmitt Me 263. This appears to have been a formality only, with Junkers continuing the work and planning production. By the time the design was ready to go into production, the plant where it was to be built was overrun by Soviet forces. While it did not reach operational status, the work was briefly continued by the Soviet Mikoyan-Gurevich (MiG) design bureau as the Mikoyan-Gurevich I-270.

The initial test deployment of the Me 163A, to acquaint prospective pilots with the world's first rocket-powered fighter, occurred with "Erprobungskommando 16" (Service Test Unit 16, EK 16), led by "Major" Wolfgang Späte and first established in late 1942, receiving their eight A-model service test aircraft by July 1943. Their initial base was as the "Erprobungsstelle" (test facility) at the Peenemünde-West field. They departed permanently the day after an RAF bombing raid on the area on 17 August 1943, moving southwards, to the base at Anklam, near the Baltic coast. Their stay was brief, as a few weeks later they were placed in northwest Germany, based at the military airfield at Bad Zwischenahn from August 1943 to August 1944. EK 16 received their first B-series armed Komets in January 1944, and was ready for action by May while at Bad Zwischenahn. "Major" Späte flew the first-ever Me 163B combat sortie on 13 May 1944 from the Bad Zwischenahn base, with the Me 163B armed prototype (V41), bearing the "Stammkennzeichen" PK+QL.

As EK 16 commenced small-scale combat operations with the Me 163B in May 1944, the Me 163B's unsurpassed velocity was something Allied fighter pilots were at a loss to counter. The "Komets" attacked singly or in pairs, often even faster than the intercepting fighters could dive. A typical Me 163 tactic was to fly vertically upward through the bombers at , climb to , then dive through the formation again, firing as they went. This approach afforded the pilot two brief chances to fire a few rounds from his cannons before gliding back to his airfield. The pilots reported it was possible to make four passes on a bomber, but only if it was flying alone. As the cockpit was unpressurized, the operational ceiling was limited by what the pilot could endure for several minutes while breathing oxygen from a mask, without losing consciousness. Pilots underwent altitude chamber training to harden them against the rigors of operating in the thin air of the stratosphere without a pressure suit. Special low fiber diets were prepared for pilots, as gas in the gastrointestinal tract would expand rapidly during ascent.

Following the initial combat trial missions of the Me 163B with EK 16, during the winter and spring of 1944 "Major" Späte formed the Luftwaffe's first dedicated Me 163 fighter wing, "Jagdgeschwader" 400 (JG 400), in Brandis, near Leipzig. JG 400's purpose was to provide additional protection for the Leuna synthetic gasoline works which were raided frequently during almost all of 1944. A further group was stationed at Stargard near Stettin to protect the large synthetic fuel plant at Pölitz (today Police, Poland). Further defensive units of rocket fighters were planned for Berlin, the Ruhr and the German Bight.

The first actions involving the Me 163B in regular Luftwaffe active service occurred on 28 July 1944, from I./JG 400's base at Brandis, when two USAAF B-17 Flying Fortress were attacked without confirmed kills. Combat operations continued from May 1944 to spring 1945. During this time, there were nine confirmed kills with 14 Me 163s lost. "Feldwebel" Siegfried Schubert was the most successful pilot, with three bombers to his credit. Allied fighter pilots soon noted the short duration of the powered flight. They would wait and, when the engine exhausted its propellant, pounce on the unpowered "Komet". However, the "Komet" was extremely manoeuvrable in gliding flight. Another Allied method was to attack the fields the Komets operated from and strafe them after the Me 163s landed. Due to the skid-based landing gear system, the Komet was immobile until the "Scheuch-Schlepper" tractor could back the trailer up to the nose of the aircraft, place its two rear arms under the wing panels, and jack up the trailer's arms to hoist the aircraft off the ground or place it back on its take-off dolly to tow it back to its maintenance area. Establishing a defensive perimeter with anti-aircraft guns ensured that Allied fighters avoided these bases.

At the end of 1944, 91 aircraft had been delivered to JG 400 but lack of fuel had kept most of them grounded. It was clear that the original plan for a huge network of Me 163 bases would never be realized. Up to that point, JG 400 had lost only six aircraft due to enemy action. Nine were lost to other causes, remarkably few for such a revolutionary and technically advanced aircraft. In the last days of the Third Reich, the Me 163 was given up in favor of the more successful Me 262. At the beginning of May 1945, Me 163 operations were stopped, the JG 400 disbanded, and many of its pilots sent to fly Me 262s. In any operational sense, the "Komet" was a failure. Although it shot down 16 aircraft, mainly four-engined bombers, it did not warrant the effort put into the project. Due to fuel shortages late in the war, few went into combat, and it took an experienced pilot with excellent shooting skills to achieve "kills". The "Komet" also spawned later weapons like the vertical-launch, similarly rocket-powered Bachem Ba 349 Natter, and the postwar, American turbojet-powered Convair XF-92 delta wing interceptor. Ultimately, the point defense role that the Me 163 played would be taken over by the surface-to-air missile (SAM), Messerschmitt's own example being the Enzian.

Captain Eric Brown RN, Chief Naval Test Pilot and commanding officer of the Captured Enemy Aircraft Flight, who tested the Me 163 at the Royal Aircraft Establishment (RAE) at Farnborough, said, "The Me 163 was an aeroplane that you could not afford to just step into the aircraft and say 'You know, I'm going to fly it to the limit.' You had very much to familiarise yourself with it because it was state-of-the-art and the technology used." Acting unofficially, after a spate of accidents involving Allied personnel flying captured German aircraft resulted in official disapproval of such flights, Brown was determined to fly a powered Komet. On around 17 May 1945, he flew an Me 163B at Husum with the help of a cooperative German ground crew, after initial towed flights in an Me 163A to familiarise himself with the handling.

The day before the flight, Brown and his ground crew had performed an engine run on the chosen Me 163B to ensure that everything was running correctly, the German crew being apprehensive should an accident befall Brown, until being given a disclaimer signed by him to the effect that they were acting under his orders. On the rocket-powered "scharfer-start" takeoff the next day, after dropping the takeoff dolly and retracting the skid, Brown later described the resultant climb as "like being in charge of a runaway train", the aircraft reaching 32,000 ft (9.76 km) altitude in 2 minutes, 45 seconds. During the flight, while practicing attacking passes at an imaginary bomber, he was surprised at how well the Komet accelerated in the dive with the engine shut down. When the flight was over Brown had no problems on the approach to the airfield, apart from the rather restricted view from the cockpit due to the flat angle of glide, the aircraft touching down at . Once down safely, Brown and his much-relieved ground crew celebrated with a drink.

Beyond Brown's unauthorised flight, the British never tested the Me 163 under power themselves; due to the danger of its hypergolic propellants it was only flown unpowered. Brown himself piloted RAE's Komet "VF241" on a number of occasions, the rocket motor being replaced with test instrumentation. When interviewed for a 1990s television programme, Brown said he had flown five tailless aircraft (which did not include the pair of American Northrop X-4s) in his career (including the British de Havilland DH 108). Referring to the Komet, he said "this is the only one that had good flight characteristics"; he called the other four "killers".

It has been claimed that at least 29 "Komets" were shipped out of Germany after the war and that of those at least 10 have been known to survive the war to be put on display in museums around the world. Most of the 10 surviving Me 163s were part of JG 400, and were captured by the British at Husum, the squadron's base at the time of Germany's surrender in 1945. According to the RAF museum, 48 aircraft were captured intact and 24 were shipped to the United Kingdom for evaluation, although only one, "VF241", was test flown (unpowered).




Eventually an elderly German woman came forward with Me 163 instruments that her late husband had collected after the war, and the engine was reproduced by a machine shop owned by Me 163 enthusiast Reinhold Opitz. The factory closed in the early 1990s and "Yellow 25" was moved to a small museum created on the site. The museum contained aircraft that had once served as gate guards, monuments and other damaged aircraft previously located on the air base. In 1997 "Yellow 25" was moved to the official Luftwaffe Museum located at the former RAF base at Berlin-Gatow, where it is displayed today alongside a restored Walter HWK 109–509 rocket engine. This particular Me 163B is one of the very few World War II–era German military aircraft, restored and preserved in a German aviation museum, to have a swastika national marking of the Third Reich, in a "low visibility" white outline form, currently displayed on the tailfin.


Of the 21 aircraft that were captured by the British, at least three have survived. They were assigned the British serial numbers AM200 to AM220.



As part of their alliance, Germany provided the Japanese Empire with plans and an example of the Me 163. One of the two submarines carrying Me 163 parts did not arrive in Japan, so at the time, the Japanese lacked all of the major parts and construction blueprints, including the turbopump, which they could not make themselves, forcing them to reverse-engineer their own design from information obtained in the Me 163 Erection & Maintenance manual obtained from Germany. The prototype J8M crashed on its first powered flight and was completely destroyed, but several variants were built and flown, including: trainers, fighters, and interceptors, with only minor differences between the versions.

The Navy version, the Mitsubishi J8M1 "Shūsui", replaced the Ho 155 cannon with the Navy's 30 mm (1.18 in) Type 5. Mitsubishi also planned on producing a version of the 163C for the Navy, known as the J8M2 "Shūsui" Model 21. A version of the 163 D/263 was known as the J8M3 "Shusui" for the Navy with the Type 5 cannon, and a Ki-202 with the Ho 155-II for the Army. Trainers were planned, roughly the equivalent of the Me 163 A-0/S; these were known as the Kugisho/Yokosuka MXY8 (Yokoi Ki-13) (an unpowered glider trainer) and Kugisho/Yokosuka MXY9 (a Tsu-11-powered motorjet trainer).

One complete example of the Japanese aircraft survives at the Planes of Fame Air Museum in California. The fuselage of a second aircraft is displayed at the Mitsubishi company's Komaki Plant Museum, at Komaki, Aichi in Japan.

A flying replica Me 163 was constructed between 1994 and 1996 by Joseph Kurtz, a former "Luftwaffe" pilot who trained to fly Me 163s, but who never flew in combat. He subsequently sold the aircraft to EADS. The replica is an unpowered glider whose shape matches that of an Me 163, although its construction completely differs: the glider is built of wood with an empty weight of , a fraction of the weight of a wartime aircraft. Reportedly, it has excellent flying characteristics. The glider is painted red to represent the Me 163 flown by Wolfgang Späte. As of 2011, it was still flying with the civil registration D-1636.

In the early 2000s, a rocket-powered airworthy replica, the "Komet II", was proposed by XCOR Aerospace, a former aerospace company that had previously built the XCOR EZ-Rocket rocket-plane. Although outwardly the same as a wartime aircraft, the "Komet II" design would have differed considerably for safety reasons. It would have been partially constructed with composite materials, powered by one of XCOR's own simpler and safer, pressure fed, liquid oxygen/alcohol engines, and retractable undercarriage would have been used instead of a takeoff dolly and landing skid.

Several static replica Me 163s are exhibited at museums



</doc>
<doc id="20487" url="https://en.wikipedia.org/wiki?curid=20487" title="Mohamed Atta">
Mohamed Atta

Mohamed Mohamed el-Amir Awad el-Sayed Atta ( ; ""  ; September 1, 1968 – September 11, 2001) was an Egyptian hijacker and one of the ringleaders of the September 11 attacks in which four United States commercial aircraft were commandeered with the intention of destroying specific civilian and military targets. He served as the hijacker-pilot of American Airlines Flight 11 which he crashed into the North Tower of the World Trade Center as part of the coordinated attacks. At 33 years of age, he was the oldest of the 19 hijackers who took part in the attacks.

Born and raised in Egypt, Atta studied architecture at Cairo University, graduating in 1990, and continued his studies in Germany at the Hamburg University of Technology. In Hamburg, Atta became involved with the al-Quds Mosque, where he met Marwan al-Shehhi, Ramzi bin al-Shibh, and Ziad Jarrah, together forming the Hamburg cell. Atta disappeared from Germany for periods of time, embarking on the hajj in 1995 but also meeting Osama bin Laden and other top al-Qaeda leaders in Afghanistan from late-1999 to early-2000. Atta and the other Hamburg cell members were recruited by bin Laden and Khalid Sheikh Mohammed for a "planes operation" in the United States. Atta returned to Hamburg in February 2000, and began inquiring about flight training in the United States.

In June 2000, Atta, Ziad Jarrah and Marwan al-Shehhi arrived in the United States to learn how to pilot planes, obtaining instrument ratings in November. Beginning in May 2001, Atta assisted with the arrival of the muscle hijackers, and in July he traveled to Spain to meet with bin al-Shibh to finalize the plot. In August 2001, Atta traveled as a passenger on several "surveillance" flights, to establish in detail how the attacks could be carried out.

On the morning of September 11, Atta boarded American Airlines Flight 11, which he and his team then hijacked. Atta took control of the plane and crashed it into the North Tower of the World Trade Center as planned. The crash led to the collapse of the tower and the deaths of over 1,600 people.

Mohamed Atta varied his name on documents, also using "Mehan Atta", "Mohammad El Amir", "Muhammad Atta", "Mohamed El Sayed", "Mohamed Elsayed", "Muhammad al-Amir", "Awag Al Sayyid Atta", and "Awad Al Sayad". In Germany, he registered his name as "Mohamed el-Amir Awad el-Sayed Atta", and went by the name Mohamed el-Amir at the Hamburg University of Technology. In his will, written in 1996, Atta gives his name as "Mohamed the son of Mohamed Elamir awad Elsayed". Atta also claimed different nationalities, sometimes Egyptian and other times telling people he was from the United Arab Emirates.

Atta was born on September 1, 1968, in Kafr el-Sheikh, located in Egypt's Nile Delta region. His father, Mohamed el-Amir Awad el-Sayed Atta, was a lawyer, educated in both sharia and civil law. His mother, Bouthayna Mohamed Mustapha Sheraqi, came from a wealthy farming and trading family and was also educated. Bouthayna and Mohamed married when she was 14, via an arranged marriage. The family had few relatives on the father's side and kept their distance from Bouthayna's family. In-laws characterized Atta's father as "austere, strict, and private," and neighbors viewed the family as reclusive. Atta was the only son; he had two older sisters who are both well-educated and successful in their careers — one as a medical doctor and the other as a professor.

When Atta was ten, his family moved to the Cairo neighborhood of Abdeen, situated near the city center. His father, who kept the family ever insulated, forbade young Atta to fraternize with the other children in their neighborhood. Having little else to do, he mostly studied at home and easily excelled in school. In 1985, Atta enrolled at Cairo University and focused his studies on engineering. He was among the highest-scoring students; by his senior year, he was admitted to an exclusive architecture program. After he graduated in 1990 with an architecture degree, he joined the Engineers Syndicate, an organization under the control of the Muslim Brotherhood. He then worked for several months at the Urban Development Center in Cairo, where he joined various building projects and dispatched diverse architectural tasks. Also in 1990, Atta's family moved into the eleventh floor of an apartment building in the Egyptian city of Giza.

Atta also got engaged to a woman lined up by his father and her family in Cairo, at late 1999, after coming back from Germany the same year. Although the marriage never happened, Atta's father mentioned they liked each other. 

Atta graduated from Cairo University with marks insufficient for the graduate program. As his father insisted that he go abroad for graduate studies, Atta, to this end, entered a German-language program at the Goethe Institute in Cairo. In 1992, his father had overheard a German couple who were visiting Egypt's capital. The couple explained at dinner that they ran an exchange program and invited Atta to continue his studies in Germany; they also offered him room and board at their home in the city. Mohamed Atta accepted and was in Germany two weeks later, in July.

In Germany, he enrolled in the urban planning graduate program at the Hamburg University of Technology. Atta initially lived with two high school teachers; however, they eventually found his closed-mindedness and introverted personality to be too much for them. Atta began adhering to the strictest Islamic diet, frequenting the most conservative mosques, socializing seldom, and acting disdainfully towards the couple's unmarried daughter who had a young child. After six months, they asked him to leave.

By early 1993, Atta had moved into university housing with two roommates, in Centrumshause. He stayed there until 1998. During that period, his roommates grew annoyed with him. He seldom bathed, and they could not bear his "complete, almost aggressive insularity". He kept to himself to such an extent that he would turn away from a salutation with supercilious silence.

At the Hamburg University of Technology, Atta studied under the guidance of the department chair, one Dittmar Machule, who specialized in the Middle East. Atta was averse to modern development. This included the construction of high-rise buildings in Cairo and other ancient cities in the region. He believed that the drab and impersonal apartment blocks, built in the 60s and 70s, ruined the beauty of old neighborhoods and robbed their people of privacy and dignity. Atta's family moved into one such eyesore in 1990; it was to him but "a shabby symbol of Egypt's haphazard attempts to modernize and its shameless embrace of the West." For his thesis, Atta concentrated on the ancient Syrian city of Aleppo. He researched the history of the urban landscape in relation to the general theme of conflict between Arab and modern civilization. He criticized how the newfangled skyscrapers and other modernizing projects were disrupting the fabric of communities by blocking common streets and altering the skyline.

Atta's professor, Dittmar Machule, brought him along on an archaeological expedition to Aleppo in 1994. The invitation had been for a three-day visit, but Atta ended up staying several weeks that August, only to visit Aleppo yet again that December. While in Syria, he met Amal, a young Palestinian woman who worked for a planning bureau in the city. Volker Hauth, who was traveling with Atta, described Amal as "attractive and self-confident. She observed Muslim customs, taking taxis to and from the office so as not to come into close physical contact with men on buses. But she was also said to be 'emancipated' and 'challenging'. Atta and Amal appeared to be attracted to each other, but Atta soon decides that "she had a quite different orientation and that the emancipation of the young lady did not fit." His nascent infatuation with her, begrudgingly realised, was the closest thing Atta knew to romance. In mid-1995, he stayed for three months in Cairo, on a grant from the Carl Duisberg Society, along with fellow students Volker Hauth and Ralph Bodenstein. The academic team inquired into the effects of redevelopment in the Islamic Cairo, the old quarter, which the government undertook to remodel for tourism. Atta stayed in Cairo awhile with his family after Hauth and Bodenstein flew back to Germany.

While in Hamburg, Atta held several positions, such as one part-time job at Plankontor, as well as another at an urban planning firm, beginning in 1992. He was let go from the firm in 1997, however, because its business had declined and "his draughtsmanship was not needed" after it bought a CAD system. Among other odd jobs to supplement his income, Atta sometimes worked at a cleaning company and sometimes bought and sold cars. Atta had harbored a desire to return to his native city, ever since he finished his studies in Hamburg; but he was prevented by the dearth of job prospects in Cairo, his family lacking the "right connections" to avail the customary nepotism. Further, after the Egyptian government had imprisoned droves of political activists, he knew better than to trust it not to target him too, with his social and political beliefs being such as they were.

After coming to Hamburg in 1992, Atta grew more religiously fanatical and frequented the mosque with greater regularity. His friends in Germany described him as an intelligent man in whom religious convictions and political motives held equal sway. He harbored anger and resentment toward the U.S. for its policy in Islamic nations of the Middle East, with nothing inflaming his ire more than the Oslo Accords and the Gulf War in particular. He was also angry and bitter at the elite in his native Egypt, who hoarded all the power for themselves, as well as at the Egyptian government, that cracked down on the dissident Muslim Brotherhood.

On August 1, 1995, Atta returned to Egypt for three months of study. Before this trip he grew out a beard, with a view to show himself as a devout Muslim and to make a political gesture thereby. Atta returned to Hamburg on October 31, 1995, only to join the pilgrimage to Mecca shortly thereafter.

In Hamburg, Atta was intensely drawn to al-Quds Mosque which adhered to a "harsh, uncompromisingly fundamentalist, and resoundingly militant" version of Sunni Islam. He made acquaintances at al-Quds; some of whom visited him on occasion at Centrumshaus. He also began teaching classes both at Al-Quds and at a Turkish mosque near the Harburg district. Atta also started and led a prayer group, which Ahmed Maklat and Mounir El Motassadeq joined. Ramzi bin al-Shibh was also there, teaching occasional classes, and became Atta's friend.

On April 11, 1996, Atta signed his last will and testament at the mosque, officially declaring his Muslim beliefs and giving 18 instructions regarding his burial. This was the same day that Israel, much to the outrage of Atta, attacked Lebanon in Operation Grapes of Wrath; signing the will "offering his life" was his response. The instructions in his last will and testament reflect both Sunni funeral practices along with some more puritanical demands from Salafism, including asking people not "to weep and cry" and to generally refrain from showing emotion. The will was signed by el-Motassadeq and a second person at the mosque.

After leaving Plankontor in the summer of 1997, Atta disappeared again and did not return until 1998. He had made no progress on his thesis. Atta phoned his graduate advisor, Machule, and mentioned family problems at home, saying, "Please understand, I don't want to talk about this." At the winter break in 1997, Atta left and did not return to Hamburg for three months. He said that he went on pilgrimage to Mecca again, just 18 months after his first time. This claim has been disputed; Terry McDermott has argued that it is unusual for someone to go on pilgrimage so soon after the first time and to spend three months there (more than Hajj requires). When Atta returned, he claimed that his passport was lost and applied for a new one, which is a common tactic to erase evidence of travel to places such as Afghanistan. When he returned in spring 1998, after disappearing for several months, he had grown a thick long beard, and "seemed more serious and aloof" than before to those who knew him.

By mid-1998, Atta was no longer eligible for university housing in Centrumshaus. He moved into a nearby apartment in the Wilhelmsburg district, where he lived with Said Bahaji and Ramzi bin al-Shibh. By early 1999, Atta had completed his thesis, and formally defended it in August 1999.

In mid-1998, Atta worked alongside Shehhi, bin al-Shibh, and Belfas, at a warehouse, packing computers in crates for shipping. The Hamburg group did not stay in Wilhelmsburg for long. The next winter, they moved into an apartment at Marienstrasse 54 in the borough of Harburg, near the Hamburg University of Technology, at which they enrolled. It was here that the Hamburg cell developed and acted more as a group. They met three or four times a week to discuss their anti-American feelings and to plot possible attacks. Many al-Qaeda members lived in this apartment at various times, including hijacker Marwan al-Shehhi, Zakariya Essabar, and others.

In late 1999, Atta, Shehhi, Jarrah, Bahaji, and bin al-Shibh decided to travel to Chechnya to fight against the Russians, but were convinced by Khalid al-Masri and Mohamedou Ould Slahi at the last minute to change their plans. They instead traveled to Afghanistan over a two-week period in late November. On November 29, 1999, Mohamed Atta boarded Turkish Airlines Flight TK1662 from Hamburg to Istanbul, where he changed to flight TK1056 to Karachi, Pakistan. After they arrived, they were selected by Al Qaeda leader Mohammed Atef as suitable candidates for the "planes operation" plot. They were all well-educated, had experience of living in western society, along with some English skills, and would be able to obtain visas. Even before bin al-Shibh had arrived, Atta, Shehhi, and Jarrah were sent to the House of Ghamdi near bin Laden's home in Kandahar, where he was waiting to meet them. Bin Laden asked them to pledge loyalty and commit to suicide missions, which Atta and the other three Hamburg men all accepted. Bin Laden sent them to see Atef to get a general overview of the mission, and then they were sent to Karachi to see Khalid Sheikh Mohammed to go over specifics.

German investigators said that they had evidence that Mohamed Atta trained at al-Qaeda camps in Afghanistan from late 1999 to early 2000. The timing of the Afghanistan training was outlined on August 23, 2002, by a senior investigator. The investigator, Klaus Ulrich Kersten was the director of Germany's federal anticrime agency, the Bundeskriminalamt. He provided the first official confirmation that Atta and two other pilots had been in Afghanistan, and he also provided the first dates of the training. Kersten said in an interview at the agency's headquarters in Wiesbaden that Atta was in Afghanistan from late 1999 until early 2000, and that there was evidence that Atta met with Osama bin Laden there.

A video surfaced in October 2006. The first chapter of the video showed bin Laden at Tarnak Farms on January 8, 2000. The second chapter showed Atta and Ziad Jarrah reading their wills together ten days later on January 18. On his return journey, Atta left Karachi on February 24, 2000, by flight TK1057 to Istanbul where he changed to flight TK1661 to Hamburg. Immediately after returning to Germany, Atta, al-Shehhi, and Jarrah reported their passports stolen, possibly to discard travel visas to Afghanistan.

On March 22, 2000, Atta was still in Germany when he sent an e-mail to the Academy of Lakeland in Florida. He inquired about flight training, "Dear sir, we are a small group of young men from different Arab countries. Now, we are living in Germany since a while for study purposes. We would like to start training for the career of airline professional pilots. In this field, we haven't yet any knowledge but we are ready to undergo an intensive training program (up to ATP and eventually higher)." Atta sent 50–60 similar e-mails to other flight training schools in the United States.

On May 17, Atta applied for a United States visa. The next day, he received a five-year B-1/B-2 (tourist/business) visa from the United States embassy in Berlin. Atta had lived in Germany for approximately five years and also had a "strong record as a student". He was therefore treated favorably and not scrutinized. After obtaining his visa, Atta took a bus on June 2 from Hamburg to Prague where he stayed overnight before traveling on to the United States the next day. Bin al-Shibh later explained that they believed it would contribute to operational security for Atta to fly out of Prague instead of Hamburg, where he traveled from previously. Likewise, Shehhi traveled from a different location, in his case via Brussels.

On June 6, 2002, ABC's "World News Tonight" broadcast an interview with Johnelle Bryant, former loan officer at the U.S. Department of Agriculture in south Florida, who told about her encounter with Mohamed Atta. This encounter took place "around the third week of April to the third week of May of 2000", before Atta's official entry date into the United States (see below). According to Bryant, Atta wanted to finance the purchase of a crop-duster. "He wanted to finance a twin-engine, six-passenger aircraft and remove the seats," Bryant told ABC's "World News Tonight". He insisted that she write his name as ATTA, that he originally was from Egypt but had moved to Afghanistan, that he was an engineer and that his dream was to go to a flight school. He asked about the Pentagon and the White House. He said he wanted to visit the World Trade Center and asked Bryant about the security there. He mentioned Al Qaeda and said the organization "could use memberships from Americans". He mentioned Osama bin Laden and said "this man would someday be known as the world's greatest leader." Bryant said "the picture that came out in the newspaper, that's exactly what that man looked like." Bryant contacted the authorities after recognising Atta in news reports. Law-enforcement officials said Bryant passed a lie-detector exam.

According to official reports, Atta flew from Prague to Newark International Airport, arriving on June 3, 2000. That month, Atta and Shehhi stayed in hotels and rented rooms in New York City on a short-term basis, Jarrah had arrived in the United States on June 27, 2000 after his flight landed at Newark, New Jersey, and Jarrah had decided to go with Shehhi and Atta to search for different flight schools in the US. They continued to inquire about flight schools and personally visited some, including Airman Flight School in Norman, Oklahoma, which they visited on July 3, 2000. Days later, Shehhi, Jarrah and Atta ended up in Venice, Florida. Atta and Shehhi established accounts at SunTrust Bank and received wire transfers from Ali Abdul Aziz Ali, Khalid Sheikh Mohammed's nephew in the United Arab Emirates. On July 6, 2000, Atta, Jarrah and Shehhi enrolled at Huffman Aviation in Venice, where they entered the Accelerated Pilot Program. When Atta and Shehhi arrived in Florida, they initially stayed with Huffman's bookkeeper and his wife in a spare room of their house. After a week, they were asked to leave because they were rude. Atta and Shehhi then moved into a small house nearby in Nokomis where they stayed for six months.

Atta began flight training on July 6, 2000, and continued training nearly every day. By the end of July, both Atta and Shehhi did solo flights. Atta earned his private pilot certificate in September, and then he and Shehhi decided to switch flight schools. Both enrolled at Jones Aviation in Sarasota and took training there for a brief time. They had problems following instructions and were both very upset when they failed their Stage 1 exam at Jones Aviation. They inquired about multi-engine planes and told the instructor that "they wanted to move quickly, because they had a job waiting in their country upon completion of their training in the U.S." In mid-October, Atta and Shehhi returned to Huffman Aviation to continue training. In November 2000, Atta earned his instrument rating, and then a commercial pilot's license in December from the Federal Aviation Administration.

Atta continued with flight training that included solo flights and simulator time. On December 22, Atta and Shehhi applied to Eagle International for large jet and simulator training for McDonnell Douglas DC-9 and Boeing 737-300 models. On December 26, Atta and Shehhi needed a tow for their rented Piper Cherokee on a taxiway of Miami International Airport after the engine shut down. On December 29 and 30, Atta and Marwan went to the Opa-locka Airport where they practiced on a Boeing 727 simulator, and they obtained Boeing 767 simulator training from Pan Am International on December 31. Atta purchased cockpit videos for Boeing 747-200, Boeing 757-200, Airbus A320 and Boeing 767-300ER models via mail-order from Sporty's Pilot Shop in Batavia, Ohio, in November and December 2000.

Records on Atta's cellphone indicated that he phoned the Moroccan embassy in Washington on January 2, just before Shehhi flew to the country. Atta flew to Spain on January 4, 2001, to coordinate with bin al-Shibh and returned to the United States on January 10. While in the United States he traveled to Lawrenceville, Georgia, where he and Shehhi visited a LA Fitness Health Club. During that time Atta flew out of Briscoe Field in Lawrenceville with a pilot, and Atta and either the pilot or Shehhi flew around the Atlanta area. They lived in the area for several months. On April 3, Atta and Shehhi rented a postal box in Virginia Beach, Virginia.

On April 11, Atta and Shehhi rented an apartment at 10001 Atlantic Blvd, Apt. 122 in Coral Springs, Florida, for $840 per month, and assisted with the arrival of the muscle hijackers. On April 16, Atta was given a citation for not having a valid driver's license, and he began steps to get the license. On May 2, Atta received his driver's license in Lauderdale Lakes, Florida. While in the United States, Atta owned a red 1989 Pontiac Grand Prix.

On June 27, Atta flew from Fort Lauderdale to Boston, Massachusetts, where he spent a day, and then continued to San Francisco for a short time, and from there to Las Vegas. On June 28, Atta arrived at McCarran International Airport in Las Vegas to meet with the three other pilots. He rented a Chevrolet Malibu from an Alamo Rent A Car agency. It is not known where he stayed that night, but on the 29th he registered at the Econo Lodge at 1150 South Las Vegas Boulevard. Here he presented an AAA membership for a discount, and paid cash for the $49.50/night room. During his trip to Las Vegas, he is thought to have used a video camera that he had rented from a Select Photo outlet back in Delray Beach, Florida.

In July 2001, Atta again left for Spain in order to meet with bin al-Shibh for the last time. On July 7, 2001, Atta flew on Swissair Flight 117 from Miami to Zürich, where he had a stopover. On July 8, Atta was recorded on surveillance video when he withdrew 1700 Swiss francs from an ATM. He used his credit card to purchase two Swiss Army knives and some chocolate in a shop at the Zürich Airport. After the stopover in Zürich, he arrived in Madrid at 4:45 pm on Swissair Flight 656, and spent several hours at the airport. Then at 8:50 pm, he checked into the Hotel Diana Cazadora in Barajas, a town near the airport. That night and twice the next morning, he called Bashar Ahmad Ali Musleh, a Jordanian student in Hamburg who served as a liaison for bin al-Shibh.

On the morning of July 9, Mohamed Atta rented a silver Hyundai Accent, which he booked from SIXT Rent-A-Car for July 9 to 16, and later extended to the 19th. He drove east out of Madrid towards the Mediterranean beach area of Tarragona. On the way, Atta stopped in Reus to pick up Ramzi bin al-Shibh at the airport. They drove to Cambrils, where they spent a night at the Hotel Monica. They checked out the next morning, and spent the next few days at an unknown location in Tarragona. The absence of other hotel stays, signed receipts or credit card stubs has led investigators to believe that the men may have met in a safe house provided by other al-Qaeda operatives in Spain. There, Atta and bin al-Shibh held a meeting to complete the planning of the attacks. Several clues have been found to link their stay in Spain to Syrian-born Imad Eddin Barakat Yarkas (Abu Dahdah), and Amer el Azizi, a Moroccan in Spain. They may have helped arrange and host the meeting in Tarragona. Yosri Fouda, who interviewed bin al-Shibh and Khalid Sheikh Mohammed (KSM) before the arrest, believes that Said Bahaji and KSM may have also been present at the meeting. Spanish investigators have said that Marwan al-Shehhi and two others later joined the meeting. Bin al-Shibh would not discuss this meeting with Fouda.

During the Spain meetings, Atta and bin al-Shibh had coordinated the details of the attacks. The 9/11 Commission obtained details about the meeting, based on interrogations of bin al-Shibh in the weeks after his arrest in September 2002. Bin al-Shibh explained that he passed along instructions from Osama bin Laden, including his desire for the attacks to be carried out as soon as possible. Bin Laden was concerned about having so many operatives in the United States. Atta confirmed that all the muscle hijackers had arrived in the United States, without any problems, but said that he needed five to six more weeks to work out details. Bin Laden also asked that other operatives not be informed of the specific data until the last minute. During the meeting, Atta and bin al-Shibh also decided on the targets to be hit, ruling out a strike on a nuclear plant. Bin al-Shibh passed along bin Laden's list of targets; bin Laden wanted the U.S. Capitol, the Pentagon, and the World Trade Center to be attacked, as they were deemed "symbols of America." If any of the hijackers could not reach their intended targets, Atta said, they were to crash the plane. They also discussed the personal difficulties Atta was having with fellow hijacker Ziad Jarrah. Bin al-Shibh was worried that Jarrah might even abandon the plan. The 9/11 Commission Report speculated that the now-convicted terrorist conspirator Zacarias Moussaoui was being trained as a possible replacement for Jarrah.

From July 13 to 16, Atta stayed at the Hotel Sant Jordi in Tarragona. After bin al-Shibh returned to Germany on July 16, 2001, Atta had three more days in Spain. He spent two nights in Salou at the beachside Casablanca Playa Hotel, then spent the last two nights at the Hotel Residencia Montsant. On July 19, Atta returned to the United States, flying on Delta Air Lines from Madrid to Fort Lauderdale, via Atlanta.

On July 22, 2001, Atta rented a Mitsubishi Galant from Alamo Rent a Car, putting 3,836 miles on the vehicle before returning it on July 26. On July 25, Atta dropped Ziad Jarrah off at Miami International Airport for a flight back to Germany. On July 26, Atta traveled via Continental Airlines to Newark, New Jersey, checked into the Kings Inn Hotel in Wayne, New Jersey, and stayed there until July 30 when he took a flight from Newark back to Fort Lauderdale.

On August 4, Atta is believed to have been at Orlando International Airport waiting to pick up suspected "20th Hijacker" Mohammed al-Qahtani from Dubai, who ended up being held by immigration as "suspicious." Atta was believed to have used a payphone at the airport to phone a number "linked to al-Qaeda" after Qahtani was denied entry.

On August 6, Atta and Shehhi rented a white, four door 1995 Ford Escort from Warrick's Rent-A-Car, which was returned on August 13. On August 6, Atta booked a flight on Spirit Airlines from Fort Lauderdale to Newark, leaving on August 7 and returning on August 9. The reservation was not used and canceled on August 9 with the reason "Family Medical Emergency". Instead, he went to Central Office & Travel in Pompano Beach to purchase a ticket for a flight to Newark, leaving on the evening of August 7 and schedule to return in the evening on August 9. Atta did not take the return flight. On August 7, Atta checked into the Wayne Inn in Wayne, New Jersey and checked out on August 9. The same day, he booked a one-way first class ticket via the Internet on America West Flight 244 from Ronald Reagan Washington National Airport to Las Vegas. Atta traveled twice to Las Vegas on "surveillance flights" rehearsing how the 9/11 attacks would be carried out. Other hijackers traveled to Las Vegas at different times in the summer of 2001.

Throughout the summer, Atta met with Nawaf al-Hazmi to discuss the status of the operation on a monthly basis.

On August 23, Atta's driver license was revoked "in absentia" after he failed to show up in traffic court to answer the earlier citation for driving without a license. On the same day, Israeli Mossad reportedly gave his name to the CIA as part of a list of 19 names they said were planning an attack in the near future. Only four of the names are known for certain, the others being Marwan al-Shehhi, Khalid al-Mihdhar and Nawaf al-Hazmi. On August 30 he was recorded purchasing a utility knife from a Wal-Mart store near the hotel where he stayed prior to 9/11.

On September 10, 2001, Atta picked up Omari from the Milner Hotel in Boston, Massachusetts, and the two terrorists drove their rented Nissan Altima to a Comfort Inn in South Portland, Maine. On the way, they were seen getting gasoline at an Exxon gas station and visited the Longfellow House in Portland that afternoon; they arrived at the hotel at 5:43 p.m. and spent the night in Room 233. While in South Portland, they were seen making two ATM withdrawals and stopping at Wal-Mart. The FBI also reported that "two middle-eastern men" were seen in the parking lot of a Pizza Hut, where Atta is known to have eaten that day.

Atta and Omari arrived early the next morning, at 5:40 a.m., at Portland International Jetport, where they left their rental car in the parking lot and boarded a 6:00 a.m. Colgan Air (US Airways Express) BE-1900C flight to Boston's Logan International Airport. In Portland, Mohamed Atta was selected by the Computer Assisted Passenger Prescreening System (CAPPS), which required his checked bags to undergo extra screening for explosives but involved no extra screening at the passenger security checkpoint.

The connection between the two flights at Logan International Airport was within Terminal B, but the two gates were not connected within security. Passengers must leave the secured area, go outdoors, cross a covered roadway, and enter another building before going through security once again. There are two separate concourses in Terminal B; the south concourse is mainly used by US Airways and the north one is mostly used by American Airlines. It had been overlooked that there would still be a security screen to pass in Boston because of this distinct detail of the terminal's arrangement. At 6:45 a.m., while at the Boston airport, Atta took a call from Flight 175 hijacker Marwan al-Shehhi. This call was apparently to confirm that the attacks were ready to begin. Atta checked in for American Airlines Flight 11, passed through security again, and boarded the flight. Atta was seated in business class, in seat 8D. At 7:59 a.m., the plane departed from Boston to Los Angeles, California, carrying 81 passengers.

The hijacking began at 8:14 a.m. — 15 minutes after the flight departed — when beverage service would be starting. At this time, the pilots stopped responding to air traffic control, and the aircraft began deviating from the planned route. At 8:18 am, flight attendants Betty Ong and Madeline Amy Sweeney began making phone calls to American Airlines to report what was happening. Ong provided information about lack of communication with the cockpit, lack of access to the cockpit, and passenger injuries. At 8:24:38 a.m., a voice believed to be Atta's was heard by air traffic controllers, saying: "We have some planes. Just stay quiet and you will be OK. We are returning to the airport." "Nobody move, everything will be OK. If you try to make any moves you'll endanger yourself and the airplane. Just stay quiet." "Nobody move, please. We are going back to the airport. Don't try to make any stupid moves." The plane's transponder was turned off at 8:21 a.m. At 8:46:35 a.m., the plane flew into the North Tower of the World Trade Center in New York city.

Because the flight from Portland to Boston had been delayed, his bags did not make it onto Flight 11. Atta's bags were later recovered in Logan International Airport, and they contained airline uniforms, flight manuals, and other items. The luggage included a copy of Atta's will, written in Arabic, as well as a list of instructions, called "The Last Night". This document is divided into three sections; the first is a fifteen point list providing detailed instructions for the last night of a martyr's life, the second gives instructions for travelling to the plane and the third from the time between boarding the plane and martyrdom. Almost all of these points discuss spiritual preparation, such as prayer and citing religious scripture.

On October 1, 2006, "The Sunday Times" released a video it had obtained "through a previously tested channel", purporting to show Mohamed Atta and Ziad Jarrah recording a martyrdom message six months earlier at a training camp in Afghanistan. The video, bearing the date of January 18, 2000, is of good resolution but contains no sound track. Lip readers have failed to decipher it. Atta and Jarrah appear in high spirits, laughing and smiling in front of the camera. They had never been pictured together before. Unidentified sources from both Al-Qaeda and the United States confirmed to The Times the video's authenticity. A separate section of the video shows Osama bin Laden addressing his followers at a complex near Kandahar. Ramzi bin al-Shibh is also identified in the video. According to "The Sunday Times", "American and German investigators have struggled to find evidence of Atta's whereabouts in January 2000 after he disappeared from Hamburg. The hour-long tape places him in Afghanistan at a decisive moment in the development of the conspiracy when he was given operational command. Months later both he and Jarrah enrolled at flying schools in America."

In the aftermath of the September 11, 2001 attacks, the names of the hijackers were released. There was some confusion regarding who Mohamed Atta was, and cases of mistaken identity. Initially, Mohamed Atta's identity was confused with that of a native Jordanian, Mahmoud Mahmoud Atta, who bombed an Israeli bus in the West Bank in 1986, killing one and severely injuring three. Mahmoud Atta was 14 years older than Atta. Mahmoud Atta, a naturalized U.S. citizen, was subsequently deported from Venezuela to the United States, extradited to Israel, tried and sentenced to life in prison. The Israeli Supreme Court later overturned his extradition and set him free. After 9/11, there also were reports stating that Mohamed Atta had attended International Officers School at Maxwell Air Force Base in Montgomery, Alabama. "The Washington Post" quoted a United States Air Force official who explained, "discrepancies in their biographical data, such as birth dates 20 years off, indicate we are probably not talking about the same people."

In the months following up to the September 11 attacks, officials at the Czech Interior Ministry asserted that Atta made a trip to Prague on April 8, 2001, to meet with an Iraqi intelligence agent named Ahmed Khalil Ibrahim Samir al-Ani. This piece of information was passed on to the FBI as "unevaluated raw intelligence". Intelligence officials have concluded that such a meeting did not occur. A Pakistani businessman named Mohammed Atta had come to Prague from Saudi Arabia on May 31, 2000, with this second Atta possibly contributing to the confusion. The Egyptian Mohamed Atta arrived at the Florenc bus terminal in Prague, from Germany, on June 2, 2000. He left Prague the next day, flying on Czech Airlines to Newark, New Jersey, U.S. In the Czech Republic, some intelligence officials say the source of the purported meeting was an Arab informant who approached the Czech intelligence service with his sighting of Atta only after Atta's photograph had appeared in newspapers all over the world. United States and Czech intelligence officials have since concluded that the person seen with Ani was mistakenly identified as Atta, and the consensus of investigators is that Atta never attended a meeting in Prague.

In 2005, Army Lt. Col. Anthony Shaffer and Congressman Curt Weldon alleged that the Defense Department data mining project, Able Danger, produced a chart that identified Atta, along with Nawaf al-Hazmi, Khalid al-Mihdhar, and Marwan al-Shehhi, as members of a Brooklyn-based al-Qaeda cell in early 2000. Shaffer largely based his allegations on the recollections of Navy Captain, Scott Phillpott, who later recanted his recollection, telling investigators that he was "convinced that Atta was not on the chart that we had." Phillpott said that Shaffer was "relying on my recollection 100 percent," and the Defense Department Inspector General's report indicated that Philpott "may have exaggerated knowing Atta's identity because he supported using Able Danger's techniques to fight terrorism."

Five witnesses who had worked on Able Danger and had been questioned by the Defense Department's Inspector General later told investigative journalists that their statements to the IG were distorted by investigators in the final IG's report, or the report omitted essential information that they had provided. The alleged distortions of the IG report centered around excluding any evidence that Able Danger had identified and tracked Atta years before 9/11.

Lt. Col. Shaffer's book also clearly indicates direct identification of the Brooklyn cell, and Mohamed Atta.

Atta's father, Mohamed el-Amir Awad el-Sayed Atta, a retired lawyer in Egypt, vehemently rejected allegations his son was involved in the September 11 attacks, and instead accused the Mossad and the United States government of having a hand in framing his son. Atta Sr. rejected media reports that stated his son was drinking wildly, and instead described his son as a quiet boy uninvolved with politics, shy and devoted to studying architecture. The elder Mr. Atta said he had spoken with Mohamed by phone the day after on September 12, 2001. He held interviews with the German news magazine "Bild am Sonntag" in late 2002, saying his son was alive and hiding in fear for his life, and that American Christians were responsible for the attacks. In a subsequent interview in 2005, Atta Sr. stated, "My son is gone. He is now with God. The Mossad killed him."

There are multiple, conflicting explanations for Atta's behavior and motivation. Political psychologist Jerrold Post has suggested that Atta and his fellow hijackers were just following orders from al-Qaeda leadership, "and whatever their destructive, charismatic leader Osama bin Laden said was the right thing to do for the sake of the cause was what they would do." In turn, political scientist, Robert Pape, has claimed that Atta was motivated by his commitment to the political cause, that he was psychologically normal, and that he was "not readily characterized as depressed, not unable to enjoy life, not detached from friends and society." By contrast, criminal justice professor, Adam Lankford, has found evidence that indicated Atta was suicidal, and that his struggles with social isolation, depression, guilt, shame, hopelessness, and rage were extraordinarily similar to the struggles of those who commit conventional suicide and murder-suicide. By this view, Atta's political and religious beliefs affected the method of his suicide and his choice of target, but they were not the underlying causes of his behavior.





</doc>
<doc id="20488" url="https://en.wikipedia.org/wiki?curid=20488" title="Messerschmitt Me 262">
Messerschmitt Me 262

The Messerschmitt Me 262, nicknamed Schwalbe (German: "Swallow") in fighter versions, or Sturmvogel (German: "Storm Bird") in fighter-bomber versions, was the world's first operational jet-powered fighter aircraft. Design work started before World War II began, but problems with engines, metallurgy and top-level interference kept the aircraft from operational status with the Luftwaffe until mid-1944. The Me 262 was faster and more heavily armed than any Allied fighter, including the British jet-powered Gloster Meteor. One of the most advanced aviation designs in operational use during World War II, the Me 262's roles included light bomber, reconnaissance and experimental night fighter versions.

Me 262 pilots claimed a total of 542 Allied aircraft shot down, although higher claims are sometimes made. The Allies countered its effectiveness in the air by attacking the aircraft on the ground and during takeoff and landing. Strategic materials shortages and design compromises on the Junkers Jumo 004 axial-flow turbojet engines led to reliability problems. Attacks by Allied forces on fuel supplies during the deteriorating late-war situation also reduced the effectiveness of the aircraft as a fighting force. Armament production within Germany was focused on more easily manufactured aircraft. In the end, the Me 262 had a negligible impact on the course of the war as a result of its late introduction and the consequently small numbers put in operational service.

While German use of the aircraft ended with the close of World War II, a small number were operated by the Czechoslovak Air Force until 1951. It also heavily influenced several designs, such as Sukhoi Su-9 (1946) and Nakajima Kikka. Captured Me 262s were studied and flight tested by the major powers, and ultimately influenced the designs of post-war aircraft such as the North American F-86 Sabre, MiG-15 and Boeing B-47 Stratojet. Several aircraft survive on static display in museums, and there are several privately built flying reproductions that use modern General Electric J85 engines.

Several years before World War II, the Germans foresaw the great potential for aircraft that used the jet engine constructed by Hans Joachim Pabst von Ohain in 1936. After the successful test flights of the world's first jet aircraft—the Heinkel He 178—within a week of the invasion of Poland to start the war, they adopted the jet engine for an advanced fighter aircraft. As a result, the Me 262 was already under development as "Projekt" 1065 (P.1065) before the start of World War II. The project originated with a request by the "Reichsluftfahrtministerium" (RLM, Ministry of Aviation) for a jet aircraft capable of one hour's endurance and a speed of at least . Dr Waldemar Voigt headed the design team, with Messerschmitt's chief of development, Robert Lusser, overseeing.

Plans were first drawn up in April 1939, and the original design was very different from the aircraft that eventually entered service, with wing root-mounted engines, rather than podded ones, when submitted in June 1939. The progression of the original design was delayed greatly by technical issues involving the new jet engine. Because the engines were slow to arrive, Messerschmitt moved the engines from the wing roots to underwing pods, allowing them to be changed more readily if needed; this would turn out to be important, both for availability and maintenance. Since the BMW 003 jets proved heavier than anticipated, the wing was swept slightly, by 18.5°, to accommodate a change in the center of gravity. Funding for the jet engine program was also initially lacking as many high-ranking officials thought the war could easily be won with conventional aircraft. Among those were Hermann Göring, head of the Luftwaffe, who cut the engine development program to just 35 engineers in February 1940 (the month before the first wooden mock-up was completed); Willy Messerschmitt, who desired to maintain mass production of the piston-powered, 1935-origin Bf 109 and the projected Me 209; and Major General Adolf Galland, who had initially supported Messerschmitt through the early development years, flying the Me 262 himself on 22 April 1943. By that time, problems with engine development had slowed production of the aircraft considerably. One particularly acute problem arose with the lack of an alloy with a melting point high enough to endure the high temperatures involved, a problem that by the end of the war had not been adequately resolved. The aircraft made its first successful flight entirely on jet power on 18 July 1942, powered by a pair of Jumo 004 engines, after a November 1941 flight (with BMW 003s) ended in a double flameout.
The project aerodynamicist on the design of the Me 262 was Ludwig Bölkow. He initially designed the wing using NACA airfoils modified with an elliptical nose section. Later in the design process, these were changed to AVL derivatives of NACA airfoils, the NACA 00011-0.825-35 being used at the root and the NACA 00009-1.1-40 at the tip. The elliptical nose derivatives of the NACA airfoils were used on the horizontal and vertical tail surfaces. Wings were of single-spar cantilever construction, with stressed skins, varying from skin thickness at the root to at the tip. To expedite construction, save weight and use less strategic materials, late in the war, wing interiors were not painted. The wings were fastened to the fuselage at four points, using a pair of and forty-two bolts.

In mid-1943, Adolf Hitler envisioned the Me 262 as a ground-attack/bomber aircraft rather than a defensive interceptor. The configuration of a high-speed, light-payload "Schnellbomber" ("fast bomber") was intended to penetrate enemy airspace during the expected Allied invasion of France. His edict resulted in the development of (and concentration on) the "Sturmvogel" variant. It is debatable to what extent Hitler's interference extended the delay in bringing the "Schwalbe" into operation; it appears engine vibration issues were at least as costly, if not more so. Albert Speer, then Minister of Armaments and War Production, in his memoirs claimed Hitler originally had blocked mass production of the Me 262, before agreeing in early 1944. Hitler rejected arguments the aircraft would be more effective as a fighter against the Allied bombers destroying large parts of Germany, and wanted it as a bomber for revenge attacks. According to Speer, Hitler felt its superior speed compared to other fighters of the era meant it could not be attacked, and so preferred it for high altitude straight flying.

The Me 262 is often referred to as a "swept wing" design as the production aircraft had a small, but significant leading edge sweep of 18.5° which likely provided an advantage by increasing the critical Mach number. Sweep, uncommon at the time, was added after the initial design of the aircraft. The engines proved heavier than originally expected, and the sweep was added primarily to position the center of lift properly relative to the center of mass. (The original 35° sweep, proposed by Adolf Busemann, was not adopted.) On 1 March 1940, instead of moving the wing backward on its mount, the outer wing was re-positioned slightly aft; the trailing edge of the midsection of the wing remained unswept. Based on data from the AVA Göttingen and wind tunnel results, the inboard section's leading edge (between the nacelle and wing root) was later swept to the same angle as the outer panels, from the "V6" sixth prototype onward throughout volume production.

Test flights began on 18 April 1941, with the Me 262 V1 example, bearing its "Stammkennzeichen" radio code letters of PC+UA, but since its intended BMW 003 turbojets were not ready for fitting, a conventional Junkers Jumo 210 engine was mounted in the V1 prototype's nose, driving a propeller, to test the Me 262 V1 airframe. When the BMW 003 engines were installed, the Jumo was retained for safety, which proved wise as both 003s failed during the first flight and the pilot had to land using the nose-mounted engine alone. The V1 through V4 prototype airframes all possessed what would become an uncharacteristic feature for most later jet aircraft designs, a fully retracting conventional gear setup with a retracting tailwheel—indeed, the very first prospective German "jet fighter" airframe design ever flown, the Heinkel He 280, used a retractable tricycle landing gear from its beginnings, and flying on jet power alone as early as the end of March 1941.

The V3 third prototype airframe, with the code PC+UC, became a true jet when it flew on 18 July 1942 in Leipheim near Günzburg, Germany, piloted by test pilot Fritz Wendel. This was almost nine months ahead of the British Gloster Meteor's first flight on 5 March 1943. Its retracting conventional tail wheel gear (similar to other contemporary piston powered propeller aircraft), a feature shared with the first four Me 262 V-series airframes, caused its jet exhaust to deflect off the runway, with the wing's turbulence negating the effects of the elevators, and the first takeoff attempt was cut short.

On the second attempt, Wendel solved the problem by tapping the aircraft's brakes at takeoff speed, lifting the horizontal tail out of the wing's turbulence. The aforementioned initial four prototypes (V1-V4) were built with the conventional gear configuration. Changing to a tricycle arrangement—a permanently fixed undercarriage on the fifth prototype (V5, code PC+UE), with the definitive fully retractable nosewheel gear on the V6 (with "Stammkennzeichen" code VI+AA, from a new code block) and subsequent aircraft corrected this problem.

Test flights continued over the next year, but engine problems continued to plague the project, the Jumo 004 being only marginally more reliable than the lower-thrust (7.83 kN/1,760 lbf) BMW 003. Airframe modifications were complete by 1942 but, hampered by the lack of engines, serial production did not begin until 1944, and deliveries were low, with 28 Me 262s in June, 59 in July, but only 20 in August.

By Summer 1943, the Jumo 004A engine had passed several 100-hour tests, with a time between overhauls of 50 hours being achieved. However, the Jumo 004A engine proved unsuitable for full-scale production because of its considerable weight and its high utilization of strategic material (Ni, Co, Mo), which were in short supply. Consequently, the 004B engine was designed to use a minimum amount of strategic materials. All high heat-resistant metal parts, including the combustion chamber, were changed to mild steel (SAE 1010) and were protected only against oxidation by aluminum coating. The total engine represented a design compromise to minimize the use of strategic materials and to simplify manufacture. With the lower-quality steels used in the 004B, the engine required overhaul after just 25 hours for a metallurgical test on the turbine. If it passed the test, the engine was refitted for a further 10 hours of usage, but 35 hours marked the absolute limit for the turbine wheel. While BMW's and Junkers' axial compressor turbojet engines were characterised by a sophisticated design that could offer considerable advantage – also used in a generalized form for the contemporary American Westinghouse J30 turbojet – the lack of rare materials for the Jumo 004 design put it at a disadvantage compared to the "partly axial-flow" Power Jets W.2/700 turbojet engine which, despite its own largely centrifugal compressor-influenced design, provided (between an operating overhaul interval of 60–65 hours) an operational life span of 125 hours. Frank Whittle concludes in his final assessment over the two engines: "it was in the quality of high temperature materials that the difference between German and British engines was most marked"

Operationally, carrying of fuel in two tanks, one each fore and aft of the cockpit; and a ventral fuselage tank beneath, the Me 262 would have a total flight endurance of 60 to 90 minutes. Fuel was usually J2 (derived from brown coal), with the option of diesel or a mixture of oil and high octane B4 aviation petrol. Fuel consumption was double the rate of typical twin-engine fighter aircraft of the era, which led to the installation of a low-fuel warning indicator in the cockpit that notified pilots when remaining fuel fell below .

Unit cost for an Me 262 airframe, less engines, armament, and electronics, was "RM"87,400. To build one airframe took around 6,400 man-hours.

On 19 April 1944, "Erprobungskommando" 262 was formed at Lechfeld just south of Augsburg, as a test unit ("Jäger Erprobungskommando Thierfelder", commanded by "Hauptmann" Werner Thierfelder) to introduce the 262 into service and train a corps of pilots to fly it. On 26 July 1944, Leutnant Alfred Schreiber with the 262 A-1a W.Nr. 130 017 damaged a Mosquito reconnaissance aircraft of No. 540 Squadron RAF PR Squadron, which was allegedly lost in a crash upon landing at an air base in Italy. Other sources state the aircraft was damaged during evasive manoeuvres and escaped.

Major Walter Nowotny was assigned as commander after the death of Thierfelder in July 1944, and the unit redesignated "Kommando Nowotny". Essentially a trials and development unit, it mounted the world's first jet fighter operations. Trials continued slowly, with initial operational missions against the Allies in August 1944, and the unit made claims for 19 Allied aircraft in exchange of six Me 262s lost.

Despite orders to stay grounded, Nowotny chose to fly a mission against an enemy bomber formation flying some above, on 8 November 1944. He claimed two P-51Ds destroyed before suffering engine failure at high altitude. Then, while diving and trying to restart his engines, he was attacked by other Mustangs, forced to bail out, and died. The "Kommando" was then withdrawn for further flight training and a revision of combat tactics to optimise the 262's strengths.

On 26 November 1944, a Me 262A-2a Sturmvogel of III."Gruppe"/KG 51 'Edelweiß' based at Rheine-Hopsten Air Base near Osnabrück was the first confirmed ground-to-air kill of a jet combat aircraft. The 262 was shot down by a Bofors gun of B.11 Detachment of 2875 Squadron RAF Regiment at the RAF forward airfield of Helmond, near Eindhoven. Others were lost to ground fire on 17 and 18 December when the same airfield was attacked at intervals by a total of 18 Me 262s and the guns of 2873 and 2875 Squadrons RAF Regiment damaged several, causing at least two to crash within a few miles of the airfield. In February 1945, a B.6 gun detachment of 2809 Squadron RAF Regiment shot down another Me 262 over the airfield of Volkel. The final appearance of 262s over Volkel was in 1945 when yet another fell to 2809's guns.

By January 1945, "Jagdgeschwader" 7 (JG 7) had been formed as a pure jet fighter wing, partly based at Parchim although it was several weeks before it was operational. In the meantime, a bomber unit—I "Gruppe", "Kampfgeschwader" 54 (KG(J) 54)—redesignated as such on 1 October 1944 through being re-equipped with, and trained to use the Me 262A-2a fighter-bomber for use in a ground-attack role. However, the unit lost 12 jets in action in two weeks for minimal returns. "Jagdverband 44" (JV 44) was another Me 262 fighter unit, of squadron ("Staffel") size given the low numbers of available personnel, formed in February 1945 by Lieutenant General Adolf Galland, who had recently been dismissed as Inspector of Fighters. Galland was able to draw into the unit many of the most experienced and decorated Luftwaffe fighter pilots from other units grounded by lack of fuel.

During March, Me 262 fighter units were able, for the first time, to mount large-scale attacks on Allied bomber formations. On 18 March 1945, thirty-seven Me 262s of JG 7 intercepted a force of 1,221 bombers and 632 escorting fighters. They shot down 12 bombers and one fighter for the loss of three Me 262s. Although a 4:1 ratio was exactly what the Luftwaffe would have needed to make an impact on the war, the absolute scale of their success was minor, as it represented only 1% of the attacking force.

In the last days of the war, Me 262s from JG 7 and other units were committed in ground assault missions, in an attempt to support German troops fighting Red Army forces. Just south of Berlin, halfway between Spremberg and the German capital, Wehrmacht's 9th Army (with elements from the 12 Army and 4th Panzer Army) was assaulting the Red Army's 1st Ukrainian Front. To support this attack, on 24 April, JG 7 dispatched thirty-one Me 262s on a strafing mission in the Cottbus-Bautzen area. Luftwaffe pilots claimed six lorries and seven Soviet aircraft, but three German jets were lost. On the evening of 27 April, thirty-six Me 262s from JG 7, III.KG(J)6 and KJ(J)54 were sent against Soviet forces that were attacking German troops in the forests north-east of Baruth. They succeeded in strafing 65 Soviet lorries, after which the Me 262s intercepted low flying IL-2 Sturmoviks searching for German tanks. The jet pilots claimed six Sturmoviks for the loss of three Messerschmitts. During operations between 28 April and 1 May Soviet fighters and ground fire downed at least ten more Me 262s from JG 7.
However, JG 7 managed to keep its jets operational until the end of the war. And on the 8th of May, at around 4:00 p.m. "Oblt." Fritz Stehle of 2./JG 7, while flying a Me 262 on the Erzgebirge, attacked a formation of Soviet aircraft. He claimed a Yakovlev Yak-9, but the plane shot down was probably a P-39 Airacobra. Soviet records show that they lost two Airacobras, one of them probably downed by Stehle, who would thus have scored the last Luftwaffe air victory of the war.
Several two-seat trainer variants of the Me 262, the Me 262 B-1a, had been adapted through the "Umrüst-Bausatz 1" factory refit package as night fighters, complete with on-board FuG 218 "Neptun" high-VHF band radar, using "Hirschgeweih" ("stag's antlers") antennae with a set of dipole elements shorter than the "Lichtenstein SN-2" had used, as the B-1a/U1 version. Serving with 10. "Staffel" "Nachtjagdgeschwader" 11, near Berlin, these few aircraft (alongside several single-seat examples) accounted for most of the 13 Mosquitoes lost over Berlin in the first three months of 1945. Intercepts were generally or entirely made using "Wilde Sau" methods, rather than AI radar-controlled interception. As the two-seat trainer was largely unavailable, many pilots made their first jet flight in a single-seater without an instructor.

Despite its deficiencies, the Me 262 clearly marked the beginning of the end of piston-engined aircraft as effective fighting machines. Once airborne, it could accelerate to speeds over , about faster than any Allied fighter operational in the European Theater of Operations.

The Me 262's top ace was probably "Hauptmann" Franz Schall with 17 kills, including six four-engine bombers and ten P-51 Mustang fighters, although fighter ace "Oberleutnant" Kurt Welter claimed 25 Mosquitos and two four-engine bombers shot down by night and two further Mosquitos by day. Most of Welter's claimed night kills were achieved by eye, even though Welter had tested a prototype Me 262 fitted with FuG 218 "Neptun" radar. Another candidate for top ace on the aircraft was "Oberstleutnant" Heinrich Bär, who is credited with 16 enemy aircraft while flying Me262s out of his total of 240 aircraft shot down.

The Me 262 was so fast that German pilots needed new tactics to attack Allied bombers. In the head-on attack, the combined closing speed of about was too high for accurate shooting, with ordnance that could only fire about 44 shells a second (650 rounds/min from each cannon) in total from the quartet of them. Even from astern, the closing speed was too great to use the short-ranged quartet of MK 108 cannon to maximum effect. Therefore, a roller-coaster attack was devised. The 262s approached from astern and about than the bombers. From about , they went into a shallow dive that took them through the escort fighters with little risk of interception. When they were about and below the bombers, they pulled up sharply to reduce speed. On levelling off, they were and overtaking the bombers at about , well placed to attack them.

Since the 30mm MK 108 cannon's short barrels and low muzzle velocity (only ) rendered it inaccurate beyond , coupled with the jet's velocity, which required breaking off at to avoid colliding with the target, Me 262 pilots normally commenced firing at . Gunners of Allied bomber aircraft found their electrically powered gun turrets had problems tracking the jets. Target acquisition was difficult because the jets closed into firing range quickly and remained in firing position only briefly, using their standard attack profile, which proved more effective.

A prominent Royal Navy test pilot, Captain Eric Brown, chief naval test pilot and commanding officer of the Captured Enemy Aircraft Flight Royal Aircraft Establishment, who tested the Me 262 noted: "This was a Blitzkrieg aircraft. You whack in at your bomber. It was never meant to be a dogfighter, it was meant to be a destroyer of bombers... The great problem with it was it did not have dive brakes. For example, if you want to fight and destroy a B-17, you come in on a dive. The 30mm cannon were not so accurate beyond . So you normally came in at and would open fire on your B-17. And your closing speed was still high and since you had to break away at to avoid a collision, you only had two seconds firing time. Now, in two seconds, you can't sight. You can fire randomly and hope for the best. If you want to sight and fire, you need to double that time to four seconds. And with dive brakes, you could have done that."

Eventually, German pilots developed new combat tactics to counter Allied bombers' defences. Me 262s, equipped with up to 24 unguided folding-fin R4M rockets—12 in each of two underwing racks, outboard of the engine nacelle—approached from the side of a bomber formation, where their silhouettes were widest, and while still out of range of the bombers' machine guns, fired a salvo of rockets with strongly brisant Hexogen-filled warheads, exactly the same explosive in the shells fired by the Me 262A's quartet of MK 108 cannon. One or two of these rockets could down even the famously rugged Boeing B-17 Flying Fortress, from the "metal-shattering" brisant effect of the fast-flying rocket's explosive warhead. The much more massive BR 21 large-calibre rockets, used from their tubular launchers in undernose locations for an Me 262A's use (one either side of the nosewheel well) were only as fast as the MK 108's shells.

Though this broadside-attack tactic was effective, it came too late to have a real effect on the war, and only small numbers of Me 262s were equipped with the rocket packs. Most of those so equipped were Me 262A-1a models, members of "Jagdgeschwader" 7. This method of attacking bombers became the standard, and mass deployment of Ruhrstahl X-4 guided missiles was cancelled. Some nicknamed this tactic the Luftwaffe's Wolf Pack, as the fighters often made runs in groups of two or three, fired their rockets, then returned to base. On 1 September 1944, USAAF General Carl Spaatz expressed the fear that if greater numbers of German jets appeared, they could inflict losses heavy enough to force cancellation of the Allied bombing offensive by daylight.

The Me 262 was difficult to counter because its high speed and rate of climb made it hard to intercept. However, as with other turbojet engines at the time, the Me 262's engines did not provide sufficient thrust at low air speeds and throttle response was slow, so that in certain circumstances such as takeoff and landing the aircraft became a vulnerable target. Another disadvantage that pioneering jet aircraft of the World War II era shared, was the high risk of compressor stall and if throttle movements were too rapid, the engine(s) could suffer a flameout. The coarse opening of the throttle would cause fuel surging and lead to excessive jet pipe temperatures. Pilots were instructed to operate the throttle gently and avoid quick changes. German engineers introduced an automatic throttle regulator later in the war but it only partly alleviated the problem.

The plane had, by contemporary standards, a high wing loading (294.0 kg/m, 60.2 lbs/ft) that required higher takeoff and landing speeds. Due to poor throttle response, the engines' tendency for airflow disruption that could cause the compressor to stall was ubiquitous. The high speed of the Me 262 also presented problems when engaging enemy aircraft, the high-speed convergence allowing Me 262 pilots little time to line up their targets or acquire the appropriate amount of deflection. This problem faces any aircraft that approaches another from behind at much higher speed, as the slower aircraft in front can always pull a tighter turn, forcing the faster aircraft to overshoot.

Luftwaffe pilots eventually learned how to handle the Me 262's higher speed and the Me 262 soon proved a formidable air superiority fighter, with pilots such as Franz Schall managing to shoot down seventeen enemy fighters in the Me 262, ten of them American P-51 Mustangs. Other notable Me 262 aces included Georg-Peter Eder, with twelve enemy fighters to his credit (including nine P-51s), Erich Rudorffer also with twelve enemy fighters to his credit, Walther Dahl with eleven (including three Lavochkin La-7s and six P-51s) and Heinz-Helmut Baudach with six (including one Spitfire and two P-51s) amongst many others.

Pilots soon learned that the Me 262 was quite maneuverable despite its high wing loading and lack of low-speed thrust, especially if attention was drawn to its effective maneuvering speeds. The controls were light and effective right up to the maximum permissible speed and perfectly harmonised. The inclusion of full span automatic leading-edge slats, something of a "tradition" on Messerschmitt fighters dating back to the original Bf 109's outer wing slots of a similar type, helped increase the overall lift produced by the wing by as much as 35% in tight turns or at low speeds, greatly improving the aircraft's turn performance as well as its landing and takeoff characteristics. As many pilots soon found out, the Me 262's clean design also meant that it, like all jets, held its speed in tight turns much better than conventional propeller-driven fighters, which was a great potential advantage in a dogfight as it meant better energy retention in maneuvers.
Too fast to catch for the escorting Allied fighters, the Me 262s were almost impossible to head off. As a result, Me 262 pilots were relatively safe from the Allied fighters, as long as they did not allow themselves to get drawn into low-speed turning contests and saved their maneuvering for higher speeds. Combating the Allied fighters could be effectively done the same way as the U.S. fighters fought the more nimble, but slower, Japanese fighters in the Pacific.

Allied pilots soon found that the only reliable way to destroy the jets, as with the even faster Me 163B "Komet" rocket fighters, was to attack them on the ground or during takeoff or landing. Luftwaffe airfields identified as jet bases were frequently bombed by medium bombers, and Allied fighters patrolled over the fields to attack jets trying to land. The Luftwaffe countered by installing extensive "flak" alleys of anti-aircraft guns along the approach lines to protect the Me 262s from the ground—and by providing top cover during the jets' takeoff and landing with the most advanced Luftwaffe single-engined fighters, the Focke-Wulf Fw 190D and (just becoming available in 1945) Focke-Wulf Ta 152H. Nevertheless, in March–April 1945, Allied fighter patrol patterns over Me 262 airfields resulted in numerous jet losses.

As the Me 262A's pioneering Junkers Jumo 004 axial-flow jet engines needed careful nursing by their pilots, these jet aircraft were particularly vulnerable during takeoff and landing. Lt. Chuck Yeager of the 357th Fighter Group was one of the first American pilots to shoot down an Me 262, which he caught during its landing approach. On 7 October 1944, Lt. Urban Drew of the 365th Fighter Group shot down two Me 262s that were taking off, while on the same day Lt. Col. Hubert Zemke, who had transferred to the Mustang equipped 479th Fighter Group, shot down what he thought was a Bf 109, only to have his gun camera film reveal that it may have been an Me 262. On 25 February 1945, Mustangs of the 55th Fighter Group surprised an entire "Staffel" of Me 262As at takeoff and destroyed six jets.

The British Hawker Tempest scored several kills against the new German jets, including the Messerschmitt Me 262. Hubert Lange, a Me 262 pilot, said: "the Messerschmitt Me 262's most dangerous opponent was the British Hawker Tempest—extremely fast at low altitudes, highly manoeuvrable and heavily armed." Some were destroyed with a tactic known to the Tempest 135 Wing as the "Rat Scramble": Tempests on immediate alert took off when an Me 262 was reported airborne. They did not intercept the jet, but instead flew towards the Me 262 and Ar 234 base at Hopsten air base. The aim was to attack jets on their landing approach, when they were at their most vulnerable, travelling slowly, with flaps down and incapable of rapid acceleration. The German response was the construction of a "flak lane" of over 150 emplacements of the 20 mm "Flakvierling" quadruple autocannon batteries at Rheine-Hopsten to protect the approaches. After seven Tempests were lost to flak at Hopsten in a week, the "Rat Scramble" was discontinued.

Adolf Busemann had proposed swept wings as early as 1935; Messerschmitt researched the topic from 1940. In April 1941, Busemann proposed fitting a 35° swept wing ("Pfeilflügel II", literally "arrow wing II") to the Me 262, the same wing-sweep angle later used on both the American F-86 Sabre and Soviet Mikoyan-Gurevich MiG-15 fighter jets. Though this was not implemented, he continued with the projected HG II and HG III ("Hochgeschwindigkeit", "high-speed") derivatives in 1944, designed with a 35° and 45° wing sweep, respectively.

Interest in high-speed flight, which led him to initiate work on swept wings starting in 1940, is evident from the advanced developments Messerschmitt had on his drawing board in 1944. While the Me 262 V9 "Hochgeschwindigkeit I" (HG I) flight-tested in 1944 had only small changes compared to combat aircraft, most notably a low-profile canopy—tried as the "Rennkabine" (literally "racing cabin") on the ninth Me 262 prototype for a short time—to reduce drag, the HG II and HG III designs were far more radical. The projected HG II combined the low-drag canopy with a 35° wing sweep and a V-tail (butterfly tail). The HG III had a conventional tail, but a 45° wing sweep and turbines embedded in the wing roots.

Messerschmitt also conducted a series of flight tests with the series production Me 262. Dive tests determined that the Me 262 went out of control in a dive at Mach 0.86, and that higher Mach numbers would cause a nose-down trim that the pilot could not counter. The resulting steepening of the dive would lead to even higher speeds and the airframe would disintegrate from excessive negative g loads.

The HG series of Me 262 derivatives was believed capable of reaching transonic Mach numbers in level flight, with the top speed of the HG III being projected as Mach 0.96 at altitude. After the war, the Royal Aircraft Establishment, at that time one of the leading institutions in high-speed research, re-tested the Me 262 to help with British attempts at exceeding Mach 1. The RAE achieved speeds of up to Mach 0.84 and confirmed the results from the Messerschmitt dive-tests. The Soviets ran similar tests.

After Willy Messerschmitt's death in 1978, the former Me 262 pilot Hans Guido Mutke claimed to have exceeded Mach 1 on 9 April 1945 in a Me 262 in a "straight-down" 90° dive. This claim relies solely on Mutke's memory of the incident, which recalls effects other Me 262 pilots observed below the speed of sound at high indicated airspeed, but with no altitude reading required to determine the speed. The pitot tube used to measure airspeed in aircraft can give falsely elevated readings as the pressure builds up inside the tube at high speeds. The Me 262 wing had only a slight sweep, incorporated for trim (center of gravity) reasons and likely would have suffered structural failure due to divergence at high transonic speeds. One airframe—the aforementioned Me 262 V9, Werknummer 130 004, with "Stammkennzeichen" of VI+AD, was prepared as the HG I test airframe with the low-profile "Rennkabine" racing-canopy and may have achieved an unofficial record speed for a turbojet-powered aircraft of , altitude unspecified, even with the recorded wartime airspeed record being set on 6 July 1944, by another Messerschmitt design—the Me 163B V18 rocket fighter setting a record, but landing with a nearly disintegrated rudder surface.

About 1,400 planes were produced, but a maximum of 200 were operational at any one time. According to sources they destroyed from 300 to 450 enemy planes, with the Allies destroying about one hundred Me 262s in the air. While Germany was bombed intensively, production of the Me 262 was dispersed into low-profile production facilities, sometimes little more than clearings in the forests of Germany and occupied countries. Through the end of February to the end of March 1945, approximately sixty Me 262s were destroyed in attacks on Obertraubling and thirty at Leipheim; the Neuburg jet plant itself was bombed on 19 March 1945.

Large, heavily protected underground factories were constructed - as with the partly-buried Weingut I complex for Jumo 004 jet engine production - to take up production of the Me 262, safe from bomb attacks. A disused mine complex under the Walpersberg mountain was adapted for production of complete aircraft. These were hauled to the flat top of the hill where a runway had been cleared, and flown out. Between 20 and 30 Me 262s were built here. The war ended before this and other underground factories could reach meaningful output. Wings were produced in Germany's oldest motorway tunnel at Engelberg, to the west of Stuttgart. At "B8 Bergkristall-Esche II" at St. Georgen/Gusen, Austria, slave labourers of concentration camp Gusen II produced fully equipped fuselages for the Me 262 at a monthly rate of 450 units on large assembly lines from early 1945. Gusen II was known as one of the harshest concentration camps; the typical life expectancy was six months. An estimated 35,000 to 50,000 people died on the forced labour details for the Me 262.

After the end of the war, the Me 262 and other advanced German technologies were quickly swept up by the Soviets, British and Americans, as part of the USAAF's Operation Lusty. Many Me 262s were found in readily repairable condition and were confiscated. The Soviets, British and Americans wished to evaluate the technology, particularly the engines.

During testing, the Me 262 was found to be faster than the British Gloster Meteor jet fighter, and had better visibility to the sides and rear (mostly due to the canopy frames and the discoloration caused by the plastics used in the Meteor's construction), and was a superior gun platform to the Meteor F.1 which had a tendency to snake at high speed and exhibited "weak" aileron response. The Me 262 had a shorter range than the Meteor and had less reliable engines.

The USAAF compared the P-80 Shooting Star and Me 262, concluding that the Me 262 was superior in acceleration and speed, with similar climb performance. The Me 262 appeared to have a higher critical Mach number than any American fighter.

The Americans also tested a Me 262A-1a/U3 unarmed photo reconnaissance version, which was fitted with a fighter nose and a smooth finish. Between May and August 1946, the aircraft completed eight flights, lasting four hours and forty minutes. Testing was discontinued after four engine changes were required during the course of the tests, culminating in two single-engine landings. These aircraft were extensively studied, aiding development of early US, British and Soviet jet fighters. The F-86, designed by engineer Edgar Schmued, used a slat design based on the Me 262's.

The Czechoslovak aircraft industry continued to produce single-seat (Avia S-92) and two-seat (Avia CS-92) variants of the Me 262 after World War II. From August 1946, a total of nine S-92s and three two-seater CS-92s were completed and test flown. They were introduced in 1947 and in 1950 were supplied to the 5th Fighter Squadron, becoming the first jet fighters to serve in the Czechoslovak Air Force. These were kept flying until 1951, when they were replaced in service by Soviet jet fighters. Both versions are on display at the Prague Aviation museum in Kbely.

In January 2003, the American Me 262 Project, based in Everett, Washington, completed flight testing to allow the delivery of partially updated spec reproductions of several versions of the Me 262 including at least two B-1c two-seater variants, one A-1c single seater and two "convertibles" that could be switched between the A-1c and B-1c configurations. All are powered by General Electric CJ610 engines and feature additional safety features, such as upgraded brakes and strengthened landing gear. The "c" suffix refers to the new CJ610 powerplant and has been informally assigned with the approval of the Messerschmitt Foundation in Germany (the Werknummer of the reproductions picked up where the last wartime produced Me 262 left off – a continuous airframe serial number run with a near 60-year production break).

Flight testing of the first newly manufactured Me 262 A-1c (single-seat) variant (Werknummer 501244) was completed in August 2005. The first of these machines (Werknummer 501241) went to a private owner in the southwestern United States, while the second (Werknummer 501244) was delivered to the Messerschmitt Foundation at Manching, Germany. This aircraft conducted a private test flight in late April 2006, and made its public debut in May at the ILA 2006. The new Me 262 flew during the public flight demonstrations. Me 262 Werknummer 501241 was delivered to the Collings Foundation as White 1 of JG 7; this aircraft offered ride-along flights starting in 2008. The third replica, a non-flyable Me 262 A-1c, was delivered to the Evergreen Aviation & Space Museum in May 2010.
"Note:"- U = " Umrüst-Bausatz" – conversion kit installed at factory level, denoted as a suffix in the form /U"n".

Rüstsatze may be applied to various sub-types of their respective aircraft type, denoted as a suffix in the form /R"n".
"Data from:"'Messerschmitt Me 262A Schwalbe


A series of reproductions was constructed by American company Legend Flyers (later Me 262 Project) of Everett, Washington. The Jumo 004 engines of the original are replaced by more reliable General Electric CJ610 engines. The first Me 262 reproduction (a two-seater) took off for the first time in December 2002 and the second one in August 2005. This one was delivered to the Messerschmitt Foundation and was presented at the ILA airshow in 2006.






</doc>
<doc id="20493" url="https://en.wikipedia.org/wiki?curid=20493" title="Masuria">
Masuria

Masuria (, , Masurian: "Mazurÿ") is a region in northeastern Poland, famous for its 2,000 lakes. Masuria occupies much of the Masurian Lake District. Administratively, it is part of the Warmian-Masurian Voivodeship (administrative area/province). Its biggest city, often regarded as its capital, is Ełk. The region covers a territory of some 10,000 km and had a population in 2013 of 59,790.

Before the 13th century, the territory was inhabited by the Old Prussians also called Baltic Prussians, a Baltic ethnic group that lived in Prussia (the area of the southeastern coastal region of the Baltic Sea neighbouring of the Baltic Sea around the Vistula Lagoon and the Curonian Lagoon). The territory later called Masuria was then known as Galindia and was probably a peripheral, deeply forested and lightly populated area. Its inhabitants spoke a language now known as Old Prussian and had their own mythology. Although a 19th-century German political entity bore their name, they were not Germans. They were converted to Roman Catholicism in the 13th century, after conquest by the Knights of the Teutonic Order.

Estimates range from about 170,000 to 220,000 Old Prussians living in the whole of Prussia around 1200. The wilderness was their natural barrier against attack by would-be invaders. During the Northern Crusades of the early 13th century, the Old Prussians used this wide forest as a brod zone of defence. They did so again against the Knights of the Teutonic Order, who had been invited to Poland by Konrad I of Masovia in 1226. The order's goal was to convert the native population to Christianity and baptise it by force if necessary. In the subsequent conquest, which lasted over 50 years, the original population was partly exterminated, particularly during the major Prussian rebellion of 1261–83. But several Prussian noble families also accommodated to the Knights in order to hold their power and possessions.

After the Order's acquisition of Prussia, Poles (or more specifically, Mazurs, that is inhabitants of the adjacent region of Mazovia) began to settle in the southeastern part of the conquered region. German, Dutch, Flemish, and Danish colonists entered the area afterward, from the northwest. The number of Polish settlers grew significantly again in the beginning of the 15th century, especially after the first and the second treaties of Thorn, in 1411 and 1466 respectively, following the Thirteen Years' War and the final defeat of the order. The Battle of Grunwald took place in western Masuria in 1410. In 1440 the anti-Teutonic Prussian Confederation was founded. In 1454 upon the Confederation's request King Casimir IV of Poland signed the act of incorporation of the entire region including Masuria to Poland and after the subsequent Thirteen Years' War Masuria came under the suzerainty of the Polish Crown, still ruled by the grand master of the Teutonic Order. Later assimilation of the German settlers as well as the Polish immigrants and native Prussian inhabitants created the new Prussian identity, although the subregional difference between the German- and Polish-speaking part remained.

The secularization of the Teutonic Order in Prussia and the conversion of Albert of Prussia to Lutheranism in 1525 brought Prussia including the area later called Masuria to Protestantism. The Knights untied their bonds to the Catholic Church and became land owning nobleman and the Duchy of Prussia was established as a vassal state of Poland. The Polish language predominated due to the many immigrants from Mazovia, who additionally settled the southern parts of Ducal Prussia, till then virgin part of (later Masuria) in the 16th century. While the southern countryside was inhabited by these - meanwhile Protestant - Polish-speakers, who took refuge, the very small southern towns constituted German mixed with Polish-speaking population. The ancient Old Prussian language survived in parts of the countryside in the northern and central parts of Ducal Prussia until the early 18th century. At that time they proved to be assimilated in the mass of German speaking villagers and farmers Areas that had many Polish language speakers were known as the Polish Departments.
Masuria became one of the leading centers of Polish Protestantism. In the mid-16th century Lyck (Ełk) and Angerburg (Węgorzewo) became significant Polish printing centers. A renowned Polish high school, which attracted Polish students from different regions, was founded in Ełk in eastern Masuria in 1546 by Hieronim Malecki, Polish translator and publisher, who contributed to the creation of the standards and patterns of the Polish literary language. The westernmost part of Masuria, the Osterode (Ostróda) county, in 1633 came under the administration of one of the last dukes of the Piast dynasty, John Christian of Brieg.

In 1656, during the Battle of Prostki, the forces of Polish–Lithuanian Commonwealth, including 2,000 Tatar raiders, beat the allied Swedish and Brandenburg army capturing Bogusław Radziwiłł. The war resulted in the destruction of most towns, 249 villages and settlements, and 37 churches were destroyed. Over 50% of the population of Masuria died within the years 1656–1657, 23,000 were killed, another 80,000 died of diseases and famine, 3,400 people were enslaved and deported to Russia. From 1709–1711, in all of Ducal Prussia between 200,000 and 245,000 out of 600,000 inhabitants died from the Black Death. In Masuria the death toll varied regionally; while 6,789 people died in the district of Rhein (Ryn) only 677 died in Seehesten (Szestno). In Lötzen (Giżycko) 800 out of 919 people died. Losses in population were compensated by migration of Protestant settlers or refugees from Scotland, Salzburg (expulsion of Protestants 1731), France (Huguenot refugees after the Edict of Fontainebleau in 1685), and especially from the counterreformed Polish–Lithuanian Commonwealth, including Polish brethren expelled from Poland in 1657. The last group of refugees to emigrate to Masuria were the Russian Philipons (as 'Old Believers' opposed to the State Church) in 1830, when King Frederick William III of Prussia granted them asylum.
After the death of Albert Frederick, Duke of Prussia in 1618, his son-in-law John Sigismund, Margrave of Brandenburg, inherited the duchy (including Masuria), combining the two territories under a single dynasty and forming Brandenburg-Prussia. The Treaty of Wehlau revoked the sovereignty of the King of Poland in 1657.

The region became part of the Kingdom of Prussia with the coronation of King Frederick I of Prussia in 1701 in Königsberg. Masuria became part of a newly created administrative province of East Prussia upon its creation in 1773. The name "Masuria" began to be used officially after new administrative reforms in Prussia after 1818. Masurians referred to themselves during that period as "Polish Prussians" or as "Staroprusaki" (Old Prussians) During the Napoleonic Wars and Polish national liberation struggles, in 1807, several towns of northern and eastern Masuria were taken over by Polish troops under the command of generals Jan Henryk Dąbrowski and Józef Zajączek. Some Masurians showed considerable support for the Polish uprising in 1831, and maintained many contacts with Russian-held areas of Poland beyond the border of Prussia, the areas being connected by common culture and language; before the uprising people visited each other's country fairs and much trade took place, with smuggling also widespread Nevertheless, their Lutheran belief and a traditional adherence to the Prussian royal family kept Masurians and Poles separated. Some early writers about Masurians - like Max Toeppen - postulated Masurians in general as mediators between German and Slav cultures.
Germanisation policies in Masuria included various strategies, first and foremost they included attempts to propagate the German language and to eradicate the Polish (Masurian) language as much as possible; German became the obligatory language in schools from 1834 on. The Lutheran churches and their vicars principally exerted their spiritual care in Masurian as concerned to Masurian mother tongue parishioners

Mother tongue of the inhabitants of Masuria, by county, during the first half of the 19th century:
After the Unification of Germany into the German Empire in 1871, the last lessons that made use of the Polish language were removed from schools in 1872. Masurians who expressed sympathy for Poland were deemed "national traitors" by German public opinion, especially after 1918 when the new Polish republic laid claims to, up to then German, areas inhabited by Polish speakers According to Stefan Berger, after 1871 the Masurians in the German Empire were seen in a view that while acknowledging their "objective" Polishness (in terms of culture and language) they felt "subjectively" German and thus should be tightly integrated into the German nation-state; Berger concludes that such arguments of German nationalists were aimed at integrating Masurian (and Silesian) territory firmly into the German Reich.

During the period of the German Empire, the Germanisation policies in Masuria became more widespread; children using Polish in playgrounds and classrooms were widely punished by corporal punishment, and authorities tried to appoint Protestant pastors who would use only German instead of bilinguality and this resulted in protests of local parishioners. According to Jerzy Mazurek the native Polish-speaking population, like in other areas with Polish inhabitants, faced discrimination of Polish language activities from Germanised local administration. In this climate a first resistance defending the rights of rural population was organized; according to Jerzy Mazurek usually by some teachers engaged in publishing Polish language newspapers.

Despite anti-Polish policies, such Polish language newspapers as the "Pruski Przyjaciel Ludu" (Prussian Friend of People) or the "Kalendarz Królewsko-Pruski Ewangelicki" (Royal Prussian Evangelical Calendar) or bilingual journals like the "Oletzkoer Kreisblatt - Tygodnik Obwodu Oleckiego" continued to be published in Masuria. In contrast to the Prussian-oriented periodicals, in the late 19th century such newspapers as "Przyjaciel Ludu Łecki" and "Mazur" were founded by members of the Warsaw-based "Komitet Centralny dla Śląska, Kaszub i Mazur" (Central Committee for Silesia, Kashubia and Masuria), influenced by Polish politicians like Antoni Osuchowski or Juliusz Bursche, to strengthen the Polish identity in Masuria. The "Gazeta Ludowa" (The Folk's Newspaper) was published in Lyck in 1896–1902, with 2,500 copies in 1897 and the "Mazur" in Ortelsburg after 1906 with 500 copies in 1908 and 2,000 prior to World War I.

Polish activists started to regard Masurians as "Polish brothers" after Wojciech Kętrzyński had published his pamphlet "O Mazurach" in 1872 and Polish activists engaged in active self-help against repressions by the German state Kętrzyński fought against attempts to Germanise Masuria
The attempts to create a Masurian Polish national consciousness, largely originating from nationalist circles of Provinz Posen, however faced the resistance of the Masurians, who, despite having similar folk traditions and linguistics to Poles, regarded themselves as Prussians and later Germans. and were loyal to the Hohenzollern dynasty, the Prussian and German state. After World War I the editor of the Polish language "Mazur" described the Masurians as "not nationally conscious, on the contrary, the most loyal subjects of the Prussian king". However, a minority of Masurians did exist who expressed Polish identity
After 1871 there appeared resistance among the Masurians towards Germanisation efforts, the so-called Gromadki movement was formed which supported use of Polish language and came into conflict with German authorities; while most of its members viewed themselves as loyal to the Prussian state, a part of them joined the Pro-Polish faction of Masurians. The programme of Germanisation started to unite and mobilise Polish people in Polish-inhabited territories held by Germany including Masuria A Polish-oriented party, the "Mazurska Partia Ludowa" ("People's Party of Masuria"), was founded in 1897. The eastern areas of the German Empire were systematically Germanised with changing of names and public signs, and the German state fostered cultural imperialism, in addition to giving financial and other support to German farmers, officials, and teachers to settle in the east.

The German authorities in their efforts of Germanisation tried to claim the Masurian language separate from Polish by classifying it as a non-Slavic language different from Polish one, this was reflected in official census
Thus the Masurian population in 1890, 143,397 was reported to the Prussian census as having German as their language (either primary or secondary), 152,186 Polish and 94,961 Masurian. In 1910, the German language was reported by German authorities as used by 197,060, Polish by 30,121 and Masurian by 171,413. Roman Catholics generally opted for the Polish language, Protestants appreciated Masurian. In 1925, German authorities reported 40,869 inhabitants as having declared Masurian as their native tongue and 2,297  as Polish. However, the last result may have been a result of politics at the time, the desire of the population to be German after the trauma evoked by 1920 plebiscite. So the province could be presented as - so called - 'purely German'; in reality the Masurian dialect was among bilinguals still in use.

Throughout industrialisation in the late 19th century about 10 percent of the Masurian populace emigrated to the Ruhr Area, where about 180,000 Masurians lived in 1914. Wattenscheid, Wanne and Gelsenkirchen were the centers of Masurian emigration and Gelsenkirchen-Schalke was even called Klein (little)-Ortelsburg before 1914. Masurian newspapers like the "Przyjaciel Ewangeliczny" and the "Gazeta Polska dla Ludu staropruskiego w Westfalii i na Mazurach" but also the German language "Altpreußische Zeitung" were published.

During World War I, the Battle of Tannenberg and the First and Second Battle of the Masurian Lakes between Imperial Germany and the Russian Empire took place within the borders of Masuria in 1914. After the war, the League of Nations held the East Prussian plebiscite on 11 July 1920 to determine if the people of the southern districts of East Prussia wanted to remain within East Prussia or to join the Second Polish Republic. The German side terrorised the local population before the plebiscite using violence, Polish organisations and activists were harassed by German militias, and those actions included attacks and some supposed murders of Polish activists; Masurs who supported voting for Poland were singled out and subjected to terror and repressions.

Names of those Masurs supporting the Polish side were published in German newspapers, and their photos presented in German shops; afterwards regular hunts were organised after them by German militias terrorizing the Polish minded population. At least 3,000 Warmian and Masurian activists who were engaged for Polish side decided to flee the region. At the same time also local police officials were engaged in active surveillance of the Polish minority and attacks against Polish activists. Before the plebiscite Poles started to flee the region to escape the German harassment and Germanisation policies.

The results determined that 99.32% of the voters in Masuria proper chose to remain with East Prussia. Notwithstanding national German agitation and intimidation, these results reflect that majority Masurians had adopted a German national identity next to a regional identity. Their traditional religious belief in Lutheranism kept them away from Polish national consciousness, dominated by Roman Catholicism. In fact almost only Catholics voted for Poland in the plebiscite. They were to be found as a majority in the villages around the capital Allenstein, the same were Polish cultural activism got hold between 1919 and 1932. However, the contemporary Polish ethnographer Adam Chętnik accused the German authorities of abuses and falsifications during the plebiscite. Moreover, the plebiscite took place during the time when Polish–Soviet War threatened to erase the Polish state. As a result, even many Poles of the region voted for Germany out of fear that if the area was allocated to Poland it would fall under Soviet rule. After the plebiscite in German areas of Masuria attacks on Polish population commenced by German mobs, and Polish priests and politicians were driven from their homes After the plebiscite at least 10,000 Poles had to flee German held Masuria to Poland.

The region of Działdowo (Soldau), where according to the official German census of 1910 ethnic Germans formed a minority of 37.3%, was excluded from the plebiscite and became part of Poland. This was reasoned with placing the railway connection between Warsaw and Danzig (Gdańsk), of vital importance to Poland as it connected central Poland with its recently obtained seacoast, completely under Polish sovereignty. Działdowo itself counted about 24,000 people of which 18,000 were Masurians.

According to the municipal administration of Rybno, after World War I Poles in Działdowo believed that they will be quickly joined with Poland, they organised secret gatherings during which the issue of rejoining Polish state with help of Polish military was discussed. According to the Rybno administration most active Poles in that subregion included Jóżwiakowscy, Wojnowscy, Grzeszczowscy families working under the guidance of politician Leon Wojnowski who protested German attempts to remain Działdowo a part of Germany after the war; other local pro-Polish activists were Alfred Wellenger, Paczyński, Tadeusz Bogdański, Jóźwiakowski.

The historian Andreas Kossert describes that the incorporation happened despite protests of the local populace, the municipal authorities and the German Government, According to Kossert 6,000 inhabitants of the region soon left the area.

In 1920 the candidate of the German Party in Poland, Ernst Barczewski, was elected to the Sejm with 74.6 percent of votes and to the Polish Senate with 34.6% of votes for the Bloc of National Minorities in 1928. During the Polish–Soviet War Działdowo was briefly occupied by the Red Army regarded as liberator from the Polish authority by the local German population, which hoisted the German flag, but it was soon recovered by the Polish Army.

During the interwar period many native inhabitants of Działdowo subregion left and migrated to Germany.

With the start of the German war against Poland on 1 September 1939, the German minority in the parts of Masuria attached to Poland after World War I, as Działdowo but also large parts of former Western-Prussia, organised themselves in paramilitary formations called Selbstschutz (selfdefense) and begun to engage in massacres of local Polish population; Poles were imprisoned, tortured and murdered while Masurians were sometimes forcefully placed on Volksliste
From now on conscripted Masurians had to serve without exception in the German army invading Poland, and Russia two years later on.

The Soldau concentration camp near to Działdowo was established in winter 1939, where 13,000 people were murdered by the Nazi German state during the war. Notable victims included the Polish bishops Antoni Julian Nowowiejski and Leon Wetmański, as well as the nun Mieczysława Kowalska. Additionally, almost 1,900 mentally ill patients from East Prussia and annexed areas of Poland were murdered there as well, in what was known as Action T4.
Polish resistance in Masuria was organised by Paweł Nowakowski "Leśnik" commander of the Home Army's Działdowo district.

Masuria was the only region of Germany directly affected by the battles of World War I. Damaged towns and villages were reconstructed with the aid of several twin towns from western Germany like Cologne to Neidenburg, Frankfurt to Lötzen and even Vienna to Ortelsburg. The architecture still is surprisingly distinct, being of modern Central European character. However Masuria was still largely agrarian-oriented and suffered from the economic decline after World War I, additionally badly affected by the creation of the Polish Corridor, which raised freight costs to the traditional markets in Germany.
The later implemented Osthilfe had only a minor influence on Masuria as it privileged larger estates, while Masurian farms were generally small.

The interwar period was characterised by ongoing Germanisation policies, intensified especially under the Nazis.

In the 1920s Masuria remained a heartland of conservatism with the German National People's Party as strongest party. The Nazi Party, having absorbed the conservative one, became the strongest party already in the Masurian constituencies in the elections of 1930 and received its best results in the poorest areas of Masuria with the highest rate of Polish speakers. Especially in the elections of 1932 and 1933 they reached up to 81 percent of votes in the district of Neidenburg and 80 percent in the district of Lyck. The Nazis used the economic crisis, which had significant effects in far-off Masuria, as well as traditional anti-Polish sentiments while at the same time Nazi political rallies were organised in the Masurian dialect during the campaigning.

In 1938, the Nazi government (1933–1945) changed thousands of still existing toponyms (especially names of cities and villages) of Old Prussian, Lithuanian and Polish origin to newly created German names; six thousand, that meant about 50% of the existing names were changed, but the countryside population stuck to their traditional names. Within six years a new renaming would take place after Poland annexed Masuria in 1945. 
According to German author Andreas Kossert, Polish parties were financed and aided by the Polish government in Warsaw, and remained splintergroups without any political influence, e.g. in the 1932 elections the Polish Party received 147 votes in Masuria proper. According to Wojciech Wrzesiński (1963), the Polish organisations in Masuria had decided to lower their activity in order to escape acts of terror performed against Polish minority activists and organisations by Nazi activists. Jerzy Lanc, a teacher and Polish national who had moved to Masuria in 1931 to establish a Polish school in Piassutten (Piasutno), died in his home of carbon monoxide poisoning, most likely murdered by local German nationalists.

Before the war the Nazi German state sent undercover operatives to spy on Polish organisations and created lists of people that were to be executed or sent to concentration camps. This mainly took place in Silesia and only according to the few catholic schools in Masuria. Information was gathered on who sent children to Polish schools, bought Polish press or took part in Polish ceremonies and organised repressions against these people were executed by Nazi militias. Polish schools, printing presses and headquarters of Polish institutions were attacked as well as homes of the most active Poles; shops owned by Poles were vandalised or demolished. Polish masses were dispersed, and Polish teachers were intimidated as members of the SS gathered under their locals performing songs like "Wenn das Polenblut vom Messer spritzt, dann geht's noch mal so gut" ("When Polish blood spurts from the knife, everything will be better").

The anti-Polish activities intensified in 1939. Those Poles were most active in politics were evicted from their own homes, while Polish newspapers and cultural houses were closed down in the region. Polish masses were banned between June and July in Warmia and Mazury.

In the final moments of August 1939 all remains of political and cultural life of Polish minority was eradicated by the Nazis, with imprisonment of Polish activists and liquidation of Polish institutions. Seweryn Pieniężny, the chief editor of "Gazeta Olsztyńska", who opposed Germanisation of Masuria, was interned. Others included Juliusz Malewski (director of Bank Ludowy of Olsztyn), Stefan Różycki, Leon Włodarczyk (activist of Polonia Warmińsko-Mazurska).

Directors of Polish schools and teachers were imprisoned, as was the staff of Polish pre-schools in the Masuria region. They were often forced to destroy Polish signs, emblems and symbols of Polish institutions.

The Nazis believed that in future, the Masurians, as a separate non-German entity, would 'naturally' disappear in the end, while those who would cling to their "foreigness" as one Nazi report mentioned, would be deported. Local Jews were considered by the Nazis to be subhuman and were to be exterminated. The Nazi authorities also executed Polish activists in Masuria and those who remained alive were sent to concentration camps.
In August 1943 the Uderzeniowe Bataliony Kadrowe attacked the village of Mittenheide (Turośl) in southern Masuria

In 1943 "Związek Mazurski" was reactivated secretly by Masurian activists of the Polish Underground State in Warsaw and led by Karol Małłek. Związek Mazurski opposed Nazi Germany and asked Polish authorities during the war to liquidate German large landowners after the victory over Nazi Germany to help in agricultural reform and settlement of Masurian population, Masurian iconoclasts opposed to Nazi Germany requested to remove German heritage sites "regardless of their cultural value". Additionally a Masurian Institute was founded by Masurian activists in Radość near Warsaw in 1943

In the final stages of World War II, Masuria was partially devastated by the retreating German and advancing Soviet armies during the Vistula-Oder Offensive. The region came under Polish rule at the war's end in the Potsdam Conference. Most of the population fled to Germany or was killed during or after the war, while those which stayed were subject to a "nationality verification", organised by the communist government of Poland. As a result, the number of native Masurians remaining in Masuria was initially relatively high, while most of the population was subsequently expelled. Poles from central Poland and the Polish areas annexed by the Soviet Union as well as Ukrainians expelled from southern Poland throughout the Operation Vistula, were resettled in Masuria.

According to the Masurian Institute the Masurian members of resistance against Nazi Germany who survived the war, became active in 1945 in the region, working in Olsztyn in cooperation with new state authorities in administration, education and cultural affairs. Historic Polish names for most of towns of Masuria were restored, but for some places new names were determined even if there were historic Polish names.

German author Andreas Kossert describes the post-war process of "national verification" as based on an ethnic racism which categorised the local populace according to their alleged ethnic background. A Polish-sounding last name or a Polish-speaking ancestor was sufficient to be regarded as "autochthonous" Polish.
In October 1946 37,736 persons were "verified" as Polish citizens while 30,804 remained "unverified". A center of such "unverified" Masurians was the district of Mrągowo, where in early 1946 out of 28,280 persons, 20,580 were "unverified", while in October, 16,385 still refused to adopt Polish citizenship. However even those who complied with the often used pressure by Polish authorities were in fact treated as Germans because of their Lutheran faith and their often rudimentary knowledge of Polish. Names were "Polonised" and the usage of the German language in public was forbidden. In the late 1940s the pressure to sign the "verification documents" grew and in February 1949 the former chief of the stalinist secret Police (UB) of Łódź, Mieczysław Moczar, started the "Great verification" campaign. Many unverified Masurians were imprisoned and accused of pro-Nazi or pro-American propaganda, even former pro-Polish activists and inmates of Nazi concentration camps were jailed and tortured. After the end of this campaign in the district of Mrągowo (Sensburg) only 166 Masurians were still "unverified".

In 1950 1,600 Masurians left the country and in 1951, 35,000 people from Masuria and Warmia managed to obtain a declaration of their German nationality by the embassies of the United States and Great Britain in Warsaw. Sixty-three percent of the Masurians in the district of Mrągowo (Sensburg) received such a document. In December 1956 Masurian pro-Polish activists signed a memorandum to the Communist Party leadership:
"The history of the people of Warmia and Masuria is full of tragedy and suffering. Injustice, hardship and pain often pressed on the shoulders of Warmians and Masurians... Dislike, injustice and violence surrounds us...They (Warmians and Masurians) demand respect for their differentness, grown in the course of seven centuries and for freedom to maintain their traditions".

Soon after the political reforms of 1956, Masurians were given the opportunity to join their families in West Germany. The majority (over 100 thousand) gradually left and after the improvement of Germano-Polish relations by the German Ostpolitik of the 1970s, 55,227 persons from Warmia and Masuria moved to West Germany in between 1971 and 1988, today approximately between 5,000 and 6,000 Masurians still live in the area, about 50 percent of them members of the German minority in Poland, the remaining half is ethnic Polish. As the Polish journalist Andrzej K. Wróblewski stated, the Polish post-war policy succeeded in what the Prussian state never managed: the creation of a German national consciousness among the Masurians.

Most of the originally Protestant churches in Masuria are now used by the Polish Roman Catholic Church as the number of Lutherans in Masuria declined from 68,500 in 1950 to 21,174 in 1961 and further to 3,536 in 1981. Sometimes, like on 23 September 1979 in the village of Spychowo (Puppen), the Lutheran Parish was even forcefully driven out of their church while liturgy was held.

In modern Masuria the native population has virtually disappeared. Masuria was incorporated into the voivodeship system of administration in 1945. In 1999 Masuria was constituted with neighbouring Warmia as a single administrative province through the creation of the Warmian-Masurian Voivodeship.

Today, numerous summer music festivals take place in Masuria, including the largest reggae festival in Poland in Ostróda, the largest country music festival in Poland in Mrągowo, and one of Poland's largest hip hop music festivals in Giżycko and Ełk.

The Masurian Szczytno-Szymany International Airport gained international attention as press reports alleged the airport to be a so-called ""black site"" involved in the CIA's network of extraordinary renditions.

Masuria and the Masurian Lake District are known in Polish as "Kraina Tysiąca Jezior" and in German as "Land der Tausend Seen", meaning "land of a thousand lakes." These lakes were ground out of the land by glaciers during the Pleistocene ice age around 14,000 - 15,000 years ago, when ice covered northeastern Europe. From that period originates the horn of a reindeer found in the vicinity of Giżycko. By 10,000 BC this ice started to melt. Great geological changes took place and even in the last 500 years the maps showing the lagoons and peninsulas on the Baltic Sea have greatly altered in appearance. More than in other parts of northern Poland, such as from Pomerania (from the River Oder to the River Vistula), this continuous stretch of lakes is popular among tourists. The terrain is rather hilly, with connecting lakes, rivers and streams. Forests account for about 30% of the area. The northern part of Masuria is covered mostly by the broadleaved forest, while the southern part is dominated by pine and mixed forests.

Two largest lakes of Poland, Śniardwy and Mamry, are located in Masuria.






</doc>
<doc id="20497" url="https://en.wikipedia.org/wiki?curid=20497" title="Magnus">
Magnus

Magnus, meaning "great" in Latin, was used as cognomen of Gnaeus Pompeius Magnus in the first century BCE. The best-known use of the name during the Roman Empire is for the fourth-century Western Roman Emperor Flavius Magnus Maximus Augustus, often just called Magnus Maximus. The name gained wider popularity in the Middle Ages among various European peoples and their royal houses, being introduced to them upon being converted to the Latin-speaking Catholic Christianity. This was especially the case with Scandinavian royalty and nobility.

As a Scandinavian forename, it was extracted from the Frankish ruler Charlemagne's Latin name "Carolus Magnus" and re-analyzed as Old Norse "magn-hús" = "power house".

Magnus may refer to:














</doc>
<doc id="20498" url="https://en.wikipedia.org/wiki?curid=20498" title="Médecins Sans Frontières">
Médecins Sans Frontières

MSF's principles and operational guidelines are highlighted in its Charter, the Chantilly Principles, and the later La Mancha Agreement. Governance is addressed in Section 2 of the Rules portion of this final document. MSF has an associative structure, where operational decisions are made, largely independently, by the five operational centres (Amsterdam, Barcelona-Athens, Brussels, Geneva and Paris). Common policies on core issues are coordinated by the International Council, in which each of the 24 sections (national offices) is represented. The International Council meets in Geneva, Switzerland, where the International Office, which coordinates international activities common to the operational centres, is also based.

MSF has general consultative status with the United Nations Economic and Social Council. It received the 1999 Nobel Peace Prize in recognition of its members' continued efforts to provide medical care in acute crises, as well as raising international awareness of potential humanitarian disasters. James Orbinski, who was the president of the organization at the time, accepted the prize on behalf of MSF. Prior to this, MSF also received the 1996 Seoul Peace Prize. Christos Christou succeeded Joanne Liu as international president in June 2019.

During the Nigerian Civil War of 1967 to 1970, the Nigerian military formed a blockade around the nation's newly independent south-eastern region, Biafra. At this time, France was one of the only major countries supportive of the Biafrans (the United Kingdom, the Soviet Union and the United States sided with the Nigerian government), and the conditions within the blockade were unknown to the world. A number of French doctors volunteered with the French Red Cross to work in hospitals and feeding centres in besieged Biafra. One of the co-founders of the organisation was Bernard Kouchner, who later became a high-ranking French politician.

After entering the country, the volunteers, in addition to Biafran health workers and hospitals, were subjected to attacks by the Nigerian army, and witnessed civilians being murdered and starved by the blockading forces. The doctors publicly criticised the Nigerian government and the Red Cross for their seemingly complicit behaviour. These doctors concluded that a new aid organisation was needed that would ignore political/religious boundaries and prioritise the welfare of victims.

The "Groupe d'intervention médicale et chirurgicale en urgence" ("Emergency Medical and Surgical Intervention Group") was formed in 1971 by French doctors who had worked in Biafra, to provide aid and to emphasize the importance of victims' rights over neutrality. At the same time, Raymond Borel, the editor of the French medical journal "TONUS", had started a group called "Secours Médical Français" ("French Medical Relief") in response to the 1970 Bhola cyclone, which killed at least 625,000 in East Pakistan (now Bangladesh). Borel had intended to recruit doctors to provide aid to victims of natural disasters. On 22 December 1971, the two groups of colleagues merged to form "Médecins Sans Frontières".

MSF's first mission was to the Nicaraguan capital, Managua, where a 1972 earthquake had destroyed most of the city and killed between 10,000 and 30,000 people. The organization, today known for its quick response in an emergency, arrived three days after the Red Cross had set up a relief mission. On 18 and 19 September 1974, Hurricane Fifi caused major flooding in Honduras and killed thousands of people (estimates vary), and MSF set up its first long-term medical relief mission.

Between 1975 and 1979, after South Vietnam had fallen to North Vietnam, millions of Cambodians emigrated to Thailand to avoid the Khmer Rouge. In response MSF set up its first refugee camp missions in Thailand. When Vietnam withdrew from Cambodia in 1989, MSF started long-term relief missions to help survivors of the mass killings and reconstruct the country's health care system. Although its missions to Thailand to help victims of war in Southeast Asia could arguably be seen as its first war-time mission, MSF saw its first mission to a true war zone, including exposure to hostile fire, in 1976. MSF spent nine years (1976–1984) assisting surgeries in the hospitals of various cities in Lebanon, during the Lebanese Civil War, and established a reputation for its neutrality and willingness to work under fire. Throughout the war, MSF helped both Christian and Muslim soldiers alike, helping whichever group required the most medical aid at the time. In 1984, as the situation in Lebanon deteriorated further and security for aid groups was minimised, MSF withdrew its volunteers.

Claude Malhuret was elected as the new president of Medicins Sans Frontieres in 1977, and soon after debates began over the future of the organisation. In particular, the concept of "témoignage" ("witnessing"), which refers to speaking out about the suffering that one sees as opposed to remaining silent, was being opposed or played down by Malhuret and his supporters. Malhuret thought MSF should avoid criticism of the governments of countries in which they were working, while Kouchner believed that documenting and broadcasting the suffering in a country was the most effective way to solve a problem.

In 1979, after four years of refugee movement from South Vietnam and the surrounding countries by foot and by boat, French intellectuals made an appeal in "Le Monde" for "A Boat for Vietnam", a project intended to provide medical aid to the refugees. Although the project did not receive support from the majority of MSF, some, including later Minister Bernard Kouchner, chartered a ship called "L’Île de Lumière" ("The Island of Light"), and, along with doctors, journalists and photographers, sailed to the South China Sea and provided some medical aid to the boat people. The splinter organisation that undertook this, Médecins du Monde, later developed the idea of humanitarian intervention as a duty, in particular on the part of Western nations such as France. In 2007 MSF clarified that for nearly 30 years MSF and Kouchner have had public disagreements on such issues as the right to intervene and the use of armed force for humanitarian reasons. Kouchner is in favour of the latter, whereas MSF stands up for an impartial humanitarian action, independent from all political, economic and religious powers.

In 1982, Malhuret and Rony Brauman (who became the organisation's president in 1982) brought increased financial independence to MSF by introducing fundraising-by-mail to better collect donations. The 1980s also saw the establishment of the other operational sections from MSF-France (1971): MSF-Belgium (1980), MSF-Switzerland (1981), MSF-Holland (1984), and MSF-Spain (1986). MSF-Luxembourg was the first support section, created in 1986. The early 1990s saw the establishment of the majority of the support sections: MSF-Greece (1990), MSF-USA (1990), MSF-Canada (1991), MSF-Japan (1992), MSF-UK (1993), MSF-Italy (1993), MSF-Australia (1994), as well as Germany, Austria, Denmark, Sweden, Norway, and Hong Kong (MSF-UAE was formed later). Malhuret and Brauman were instrumental in professionalising MSF. In December 1979, after the Soviet army had invaded Afghanistan, field missions were immediately set up to provide medical aid to the mujahideen, and in February 1980, MSF publicly denounced the Khmer Rouge. During the 1983–1985 famine in Ethiopia, MSF set up nutrition programmes in the country in 1984, but was expelled in 1985 after denouncing the abuse of international aid and the forced resettlements. MSF's explicit attacks on the Ethiopian government led to other NGOs criticizing their abandonment of their supposed neutrality and contributed to a series of debates in France around humanitarian ethics. The group also set up equipment to produce clean drinking water for the population of San Salvador, capital of El Salvador, after 10 October 1986 earthquake that struck the city. In 2014, the European Speedster Assembly had contributed $717,000 to MSF.

Since 1979, MSF has been providing medical humanitarian assistance in Sudan, a nation plagued by starvation and the civil war, prevalent malnutrition and one of the highest maternal mortality rates in the world. In March 2009, it is reported that MSF has employed 4,590 field staff in Sudan tackling issues such as armed conflicts, epidemic diseases, health care and social exclusion. MSF's continued presence and work in Sudan is one of the organization's largest interventions. MSF provides a range of health care services including nutritional support, reproductive healthcare, Kala-Azar treatment, counselling services and surgery to the people living in Sudan. Common diseases prevalent in Sudan include tuberculosis, kala-azar also known as visceral leishmaniasis, meningitis, measles, cholera, and malaria.

Kala-azar, also known as visceral leishmaniasis, has been one of the major health problems in Sudan. After the Comprehensive Peace Agreement between North and Southern Sudan on 9 January 2005, the increase in stability within the region helped further efforts in healthcare delivery. Médicins Sans Frontières tested a combination of sodium stibogluconate and paromomycin, which would reduce treatment duration (from 30 to 17 days) and cost in 2008. In March 2010, MSF set up its first Kala-Azar treatment centre in Eastern Sudan, providing free treatment for this otherwise deadly disease. If left untreated, there is a fatality rate of 99% within 1–4 months of infection. Since the treatment centre was set up, MSF has cured more than 27,000 Kala-Azar patients with a success rate of approximately 90–95%. There are plans to open an additional Kala-Azar treatment centre in Malakal, Southern Sudan to cope with the overwhelming number of patients that are seeking treatment. MSF has been providing necessary medical supplies to hospitals and training Sudanese health professionals to help them deal with Kala-Azar. MSF, Sudanese Ministry of Health and other national and international institutions are combining efforts to improve on the treatment and diagnosis of Kala-Azar. Research on its cures and vaccines are currently being conducted. In December 2010, South Sudan was hit with the worst outbreak of Kala-Azar in eight years. The number of patients seeking treatment increased eight-fold as compared to the year before.

Sudan's latest civil war began in 1983 and ended in 2005 when a peace agreement was signed between North Sudan and South Sudan. MSF medical teams were active throughout and prior to the civil war, providing emergency medical humanitarian assistance in multiple locations. The situation of poor infrastructure in the South was aggravated by the civil war and resulted in the worsening of the region's appalling health indicators. An estimated 75 percent of people in the nascent nation has no access to basic medical care and 1 in seven women dies during childbirth. Malnutrition and disease outbreaks are perennial concerns as well. In 2011, MSF clinic in Jonglei State, South Sudan was looted and attacked by raiders. Hundreds, including women and children were killed. Valuable items including medical equipment and drugs were lost during the raid and parts of the MSF facilities were destroyed in a fire. The incident had serious repercussions as MSF is the only primary health care provider in this part of Jonglei State.

The early 1990s saw MSF open a number of new national sections, and at the same time, set up field missions in some of the most dangerous and distressing situations it had ever encountered.

In 1990, MSF first entered Liberia to help civilians and refugees affected by the Liberian Civil War. Constant fighting throughout the 1990s and the Second Liberian Civil War have kept MSF volunteers actively providing nutrition, basic health care, and mass vaccinations, and speaking out against attacks on hospitals and feeding stations, especially in Monrovia.

Field missions were set up to provide relief to Kurdish refugees who had survived the al-Anfal Campaign, for which evidence of atrocities was being collected in 1991. 1991 also saw the beginning of the civil war in Somalia, during which MSF set up field missions in 1992 alongside a UN peacekeeping mission. Although the UN-aborted operations by 1993, MSF representatives continued with their relief work, running clinics and hospitals for civilians.

MSF first began work in Srebrenica (in Bosnia and Herzegovina) as part of a UN convoy in 1993, one year after the Bosnian War had begun. The city had become surrounded by the Bosnian Serb Army and, containing about 60,000 Bosniaks, had become an enclave guarded by a United Nations Protection Force. MSF was the only organisation providing medical care to the surrounded civilians, and as such, did not denounce the genocide for fear of being expelled from the country (it did, however, denounce the lack of access for other organisations). MSF was forced to leave the area in 1995 when the Bosnian Serb Army captured the town. 40,000 Bosniak civilian inhabitants were deported, and approximately 7,000 were killed in mass executions.

When the genocide in Rwanda began in April 1994, some delegates of MSF working in the country were incorporated into the International Committee of the Red Cross (ICRC) medical team for protection. Both groups succeeded in keeping all main hospitals in Rwanda's capital Kigali operational throughout the main period of the genocide. MSF, together with several other aid organisations, had to leave the country in 1995, although many MSF and ICRC volunteers worked together under the ICRC's rules of engagement, which held that neutrality was of the utmost importance. These events led to a debate within the organisation about the concept of balancing neutrality of humanitarian aid workers against their witnessing role. As a result of its Rwanda mission, the position of MSF with respect to neutrality moved closer to that of the ICRC, a remarkable development in the light of the origin of the organisation.
The ICRC lost 56 and MSF lost almost one hundred of their respective local staff in Rwanda, and MSF-France, which had chosen to evacuate its team from the country (the local staff were forced to stay), denounced the murders and demanded that a French military intervention stop the genocide. MSF-France introduced the slogan ""One cannot stop a genocide with doctors"" to the media, and the controversial Opération Turquoise followed less than one month later. This intervention directly or indirectly resulted in movements of hundreds of thousands of Rwandan refugees to Zaire and Tanzania in what became known as the Great Lakes refugee crisis, and subsequent cholera epidemics, starvation and more mass killings in the large groups of civilians. MSF-France returned to the area and provided medical aid to refugees in Goma.

At the time of the genocide, competition between the medical efforts of MSF, the ICRC, and other aid groups had reached an all-time high, but the conditions in Rwanda prompted a drastic change in the way humanitarian organisations approached aid missions. The "Code of Conduct for the International Red Cross and Red Crescent Movement and NGOs in Disaster Relief Programmes" was created by the ICRC in 1994 to provide a framework for humanitarian missions and MSF is a signatory of this code. The code advocates the provision of humanitarian aid only, and groups are urged not to serve any political or religious interest, or be used as a tool for foreign governments. MSF has since still found it necessary to condemn the actions of governments, such as in Chechnya in 1999, but has not demanded another military intervention since then.

In the late 1990s, MSF missions were set up to treat tuberculosis and anaemia in residents of the Aral Sea area, and look after civilians affected by drug-resistant disease, famine, and epidemics of cholera and AIDS. They vaccinated 3 million Nigerians against meningitis during an epidemic in 1996 and denounced the Taliban's neglect of health care for women in 1997. Arguably, the most significant country in which MSF set up field missions in the late 1990s was Sierra Leone, which was involved in a civil war at the time. In 1998, volunteers began assisting in surgeries in Freetown to help with an increasing number of amputees, and collecting statistics on civilians (men, women and children) being attacked by large groups of men claiming to represent ECOMOG. The groups of men were travelling between villages and systematically chopping off one or both of each resident's arms, raping women, gunning down families, razing houses, and forcing survivors to leave the area. Long-term projects following the end of the civil war included psychological support and phantom limb pain management.

The Campaign for Access to Essential Medicines was created in late 1999, providing MSF with a new voice with which to bring awareness to the lack of effective treatments and vaccines available in developing countries. In 1999, the organisation also spoke out about the lack of humanitarian support in Kosovo and Chechnya, having set up field missions to help civilians affected by the respective political situations. Although MSF had worked in the Kosovo region since 1993, the onset of the Kosovo War prompted the movement of tens of thousands of refugees, and a decline in suitable living conditions. MSF provided shelter, water and health care to civilians affected by NATO's strategic bombing campaigns.

A serious crisis within MSF erupted in connection with the organisation's work in Kosovo when the Greek section of MSF was expelled from the organization. The Greek MSF section had gained access to Serbia at the cost of accepting Serb government imposed limits on where it could go and what it could see – terms that the rest of the MSF movement had refused. A non-MSF source alleged that the exclusion of the Greek section happened because its members extended aid to both Albanian and Serbian civilians in Pristina during NATO's bombing,
The rift was healed only in 2005 with the re-admission of the Greek section to MSF.

A similar situation was found in Chechnya, whose civilian population was largely forced from their homes into unhealthy conditions and subjected to the violence of the Second Chechen War.

MSF has been working in Haiti since 1991, but since President Jean-Bertrand Aristide was forced from power, the country has seen a large increase in civilian attacks and rape by armed groups. In addition to providing surgical and psychological support in existing hospitals – offering the only free surgery available in Port-au-Prince – field missions have been set up to rebuild water and waste management systems and treat survivors of major flooding caused by Hurricane Jeanne; patients with HIV/AIDS and malaria, both of which are widespread in the country, also receive better treatment and monitoring. As a result of 12 January 2010 Haiti earthquake, reports from Haiti indicated that all three of the organisation's hospitals had been severely damaged; one collapsing completely and the other two having to be abandoned. Following the quake, MSF sent about nine planes loaded with medical equipment and a field hospital to help treat the victims. However, the landings of some of the planes had to be delayed due to the massive number of humanitarian and military flights coming in.

The Kashmir Conflict in northern India resulted in a more recent MSF intervention (the first field mission was set up in 1999) to help civilians displaced by fighting in Jammu and Kashmir, as well as in Manipur. Psychological support is a major target of missions, but teams have also set up programmes to treat tuberculosis, HIV/AIDS and malaria. Mental health support has been of significant importance for MSF in much of southern Asia since the 2004 Indian Ocean earthquake.

MSF went through a long process of self-examination and discussion in 2005–2006. Many issues were debated, including the treatment "nationals" as well as "fair employment" and self-criticism.

MSF has been active in a large number of African countries for decades, sometimes serving as the sole provider of health care, food, and water. Although MSF has consistently attempted to increase media coverage of the situation in Africa to increase international support, long-term field missions are still necessary. Treating and educating the public about HIV/AIDS in sub-Saharan Africa, which sees the most deaths and cases of the disease in the world, is a major task for volunteers. Of the 14.6 million people in need of anti-retroviral treatment the WHO estimated that only 5.25 million people were receiving it in developing countries, and MSF continues to urge governments and companies to increase research and development into HIV/AIDS treatments to decrease cost and increase availability. "(See AIDS in Africa for more information)"

Although active in the Congo region of Africa since 1985, the First and Second Congo War brought increased violence and instability to the area. MSF has had to evacuate its teams from areas such as around Bunia, in the Ituri district due to extreme violence, but continues to work in other areas to provide food to tens of thousands of displaced civilians, as well as treat survivors of mass rapes and widespread fighting. The treatment and possible vaccination against diseases such as cholera, measles, polio, Marburg fever, sleeping sickness, HIV/AIDS, and Bubonic plague is also important to prevent or slow down epidemics.

MSF has been active in Uganda since 1980, and provided relief to civilians during the country's guerrilla war during the . However, the formation of the Lord's Resistance Army saw the beginning of a long campaign of violence in northern Uganda and southern Sudan. Civilians were subjected to mass killings and rapes, torture, and abductions of children, who would later serve as sex slaves or child soldiers. Faced with more than 1.5 million people displaced from their homes, MSF set up relief programmes in internally displaced person (IDP) camps to provide clean water, food and sanitation. Diseases such as tuberculosis, measles, polio, cholera, ebola, and HIV/AIDS occur in epidemics in the country, and volunteers provide vaccinations (in the cases of measles and polio) and/or treatment to the residents. Mental health is also an important aspect of medical treatment for MSF teams in Uganda since most people refuse to leave the IDP camps for constant fear of being attacked.

MSF first camp set up a field mission in Côte d'Ivoire in 1990, but ongoing violence and the 2002 division of the country by rebel groups and the government led to several massacres, and MSF teams have even begun to suspect that an ethnic cleansing is occurring. Mass measles vaccinations, tuberculosis treatment and the re-opening of hospitals closed by fighting are projects run by MSF, which is the only group providing aid in much of the country.

MSF has strongly promoted the use of contraception in Africa.

During the Ebola outbreak in West Africa in 2014, MSF met serious medical demands largely on its own, after the organisation's early warnings were largely ignored.

In 2014 MSF partnered with satellite operator SES, other NGOs Archemed, Fondation Follereau, Friendship Luxembourg and German Doctors, and the Luxembourg government in the pilot phase of SATMED, a project to use satellite broadband technology to bring eHealth and telemedicine to isolated areas of developing countries. SATMED was first deployed in Sierra Leone in support of the fight against Ebola.

MSF-Burundi has aided in attending to casualties suffered in the 2019 Burundi landslides.

MSF first provided medical help to civilians and refugees who have escaped to camps along the Thai-Cambodian border in 1979. Due to long decades of war, a proper health care system in the country was severely lacking and MSF moved inland in 1989 to help restructure basic medical facilities.

In 1999, Cambodia was hit with a malaria epidemic. The situation of the epidemic was aggravated by a lack of qualified practitioners and poor quality control which led to a market of fake antimalarial drugs. Counterfeit antimalarial drugs were responsible for the deaths of at least 30 people during the epidemic. This has prompted efforts by MSF to set up and fund a malaria outreach project and utilise Village Malaria Workers. MSF also introduced a switching of first-line treatment to a combination therapy (Artesunate and Mefloquine) to combat resistance and fatality of old drugs that were used to treat the disease traditionally.

Cambodia is one of the hardest hit HIV/AIDS countries in Southeast Asia. In 2001, MSF started introducing antiretroviral (ARV) therapy to AIDS patients for free. This therapy prolongs the patients' lives and is a long-term treatment. In 2002, MSF established chronic diseases clinics with the Cambodian Ministry of Health in various provinces to integrate HIV/AIDS treatment, alongside hypertension, diabetes, and arthritis which have high prevalence rate. This aims to reduce facility-related stigma as patients are able to seek treatment in a multi-purpose clinic in contrast to a HIV/AIDS specialised treatment centre.

MSF also provided humanitarian aid in times of natural disaster such as a major flood in 2002 which affected up to 1.47 million people. MSF introduced a community-based tuberculosis programme in 2004 in remote villages, where village volunteers are delegated to facilitate the medication of patients. In partnership with local health authorities and other NGOs, MSF encouraged decentralized clinics and rendered localized treatments to more rural areas from 2006. Since 2007, MSF has extended general health care, counselling, HIV/AIDS and TB treatment to prisons in Phnom Penh via mobile clinics. However, poor sanitation and lack of health care still prevails in most Cambodian prisons as they remain as some of the world's most crowded prisons.

In 2007, MSF worked with the Cambodian Ministry of Health to provide psychosocial and technical support in offering pediatric HIV/AIDS treatment to affected children. MSF also provided medical supplies and staff to help in one of the worst dengue outbreaks in 2007, which had more than 40,000 people hospitalized, killing 407 people, primarily children.

In 2010, Southern and Eastern provinces of Cambodia were hit with a cholera epidemic and MSF responded by providing medical support that were adapted for usage in the country.

Cambodia is one of 22 countries listed by WHO as having a high burden of tuberculosis. WHO estimates that 64% of all Cambodians carry the tuberculosis mycobacterium. Hence, MSF has since shifted its focus away from HIV/AIDS to tuberculosis, handing over most HIV-related programs to local health authorities.

The 2011 Libyan civil war has prompted efforts by MSF to set up a hospital and mental health services to help locals affected by the conflict. The fighting created a backlog of patients that needed surgery. With parts of the country slowly returning to livable, MSF has started working with local health personnel to address the needs. The need for psychological counseling has increased and MSF has set up mental health services to address the fears and stress of people living in tents without water and electricity. Currently MSF is the only International Aid organisation with actual presence in the country.

MSF is providing Maritime Search And Rescue (SAR) services on the Mediterranean Sea to save the lives of migrants attempting to cross with unseaworthy boats. The Mission started in 2015 after the EU ended its major SAR operation Mare Nostrum severely diminishing much needed SAR capacities in the Mediterranean. Throughout the mission MSF has operated its own vessels like the Bourbon Argos (2015–2016), Dignity I (2015–2016) and Prudence (2016–2017). MSF has also provided medical teams to support other NGOs and their ships like the MOAS Phoenix (2015) or the Aquarius with SOS Méditerranée (2017–2018). In August 2017 MSF decided to suspend the activities of the Prudence protesting restrictions and threats by the Libyan "Coast Guard".

In December 2018 MSF and SOS Méditerranée were forced to end operations of the Aquarius, the last remaining vessel supported by MSF. This came after attacks by EU states that stripped the vessel of its registration and produced criminal accusations against MSF. Up to then 80,000 people were rescued or assisted since the beginning of the mission.

MSF is involved in Sri Lanka, where a 26 year civil war ended in 2009 and MSF has adapted its activities there to continue its mission. For example, it helps with physical therapy for patients with spinal cord injuries. It conducts counseling sessions, and has set up an “operating theatre for reconstructive orthopaedic surgery and supplied specialist surgeons, anaesthetists and nurses to operate on patients with complicated war-related injuries.”

MSF is involved in trying to help with the humanitarian crisis caused by the Yemeni's Civil War. The organisation operates eleven hospitals and health centres in Yemen and provides support to another 18 hospitals or health centres. According to MSF, since October 2015, four of its hospitals and one ambulance have been destroyed by Saudi-led coalition airstrikes. In August 2016, an airstrike on Abs hospital killed 19 people, including one MSF staff member, and wounded 24. According to MSF, the GPS coordinates of the hospital were repeatedly shared with all parties to the conflict, including the Saudi-led coalition, and its location was well-known.

Before a field mission is established in a country, an MSF team visits the area to determine the nature of the humanitarian emergency, the level of safety in the area and what type of aid is needed (this is called an "exploratory mission").

Medical aid is the main objective of most missions, although some missions help in such areas as water purification and nutrition.

A field mission team usually consists of a small number of coordinators to head each component of a field mission, and a "head of mission." The head of mission usually has the most experience in humanitarian situations of the members of the team, and it is his/her job to deal with the media, national governments and other humanitarian organizations. The head of mission does not necessarily have a medical background.

Medical volunteers include physicians, surgeons, nurses, and various other specialists. In addition to operating the medical and nutrition components of the field mission, these volunteers are sometimes in charge of a group of local medical staff and provide training for them.

Although the medical volunteers almost always receive the most media attention when the world becomes aware of an MSF field mission, there are a number of non-medical volunteers who help keep the field mission functioning. Logisticians are responsible for providing everything that the medical component of a mission needs, ranging from security and vehicle maintenance to food and electricity supplies. They may be engineers and/or foremen, but they usually also help with setting up treatment centres and supervising local staff. Other non-medical staff are water/sanitation specialists, who are usually experienced engineers in the fields of water treatment and management and financial/administration/human resources experts who are placed with field missions.

Vaccination campaigns are a major part of the medical care provided during MSF missions. Diseases such as diphtheria, measles, meningitis, tetanus, pertussis, yellow fever, polio, and cholera, all of which are uncommon in developed countries, may be prevented with vaccination. Some of these diseases, such as cholera and measles, spread rapidly in large populations living in close proximity, such as in a refugee camp, and people must be immunised by the hundreds or thousands in a short period of time. For example, in Beira, Mozambique in 2004, an experimental cholera vaccine was received twice by approximately 50,000 residents in about one month.

An equally important part of the medical care provided during MSF missions is AIDS treatment (with antiretroviral drugs), AIDS testing, and education. MSF is the only source of treatment for many countries in Africa, whose citizens make up the majority of people with HIV and AIDS worldwide. Because antiretroviral drugs (ARVs) are not readily available, MSF usually provides treatment for opportunistic infections and educates the public on how to slow transmission of the disease.

In most countries, MSF increases the capabilities of local hospitals by improving sanitation, providing equipment and drugs, and training local hospital staff. When the local staff is overwhelmed, MSF may open new specialised clinics for treatment of an endemic disease or surgery for victims of war. International staff start these clinics but MSF strives to increase the local staff's ability to run the clinics themselves through training and supervision. In some countries, like Nicaragua, MSF provides public education to increase awareness of reproductive health care and venereal disease.

Since most of the areas that require field missions have been affected by a natural disaster, civil war, or endemic disease, the residents usually require psychological support as well. Although the presence of an MSF medical team may decrease stress somewhat among victims, often a team of psychologists or psychiatrists work with victims of depression, domestic violence and substance abuse. The doctors may also train local mental health staff.

Often in situations where an MSF mission is set up, there is moderate or severe malnutrition as a result of war, drought, or government economic mismanagement. Intentional starvation is also sometimes used during a war as a weapon, and MSF, in addition to providing food, brings awareness to the situation and insists on foreign government intervention. Infectious diseases and diarrhoea, both of which cause weight loss and weakening of a person's body (especially in children), must be treated with medication and proper nutrition to prevent further infections and weight loss. A combination of the above situations, as when a civil war is fought during times of drought and infectious disease outbreaks, can create famine.
In emergency situations where there is a lack of nutritious food, but not to the level of a true famine, protein-energy malnutrition is most common among young children. Marasmus, a form of calorie deficiency, is the most common form of childhood malnutrition and is characterised by severe wasting and often fatal weakening of the immune system. Kwashiorkor, a form of calorie and protein deficiency, is a more serious type of malnutrition in young children, and can negatively affect physical and mental development. Both types of malnutrition can make opportunistic infections fatal. In these situations, MSF sets up "Therapeutic Feeding Centres" for monitoring the children and any other malnourished individuals.

A Therapeutic Feeding Centre (or Therapeutic Feeding Programme) is designed to treat severe malnutrition through the gradual introduction of a special diet intended to promote weight gain after the individual has been treated for other health problems. The treatment programme is split between two phases:

MSF uses foods designed specifically for treatment of severe malnutrition. During phase 1, a type of therapeutic milk called F-75 is fed to patients. F-75 is a relatively low energy, low fat/protein milk powder that must be mixed with water and given to patients to prepare their bodies for phase 2. During phase 2, therapeutic milk called F-100, which is higher in energy/fat/protein content than F-75, is given to patients, usually along with a peanut butter mixture called Plumpy'nut. F-100 and Plumpy'nut are designed to quickly provide large amounts of nutrients so that patients can be treated efficiently. Other special food fed to populations in danger of starvation includes enriched flour and porridge, as well as a high protein biscuit called BP5. BP5 is a popular food for treating populations because it can be distributed easily and sent home with individuals, or it can be crushed and mixed with therapeutic milk for specific treatments.

Dehydration, sometimes due to diarrhoea or cholera, may also be present in a population, and MSF set up rehydration centres to combat this. A special solution called Oral Rehydration Solution (ORS), which contains glucose and electrolytes, is given to patients to replace fluids lost. Antibiotics are also sometimes given to individuals with diarrhoea if it is known that they have cholera or dysentery.

Clean water is essential for hygiene, for consumption and for feeding programmes (for mixing with powdered therapeutic milk or porridge), as well as for preventing the spread of water-borne disease. As such, MSF water engineers and volunteers must create a source of clean water. This is usually achieved by modifying an existing water well, by digging a new well and/or starting a water treatment project to obtain clean water for a population. Water treatment in these situations may consist of storage sedimentation, filtration and/or chlorination depending on available resources.

Sanitation is an essential part of field missions, and it may include education of local medical staff in proper sterilisation techniques, sewage treatment projects, proper waste disposal, and education of the population in personal hygiene. Proper wastewater treatment and water sanitation are the best way to prevent the spread of serious water-borne diseases, such as cholera. Simple wastewater treatment systems can be set up by volunteers to protect drinking water from contamination. Garbage disposal could include pits for normal waste and incineration for medical waste. However, the most important subject in sanitation is the education of the local population, so that proper waste and water treatment can continue once MSF has left the area.

In order to accurately report the conditions of a humanitarian emergency to the rest of the world and to governing bodies, data on a number of factors are collected during each field mission. The rate of malnutrition in children is used to determine the malnutrition rate in the population, and then to determine the need for feeding centres. Various types of mortality rates are used to report the seriousness of a humanitarian emergency, and a common method used to measure mortality in a population is to have staff constantly monitoring the number of burials at cemeteries. By compiling data on the frequency of diseases in hospitals, MSF can track the occurrence and location of epidemic increases (or "seasons") and stockpile vaccines and other drugs. For example, the "Meningitis Belt" (sub-Saharan Africa, which sees the most cases of meningitis in the world) has been "mapped" and the meningitis season occurs between December and June. Shifts in the location of the Belt and the timing of the season can be predicted using cumulative data over many years.

In addition to epidemiological surveys, MSF also uses population surveys to determine the rates of violence in various regions. By estimating the scopes of massacres, and determining the rate of kidnappings, rapes, and killings, psychosocial programmes can be implemented to lower the suicide rate and increase the sense of security in a population. Large-scale forced migrations, excessive civilian casualties and massacres can be quantified using surveys, and MSF can use the results to put pressure on governments to provide help, or even expose genocide. MSF conducted the first comprehensive mortality survey in Darfur in 2004.
However, there may be ethical problems in collecting these statistics.

The Campaign for Access to Essential Medicines was initiated in 1999 to increase access to essential medicines in developing countries. "Essential medicines" are those drugs that are needed in sufficient supply to treat a disease common to a population. However, most diseases common to populations in developing countries are no longer common to populations in developed countries; therefore, pharmaceutical companies find that producing these drugs is no longer profitable and may raise the price per treatment, decrease development of the drug (and new treatments) or even stop production of the drug. MSF often lacks effective drugs during field missions, and started the campaign to put pressure on governments and pharmaceutical companies to increase funding for essential medicines.

In recent years, the organization has tried to use its influence to urge the drug maker Novartis to drop its case against India's patent law that prevents Novartis from patenting its drugs in India. A few years earlier, Novartis also sued South Africa to prevent it from importing cheaper AIDS drugs. Dr. Tido von Schoen-Angerer, director of DWB's Campaign for Access to Essential Medicines, says, "Just like five years ago, Novartis, with its legal actions, is trying to stand in the way of people's right to access the medicines they need."

On 1 April 2013, it was announced that the Indian court invalidated Novartis's patent on Gleevec. This decision makes the drug available via generics on the Indian market at a considerably lower price.

Aside from injuries and death associated with stray bullets, mines and epidemic disease, MSF volunteers are sometimes attacked or kidnapped for political reasons. In some countries afflicted by civil war, humanitarian-aid organizations are viewed as helping the enemy. If an aid mission is perceived to be exclusively set up for victims on one side of the conflict, it may come under attack for that reason. However, the War on Terrorism has generated attitudes among some groups in US-occupied countries that non-governmental aid organizations such as MSF are allied with or even work for the Coalition forces. Since the United States has labelled its operations "humanitarian actions," independent aid organizations have been forced to defend their positions, or even evacuate their teams. Insecurity in cities in Afghanistan and Iraq rose significantly following United States operations, and MSF has declared that providing aid in these countries was too dangerous. The organization was forced to evacuate its teams from Afghanistan on 28 July 2004, after five volunteers (Afghans Fasil Ahmad and Besmillah, Belgian Hélène de Beir, Norwegian Egil Tynæs, and Dutchman Willem Kwint) were killed on 2 June in an ambush by unidentified militia near Khair Khāna in Badghis Province. In June 2007, Elsa Serfass, a volunteer with MSF-France, was killed in the Central African Republic and in January 2008, two expatriate staff (Damien Lehalle and Victor Okumu) and a national staff member (Mohammed Bidhaan Ali) were killed in an organized attack in Somalia resulting in the closing of the project.

Arrests and abductions in politically unstable regions can also occur for volunteers, and in some cases, MSF field missions can be expelled entirely from a country. Arjan Erkel, Head of Mission in Dagestan in the North Caucasus, was kidnapped and held hostage in an unknown location by unknown abductors from 12 August 2002 until 11 April 2004. Paul Foreman, head of MSF-Holland, was arrested in Sudan in May 2005 for refusing to divulge documents used in compiling a report on rapes carried out by the pro-government Janjaweed militias (see Darfur conflict). Foreman cited the privacy of the women involved, and MSF alleged that the Sudanese government had arrested him because it disliked the bad publicity generated by the report.

On 14 August 2013, MSF announced that it was closing all of its programmes in Somalia due to attacks on its staff by Al-Shabaab militants and perceived indifference or inurement to this by the governmental authorities and wider society.

On 3 October 2015, 14 staff and 28 others died when an MSF hospital was bombed by American forces during the Battle of Kunduz.

On 27 October 2015, an MSF hospital in Sa'dah, Yemen was bombed by the Saudi Arabia-led military coalition.

On 28 November 2015, an MSF-supported hospital was barrel-bombed by a Syrian Air Force helicopter, killing seven and wounding forty-seven people near Homs, Syria.

On 10 January 2016, an MSF-supported hospital in Sa'dah was bombed by the Saudi Arabia-led military coalition, killing six people.

On 15 February 2016, two MSF-supported hospitals in Idlib District and Aleppo, Syria were bombed, killing at least 20 and injuring dozens of patients and medical personnel. Both Russia and the United States denied responsibility and being in the area at the time.

On 28 April 2016, an MSF hospital in Aleppo was bombed, killing 50, including six staff and patients.

On 12 May 2020, an MSF-supported hospital in Dasht-e-Barchi, Kabul, Afghanistan was attacked by an unknown assailant. They attack left 24 people dead and at least 20 more injured.

"" is an award-winning documentary film by Mark N. Hopkins that tells the story of four MSF volunteer doctors confronting the challenges of medical work in war-torn areas of Liberia and Congo. It premiered at the 2008 Venice Film Festival and was theatrically released in the United States in 2010.

The then president of MSF, James Orbinski, gave the Nobel Peace Prize speech on behalf of the organization. In the opening, he discusses the conditions of the victims of the Rwandan genocide and focuses on one of his woman patients:

Orbinski affirmed the organization's commitment to publicizing the issues MSF encountered, stating

On 7 October 2015, President Barack Obama, also Nobel Peace Prize winner and at that time commander in chief, issued an apology to Doctors Without Borders for Kunduz hospital airstrike. Doctors Without Borders were not mollified by Obama's apology.

MSF received the Lasker-Bloomberg Public Service Award in 2015 from the New York based Lasker Foundation.

A number of other non-governmental organizations have adopted names ending in "Sans Frontières" or "Without Borders", inspired by Médecins Sans Frontières: for example, Engineers Without Borders, Payasos Sin Fronteras (Clowns Without Borders) and Reporters Without Borders.
During the 2019–20 Hong Kong protests, there were conflicts between the civilians and the police. Some injured protesters feared arrest if they sought medical assistance from government hospitals, and some sought assistance from MSF. 

In October conflicts erupted at multiple universities in Hong Kong, with police using tear gas. MSF stated that they considered the state medical assistance to be adequate, but were heavily criticised by supporters of the protests, including artists Gregory Wong and Gloria Yip who stated that they would no longer donate to MSF.





</doc>
<doc id="20501" url="https://en.wikipedia.org/wiki?curid=20501" title="Moose">
Moose

The moose (North America) or elk (Eurasia), "Alces alces", is a member of the New World deer subfamily and is the largest and heaviest extant species in the deer family. Most adult male moose have distinctive broad, palmate ("open-hand shaped") antlers; most other members of the deer family have antlers with a dendritic ("twig-like") configuration. Moose typically inhabit boreal forests and temperate broadleaf and mixed forests of the Northern Hemisphere in temperate to subarctic climates. Hunting and other human activities have caused a reduction in the size of the moose's range over time. It has been reintroduced to some of its former habitats. Currently, most moose occur in Canada, Alaska, New England (with Maine having the most of the lower 48 states), Fennoscandia, the Baltic states, and Russia. Its diet consists of both terrestrial and aquatic vegetation. The most common moose predators are the gray wolf along with bears and humans. Unlike most other deer species, moose do not form herds and are solitary animals, aside from calves who remain with their mother until the cow begins estrus (typically at 18 months after birth of the calf), at which point the cow chases away young bulls. Although generally slow-moving and sedentary, moose can become aggressive and move quickly if angered or startled. Their mating season in the autumn features energetic fights between males competing for a female.

"Alces alces" is called a "moose" in North American English, but an "elk" in British English; its scientific name comes from its name in Latin. The word "elk" in North American English refers to a completely different species of deer, "Cervus canadensis", also called the wapiti. A mature male moose is called a bull, a mature female a cow, and an immature moose of either sex a calf.

The word "elk" originated in Proto-Germanic, from which Old English evolved and has cognates in other Indo-European languages, e.g. "elg" in Danish/Norwegian; "älg" in Swedish; "alnis" in Latvian; "Elch" in German; and "łoś" in Polish (Latin "alcē" or "alcēs" and Ancient Greek "álkē" are probably Germanic loanwords). In the continental-European languages, these forms of the word "elk" almost always refer to "Alces alces".

The word "moose" had first entered English by 1606 and is borrowed from the Algonquian languages (compare the Narragansett "moos" and Eastern Abenaki "mos"; according to early sources, these were likely derived from "moosu", meaning "he strips off"), and possibly involved forms from multiple languages mutually reinforcing one another. The Proto-Algonquian form was "*mo·swa".

The moose became extinct in Britain during the Bronze Age, long before the European arrival in the Americas. The youngest bones were found in Scotland and are roughly 3,900 years old. The word "elk" remained in usage because of its existence in continental Europe; however, without any living animals around to serve as a reference, the meaning became rather vague to most speakers of English, who used "elk" to refer to "large deer" in general. Dictionaries of the 18th century simply described "elk" as a deer that was "as large as a horse".

Confusingly, the word "elk" is used in North America to refer to a different animal, "Cervus canadensis", which is also called by the Algonquian indigenous name, "wapiti". The British began colonizing America in the 17th century, and found two common species of deer for which they had no names. The wapiti appeared very similar to the red deer of Europe (which itself was almost extinct in Southern Britain) although it was much larger and was not red. The moose was a rather strange-looking deer to the colonists, and they often adopted local names for both. In the early days of American colonization, the wapiti was often called a gray moose and the moose was often called a black moose, but early accounts of the animals varied wildly, adding to the confusion.

The wapiti is superficially very similar to the red deer of central and western Europe, although it is distinctly different behaviorally and genetically. Early European explorers in North America, particularly in Virginia where there were no moose, called the wapiti "elk" because of its size and resemblance to familiar-looking deer like the red deer. The moose resembled the "German elk" (the moose of continental Europe), which was less familiar to the British colonists. For a long time neither species had an official name, but were called a variety of things. Eventually, in North America the wapiti became known as an elk while the moose retained its Anglicized Native-American name. In 1736, Samuel Dale wrote to the Royal Society of Great Britain:
The common light-grey moose, called by the Indians, Wampoose, and the large or black-moose, which is the beast whose horns I herewith present. As to the grey moose, I take it to be no larger than what Mr. John Clayton, in his account of the Virginia Quadrupeds, calls the Elke ... was in all respects like those of our red-deer or stags, only larger ... The black moose is (by all that have hitherto writ of it) accounted a very large creature. ... The stag, buck, or male of this kind has a palmed horn, not like that of our common or fallow-deer, but the palm is much longer, and more like that of the "German elke."

Moose require habitat with adequate edible plants (e.g., pond grasses, young trees and shrubs), cover from predators, and protection from extremely hot or cold weather. Moose travel among different habitats with the seasons to address these requirements. Moose are cold-adapted mammals with thickened skin, dense, heat-retaining coat, and a low surface:volume ratio, which provides excellent cold tolerance but poor heat tolerance. Moose survive hot weather by accessing shade or cooling wind, or by immersion in cool water. In hot weather, moose are often found wading or swimming in lakes or ponds. When heat-stressed, moose may fail to adequately forage in summer and may not gain adequate body fat to survive the winter. Also, moose cows may not calve without adequate summer weight gain. Moose require access to both young forest for browsing and mature forest for shelter and cover. Forest disturbed by fire and logging promotes the growth of fodder for moose. Moose also require access to mineral licks, safe places for calving and aquatic feeding sites.

Moose avoid areas with little or no snow as this increases the risk of predation by wolves and avoid areas with deep snow, as this impairs mobility. Thus, moose select habitat on the basis of trade-offs between risk of predation, food availability, and snow depth. With reintroduction of bison into boreal forest, there was some concern that bison would compete with moose for winter habitat, and thereby worsen the population decline of moose. However, this does not appear to be a problem. Moose prefer sub-alpine shrublands in early winter, while bison prefer wet sedge valley meadowlands in early-winter. In late-winter, moose prefer river valleys with deciduous forest cover or alpine terrain above the tree line, while bison preferred wet sedge meadowlands or sunny southern grassy slopes.

After expanding for most of the 20th century, the moose population of North America has been in steep decline since the 1990s. Populations expanded greatly with improved habitat and protection, but now the moose population is declining rapidly. This decline has been attributed to opening of roads and landscapes into the northern range of moose, allowing deer to become populous in areas where they were not previously common. This encroachment by deer on moose habitat brought moose into contact with previously unfamiliar pathogens, including brainworm and liver fluke, and these parasites are believed to have contributed to the population decline of moose.
In North America, the moose range includes almost all of Canada (excluding the arctic and Vancouver Island), most of Alaska, northern New England and upstate New York, the upper Rocky Mountains, northern Minnesota, northern Wisconsin,
Michigan's Upper Peninsula, and Isle Royale in Lake Superior. This massive range, containing diverse habitats, contains four of the six North American subspecies. In the West, moose populations extend well north into Canada (British Columbia and Alberta), and more isolated groups have been verified as far south as the mountains of Utah and Colorado and as far west as the Lake Wenatchee area of the Washington Cascades. The range includes Wyoming, Montana, Idaho, and smaller areas of Washington and Oregon. Moose have extended their range southwards in the western Rocky Mountains, with initial sightings in Yellowstone National Park in 1868, and then to the northern slope of the Uinta Mountains in Utah in the first half of the twentieth century. This is the southernmost naturally established moose population in the United States. In 1978, a few breeding pairs were reintroduced in western Colorado, and the state's moose population is now more than 1,000.

In northeastern North America, the Eastern moose's history is very well documented: moose meat was often a staple in the diet of Native Americans going back centuries, with a tribe that occupied present day coastal Rhode Island giving the animal its distinctive name, adopted into American English. The Native Americans often used moose hides for leather and its meat as an ingredient in pemmican, a type of dried jerky used as a source of sustenance in winter or on long journeys. Eastern tribes also valued moose leather as a source for moccasins and other items.

The historical range of the subspecies extended from well into Quebec, the Maritimes, and Eastern Ontario south to include all of New England finally ending in the very northeastern tip of Pennsylvania in the west, cutting off somewhere near the mouth of the Hudson River in the south . The moose has been extinct in much of the eastern U.S. for as long as 150 years, due to colonial era overhunting and destruction of its habitat: Dutch, French, and British colonial sources all attest to its presence in the mid 17th century from Maine south to areas within a hundred miles of present-day Manhattan. However, by the 1870s, only a handful of moose existed in this entire region in very remote pockets of forest; less than 20% of suitable habitat remained.

Since the 1980s, however, moose populations have rebounded, thanks to regrowth of plentiful food sources, abandonment of farmland, better land management, clean-up of pollution, and natural dispersal from the Canadian Maritimes and Quebec. South of the Canada–US border, Maine has most of the population with a 2012 headcount of about 76,000 moose. Dispersals from Maine over the years have resulted in healthy, growing populations each in Vermont and New Hampshire, notably near bodies of water and as high up as above sea level in the mountains. In Massachusetts, moose had gone extinct by 1870, but re-colonized the state in the 1960s, with the population expanding from Vermont and New Hampshire; by 2010, the population was estimated at 850–950. Moose reestablished populations in eastern New York and Connecticut and appeared headed south towards the Catskill Mountains, a former habitat.

In the Midwest U.S., moose are primarily limited to the upper Great Lakes region, but strays, primarily immature males, have been found as far south as eastern Iowa. For unknown reasons, the moose population is declining rapidly in the Midwest.

Moose were successfully introduced on Newfoundland in 1878 and 1904, where they are now the dominant ungulate, and somewhat less successfully on Anticosti Island in the Gulf of Saint Lawrence.

Since the 1990s, moose populations have declined dramatically in much of temperate North America, although they remain stable in arctic and subarctic regions. The exact causes of specific die-offs are not determined, but most documented mortality events were due to wolf predation, bacterial infection due to injuries sustained from predators, and parasites from whitetail deer to which moose have not developed a natural defense, such as liver flukes, brain worms and winter tick infestations. Predation of moose calves by brown bear is also significant. One of the leading hypotheses among biologists for generalized, nonhunting declines in moose populations at the southern extent of their range is increasing heat stress brought on by the rapid seasonal temperature upswings as a result of human-induced climate change. Biologists studying moose populations typically use warm-season, heat-stress thresholds of between . However, the minor average temperature increase of 0.83–1.11 °C (1.5–2 °F), over the last 100 years, has resulted in milder winters that induce favorable conditions for ticks, parasites and other invasive species to flourish within the southern range of moose habitat in North America. This leading hypothesis is supported by mathematical models that explore moose-population responses to future climate-change projections.

The moose population in New Hampshire fell from 7,500 in the early 2000s to a current estimate of 4,000 and in Vermont the numbers were down to 2,200 from a high of 5,000 animals in 2005. Much of the decline has been attributed to the winter tick with about 70% of the moose calf deaths across Maine and New Hampshire due to the parasite. Moose with heavy tick infections will rub their fur down to the skin raw trying to get the ticks off, making them look white when their outer coat rubs off. Locals call them ghost moose. Loss of the insulating winter coat through attempts to rid the moose of winter tick increases the risk of hypothermia in winter.

In Europe, moose are currently found in large numbers throughout Norway, Sweden, Finland, Latvia, Estonia, Poland, with more modest numbers in the southern Czech Republic, Belarus and northern Ukraine. They are also widespread through Russia on up through the borders with Finland south towards the border with Estonia, Belarus and Ukraine and stretching far away eastwards to the Yenisei River in Siberia. The European moose was native to most temperate areas with suitable habitat on the continent and even Scotland from the end of the last Ice Age, as Europe had a mix of temperate boreal and deciduous forest. Up through Classical times, the species was certainly thriving in both Gaul and Magna Germania, as it appears in military and hunting accounts of the age. However, as the Roman era faded into medieval times, the beast slowly disappeared: soon after the reign of Charlemagne, the moose disappeared from France, where its range extended from Normandy in the north to the Pyrenees in the south. Farther east, it survived in Alsace and the Netherlands until the 9th century as the marshlands in the latter were drained and the forests were cleared away for feudal lands in the former. It was gone from Switzerland by the year 1000, from the western Czech Republic by 1300, from Mecklenburg in Germany by c. 1600, and from Hungary and the Caucasus since the 18th and 19th century, respectively.

By the early 20th century, the very last strongholds of the European moose appeared to be in Fennoscandian areas and patchy tracts of Russia, with a few migrants found in what is now Estonia and Lithuania. The USSR and Poland managed to restore portions of the range within its borders (such as the 1951 reintroduction into Kampinos National Park and the later 1958 reintroduction in Belarus), but political complications limited the ability to reintroduce it to other portions of its range. Attempts in 1930 and again in 1967 in marshland north of Berlin were unsuccessful. At present in Poland, populations are recorded in the Biebrza river valley, Kampinos, and in Białowieża Forest. It has migrated into other parts of Eastern Europe and has been spotted in eastern and southern Germany. Unsuccessful thus far in recolonizing these areas via natural dispersal from source populations in Poland, Belarus, Ukraine, Czech Republic and Slovakia, it appears to be having more success migrating south into the Caucasus. It is listed under Appendix III of the Bern Convention.

In 2008, two moose were reintroduced into the Scottish Highlands in Alladale Wilderness Reserve.

The East Asian moose populations confine themselves mostly to the territory of Russia, with much smaller populations in Mongolia and Northeastern China. Moose populations are relatively stable in Siberia and increasing on the Kamchatka Peninsula. In Mongolia and China, where poaching took a great toll on moose, forcing them to near extinction, they are protected, but enforcement of the policy is weak and demand for traditional medicines derived from deer parts is high. In 1978, the Regional Hunting Department transported 45 young moose to the center of Kamchatka. These moose were brought from Chukotka, home to the largest moose on the planet. Kamchatka now regularly is responsible for the largest trophy moose shot around the world each season. As it is a fertile environment for moose, with a milder climate, less snow, and an abundance of food, moose quickly bred and settled along the valley of the Kamchatka River and many surrounding regions. The population in the past 20 years has risen to over 2,900 animals.

The size of the moose varies. Following Bergmann's rule, population in the south ("A. a. cameloides") usually grow smaller, while moose in the north and northeast ("A. a. buturlini") can match the imposing sizes of the Alaskan moose ("A. a. gigas") and are prized by trophy hunters.

In 1900, an attempt to introduce moose into the Hokitika area failed; then in 1910 ten moose (four bulls and six cows) were introduced into Fiordland. This area is considered a less than suitable habitat, and subsequent low numbers of sightings and kills have led to some presumption of this population's failure. The last proven sighting of a moose in New Zealand was in 1952. However, a moose antler was found in 1972, and DNA tests showed that hair collected in 2002 was from a moose. There has been extensive searching, and while automated cameras failed to capture photographs, evidence was seen of bedding spots, browsing, and antler marks.

North America:

Europe and Asia:

Bull moose have antlers like other members of the deer family. Cows select mates based on antler size. Bull moose use dominant displays of antlers to discourage competition and will spar or fight rivals. The size and growth rate of antlers is determined by diet and age; symmetry reflects health.

The male's antlers grow as cylindrical beams projecting on each side of the head at right angles to the midline of the skull, and then fork. The lower prong of this fork may be either simple, or divided into two or three tines, with some flattening. Most moose have antlers that are broad and palmate (flat) with tines (points) along the outer edge.. Within the ecologic range of the moose in Europe, those in northerly locales display the palmate pattern of antlers, while the antlers of European moose over the southerly portion of its range are typically of the cervina dendritic pattern and comparatively small, perhaps due to evolutionary pressures of hunting by humans, who prize the large palmate antlers. European moose with antlers intermediate between the palmate and the dendritic form are found in the middle of the north-south range. Moose with antlers have more acute hearing than those without antlers; a study of trophy antlers using a microphone found that the palmate antler acts as a parabolic reflector, amplifying sound at the moose's ear.

The antlers of mature Alaskan adult bull moose (5 to 12 years old) have a normal maximum spread greater than . By the age of 13, moose antlers decline in size and symmetry. The widest spread recorded was across. (An Alaskan moose also holds the record for the heaviest weight at .)

Antler beam diameter, not the number of tines, indicates age. In North America, moose ("A. a. americanus") antlers are usually larger than those of Eurasian moose and have two lobes on each side, like a butterfly. Eurasian moose antlers resemble a seashell, with a single lobe on each side. In the North Siberian moose ("A. a. bedfordiae"), the posterior division of the main fork divides into three tines, with no distinct flattening. In the common moose ("A. a. alces") this branch usually expands into a broad palmation, with one large tine at the base and a number of smaller snags on the free border. There is, however, a Scandinavian breed of the common moose in which the antlers are simpler and recall those of the East Siberian animals. The palmation appears to be more marked in North American moose than in the typical Scandinavian moose.
After the mating season males drop their antlers to conserve energy for the winter. A new set of antlers will then regrow in the spring. Antlers take three to five months to fully develop, making them one of the fastest growing animal organs. Antler growth is "nourished by an extensive system of blood vessels in the skin covering, which contains numerous hair follicles that give it a 'velvet' texture." This requires intense grazing on a highly-nutritious diet. By September the velvet is removed by rubbing and thrashing which changes the colour of the antlers. Immature bulls may not shed their antlers for the winter, but retain them until the following spring. Birds, carnivores and rodents eat dropped antlers as they are full of protein and moose themselves will eat antler velvet for the nutrients.

If a bull moose is castrated, either by accidental or chemical means, he will quickly shed his current set of antlers and then immediately begin to grow a new set of misshapen and deformed antlers that he will wear the rest of his life without ever shedding again. The distinctive-looking appendages (often referred to as "devil's antlers") are the source of several myths and legends among many groups of Inuit as well as several other tribes of indigenous peoples of North America.

In extremely rare circumstances, a cow moose may grow antlers. This is usually attributed to a hormone imbalance.

The moose proboscis is distinctive among the living cervids due to its large size; it also features nares that can be sealed shut when the moose is browsing aquatic vegetation. The moose proboscis likely evolved as an adaptation to aquatic browsing, with loss of the rhinarium, and development of a superior olfactory column separate from an inferior respiratory column. This separation contributes to the moose's keen sense of smell, which they employ to detect water sources, to find food under snow, and to detect mates or predators.

As with all members of the order Artiodactyla (even-toed ungulates), moose feet have two large keratinized hooves corresponding to the third and fourth toe, with two small posterolateral dewclaws (vestigial digits), corresponding to the second and fifth toe. The hoof of the fourth digit is broader than that of the third digit, while the inner hoof of the third digit is longer than that of the fourth digit. This foot configuration may favor striding on soft ground. The moose hoof splays under load, increasing surface area, which limits sinking of the moose foot into soft ground or snow, and which increases efficiency when swimming. The body weight per footprint surface area of the moose foot is intermediate between that of the pronghorn foot, (which have stiff feet lacking dewclaws—optimized for high-speed running) and the caribou foot (which are more rounded with large dewclaws, optimized for walking in deep snow). The moose's body weight per surface area of footprint is about twice that of the caribou's.

On firm ground, a bull moose leaves a visible impression of the dewclaws in its footprint, while a cow moose or calf does not leave a dewclaw impression. On soft ground or mud, bull, cow, and calf footprints may all show dewclaw impressions.

Their fur consist of two layers; top layer of long guard hairs and a soft wooly undercoat. The guard hairs are hollow and filled with air for better insulation, which also helps them stay afloat when swimming.

Both male and female moose have a dewlap or bell, which is a fold of skin under the chin. Its exact use is unknown, but theories state that it might be used in mating, as a visual and olfactory signal, or as a dominance signal by males, as are the antlers.

The tail is short (6 cm to 8 cm in length) and vestigial in appearance; unlike other ungulates the moose tail is too short to swish away insects.

On average, an adult moose stands high at the shoulder, which is more than a foot higher than the next largest deer on average, the wapiti. Males (or "bulls") normally weigh from and females (or "cows") typically weigh , depending on racial or clinal as well as individual age or nutritional variations. The head-and-body length is , with the vestigial tail adding only a further . The largest of all the races is the Alaskan subspecies ("A. a. gigas"), which can stand over at the shoulder, has a span across the antlers of and averages in males and in females.<ref name="Nancy Long / Kurt Savikko"></ref> Typically, however, the antlers of a mature bull are between 1.2 m (3.9 ft) and 1.5 m (4.9 ft). The largest confirmed size for this species was a bull shot at the Yukon River in September 1897 that weighed and measured high at the shoulder. There have been reported cases of even larger moose, including a bull killed in 2004 that weighed , and a bull that reportedly scaled , but none are authenticated and some may not be considered reliable. Behind only the two species of bison, the moose is the second largest of extant terrestrial wildlife after the bisons in North America, Siberia, and Europe.

The moose is a herbivore and is capable of consuming many types of plant or fruit. The average adult moose needs to consume per day to maintain its body weight. Much of a moose's energy is derived from terrestrial vegetation, mainly consisting of forbs and other non-grasses, and fresh shoots from trees such as willow and birch. These plants are rather low in sodium, and moose generally need to consume a good quantity of aquatic plants. While much lower in energy, aquatic plants provide the moose with its sodium requirements, and as much as half of their diet usually consists of aquatic plant life. In winter, moose are often drawn to roadways, to lick salt that is used as a snow and ice melter. A typical moose, weighing , can eat up to of food per day.

Moose lack upper front teeth, but have eight sharp incisors on the lower jaw. They also have a tough tongue, lips and gums, which aid in the eating of woody vegetation. Moose have six pairs of large, flat molars and, ahead of those, six pairs of premolars, to grind up their food. A moose's upper lip is very sensitive, to help distinguish between fresh shoots and harder twigs, and is prehensile, for grasping their food. In the summer, moose may use this prehensile lip for grabbing branches and pulling, stripping the entire branch of leaves in a single mouthful, or for pulling forbs, like dandelions, or aquatic plants up by the base, roots and all. A moose's diet often depends on its location, but they seem to prefer the new growths from deciduous trees with a high sugar content, such as white birch, trembling aspen and striped maple, among many others. To reach high branches, a moose may bend small saplings down, using its prehensile lip, mouth or body. For larger trees a moose may stand erect and walk upright on its hind legs, allowing it to reach branches up to or higher above the ground.

Moose also eat many aquatic plants, including lilies and pondweed. Moose are excellent swimmers and are known to wade into water to eat aquatic plants. This trait serves a second purpose in cooling down the moose on summer days and ridding itself of black flies. Moose are thus attracted to marshes and river banks during warmer months as both provide suitable vegetation to eat and water to wet themselves in. Moose have been known to dive over to reach plants on lake bottoms, and the complex snout may assist the moose in this type of feeding. Moose are the only deer that are capable of feeding underwater. As an adaptation for feeding on plants underwater, the nose is equipped with fatty pads and muscles that close the nostrils when exposed to water pressure, preventing water from entering the nose. Other species can pluck plants from the water too, but these need to raise their heads in order to swallow.
Moose are not grazing animals but browsers (concentrate selectors). Like giraffes, moose carefully select foods with less fiber and more concentrations of nutrients. Thus, the moose's digestive system has evolved to accommodate this relatively low-fiber diet. Unlike most hooved, domesticated animals (ruminants), moose cannot digest hay, and feeding it to a moose can be fatal. The moose's varied and complex diet is typically expensive for humans to provide, and free-range moose require a lot of forested acreage for sustainable survival, which is one of the main reasons moose have never been widely domesticated.

A full-grown moose has few enemies except Siberian tigers ("Panthera tigris altaica") which regularly prey on adult moose, but a pack of gray wolves ("Canis lupus") can still pose a threat, especially to females with calves. Brown bears ("Ursus arctos") are also known to prey on moose of various sizes and are the only predator besides the wolf to attack moose both in Eurasia and North America. However, brown bears are more likely to take over a wolf kill or to take young moose than to hunt adult moose on their own. American black bears ("Ursus americanus") and cougars ("Puma concolor") can be significant predators of moose calves in May and June and can, in rare instances, prey on adults (mainly cows rather than the larger bulls). Wolverine ("Gulo gulo") are most likely to eat moose as carrion but have killed moose, including adults, when the large ungulates are weakened by harsh winter conditions. Killer whales ("Orcinus orca") are the moose's only known marine predator as they have been known to prey on moose swimming between islands out of North America's Northwest Coast, however, there is at least one recorded instance of a moose preyed upon by a Greenland shark.

In some areas, moose are the primary source of food for wolves. Moose usually flee upon detecting wolves. Wolves usually follow moose at a distance of , occasionally at a distance of . Attacks from wolves against young moose may last seconds, though sometimes they can be drawn out for days with adults. Sometimes, wolves will chase moose into shallow streams or onto frozen rivers, where their mobility is greatly impeded. Moose will sometimes stand their ground and defend themselves by charging at the wolves or lashing out at them with their powerful hooves. Wolves typically kill moose by tearing at their haunches and perineum, causing massive blood loss. Occasionally, a wolf may immobilise a moose by biting its sensitive nose, the pain of which can paralyze a moose. Wolf packs primarily target calves and elderly animals, but can and will take healthy, adult moose. Moose between the ages of two and eight are seldom killed by wolves. Though moose are usually hunted by packs, there are cases in which single wolves have successfully killed healthy, fully-grown moose.

Research into moose predation suggests that their response to perceived threats is learned rather than instinctual. In practical terms this means moose are more vulnerable in areas where wolf or bear populations were decimated in the past but are now rebounding. These same studies suggest, however, that moose learn quickly and adapt, fleeing an area if they hear or smell wolves, bears, or scavenger birds such as ravens.

Moose are also subject to various diseases and forms of parasitism. In northern Europe, the moose botfly is a parasite whose range seems to be spreading.

Moose are mostly diurnal. They are generally solitary with the strongest bonds between mother and calf. Although moose rarely gather in groups, there may be several in close proximity during the mating season.

Rutting and mating occurs in September and October. During the rut, mature bulls will cease feeding completely for a period of approximately two weeks; this fasting behavior has been attributed to neurophysiological changes related to redeployment of olfaction for detection of moose urine and moose cows. The males are polygamous and will seek several females to breed with. During this time both sexes will call to each other. Males produce heavy grunting sounds that can be heard from up to 500 meters away, while females produce wail-like sounds. Males will fight for access to females. Initially, the males assess which of them is dominant and one bull may retreat, however, the interaction can escalate to a fight using their antlers.

Female moose have an eight-month gestation period, usually bearing one calf, or twins if food is plentiful, in May or June. Twinning can run as high as 30% to 40% with good nutrition Newborn moose have fur with a reddish hue in contrast to the brown appearance of an adult. The young will stay with the mother until just before the next young are born. The life span of an average moose is about 15–25 years. Moose populations are stable at 25 calves for every 100 cows at 1 year of age. With availability of adequate nutrition, mild weather, and low predation, moose have a huge potential for population expansion.

Moose are not usually aggressive towards humans, but can be provoked or frightened to behave with aggression. In terms of raw numbers, they attack more people than bears and wolves combined, but usually with only minor consequences. In the Americas, moose injure more people than any other wild mammal, and worldwide, only hippopotamuses injure more. When harassed or startled by people or in the presence of a dog, moose may charge. Also, as with bears or any wild animal, moose that have become used to being fed by people may act aggressively when denied food. During the fall mating season, bulls may be aggressive toward humans because of the high hormone levels they experience. Cows with young calves are very protective and will attack humans who come too close, especially if they come between mother and calf. Unlike other dangerous animals, moose are not territorial, and do not view humans as food, and will therefore usually not pursue humans if they simply run away.
Like any wild animal, moose are unpredictable. They are most likely to attack if annoyed or harassed, or if approached too closely. A moose that has been harassed may vent its anger on anyone in the vicinity, and they often do not make distinctions between their tormentors and innocent passers-by. Moose are very limber animals with highly flexible joints and sharp, pointed hooves, and are capable of kicking with both front and back legs. Unlike other large, hooved mammals, such as horses, moose can kick in all directions including sideways. Therefore, there is no safe side from which to approach. However, moose often give warning signs prior to attacking, displaying their aggression by means of body language. Maintained eye contact is usually the first sign of aggression, while laid-back ears or a lowered head is a definite sign of agitation. If the hairs on the back of the moose's neck and shoulders (hackles) stand up, a charge is usually imminent. The Anchorage Visitor Centers warn tourists that "...a moose with its hackles raised is a thing to fear."

Studies suggest that the calls made by female moose during the rut not only call the males but can actually induce a bull to invade another bull's harem and fight for control of it. This in turn means that the cow moose has at least a small degree of control over which bulls she mates with.

Moose often show aggression to other animals as well; especially predators. Bears are common predators of moose calves and, rarely, adults. Alaskan moose have been reported to successfully fend off attacks from both black and brown bears. Moose have been known to stomp attacking wolves, which makes them less preferred as prey to the wolves. Moose are fully capable of killing bears and wolves. A moose of either sex that is confronted by danger may let out a loud roar, more resembling that of a predator than a prey animal. European moose are often more aggressive than North American moose, such as the moose in Sweden, which often become very agitated at the sight of a predator. However, like all ungulates known to attack predators, the more aggressive individuals are always darker in color.

European rock drawings and cave paintings reveal that moose have been hunted since the Stone Age. Excavations in Alby, Sweden, adjacent to the Stora Alvaret have yielded moose antlers in wooden hut remains from 6000 BCE, indicating some of the earliest moose hunting in northern Europe. In northern Scandinavia one can still find remains of trapping pits used for hunting moose. These pits, which can be up to in area and deep, would have been camouflaged with branches and leaves. They would have had steep sides lined with planks, making it impossible for the moose to escape once it fell in. The pits are normally found in large groups, crossing the moose's regular paths and stretching over several km. Remains of wooden fences designed to guide the animals toward the pits have been found in bogs and peat. In Norway, an early example of these trapping devices has been dated to around 3700 BC. Trapping elk in pits is an extremely effective hunting method. As early as the 16th century the Norwegian government tried to restrict their use; nevertheless, the method was in use until the 19th century.

The earliest recorded description of the moose is in Julius Caesar's "Commentarii de Bello Gallico", where it is described thus:

There are also [animals], which are called moose. The shape of these, and the varied color of their skins, is much like roes, but in size they surpass them a little and are destitute of horns, and have legs without joints and ligatures; nor do they lie down for the purpose of rest, nor, if they have been thrown down by any accident, can they raise or lift themselves up. Trees serve as beds to them; they lean themselves against them, and thus reclining only slightly, they take their rest; when the huntsmen have discovered from the footsteps of these animals whither they are accustomed to betake themselves, they either undermine all the trees at the roots, or cut into them so far that the upper part of the trees may appear to be left standing. When they have leant upon them, according to their habit, they knock down by their weight the unsupported trees, and fall down themselves along with them.
In book 8, chapter 16 of Pliny the Elder's "Natural History" from 77 CE, the elk and an animal called achlis, which is presumably the same animal, are described thus:
Moose are hunted as a game species in many of the countries where they are found. Moose meat tastes, wrote Henry David Thoreau in "The Maine Woods", "like tender beef, with perhaps more flavour; sometimes like veal". While the flesh has protein levels similar to those of other comparable red meats (e.g. beef, deer and wapiti), it has a low fat content, and the fat that is present consists of a higher proportion of polyunsaturated fats than saturated fats.

Dr. Valerius Geist, who emigrated to Canada from the Soviet Union, wrote in his 1999 book "Moose: Behaviour, Ecology, Conservation":

Boosting moose populations in Alaska for hunting purposes is one of the reasons given for allowing aerial or airborne methods to remove wolves in designated areas, e.g., Craig Medred: "A kill of 124 wolves would thus translate to [the survival of] 1488 moose or 2976 caribou or some combination thereof". Some scientists believe that this artificial inflation of game populations is actually detrimental to both caribou and moose populations as well as the ecosystem as a whole. This is because studies have shown that when these game populations are artificially boosted, it leads to both habitat destruction and a crash in these populations.

Cadmium levels are high in Finnish elk liver and kidneys, with the result that consumption of these organs from elk more than one year old is prohibited in Finland. As a result of a study reported in 1988, the Ontario Ministry of Natural Resources recommended against the consumption of moose and deer kidneys and livers. Levels of cadmium were found to be considerably higher than in Scandinavia. The New Brunswick Department of Natural Resources advises hunters not to consume cervid offal.

Cadmium intake has been found to be elevated amongst all consumers of elk meat, though the elk meat was found to contribute only slightly to the daily cadmium intake. However the consumption of moose liver or kidneys significantly increased cadmium intake, with the study revealing that heavy consumers of moose organs have a relatively narrow safety margin below the levels which would probably cause adverse health effects.

The center of mass of a moose is above the hood of most passenger cars. In a collision, the impact crushes the front roof beams and individuals in the front seats. Collisions of this type are frequently fatal; seat belts and airbags offer little protection. In collisions with higher vehicles (such as trucks), most of the deformation is to the front of the vehicle and the passenger compartment is largely spared. Moose collisions have prompted the development of a vehicle test referred to as the "moose test" (, ). A Massachusetts study found that moose–vehicular collisions had a very high human fatality rate and that such collisions caused the death of 3% of the Massachusetts moose population annually.

Moose warning signs are used on roads in regions where there is a danger of collision with the animal. The triangular warning signs common in Sweden, Norway, and Finland have become coveted souvenirs among tourists traveling in these countries, causing road authorities so much expense that the moose signs have been replaced with imageless generic warning signs in some regions.

In Ontario, Canada, an estimated 265 moose die each year as a result of collision with trains. Moose–train collisions were more frequent in winters with above-average snowfall. In January 2008, the Norwegian newspaper "Aftenposten" estimated that some 13,000 moose had died in collisions with Norwegian trains since 2000. The state agency in charge of railroad infrastructure (Jernbaneverket) plans to spend 80 million Norwegian kroner to reduce collision rate in the future by fencing the railways, clearing vegetation from near the tracks, and providing alternative snow-free feeding places for the animals elsewhere.

In the Canadian province of New Brunswick, collisions between automobiles and moose are frequent enough that all new highways have fences to prevent moose from accessing the road, as has long been done in Finland, Norway, and Sweden. A demonstration project, Highway 7 between Fredericton and Saint John, which has one of the highest frequencies of moose collisions in the province, did not have these fences until 2008, although it was and continues to be extremely well signed. Newfoundland and Labrador recommended that motorists use caution between dusk and dawn because that is when moose are most active and most difficult to see, increasing the risk of collisions. Local moose sightings are often reported on radio stations so that motorists can take care while driving in particular areas. An electronic "moose detection system" was installed on two sections of the Trans-Canada Highway in Newfoundland in 2011, but the system proved unreliable and was removed in 2015.

In Sweden, a road will not be fenced unless it experiences at least one moose accident per km per year.

In eastern Germany, where the scarce population is slowly increasing, there were two road accidents involving moose since 2000.

Domestication of moose was investigated in the Soviet Union before World War II. Early experiments were inconclusive, but with the creation of a moose farm at Pechora-Ilych Nature Reserve in 1949, a small-scale moose domestication program was started, involving attempts at selective breeding of animals on the basis of their behavioural characteristics. Since 1963, the program has continued at Kostroma Moose Farm, which had a herd of 33 tame moose as of 2003. Although at this stage the farm is not expected to be a profit-making enterprise, it obtains some income from the sale of moose milk and from visiting tourist groups. Its main value, however, is seen in the opportunities it offers for the research in the physiology and behavior of the moose, as well as in the insights it provides into the general principles of animal domestication.

In Sweden, there was a debate in the late 18th century about the national value of using the moose as a domestic animal. Among other things, the moose was proposed to be used in postal distribution, and there was a suggestion to develop a moose-mounted cavalry. Such proposals remained unimplemented, mainly because the extensive hunting for moose that was deregulated in the 1790s nearly drove it to extinction. While there have been documented cases of individual moose being used for riding and/or pulling carts and sleds, Björklöf concludes no wide-scale usage has occurred outside fairy tales.

Moose are an old genus. Like its relatives, "Odocoileus" and "Capreolus", the genus "Alces" gave rise to very few species that endured for long periods of time. This differs from the "Megacerines", such as the Irish elk, which evolved many species before going extinct. Some scientists, such as Adrian Lister, grouped all the species into one genus, while others, such as Augusto Azzaroli, used "Alces" for the living species, placing the fossil species into the genera "Cervalces" and "Libralces".

The earliest known species is "Libralces gallicus" (French moose), which lived in the Pliocene epoch, about 2 million years ago. "Libralces gallicus" came from the warm savannas of Pliocene Europe, with the best-preserved skeletons being found in southern France. "L. gallicus" was 1.25 times larger than the Alaskan moose in linear dimensions, making it nearly twice as massive. "L. gallicus" had many striking differences from its modern descendants. It had a longer, narrower snout and a less-developed nasal cavity, more resembling that of a modern deer, lacking any sign of the modern moose-snout. Its face resembled that of the modern wapiti. However, the rest of its skull structure, skeletal structure and teeth bore strong resemblance to those features that are unmistakable in modern moose, indicating a similar diet. Its antlers consisted of a horizontal bar long, with no tines, ending in small palmations. Its skull and neck structure suggest an animal that fought using high-speed impacts, much like the Dall sheep, rather than locking and twisting antlers the way modern moose combat. Their long legs and bone structure suggest an animal that was adapted to running at high speeds over rough terrain.

"Libralces" existed until the middle Pleistocene epoch and were followed briefly by a species called "Cervalces carnutorum". The main differences between the two consisted of shortening of the horizontal bar in the antlers and broadening of the palmations, indicating a likely change from open plains to more forested environments, and skeletal changes that suggest an adaptation to marshy environments.

"Cervalces carnutorum" was soon followed by a much larger species called "Cervalces latifrons" (broad-fronted stag-moose). The Pleistocene epoch was a time of gigantism, in which most species were much larger than their descendants of today, including exceptionally large lions, hippopotamuses, mammoths, and deer. Many fossils of "Cervalces latifrons" have been found in Siberia, dating from about 1.2 to 0.5 million years ago. This is most likely the time at which the species migrated from the Eurasian continent to North America. Like its descendants, it inhabited mostly northern latitudes, and was probably well-adapted to the cold. "Cervalces latifrons" was the largest deer known to have ever existed, standing more than tall at the shoulders. This is bigger than even the Irish elk (megacerine), which was tall at the shoulders. Its antlers were smaller than the Irish elk's, but comparable in size to those of "Libralces gallicus". However, the antlers had a shorter horizontal bar and larger palmations, more resembling those of a modern moose.

"Alces alces" (the modern moose) appeared during the late Pleistocene epoch. The species arrived in North America at the end of the Pleistocene and coexisted with a late-surviving variety or relative of "Cervalces latifrons", which Azzaroli classified as a separate species called "Cervalces scotti", or the American stag-moose.





</doc>
<doc id="20503" url="https://en.wikipedia.org/wiki?curid=20503" title="Medieval warfare">
Medieval warfare

Medieval warfare is the European warfare of the Middle Ages. Technological, cultural, and social developments had forced a severe transformation in the character of warfare from antiquity, changing military tactics and the role of cavalry and artillery (see military history). In terms of fortification, the Middle Ages saw the emergence of the castle in Europe, which then spread to Western Asia.

Publius Flavius Vegetius Renatus wrote "De re militari (Concerning Military Matters)" possibly in the late 4th century. Described by historian Walter Goffart as "the bible of warfare throughout the Middle Ages", "De re militari" was widely distributed through the Latin West. While Western Europe relied on a single text for the basis of its military knowledge, the Byzantine Empire in Southeastern Europe had a succession of military writers. Though Vegetius had no military experience and "De re militari" was derived from the works of Cato and Frontinus, his books were the standard for military discourse in Western Europe from their production until the 16th century.

"De re militari" was divided into five books: who should be a soldier and the skills they needed to learn, the composition and structure of an army, field tactics, how to conduct and withstand sieges, and the role of the navy. According to Vegetius, infantry was the most important element of an army because it was cheap compared to cavalry and could be deployed on any terrain. One of the tenets he put forward was that a general should only engage in battle when he was sure of victory or had no other choice. As archaeologist Robert Liddiard explains, "Pitched battles, particularly in the eleventh and twelfth centuries, were rare."

Although his work was widely reproduced, and over 200 copies, translations, and extracts survive today, the extent to which Vegetius affected the actual practice of warfare as opposed to its concept is unclear because of his habit of stating the obvious. Historian Michael Clanchy noted "the medieval axiom that laymen are illiterate and its converse that clergy are literate", so it may be the case that few soldiers read Vegetius' work. While their Roman predecessors were well-educated and had been experienced in warfare, the European nobility of the early Medieval period were not renowned for their education, but from the 12th century, it became more common for them to read.

Some soldiers regarded the experience of warfare as more valuable than reading about it; for example, Geoffroi de Charny, a 14th  century knight who wrote about warfare, recommended that his audience should learn by observing and asking advice from their superiors. While it is uncertain to what extent his work was read by the warrior class as opposed to the clergy, Vegetius remained prominent in the literature on warfare in the medieval period. In 1489, King Henry  VII of England commissioned the translation of "De re militari" into English, "so every gentleman born to arms and all manner of men of war, captains, soldiers, victuallers and all others would know how they ought to behave in the feats of wars and battles".

In Europe, breakdowns in centralized power led to the rise of several groups that turned to large-scale pillage as a source of income. Most notably the Vikings, Arabs, Mongols, Huns, Cumans, Tartars, and Magyars raided significantly. As these groups were generally small and needed to move quickly, building fortifications was a good way to provide refuge and protection for the people and the wealth in the region.

These fortifications evolved throughout the Middle Ages, the most important form being the castle, a structure which has become almost synonymous with the Medieval era in the popular eye. The castle served as a protected place for the local elites. Inside a castle they were protected from bands of raiders and could send mounted warriors to drive the enemy from the area, or to disrupt the efforts of larger armies to supply themselves in the region by gaining local superiority over foraging parties that would be impossible against the whole enemy host.

Fortifications were a very important part of warfare because they provided safety to the lord, his family, and his servants. They provided refuge from armies too large to face in open battle. The ability of the heavy cavalry to dominate a battle on an open field was useless against fortifications. Building siege engines was a time-consuming process, and could seldom be effectively done without preparations before the campaign. Many sieges could take months, if not years, to weaken or demoralize the defenders sufficiently. Fortifications were an excellent means of ensuring that the elite could not be easily dislodged from their lands – as Count Baldwin of Hainaut commented in 1184 on seeing enemy troops ravage his lands from the safety of his castle, "they can't take the land with them".

In the Medieval period besieging armies used a wide variety of siege engines including: scaling ladders; battering rams; siege towers and various types of catapults such as the mangonel, onager, ballista, and trebuchet. Siege techniques also included mining in which tunnels were dug under a section of the wall and then rapidly collapsed to destabilize the wall's foundation. Another technique was to bore into the enemy walls, however, this was not nearly as effective as other methods due to the thickness of castle walls.

Advances in the prosecution of sieges encouraged the development of a variety of defensive counter-measures. In particular, Medieval fortifications became progressively stronger – for example, the advent of the concentric castle from the period of the Crusades – and more dangerous to attackers – witness the increasing use of machicolations, as well the preparation of hot or incendiary substances. Arrow slits, concealed doors for sallies, and deep water wells were also integral to resisting siege at this time. Designers of castles paid particular attention to defending entrances, protecting gates with drawbridges, portcullises and barbicans. Wet animal skins were often draped over gates to repel fire. Moats and other water defences, whether natural or augmented, were also vital to defenders.

In the Middle Ages, virtually all large cities had city walls – Dubrovnik in Dalmatia is an impressive and well-preserved example – and more important cities had citadels, forts or castles. Great effort was expended to ensure a good water supply inside the city in case of siege. In some cases, long tunnels were constructed to carry water into the city. In other cases, such as the Ottoman siege of Shkodra, Venetian engineers had designed and installed cisterns that were fed by rain water channeled by a system of conduits in the walls and buildings. Complex systems of tunnels were used for storage and communications in medieval cities like Tábor in Bohemia. Against these would be matched the mining skills of teams of trained sappers, who were sometimes employed by besieging armies.

Until the invention of gunpowder-based weapons (and the resulting higher-velocity projectiles), the balance of power and logistics favoured the defender. With the invention of gunpowder, the traditional methods of defence became less and less effective against a determined siege.

The medieval knight was usually a mounted and armoured soldier, often connected with nobility or royalty, although (especially in north-eastern Europe) knights could also come from the lower classes, and could even be enslaved persons. The cost of their armour, horses, and weapons was great; this, among other things, helped gradually transform the knight, at least in western Europe, into a distinct social class separate from other warriors. During the crusades, holy orders of Knights fought in the Holy Land (see Knights Templar, the Hospitallers, etc.).

The light cavalry consisted usually of lighter armed and armoured men, who could have lances, javelins or missile weapons, such as bows or crossbows. In much of the Middle Ages, light cavalry usually consisted of wealthy commoners. Later in the Middle Ages, light cavalry would also include sergeants who were men who had trained as knights but could not afford the costs associated with the title. Light cavalry was used as scouts, skirmishers or outflankers. Many countries developed their styles of light cavalries, such as Hungarian mounted archers, Spanish jinetes, Italian and German mounted crossbowmen and English currours.

The infantry was recruited and trained in a wide variety of manners in different regions of Europe all through the Middle Ages, and probably always formed the most numerous part of a medieval field army. Many infantrymen in prolonged wars would be mercenaries. Most armies contained significant numbers of spearmen, archers and other unmounted soldiers.

In the earliest Middle Ages, it was the obligation of every noble to respond to the call to battle with his equipment, archers, and infantry. This decentralized system was necessary due to the social order of the time but could lead to motley forces with variable training, equipment and abilities. The more resources the noble had access to, the better his troops would typically be.

Typically the feudal armies consisted of a core of highly skilled knights and their household troops, mercenaries hired for the time of the campaign and feudal levies fulfilling their feudal obligations, who usually were little more than rabble. They could, however, be efficient in disadvantageous terrain. Towns and cities could also field militias.

As central governments grew in power, a return to the citizen and mercenary armies of the classical period also began, as central levies of the peasantry began to be the central recruiting tool. It was estimated that the best infantrymen came from the younger sons of free land-owning yeomen, such as the English archers and Swiss pikemen. England was one of the most centralized states in the Late Middle Ages, and the armies that fought the Hundred Years' War were mostly paid, professionals.

In theory, every Englishman had an obligation to serve for forty days. Forty days was not long enough for a campaign, especially one on the continent. Thus the scutage was introduced, whereby most Englishmen paid to escape their service and this money was used to create a permanent army. However, almost all high medieval armies in Europe were composed of a great deal of paid core troops, and there was a large mercenary market in Europe from at least the early 12th century.

As the Middle Ages progressed in Italy, Italian cities began to rely mostly on mercenaries to do their fighting rather than the militias that had dominated the early and high medieval period in this region. These would be groups of career soldiers who would be paid a set rate. Mercenaries tended to be effective soldiers, especially in combination with standing forces, but in Italy, they came to dominate the armies of the city-states. This made them problematic; while at war they were considerably more reliable than a standing army, at peacetime they proved a risk to the state itself like the Praetorian Guard had once been.

Mercenary-on-mercenary warfare in Italy led to relatively bloodless campaigns which relied as much on manoeuvre as on battles, since the condottieri recognized it was more efficient to attack the enemy's ability to wage war rather than his battle forces, discovering the concept of indirect warfare 500 years before Sir Basil Liddell Hart, and attempting to attack the enemy supply lines, his economy and his ability to wage war rather than risking an open battle, and manoeuvre him into a position where risking a battle would have been suicidal. Machiavelli understood this indirect approach as cowardice.

Weapons
Medieval weapons consisted of many different types of ranged and hand-held objects:

Armour

Artillery and Siege engine

Animals

The practice of carrying relics into battle is a feature that distinguishes medieval warfare from its predecessors or early modern warfare and possibly inspired by biblical references. The presence of relics was believed to be an important source of supernatural power that served both as a spiritual weapon and a form of defence; the relics of martyrs were considered by Saint John Chrysostom much more powerful than "walls, trenches, weapons and hosts of soldiers"

In Italy, the "carroccio" or "carro della guerra", the "war wagon", was an elaboration of this practice that developed during the 13th century. The "carro della guerra" of Milan was described in detail in 1288 by Bonvesin de la Riva in his book on the "Marvels of Milan". Wrapped in scarlet cloth and drawn by three yoke of oxen that were caparisoned in white with the red cross of Saint Ambrose, the city's patron, it carried a crucifix so massive it took four men to step it in place, like a ship's mast.

Medieval Warfare largely predated the use of supply trains- which meant that armies had to acquire food supplies from whatever territory they were passing through, this meant that large scale looting by soldiers was unavoidable, and was actively encouraged by the 14th century with its emphasis on "chevauchée" tactics, or use of units of light cavalry who would loot and pillage hostile territory in order to distract and demoralize the enemy while denying them their supplies.

Through the medieval period, soldiers were responsible for supplying themselves, either through foraging, looting, or purchases. Even so, military commanders often provided their troops with food and supplies, but this would be provided instead of the soldiers' wages, or soldiers would be expected to pay for it from their wages, either at cost or even with a profit.

In 1294, the same year John II de Balliol of Scotland refused to support Edward I of England's planned invasion of France, Edward I implemented a system in Wales and Scotland where sheriffs would acquire foodstuffs, horses and carts from merchants with compulsory sales at prices fixed below typical market prices under the Crown's rights of prise and purveyance. These goods would then be transported to Royal Magazines in southern Scotland and along the Scottish border where English conscripts under his command could purchase them. This continued during the First War of Scottish Independence which began in 1296, though the system was unpopular and was ended with Edward I's death in 1307.

Starting under the rule of Edward II in 1307 and ending under the rule of Edward III in 1337, the English instead used a system where merchants would be asked to meet armies with supplies for the conscripts to purchase. This led to discontent as the merchants saw an opportunity to profiteer, forcing conscripts to pay well above normal market prices for food.

As Edward III went to war with France in the Hundred Years' War (starting in 1337), the English returned to a practice of foraging and looting to meet their logistical needs. This practice lasted throughout war, extending through the remainder of Edward III's reign into the reign of Henry VI.

The waters surrounding Europe can be grouped into two types which affected the design of craft that traveled and therefore the warfare. The Mediterranean and Black Seas were free of large tides, generally calm, and had predictable weather. The seas around the north and west of Europe experienced stronger and less predictable weather. The weather gauge, the advantage of having a following wind, was an important factor in naval battles, particularly to the attackers. Typically westerlies (winds blowing from west to east) dominated Europe, giving naval powers to the west an advantage. Medieval sources on the conduct of medieval naval warfare are less common than those about land-based war. Most medieval chroniclers had no experience of life on the sea and generally were not well informed. Maritime archaeology has helped provide information.

Early in the medieval period, ships in the context of warfare were used primarily for transporting troops. In the Mediterranean, naval warfare in the Middle Ages was similar to that under late Roman Empire: fleets of galleys would exchange missile fire and then try to board bow first to allow marines to fight on deck. This mode of naval warfare remained the same into the early modern period, as, for example, at the Battle of Lepanto. Famous admirals included Roger of Lauria, Andrea Doria and Hayreddin Barbarossa.

Galleys were not suitable for the colder and more turbulent North Sea and the Atlantic Ocean, although they saw occasional use. Bulkier ships were developed which were primarily sail-driven, although the long lowboard Viking-style rowed longship saw use well into the 15th century. Their main purpose in the north remained the transportation of soldiers to fight on the decks of the opposing ship (as, for example, at the Battle of Svolder or the Battle of Sluys).
Late medieval sailing warships resembled floating fortresses, with towers in the bows and at the stern (respectively, the forecastle and aftcastle). The large superstructure made these warships quite unstable, but the decisive defeats that the more mobile but considerably lower boarded longships suffered at the hands of high-boarded cogs in the 15th century ended the issue of which ship type would dominate northern European warfare.

The introduction of guns was the first steps towards major changes in naval warfare, but it only slowly changed the dynamics of ship-to-ship combat. The first guns on ships were introduced in the 14th century and consisted of small wrought-iron pieces placed on the open decks and in the fighting tops, often requiring only one or two men to handle them. They were designed to injure, kill or simply stun, shock and frighten the enemy before boarding.

As guns were made more durable to withstand stronger gunpowder charges, they increased their potential to inflict critical damage to the vessel rather than just their crews. Since these guns were much heavier than the earlier anti-personnel weapons, they had to be placed lower in the ships, and fire from gunports, to avoid ships becoming unstable. In Northern Europe the technique of building ships with clinker planking made it difficult to cut ports in the hull; clinker-built (or clench-built) ships had much of their structural strength in the outer hull. The solution was the gradual adoption of carvel-built ships that relied on an internal skeleton structure to bear the weight of the ship.
The first ships to actually mount heavy cannon capable of sinking ships were galleys, with large wrought-iron pieces mounted directly on the timbers in the bow. The first example is known from a woodcut of a Venetian galley from 1486. Heavy artillery on galleys was mounted in the bow which fit conveniently with the long-standing tactical tradition of attacking head-on and bow-first. The ordnance on galleys was quite heavy from its introduction in the 1480s, and capable of quickly demolishing medieval-style stone walls that still prevailed until the 16th century.

This temporarily upended the strength of older seaside fortresses, which had to be rebuilt to cope with gunpowder weapons. The addition of guns also improved the amphibious abilities of galleys as they could assault supported with heavy firepower, and could be even more effectively defended when beached stern-first. Galleys and similar oared vessels remained uncontested as the most effective gun-armed warships in theory until the 1560s, and in practice for a few decades more, and were considered a grave risk to sailing warships.

In the Medieval period, the mounted cavalry long held sway on the battlefield. Heavily armoured mounted knights represented a formidable foe for reluctant peasant draftees and lightly armoured freemen. To defeat mounted cavalry, infantry used swarms of missiles or a tightly packed phalanx of men, techniques honed in antiquity by the Greeks.

The use of long pikes and densely packed foot troops was not uncommon in the Middle Ages. The Flemish footmen at the Battle of the Golden Spurs met and overcame French knights in 1302, as the Lombards did in Legnano in 1176 and the Scots held their own against heavily armoured English invaders. During the St. Louis crusade, dismounted French knights formed a tight lance-and-shield phalanx to repel Egyptian cavalry. The Swiss used pike tactics in the late medieval period. While pikemen usually grouped and awaited a mounted attack, the Swiss developed flexible formations and aggressive manoeuvring, forcing their opponents to respond. The Swiss won at Morgarten, Laupen, Sempach, Grandson and Murten, and between 1450 and 1550 every leading prince in Europe (except the English and Scottish) hired Swiss pikemen, or emulated their tactics and weapons (e.g., the German Landsknechte).

The Welsh & English longbowman used a single-piece longbow (but some bows later developed a composite design) to deliver arrows that could penetrate contemporary mail and damage/dent plate armour. The longbow was a difficult weapon to master, requiring long years of use and constant practice. A skilled longbowman could shoot about 12 shots per minute. This rate of fire was far superior to competing weapons like the crossbow or early gunpowder weapons. The nearest competitor to the longbow was the much more expensive crossbow, used often by urban militias and mercenary forces. The crossbow had greater penetrating power and did not require the extended years of training. However, it lacked the rate of fire of the longbow.

At Crécy and Agincourt bowmen unleashed clouds of arrows into the ranks of knights. At Crécy, even 5,000 Genoese crossbowmen could not dislodge them from their hill. At Agincourt, thousands of French knights were brought down by armour-piercing bodkin point arrows and horse-maiming broadheads. Longbowmen decimated an entire generation of the French nobility.

In 1326 the earliest known European picture of a gun appeared in a manuscript by Walter de Milemete. 
In 1350, Petrarch wrote that the presence of cannons on the battlefield was 'as common and familiar as other kinds of arms'.

Early artillery played a limited role in the Hundred Years' War, and it became indispensable in the Italian Wars of 1494–1559, marking the beginning of early modern warfare.
Charles VIII, during his invasion of Italy, brought with him the first truly mobile siege train: culverins and bombards mounted on wheeled carriages, which could be deployed against an enemy stronghold immediately after arrival.

The initial Muslim conquests began in the 7th century after the death of the Islamic prophet Muhammad, and were marked by a century of rapid Arab expansion beyond the Arabian Peninsula under the Rashidun and Umayyad Caliphates. Under the Rashidun, the Arabs conquered the Persian Empire, along with Roman Syria and Roman Egypt during the Byzantine-Arab Wars, all within just seven years from 633 to 640. Under the Umayyads, the Arabs annexed North Africa and southern Italy from the Romans and the Arab Empire soon stretched from parts of the Indian subcontinent, across Central Asia, the Middle East, North Africa, and southern Italy, to the Iberian Peninsula and the Pyrenees.

The early Arab army mainly consisted of camel-mounted infantry, alongside a few Bedouin cavalry. Constantly outnumbered by their opponent, they did, however, possess the advantage of strategic mobility, their camel-borne nature allowing them to constantly outmanoeuvre larger Byzantine and Sassanid armies to take prime defensive positions. The Rashidun cavalry, while lacking the number and mounted archery skill of their Roman and Persian counterparts was for the most part skillfully employed, and played a decisive role in many crucial battles such as Battle of Yarmouk.

In contrast, the Roman army and Persian army at the time both had large numbers of heavy infantry and heavy cavalry (cataphracts and clibanarii) that were better equipped, heavily protected, and more experienced and disciplined. The Arab invasions came at a time when both ancient powers were exhausted from the protracted Byzantine–Sassanid Wars, particularly the bitterly fought Byzantine–Sassanid War of 602–628 which had brought both empires close to collapse. Also, the typically multi-ethnic Byzantine force was always racked by dissension and lack of command unity, a similar situation also being encountered among the Sassanids who had been embroiled in a bitter civil war for a decade before the coming of the Arabs. In contrast, the Ridda Wars had forged the Caliphate's army into a united and loyal fighting force.

The Vikings were a feared force in Europe because of their savagery and speed of their attacks. Whilst seaborne raids were nothing new at the time, the Vikings refined the practice to a science through their shipbuilding, tactics and training. Unlike other raiders, the Vikings made a lasting impact on the face of Europe. During the Viking age, their expeditions, frequently combining raiding and trading, penetrated most of the old Frankish Empire, the British Isles, the Baltic region, Russia, and both Muslim and Christian Iberia. Many served as mercenaries, and the famed Varangian Guard, serving the Emperor of Constantinople, was drawn principally of Scandinavian warriors.

Viking longships were swift and easily manoeuvered; they could navigate deep seas or shallow rivers, and could carry warriors that could be rapidly deployed directly onto land due to the longships being able to land directly. The longship was the enabler of the Viking style of warfare that was fast and mobile, relying heavily on the element of surprise, and they tended to capture horses for mobility rather than carry them on their ships. The usual method was to approach a target stealthily, strike with surprise and then retire swiftly. The tactics used were difficult to stop, for the Vikings, like guerrilla-style raiders elsewhere, deployed at a time and place of their choosing. The fully armoured Viking raider would wear an iron helmet and a maille hauberk, and fight with a combination of axe, sword, shield, spear or great "Danish" two-handed axe, although the typical raider would be unarmoured, carrying only a bow and arrows, a knife "seax", a shield and spear; the swords and the axes were much less common.

Almost by definition, opponents of the Vikings were ill-prepared to fight a force that struck at will, with no warning. European countries with a weak system of government would be unable to organize a suitable response and would naturally suffer the most to Viking raiders. Viking raiders always had the option to fall back in the face of a superior force or stubborn defence and then reappear to attack other locations or retreat to their bases in what is now Sweden, Denmark, Norway and their Atlantic colonies. As time went on, Viking raids became more sophisticated, with coordinated strikes involving multiple forces and large armies, as the "Great Heathen Army" that ravaged Anglo-Saxon England in the 9th century. In time, the Vikings began to hold on to the areas they raided, first wintering and then consolidating footholds for further expansion later.

With the growth of centralized authority in the Scandinavian region, Viking raids, always an expression of "private enterprise", ceased and the raids became pure voyages of conquest. In 1066, King Harald Hardråde of Norway invaded England, only to be defeated by Harold Godwinson, who in turn was defeated by William of Normandy, descendant of the Viking Rollo, who had accepted Normandy as a fief from the Frankish king. The three rulers had their claims to the English crown (Harald probably primarily on the overlord-ship of Northumbria) and it was this that motivated the battles rather than the lure of plunder.

At that point, the Scandinavians had entered their medieval period and consolidated their kingdoms of Denmark, Norway, and Sweden. This period marks the end of significant raider activity both for plunder or conquest. The resurgence of centralized authority throughout Europe limited opportunities for traditional raiding expeditions in the West, whilst the Christianisation of the Scandinavian kingdoms themselves encouraged them to direct their attacks against the still predominantly pagan regions of the eastern Baltic. The Scandinavians started adapting more continental European ways, whilst retaining an emphasis on naval power – the "Viking" clinker-built warship was used in the war until the 14th century at least. However, developments in shipbuilding elsewhere removed the advantage the Scandinavian countries had previously enjoyed at sea, whilst castle building throughout frustrated and eventually ended Viking raids. Natural trading and diplomatic links between Scandinavia and Continental Europe ensured that the Scandinavians kept up to date with continental developments in warfare.

The Scandinavian armies of the High Middle Ages followed the usual pattern of the Northern European armies, but with a stronger emphasis on infantry. The terrain of Scandinavia favoured heavy infantry, and whilst the nobles fought mounted in the continental fashion, the Scandinavian peasants formed a well-armed and well-armoured infantry, of which approximately 30% to 50% would be archers or crossbowmen. The crossbow, the flatbow and the longbow were especially popular in Sweden and Finland. The chainmail, the lamellar armour and the coat of plates were the usual Scandinavian infantry armour before the era of plate armour.

By 1241, having conquered large parts of Russia, the Mongols continued the invasion of Europe with a massive three-pronged advance, following the fleeing Cumans, who had established an uncertain alliance with King Bela IV of Hungary. They first invaded Poland, and finally, Hungary, culminating in the crushing defeat of the Hungarians in the Battle of Mohi. The Mongol aim seems to have consistently been to defeat the Hungarian-Cuman alliance. The Mongols raided across the borders to Austria and Bohemia in the summer when the Great Khan died, and the Mongol princes returned home to elect a new Great Khan.

The Golden Horde would frequently clash with Hungarians, Lithuanians and Poles in the thirteenth century, with two large raids in the 1260s and 1280s respectively. In 1284 the Hungarians repelled the last major raid into Hungary, and in 1287 the Poles repelled a raid against them. The instability in the Golden Horde seems to have quieted the western front of the Horde. Also, the large scale invasions and raiding that had previously characterized the expansion of the Mongols was cut short probably in some part due to the death of the last great Mongol leader, Tamerlane.

The Hungarians and Poles had responded to the mobile threat by extensive fortification-building, army reform in the form of better-armoured cavalry, and refusing battle unless they could control the site of the battlefield to deny the Mongols local superiority. The Lithuanians relied on their forested homelands for defence and used their cavalry for raiding into Mongol-dominated Russia. When attacking fortresses they would launch dead or diseased animals into fortresses to help spread disease.

An early Turkic group, the Seljuks, were known for their cavalry archers. These fierce nomads were often raiding empires, such as the Byzantine Empire, and they scored several victories using mobility and timing to defeat the heavy cataphracts of the Byzantines.

One notable victory was at Manzikert, where conflict among the generals of the Byzantines gave the Turks the perfect opportunity to strike. They hit the cataphracts with arrows, and outmanoeuvred them, then rode down their less mobile infantry with light cavalry that used scimitars. When gunpowder was introduced, the Ottoman Turks of the Ottoman Empire hired the mercenaries that used the gunpowder weapons and obtained their instruction for the Janissaries. Out of these Ottoman soldiers rose the Janissaries ("yeni ceri"; "new soldier"), from which they also recruited many of their heavy infantry. Along with the use of cavalry and early grenades, the Ottomans mounted an offensive in the early Renaissance period and attacked Europe, taking Constantinople by massed infantry assaults.

Like many other nomadic peoples, the Turks featured a core of heavy cavalry from the upper classes. These evolved into the Sipahis (feudal landholders similar to western knights and Byzantine "pronoiai") and Qapukulu ("door slaves", taken from youth like Janissaries and trained to be royal servants and elite soldiers, mainly cataphracts).






</doc>
<doc id="20505" url="https://en.wikipedia.org/wiki?curid=20505" title="Magnetic tape">
Magnetic tape

Magnetic tape is a medium for magnetic recording, made of a thin, magnetizable coating on a long, narrow strip of plastic film. It was developed in Germany in 1928, based on magnetic wire recording. Devices that record and playback audio and video using magnetic tape are tape recorders and video tape recorders respectively. A device that stores computer data on magnetic tape is known as a tape drive.

Magnetic tape revolutionized sound recording and reproduction and broadcasting. It allowed radio, which had always been broadcast live, to be recorded for later or repeated airing. It allowed gramophone records to be recorded in multiple parts, which were then mixed and edited with tolerable loss in quality. It was a key technology in early computer development, allowing unparalleled amounts of data to be mechanically created, stored for long periods, and rapidly accessed.

In recent decades, other technologies have been developed that can perform the functions of magnetic tape. In many cases, these technologies have replaced tape. Despite this, innovation in the technology continues, and Sony and IBM continue to produce new magnetic tape drives. Linear Tape-Open is a magnetic tape-based medium used in computer systems for data backup, since it provides large capacities at a low cost, and works differently than common hard drives or solid-state drives, reducing the chance of it failing due to similar reasons. 

Over time, magnetic tape made in the 1970s and 1980s can suffer from a type of deterioration called sticky-shed syndrome. It is caused by hydrolysis of the binder in the tape and can render the tape unusable.

The "oxide side" of a tape is the surface that can be magnetically manipulated by a tape head. This is the side that stores the information, the opposite side is simply a "substrate" to give the tape strength and flexibility. The name originates from the fact that the magnetic side of most tapes is typically made of iron oxide, though chromium is used for some tapes. An adhesive "binder" between the oxide and the substrate holds the two sides together.

In all tape formats, a tape drive uses motors to wind the tape from one reel to another, passing over tape heads to read, write or erase as it moves.

Magnetic tape was invented for recording sound by Fritz Pfleumer in 1928 in Germany, based on the invention of magnetic wire recording by Oberlin Smith in 1888 and Valdemar Poulsen in 1898. Pfleumer's invention used a ferric oxide () powder coating on a long strip of paper. This invention was further developed by the German electronics company AEG, which manufactured the recording machines and BASF, at the time a division of IG Farben, which manufactured the tape. In 1933, working for AEG, Eduard Schuller developed the ring-shaped tape head. Previous head designs were needle-shaped and tended to shred the tape. Another important discovery made in this period was the technique of AC biasing, which improved the fidelity of the recorded audio signal by increasing the effective linearity of the recording medium.

Due to the escalating political tensions, and the outbreak of World War II, these developments in Germany were largely kept secret. Although the Allies knew from their monitoring of Nazi radio broadcasts that the Germans had some new form of recording technology, its nature was not discovered until the Allies acquired German recording equipment as they invaded Europe at the end of the war. It was only after the war that Americans, particularly Jack Mullin, John Herbert Orr, and Richard H. Ranger, were able to bring this technology out of Germany and develop it into commercially viable formats. Bing Crosby, an early adopter of the technology, made a large investment in the tape hardware manufacturer Ampex.

A wide variety of audio tape recorders and formats have been developed since, most significantly reel-to-reel and Compact Cassette.

Digital recording to flash memory and hard disk has largely supplanted magnetic tape for most purposes. However "tape" as a verb and as a noun has remained the common parlance for the recording process.

Some magnetic tape-based formats include:


The practice of recording and editing audio using magnetic tape rapidly established itself as an obvious improvement over previous methods. Many saw the potential of making the same improvements in recording the video signals used by television. Video signals use more bandwidth than audio signals. Existing audio tape recorders could not practically capture a video signal. Many set to work on resolving this problem. Jack Mullin (working for Bing Crosby) and the BBC both created crude working systems that involved moving the tape across a fixed tape head at very high speeds. Neither system saw much use. It was the team at Ampex, led by Charles Ginsburg, that made the breakthrough of using a spinning recording head and normal tape speeds to achieve a very high head-to-tape speed that could record and reproduce the high bandwidth signals of video. The Ampex system was called Quadruplex and used tape, mounted on reels like audio tape, which wrote the signal in what is now called "transverse scan".

Later improvements by other companies, particularly Sony, led to the development of helical scan and the enclosure of the tape reels in an easy-to-handle videocassette cartridge. Nearly all modern videotape systems use helical scan and cartridges. Videocassette recorders used to be common in homes and television production facilities, but many functions of the VCR have been replaced with more modern technology. Since the advent of digital video and computerized video processing, optical disc media and digital video recorders can now perform the same role as videotape. These devices also offer improvements like random access to any scene in the recording and the ability to pause a live program and have replaced videotape in many situations.

Some magnetic tape-based formats include:

Magnetic tape was first used to record computer data in 1951 on the Eckert-Mauchly UNIVAC I. The system's UNISERVO I tape drive used a thin strip of one half inch (12.65 mm) wide metal, consisting of nickel-plated bronze (called Vicalloy). Recording density was 100 characters per inch (39.37 characters/cm) on eight tracks.

Early IBM 7 track tape drives were floor-standing and used vacuum columns to mechanically buffer long U-shaped loops of tape. The two tape reels visibly fed tape through the columns, intermittently spinning 10.5 inch open reels in rapid, unsynchronized bursts, resulting in visually striking action. Stock shots of such vacuum-column tape drives in motion were widely used to represent mainframe computers in movies and television.

Most modern magnetic tape systems use reels that are much smaller than the 10.5 inch open reels and are fixed inside a cartridge to protect the tape and facilitate handling. Many late 1970s and early 1980s home computers used Compact Cassettes, encoded with the Kansas City standard, or alternate encodings. Modern cartridge formats include LTO, DLT, and DAT/DDC.

Tape remains a viable alternative to disk in some situations due to its lower cost per bit. This is a large advantage when dealing with large amounts of data. Though the areal density of tape is lower than for disk drives, the available surface area on a tape is far greater. The highest capacity tape media are generally on the same order as the largest available disk drives (about 5 TB in 2011). Tape has historically offered enough advantage in cost over disk storage to make it a viable product, particularly for backup, where media removability is necessary.

Tape has the benefit of a comparatively long duration during which the media can be guaranteed to retain the data stored on the media. Fifteen (15) to thirty (30) years of archival data storage is cited by manufacturers of modern data tape such as Linear Tape-Open media.

In 2002, Imation received a US$11.9 million grant from the U.S. National Institute of Standards and Technology for research into increasing the data capacity of magnetic tape.

In 2014, Sony and IBM announced that they had been able to record 148 gigabits per square inch with magnetic tape media developed using a new vacuum thin-film forming technology able to form extremely fine crystal particles, allowing true tape capacity of 185 TB.




</doc>
<doc id="20511" url="https://en.wikipedia.org/wiki?curid=20511" title="Mabo v Queensland (No 2)">
Mabo v Queensland (No 2)

Mabo v Queensland (No 2) (commonly known as Mabo) was a landmark High Court of Australia decision in 1992 recognising native title in Australia for the first time. It acknowledged that Aboriginal and Torres Strait Islander peoples had occupied the land before the arrival of British settlers, and had rights over the land.

The High Court held that the doctrine of "terra nullius", which imported all laws of England to a new land, did not apply in circumstances where there were already inhabitants present – even if those inhabitants had been regarded at the time as "uncivilised". Consequently, the Court held that the rules of reception of international English law that applied were not those applicable where the land was barren and unprotected, but rather the rules that applied where an existing people were settled. The result was that existing customary laws which were present at the time of settlement survived the reception of English law to the extent not modified or excluded by subsequent inconsistent laws and acts of the British. Relevantly, that existing law included indigenous land title. As such, any Indigenous land rights which had not been extinguished by subsequent grants by the Crown continued to exist in Australia.

In so ruling, the High Court overturned "Milirrpum v Nabalco Pty Ltd", a contrary decision of the Supreme Court of the Northern Territory.

The plaintiffs, headed by land rights campaigner Eddie Mabo, sought declarations, "inter alia", that the Meriam people were entitled to the Mer Islands "as owners; as possessors; as occupiers; or as persons entitled to use and enjoy the said islands".


Five judgments were delivered in the High Court, by (1) Justice Brennan, (2) Justice Deane and Justice Gaudron, (3) Justice Toohey, (4) Justice Dawson, the only dissenter, and (5) Chief Justice Mason and Justice McHugh.

The decision was based on the findings of fact made by Justice Moynihan of the Supreme Court of Queensland: that the Mer Islanders had a strong sense of relationship to the islands and regarded the land as theirs. All of the judges, except Justice Dawson, agreed that:

The "Mabo" decision presented many legal and political questions, including:

In response to the judgment, the Parliament of Australia, controlled by the Labor Party led by Prime Minister Paul Keating, enacted the "Native Title Act 1993", which established the National Native Title Tribunal (NTTA) to make native title determinations in the first instance, appealable to the Federal Court of Australia, and thereafter the High Court. Following "Wik Peoples v Queensland" (1996), Parliament amended the "Native Title Act" with the "Native Title Amendment Act 1998".

Ten years following the Mabo decision, Mrs Mabo claimed that issues remained within the community about land on Mer.

On 1 February 2014, the traditional owners of land on Badu Island received freehold title to in an act of the Queensland Government. An Indigenous Land Use Agreement (ILUA) was signed on 7 July 2014.

In his judgment, Justice Brennan stated that the definition of a person's Aboriginal Australian identity, or Aboriginality, depends on a tripartite test:

This test was subsequently used in other cases, such as "Love v Commonwealth of Australia; Thoms v Commonwealth of Australia" ([2020] HCA 3), where two men who has thus been determined as Aboriginal, could not be deported as aliens under the provisions of the "Migration Act 1958", after both had earlier been convicted of criminal offences and served time in prison until 2018.

A straight-to-TV film titled "Mabo" was produced in 2012 by Blackfella Films in association with the ABC and SBS. It provided a dramatised account of the case, focusing on the effect it had on Mabo and his family.

The case was also referenced as background to the plot in the 1997 comedy "The Castle".

In 2009 as part of the Q150 celebrations, the Mabo High Court of Australia decision was announced as one of the Q150 Icons of Queensland for its role as a "Defining Moment".





</doc>
<doc id="20512" url="https://en.wikipedia.org/wiki?curid=20512" title="MeatballWiki">
MeatballWiki

MeatballWiki was a wiki dedicated to online communities, network culture, and hypermedia.

According to founder Sunir Shah, it ran on "a hacked-up version of UseModWiki". In April 2013, after several spam attacks and a period of downtime, the site was made read-only.

MeatballWiki was started in 2000 by Sunir Shah, a forum administrator from Ontario, Canada, on Clifford Adams's Internet domain usemod.com. MeatballWiki was created as a place for discussion about Ward Cunningham's WikiWikiWeb and its operation, which were beyond the scope of WikiWikiWeb. As Sunir Shah stated in the WikiWikiWeb page referring to MeatballWiki: "Community discussions about how to run the community itself should be left here. Abstract discussions, or objective analyses of community are encouraged on MeatballWiki." Shah created this site "as a friendly fork of WikiWikiWeb." About the Meatball project, the website says: "The web, and media like it, looks like a big bowl of meatball spaghetti. You've got content--the meatballs--linked together with the spaghetti."

The original intent of MeatballWiki was to offer observations and opinions about wikis and their online communities, with the intent of helping online communities, culture and hypermedia. Being a community about communities, MeatballWiki became the launching point for other wiki-based projects and a general resource for broader wiki concepts, reaching "cult status". It describes the general tendencies observed on wikis and other on-line communities, for example the life cycles of wikis and people's behavior on them.
What differentiates MeatballWiki from many online meta-communities is that participants spend much of their time talking about sociology rather than technology, and when they do talk about technology, they do so in a social context.

The MeatballWiki members created a "bus tour" through existing wikis.

Barnstars - badges that wiki editors use to express appreciation for another editor's work - were invented on MeatballWiki and adapted by Wikipedia in 2003.




</doc>
<doc id="20513" url="https://en.wikipedia.org/wiki?curid=20513" title="Marrakesh">
Marrakesh

Marrakesh ( or ; "Marākiš"; , ) is the fourth largest city in the Kingdom of Morocco. It is the capital of the mid-southwestern region of Marrakesh-Safi. It is west of the foothills of the Atlas Mountains. Marrakesh is southwest of Tangier, southwest of the Moroccan capital of Rabat, south of Casablanca, and northeast of Agadir.

The region has been inhabited by Berber farmers since Neolithic times. The city was founded in 1070 by Emir Abu Bakr ibn Umar as the imperial capital of the Almoravid Empire. The city was one of Morocco's four imperial cities. In the 12th century, the Almoravids built many madrasas (Quranic schools) and mosques in Marrakesh that bear Andalusian influences. The red walls of the city, built by Ali ibn Yusuf in 1122–1123, and various buildings constructed in red sandstone during this period, have given the city the nickname of the "Red City" () or "Ochre City" (). Marrakesh grew rapidly and established itself as a cultural, religious, and trading center for the Maghreb and sub-Saharan Africa. Jemaa el-Fnaa is the busiest square in Africa.

After a period of decline, the city was surpassed by Fez, but in the early 16th century, Marrakesh again became the capital of the kingdom. The city regained its preeminence under wealthy Saadian sultans Abu Abdallah al-Qaim and Ahmad al-Mansur, who embellished the city with sumptuous palaces such as the El Badi Palace (1578) and restored many ruined monuments. Beginning in the 17th century, the city became popular among Sufi pilgrims for its seven patron saints who are entombed here. In 1912 the French Protectorate in Morocco was established and T'hami El Glaoui became Pasha of Marrakesh and held this position nearly throughout the protectorate until the role was dissolved upon the independence of Morocco and the reestablishment of the monarchy in 1956. In 2009, Marrakesh mayor Fatima Zahra Mansouri became the second woman to be elected mayor in Morocco.

Marrakesh comprises an old fortified city packed with vendors and their stalls. This medina quarter is a UNESCO World Heritage Site. Today it is one of the busiest cities in Africa and serves as a major economic center and tourist destination. Tourism is strongly advocated by the reigning Moroccan monarch, Mohammed VI, with the goal of doubling the number of tourists visiting Morocco to 20 million by 2020. Despite the economic recession, real estate and hotel development in Marrakesh have grown dramatically in the 21st century. Marrakesh is particularly popular with the French, and numerous French celebrities own property in the city. Marrakesh has the largest traditional market ("souk") in Morocco, with some 18 "souks" selling wares ranging from traditional Berber carpets to modern consumer electronics. Crafts employ a significant percentage of the population, who primarily sell their products to tourists.

Marrakesh is served by Ménara International Airport and by Marrakesh railway station which connects the city to Casablanca and northern Morocco. Marrakesh has several universities and schools, including Cadi Ayyad University. A number of Moroccan football clubs are here, including Najm de Marrakech, KAC Marrakech, Mouloudia de Marrakech and Chez Ali Club de Marrakech. The Marrakesh Street Circuit hosts the World Touring Car Championship, Auto GP and FIA Formula Two Championship races.

The exact meaning of the name is debated. One possible origin of the name Marrakesh is from the Berber (Amazigh) words "amur (n) akush" (ⴰⵎⵓⵔ ⵏ ⴰⴽⵓⵛ), which means "Land of God". According to historian Susan Searight, however, the town's name was first documented in an 11th-century manuscript in the Qarawiyyin library in Fez, where its meaning was given as "country of the sons of Kush". The word "mur" is used now in Berber mostly in the feminine form "tamurt". The same word "mur" appears in Mauretania, the North African kingdom from antiquity, although the link remains controversial as this name possibly originates from μαύρος "mavros", the ancient Greek word for black. The common English spelling is "Marrakesh", although "Marrakech" (the French spelling) is also widely used. The name is spelt "Mṛṛakc" in the Berber Latin alphabet, "Marraquexe" in Portuguese, "Marraquech" in Spanish, and "Mer-raksh" in Moroccan Arabic.

From medieval times until around the beginning of the 20th century, the entire country of Morocco was known as the "Kingdom of Marrakesh", as the kingdom's historic capital city was often Marrakesh. The name for Morocco is still "Marrakesh" () to this day in Persian and Urdu as well as many other South Asian languages. Various European names for Morocco (Marruecos, Marrocos, Maroc, Marokko, etc.) are directly derived from the Berber word "Murakush". Conversely, the city itself was in earlier times simply called "Marocco City" (or similar) by travelers from abroad. The name of the city and the country diverged after the Treaty of Fez divided Morocco into a French protectorate in Morocco and Spanish protectorate in Morocco, but the old interchangeable usage lasted widely until about the interregnum of Mohammed Ben Aarafa (1953–1955). The latter episode set in motion the country's return to independence, when Morocco officially became ("al-Mamlaka al-Maġribiyya", "The Maghreb Kingdom"), its name no longer referring to the city of Marrakesh. Marrakesh is known by a variety of nicknames, including the "Red City", the "Ochre City" and "the Daughter of the Desert", and has been the focus of poetic analogies such as one comparing the city to "a drum that beats an African identity into the complex soul of Morocco."

The Marrakesh area was inhabited by Berber farmers from Neolithic times, and numerous stone implements have been unearthed in the area. Marrakesh was founded by Abu Bakr ibn Umar, chieftain and second cousin of the Almoravid king Yusuf ibn Tashfin (c. 1061–1106). Historical sources cite a variety of dates for this event ranging between 1062 (454 in the Hijri calendar), according to Ibn Abi Zar and Ibn Khaldun, and 1078 (470 AH), according to Muhammad al-Idrisi. The date most commonly accepted by modern historians is 1070 although 1062 is still cited by some writers. Under the berber dynasty of the Almoravids, pious and learned warriors from the desert, numerous mosques and madrasas (Quranic schools) were built, developing the community into a trading centre for the Maghreb and sub-Saharan Africa. Marrakesh grew rapidly and established itself as a cultural and religious centre, supplanting Aghmat, which had long been the capital of Haouz. Andalusian craftsmen from Cordoba and Seville built and decorated numerous palaces in the city, developing the Umayyad style characterised by carved domes and cusped arches. This Andalusian influence merged with designs from the Sahara and West Africa, creating a unique style of architecture which was fully adapted to the Marrakesh environment. Yusuf ibn Tashfin completed the city's first mosque (the Ben Youssef mosque, named after his son), built houses, minted coins, and brought gold and silver to the city in caravans. The city became the capital of the Almoravid Emirate, stretching from the shores of Senegal to the centre of Spain and from the Atlantic coast to Algiers.
Marrakesh is one of the great citadels of the Muslim world. The city was fortified by Tashfin's son, Ali ibn Yusuf, who in 1122–1123 built the ramparts which remain to this day, completed further mosques and palaces, and developed an underground water system in the city known as the "rhettara" to irrigate his new garden. In 1125, the preacher Ibn Tumert settled in Tin Mal in the mountains to the south of Marrakesh. He preached against the Almoravids and influenced a revolt which succeeded in bringing about the fall of nearby Aghmat, but stopped short of bringing down Marrakesh following an unsuccessful siege in 1130. The Almohads, Masmouda tribesmen from the High Atlas mountains who practiced orthodox Islam, took the city in 1147 under leader Abd al-Mu'min. After a long siege and the killing of some 7,000 people, the last of the Almoravids were exterminated apart from those who sought exile in the Balearic Islands. As a result, almost all the city's monuments were destroyed. The Almohads constructed a range of palaces and religious buildings, including the famous Koutoubia Mosque (1184–1199), and built upon the ruins of an Almoravid palace. It was a twin of the Giralda in Seville and the unfinished Hassan Tower in Rabat, all built by the same designer. The Kasbah housed the residence of the caliph, a title borne by the Almohad rulers from the reign of Abd al-Mu'min, rivaling the far eastern Abbasid Caliphate. The Kasbah was built by the caliph Yaqub al-Mansur. The irrigation system was perfected to provide water for new palm groves and parks, including the Menara Garden. As a result of its cultural reputation, Marrakesh attracted many writers and artists, especially from Andalusia, including the famous philosopher Averroes of Cordoba.

The death of Yusuf II in 1224 began a period of instability. Marrakesh became the stronghold of the Almohad tribal sheikhs and the "ahl ad-dar" (descendants of Ibn Tumart), who sought to claw power back from the ruling Almohad family. Marrakesh was taken, lost and retaken by force multiple times by a stream of caliphs and pretenders, such as during the brutal seizure of Marrakesh by the Sevillan caliph Abd al-Wahid II al-Ma'mun in 1226, which was followed by a massacre of the Almohad tribal sheikhs and their families and a public denunciation of Ibn Tumart's doctrines by the caliph from the pulpit of the Kasbah Mosque. After al-Ma'mun's death in 1232, his widow attempted to forcibly install her son, acquiring the support of the Almohad army chiefs and Spanish mercenaries with the promise to hand Marrakesh over to them for the sack. Hearing of the terms, the people of Marrakesh sought to make an agreement with the military captains and saved the city from destruction with a sizable payoff of 500,000 dinars. In 1269, Marrakesh was conquered by nomadic Zenata tribes who overran the last of the Almohads. The city then fell into a state of decline, which soon led to the loss of its status as capital to rival city Fez.
In the early 16th century, Marrakesh again became the capital of the kingdom, after a period when it was the seat of the Hintata emirs. It quickly reestablished its status, especially during the reigns of the Saadian sultans Abu Abdallah al-Qaim and Ahmad al-Mansur. Thanks to the wealth amassed by the Sultans, Marrakesh was embellished with sumptuous palaces while its ruined monuments were restored. El Badi Palace, built by Ahmad al-Mansur in 1578, was a replica of the Alhambra Palace, made with costly and rare materials including marble from Italy, gold dust from Sudan, porphyry from India and jade from China. The palace was intended primarily for hosting lavish receptions for ambassadors from Spain, England, and the Ottoman Empire, showcasing Saadian Morocco as a nation whose power and influence reached as far as the borders of Niger and Mali. Under the Saadian dynasty, Marrakesh regained its former position as a point of contact for caravan routes from the Maghreb, the Mediterranean and sub-Saharan Africa. 

For centuries Marrakesh has been known as the location of the tombs of Morocco's seven patron saints ("sebaatou rizjel"). When sufism was at the height of its popularity during the late 17th-century reign of Moulay Ismail, the festival of these saints was founded by Abu Ali al-Hassan al-Yusi at the request of the sultan. The tombs of several renowned figures were moved to Marrakesh to attract pilgrims, and the pilgrimage associated with the seven saints is now a firmly established institution. Pilgrims visit the tombs of the saints in a specific order, as follows: Sidi Yusuf Ali Sanhaji (1196–97), a leper; Qadi Iyyad or qadi of Ceuta (1083–1149), a theologian and author of Ash-Shifa (treatises on the virtues of Muhammad); Sidi Bel Abbas (1130–1204), known as the patron saint of the city and most revered in the region; Sidi Muhammad al-Jazuli (1465), a well known Sufi who founded the Jazuli brotherhood; Abdelaziz al-Tebaa (1508), a student of al-Jazuli; Abdallah al-Ghazwani (1528), known as Moulay al-Ksour; and Sidi Abu al-Qasim Al-Suhayli, (1185), also known as Imam al-Suhayli. Until 1867, European Christians were not authorized to enter the city unless they acquired special permission from the sultan; east European Jews were permitted.

During the early 20th century, Marrakesh underwent several years of unrest. After the premature death in 1900 of the grand vizier Ba Ahmed, who had been designated regent until the designated sultan Abd al-Aziz became of age, the country was plagued by anarchy, tribal revolts, the plotting of feudal lords, and European intrigues. In 1907, Marrakesh caliph Moulay Abd al-Hafid was proclaimed sultan by the powerful tribes of the High Atlas and by Ulama scholars who denied the legitimacy of his brother, Abd al-Aziz. It was also in 1907 that Dr. Mauchamp, a French doctor, was murdered in Marrakesh, suspected of spying for his country. France used the event as a pretext for sending its troops from the eastern Moroccan town of Oujda to the major metropolitan center of Casablanca in the west. The French colonial army encountered strong resistance from Ahmed al-Hiba, a son of Sheikh Ma al-'Aynayn, who arrived from the Sahara accompanied by his nomadic Reguibat tribal warriors. On 30 March 1912, the French Protectorate in Morocco was established. After the Battle of Sidi Bou Othman, which saw the victory of the French Mangin column over the al-Hiba forces in September 1912, the French seized Marrakesh. The conquest was facilitated by the rallying of the Imzwarn tribes and their leaders from the powerful Glaoui family, leading to a massacre of Marrakesh citizens in the resulting turmoil.

T'hami El Glaoui, known as "Lord of the Atlas", became Pasha of Marrakesh, a post he held virtually throughout the 44-year duration of the Protectorate (1912–1956). Glaoui dominated the city and became famous for his collaboration with the general residence authorities, culminating in a plot to dethrone Mohammed Ben Youssef (Mohammed V) and replace him with the Sultan's cousin, Ben Arafa. Glaoui, already known for his amorous adventures and lavish lifestyle, became a symbol of Morocco's colonial order. He could not, however, subdue the rise of nationalist sentiment, nor the hostility of a growing proportion of the inhabitants. Nor could he resist pressure from France, who agreed to terminate its Moroccan Protectorate in 1956 due to the launch of the Algerian War (1954–1962) immediately following the end of the war in Indochina (1946–1954), in which Moroccans had been conscripted to fight in Vietnam on behalf of the French Army. After two successive exiles to Corsica and Madagascar, Mohammed Ben Youssef was allowed to return to Morocco in November 1955, bringing an end to the despotic rule of Glaoui over Marrakesh and the surrounding region. A protocol giving independence to Morocco was then signed on 2 March 1956 between French Foreign Minister Christian Pineau and M’Barek Ben Bakkai.
Since the independence of Morocco, Marrakesh has thrived as a tourist destination. In the 1960s and early 1970s, the city became a trendy "hippie mecca". It attracted numerous western rock stars and musicians, artists, film directors and actors, models, and fashion divas, leading tourism revenues to double in Morocco between 1965 and 1970. Yves Saint Laurent, The Beatles, The Rolling Stones and Jean-Paul Getty all spent significant time in the city; Laurent bought a property here and renovated the Majorelle Gardens. Expatriates, especially those from France, have invested heavily in Marrakesh since the 1960s and developed many of the "riads" and palaces. Old buildings were renovated in the Old Medina, new residences and commuter villages were built in the suburbs, and new hotels began to spring up.

United Nations agencies became active in Marrakesh beginning in the 1970s, and the city's international political presence has subsequently grown. In 1985, UNESCO declared the old town area of Marrakesh a UNESCO World Heritage Site, raising international awareness of the cultural heritage of the city. In the 1980s, Patrick Guerand-Hermes purchased the Ain el Quassimou, built by the family of Leo Tolstoy. On 15 April 1994, the Marrakesh Agreement was signed here to establish the World Trade Organisation, and in March 1997 Marrakesh served as the site of the World Water Council's first World Water Forum, which was attended by over 500 international participants.

From November 7 to 18, 2016, the city of Marrakesh was host to the meeting of United Nations Framework Convention on Climate Change (UNFCCC), known as the 22nd Session of the Conference of the Parties, or COP 22. Also known as 2016 United Nations Climate Change Conference it also served as the first meeting of the governing body of the Paris Agreement, known by the acronym CMA1. The UNFCCC secretariat (UN Climate Change) was established in 1992 when countries adopted the UNFCCC. In recent years, the secretariat also supports the Marrakech Partnership for Global Climate Action, agreed by governments to signal that successful climate action requires strong support from a wide range of actors, including regions, cities, business, investors and all parts of civil society. Commencing six months ahead of the start of the UN Climate Change Conference in Marrakesh, construction work at the Bab Ighli site was launched. The site was composed of two zones. The “Blue Zone”, placed under the authority of the United Nations, and spanning 154,000 m2 and consisting notably of two plenary rooms, 30 conference and meeting rooms for negotiators and 10 meeting rooms reserved for observers. The second zone, the "Green Zone", was reserved for non-state actors, NGOs, private companies, state institutions and organizations, and local authorities within two areas (“civil society” and “innovations”) each measuring 12,000 m2. The area will also include spaces dedicated to exhibitions and restaurants. The total surface of the Bab Ighli site will be 223,647 m2 (more than 80,000 m2 covered by a roof).

In the 21st century, property and real estate development in the city has boomed, with a dramatic increase in new hotels and shopping centres, fuelled by the policies of Mohammed VI of Morocco, who aims to increase the number of tourists annually visiting Morocco to 20 million by 2020. In 2010, a major gas explosion occurred in the city. On 28 April 2011, a bomb attack took place in the Jemaa el-Fnaa square, killing 15 people, mainly foreigners. The blast destroyed the nearby Argana Cafe. Police sources arrested three suspects and claimed the chief suspect was loyal to Al-Qaeda, although Al-Qaeda in the Islamic Maghreb denied involvement. In November 2016 the city hosted the 2016 United Nations Climate Change Conference.

By road, Marrakesh is southwest of Tangier, southwest of the Moroccan capital of Rabat, southwest of Casablanca, southwest of Beni Mellal, east of Essaouira, and northeast of Agadir. The city has expanded north from the old centre with suburbs such as Daoudiat, Diour El Massakine, Sidi Abbad, Sakar and Amerchich, to the southeast with Sidi Youssef Ben Ali, to the west with Massira and Targa, and southwest to M'hamid beyond the airport. On the P2017 road leading south out of the city are large villages such as Douar Lahna, Touggana, Lagouassem, and Lahebichate, leading eventually through desert to the town of Tahnaout at the edge of the High Atlas, the highest mountainous barrier in North Africa. The average elevation of the snow-covered High Atlas lies above . It is mainly composed of Jurassic limestone. The mountain range runs along the Atlantic coast, then rises to the east of Agadir and extends northeast into Algeria before disappearing into Tunisia.
The Ourika River valley is about south of Marrakesh. The "silvery valley of the Ourika river curving north towards Marrakesh", and the "red heights of Jebel Yagour still capped with snow" to the south are sights in this area. David Prescott Barrows, who describes Marrakesh as Morocco's "strangest city", describes the landscape in the following terms: "The city lies some fifteen or twenty miles [25–30 km] from the foot of the Atlas mountains, which here rise to their grandest proportions. The spectacle of the mountains is superb. Through the clear desert air the eye can follow the rugged contours of the range for great distances to the north and eastward. The winter snows mantle them with white, and the turquoise sky gives a setting for their grey rocks and gleaming caps that is of unrivaled beauty."

With 130,000 hectares of greenery and over 180,000 palm trees in its Palmeraie, Marrakesh is an oasis of rich plant variety. Throughout the seasons, fragrant orange, fig, pomegranate and olive trees display their color and fruits in Agdal Garden, Menara Garden and other gardens in the city. The city's gardens feature numerous native plants alongside other species that have been imported over the course of the centuries, including giant bamboos, yuccas, papyrus, palm trees, banana trees, cypress, philodendrons, rose bushes, bougainvilleas, pines and various kinds of cactus plants.

A hot semi-arid climate (Köppen climate classification "BSh") predominates at Marrakesh. Average temperatures range from in the winter to in the summer. The relatively wet winter and dry summer precipitation pattern of Marrakesh mirrors precipitation patterns found in Mediterranean climates. However, the city receives less rain than is typically found in a Mediterranean climate, resulting in a semi-arid climate classification. Between 1961 and 1990 the city averaged of precipitation annually. Barrows says of the climate, "The region of Marrakesh is frequently described as desert in character, but, to one familiar with the southwestern parts of the United States, the locality does not suggest the desert, but rather an area of seasonal rainfall, where moisture moves underground rather than by surface streams, and where low brush takes the place of the forests of more heavily watered regions. The location of Marrakesh on the north side of the Atlas, rather than the south, prevents it from being described as a desert city, but it remains the northern focus of the Saharan lines of communication, and its history, its types of dwellers, and its commerce and arts, are all related to the great south Atlas spaces that reach further into the Sahara desert."

According to the 2014 census, the population of Marrakesh was 928,850 against 843,575 in 2004. The number of households in 2014 was 217,245 against 173,603 in 2004.

Marrakesh is a vital component to the economy and culture of Morocco. Improvements to the highways from Marrakesh to Casablanca, Agadir and the local airport have led to a dramatic increase in tourism in the city, which now attracts over two million tourists annually. Because of the importance of tourism to Morocco's economy, King Mohammed VI has vowed to attract 20 million tourists a year to Morocco by 2020, doubling the number of tourists from 2012. The city is popular with the French, and many French celebrities have bought property in the city, including fashion moguls Yves St Laurent and Jean-Paul Gaultier. In the 1990s very few foreigners lived in the city, but real estate developments have dramatically increased in the last 15 years; by 2005 over 3,000 foreigners had purchased properties in the city, lured by its culture and the relatively cheap house prices. It has been cited in French weekly magazine "Le Point" as the second St Tropez: "No longer simply a destination for a scattering of adventurous elites, bohemians or backpackers seeking Arabian Nights fantasies, Marrakech is becoming a desirable stopover for the European jet set." However, despite the tourism boom, the majority of the city's inhabitants are still poor, and , some 20,000 households still have no access to water or electricity. Many enterprises in the city are facing colossal debt problems.

Despite the global economic crisis that began in 2007, investments in real estate progressed substantially in 2011 both in the area of tourist accommodation and social housing. The main developments have been in facilities for tourists including hotels and leisure centres such as golf courses and health spas, with investments of 10.9 billion dirham (US$1.28 billion) in 2011. The hotel infrastructure in recent years has experienced rapid growth. In 2012, alone, 19 new hotels were scheduled to open, a development boom often compared to Dubai. Royal Ranches Marrakech, one of Gulf Finance House's flagship projects in Morocco, is a resort under development in the suburbs and one of the world's first five star Equestrian Resorts. The resort is expected to make a significant contribution to the local and national economy, creating many jobs and attracting thousands of visitors annually; as of April 2012 it was about 45% complete. 
The Avenue Mohammed VI, formerly Avenue de France, is a major city thoroughfare. It has seen rapid development of residential complexes and many luxury hotels. Avenue Mohammed VI contains what is claimed to be Africa's largest nightclub: Pacha Marrakech, a trendy club that plays house and electro house music. It also has two large cinema complexes, Le Colisée à Gueliz and Cinéma Rif, and a new shopping precinct, Al Mazar.

Trade and crafts are extremely important to the local tourism-fueled economy. There are 18 "souks" in Marrakesh, employing over 40,000 people in pottery, copperware, leather and other crafts. The "souks" contain a massive range of items from plastic sandals to Palestinian-style scarves imported from India or China. Local boutiques are adept at making western-style clothes using Moroccan materials. The "Birmingham Post" comments: "The "souk" offers an incredible shopping experience with a myriad of narrow winding streets that lead through a series of smaller markets clustered by trade. Through the squawking chaos of the poultry market, the gory fascination of the open-air butchers' shops and the uncountable number of small and specialist traders, just wandering around the streets can pass an entire day." Marrakesh has several supermarkets including Marjane Acima, Asswak Salam and Carrefour, and three major shopping centres, Al Mazar Mall, Plaza Marrakech and Marjane Square; a branch of Carrefour opened in Al Mazar Mall in 2010. Industrial production in the city is centred in the neighbourhood of Sidi Ghanem Al Massar, containing large factories, workshops, storage depots and showrooms. Ciments Morocco, a subsidiary of a major Italian cement firm, has a factory in Marrakech. The AeroExpo Marrakech International Exhibition of aeronautical industries and services is held here, as is the Riad Art Expo.

Marrakesh is one of North Africa's largest centers of wildlife trade, despite the illegality of most of this trade. Much of this trade can be found in the medina and adjacent squares. Tortoises are particularly popular for sale as pets, but Barbary macaques and snakes can also be seen. The majority of these animals suffer from poor welfare conditions in these stalls.

Marrakesh, the regional capital, constitutes a prefecture-level administrative unit of Morocco, Marrakech Prefecture, forming part of the region of Marrakech-Safi. Marrakesh is a major centre for law and jurisdiction in Morocco and most of the major courts of the region are here. These include the regional Court of Appeal, the Commercial Court, the Administrative Court, the Court of First Instance, the Court of Appeal of Commerce, and the Administrative Court of Appeal. Numerous organizations of the region are based here, including the regional government administrative offices, the Regional Council of Tourism office, and regional public maintenance organisations such as the Governed Autonomous Water Supply and Electricity and Maroc Telecom.

Testament to Marrakesh's development as a modern city, on 12 June 2009, Fatima-Zahra Mansouri, a then 33-year-old lawyer and daughter of a former assistant to the local authority chief in Marrakesh, was elected the first female mayor of the city, defeating outgoing Mayor Omar Jazouli by 54 votes to 35 in a municipal council vote. Mansouri became the second woman in the history of Morocco to obtain a mayoral position, after Asma Chaabi, mayor of Essaouira. The Secretary General of her Authenticity and Modernity Party (PAM), Mohamed Cheikh Biadillah, stated that "her election reflects the image of a modern Morocco." Her appointment was shrouded in controversy and resulted in her temporarily losing her seat the following month after a court ruled that the election had been fixed. The court found that "some ballots were distributed before the legal date and some vote records were destroyed." Her party called for a 48-hour strike to "protest the plot against the democratic process." On 7 July 2011, Mansouri presented her resignation from the city council of Marrakesh, but reconsidered her decision the next day.

Since the legislative elections in November 2011, the ruling political party in Marrakesh has, for the first time, been the Justice and Development Party or PDJ which also rules at the national level. The party, which advocates Islamism and Islamic democracy, won five seats; the National Rally of Independents (RNI) took one seat, while the PAM won three. In the partial legislative elections for the Guéliz Ennakhil constituency in October 2012, the PDJ under the leadership of Ahmed El Moutassadik was again declared the winner with 10,452 votes. The PAM, largely consisting of friends of King Mohammed VI, came in second place with 9,794 votes.

The Jemaa el-Fnaa is one of the best-known squares in Africa and is the centre of city activity and trade. It has been described as a "world-famous square", "a metaphorical urban icon, a bridge between the past and the present, the place where (spectacularized) Moroccan tradition encounters modernity." It has been part of the UNESCO World Heritage site since 1985. The name roughly means "the assembly of trespassers" or malefactors. Jemaa el-Fnaa was renovated along with much of the Marrakech city, whose walls were extended by Abu Yaqub Yusuf and particularly by Yaqub al-Mansur in 1147–1158. The surrounding mosque, palace, hospital, parade ground and gardens around the edges of the marketplace were also overhauled, and the Kasbah was fortified. Subsequently, with the fluctuating fortunes of the city, Jemaa el-Fnaa saw periods of decline and renewal.
Historically this square was used for public decapitations by rulers who sought to maintain their power by frightening the public. The square attracted dwellers from the surrounding desert and mountains to trade here, and stalls were raised in the square from early in its history. The square attracted tradesmen, snake charmers ("wild, dark, frenzied men with long disheveled hair falling over their naked shoulders"), dancing boys of the Chleuh Atlas tribe, and musicians playing pipes, tambourines and African drums. Richard Hamilton said that Jemaa el-Fnaa once "reeked of Berber particularism, of backward-looking, ill-educated countrymen, rather than the reformist, pan-Arab internationalism and command economy that were the imagined future." Today the square attracts people from a diversity of social and ethnic backgrounds and tourists from all around the world. Snake charmers, acrobats, magicians, mystics, musicians, monkey trainers, herb sellers, story-tellers, dentists, pickpockets, and entertainers in medieval garb still populate the square.

Marrakesh has the largest traditional market in Morocco and the image of the city is closely associated with its "souks". Paul Sullivan cites the "souks" as the principal shopping attraction in the city: "A honeycomb of intricately connected alleyways, this fundamental section of the old city is a micro-medina in itself, comprising a dizzying number of stalls and shops that range from itsy kiosks no bigger than an elf's wardrobe to scruffy store-fronts that morph into glittering Aladdin's Caves once you're inside." Historically the souks of Marrakesh were divided into retail areas for particular goods such as leather, carpets, metalwork and pottery. These divisions still roughly exist but with significant overlap. Many of the souks sell items like carpets and rugs, traditional Muslim attire, leather bags, and lanterns. Haggling is still a very important part of trade in the souks.

One of the largest "souks" is Souk Semmarine, which sells everything from brightly coloured bejewelled sandals and slippers and leather pouffes to jewellery and kaftans. Souk Ableuh contains stalls which specialize in lemons, chilis, capers, pickles, green, red, and black olives, and mint, a common ingredient of Moroccan cuisine and tea. Similarly, Souk Kchacha specializes in dried fruit and nuts, including dates, figs, walnuts, cashews and apricots. Rahba Qedima contains stalls selling hand-woven baskets, natural perfumes, knitted hats, scarves, tee shirts, Ramadan tea, ginseng, and alligator and iguana skins. Criee Berbiere, to the northeast of this market, is noted for its dark Berber carpets and rugs. Souk Siyyaghin is known for its jewellery, and Souk Smata nearby is noted for its extensive collection of babouches and belts. Souk Cherratine specializes in leatherware, and Souk Belaarif sells modern consumer goods. Souk Haddadine specializes in ironware and lanterns.

Ensemble Artisanal is a government-run complex of small arts and crafts which offers a range of leather goods, textiles and carpets. Young apprentices are taught a range of crafts in the workshop at the back of this complex.

The ramparts of Marrakesh, which stretch for some around the medina of the city, were built by the Almoravids in the 12th century as protective fortifications. The walls are made of a distinct orange-red clay and chalk, giving the city its nickname as the "red city"; they stand up to high and have 20 gates and 200 towers along them. Bab Agnaou was built in the 12th century during the Almohad dynasty. The Berber name Agnaou, like Gnaoua, refers to people of Sub-Saharan African origin (cf. Akal-n-iguinawen – land of the black). The gate was called Bab al Kohl (the word "kohl" also meaning "black") or Bab al Qsar (palace gate) in some historical sources. The corner-pieces are embellished with floral decorations. This ornamentation is framed by three panels marked with an inscription from the Quran in Maghrebi script using foliated Kufic letters, which were also used in Al-Andalus. Bab Agnaou was renovated and its opening reduced in size during the rule of sultan Mohammed ben Abdallah. Bab Aghmat is east of the Jewish and Muslim cemeteries, and is near the tomb of Ali ibn Yusuf. Bab Berrima with its solid towers stands near the Badi Palace. Bab er Robb is a southern exit from the city, near Bab Agnaou. Built in the 12th century, it provides access to roads leading to the mountain towns of Amizmiz and Asni. Bab el Khemis—in the medina's northeastern corner, and so-called for the open-air Thursday market (Souq el Khemis)—is one of the city's main gates and features a man-made spring. Bab Doukkala (in the northwestern part of the city wall) is in general more massive but less ornamented than the other gates; it takes its name from Doukkala area on the Atlantic coast, well to the north of Marrakesh.

The Menara gardens are to the west of the city, at the gates of the Atlas mountains. They were built around 1130 by the Almohad ruler Abd al-Mu'min. The name "menara" derives from the pavilion with its small green pyramid roof ("menzeh"). The pavilion was built during the 16th century Saadi dynasty and renovated in 1869 by sultan Abderrahmane of Morocco, who used to stay here in summertime.

The pavilion and a nearby artificial lake are surrounded by orchards and olive groves. The lake was created to irrigate the surrounding gardens and orchards using a sophisticated system of underground channels called a "qanat". The basin is supplied with water through an old hydraulic system which conveys water from the mountains approximately away from Marrakesh. There is also a small amphitheater and a symmetrical pool where films are screened. Carp fish can be seen in the pond.

The Majorelle Garden, on Avenue Yacoub el Mansour, was at one time the home of the landscape painter Jacques Majorelle. Famed designer Yves Saint Laurent bought and restored the property, which features a stele erected in his memory, and the Museum of Islamic Art, which is housed in a dark blue building. The garden, open to the public since 1947, has a large collection of plants from five continents including cacti, palms and bamboo.

The Agdal Gardens, south of the medina and also built in the 12th century, are royal orchards surrounded by "pise" walls. Measuring in size, the gardens feature citrus, apricot, pomegranate, olive and cypress trees. Sultan Moulay Hassan's harem resided at the Dar al Baida pavilion, which was within these gardens. This site is also known for its historic swimming pool, where a Sultan is said to have drowned.

The Koutoubia Gardens are behind the Koutoubia Mosque. They feature orange and palm trees, and are frequented by storks. The Mamounia Gardens, more than 100 years old and named after Prince Moulay Mamoun, have olive and orange trees as well as a variety of floral displays.

In 2016, artist André Heller opened the acclaimed garden ANIMA near Ourika, which combines a large collection of plants, palms, bamboo and cacti as well as works by Keith Haring, Auguste Rodin, Hans Werner Geerdts and other artists.

The historic wealth of the city is manifested in palaces, mansions and other lavish residences. The main palaces are El Badi Palace, the Royal Palace and Bahia Palace. "Riads" (Moroccan mansions) are common in Marrakesh. Based on the design of the Roman villa, they are characterized by an open central garden courtyard surrounded by high walls. This construction provided the occupants with privacy and lowered the temperature within the building. Buildings of note inside the Medina are Riad Argana, Riad Obry, Riad Enija, Riad el Mezouar, Riad Frans Ankone, Dar Moussaine, Riad Lotus, Riad Elixir, Riad les Bougainvilliers, Riad Dar Foundouk, Dar Marzotto, Dar Darma, and Riad Pinco Pallino. Others of note outside the Medina area include Ksar Char Bagh, Amanjena, Villa Maha, Dar Ahlam, Dar Alhind and Dar Tayda.

The El Badi Palace flanks the eastern side of the Kasbah. It was built by Saadian sultan Ahmad al-Mansur after his success against the Portuguese at the Battle of the Three Kings in 1578. The lavish palace, which took around a quarter of a century to build, was funded by compensation from the Portuguese and African gold and sugar cane revenue. This allowed Carrara marble to be brought from Italy and other materials to be shipped from France, Spain and India. It is a larger version of the Alhambra's Court of the Lions. Although the palace is now a ruin with little left but the outer walls, the site has become the location of the annual Marrakech Folklore Festival and other events.

The Royal Palace, also known as Dar el-Makhzen, is next to the Badi Palace. The Almohads built the palace in the 12th century on the site of their kasba, and it was partly remodeled by the Saadians in the 16th century and the Alaouites in the 17th century. Historically it was one of the palaces owned by the Moroccan king, who employed some of the most talented craftsmen in the city for its construction. The palace is not open to the public, and is now privately owned by French businessman Dominique du Beldi. The rooms are large, with unusually high ceilings for Marrakesh, with "zellij" (elaborate geometric terracotta tile work covered with enamel) and cedar painted ceilings.

The Bahia Palace, set in extensive gardens, was built in the late 19th century by the Grand Vizier of Marrakesh, Si Ahmed ben Musa (Bou-Ahmed). Bou Ahmed resided here with his four wives, 24 concubines and many children. With a name meaning "brilliance", it was intended to be the greatest palace of its time, designed to capture the essence of Islamic and Moroccan architectural styles. Bou-Ahmed paid special attention to the privacy of the palace in its construction and employed architectural features such as multiple doors which prevented passers-by from seeing into the interior. The palace took seven years to build, with hundreds of craftsmen from Fez working on its wood, carved stucco and "zellij". The palace is set in a two-acre (8,000 m²) garden with rooms opening onto courtyards. The palace acquired a reputation as one of the finest in Morocco and was the envy of other wealthy citizens. Upon the death of Bou-Ahmed in 1900, the palace was raided by Sultan Abd al-Aziz.

The Koutoubia Mosque is the largest mosque in the city, in the southwest of the medina quarter of Marrakesh, within sight of the Jemaa al-Fnaa. It was completed under the reign of the Almohad Caliph Yaqub al-Mansur (1184–1199), and has inspired other buildings such as the Giralda of Seville and the Hassan Tower of Rabat. The mosque is made of red stone and brick and measures long and wide. The minaret is constructed from sandstone and stands high. It was originally covered with Marrakshi pink plaster, but in the 1990s experts opted to remove the plaster to expose the original stone work. The spire atop the minaret is decorated with gilded copper balls that decrease in size towards the top, a style unique to Morocco.

Ben Youssef Mosque, distinguished by its green tiled roof and minaret, is in the medina and is Marrakesh's oldest mosque. It was originally built in the 12th century by the Almoravid Sultan Ali ibn Yusuf, whom it is named after. It served as the city's main Friday mosque. After being abandoned during the Almohad period and falling into ruin, it was rebuilt in the 1560s by the Saadian sultan Abdallah al-Ghalib and then completely rebuilt again by the Alaouite sultan Moulay Sliman at the beginning of the 19th century, with construction of the minaret finishing in 1819 or 1820. This reconstruction has eliminated all traces of the original mosque, and the current mosque is much smaller than the original and features a very different "qibla" orientation. Abdallah al-Ghalib was also responsible for building the adjacent Ben Youssef Madrasa, which contained a library and operated as an educational institution up until the 20th century.

The only Almoravid-era remnant of the original mosque is the nearby Koubba Ba’adiyn, a two-storied ablutions kiosk discovered in a sunken location next to the mosque site in 1948. It demonstrates a sophisticated style and is an important piece of historical Moroccan architecture. Its arches are scalloped on the first floor, while those on the second floor bear a twin horseshoe shape embellished with a turban motif. The dome of the kiosk is framed by a battlement decorated with arches and seven-pointed stars. The interior of the octagonal arched dome is decorated with distinctive carvings bordered by a Kufic frieze inscribed with the name of its patron, Sultan Ali ibn Yusuf. The squinches at the corners of the dome are covered with muqarnas. The kiosk has motifs of pine cones, palms and acanthus leaves which are also replicated in the Ben Youssef Madrasa.

The Kasbah Mosque overlooks Place Moulay Yazid in the Kasbah district of Marrakesh, close to the El Badi Palace. It was built by the Almohad caliph Yaqub al-Mansour in the late 12th century to serve as the main mosque of the kasbah (citadel) where he and his high officials resided. It features a unique floor plan and courtyard layout that sets it apart from other classic Moroccan mosques. It contended with the Koutoubia Mosque for prestige and the decoration of its minaret was highly influential in subsequent Moroccan architecture. The mosque was repaired by the Saadi sultan Moulay Abd Allah al-Ghalib following a devastating explosion at a nearby gunpowder reserve in the second half of the 16th century. Notably, the Saadian Tombs were built just outside its qibla (southern) wall, and visitors pass behind the mosque to see them today.

The Mouassine Mosque (also known as the Al Ashraf Mosque) was built by the Saadian Sultan Moulay Abdallah al-Ghalib between 1562–63 and 1572–73. It is in the Mouassine district and is part of a larger architectural complex which includes a library, hammam (public bathhouse), madrasa (school) and a triple arched fountain known as the Mouassine Fountain. The fountain, which provided locals with access to water, is one of the largest and most important in the city, decorated with geometric patterns and Arabic inscriptions. Along with Bab Doukkala Mosque further west, which was built around the same time, the Mouassine Mosque appears to have been originally designed to anchor the development of new neighbourhoods after the relocation of the Jewish district from this area to the new "mellah" near the Kasbah.

The Saadian Tombs were built in the 16th century as a royal necropolis for the Saadian sultans and their family members. It is next to the south wall of the Kasbah Mosque. It was lost for many years until the French rediscovered it in 1917 using aerial photographs. The mausoleum comprises the remains of about sixty members of the Saadi Dynasty which originated in the valley of the Draa River. Among the tombs are those of Saadian sultan Ahmad al-Mansur and his family; al-Mansur buried his mother in this dynastic necropolis in 1590 after enlarging the original square funeral structure constructed by Abdallah al-Ghalib. His own mausoleum, richly embellished, was modeled on the Nasrid mausoleum in the Alhambra of Granada, Spain. It comprises a roof of carved and painted cedar wood supported on twelve columns of carrara marble, as well as walls covered in elaborate geometric patterns in "zellij" tilework, Arabic calligraphic inscriptions, and vegetal motifs in carved stucco. The chamber also contains the graves of his close family members and some of his successors, many of which are covered with horizontal tombstones of finely carved marble. His chamber is adjoined by two other rooms, the largest of which was originally a prayer room, equipped with a "mihrab", which was later repurposed as a mausoleum for members of the Alaouite dynasty.

The medina holds the tombs of the seven patron saints of the city, which are visited every year by pilgrims during the week-long "ziara" pilgrimage. A pilgrimage to the tombs offers an alternative to the "hajj" to Mecca and Medina for people of western Morocco who could not visit Arabia due to the arduous and costly journey involved. This ritual is performed over seven days in the following order: Sidi Yusuf ibn Ali Sanhaji, Sidi al-Qadi Iyyad al-Yahsubi, Sidi Bel Abbas, Sidi Mohamed ibn Sulayman al-Jazouli, Sidi Abdellaziz Tabba'a, Sidi Abdellah al-Ghazwani, and lastly, Sidi Abderrahman al-Suhayli. Many of these mausoleums also serve as the focus of their own zawiyas (Sufi religious complexes with mosques), including: the Zawiya and mosque of Sidi Bel Abbes (the most important of them), the Zawiya of al-Jazuli, the Zawiya of Sidi Abdellaziz, the Zawiya of Sidi Yusuf ibn Ali, and the Zawiya of Sidi al-Ghazwani (also known as Moulay el-Ksour).

The old Jewish Quarter ("Mellah") is in the kasbah area of the city's medina, east of Place des Ferblantiers. It was created in 1558 by the Saadians at the site where the sultan's stables were. At the time, the Jewish community consisted of a large portion of the city's bankers, jewelers, metalworkers, tailors and sugar traders. During the 16th century, the Mellah had its own fountains, gardens, synagogues and souks. Until the arrival of the French in 1912, Jews could not own property outside of the Mellah; all growth was consequently contained within the limits of the neighborhood, resulting in narrow streets, small shops and higher residential buildings. The Mellah, today reconfigured as a mainly residential zone renamed Hay Essalam, currently occupies an area smaller than its historic limits and has an almost entirely Muslim population. The Alzama Synagogue, built around a central courtyard, is in the Mellah. The Jewish cemetery here is the largest of its kind in Morocco. Characterized by white-washed tombs and sandy graves, the cemetery is within the Medina on land adjacent to the Mellah.

As one of the principal tourist cities in Africa, Marrakesh has over 400 hotels. Mamounia Hotel is a five-star hotel in the Art Deco-Moroccan fusion style, built in 1925 by Henri Prost and A. Marchis. It is considered the most eminent hotel of the city and has been described as the "grand dame of Marrakesh hotels." The hotel has hosted numerous internationally renowned people including Winston Churchill, Prince Charles of Wales and Mick Jagger. Churchill used to relax within the gardens of the hotel and paint there. The 231-room hotel, which contains a casino, was refurbished in 1986 and again in 2007 by French designer Jacques Garcia. Other hotels include Eden Andalou Hotel, Hotel Marrakech, Sofitel Marrakech, Palm Plaza Hotel & Spa, Royal Mirage Hotel, Piscina del Hotel, and Palmeraie Palace at the Palmeraie Rotana Resort. In March 2012, Accor opened its first Pullman-branded hotel in Marrakech, Pullman Marrakech Palmeraie Resort & Spa. Set in a olive grove at La Palmeraie, the hotel has 252 rooms, 16 suites, six restaurants and a conference room.

The Marrakech Museum, housed in the Dar Menebhi Palace in the old city centre, was built at the end of the 19th century by Mehdi Menebhi. The palace was carefully restored by the Omar Benjelloun Foundation and converted into a museum in 1997. The house itself represents an example of classical Andalusian architecture, with fountains in the central courtyard, traditional seating areas, a hammam and intricate tilework and carvings. It has been cited as having "an orgy of stalactite stucco-work" which "drips from the ceiling and combines with a mind-boggling excess of "zellij" work." The museum holds exhibits of both modern and traditional Moroccan art together with fine examples of historical books, coins and pottery produced by Moroccan Jewish, Berber and Arab peoples.

Dar Si Said Museum, also known as the Museum of Moroccan Arts is to the north of the Bahia Palace. It was the mansion of Si Said, brother to Grand Vizier Ba Ahmad, and was constructed at the same time as Ahmad's own Bahia Palace. The collection of the museum is considered to be one of the finest in Morocco, with "jewellery from the High Atlas, the Anti Atlas and the extreme south; carpets from the Haouz and the High Atlas; oil lamps from Taroudannt; blue pottery from Safi and green pottery from Tamgroute; and leatherwork from Marrakesh." Among its oldest and most significant artifacts is an early 11th-century marble basin from the late caliphal period of Cordoba, Spain. 

The Museum of Islamic Art ("Musée d'Art Islamique") is a blue-coloured building in the Marjorelle Gardens. The private museum was created by Yves Saint Laurent and Pierre Bergé in the home of Jacques Majorelle, who had his art studio there. Recently renovated, its small exhibition rooms have displays of Islamic artifacts and decorations including Irke pottery, polychrome plates, jewellery, and antique doors.

Two types of music are traditionally associated with Marrakesh. Berber music is influenced by Andalusian classical music and typified by its "oud" accompaniment. By contrast, Gnaoua music is loud and funky with a sound reminiscent of the Blues. It is performed on handmade instruments such as castanets, "ribabs" (three-stringed banjos) and "deffs" (handheld drums). Gnaoua music's rhythm and crescendo take the audience into a mood of trance; the style is said to have emerged in Marrakesh and Essaouira as a ritual of deliverance from slavery. More recently, several Marrakesh female music groups have also risen to popularity.

The Théâtre Royal de Marrakesh, the Institut Français and Dar Chérifa are major performing arts institutions in the city. The Théâtre Royal, built by Tunisian architect Charles Boccara, puts on theatrical performances of comedy, opera, and dance in French and Arabic. A greater number of theatrical troupes perform outdoors and entertain tourists on the main square and the streets, especially at night. Christopher Hudson of the "Daily Mail" noted that "men dressed as women performed bawdy street theatre, to the delight of a ring of onlookers of all ages."

The arts and crafts of Marrakesh have had a wide and enduring impact on Moroccan handicrafts to the present day. Riad décor is widely used in carpets and textiles, ceramics, woodwork, metal work and "zelij". Carpets and textiles are weaved, sewn or embroidered, sometimes used for upholstering. Moroccan women who practice craftsmanship are known as "Maalems" (expert craftspeople) and make such fine products as Berber carpets and shawls made of "sabra" (cactus silk). Ceramics are in monochrome Berber-style only, a limited tradition depicting bold forms and decorations.

Wood crafts are generally made of cedar, including the "riad" doors and palace ceilings. Orange wood is used for making ladles known as "harira" (lentil soup ladles). "Thuya" craft products are made of caramel coloured "thuya", a conifer indigenous to Morocco. Since this species is almost extinct, these trees are being replanted and promoted by the artists' cooperative Femmes de Marrakech.

Metalwork made in Marrakesh includes brass lamps, iron lanterns, candle holders made from recycled sardine tins, and engraved brass teapots and tea trays used in the traditional serving of tea. Contemporary art includes sculpture and figurative paintings. Blue veiled "Tuareg" figurines and calligraphy paintings are also popular.

Festivals, both national and Islamic, are celebrated in Marrakesh and throughout the country, and some of them are observed as national holidays. Cultural festivals of note held in Marrakesh include the National Folklore Festival, the Marrakech Festival of Popular Arts (in which a variety of famous Moroccan musicians and artists participate), international folklore festival Marrakech Folklore Days and the Berber Festival. The International Film Festival of Marrakech, which aspires to be the North African version of the Cannes Film Festival, was established in 2001. The festival, which showcases over 100 films from around the world annually, has attracted Hollywood stars such as Martin Scorsese, Francis Ford Coppola, Susan Sarandon, Jeremy Irons, Roman Polanski and many European, Arabic and Indian film stars. The Marrakech Bienniale was established in 2004 by Vanessa Branson as a cultural festival in various disciplines, including visual arts, cinema, video, literature, performing arts, and architecture.

Surrounded by lemon, orange, and olive groves, the city's culinary characteristics are rich and heavily spiced but not hot, using various preparations of "Ras el hanout" (which means "Head of the shop"), a blend of dozens of spices which include ash berries, chilli, cinnamon, grains of paradise, monk's pepper, nutmeg, and turmeric. A specialty of the city and the symbol of its cuisine is "tanjia marrakshia" a local "tajine" prepared with beef meat, spices and "smen" and slow-cooked in a traditional oven in hot ashes. Tajines can be prepared with chicken, lamb, beef or fish, adding fruit, olives and preserved lemon, vegetables and spices, including cumin, peppers, saffron, turmeric, and "ras el hanout". The meal is prepared in a tajine pot and slow-cooked with steam. Another version of tajine includes vegetables and chickpeas seasoned with flower petals. Tajines may also be basted with "smen" moroccan ghee that has a flavour similar to blue cheese.

Shrimp, chicken and lemon-filled "briouats" are another traditional specialty of Marrakesh. Rice is cooked with saffron, raisins, spices, and almonds, while couscous may have added vegetables. A "pastilla" is a filo-wrapped pie stuffed with minced chicken or pigeon that has been prepared with almonds, cinnamon, spices and sugar. Harira soup in Marrakesh typically includes lamb with a blend of chickpeas, lentils, vermicelli, and tomato paste, seasoned with coriander, spices and parsley. "Kefta" (mince meat), liver in "crépinette", "merguez" and tripe stew are commonly sold at the stalls of Jemaa el-Fnaa.

The desserts of Marrakesh include "chebakia" (sesame spice cookies usually prepared and served during Ramadan), tartlets of filo dough with dried fruit, or cheesecake with dates.

The Moroccan tea culture is practiced in Marrakesh; green tea with mint is served with sugar from a curved teapot spout into small glasses. Another popular non-alcoholic drink is orange juice. Under the Almoravids, alcohol consumption was common; historically, hundreds of Jews produced and sold alcohol in the city. In the present day, alcohol is sold in some hotel bars and restaurants.

Marrakesh has several universities and schools, including Cadi Ayyad University (also known as the University of Marrakech), and its component, the École nationale des sciences appliquées de Marrakech (ENSA Marrakech), which was created in 2000 by the Ministry of Higher Education and specializes in engineering and scientific research, and the La faculté des sciences et techniques-gueliz which known to be number one in Morocco in its kind of faculties. Cadi Ayyad University was established in 1978 and operates 13 institutions in the Marrakech Tensift Elhaouz and Abda Doukkala regions of Morocco in four main cities, including Kalaa of Sraghna, Essaouira and Safi in addition to Marrakech. Sup de Co Marrakech, also known as the École Supérieure de Commerce de Marrakech, is a private four-year college that was founded in 1987 by Ahmed Bennis. The school is affiliated with the École Supérieure de Commerce of Toulouse, France; since 1995 the school has built partnership programs with numerous American universities including the University of Delaware, University of St. Thomas, Oklahoma State University, National-Louis University, and Temple University. 

The Ben Youssef Madrasa, north of the Medina, was an Islamic college in Marrakesh named after the Almoravid sultan Ali ibn Yusuf (1106–1142) who expanded the city and its influence considerably. It is the largest madrasa in all of Morocco and was one of the largest theological colleges in North Africa, at one time housing as many as 900 students.

The college, which was affiliated with the neighbouring Ben Youssef Mosque, was founded during the Marinid dynasty in the 14th century by Sultan Abu al-Hassan.

This education complex specialized in Quranic law and was linked to similar institutions in Fez, Taza, Salé, and Meknes. The Madrasa was re-constructed by the Saadian Sultan Abdallah al-Ghalib (1557–1574) in 1564 as the largest and most prestigious madrasa in Morocco. The construction ordered by Abdallah al-Ghalib was completed in 1565, as attested by the inscription in the prayer room. Its 130 student dormitory cells cluster around a courtyard richly carved in cedar, marble and stucco. In accordance with Islam, the carvings contain no representation of humans or animals, consisting entirely of inscriptions and geometric patterns. One of the school's best known teachers was Mohammed al-Ifrani (1670–1745). After a temporary closure beginning in 1960, the building was refurbished and reopened to the public as a historical site in 1982.

Football clubs based in Marrakesh include Najm de Marrakech, KAC Marrakech, Mouloudia de Marrakech and Chez Ali Club de Marrakech. The city contains the Circuit International Automobile Moulay El Hassan a race track which hosts the World Touring Car Championship and from 2017 FIA Formula E. The Marrakech Marathon is also held here. Roughly 5000 runners turn out for the event annually.
Also, here takes place Grand Prix Hassan II tennis tournament (on clay) part of ATP World Tour series.

Golf is a popular sport in Marrakech. The city has three golf courses just outside the city limits and played almost through the year. The three main courses are the Golf de Amelikis on the road to Ourazazate, the Palmeraie Golf Palace near the Palmeraie, and the Royal Golf Club, the oldest of the three courses.

The Marrakesh railway station is linked by several trains running daily to other major cities in Morocco such as Casablanca, Tangiers, Fez, Meknes and Rabat. The Casablanca–Tangier high-speed rail line opened in November 2018.

In 2015, a is proposed.

The main road network within and around Marrakesh is well paved. The major highway connecting Marrakesh with Casablanca to the south is A7, a toll expressway, in length. The road from Marrakesh to Settat, a stretch, was inaugurated by King Mohammed VI in April 2007, completing the highway to Tangiers. Highway A7 connects also Marrakesh to Agadir, to the south-west.

The Marrakesh-Menara Airport (RAK) is southwest of the city centre. It is an international facility that receives several European flights as well as flights from Casablanca and several Arab nations. The airport is at an elevation of at . It has two formal passenger terminals, but these are more or less combined into one large terminal. A third terminal is being built. The existing T1 and T2 terminals offer a space of and have a capacity of 4.5 million passengers per year. The blacktopped runway is long and wide. The airport has parking space for 14 Boeing 737 and four Boeing 747 aircraft. The separate freight terminal has of covered space.

Marrakesh has long been an important centre for healthcare in Morocco, and the regional rural and urban populations alike are reliant upon hospitals in the city. The psychiatric hospital installed by the Merinid Caliph Ya'qub al-Mansur in the 16th century was described by the historian 'Abd al-Wahfd al- Marrakushi as one of the greatest in the world at the time. A strong Andalusian influence was evident in the hospital, and many of the physicians to the Caliphs came from places such as Seville, Zaragoza and Denia in eastern Spain.

A severe strain has been placed upon the healthcare facilities of the city in the last decade as the city population has grown dramatically. Ibn Tofail University Hospital is one of the major hospitals of the city. In February 2001, the Moroccan government signed a loan agreement worth eight million U.S. dollars with The OPEC Fund for International Development to help improve medical services in and around Marrakesh, which led to expansions of the Ibn Tofail and Ibn Nafess hospitals. Seven new buildings were constructed, with a total floor area of . New radiotherapy and medical equipment was provided and of existing hospital space was rehabilitated.
In 2009, king Mohammed VI inaugurated a regional psychiatric hospital in Marrakesh, built by the Mohammed V Foundation for Solidarity, costing 22 million "dirhams" (approximately 2.7 million U.S. dollars). 
The hospital has 194 beds, covering an area of . Mohammed VI has also announced plans for the construction of a 450 million dirham military hospital in Marrakesh.

Marrakesh is twinned with:






</doc>
<doc id="20515" url="https://en.wikipedia.org/wiki?curid=20515" title="Matilda of Ringelheim">
Matilda of Ringelheim

Matilda of Ringelheim (c. 89214 March 968), also known as Saint Matilda, was a Saxon noblewoman. Due to her marriage to Henry I in 909, she became the first Ottonian queen. Her eldest son, Otto I, restored the Holy Roman Empire in 962. Mathilde founded several spiritual institutions and women's convents. She was considered to be extremely pious, righteous and charitable. Mathilde’s two hagiographical biographies and "The Deeds of the Saxons" serve as authoritative sources about her life and work.

Mathilde, daughter of Reinhild and the Saxon Count Dietrich (himself a descendant of the Saxon duke Widukind who fought against Charlemagne) was born in around 892, and was raised by her grandmother Mathilde in Herford Abbey. She had three sisters; Amalrada, Bia, and Fridarun, who married Charles III, king of West Francia; and a brother Beuve II, the Bishop of Châlons-sur-Marne. Due to Fridarun’s marriage to count Wichmann the Elder, there was an alliance between the House of Billung and the Ottonian family, which expanded their possessions to the west. In 909, she married Henry, at the time Duke of Saxony and later East-Franconian king, after his first marriage to Hatheburg of Merseburg was cancelled. She gave birth to five mutual children: Otto (912-973), who was crowned the Holy Roman Emperor in 962; Henry (919/22-955), who was appointed Duke of Bavaria in 948; Bruno (925-965), who was elected Archbishop of Cologne in 953 and Duke of Lorraine in 954; Hedwig (d. 965/80), who married the West Frankish duke, Hugh the Great; and Gerberga (d. 968/69), who first married Gilbert, Duke of Lorraine and later the Carolingian King Louis IV of France. 

In 929, Mathilde received her dowry, that Henry gave to her in the so-called "Hausordnung". It consisted of goods in Quedlinburg, Pöhlde, Nordhausen, Grona (near Göttingen), and Duderstadt. During her time as queen, she took an interest in women’s monasteries and is said to have had an influence on her husbands reign by having a strong sense of justice.

After Henry’s death 936 in Memleben, he was buried in Quedlinburg, where Queen Mathilde founded a convent the same year. She lived there during the following years and took care of the family’s memorialization. Thus Quedlinburg Abbey became the most important center of prayer and commemoration of the dead in the East-Franconian Empire. Like in other convents, daughters of noble families where raised in Quedlinburg, to later become Abesses in order to secure the families influence. One of them was her own granddaughter Matilda, daughter of Otto I and Adelheid of Burgundy, to whom she passed on the conducting of the convent in 966, after 30 years of leadership. The younger Mathilde therefore became the first abbess of the convent in Quedlinburg. With her other goods, Queen Mathilde founded further convents, one of them in 947 in Enger . Her last foundation was the convent of Nordhausen in 961.

Mathilde’s handling of her dowry, which she had received from King Henry I previous to his death, was subject to a dispute between her and Otto I during the years 936-946.
Otto made a claim on his mother's possessions, which eventually led to her fleeing into exile. Otto's wife, Queen Eadgyth, is said to have brought about the reconciliation in which Mathilde left her goods and Otto was forgiven for his actions. 

The exact circumstances of this feud are still controversial to this day, but in order to protect her goods, Mathilde acquired papal privileges for all monasteries in eastern Saxony in the period before her death in early 968. However, these efforts were ignored when Theophanu, the wife of Otto II, received Mathilde’s dowry after she died.

After a long illness, Queen Mathilde died on 14 March 968, in the convent of Quedlinburg. She was buried in Quedlinburg Abbey, next to her late husband. Throughout her life, Mathilde was dedicated to charity and her spiritual foundations- as expressed several times in her two hagiographies. A commemorative plaque dedicated to her can be found in the Walhalla memorial near Regensburg, Germany. Mathilde is the patron of the St. Mathilde church in Laatzen (Germany), the St. Mathilde church in Quedlinburg (Germany), the Melkite church in Aleppo (Syria) and the Mathilden-Hospital in Herford (Germany). Her feast day is 14 March.





</doc>
<doc id="20516" url="https://en.wikipedia.org/wiki?curid=20516" title="Monometer">
Monometer

In poetry, a monometre is a line of verse with just one metrical foot.

Monometer can be exemplified by this portion of Robert Herrick's poem "Upon His Departure Hence":
<poem style="margin-left: 2em;">
Thus I
Passe by,
And die:
As one,
Unknown,
And gone.
</poem>




</doc>
<doc id="20517" url="https://en.wikipedia.org/wiki?curid=20517" title="Mazar-i-Sharif">
Mazar-i-Sharif

Mazār-i-Sharīf (Dari and ; ), also called Mazār-e Sharīf, or just Mazar, is the fourth-largest city of Afghanistan, with a 2015 UN–Habitat population estimate 427,600. It is the capital of Balkh province and is linked by highways with Kunduz in the east, Kabul in the southeast, Herat in the southwest and Termez in Uzbekistan in the north. It is about from the Uzbek border. The city also serves as one of the many tourist attractions because of its famous shrines as well as the Islamic and Hellenistic archeological sites. The ancient city of Balkh is also nearby.

The name "Mazar-i-Sharif" means "Tomb of the Prince", a reference to the large, blue-tiled sanctuary and mosque in the center of the city known as the Shrine of Ali or the "Blue Mosque". Some people believe that the tomb of Ali ibn Abi Talib, the cousin and son-in-law of the Islamic prophet Muhammad, is at this mosque in Mazar-i-Sharif, after Ali's remains were transferred to Mazar-i-Sharif as per request of Ja'far as-Sadiq. This is however rejected by other Muslims, as the majority believe he is buried in Najaf, Iraq.

The region around Mazar-i-Sharif has been historically part of Greater Khorasan and was controlled by the Tahirids followed by the Saffarids, Samanids, Ghaznavids, Ghurids, Ilkhanates, Timurids, and Khanate of Bukhara until the mid-18th century when it became part of the Durrani Empire after a friendship treaty was signed between emirs Murad Beg and Ahmad Shah Durrani. Mazar-i-Sharif is also known for the famous Afghan song "Bia ke berem ba Mazar" ("Come let's go to Mazar") by Sarban.

Mazar-i-Sharif is the regional hub of northern Afghanistan, located in close proximity to both Uzbekistan and Tajikistan. It is also home to an international airport. It has the highest percentage of built-up land (91%) of all the Afghan provincial capitals, and it has additional built-up area extending beyond the municipal boundary but forming a part of the larger urban area. It is also the lowest-lying major city in the country, about above sea level. The city was spared of the devastation that occurred in the country's other large cities during the Soviet–Afghan War and subsequent civil war, and is today regarded one of the safest cities in the country.

The Achaemenids controlled the region from the sixth century BCE. Alexander the Great conquered the area but was then incorporated to the Seleucid Empire after his death. The decline of the Seleucids consequently led to the emergence of the Greco-Bactrian kingdom. Around 130 BCE, the Sakas occupied the region and the Greco-Bactrian kingdom fell. The Yuezhi took Mazar-i-Sharif and the surrounding area which led to the creation of the Kushan Empire. The Sasanians subsequently controlled the area after the fall of the Kushans. The Islamic conquests reached Mazar-i-Sharif in 651 CE.

The region around Mazar-i-Sharif has been historically part of Greater Khorasan and was controlled by the Tahirids followed by the Saffarids, Samanids, Ghaznavids, Ghurids, Ilkhanates, Timurids, and Khanate of Bukhara. According to tradition, the city of Mazar-i-Sharif owes its existence to a dream. At the beginning of the 12th century, a local mullah had a dream in which the 7th century Ali bin Abi Talib, cousin and son-in-law of Muhammad, appeared to reveal that he had been secretly buried near the city of Balkh.

The famous Jalal al-Din Rumi was born in this area but like many historical figures his exact location of birth cannot be confirmed. His father Baha' Walad was descended from the first caliph Abu Bakr and was influenced by the ideas of Ahmad Ghazali, brother of the famous philosopher. Baha' Walad's sermons were published and still exist as Divine Sciences (Ma'arif). Rumi completed six books of mystical poetry and tales called Masnavi before he died in 1273.

After conducting researches in the 12th century, the Seljuk sultan Ahmed Sanjar ordered a city and shrine to be built on the location, where it stood until its destruction by Genghis Khan and his Mongol army in the 13th century. Although later rebuilt, Mazar stood in the shadow of its neighbor Balkh. During the nineteenth century, due to the absence of drainage systems and the weak economy of the region, the excess water of this area flooded many acres of the land in the vicinity of residential areas causing a malaria epidemic in the region. Thus the ruler of North Central Afghanistan decided to shift the capital of the city of Mazar-i-Sharif.

The Mazar-i-Sharif means "the noble shrine". This name represents the Blue Mosque which is widely known to be the grave of Ali (Muhammad's son-in-law).

The city along with the region south of the Amu Darya became part of the Durrani Empire in around 1750 after a treaty of friendship was reached between Mohammad Murad Beg and Ahmad Shah Durrani, the founding father of Afghanistan. In the late 1870s, Emir Sher Ali Khan ruled the area from his Tashkurgan Palace in Mazar-i Sharif. This northern part of Afghanistan was un-visited by the British-led Indian forces during the Anglo-Afghan wars of the 19th century.

During the 1980s Soviet–Afghan War, Mazar-i-Sharif was a strategic base for the Soviet Army as they used its airport to launch air strikes on mujahideen rebels. Mazar-i-Sharif was also the main city that linked to Soviet territory in the north, especially the roads leading to the Uzbek Soviet Socialist Republic. As a garrison for the Soviet-backed Afghan Army, the city was under the command of General Abdul Rashid Dostum. Mujahideen militias Hezbe Wahdat and Jamiat-e Islami both attempted to contest the city but were repelled by the Army.

Dostum mutinied against Mohammad Najibullah's government on March 19, 1992, shortly before its collapse, and formed his new party and militia, Junbish-e Milli. The party took over the city the next day. Afterwards Mazar-i-Sharif became the "de facto" capital of a relatively stable and secular proto-state in northern Afghanistan under the rule of Dostum. The city remained peaceful and prosperous, whilst rest of the nation disintegrated and was slowly taken over by fundamentalist Taliban forces. The city was called at the time a "glittering jewel in Afghanistan's battered crown". Money rolled in from foreign donors Russia, Turkey, newly independent Uzbekistan and others, with whom Dostum had established close relations. He printed his own currency for the region and established his own airline.

This peace was shattered in May 1997 when he was betrayed by one of his generals, warlord Abdul Malik Pahlawan who allied himself with the Taliban, forcing him to flee from Mazar-i-Sharif as the Taliban were getting ready to take the city through Pahlawan. Afterwards Pahlawan himself mutinied the Taliban on the deal and it was reported that between May and July 1997 that Pahlawan executed thousands of Taliban members, that he personally did many of the killings by slaughtering the prisoners as a revenge for the 1995 death of Abdul Ali Mazari. "He is widely believed to have been responsible for the brutal massacre of up to 3,000 Taliban prisoners after inviting them into Mazar-i-Sharif." Several of the Taliban escaped the slaughtering and reported what had happened. Meanwhile, Dostum came back and took the city again from Pahlawan.

However the Taliban retaliated in 1998 attacking the city and killing an estimated 8,000 noncombatants (see Battles of Mazar-i-Sharif (1997–98)). At 10 am on 8 August 1998, the Taliban entered the city and for the next two days drove their pickup trucks "up and down the narrow streets of Mazar-i-Sharif shooting to the left and right and killing everything that moved—shop owners, cart pullers, women and children shoppers and even goats and donkeys." More than 8000 noncombatants were reported killed in Mazar-i-Sharif and later in Bamiyan. In addition, the Taliban were criticized for forbidding anyone from burying the corpses for the first six days (contrary to the injunctions of Islam, which demands immediate burial) while the remains rotted in the summer heat and were eaten by dogs. The Taliban also reportedly sought out and massacred members of the Hazara, while in control of Mazar.

Following the September 11 attacks in 2001, Mazar-i-Sharif was the first Afghan city to fall to the U.S.-backed Northern Alliance (United Front). The Taliban's defeat in Mazar quickly turned into a rout from the rest of the north and west of Afghanistan. After the Battle of Mazar-i-Sharif in November 2001, the city was officially captured by forces of the Northern Alliance. They were joined by the United States Special Operations Forces and supported by U.S. Air Force aircraft. As many as 3,000 Taliban fighters who surrendered were reportedly massacred by the Northern Alliance after the battle, and reports also place U.S. ground troops at the scene of the massacre. The Irish documentary "" investigated these allegations. Filmmaker Doran claims that mass graves of thousands of victims were found by United Nations investigators. The Bush administration reportedly blocked investigations into the incident.
The city slowly came under the control of the Karzai administration after 2002, which is led by President Hamid Karzai. The 209th Corps (Shaheen) of the Afghan National Army is based at Mazar-i-Sharif, which provides military assistance to northern Afghanistan. The Afghan Border Police headquarters for the Northern Zone is also located in the city. Despite the security put in place, there are reports of Taliban activities and assassinations of tribal elders. Officials in Mazar-i-Sharif reported that between 20 and 30 Afghan tribal elders have been assassinated in Balkh Province in the last several years. There is no conclusive evidence as to who is behind it but majority of the victims are said to have been associated with the Hezb-i Islami political party.

Small-scale clashes between militias belonging to different commanders persisted throughout 2002, and were the focus of intensive UN peace-brokering and small arms disarmament programme. After some pressure, an office of the Afghan Independent Human Rights Commission opened an office in Mazar in April 2003. There were reports about northern Pashtun civilians being ethnically cleansed by the other groups, mainly by ethnic Tajiks, Hazaras and Uzbeks.

NATO-led peacekeeping forces in and around the city provide assistance to the Afghan government. ISAF Regional Command North, led by Germany, is stationed at Camp Marmal which lies next to Mazar-i-Sharif Airport. Since 2006, Provincial Reconstruction Team Mazar-i-Sharif had unit commanders from Sweden on loan to ISAF. The unit is stationed at Camp Northern Lights which is located west of Camp Marmal. Camp Nidaros, located within Camp Marmal, has soldiers from Latvia and Norway and is led by an ISAF-officer from Norway.

In 2006, the discovery of new Hellenistic remains was announced.

On April 1, 2011, as many as ten foreign employees working for United Nations Assistance Mission in Afghanistan (UNAMA) were killed by angry demonstrators in the city (see "2011 Mazar-i-Sharif attack"). The demonstration was organized in retaliation to pastors Terry Jones and Wayne Sapp's March 21 Qur'an-burning in Florida, United States. Among the dead were five Nepalese, a Norwegian, Romanian and Swedish nationals, two of them were said to be decapitated. Terry Jones, the American pastor who was going to burn Islam's Holy Book, denied his responsibility for incitement. President Barack Obama strongly condemned both the Quran burning, calling it an act of "extreme intolerance and bigotry", and the "outrageous" attacks by protesters, referring to them as "an affront to human decency and dignity." "No religion tolerates the slaughter and beheading of innocent people, and there is no justification for such a dishonorable and deplorable act." U.S. legislators, including Senate Majority Leader Harry Reid, also condemned both the burning and the violence in reaction to it.

By July 2011 violence grew to a record high in the insurgency. In late July 2011, NATO troops also handed control of Mazar-i-Sharif to local forces amid rising security fears just days after it was hit by a deadly bombing. Mazar-i-Sharif is the sixth of seven areas to transition to Afghan control, but critics say the timing is political and there is skepticism over Afghan abilities to combat the Taliban insurgency.

On 10 November 2016, a suicide attacker rammed a truck bomb into the wall of the German consulate in Mazar-i-Sharif. At least four people were killed and more than one hundred others were injured.

On 21 April 2017, a coordinated Taliban attack killed more than 100 people at Camp Shaheen, the Afghan Army base in Mazar-i-Sharif.

In November 2018, it was revealed to the outside world that 40 houses in Qazil Abad, an immediate suburb of Mazar-i-Sharif, used unexploded Soviet Grad surface-to-surface rockets as construction materials. As a result, several people were killed and wounded from a few explosions over the years. These rockets, left behind by the Soviet Army in 1989 at the end of the Soviet–Afghan War, were used as cheap building materials by the poor residents of the village. It was estimated that over 400 rockets were incorporated into the village as wall and ceiling beams, door-stoppers, and even a footbridges used by children. When the outside world discovered this fact, the Danish demining group of the Danish Refugee Council visited the village and, after asking the residents, began demining and repairing the buildings, safely removing and disposing of the rockets through controlled detonation at the border with Uzbekistan.

Mazar-i-Sharif has a cold steppe climate (Köppen climate classification "BSk") with hot summers and cold winters. Precipitation is low and mostly falls between December and April. The climate in Mazar-i-Sharif is very hot during the summer with daily temperatures of over from June to August. The winters are cold with temperatures falling below freezing; it may snow from November through March.

The modern city of Mazar-i Sharif is centred around the Shrine of Ali. Much restored, it is one of Afghanistan's most glorious monuments. Outside Mazar-i Sharif lies the ancient city of Balkh. The city is a centre for the traditional buzkashi sport, and the Blue Mosque is the focus of northern Afghanistan's Nowruz celebration. Although most Muslims believe that the real grave of Ali is found within Imam Ali Mosque in Najaf, Iraq, others still come to Mazar-i-Sharif to pay respect.



The city of Mazar-i-Sharif has a total population of 693,000 (2015), and is the third largest city of Afghanistan in terms of population. It has a total land area of 8,304 Hectares with 77,615 total number of dwellings.

Mazar-i-Sharif is a multiethnic and multilingual society of around 375,000 people. There is no official government report on the exact ethnic make-over but a map appeared in the November 2003 issue of the National Geographic magazine showing Tajiks 20%, Hazaras 55%, Pashtun 0.1%, Turkmen 8%, and Uzbeks 17%. Occasional ethnic violence have been reported in the region in the last decades, mainly between Pashtuns and the other groups. Some latest news reports mentioned assassinations taking place in the area but with no evidence as to who is behind it.

The dominant language in Mazar-i-Sharif is Dari, an eastern variety of Persian, followed by Uzbeki and Pashto.

Mazar-i-Sharif serves as the major trading center in northern Afghanistan. The local economy is dominated by trade, agriculture and Karakul sheep farming. Small-scale oil and gas exploitation have also boosted the city's prospects. It is also the location of consulates of India and Pakistan for trading and political links.

It became the first city in Afghanistan to connect itself by rail with a neighboring country. Rail service from Mazar-i-Sharif to Uzbekistan began in December 2011 and cargo on freight trains arrive at a station near Mazar-i-Sharif Airport, where the goods are reloaded onto trucks or airplanes and sent to their last destinations across Afghanistan.

As of June 2016 Mazar-i-Sharif Airport had direct air connections to Kabul, Mashad, Tehran, and Istanbul.

Highway AH76 links Mazar-i-Sharif to Sheberghan in the west, and Pul-e Khomri and Kabul to the south-east. Roads to the east link it to Kunduz. Roads to the north link it to the Uzbek border town Termez, where it becomes highway M39 going north to Samarkand and Tashkent. Roads to the south link it to Bamiyan Province and the mountainous range of central Afghanistan.







</doc>
<doc id="20518" url="https://en.wikipedia.org/wiki?curid=20518" title="Metaphor">
Metaphor

A metaphor is a figure of speech that, for rhetorical effect, directly refers to one thing by mentioning another. It may provide (or obscure) clarity or identify hidden similarities between two ideas. Metaphors are often compared with other types of figurative language, such as antithesis, hyperbole, metonymy and simile. One of the most commonly cited examples of a metaphor in English literature comes from the "All the world's a stage" monologue from "As You Like It":

<poem>All the world's a stage,
And all the men and women merely players;
They have their exits and their entrances ...
This quotation expresses a metaphor because the world is not literally a stage. By asserting that the world is a stage, Shakespeare uses points of comparison between the world and a stage to convey an understanding about the mechanics of the world and the behavior of the people within it.

According to the linguist Anatoly Liberman, "the use of metaphors is relatively late in the modern European languages; it is, in principle, a post-Renaissance phenomenon". In contrast, in the ancient Hebrew psalms (around 1000 B.C.), one finds already vivid and poetic examples of metaphor such as, "...He is like a tree planted by streams of water, yielding its fruit in season, whose leaf does not wither." At the other extreme, some recent linguistic theories view all language in essence as metaphorical.. 

The English "metaphor" derived from the 16th-century Old French word "métaphore", which comes from the Latin "metaphora", "carrying over", in turn from the Greek μεταφορά ("metaphorá"), "transfer", from μεταφέρω ("metapherō"), "to carry over", "to transfer" and that from μετά ("meta"), "after, with, across" + φέρω ("pherō"), "to bear", "to carry".

"The Philosophy of Rhetoric" (1937) by rhetorician I. A. Richards describes a metaphor as having two parts: the tenor and the vehicle. The tenor is the subject to which attributes are ascribed. The vehicle is the object whose attributes are borrowed. In the previous example, "the world" is compared to a stage, describing it with the attributes of "the stage"; "the world" is the tenor, and "a stage" is the vehicle; "men and women" is the secondary tenor, and "players" is the secondary vehicle.

Other writers employ the general terms 'ground' and 'figure' to denote the tenor and the vehicle. Cognitive linguistics uses the terms 'target' and 'source', respectively.

Psychologist Julian Jaynes coined the terms 'metaphrand' and 'metaphier', plus two new concepts, 'paraphrand' and 'paraphier'.

'Metaphrand' is equivalent to the metaphor-theory terms 'tenor', 'target', and 'ground'. 'Metaphier' is equivalent to the metaphor-theory terms 'vehicle', 'figure', and 'source'. In a simple metaphor, an obvious attribute of the metaphier exactly characterizes the metaphrand (e.g. the ship plowed the seas). With an inexact metaphor, however, a metaphier might have associated attributes or nuances – its paraphiers – that enrich the metaphor because they "project back" to the metaphrand, potentially creating new ideas – the paraphrands – associated thereafter with the metaphrand or even leading to a new metaphor. For example, in the metaphor "Pat is a tornado", the metaphrand is "Pat", the metaphier is "tornado". As metaphier, "tornado" carries paraphiers such as power, storm and wind, counterclockwise motion, and danger, threat, destruction, etc. The metaphoric meaning of "tornado" is inexact: one might understand that 'Pat is powerfully destructive' through the paraphrand of physical and emotional destruction; another person might understand the metaphor as 'Pat can spin out of control'. In the latter case, the paraphier of 'spinning motion' has become the paraphrand 'psychological spin', suggesting an entirely new metaphor for emotional unpredictability, a possibly apt description for a human being hardly applicable to a tornado.
Based on his analysis, Jaynes claims that metaphors not only enhance description, but "increase enormously our powers of perception...and our understanding of [the world], and literally create new objects".

Metaphors are most frequently compared with similes. It is said, for instance, that a metaphor is 'a condensed analogy' or 'analogical fusion' or that they 'operate in a similar fashion' or are 'based on the same mental process' or yet that 'the basic processes of analogy are at work in metaphor'. It is also pointed out that 'a border between metaphor and analogy is fuzzy' and 'the difference between them might be described (metaphorically) as the distance between things being compared'. A metaphor asserts the objects in the comparison are identical on the point of comparison, while a simile merely asserts a similarity through use of words such as "like" or "as". For this reason a common-type metaphor is generally considered more forceful than a simile.

The metaphor category contains these specialized types:

Metaphor is distinct from metonymy, both constituting two fundamental modes of thought. Metaphor works by bringing together concepts from different conceptual domains, whereas metonymy uses one element from a given domain to refer to another closely related element. A metaphor creates new links between otherwise distinct conceptual domains, whereas a metonymy relies on pre-existent links within them.

For example, in the phrase "lands belonging to the crown", the word "crown" is a metonymy because some monarchs do indeed wear a crown, physically. In other words, there is a pre-existent link between "crown" and "monarchy". On the other hand, when Ghil'ad Zuckermann argues that the Israeli language is a "phoenicuckoo cross with some magpie characteristics", he is using a metaphor. There is no physical link between a language and a bird. The reason the metaphors "phoenix" and "cuckoo" are used is that on the one hand hybridic "Israeli" is based on Hebrew, which, like a phoenix, rises from the ashes; and on the other hand, hybridic "Israeli" is based on Yiddish, which like a cuckoo, lays its egg in the nest of another bird, tricking it to believe that it is its own egg. Furthermore, the metaphor "magpie" is employed because, according to Zuckermann, hybridic "Israeli" displays the characteristics of a magpie, "stealing" from languages such as Arabic and English.

A dead metaphor is a metaphor in which the sense of a transferred image has become absent. The phrases "to grasp a concept" and "to gather what you've understood" use physical action as a metaphor for understanding. The audience does not need to visualize the action; dead metaphors normally go unnoticed. Some distinguish between a dead metaphor and a cliché. Others use "dead metaphor" to denote both.

A mixed metaphor is a metaphor that leaps from one identification to a second inconsistent with the first, e.g.:

This form is often used as a parody of metaphor itself:
An extended metaphor, or conceit, sets up a principal subject with several subsidiary subjects or comparisons. In the above quote from "As You Like It", the world is first described as a stage and then the subsidiary subjects men and women are further described in the same context.

An implicit metaphor has no specified tenor, although the vehicle is present. M. H. Abrams offers the following as an example of an implicit metaphor: "That reed was too frail to survive the storm of its sorrows". The reed is the vehicle for the implicit tenor, someone's death, and the "storm" is the vehicle for the person's "sorrows".

Metaphor can serve as a device for persuading an audience of the user's argument or thesis, the so-called rhetorical metaphor.

Aristotle writes in his work the "Rhetoric" that metaphors make learning pleasant: "To learn easily is naturally pleasant to all people, and words signify something, so whatever words create knowledge in us are the pleasantest." When discussing Aristotle's "Rhetoric", Jan Garret stated "metaphor most brings about learning; for when [Homer] calls old age "stubble", he creates understanding and knowledge through the genus, since both old age and stubble are [species of the genus of] things that have lost their bloom." Metaphors, according to Aristotle, have "qualities of the exotic and the fascinating; but at the same time we recognize that strangers do not have the same rights as our fellow citizens".

Educational psychologist Andrew Ortony gives more explicit detail: "Metaphors are necessary as a communicative device because they allow the transfer of coherent chunks of characteristics -- perceptual, cognitive, emotional and experiential -- from a vehicle which is known to a topic which is less so. In so doing they circumvent the problem of specifying one by one each of the often unnameable and innumerable characteristics; they avoid discretizing the perceived continuity of experience and are thus closer to experience and consequently more vivid and memorable."

As a characteristic of speech and writing, metaphors can serve the poetic imagination. This allows Sylvia Plath, in her poem "Cut", to compare the blood issuing from her cut thumb to the running of a million soldiers, "redcoats, every one"; and enabling Robert Frost, in "The Road Not Taken", to compare a life to a journey.

Metaphors can be implied and extended throughout pieces of literature.

Sonja K. Foss characterizes metaphors as "nonliteral comparisons in which a word or phrase from one domain of experience is applied to another domain".
She argues that since reality is mediated by the language we use to describe it, the metaphors we use shape the world and our interactions to it.
The term metaphor is used to describe more basic or general aspects of experience and cognition:

Some theorists have suggested that metaphors are not merely stylistic, but that they are cognitively important as well. In "Metaphors We Live By", George Lakoff and Mark Johnson argue that metaphors are pervasive in everyday life, not just in language, but also in thought and action. A common definition of metaphor can be described as a comparison that shows how two things that are not alike in most ways are similar in another important way. They explain how a metaphor is simply understanding and experiencing one kind of thing in terms of another, called a "conduit metaphor". A speaker can put ideas or objects into containers, and then send them along a conduit to a listener who removes the object from the container to make meaning of it. Thus, communication is something that ideas go into, and the container is separate from the ideas themselves. Lakoff and Johnson give several examples of daily metaphors in use, including "argument is war" and "time is money". Metaphors are widely used in context to describe personal meaning. The authors suggest that communication can be viewed as a machine: "Communication is not what one does with the machine, but is the machine itself."

Cognitive linguists emphasize that metaphors serve to facilitate the understanding of one conceptual domain—typically an abstraction such as "life", "theories" or "ideas"—through expressions that relate to another, more familiar conceptual domain—typically more concrete, such as "journey", "buildings" or "food". For example: we "devour" a book of "raw" facts, try to "digest" them, "stew" over them, let them "simmer on the back-burner", "regurgitate" them in discussions, and "cook" up explanations, hoping they do not seem "half-baked".

Lakoff and Johnson greatly contributed to establishing the importance of conceptual metaphor as a framework for thinking in language, leading scholars to investigate the original ways in which writers used novel metaphors and question the fundamental frameworks of thinking in conceptual metaphors.

From a sociological, cultural, or philosophical perspective, one asks to what extent ideologies maintain and impose conceptual patterns of thought by introducing, supporting, and adapting fundamental patterns of thinking metaphorically. To what extent does the ideology fashion and refashion the idea of the nation as a container with borders? How are enemies and outsiders represented? As diseases? As attackers? How are the metaphoric paths of fate, destiny, history, and progress represented? As the opening of an eternal monumental moment (German fascism)? Or as the path to communism (in Russian or Czech for example)?

Some cognitive scholars have attempted to take on board the idea that different languages have evolved radically different concepts and conceptual metaphors, while others hold to the Sapir-Whorf hypothesis. German philologist Wilhelm von Humboldt contributed significantly to this debate on the relationship between culture, language, and linguistic communities. Humboldt remains, however, relatively unknown in English-speaking nations. Andrew Goatly, in "Washing the Brain", takes on board the dual problem of conceptual metaphor as a framework implicit in the language as a system and the way individuals and ideologies negotiate conceptual metaphors. Neural biological research suggests some metaphors are innate, as demonstrated by reduced metaphorical understanding in psychopathy.

James W. Underhill, in "Creating Worldviews: Ideology, Metaphor & Language" (Edinburgh UP), considers the way individual speech adopts and reinforces certain metaphoric paradigms. This involves a critique of both communist and fascist discourse. Underhill's studies are situated in Czech and German, which allows him to demonstrate the ways individuals are thinking both within and resisting the modes by which ideologies seek to appropriate key concepts such as "the people", "the state", "history", and "struggle".

Though metaphors can be considered to be "in" language, Underhill's chapter on French, English and ethnolinguistics demonstrates that we cannot conceive of language or languages in anything other than metaphoric terms.

Metaphors can map experience between two nonlinguistic realms. Musicologist Leonard B. Meyer demonstrated how purely rhythmic and harmonic events can express human emotions. It is an open question whether synesthesia experiences are a sensory version of metaphor, the "source" domain being the presented stimulus, such as a musical tone, and the target domain, being the experience in another modality, such as color.

Art theorist Robert Vischer argued that when we look at a painting, we "feel ourselves into it" by imagining our body in the posture of a nonhuman or inanimate object in the painting. For example, the painting "The Lonely Tree" by Caspar David Friedrich shows a tree with contorted, barren limbs. Looking at the painting, we imagine our limbs in a similarly contorted and barren shape, evoking a feeling of strain and distress. Nonlinguistic metaphors may be the foundation of our experience of visual and musical art, as well as dance and other art forms.

In historical onomasiology or in historical linguistics, a metaphor is defined as a semantic change based on a similarity in form or function between the original concept and the target concept named by a word.

For example, mouse: "small, gray rodent with a long tail" → "small, gray computer device with a long cord".

Some recent linguistic theories view all language in essence as metaphorical.

Friedrich Nietzsche makes metaphor the conceptual center of his early theory of society in "On Truth and Lies in the Non-Moral Sense". Some sociologists have found his essay useful for thinking about metaphors used in society and for reflecting on their own use of metaphor. Sociologists of religion note the importance of metaphor in religious worldviews, and that it is impossible to think sociologically about religion without metaphor.





</doc>
<doc id="20521" url="https://en.wikipedia.org/wiki?curid=20521" title="Monk (disambiguation)">
Monk (disambiguation)

A monk is a person who practices a strict religious and ascetic lifestyle.

Monk may also refer to:











</doc>
<doc id="20522" url="https://en.wikipedia.org/wiki?curid=20522" title="Maasai Mara">
Maasai Mara

Maasai Mara, also known as Masai Mara, and locally simply as The Mara, is a large national game reserve in Narok, Kenya, contiguous with the Serengeti National Park in Tanzania. It is named in honor of the Maasai people, the ancestral inhabitants of the area, who migrated to the area from the Nile Basin. Their description of the area when looked at from afar: "Mara" means "spotted" in the local Maasai language, due to the many short bushy trees which dot the landscape.

Maasai Mara is one of the most famous and important wildlife conservation and wilderness areas in Africa, world-renowned for its exceptional populations of lion, African leopard, cheetah and African bush elephant. It also hosts the Great Migration, which secured it as one of the Seven Natural Wonders of Africa, and as one of the ten Wonders of the World. 

The Greater Mara ecosystem encompasses areas known as the Maasai Mara National Reserve, the Mara Triangle, and several Maasai Conservancies, including Koiyaki, Lemek, Ol Chorro Oirowua, Mara North, Olkinyei, Siana, Maji Moto, Naikara, Ol Derkesi, Kerinkani, Oloirien, and Kimintet.

When it was originally established in 1961 as a wildlife sanctuary the Mara covered only of the current area, including the Mara Triangle. The area was extended to the east in 1961 to cover and converted to a game reserve. The Narok County Council (NCC) took over management of the reserve at this time. Part of the reserve was given National Reserve status in 1974, and the remaining area of was returned to local communities. An additional were removed from the reserve in 1976, and the park was reduced to in 1984.

In 1994, the TransMara County Council (TMCC) was formed in the western part of the reserve, and control was divided between the new council and the existing Narok County Council. In May 2001, the not-for-profit Mara Conservancy took over management of the Mara Triangle.

The Maasai people make up a community that spans across northern, central and southern Kenya and northern parts of Tanzania. As pastoralists, the community holds the belief that they own all of the cattle in the world. The Maasai rely off of their lands to sustain their cattle, as well as themselves and their families. Prior to the establishment of the reserve as a protected area for the conservation of wildlife and wilderness, the Maasai were forced to move out of their native lands.

Tradition continues to play a major role in the lives of modern day Maasai people, who are known for their tall stature, patterned shukas and beadwork. It is estimated that there are approximately half a million individuals that speak the Maa language and this number includes not only the Maasai but also Samburu and Camus people in Kenya.

The total area under conservation in the Greater Maasai Mara ecosystem amounts to almost . It is the northernmost section of the Mara-Serengeti ecosystem, which covers some in Tanzania and Kenya. It is bounded by the Serengeti Park to the south, the Siria / Oloololo escarpment to the west, and Maasai pastoral ranches to the north, east and west. Rainfall in the ecosystem increases markedly along a southeast–northwest gradient, varies in space and time, and is markedly bimodal. The Sand, Talek River and Mara River are the major rivers draining the reserve. Shrubs and trees fringe most drainage lines and cover hillslopes and hilltops.

The terrain of the reserve is primarily open grassland with seasonal riverlets. In the south-east region are clumps of the distinctive acacia tree. The western border is the Esoit (Siria) Escarpment of the East African Rift, which is a system of rifts some long, from Ethiopia's Red Sea through Kenya, Tanzania, Malawi and into Mozambique. Wildlife tends to be most concentrated here, as the swampy ground means that access to water is always good, while tourist disruption is minimal. The easternmost border is from Nairobi, and hence it is the eastern regions which are most visited by tourists.

The rains are biannual, with two distinct rainy seasons. Local farmers have referred to these as the 'long rains' which last approximately six to eight weeks in April and May and the 'short rains' in November and December which last approximately four weeks. 

Elevation: ;
Rainfall: /month;
Temperature range: 

Wildebeest, topi, zebra, and Thomson's gazelle migrate into and occupy the Mara reserve, from the Serengeti plains to the south and Loita Plains in the pastoral ranches to the north-east, from July to October or later. Herds of all three species are also resident in the reserve.

Hippopotami and crocodiles are found in large groups in the Mara and Talek rivers. Hyenas, cheetahs, jackals, servals and bat-eared foxes can also be found in the reserve. The plains between the Mara River and the Esoit Siria Escarpment are probably the best area for game viewing, in particular regarding lion and cheetah.

Wildebeest are the dominant inhabitants of the Maasai Mara, and their numbers are estimated in the millions. Around July of each year, these animals migrate north from the Serengeti plains in search of fresh pasture, and return to the south around October. The Great Migration is one of the most impressive natural events worldwide, involving some 1,300,000 blue wildebeest, 500,000 Thomson's gazelles, 97,000 Topi, 18,000 common elands, and 200,000 Grant's zebras.

Antelopes can be found, including Grant's gazelles, impalas, duikers and Coke's hartebeests. The plains are also home to the distinctive Masai giraffe. The large roan antelope and the nocturnal bat-eared fox, rarely present elsewhere in Kenya, can be seen within the reserve borders.

More than 470 species of birds have been identified in the park, many of which are migrants, with almost 60 species being raptors. Birds that call this area home for at least part of the year include: vultures, marabou storks, secretary birds, hornbills, crowned cranes, ostriches, long-crested eagles, African pygmy-falcons and the lilac-breasted roller, which is the national bird of Kenya.

The Maasai Mara is administered by the Narok County government. The more visited eastern part of the park known as the Maasai Mara National Reserve is managed by the Narok County Council. The Mara Triangle in the western part is managed by the Trans-Mara county council, which has contracted management to the Mara Conservancy since the early 2000s.

The outer areas are conservancies that are administered by Group Ranch Trusts of the Maasai community, although this approach has been criticised for benefitting just a few powerful individuals rather than the majority of landowners. Although there has been a rise in fencing on private land in recent years, the wildlife roam freely across both the reserve and conservancies.

The Maasai Mara is a major research centre for the spotted hyena. With two field offices in the Mara, the Michigan State University based Kay E. Holekamp Lab studies the behaviour and physiology of this predator, as well as doing comparison studies between large predators in the Mara Triangle and their counterparts in the eastern part of the Mara.

A flow assessment and trans-boundary river basin management plan between Kenya and Tanzania was completed for the river to sustain the ecosystem and the basic needs of 1 million people who depend on its water.

The Mara Predator Project also operates in the Maasai Mara, cataloging and monitoring lion populations throughout the region. Concentrating on the northern conservancies where communities coexist with wildlife, the project aims to identify population trends and responses to changes in land management, human settlements, livestock movements and tourism. 

Since October 2012, the Mara-Meru Cheetah Project has worked in the Mara monitoring cheetah population, estimating population status and dynamics, and evaluating the predator impact and human activity on cheetah behavior and survival. The head of the Project, Elena Chelysheva, was working in 2001-2002 as Assistant Researcher at the Kenya Wildlife Service (KWS) Maasai-Mara Cheetah Conservation Project. At that time, she developed original method of cheetah identification based on visual analysis of the unique spot patterns on front limbs (from toes to shoulder) and hind limbs (from toes to the hip), and spots and rings on the tail. Collected over the years, photographic data allows the project team to trace kinship between generations and build Mara cheetah pedigree. The data collected helps to reveal parental relationship between individuals, survival rate of cubs, cheetah lifespan and personal reproductive history.

The Maasai Mara is one of the most famous safari destinations in Africa. Entry fees are currently US$ 70 for adult non-East African Residents per 24 hours (if staying at a property inside the Reserve) or US$ 80 if outside the reserve, and $40 for children. There are a number of lodges and tented camps catering for tourists inside or bordering the Reserve and within the various separate Conservancies which border the main reserve. However, the main reserve is unfenced even along the border with Serengeti (Tanzania) which means there is free movement of wildlife throughout the ecosystem. 

Although one third of the whole Maasai Mara in the western part if the larger reserve, The Mara Triangle has only two permanent lodges within its boundaries, namely the Mara Serena Lodge and Little Governors Camp (compared to the numerous camps and lodges on the Narok side) and has well maintained, all weather gravel roads. The rangers patrol regularly which means that there is less poaching and excellent game viewing. There is also strict control over vehicle numbers around animal sightings, allowing for a better experience when out on a game drive. Most lodges within the region charge higher rates during the Migration season, although the Maasai Mara is home to prolific wildlife year-round. 

There are several airfields which serve the camps and lodges in the Maasai Mara, including Mara Serena Airstrip, Musiara Airstrip and Keekorok, Kichwa Tembo, Ngerende Airport, Ol Kiombo and Angama Mara Airstrips, and several airlines such as SafariLink and AirKenya fly scheduled services from Nairobi and elsewhere multiple times a day. Helicopter flights over the reserve are limited to a minimum height of 1,500 ft.

Game drives are the most popular activity in the Maasai Mara, but other activities include hot air ballooning, nature walks, photographic safaris and cultural experiences.

The BBC Television show titled ""Big Cat Diary"" was filmed in the Maasai Mara. The show followed the lives of the big cats living in the reserve. The show highlighted scenes from the Reserve's Musiara marsh area and the Leopard Gorge, the Fig Tree Ridge areas and the Mara River, separating the Serengeti and the Maasai Mara.

In 2018, the Angama Foundation, a non-profit affiliated with Angama Mara, one of the Mara's luxury safari camps, launched the Greatest Maasai Mara Photographer of the Year competition, showcasing the Mara as a year-round destination and raise funds for conservation initiatives active in the Mara. The inaugural winner was British photographer Anup Shah. The 2019 winner was Lee-Anne Robertson from South Africa.

A study funded by WWF and conducted by ILRI between 1989 and 2003 monitored hoofed species in the Mara on a monthly basis, and found that losses were as high as 75 percent for giraffes, 80 percent for common warthogs, 76 percent for hartebeest, and 67 percent for impala. The study blames the loss of animals on increased human settlement in and around the reserve. The higher human population density leads to an increased number of livestock grazing in the park and an increase in poaching. The article claims, "The study provides the most detailed evidence to date on the declines in the ungulate (hoofed animals) populations in the Mara and how this phenomenon is linked to the rapid expansion of human populations near the boundaries of the reserve."

The rise of local populations in areas neighbouring the reserve has led to the formation of conservation organisations such as the Mara Elephant Project who aim to ensure the peaceful and prosperous co-existence of humans alongside wildlife. Human wildlife conflict is seen as a leading threat to the reserve as the population continues to grow.




</doc>
<doc id="20523" url="https://en.wikipedia.org/wiki?curid=20523" title="Maasai people">
Maasai people

The Maasai () are a Nilotic ethnic group inhabiting northern, central and southern Kenya and northern Tanzania. They are among the best known local populations internationally due to their residence near the many game parks of the African Great Lakes, and their distinctive customs and dress. The Maasai speak the Maa language (ɔl Maa), a member of the Nilotic language family that is related to the Dinka, Kalenjin and Nuer languages. Except for some elders living in rural areas, most Maasai people speak the official languages of Kenya and Tanzania, Swahili and English. The Maasai population has been reported as numbering 1,189,522 in Kenya in the 2019 census, compared to 377,089 in the 1989 census.

The Tanzanian and Kenyan governments have instituted programs to encourage the Maasai to abandon their traditional semi-nomadic lifestyle, but the people have continued their age-old customs. Many Maasai tribes throughout Tanzania and Kenya welcome visits to their villages to experience their culture, traditions, and lifestyle, in return for a fee.

The Maasai inhabit the African Great Lakes region and arrived via the South Sudan. Most Nilotic speakers in the area, including the Maasai, the Turkana and the Kalenjin, are pastoralists, and are famous for their fearsome reputations as warriors and cattle-rustlers. The Maasai and other groups in East Africa have adopted customs and practices from neighboring Cushitic-speaking groups, including the age set system of social organization, circumcision, and vocabulary terms.

According to their oral history, the Maasai originated from the lower Nile valley north of Lake Turkana (Northwest Kenya) and began migrating south around the 15th century, arriving in a long trunk of land stretching from what is now northern Kenya to what is now central Tanzania between the 17th and late 18th century. Many ethnic groups that had already formed settlements in the region were forcibly displaced by the incoming Maasai, while other, mainly Southern Cushitic groups, were assimilated into Maasai society. The Nilotic ancestors of the Kalenjin likewise absorbed some early Cushitic populations.

The Maasai territory reached its largest size in the mid-19th century, and covered almost all of the Great Rift Valley and adjacent lands from Mount Marsabit in the north to Dodoma in the south. At this time the Maasai, as well as the larger Nilotic group they were part of, raised cattle as far east as the Tanga coast in Tanganyika (now mainland Tanzania). Raiders used spears and shields, but were most feared for throwing clubs (orinka) which could be accurately thrown from up to 70 paces (appx. 100 metres). In 1852, there was a report of a concentration of 800 Maasai warriors on the move in what is now Kenya. In 1857, after having depopulated the "Wakuafi wilderness" in what is now southeastern Kenya, Maasai warriors threatened Mombasa on the Kenyan coast.
Because of this migration, the Maasai are the southernmost Nilotic speakers. The period of expansion was followed by the Maasai "Emutai" of 1883–1902. This period was marked by epidemics of contagious bovine pleuropneumonia, rinderpest (see 1890s African rinderpest epizootic), and smallpox. The estimate first put forward by a German lieutenant in what was then northwest Tanganyika, was that 90% of cattle and half of wild animals perished from rinderpest. German doctors in the same area claimed that "every second" African had a pock-marked face as the result of smallpox. This period coincided with drought. Rains failed completely in 1897 and 1898.

The Austrian explorer Oscar Baumann travelled in Maasai lands between 1891 and 1893, and described the old Maasai settlement in the Ngorongoro Crater in the 1894 book "Durch Massailand zur Nilquelle" ("Through the lands of the Maasai to the source of the Nile"): "There were women wasted to skeletons from whose eyes the madness of starvation glared ... warriors scarcely able to crawl on all fours, and apathetic, languishing elders. Swarms of vultures followed them from high, awaiting their certain victims." By one estimate two-thirds of the Maasai died during this period.

Starting with a 1904 treaty, and followed by another in 1911, Maasai lands in Kenya were reduced by 60% when the British evicted them to make room for settler ranches, subsequently confining them to present-day Samburu, Laikipia, Kajiado and Narok districts. Maasai in Tanganyika (now mainland Tanzania) were displaced from the fertile lands between Mount Meru and Mount Kilimanjaro, and most of the fertile highlands near Ngorongoro in the 1940s. More land was taken to create wildlife reserves and national parks: Amboseli National Park, Nairobi National Park, Maasai Mara, Samburu National Reserve, Lake Nakuru National Park and Tsavo in Kenya; and Lake Manyara, Ngorongoro Conservation Area, Tarangire and Serengeti National Park in what is now Tanzania.

Maasai are pastoralist and have resisted the urging of the Tanzanian and Kenyan governments to adopt a more sedentary lifestyle. They have demanded grazing rights to many of the national parks in both countries.

The Maasai people stood against slavery and lived alongside most wild animals with an aversion to eating game and birds. Maasai land now has East Africa's finest game areas. Maasai society never condoned traffic of human beings, and outsiders looking for people to enslave avoided the Maasai.

Essentially there are twenty-two geographic sectors or sub tribes of the Maasai community, each one having its own customs, appearance, leadership and dialects. These subdivisions are known as 'nations' or 'iloshon' in the Maa language: the Keekonyokie, Damat, Purko, Wuasinkishu, Siria, Laitayiok, Loitai, Kisonko, Matapato, Dalalekutuk, Loodokolani, Kaputiei, Moitanik, Ilkirasha, Samburu, Lchamus, Laikipia, Loitokitoki, Larusa, Salei, Sirinket and Parakuyo.

Recent advances in genetic analyses have helped shed some light on the ethnogenesis of the Maasai people. Genetic genealogy, a tool that uses the genes of modern populations to trace their ethnic and geographic origins, has also helped clarify the possible background of the modern Maasai.

The Maasai's autosomal DNA has been examined in a comprehensive study by Tishkoff et al. (2009) on the genetic affiliations of various populations in Africa. According to the study's authors, the Maasai "have maintained their culture in the face of extensive genetic introgression". Tishkoff et al. also indicate that: "Many Nilo-Saharan-speaking populations in East Africa, such as the Maasai, show multiple cluster assignments from the Nilo-Saharan [...] and Cushitic [...] AACs, in accord with linguistic evidence of repeated Nilotic assimilation of Cushites over the past 3000 years and with the high frequency of a shared East African–specific mutation associated with lactose tolerance."

A Y chromosome study by Wood et al. (2005) tested various Sub-Saharan populations, including 26 Maasai males from Kenya, for paternal lineages. The authors observed haplogroup E1b1b-M35 (not M78) in 35% of the studied Maasai. E1b1b-M35-M78 in 15%, their ancestor with the more northerly Cushitic males, who possess the haplogroup at high frequencies lived more than 13 000 years ago. The second most frequent paternal lineage among the Maasai was Haplogroup A3b2, which is commonly found in Nilotic populations, such as the Alur; it was observed in 27% of Maasai males. The third most frequently observed paternal DNA marker in the Maasai was E1b1a1-M2 (E-P1), which is very common in the Sub-Saharan region; it was found in 12% of the Maasai samples. Haplogroup B-M60 was also observed in 8% of the studied Maasai, which is also found in 30% (16/53) of Southern Sudanese Nilotes.

According to an mtDNA study by Castri et al. (2008), which tested Maasai individuals in Kenya, the maternal lineages found among the Maasai are quite diverse, but similar in overall frequency to that observed in other Nilo-Hamitic populations from the region, such as the Samburu. Most of the tested Maasai belonged to various macro-haplogroup L sub-clades, including L0, L2, L3, L4 and L5. Some maternal gene flow from North and Northeast Africa was also reported, particularly via the presence of mtDNA haplogroup M lineages in about 12.5% of the Maasai samples.

Maasai society is strongly patriarchal in nature, with elder men, sometimes joined by retired elders, deciding most major matters for each Maasai group. A full body of oral law covers many aspects of behavior. Formal capital punishment is unknown, and normally payment in cattle will settle matters. An out-of-court process is also practiced called "amitu", 'to make peace', or "arop", which involves a substantial apology.
The monotheistic Maasai worship a single deity called "Enkai" or "Engai". Engai has a dual nature: Engai Narok (Black God) is benevolent, and Engai Na-nyokie (Red God) is vengeful. There are also two pillars or totems of Maasai society: Oodo Mongi, the Red Cow and Orok Kiteng, the Black Cow with a subdivision of five clans or family trees. The Maasai also have a totemic animal, which is the lion; however, the animal can be killed. The way the Maasai kill the lion differs from trophy hunting as it is used in the rite of passage ceremony. The "Mountain of God", Ol Doinyo Lengai, is located in northernmost Tanzania and can be seen from Lake Natron in southernmost Kenya. The central human figure in the Maasai religious system is the whose roles include shamanistic healing, divination and prophecy, and ensuring success in war or adequate rainfall. Today, they have a political role as well due to the elevation of leaders. Whatever power an individual laibon had was a function of personality rather than position. Many Maasai have also adopted Christianity and Islam. The Maasai are known for their intricate jewelry and for decades, have sold these items to tourists as a business.

A once high infant mortality rate among the Maasai has led to babies not truly being recognized until they reach an age of 3 months "ilapaitin". Educating Maasai women to use clinics and hospitals during pregnancy has enabled more infants to survive. The exception is found in extremely remote areas. For Maasai living a traditional life, the end of life is virtually without ceremony, and the dead are left out for scavengers. A corpse rejected by scavengers is seen as having something wrong with it, and liable to cause social disgrace; therefore, it is not uncommon for bodies to be covered in fat and blood from a slaughtered ox. Burial has in the past been reserved for great chiefs, since it is believed to be harmful to the soil.

Traditional Maasai lifestyle centres around their cattle which constitute their primary source of food. The measure of a man's wealth is in terms of cattle and children. A herd of 50 cattle is respectable, and the more children the better. A man who has plenty of one but not the other is considered to be poor. A Maasai religious belief relates that God gave them all the cattle on earth, leading to the belief that rustling cattle from other tribes is a matter of taking back what is rightfully theirs, a practice that has become much less common.

All of the Maasai's needs for food are met by their cattle. They eat the meat, drink the milk daily, and drink the blood on occasion. Bulls, goats, and lambs are slaughtered for meat on special occasions and for ceremonies. Though the Maasai's entire way of life has historically depended on their cattle, more recently with their cattle dwindling, the Maasai have grown dependent on food such as sorghum, rice, potatoes and cabbage (known to the Maasai as goat leaves).

A traditional pastoral lifestyle has become increasingly difficult due to outside influences of the modern world. Garrett Hardin's article, outlining the "tragedy of the commons", as well as Melville Herskovits' "cattle complex" helped to influence ecologists and policy makers about the harm Maasai pastoralists were causing to savannah rangelands. This concept was later proven false by anthropologists but is still deeply ingrained in the minds of ecologists and Tanzanian officials. This influenced British colonial policy makers in 1951 to remove all Maasai from the Serengeti National Park and relegate them to areas in and around the Ngorongoro Conservation Area (NCA). The plan for the NCA was to put Maasai interests above all else, but this promise was never met. The spread of HIV was rampant.

Due to an increase in Maasai population, loss of cattle populations to disease, and lack of available rangelands because of new park boundaries and the incursion of settlements and farms by other tribes (this is also the chief reason for the decline in wildlife-habitat loss, with the second being poaching), the Maasai were forced to develop new ways of sustaining themselves. Many Maasai began to cultivate maize and other crops to get by, a practice that was culturally viewed negatively. Cultivation was first introduced to the Maasai by displaced WaArusha and WaMeru women who were married to Maasai men; subsequent generations practiced a mixed livelihood. To further complicate their situation, in 1975 the Ngorongoro Conservation Area banned cultivation practices. In order to survive they are forced to participate in Tanzania's monetary economy. They have to sell their animals and traditional medicines in order to buy food. The ban on cultivation was lifted in 1992 and cultivation has again become an important part of Maasai livelihood. Park boundaries and land privatisation has continued to limit grazing area for the Maasai and have forced them to change considerably.

Over the years, many projects have begun to help Maasai tribal leaders find ways to preserve their traditions while also balancing the education needs of their children for the modern world.

The emerging forms of employment among the Maasai people include farming, business (selling of traditional medicine, running of restaurants/shops, buying and selling of minerals, selling milk and milk products by women, embroideries), and wage employment (as security guards/ watchmen, waiters, tourist guides), and others who are engaged in the public and private sectors.

Many Maasai have moved away from the nomadic life to positions in commerce and government. Yet despite the sophisticated urban lifestyle they may lead, many will happily head homewards dressed in designer clothes, only to emerge from the traditional family homestead wearing a shuka (colourful piece of cloth), cow hide sandals and carrying a wooden club (o-rinka) - at ease with themselves.

The central unit of Maasai society is the age-set. Young boys are sent out with the calves and lambs as soon as they can toddle, but childhood for boys is mostly playtime, with the exception of ritual beatings to test courage and endurance. Girls are responsible for chores such as cooking and milking, skills which they learn from their mothers at an early age. Every 15 years or so, a new and individually named generation of Morans or Il-murran (warriors) will be initiated. This involves most boys between 12 and 25, who have reached puberty and are not part of the previous age-set. One rite of passage from boyhood to the status of junior warrior is a circumcision ceremony performed without anaesthetic. In modern times, boys living close to towns with doctors may endure the ceremony in safer conditions, but still without anaesthetic because they must endure the pain that will lead them to manhood. This ritual is typically performed by the elders, who use a sharpened knife and makeshift cattle hide bandages for the procedure. The Maa word for circumcision is emorata. The boy must endure the operation in silence. Expressions of pain bring dishonor, albeit temporarily. Any exclamations can cause a mistake in the delicate and tedious process, which can result in lifelong scarring, dysfunction, and pain. The healing process will take 3–4 months, during which urination is painful and nearly impossible at times, and boys must remain in black clothes for a period of 4–8 months.

During this period, the newly circumcised young men will live in a "manyatta", a "village" built by their mothers. The manyatta has no encircling barricade for protection, emphasizing the warrior role of protecting the community. No inner kraal is built, since warriors neither own cattle nor undertake stock duties. Further rites of passage are required before achieving the status of senior warrior, culminating in the eunoto ceremony, the "coming of age".

When a new generation of warriors is initiated, the existing Il-murran will graduate to become junior elders, who are responsible for political decisions until they in turn become senior elders. This graduation from warrior to junior elder takes place at a large gathering known as Eunoto. The long hair of the former warriors is shaved off; elders must wear their hair short. Warriors are not allowed to have sexual relations with circumcised women, though they may have girlfriends who are uncircumcised girls. At Eunoto, the warriors who managed to abide by this rule are specially recognized.

The warriors spend most of their time now on walkabouts throughout Maasai lands, beyond the confines of their sectional boundaries. They are also much more involved in cattle trading than they used to be, developing and improving basic stock through trades and bartering rather than stealing as in the past.

One myth about the Maasai is that each young man is supposed to kill a lion before he is circumcised. Lion hunting was an activity of the past, but it has been banned in Southeast Africa – yet lions are still hunted when they maul Maasai livestock, and young warriors who engage in traditional lion killing do not face significant consequences. Increasing concern regarding lion populations has given rise to at least one program which promotes accepting compensation when a lion kills livestock, rather than hunting and killing the predator. Nevertheless, killing a lion gives one great value and celebrity status in the community.

Young women also undergo excision ("female circumcision", "female genital mutilation," "emorata") as part of an elaborate rite of passage ritual called "Emuatare," the ceremony that initiates young Maasai girls into adulthood through ritual circumcision and then into early arranged marriages. The Maasai believe that female circumcision is necessary and Maasai men may reject any woman who has not undergone it as either not marriageable or worthy of a much-reduced bride price. In Eastern Africa, uncircumcised women, even those highly educated members of parliament like Linah Kilimo, can be accused of not being mature enough to be taken seriously. To others the practice of female circumcision is known as female genital mutilation, and draws a great deal of criticism from both abroad and many women who have undergone it, such as Maasai activist Agnes Pareyio. It has recently been replaced in some instances by a "cutting with words" ceremony involving singing and dancing in place of the mutilation. However, the practice remains deeply ingrained and valued by the culture. The Maa word for circumcision, "emorata," is used for both female and male genital mutilation. Female genital cutting is illegal in both Kenya and Tanzania. These circumcisions are usually performed by an invited 'practitioner' who is often not Maasai, usually from a Dorobo group. The knives and blades which make the cut are fashioned by blacksmiths, il-kunono, who make their weapons for the Maasai who do not make their own:(knives, short swords (ol alem or simi or seme), spears, etc.). Similarly to the young men, women who will be circumcised wear dark clothing, paint their faces with markings, and then cover their faces on completion of the ceremony.

Married women who become pregnant are excused from all heavy work such as milking and gathering firewood. Sexual relations are also banned and there are specific rules applied to pregnant women.
The Maasai are traditionally polygynous; this is thought to be a long-standing and practical adaptation to high infant and warrior mortality rates. Polyandry is also practiced. However, today this practice is usually abandoned. A woman marries not just her husband but the entire age group. Men are expected to give up their bed to a visiting age-mate guest; however, today this practice is usually abandoned. The woman decides strictly on her own if she will join the visiting male. Any child which may result is the husband's child and his descendant in the patrilineal order of Maasai society. "Kitala", a kind of divorce or refuge, is possible in the house of a wife's father, usually for gross mistreatment of the wife. Repayment of the bride price, custody of children, etc., are mutually agreed upon.

Maasai music traditionally consists of rhythms provided by a chorus of vocalists singing harmonies while a song leader, or olaranyani, sings the melody. The olaranyani is usually the singer who can best sing that song, although several individuals may lead a song. The olaranyani begins by singing a line or title (namba) of a song. The group will respond with one unanimous call in acknowledgment, and the olaranyani will sing a verse over the group's rhythmic throat singing. Each song has its specific namba structure based on call-and-response. Common rhythms are variations of 5/4, 6/4 and 3/4 time signatures. Lyrics follow a typical theme and are often repeated verbatim over time. Neck movements accompany singing. When breathing out the head is leaned forward. The head is tilted back for an inward breath. Overall the effect is one of polyphonic syncopation. Unlike most other African tribes, Maasai widely use drone polyphony.

Women chant lullabies, humming songs, and songs praising their sons. Nambas, the call-and-response pattern, repetition of nonsensical phrases, monophonic melodies, repeated phrases following each verse being sung on a descending scale, and singers responding to their own verses are characteristic of singing by females. When many Maasai women gather together, they sing and dance among themselves.

One exception to the vocal nature of Maasai music is the use of the horn of the Greater Kudu to summon morans for the Eunoto ceremony.

Both singing and dancing sometimes occur around manyattas, and involve flirting. Young men will form a line and chant rhythmically, "Oooooh-yah", with a growl and staccato cough along with the thrust and withdrawal of their lower bodies. Girls stand in front of the men and make the same pelvis lunges while singing a high dying fall of "Oiiiyo..yo" in counterpoint to the men. Although bodies come in close proximity, they do not touch.

Eunoto, the coming of age ceremony of the warrior, can involve ten or more days of singing, dancing and ritual. The warriors of the Il-Oodokilani perform a kind of march-past as well as the adumu, or aigus, sometimes referred as "the jumping dance" by non-Maasai. (Both adumu and aigus are Maa verbs meaning "to jump" with adumu meaning "To jump up and down in a dance".) Warriors are well known for, and often photographed during, this competitive jumping. A circle is formed by the warriors, and one or two at a time will enter the center to begin jumping while maintaining a narrow posture, never letting their heels touch the ground. Members of the group may raise the pitch of their voices based on the height of the jump.
The girlfriends of the moran (intoyie) parade themselves in their most spectacular costumes as part of the eunoto. The mothers of the moran sing and dance in tribute to the courage and daring of their sons.

The piercing and stretching of earlobes is common among the Maasai as with other tribes. Various materials have been used to both pierce and stretch the lobes, including thorns for piercing, twigs, bundles of twigs, stones, the cross section of elephant tusks and empty film canisters. Fewer and fewer Maasai, particularly boys, follow this custom. Women wear various forms of beaded ornaments in both the ear lobe, and smaller piercings at the top of the ear. Amongst Maasai males, circumcision is practiced as a ritual of transition from boyhood to manhood. Women are also circumcised (as described above).

The removal of deciduous canine tooth buds in early childhood is a practice that has been documented in the Maasai of Kenya and Tanzania. There exists a strong belief among the Maasai that diarrhea, vomiting and other febrile illnesses of early childhood are caused by the gingival swelling over the canine region, which is thought to contain 'worms' or 'nylon' teeth. This belief and practice is not unique to the Maasai. In rural Kenya a group of 95 children aged between six months and two years were examined in 1991/92. 87% were found to have undergone the removal of one or more deciduous canine tooth buds. In an older age group (3–7 years of age), 72% of the 111 children examined exhibited missing mandibular or maxillary deciduous canines.

Traditionally, the Maasai diet consisted of raw meat, raw milk, and raw blood from cattle. Note that the Maasai cattle are of the Zebu variety. In the summer of 1935 Dr. Weston A. Price visited the Maasai and reported that according to Dr. Anderson from the local government hospital in Kenya most tribes were disease-free. Many had not a single tooth attacked by dental caries nor a single malformed dental arch. In particular the Maasai had a very low 0.4% of bone caries. He attributed that to their diet consisting of (in order of volume) raw milk, raw blood, raw meat and some vegetables and fruits, although in many villages they do not eat any fruit or vegetables at all. He noted that when available every growing child and every pregnant or lactating woman would receive a daily ration of raw blood. Dr. Weston A. Price also noted the government efforts back in 1935 to turn the Maasai into farmers. An ILCA study (Nestel 1989) states: "Today, the stable diet of the Maasai consists of cow's milk and maize-meal. The former is largely drunk fresh or in sweet tea and the latter is used to make a liquid or solid porridge. The solid porridge is known as ugali and is eaten with milk; unlike the liquid porridge, ugali is not prepared with milk. Animal fats or butter are used in cooking, primarily of porridge, maize, and beans. Butter is also an important infant food."

Studies by the International Livestock Centre for Africa (Bekure et al. 1991) shows a very great change in the diet of the Maasai towards non-livestock products with maize comprising 12–39 percent and sugar 8–13 percent; about one litre of milk is consumed per person daily.
Most of the milk is consumed as fermented milk or buttermilk (a by-product of butter making). Milk consumption figures are very high by any standards. The needs for protein and essential amino acids are more than adequately satisfied. However, the supply of iron, niacin, vitamin C, vitamin A, thiamine and energy are never fully met by a purely milk diet. Due to changing circumstances, especially the seasonal nature of the milk supply and frequent droughts, most pastoralists, including the Maasai, now include substantial amounts of grain in their diets.

The Maasai herd goats and sheep, including the Red Maasai sheep, as well as the more prized cattle. Electrocardiogram tests applied to 400 young adult male Maasai found no evidence whatsoever of heart disease, abnormalities or malfunction. Further study with carbon-14 tracers showed that the average cholesterol level was about 50 percent of that of an average American. These findings were ascribed to the amazing fitness of morans, which was evaluated as "Olympic standard".

Soups are probably the most important use of plants for food by Maasai. "Acacia nilotica" is the most frequently used soup plant. The root or stem bark is boiled in water and the decoction drunk alone or added to soup. The Maasai are fond of taking this as a drug, and is known to make them energetic, aggressive and fearless. Maasai eat soup laced with bitter bark and roots containing cholesterol-lowering saponins; those urban Maasai who don't have access to the bitter plants tend to develop heart disease. Although consumed as snacks, fruits constitute a major part of the food ingested by children and women looking after cattle as well as morans in the wilderness.

The mixing of cattle blood, obtained by nicking the jugular vein, and milk is done to prepare a ritual drink for special celebrations and as nourishment for the sick. However, the inclusion of blood in the traditional diet is waning due to the reduction of livestock numbers. More recently, the Maasai have grown dependent on food produced in other areas such as maize meal, rice, potatoes, cabbage (known to the Maasai as goat leaves) etc. The Maasai who live near crop farmers have engaged in cultivation as their primary mode of subsistence. In these areas, plot sizes are generally not large enough to accommodate herds of animals; thus the Maasai are forced to farm.

As a historically nomadic and then semi-nomadic people, the Maasai have traditionally relied on local, readily available materials and indigenous technology to construct their housing. The traditional Maasai house was in the first instance designed for people on the move and was thus very impermanent in nature. The houses are either somewhat rectangular shaped with extensions or circular, and are constructed by able-bodied women. The structural framework is formed of timber poles fixed directly into the ground and interwoven with a lattice of smaller branches wattle, which is then plastered with a mix of mud, sticks, grass, cow dung, human urine, and ash. The cow dung ensures that the roof is waterproof. The enkaj or engaji is small, measuring about 3 × 5 m and standing only 1.5 m high. Within this space, the family cooks, eats, sleeps, socializes, and stores food, fuel, and other household possessions. Small livestock are also often accommodated within the enkaji. Villages are enclosed in a circular fence (an enkang) built by the men, usually of thorned acacia, a native tree. At night, all cows, goats, and sheep are placed in an enclosure in the centre, safe from wild animals.

Clothing changes by age and location. Young men, for instance, wear black for several months following their circumcision. However, red is a favored colour. Blue, black, striped, and checkered cloth are also worn, as are multicolored African designs. The Maasai began to replace animal skin, calf hides and sheep skin, with commercial cotton cloth in the 1960s.

Shúkà is the Maa word for sheets traditionally worn wrapped around the body. These are typically red, though with some other colors (e.g. blue) and patterns (e.g. plaid). Pink, even with flowers, is not shunned by warriors. One piece garments known as kanga, a Swahili term, are common. Maasai near the coast may wear kikoi, a type of sarong that comes in many different colors and textiles. However, the preferred style is stripes.

Many Maasai in Tanzania wear simple sandals, which were until recently made from cowhides. They are now soled with tire strips or plastic. Both men and women wear wooden bracelets. The Maasai women regularly weave and bead jewellery. This bead work plays an essential part in the ornamentation of their body. Although there are variations in the meaning of the color of the beads, some general meanings for a few colors are: white, peace; blue, water; red, warrior/blood/bravery.

Beadworking, done by women, has a long history among the Maasai, who articulate their identity and position in society through body ornaments and body painting. Before contact with Europeans, the beads were produced mostly from local raw materials. White beads were made from clay, shells, ivory, or bone. Black and blue beads were made from iron, charcoal, seeds, clay, or horn. Red beads came from seeds, woods, gourds, bone, ivory, copper, or brass. When late in the nineteenth century, great quantities of brightly colored European glass beads arrived in Southeast Africa, beadworkers replaced the older beads with the new materials and began to use more elaborate color schemes. Currently, dense, opaque glass beads with no surface decoration and a naturally smooth finish are preferred.

Head shaving is common at many rites of passage, representing the fresh start that will be made as one passes from one to another of life's chapters. Warriors are the only members of the Maasai community to wear long hair, which they weave in thinly braided strands.

Upon reaching the age of 3 "moons", the child is named and the head is shaved clean apart from a tuft of hair, which resembles a cockade, from the nape of the neck to the forehead. The cockade symbolizes the "state of grace" accorded to infants. A woman who has miscarried in a previous pregnancy would position the hair at the front or back of the head, depending on whether she had lost a boy or a girl. This would symbolize the healing of the woman.

Two days before boys are circumcised, their heads are shaved. The young warriors then allow their hair to grow, and spend a great deal of time styling the hair. It is dressed with animal fat and ocher, and parted across the top of the head at ear level. Hair is then plaited: parted into small sections which are divided into two and twisted, first separately then together. Cotton or wool threads may be used to lengthen hair. The plaited hair may hang loose or be gathered together and bound with leather. When warriors go through the "Eunoto", and become elders, their long plaited hair is shaved off.

As males have their heads shaved at the passage from one stage of life to another, a bride to be will have her head shaved, and two rams will be slaughtered in honor of the occasion.





</doc>
<doc id="20524" url="https://en.wikipedia.org/wiki?curid=20524" title="Medieval fortification">
Medieval fortification

Medieval fortification refers to medieval military methods that cover the development of fortification construction and use in Europe, roughly from the fall of the Western Roman Empire to the Renaissance. During this millennium, fortifications changed warfare, and in turn were modified to suit new tactics, weapons and siege techniques.

Towers of medieval castles were usually made of stone or sometimes (but rarely) wood. Often toward the later part of the era they included battlements and arrow loops. Arrow loops were vertical slits in the wall through which archers inside shot arrows at the attackers, but made it extremely difficult for attackers to get many arrows back through at the defenders.

An exact nature of the walls of a medieval town or city would depend on the resources available for building them, the nature of the terrain, and the perceived threat. In northern Europe, early in the period, walls were likely to have been constructed of wood and proofed against small forces. Especially where stone was readily available for building, the wood will have been replaced by stone to a higher or lower standard of security. This would have been the pattern of events in the Five Boroughs of the Danelaw in England.

In many cases, the wall would have had an internal and an external "pomoerium". This was a strip of clear ground immediately adjacent the wall. The word is from the late medieval, derived from the classical Latin "post murum" ("behind the wall").

An external pomoerium, stripped of bushes and building, gave defenders a clear view of what was happening outside and an unobstructed field of shot. An internal pomoerium gave ready access to the rear of the curtain wall to facilitate movement of the garrison to a point of need. By the end of the sixteenth century, the word had developed further in common use, into "pomery".
Also by that time, the medieval walls were no longer secure against a serious threat from an army, as they were not designed to be strong enough to resist cannon fire. They were sometimes rebuilt, as at Berwick on Tweed, or retained for use against thieves and other threats of a lower order. Very elaborate and complex schemes for town defences were developed in the Netherlands and France, but these belong mainly to the post-medieval periods. By 1600, the medieval wall is likely to have been seen more as a platform for displaying hangings and the pomery as a gathering ground for spectators, or as a source of building stone and a site for its use, respectively. However, a few, such as those of Carcassonne and Dubrovnik, survived fairly well and have been restored to a nearly complete state.

Medieval walls that were no longer adequate for defending were succeeded by the star fort. After the invention of the explosive shell, star forts became obsolete as well.

Harbours or some sort of water access was often essential to the construction of medieval fortification. It was a direct route for trading and fortification. Having direct access to a body of water provided a route for resupply in times of war, an additional method of transportation in times of peace, and potential drinking water for a besieged castle or fortification. The concept of rivers or harbours coming directly up to the walls of fortifications was especially used by the English as they constructed castles throughout Wales.
There is evidence that harbours were fortified, with wooden structures in the water creating a semi-circle around the harbour, or jetties, as seen in an artists reconstruction of Hedeby, in Denmark, with an opening for ships to access the land. Usually, these wooden structures would have small bases at either end, creating a 'watch' and defense platform.

Religion was a central part of the lives of medieval soldiers, and churches, chapels, monasteries, and other buildings of religious function were often included within the walls of any fortification, be it temporary or permanent. A place to conduct religious services was usually essential to the morale of the soldiers.

Motte-and-bailey was the prevalent form of castle during 11th and 12th centuries. A courtyard (called a bailey) was protected by a ditch and a palisade (strong timber fence). Often the entrance was protected by a lifting bridge, a drawbridge or a timber gate tower. Inside the bailey were stables, workshops, and a chapel.

The motte was the final refuge in this type of castle. It was a raised earth mound, and varied considerably, with these mounds being 3 metres to 30 metres in height (10 feet to 100 feet), and from in diameter. There was a tower on top of the motte. In most cases, the tower was made of timber, though some were also made of stones. Stone towers were found in natural mounds, as artificial ones were not strong enough to support stone towers. Larger mottes had towers with many rooms, including the great hall. Smaller ones had only a watch tower.

Construction could sometimes take decades. The string of Welsh castles Edward I of England had built were an exception in that he focused much of the resources of his kingdom on their speedy construction. In addition to paid workers, forced levies of labourers put thousands of men on each site and shortened construction to a few years.

Nature could provide very effective defenses for the castle. For this reason many castles were built on larger hills, cliffs, close to rivers, lakes or even caves.

Materials that were used in the building of castles varied through history. Wood was used for most castles until 1066. They were cheap and were quick to construct. The reason wood fell into disuse as a material is that it is quite flammable. Soon stone became more popular.

Stone castles took years to construct depending on the overall size of the castle. Stone was stronger and of course much more expensive than wood. Most stone had to be quarried miles away, and then brought to the building site. But with the invention of the cannon and gunpowder, castles soon lost their power.

Costs for the walls depended on the material used. Wood would cost very little and was quick to build, but was weak. Stone was strong but very expensive and time-consuming to construct.

Manpower in the medieval era in Europe consisted mainly of serfs.

The height of walls varied widely by castle, but were often thick. They were usually topped with crenellation or parapets that offered protection to defenders. Some also featured machicolations (from the French "machicoulis", approximately "neck-crusher") which consisted of openings between a wall and a parapet, formed by corbelling out the latter, allowing defenders to throw stones, boiling water, and so forth, upon assailants below. Some castles featured additional inner walls, as additional fortifications from which to mount a defense if outer walls were breached.

Any entrance through a wall, being an opening, forms an obvious weak point. To be practical, the entryway would have to accommodate supplies being brought through, yet difficult for attackers to breach. For example, passage over ditches or moats would have to be withdrawn to deny attackers. The use of multiple walls or ditches around an entrance would also make it difficult for defenders to use the entrance practically, necessitating better methods of control. Gates came in many forms, from the simple stone buttress and timber blocks, to the massive and imposing stone archways and thick wooden doors most associated with medieval citadels.

A killing field was an area between the main wall and a secondary wall, so when the first wall was breached the attackers would run into the killing field to be confronted by another wall from which soldiers bombarded them. Soldiers would be positioned atop the second wall and armed with any variety of weapons, ranging from bows to crossbows to simple rocks.

A moat was a common addition to medieval fortifications, and the principal purpose was to simply increase the effective height of the walls and to prevent digging under the walls. In many instances, natural water paths were used as moats, and often extended through ditches to surround as much of the fortification as possible. Provided this was not so unnaturally contrived as to allow an attacker to drain the system, it served two defensive purposes. It made approaching the curtain wall of the castle more difficult and the undermining of the wall virtually impossible. To position a castle on a small island was very favorable from a defensive point of view, although it made deliveries of supplies and building materials more cumbersome and expensive.

A keep is a strong central tower which normally forms the heart of a castle. Often the keep is the most defended area of a castle, and as such may form the main habitation area for a noble or lord, or contain important stores such as the armoury or the main well.

Stairs were also constructed to contain trick or stumble steps. These were steps that had different rise height or tread depth from the rest and would cause anyone running up the stairs to stumble or fall, so slowing down the attackers' progress.

A typical exterior wooden door might be made out of two layers of oak planks. The grain of the wood would run vertically on the front layer and horizontally on the back, like a simple form of plywood. The two layers would be held together by iron studs, and the structure might be strengthened and stiffened with iron bands.

The studs themselves were pointed on the front so that attackers would damage their weapons (swords, axes, etc.) while trying to break through.

From the mid-15th century onwards, the power of cannons grew and medieval walls became obsolete as they were too thin to offer any realistic protection against prolonged bombardment. As a consequence of this, medieval walls were often upgraded with the addition of artillery platforms or bastions, and battlements were replaced by thick parapets with embrasures. In many cases, the medieval walls were dismantled and their stonework, which was still valuable as construction material, was reused in the construction of the new fortifications. The resulting space is often seen in old city centers of Europe even to this day, as broader streets often outline where the old wall once stood (evident for example in Prague and Florence, Italy).

The transition between medieval and early modern fortification can be seen in the fortifications of Rhodes in Greece and the fortifications of Famagusta in Cyprus.

Just as modern military engineers enhance field fortifications with obstacles such as barbed wire, medieval engineers used a number of obstacle types including abatis, caltrops, cheval de frise, and trou de loup.





</doc>
<doc id="20527" url="https://en.wikipedia.org/wiki?curid=20527" title="Mark Whitacre">
Mark Whitacre

Mark Edward Whitacre (born May 1, 1957) is an American business executive who came to public attention in 1995 when, as president of the Decatur, Illinois-based BioProducts Division at Archer Daniels Midland (ADM), he became the highest-level corporate executive in U.S. history to become a Federal Bureau of Investigation (FBI) whistleblower. For three years (1992–95), Whitacre acted as a cooperating witness for the FBI, which was investigating ADM for price fixing. In the late 1990s Whitacre was sentenced to 9 years in federal prison for embezzling $9.5 million from ADM at the same time he was assisting the federal price-fixing investigation. 

ADM investigated Whitacre's activities and, upon discovering suspicious activity, requested the FBI investigate Whitacre for embezzlement. As a result of $9.5 million in various frauds, Whitacre lost his whistleblower's immunity, and consequently spent eight and a half years in federal prison. He was released in December 2006. Whitacre is currently the chief science officer and President of Operations at Cypress Systems, a California biotechnology firm.

Whitacre was born on May 1, 1957 and grew up in Morrow, Ohio. He holds B.S. and M.S. degrees from Ohio State University, and earned a Ph.D. in Nutritional Biochemistry from Cornell University (1983).

Whitacre was a Ph.D. scientist at Ralston Purina after he graduated from Cornell University in early 1983. He then was Vice President at Degussa from 1984 to 1989 prior to joining ADM. In late 1989, Whitacre became the President of the BioProducts Division at ADM. In 1992, he was promoted to Corporate Vice President of ADM, as well as being President of the BioProducts Division. On August 9, 1995, Whitacre was terminated for conducting a $9.5 million fraud after ADM learned that he was an FBI informant for three years. Whitacre wore a wire for the FBI assisting in one of the largest price fixing cases in U.S. history, against ADM.

After leaving ADM in August 1995, Whitacre was hired as the CEO of Future Health Technologies (FHT), which soon was renamed Biomar International. He worked at Biomar until his incarceration began during early 1998.

In December 2006, after his release from federal prison, Whitacre was hired by Cypress Systems Inc., a California biotechnology company, as the President of Technology and Business Development. In March 2008, Whitacre was promoted to the company's Chief Operating Officer (COO) and President of operations.

In 1992, during an ADM-initiated investigation of corporate espionage and sabotage, Whitacre informed an FBI agent that he and other ADM executives were involved in an illegal multinational lysine price-fixing scheme. Whitacre's wife pressured him into becoming a whistleblower after she threatened to go to the FBI herself.

Over the next three years, Whitacre worked with FBI agents to collect information and record conversations with both ADM executives and its competitors. ADM ultimately settled federal charges for more than $100 million and paid hundreds of millions of dollars more to plaintiffs and customers ($400 million alone on a high-fructose corn syrup class action case).

A few years into the price-fixing investigation, Whitacre confessed to his FBI handlers that he had been involved with corporate kickbacks and money laundering at ADM. Whitacre was later convicted of embezzling $9 million; some of this criminal activity occurred during the time he was cooperating with the FBI.

Whitacre pled guilty to tax evasion and fraud and was sent to prison on March 4, 1998. Although some officials in the FBI and the Department of Justice opposed the length of the penalty, Whitacre was sentenced to nine years in Federal prison. In December 2006, he was released on good behavior after serving eight and a half years.

In his 2000 book, "The Informant", Kurt Eichenwald, a former "The New York Times" reporter, portrays Whitacre as a complex figure: while working for the FBI as one of the best and most effective undercover cooperating witnesses the U.S. government ever had, Whitacre was simultaneously committing a $9 million white-collar crime. According to Eichenwald, preceding the investigation Whitacre was scammed by a group in Nigeria in an advance fee fraud, and suggests that Whitacre's losses in the scam may have been the initial reason behind his embezzlement activity at ADM.

Eichenwald writes that Whitacre lied and became delusional in a failed attempt to save himself, making the FBI investigation much more difficult. "The Informant" details Whitacre's bizarre behavior, including Whitacre cracking under pressure, increasing his mania, telling the media that FBI agents tried to force him to destroy tapes (a story that Whitacre later recanted), and attempting suicide. Two doctors later diagnosed Whitacre as suffering from bipolar disorder. Eichenwald concludes that Whitacre's sentence was unjust because of his mental instability at the time.

Eichenwald, two prosecutors, an FBI agent, and Mark Whitacre (during his incarceration) were featured on a September 15, 2000, episode of the radio program "This American Life" about the ADM case. Eichenwald referred to Whitacre's sentence as "excessive and a law enforcement failure" because Whitacre never received credit for his substantial cooperation in assisting the government with the massive price-fixing case.

Eichenwald's account of Whitacre was called into question by Alan Guebert.

"The Informant!" is a Warner Bros. feature film released on September 18, 2009. Produced by Jennifer Fox and directed by Steven Soderbergh, the dark comedy/drama film stars Matt Damon as Whitacre. The screenplay by Scott Z. Burns is based on Kurt Eichenwald's book, "The Informant", with most of the filming done in Central Illinois (Blue Mound, Moweaqua and Decatur). In the movie, the character of Whitacre is portrayed as exhibiting bizarre behavior, including delusions, mania, and compulsively lying. It was eventually learned that Whitacre was suffering from bipolar disorder.

In his 2000 book, "Rats In The Grain", attorney James B. Lieber focuses on ADM's price-fixing trial and presents Whitacre as an American hero overpowered by ADM's vast political clout. "Rats In The Grain" presents evidence that the U.S. Department of Justice often subjugated itself to ADM's political power and well-connected attorneys in prosecuting Whitacre. Lieber reveals that, in 1996, "ADM CEO, Mr. Dwayne Andreas, told "The Washington Post" that he had known about Whitacre's frauds for three years" and speculates that Whitacre was fired and turned over to the Federal authorities only after ADM learned he had been working as an FBI mole. If he knew about Whitacre's embezzlement for three years, Lieber asks, why didn't Andreas fire Whitacre immediately? Lieber surmises: "There were only two logical explanations for Andreas' behavior: either he did not think the funds were stolen (in other words, they were approved) or he didn't care." Based on the fact that other ADM executives committed crimes such as financial fraud by a former treasurer and technology thefts by others, Lieber concludes that fraud was well-known and widespread at ADM during the 1990s. Lieber suggests that ADM would have not turned Whitacre over to the authorities if he had not been a mole for the FBI.

Like Eichenwald, Lieber concludes that Whitacre's lengthy prison sentence was excessive and unjust when one takes into account Whitacre's cooperation in the much larger price-fixing case.

Lieber also poses this question: "Where will the government obtain the next Mark Whitacre after potential whistleblowers observe how Whitacre was treated?"

Appeals for Whitacre's full pardon or clemency to the White House were supported by several current and former justice department officials: Dean Paisley, a retired 25-year veteran and former FBI supervisor on the price-fixing case; two other FBI agents involved with the case; a former Attorney General of the United States; one of the former Asst. U.S. Attorneys who prosecuted Whitacre; two prosecutors from the Canadian Department of Justice; several Senators and Congressmen; Cornell University and Ohio State University professors; Major League Baseball Hall of Famer Harmon Killebrew; Chuck Colson; and numerous top executives of corporations.

In 2008, more than ten years after the original conviction, Paisley and two other FBI agents went public with praise for Whitacre. Paisley concluded that "Whitacre's fraud case was minuscule as compared to the ADM case Whitacre cooperated with." "Had it not been for the fraud conviction," Paisley said, "he would be a national hero. Well, he is a national hero." Paisley added, "Without him, the biggest antitrust case we've ever had would not have been." On August 4, 2010, in a Discovery Channel documentary, "Undercover: Operation Harvest King", several FBI agents stated that "Whitacre got a raw deal." In addition, official letters from the FBI in support of a Whitacre pardon were published in Floyd Perry's September 2009 book, "Mark Whitacre: Against All Odds."

A Discovery Channel TV documentary titled "Undercover: Operation Harvest King", which documents Mark Whitacre's role in the ADM price fixing case, aired several times during 2009 and 2010. Discovery Channel interviewed the three FBI agents who handled the Mark Whitacre/ADM case (i.e., Dean Paisley, Brian Shepard and Robert Herndon), along with Mark and Ginger Whitacre. The 45-minute documentary is archived.

Whitacre married his high school sweetheart, Ginger Gilbert, on June 16, 1979. Together they have three children.

Whitacre became a Christian during his incarceration, and since his prison release during December 2006, he has been often interviewed by the Christian community-including the Christian Broadcasting Network (CBN)-about redemption, second chances, and the importance of his faith. Forbes reported that Whitacre was guest speaker at the Quantico FBI Academy during 2011 about second chances, and he was keynote speaker for the 40th Annual NAPSA (Pre Trial Services and U.S. Federal Probation) Conference in 2012 along with Robert F. Kennedy Jr.






</doc>
<doc id="20531" url="https://en.wikipedia.org/wiki?curid=20531" title="Marrakesh Agreement">
Marrakesh Agreement

The Marrakesh Agreement, manifested by the Marrakesh Declaration, was an agreement signed in Marrakesh, Morocco, by 123 nations on 15 April 1994, marking the culmination of the 8-year-long Uruguay Round and establishing the World Trade Organization, which officially came into being on 1 January 1995.

The agreement developed out of the General Agreement on Tariffs and Trade (GATT), supplemented by a number of other agreements on issues including trade in services, sanitary and phytosanitary measures, trade-related aspects of intellectual property and technical barriers to trade. It also established a new, more efficient and legally binding means of dispute resolution. The various agreements which make up the Marrakesh Agreement combine as an indivisible whole; no entity can be party to any one agreement without being party to them all.



</doc>
<doc id="20534" url="https://en.wikipedia.org/wiki?curid=20534" title="Mad">
Mad

Mad, mad, or MAD may refer to:









</doc>
<doc id="20537" url="https://en.wikipedia.org/wiki?curid=20537" title="Mainz">
Mainz

Mainz (; ; , ) is the capital and largest city of Rhineland-Palatinate, Germany. The city is located on the Rhine river at its confluence with the Main river, opposite Wiesbaden on the border with Hesse. Mainz is an independent city with a population of 217,118 (2018) and forms part of the Frankfurt Rhine-Main Metropolitan Region.

Mainz was founded by the Romans in the 1st century BC during the Classical antiquity era, serving as a military fortress on the northernmost frontier of the Roman Empire and as the provincial capital of Germania Superior. Mainz became an important city in the 8th century AD as part of the Holy Roman Empire, becoming the capital of the Electorate of Mainz and seat of the Archbishop-Elector of Mainz, the Primate of Germany. Mainz is famous as the home of Johannes Gutenberg, the inventor of the movable-type printing press, who in the early 1450s manufactured his first books in the city, including the Gutenberg Bible. Mainz was heavily damaged during World War II, with more than 30 air raids destroying about 80 percent of the city's center, including most of the historic buildings. Today, Mainz is a transport hub and a center of wine production.

Mainz is located on the 50th latitude, on the left bank of the river Rhine, opposite the confluence of the Main with the Rhine. The population in the early 2012 was 200,957, an additional 18,619 people maintain a primary residence elsewhere but have a second home in Mainz. The city is part of the Rhein Metro area comprising 5.8 million people. Mainz can easily be reached from Frankfurt International Airport in 25 minutes by commuter railway (Line S8).
Mainz is a river port city as the Rhine which connects with its main tributaries, such as the Neckar, the Main and, later, the Moselle and thereby continental Europe with the Port of Rotterdam and thus the North Sea. Mainz's history and economy are closely tied to its proximity to the Rhine historically handling much of the region's waterborne cargo. Today's huge container port hub allowing trimodal transport is located on the North Side of the town. The river also provides another positive effect, moderating Mainz's climate; making waterfront neighborhoods slightly warmer in winter and cooler in summer.

After the last ice age, sand dunes were deposited in the Rhine valley at what was to become the western edge of the city. The Mainz Sand Dunes area is now a nature reserve with a unique landscape and rare "steppe" vegetation for this area.

While the Mainz legion camp was founded in 13/12 BC on the Kästrich hill, the associated vici and canabae (civilian settlements) were erected in the direction of the Rhine. Historical sources and archaeological findings both prove the importance of the military and civilian Mogontiacum as a port city on the Rhine.

Mainz experiences an oceanic climate (Köppen climate classification "Cfb").

The Roman stronghold or "castrum Mogontiacum", the precursor to Mainz, was founded by the Roman general Drusus perhaps as early as 13/12 BC. As related by Suetonius the existence of "Mogontiacum" is well established by four years later (the account of the death and funeral of Nero Claudius Drusus), though several other theories suggest the site may have been established earlier. Although the city is situated opposite the mouth of the Main, the name of Mainz is not from Main, the similarity being perhaps due to diachronic analogy. Main is from Latin "Menus", the name the Romans used for the river. Linguistic analysis of the many forms that the name "Mainz" has taken on make it clear that it is a simplification of "Mogontiacum". The name appears to be Celtic and ultimately it is. However, it had also become Roman and was selected by them with a special significance. The Roman soldiers defending Gallia had adopted the Gallic god Mogons (Mogounus, Moguns, Mogonino), for the meaning of which etymology offers two basic options: "the great one", similar to Latin magnus, which was used in aggrandizing names such as "Alexander magnus", "Alexander the Great" and "Pompeius magnus", "Pompey the great", or the god of "might" personified as it appears in young servitors of any type whether of noble or ignoble birth.

Mogontiacum was an important military town throughout Roman times, probably due to its strategic position at the confluence of the Main and the Rhine. The town of "Mogontiacum" grew up between the fort and the river. The castrum was the base of Legio XIV "Gemina" and XVI "Gallica" (AD 9–43), XXII "Primigenia", IV "Macedonica" (43–70), I "Adiutrix" (70–88), XXI "Rapax" (70–89), and XIV "Gemina" (70–92), among others. Mainz was also a base of a Roman river fleet, the Classis Germanica. Remains of Roman troop ships (navis lusoria) and a patrol boat from the late 4th century were discovered in 1982/86 and may now be viewed in the "Museum für Antike Schifffahrt". A temple dedicated to Isis Panthea and Magna Mater was discovered in 2000 and is open to the public. The city was the provincial capital of Germania Superior, and had an important funeral monument dedicated to Drusus, to which people made pilgrimages for an annual festival from as far away as Lyon. Among the famous buildings were the largest theatre north of the Alps and a bridge across the Rhine. The city was also the site of the assassination of emperor Severus Alexander in 235.

Alemanni forces under Rando sacked the city in 368. From the last day of 405 or 406, the Siling and Asding Vandals, the Suebi, the Alans, and other Germanic tribes crossed the Rhine, possibly at Mainz. Christian chronicles relate that the bishop, Aureus, was put to death by the Alemannian Crocus. The way was open to the sack of Trier and the invasion of Gaul.

Throughout the changes of time, the Roman castrum never seems to have been permanently abandoned as a military installation, which is a testimony to Roman military judgement. Different structures were built there at different times. The current citadel originated in 1660, but it replaced previous forts. It was used in World War II. One of the sights at the citadel is still the cenotaph raised by legionaries to commemorate their Drusus.

Through a series of incursions during the 4th century Alsace gradually lost its Belgic ethnic character of formerly Germanic tribes among Celts ruled by Romans and became predominantly influenced by the Alamanni. The Romans repeatedly re-asserted control; however, the troops stationed at Mainz became chiefly non-Italic and the emperors had only one or two Italian ancestors in a pedigree that included chiefly peoples of the northern frontier.

The last emperor to station troops serving the western empire at Mainz was Valentinian III (reigned 425–455), who relied heavily on his "Magister militum per Gallias", Flavius Aëtius. By that time the army included large numbers of troops from the major Germanic confederacies along the Rhine, the Alamanni, the Saxons and the Franks. The Franks were an opponent that had risen to power and reputation among the Belgae of the lower Rhine during the 3rd century and repeatedly attempted to extend their influence upstream. In 358 the emperor Julian bought peace by giving them most of Germania Inferior, which they possessed anyway, and imposing service in the Roman army in exchange.

European factions in the time of master Aëtius included Celts, Goths, Franks, Saxons, Alamanni, Huns, Italians, and Alans as well as numerous other minor peoples. Aëtius played them all off against one another in a masterly effort to keep the peace under Roman sovereignty. He used Hunnic troops a number of times. At last a day of reckoning arrived between Aëtius and Attila, both commanding polyglot, multi-ethnic troops. Attila went through Alsace in 451, devastating the country and destroying Mainz and Trier with their Roman garrisons. Shortly after he was thwarted by Flavius Aëtius at the Battle of Châlons, the largest of the ancient world.

Aëtius was not to enjoy the victory long. He was assassinated in 454 by the hand of his employer, who in turn was stabbed to death by friends of Aëtius in 455. As far as the north was concerned this was the effective end of the Roman empire there. After some sanguinary but relatively brief contention a former subordinate of Aëtius, Ricimer, became commander in chief, and was named Patrician. His father was a Suebian; his mother, a princess of the Visigoths. Ricimer did not rule the north directly but set up a client province there, which functioned independently. The capital was at Soissons. Even then its status was equivocal. Many insisted it was the Kingdom of Soissons. which extended across northern France and was ruled in the name of Rome by Aegidius, an ally of emperor Majorian, 457–461, who died about 464. He was succeeded by his son, Syagrius, who was defeated by Clovis in 486.

Previously the first of the Merovingians, Clodio, had been defeated by Aëtius at about 430. His son, Merovaeus, fought on the Roman side against Attila, and his son, Childeric, served in the domain of Soissons. Meanwhile, the Franks were gradually infiltrating and assuming power in this domain from Txxandria (northern Belgium which had been given to them by the Romans to protect as allies). They also moved up the Rhine and created a domain in the region of the former Germania Superior with capital at Cologne. They became known as the Ripuarian Franks as opposed to the Salian Franks. Events moved rapidly in the late 5th century.

After the fall of the Western Roman Empire in 476, the Franks under the rule of Clovis I gained control over western Europe by the year 496. Clovis, son of Childeric, became king of the Salians in 481, ruling from Tournai. In 486 he defeated Syagrius, last governor of the Soissons domain, and took northern France. He extended his reign to Cambrai and Tongeren in 490–491, and repelled the Alamanni in 496. Also in that year he converted to Catholicism from non-Arian Christianity. Clovis annexed the kingdom of Cologne in 508. Thereafter, Mainz, in its strategic position, became one of the bases of the Frankish kingdom. Mainz had sheltered a Christian community long before the conversion of Clovis. His successor Dagobert I reinforced the walls of Mainz and made it one of his seats. A solidus of Theodebert I (534–548) was minted at Mainz.

Charlemagne (768–814), through a succession of wars against other tribes, built a vast Frankian empire in Europe. Mainz from its central location became important to the empire and to Christianity. Meanwhile, language change was gradually working to divide the Franks. Mainz spoke a dialect termed Ripuarian. On the death of Charlemagne, distinctions between France and Germany began to be made. Mainz was not central any longer but was on the border, creating a question of the nationality to which it belonged, which descended into modern times as the question of Alsace-Lorraine.

In the early Middle Ages, Mainz was a centre for the Christianisation of the German and Slavic peoples. The first archbishop in Mainz, Boniface, was killed in 754 while trying to convert the Frisians to Christianity and is buried in Fulda. Boniface held a personal title of archbishop; Mainz became a regular archbishopric see in 781, when Boniface's successor Lullus was granted the pallium by Pope Adrian I. Harald Klak, king of Jutland, his family and followers, were baptized at Mainz in 826, in the abbey of St. Alban's. Other early archbishops of Mainz include Rabanus Maurus, the scholar and author, and Willigis (975–1011), who began construction on the current building of the Mainz Cathedral and founded the monastery of St. Stephan.

From the time of Willigis until the end of the Holy Roman Empire in 1806, the Archbishops of Mainz were archchancellors of the Empire and the most important of the seven Electors of the German emperor. Besides Rome, the diocese of Mainz today is the only diocese in the world with an episcopal see that is called a Holy See ("sancta sedes"). The Archbishops of Mainz traditionally were "primas germaniae", the substitutes of the Pope north of the Alps.

In 1244, Archbishop Siegfried III granted Mainz a city charter, which included the right of the citizens to establish and elect a city council. The city saw a feud between two archbishops in 1461, namely Diether von Isenburg, who was elected Archbishop by the cathedral chapter and supported by the citizens, and Adolf II von Nassau, who had been named archbishop for Mainz by the pope. In 1462, the Archbishop Adolf raided the city of Mainz, plundering and killing 400 inhabitants. At a tribunal, those who had survived lost all their property, which was then divided between those who promised to follow Adolf. Those who would not promise to follow Adolf (amongst them Johannes Gutenberg) were driven out of the town or thrown into prison. The new archbishop revoked the city charter of Mainz and put the city under his direct rule. Ironically, after the death of Adolf II his successor was again Diether von Isenburg, now legally elected by the chapter and named by the Pope.

The Jewish community of Mainz dates to the 10th century CE. It is noted for its religious education. Rabbi Gershom ben Judah (960–1040) taught there, among others. He concentrated on the study of the Talmud, creating a German Jewish tradition. Mainz is also the legendary home of the martyred Rabbi Amnon of Mainz, composer of the Unetanneh Tokef prayer. The Jews of Mainz, Speyer and Worms created a supreme council to set standards in Jewish law and education in the 12th century.

The city of Mainz responded to the Jewish population in a variety of ways, behaving, in a sense, in a bipolar fashion towards them. Sometimes they were allowed freedom and were protected; at other times, they were persecuted. The Jews were expelled in 1012, 1462 (after which they were invited to return), and in 1474. Jews were attacked in 1096 and by mobs in 1283. Outbreaks of the Black Death were usually blamed on the Jews, at which times they were massacred, such as the burning of 11 Jews alive in 1349.

Nowadays the Jewish community is growing rapidly, and a new synagogue by the architect Manuel Herz was constructed in 2010 on the site of the one destroyed by the Nazis on "Kristallnacht" in 1938. The community itself has 1,034 members, according to the Central Council of Jews in Germany, and at least twice as many Jews altogether since many are unaffiliated with Judaism.

During the French Revolution, the French Revolutionary army occupied Mainz in 1792; the Archbishop of Mainz, Friedrich Karl Josef von Erthal, had already fled to Aschaffenburg by the time the French marched in. On 18 March 1793, the Jacobins of Mainz, with other German democrats from about 130 towns in the Rhenish Palatinate, proclaimed the 'Republic of Mainz'. Led by Georg Forster, representatives of the Mainz Republic in Paris requested political affiliation of the Mainz Republic with France, but too late: Prussia was not entirely happy with the idea of a democratic free state on German soil (although the French dominated Mainz was neither free nor democratic). Prussian troops had already occupied the area and besieged Mainz by the end of March 1793. After a siege of 18 weeks, the French troops in Mainz surrendered on 23 July 1793; Prussians occupied the city and ended the Republic of Mainz. It came to the Battle of Mainz in 1795 between Austria and France. Members of the Mainz Jacobin Club were mistreated or imprisoned and punished for treason.

In 1797, the French returned. The army of Napoléon Bonaparte occupied the German territory to the west of the Rhine, and the Treaty of Campo Formio awarded France this entire area. On 17 February 1800, the French "Département du Mont-Tonnerre" was founded here, with Mainz as its capital, the Rhine being the new eastern frontier of la Grande Nation. Austria and Prussia could not but approve this new border with France in 1801. However, after several defeats in Europe during the next years, the weakened Napoléon and his troops had to leave Mainz in May 1814.

In 1816, the part of the former French Département which is known today as Rhenish Hesse () was awarded to the Hesse-Darmstadt, Mainz being the capital of the new Hessian province of Rhenish Hesse. From 1816 to 1866, to the German Confederation Mainz was the most important fortress in the defence against France, and had a strong garrison of Austrian, Prussian and Bavarian troops.

In the afternoon of 18 November 1857, a huge explosion rocked Mainz when the city's powder magazine, the "Pulverturm", exploded. Approximately 150 people were killed and at least 500 injured; 57 buildings were destroyed and a similar number severely damaged in what was to be known as the "Powder Tower Explosion" or "Powder Explosion".

During the Austro-Prussian War in 1866, Mainz was declared a neutral zone. After the founding of the German Empire in 1871, Mainz no longer was as important a stronghold, because in the war of 1870/71 France had lost the territory of Alsace-Lorraine to Germany (which France had occupied piece by piece 1630/1795), and this defined the new border between the two countries.

For centuries the inhabitants of the fortress of Mainz had suffered from a severe shortage of space which led to disease and other inconveniences. In 1872 Mayor Carl Wallau and the council of Mainz persuaded the military government to sign a contract to expand the city. Beginning in 1874, the city of Mainz assimilated the "Gartenfeld", an idyllic area of meadows and fields along the banks of the Rhine to the north of the rampart. The city expansion more than doubled the urban area which allowed Mainz to participate in the industrial revolution which had previously avoided the city for decades.

Eduard Kreyßig was the man who made this happen. Having been the master builder of the city of Mainz since 1865, Kreyßig had the vision for the new part of town, the "Neustadt". He also planned the first sewer system for the old part of town since Roman times and persuaded the city government to relocate the railway line from the Rhine side to the west end of the town. The main station was built from 1882 to 1884 according to the plans of Philipp Johann Berdellé.
The Mainz master builder constructed a number of state-of-the-art public buildings, including the Mainz town hall — which was the largest of its kind in Germany at that time — as well a synagogue, the Rhine harbour and a number of public baths and school buildings. Kreyßig's last work was Christ Church ("Christuskirche"), the largest Protestant church in the city and the first building constructed solely for the use of a Protestant congregation. In 1905 the demolition of the entire circumvallation and the Rheingauwall was taken in hand, according to imperial order of Wilhelm II.

During the German Revolution of 1918 the Mainz Workers' and Soldiers' Council was formed which ran the city from 9 November until the arrival of French troops under the terms of the occupation of the Rhineland agreed in the Armistice. The French occupation was confirmed by the Treaty of Versailles which went into effect 28 June 1919. The Rhineland (in which Mainz is located) was to be a demilitarized zone until 1935 and the French garrison, representing the "Triple Entente", was to stay until reparations were paid.

In 1923 Mainz participated in the Rhineland separatist movement that proclaimed a republic in the Rhineland. It collapsed in 1924. The French withdrew on 30 June 1930. Adolf Hitler became chancellor of Germany in January 1933 and his political opponents, especially those of the Social Democratic Party, were either incarcerated or murdered. Some were able to move away from Mainz in time. One was the political organizer for the SPD, Friedrich Kellner, who went to Laubach, where as the chief justice inspector of the district court he continued his opposition against the Nazis by recording their misdeeds in a 900-page diary.

In March 1933, a detachment from the National Socialist Party in Worms brought the party to Mainz. They hoisted the swastika on all public buildings and began to denounce the Jewish population in the newspapers. In 1936, the Nazis remilitarized the Rhineland with great fanfare, the first move of Nazi Germany's meteoric expansion. The former Triple Entente took no action.

During World War II the citadel at Mainz hosted the Oflag XII-B prisoner of war camp.

The Bishop of Mainz, Albert Stohr, formed an organization to help Jews escape from Germany.

During World War II, more than 30 air raids destroyed about 80 percent of the city's center, including most of the historic buildings. Mainz was captured on 22 March 1945 against uneven German resistance (staunch in some sectors and weak in other parts of the city) by the 90th Infantry Division under William A. McNulty, a formation of the XII Corps under Third Army commanded by General George S. Patton, Jr. Patton used the ancient strategic gateway through "Germania Superior" to cross the Rhine south of Mainz, drive down the Danube towards Czechoslovakia and end the possibility of a Bavarian redoubt crossing the Alps in Austria when the war ended.

From 1945 to 1949, the city was part of the French zone of occupation. When the federal state of Rhineland-Palatinate was founded on 30 August 1946 by the commander of the French army on the French occupation zone Marie Pierre Kœnig, Mainz became capital of the new state. In 1962, the diarist, Friedrich Kellner, returned to spend his last years in Mainz. His life in Mainz, and the impact of his writings, is the subject of the Canadian documentary "".

Following the withdrawal of French forces from Mainz, the United States Army Europe occupied the military bases in Mainz. Today USAREUR only occupies McCulley Barracks in Wackernheim and the Mainz Sand Dunes for training area. Mainz is home to the headquarters of the "Bundeswehr"'s "Landeskommando" Rhineland-Palatinate and other units.

The following list shows the largest minority groups in Mainz :

The destruction caused by the bombing of Mainz during World War II led to the most intense phase of building in the history of the town. During the last war in Germany, more than 30 air raids destroyed about 80 percent of the city's center, including most of the historic buildings. The destructive attack on the afternoon of 27 February 1945 remains the most destructive of all 33 bombings that Mainz has suffered in World War II in the collective memory of most of the population living then. The air raid caused most of the dead and made an already hard-hit city largely leveled.

Nevertheless, the post-war reconstruction took place very slowly. While cities such as Frankfurt had been rebuilt fast by a central authority, only individual efforts were initially successful in rebuilding Mainz. The reason for this was that the French wanted Mainz to expand and to become a model city. Mainz lay within the French-controlled sector of Germany and it was a French architect and town-planner, Marcel Lods, who produced a Le Corbusier-style plan of an ideal architecture. But the very first interest of the inhabitants was the restoration of housing areas. Even after the failure of the model city plans it was the initiative of the French (founding of the Johannes Gutenberg University of Mainz, elevation of Mainz to the state capital of Rhineland-Palatinate, the early resumption of the Mainz carnival) driving the city in a positive development after the war. The City Plan of 1958 by Ernst May allowed a regulated reconstruction for the first time. In 1950, the seat of the government of Rhineland-Palatinate had been transferred to the new Mainz and in 1963 the seat of the new ZDF, notable architects were Adolf Bayer, Richard Jörg and Egon Hartmann. At the time of the two-thousand-years-anniversary in 1962 the city was largely reconstructed. During the 1950s and 1960s the Oberstadt had been extended, Münchfeld and Lerchenberg added as suburbs, the Altstadttangente (intersection of the old town), new neighbourhoods as Westring and Südring contributed to the extension. By 1970 there remained only a few ruins. The new town hall of Mainz had been designed by Arne Jacobsen and finished by Dissing+Weitling. The town used Jacobsens activity for the Danish Novo erecting a new office and warehouse building to contact him. The urban renewal of the old town changed the inner city. In the framework of the preparation of the cathedrals millennium, pedestrian zones were developed around the cathedral, in northern direction to the Neubrunnenplatz and in southern direction across the Leichhof to the Augustinerstraße and Kirschgarten. The 1980s brought the renewal of the façades on the Markt and a new inner-city neighbourhood on the Kästrich. During the 1990s the Kisselberg between Gonsenheim and Bretzenheim, the "Fort Malakoff Center" at the site of the old police barracks, the renewal of the Main Station and the demolition of the first post-war shopping center at the Markt followed by the erection of a new historicising building at the same place.


The city of Mainz is divided into 15 local districts according to the main statute of the city of Mainz. Each local district has a district administration of 13 members and a directly elected mayor, who is the chairman of the district administration. This local council decides on important issues affecting the local area, however, the final decision on new policies is made by the Mainz's municipal council.

In accordance with section 29 paragraph 2 Local Government Act of Rhineland-Palatinate, which refers to municipalities of more than 150,000 inhabitants, the city council has 60 members.

Districts of the town are:
Until 1945, the districts of Bischofsheim (now an independent town), Ginsheim-Gustavsburg (which together are an independent town) belonged to Mainz. The former districts Amöneburg, Kastel, and Kostheim — (in short, "AKK") are now administrated by the city of Wiesbaden (on the north bank of the river). The AKK was separated from Mainz when the Rhine was designated the boundary between the French occupation zone (the later state of Rhineland-Palatinate) and the U.S. occupation zone (Hesse) in 1945.

The coat of arms of Mainz is derived from the coat of arms of the Archbishops of Mainz and features two six-spoked silver wheels connected by a silver cross on a red background.

Mainz is home to a Carnival, the "Mainzer Fassenacht" or "Fastnacht", which has developed since the early 19th century. Carnival in Mainz has its roots in the criticism of social and political injustices under the shelter of cap and bells. Today, the uniforms of many traditional Carnival clubs still imitate and caricature the uniforms of the French and Prussian troops of the past. The height of the carnival season is on Rosenmontag ("rose Monday"), when there is a large parade in Mainz, with more than 500,000 people celebrating in the streets.

The first ever Katholikentag, a festival-like gathering of German Catholics, was held in Mainz in 1848.
Johannes Gutenberg, credited with the invention of the modern printing press with movable type, was born here and died here. Since 1968 the Mainzer Johannisnacht commemorates the person Johannes Gutenberg in his native city. The Mainz University, which was refounded in 1946, is named after Gutenberg; the earlier University of Mainz that dated back to 1477 had been closed down by Napoleon's troops in 1798.

Mainz was one of three important centers of Jewish theology and learning in Central Europe during the Middle Ages. Known collectively as "Shum", the cities of Speyer, Worms and Mainz played a key role in the preservation and propagation of Talmudic scholarship.

The city is the seat of Zweites Deutsches Fernsehen (literally, "Second German Television", ZDF), one of two federal nationwide TV broadcasters. There are also a couple of radio stations based in Mainz.

Other cultural aspects of the city include:


The local football club 1. FSV Mainz 05 has a long history in the German football leagues. Since 2004 it has competed in the Bundesliga (First German soccer league) except a break in second level in 2007–08 season. Mainz is closely associated with renowned coach Jürgen Klopp, who spent the vast majority of his playing career at the club and was also the manager for seven years, leading the club to Bundesliga football for the first time. After leaving Mainz Klopp went on to win two Bundesliga titles and reaching a Champions League final with Borussia Dortmund. In the summer 2011 the club opened its new stadium called Coface Arena, which was later renamed to Opel Arena. Further relevant football clubs are TSV Schott Mainz, SV Gonsenheim, Fontana Finthen, FC Fortuna Mombach and FVgg Mombach 03.

The local wrestling club ASV Mainz 1888 is currently in the top division of team wrestling in Germany, the Bundesliga. In 1973, 1977 and 2012 the ASV Mainz 1888 won the German championship.

In 2007 the Mainz Athletics won the German Men's Championship in baseball.

As a result of the 2008 invasion of Georgia by Russian troops, Mainz acted as a neutral venue for the Georgian Vs Republic of Ireland football game.

The biggest basketball club in the city is the ASC Theresianum Mainz. Its men's team is playing in the Regionalliga and its women's team is playing in the 2.DBBL.

Universitäts-Sportclub Mainz (University Sports Club Mainz) is a German sports club based in Mainz. It was founded on 9 September 1959 by Berno Wischmann primarily for students of the University of Mainz. It is considered one of the most powerful Athletics Sports clubs in Germany. 50 athletes of USC have distinguished themselves in a half-century in club history at Olympic Games, World and European Championships. In particular in the decathlon dominated USC athletes for decades: Already at the European Championships in Budapest in 1966 Mainz won three (Werner von Moltke, Jörg Mattheis and Horst Beyer) all decathlon medals. In the all-time list of the USC, there are nine athletes who have achieved more than 8,000 points – at the head of Siegfried Wentz (8762 points in 1983) and Guido Kratschmer (1980 world record with 8667 points). Most successful athlete of the association is more fighter, sprinter and long jumper Ingrid Becker (Olympic champion in 1968 in the pentathlon and Olympic champion in 1972 in the 4 × 100 Metres Relay and European champion in 1971 in the long jump). Most famous athletes of the present are the sprinter Marion Wagner (world champion in 2001 in the 4 × 100 Metres Relay) and the pole vaulters Carolin Hingst (Eighth of the 2008 Olympics in Beijing) and Anna Battke.

Three world titles adorn the balance of USC Mainz. For the discus thrower Lars Riedel attended (1991 and 1993) and the already mentioned sprinter Marion Wagner (2001). Added to 5 titles at the European Championships, a total of 65 international medals and 260 victories at the German Athletics Championships.

The players of USC's basketball section played from the season 1968/69 to the season 1974/75 in the National Basketball League (BBL) of the German Basketball Federation (DBB). As a finalist to winning the DBB Cup in 1971 USC Mainz played in the 1971–72 FIBA European Cup Winners' Cup against the Italian Cup winners of Fides Napoli.

The Baseball and Softball Club Mainz Athletics is a German baseball and softball club located in the city of Mainz in Rhineland-Palatinate. The Athletics is one of the largest clubs in the Baseball-Bundesliga Süd in terms of membership, claiming to have hundreds of active players. The club has played in the Baseball-Bundesliga for more than two decades, and has won the German Championship in 2007 and 2016.

Mainz is one of the centers of the German wine economy as a center for wine trade and the seat of the state's wine minister. Due to the importance and history of the wine industry for the federal state, Rhineland-Palatinate is the only state to have such a department.

Since 2008, the city is also member of the Great Wine Capitals Global Network (GWC), an association of well-known wineculture-cities of the world.
Many wine traders also work in the town. The sparkling wine producer Kupferberg produced in Mainz-Hechtsheim and even Henkell — now located on the other side of the river Rhine — had been founded once in Mainz. The famous Blue Nun, one of the first branded wines, had been marketed by the family Sichel.

Mainz had been a wine growing region since Roman times and the image of the wine town Mainz is fostered by the tourist center. The "Haus des Deutschen Weines" (English: House of German Wine), is located in beside the theater. The Mainzer Weinmarkt (wine market) is one of the great wine fairs in Germany.

The Schott AG, one of the world's largest glass manufactures, as well as the Werner & Mertz, a large chemical factory, are based in Mainz. Other companies such as IBM, QUINN Plastics, or Novo Nordisk have their German administration in Mainz as well.

Johann-Joseph Krug, founder of France's famous Krug champagne house in 1843, was born in Mainz in 1800.

Mainz is a major transport hub in southern Germany. It is an important component in European distribution, as it has the fifth largest inter-modal port in Germany. The Port of Mainz, now handling mainly containers, is a sizable industrial area to the north of the city, along the banks of the Rhine. In order to open up space along the city's riverfront for residential development, it was shifted further northwards in 2010.

Mainz Central Station or "Mainz Hauptbahnhof", is frequented by 80,000 travelers and visitors each day and is therefore one of the busiest 21 stations in Germany. It is a stop for the S-Bahn line S8 of the Rhein-Main-Verkehrsverbund. Additionally, the Mainbahn line to Frankfurt Hbf starts at the station. It is served by 440 daily local and regional trains (StadtExpress, RE and RB) and 78 long-distance trains (IC, EC and ICE). Intercity-Express lines connect Mainz with Frankfurt (Main), Karlsruhe Hbf, Worms Hauptbahnhof and Koblenz Hauptbahnhof. It is a terminus of the West Rhine Railway and the Mainz–Ludwigshafen railway, as well as the Alzey–Mainz Railway erected by the Hessische Ludwigsbahn in 1871. Access to the East Rhine Railway is provided by the Kaiserbrücke, a railway bridge across the Rhine at the north end of Mainz.

The station is an interchange point for the Mainz tramway network, and an important bus junction for the city and region (RNN, ORN and MVG).

Mainz offers a wide array of bicycle transportation facilities and events, including several miles of on-street bike lanes. The Rheinradweg (Rhine Cycle Route) is an international cycle route, running from the source to the mouth of the Rhine, traversing four countries at a distance of . Another cycling tour runs towards Bingen and further to the Middle Rhine, a UNESCO World Heritage Site (2002).

Mainz is served by Frankfurt Airport, the busiest airport by passenger traffic in Germany by far, the third busiest in Europe and the ninth busiest worldwide in 2009. Located about east of Mainz, it is connected to the city by an S-Bahn line.

The small Mainz Finthen Airport, located just southwest of Mainz, is used by general aviation only. Another airport, Frankfurt-Hahn Airport located about west of Mainz, is served by a few low-cost carriers.


Mainz is twinned with:

Mainz has a number of different names in other languages and dialects. In Latin it is known as ' or ' and, in the local West Middle German dialect, it is "Määnz" or "Meenz". It is known as ' in French, ' in Italian, ' in Spanish, ' in Portuguese, ' in Polish, "Magentza" () in Yiddish, and ' in Czech and Slovakian.

Before the 20th century, Mainz was commonly known in English as "Mentz" or by its French name of "Mayence". It is the namesake of two American cities named Mentz.





</doc>
<doc id="20540" url="https://en.wikipedia.org/wiki?curid=20540" title="Maria Feodorovna (Dagmar of Denmark)">
Maria Feodorovna (Dagmar of Denmark)

Maria Feodorovna (26 November 1847 – 13 October 1928), known before her marriage as Princess Dagmar of Denmark, was a Danish princess and Empress of Russia as spouse of Emperor Alexander III (reigned 1881–1894). She was the second daughter and fourth child of King Christian IX of Denmark and Louise of Hesse-Kassel; her siblings included Queen Alexandra of the United Kingdom, King Frederick VIII of Denmark and King George I of Greece. Her eldest son became the last Russian monarch, Tsar Nicholas II of Russia. She lived for ten years after he and his family were killed.

Princess Marie Sophie Frederikke Dagmar was born at the Yellow Palace in Copenhagen. Her father was Prince Christian of Schleswig-Holstein-Sonderburg-Glücksburg, a member of a relatively impoverished princely cadet line. Her mother was Princess Louise of Hesse-Kassel.

She was baptised as a Lutheran and named after her kinswoman Marie Sophie of Hesse-Kassel, Queen Dowager of Denmark as well as the medieval Danish queen, Dagmar of Bohemia. Her godmother was Queen Caroline Amalie of Denmark. Growing up, she was known by the name Dagmar. Most of her life, she was known as Maria Feodorovna, the name which she took when she converted to Orthodoxy immediately before her 1866 marriage to the future Emperor Alexander III. She was known within her family as "Minnie".

In 1852 Dagmar's father became heir-presumptive to the throne of Denmark, largely due to the succession rights of his wife Louise as niece of King Christian VIII. In 1853, he was given the title Prince of Denmark and he and his family were given an official summer residence, Bernstorff Palace. Dagmar's father became King of Denmark in 1863 upon the death of King Frederick VII.

Due to the brilliant marital alliances of his children, he became known as the "Father-in-law of Europe." Dagmar's eldest brother would succeed his father as King Frederick VIII of Denmark (one of whose sons would be elected as King of Norway). Her elder, and favourite, sister, Alexandra married Albert Edward, the Prince of Wales (the future King Edward VII) in March 1863. Alexandra, along with being queen consort of King Edward VII, was also mother of George V of the United Kingdom, which helps to explain the striking resemblance between their sons Nicholas II and George V. Within months of Alexandra's marriage, Dagmar's second older brother, Wilhelm, was elected as King George I of the Hellenes. Her younger sister was Thyra, Duchess of Cumberland. She also had another younger brother, Valdemar.

During her upbringing, Dagmar, together with her sister Alexandra, was given swimming lessons by the Swedish pioneer of swimming for women, Nancy Edberg; she would later welcome Edberg to Russia, where she came on royal scholarship to hold swimming lessons for women.

The rise of Slavophile ideology in the Russian Empire led Alexander II of Russia to search for a bride for the heir apparent, Tsarevich Nicholas Alexandrovich, in countries other than the German states that had traditionally provided consorts for the tsars. In 1864, Nicholas, or "Nixa" as he was known in his family, went to Denmark where he was betrothed to Dagmar. On 22 April 1865 he died from meningitis. His last wish was that Dagmar would marry his younger brother, the future Alexander III. Dagmar was distraught after her young fiancé's death. She was so heartbroken when she returned to her homeland that her relatives were seriously worried about her health. She had already become emotionally attached to Russia and often thought of the huge, remote country that was to have been her home. The disaster had brought her very close to "Nixa's" parents, and she received a letter from Alexander II in which the Emperor attempted to console her. He told Dagmar in very affectionate terms that he hoped she would still consider herself a member of their family. In June 1866, while on a visit to Copenhagen, the Tsarevich Alexander asked Dagmar for her hand. They had been in her room looking over photographs together.

Dagmar left Copenhagen on 1 September 1866. Hans Christian Andersen, who had occasionally been invited to tell stories to Dagmar and her siblings when they were children, was among the crowd which flocked to the quay in order to see her off. The writer remarked in his diary, "Yesterday, at the quay, while passing me by, she stopped and took me by the hand. My eyes were full of tears. What a poor child! Oh Lord, be kind and merciful to her! They say that there is a brilliant court in Saint Petersburg and the tsar's family is nice; still, she heads for an unfamiliar country, where people are different and religion is different and where she will have none of her former acquaintances by her side."

Dagmar was warmly welcomed in Kronstadt by Grand Duke Constantine Nikolaevich of Russia and escorted to St. Petersburg, where she was greeted by her future mother-in-law and sister-in-law on 24 September. On the 29th, she made her formal entry in to the Russian capital dressed in a Russian national costume in blue and gold and traveled with the Empress to the Winter Palace where she was introduced to the Russian public on a balcony. Catherine Radziwill described the occasion: ”rarely has a foreign princess been greeted with such enthusiasm… from the moment she set foot on Russian soil, succeeded in winning to herself all hearts. Her smile, the delightful way she had of bowing to the crowds…, laid immediately the foundation of …popularity” 

She converted to Orthodoxy and became Grand Duchess Maria Feodorovna of Russia. The lavish wedding took place on in the Imperial Chapel of the Winter Palace in Saint Petersburg. Financial constraints had prevented her parents from attending the wedding, and in their stead, they sent her brother, Crown Prince Frederick. Her brother-in-law, the Prince of Wales, had also travelled to Saint Petersburg for the ceremony; pregnancy had prevented the Princess of Wales from attending. After the wedding night, Alexander wrote in his diary, "I took off my slippers and my silver embroidered robe and felt the body of my beloved next to mine... How I felt then, I do not wish to describe here. Afterwards we talked for a long time." After the many wedding parties were over the newlyweds moved into the Anichkov Palace in Saint Petersburg where they were to live for the next 15 years, when they were not taking extended holidays at their summer villa Livadia in the Crimean Peninsula.

Maria Feodorovna was beautiful and well received by her Russian peers. Early on, she made it a priority to learn the Russian language and to try to understand the Russian people. She rarely interfered with politics, preferring to devote her time and energies to her family, charities, and the more social side of her position. She had also seen the student protests of Kiev and St. Petersburg in the 1860s, and when police were beating students, the students cheered on Maria Feodorovna to which she replied, "They were quite loyal, they cheered me. Why do you allow the police to treat them so brutally?" Her one exception to official politics was her militant anti-German sentiment because of the annexation of Danish territories by Prussia in 1864, a sentiment also expressed by her sister, Alexandra. Prince Gorchakov remarked about that policy that 'it is our belief, that Germany will not forget that both in Russia and in England [sic] a Danish Princess has her foot on the steps of the throne". Maria Feodorovna suffered a miscarriage in 1866 in Denmark while she was horseback riding.

On 18 May 1868, Maria Feodorovna gave birth to her eldest son, Nicholas. Her next son, Alexander Alexandrovich, born in 1869, died from meningitis in infancy. She would bear Alexander four more children who reached adulthood: George (b. 1871), Xenia (b. 1875), Michael (b. 1878), and Olga (b. 1882). As a mother, she doted on and was quite possessive of her sons. She had a more distant relationship with her daughters.

In 1873, Maria, Alexander, and their two eldest sons made a journey to the United Kingdom. The imperial couple and their children were entertained at Marlborough House by the Prince and Princess of Wales. The royal sisters Maria and Alexandra delighted London society by dressing alike at social gatherings. The following year, Maria and Alexander welcomed the Prince and Princess of Wales to St. Petersburg; they had come for the wedding of the Prince's younger brother, Alfred, to Grand Duchess Maria Alexandrovna, daughter of Tsar Alexander II and the sister of the tsarevich.

On the morning of 13 March 1881, her father-in-law Alexander II of Russia, aged 62, was killed by a bomb on the way back to the Winter Palace from a military parade. In her diary, Maria later described how the wounded, still living Emperor was taken to the palace: "His legs were crushed terribly and ripped open to the knee; a bleeding mass, with half a boot on the right foot, and only the sole of the foot remaining on the left." Alexander II died a few hours later. Although the people were not enamoured of the new emperor, they adored Russia's new empress. As Maria's contemporaries said of her: "She is truly an empress." She was not altogether pleased with her new status. In her diary she wrote, "Our happiest and serenest times are now over. My peace and calm are gone, for now I will only ever be able to worry about Sasha." Despite being haunted by her father-in-law's gruesome death and her anxiety over the safety of her husband, at Alexander II's funeral, she was at least afforded the comfort of the presence of her brother-in-law and favourite sister, the Prince and Princess of Wales, the latter of whom, despite her husband's reluctance and Queen Victoria's objections, stayed on in Russia with Maria for several weeks after the funeral.

Alexander and Maria were crowned at the Assumption Cathedral in the Kremlin in Moscow on 27 May 1883. Just before the coronation, a major conspiracy had been uncovered, which cast a pall over the celebration. Nevertheless, over 8000 guests attended the splendid ceremony. Because of the many threats against Maria and Alexander III, the head of the security police, General Cherevin, shortly after the coronation urged the Tsar and his family to relocate to Gatchina Palace, a more secure location 50 kilometres outside St. Petersburg. The huge palace had 900 rooms and was built by Catherine the Great. The Romanovs heeded the advice. Maria and Alexander III lived at Gatchina for 13 years, and it was here that their five surviving children grew up. Under heavy guard, Alexander III and Maria made periodic trips from Gatchina to the capital to take part in official events.

Maria is described as a success in her social role as Empress, loved to dance at the balls of high society and became a popular socialite and hostess of the Imperial balls; her daughter Olga commented, “Court life had to run in splendor, and there my mother played her part without a single false step.”, and a contemporary remarked on her success: “of the long gallery of Tsarinas who have sat in state in the Kremlin or paced in the Winter Palace, Marie Feodorovna was perhaps the most brilliant”. She longed for the balls and gatherings in the Winter Palace. These also occurred at Gatchina. Alexander used to enjoy joining in with the musicians, although he would end up sending them off one by one. When that happened, Maria knew the party was over.
As tsarevna, and then as tsarina, Maria Feodorovna had something of a social rivalry with the popular Grand Duchess Marie Pavlovna, wife of her Russian brother-in-law, Grand Duke Vladimir. This rivalry had echoed the one shared by their husbands, and served to exacerbate the rift within the family. While Maria Feodorovna knew better than to publicly criticise both the Grand Duke and Duchess in public, Marie Pavlovna had earned the caustic epithet of "Empress Vladimir" from the tsarina.

Nearly each summer, Maria, Alexander and their children would make an annual trip to Denmark, where her parents, King Christian IX and Queen Louise, hosted family reunions. Maria's brother, King George I, and his wife, Queen Olga, would come up from Athens with their children, and the Princess of Wales, often without her husband, would come with some of her children from the United Kingdom. In contrast to the tight security observed in Russia, the tsar, tsarina and their children relished the relative freedom that they could enjoy at Bernstorff and Fredensborg. The annual family meetings of monarchs in Denmark was regarded as suspicious in Europe, where many assumed they secretly discussed state affairs. Bismarck nicknamed Fredensborg “Europe’s Whispering Gallery” and accused Queen Louise of plotting against him with her children. Maria also had a good relationship with the majority of her in-laws, and was often asked to act as a mediator between them and the tsar. In the words of her daughter Olga: “She proved herself extremely tactful with her in-laws, which was no easy task”.
During Alexander III's reign, the monarchy's opponents quickly disappeared underground. A group of students had been planning to assassinate Alexander III on the sixth anniversary of his father's death at the Peter and Paul Cathedral in St. Petersburg. The plotters had stuffed hollowed-out books with dynamite, which they intended to throw at the Tsar when he arrived at the cathedral. However, the Russian secret police uncovered the plot before it could be carried out. Five students were hanged in 1887; amongst them was Aleksandr Ulyanov, older brother of Vladimir Lenin.

The biggest threat to the lives of the tsar and his family, however, came not from terrorists, but from a derailment of the imperial train in the fall of 1888. Maria and her family had been at lunch in the dining car when the train jumped the tracks and slid down an embankment, causing the roof of the dining car to nearly cave in on them.

When Maria's eldest sister Alexandra visited Gatchina in July 1894, she was surprised to see how weak her brother-in-law Alexander III had become. At the time Maria had long known that he was ill and did not have long left. She now turned her attention to her eldest son, the future Nicholas II, for it was on him that both her personal future and the future of the dynasty now depended.

Nicholas had long had his heart set on marrying Princess Alix of Hesse-Darmstadt, a favourite grandchild of Queen Victoria. Despite the fact that she was their godchild, neither Alexander III nor Maria approved of the match. Nicholas summed up the situation as follows: "I wish to move in one direction, and it is clear that Mama wishes me to move in another – my dream is to one day marry Alix." Maria and Alexander found Alix shy and somewhat peculiar. They were also concerned that the young Princess was not possessed of the right character to be Empress of Russia. Nicholas's parents had known Alix as a child and formed the impression that she was hysterical and unbalanced, which may have been due to the loss of her mother and youngest sister, Marie, to diphtheria when she was just six. It was only when Alexander III's health was beginning to fail that they reluctantly gave permission for Nicholas to propose.

On 1 November 1894, Alexander III died aged just 49 at Livadia. In her diary Maria wrote, "I am utterly heartbroken and despondent, but when I saw the blissful smile and the peace in his face that came after, it gave me strength." Two days later, the Prince and Princess of Wales arrived at Livadia from London. While the Prince of Wales took it upon himself to involve himself in the preparations for the funeral, the Princess of Wales spent her time comforting grieving Maria, including praying with her and sleeping at her bedside. 
Maria Feodorovna's birthday was a week after the funeral, and as it was a day in which court mourning could be somewhat relaxed, Nicholas used the day to marry Alix of Hesse-Darmstadt, who took the name Alexandra Feodorovna.

Once the death of Alexander III had receded, Maria again took a brighter view of the future. "Everything will be all right", as she said. Maria continued to live in the Anichkov Palace in St. Petersburg and at Gatchina Palace. In May 1896, she travelled to Moscow for the coronation of Nicholas and Alexandra.

As a new Imperial Train was constructed for Nicholas II in time for his coronation, Alexander III's "Temporary Imperial Train" (composed of the cars that had survived the Borki disaster and a few converted standard passenger cars) was transferred to the Empress Dowager's personal use.

During the first years of her son's reign, Maria often acted as the political adviser to the Tsar. Uncertain of his own ability and aware of her connections and knowledge, Tsar Nicholas II often told the ministers that he would ask her advice before making decisions, and the ministers sometimes suggested this themselves. It was reportedly on her advice that Nicholas initially kept his father's ministers. Maria herself estimated that her son was of a weak character and that it was better that he was influenced by her than someone worse. Her daughter Olga remarked upon her influence: “she had never before taken the least interest … now she felt it was her duty. Her personality was magnetic and her zest of activity was incredible. She had her finger on every educational pulse in the empire. She would work her secretaries to shreds, but she did not spare herself. Even when bored in committee she never looked bored. Her manner and, above all, her tact conquered everybody”. After the death of her spouse, Maria came to be convinced that Russia needed reforms to avoid a revolution. According to courtier Paul Benckendorff there was a scene when Maria asked her son not to appoint the conservative Wahl as minister for internal affairs: “during which one [the empress dowager] almost threw herself at his [the tsar's] knees' begging him not to make this appointment and to choose someone who could make concessions. She said that if Nicholas did not agree, she would 'leave for Denmark, and then without me here let them twist your head around'". Nicholas did appoint her favored candidate, and she reportedly told her favoured candidate the liberal reformist Peter Sviatopolk-Mirsky to accept by saying: “You must fulfil my son’s wish; If you do, I will give you a kiss”. After the birth of a son to the tsar the same year, however, Nicholas II replaced his mother as his political confidant and adviser with his wife, Empress Alexandra.

Maria Feodorovna's grandson-in-law, Prince Felix Yusupov, noted that she had great influence in the Romanov family. Sergei Witte praised her tact and diplomatic skill. Nevertheless, despite her social tact, she did not get along well with her daughter-in-law, Tsarina Alexandra, holding her responsible for many of the woes that beset her son Nicholas and the Russian Empire in general. She was appalled with Alexandra's inability to win favour with public, and also that she did not give birth to an heir until almost ten years after her marriage, after bearing four daughters. The fact that Russian court custom dictated that an empress dowager took precedence over an empress consort, combined with the possessiveness that Maria had of her sons, and her jealousy of Empress Alexandra only served to exacerbate tensions between mother-in-law and daughter-in-law. Sophie Buxhoeveden remarked of this conflict: “Without actually clashing they seemed fundamentally unable … to understand one another”, and her daughter Olga commented: “they had tried to understand each other and failed. They were utterly different in character, habits and outlook”. Maria was sociable and a good dancer, with an ability to ingratiate herself with people, while Alexandra, though intelligent and beautiful, was very shy and closed herself off from the Russian people.

By the turn of the twentieth century, Maria was spending increasing time abroad. In 1906, following the death of their father, King Christian IX, she and her sister, Alexandra, who had become queen-consort of the United Kingdom in 1901, purchased the villa of Hvidøre. The following year, a change in political circumstances allowed Maria Feodorovna to be welcomed to England by King Edward VII and Queen Alexandra, Maria's first visit to England since 1873. Following a visit in early 1908, Maria Feodorovna was present at her brother-in-law and sister's visit to Russia that summer. A little under two years later, Maria Feodorovna travelled to England yet again, this time for the funeral of her brother-in-law, King Edward VII, in May 1910. During her nearly three-month visit to England in 1910, Maria Feodorovna attempted, unsuccessfully, to get her sister, now Queen Dowager Alexandra, to claim a position of precedence over her daughter-in-law, Queen Mary.

Empress Maria Feodorovna, the mistress of Langinkoski retreat, was also otherwise a known friend of Finland. During the first russification period, she tried to have her son halt the constraining of the grand principality's autonomy and to recall the unpopular Governor-General Bobrikov from Finland to some other position in Russia itself. During the second russification period, at the start of the First World War, the Empress Dowager, travelling by her special train through Finland to Saint Petersburg, expressed her continued disapprobation for the russification of Finland by having an orchestra of a welcoming committee play the March of the Pori Regiment and the Finnish national anthem "Maamme", which at the time were under the explicit ban from Franz Albert Seyn, the Governor-General of Finland.

In 1899, Maria's second son, George, died of tuberculosis in the Caucasus. During the funeral, she kept her composure, but at the end of the service, she ran from the church clutching her son's top hat that been atop the coffin and collapsed in her carriage sobbing. Two years later, according to her daughter, Grand Duchess Olga, she arranged Olga's disastrous marriage to Peter, Duke of Oldenburg. For years Nicholas refused to grant his unhappy sister a divorce, only relenting in 1916 in the midst of the War. When Olga attempted to contract a morganatic marriage with Nikolai Kulikovsky, Maria Feodorovna and the tsar tried to dissuade her, yet, they did not protest too vehemently. Indeed, Maria Feodorovna was one of the few people who attended the wedding in November 1916. In 1912, Maria faced trouble with her youngest son, when he secretly married his mistress, much to the outrage and scandal of both Maria Feodorovna and Nicholas.

Maria Feodorovna disliked Rasputin, whom she regarded to be a dangerous charlatan, and when his activities damaged the reputation of the Tsar, she asked the Tsar and Empress to remove him from their vicinity. When the Tsar remained silent and Empress Alexandra answered and refused for both of them, Maria assumed the empress was the true regent and that she also lacked the capability for such a position: “My poor daughter-in-law does not perceive that she is ruining the dynasty and herself. She sincerely believes in the holiness of an adventurer, and we are powerless to ward off the misfortune, which is sure to come." When the Tsar dismissed minister Vladimir Kokovtsov in February 1914 on the advice of Alexandra, Maria again reproached her son, who answered in such a way that she became even more convinced that Alexandra was the real ruler of Russia, and she called upon Kokovtsov and said to him: “My daughter-in-law does not like me; she thinks that I am jealous of her power. She does not perceive that my one aspiration is to see my son happy. Yet I see we are nearing some kind of catastrophe and the Tsar listens to no one but flatterers… Why do you not tell the Tsar everything that you think and know… if it is not already too late”.

In May 1914 Maria Feodorovna travelled to England to visit her sister. While she was in London, World War I broke out (July 1914), forcing her to hurry home to Russia. In Berlin the German authorities prevented her train from continuing toward the Russian border. Instead she had to return to Russia by way of (neutral) Denmark and Finland. Upon her return in August, she took up residence at Yelagin Palace, which was closer to St. Petersburg (renamed Petrograd in August 1914) than Gatchina. During the war she served as president of Russia's Red Cross. As she had done a decade earlier in the Russo-Japanese War of 1904-1905, she also financed a sanitary train.

During the war, there was great concern within the imperial house about the influence Empress Alexandra had upon state affairs through the Tsar, and the influence Grigori Rasputin was believed to have upon her, as it was considered to provoke the public and endanger the safety of the imperial throne and the survival of the monarchy. 
On behalf of the imperial relatives of the Tsar, both the Empress's sister Grand Duchess Elizabeth Feodorovna and her cousin Grand Duchess Victoria Feodorovna had been selected to mediate and ask Empress Alexandra to banish Rasputin from court to protect her and the throne's reputation, but without success. In parallel, several of the Grand Dukes had tried to intervene with the Tsar, but with no more success.

During this conflict of 1916-1917, Grand Duchess Maria Pavlovna reportedly planned a coup d'état to depose the Tsar with the help of four regiments of the imperial guard which were to invade the Alexander Palace, force the Tsar to abdicate and replace him with his underage son under the regency of her son Grand Duke Kirill.

There are documents that support the fact that in this critical situation, Maria Feodorovna was involved in a planned coup d'état to depose her son from the throne in order to save the monarchy. The plan was reportedly for Maria to make a final ultimatum to the Tsar to banish Rasputin unless he wished for her to leave the capital, which would be the signal to unleash the coup. Exactly how she planned to replace her son is unconfirmed, but two versions are available: first, that Grand Duke Paul Alexandrovich of Russia would take power in her name, and that she herself would thereafter become ruling empress; the other version further claims that Grand Duke Paul Alexandrovich of Russia would replace the Tsar with his son, the heir to the throne, Maria's grandson Alexey, upon which Maria and Paul Alexandrovich would share power as regents during his minority. Maria was asked to make her appeal to the Tsar after Empress Alexandra had asked the Tsar to dismiss minister Polianov. Initially, she refused to make the appeal, and her sister-in-law Grand Duchess Maria Pavlovna stated to the French Ambassador: “It’s not want of courage or inclination that keeps her back. It's better that she don’t. She’s too outspoken and imperious. The moment she starts to lecture her son, her feelings run away with her; she sometimes says the exact opposite of what she should; she annoys and humiliates him. Then he stands on his dignity and reminds his mother he is the emperor. They leave each other in a rage.” Eventually, she was however convinced to make the appeal. Reportedly, Empress Alexandra was informed about the planned coup, and when Maria Feodorovna made the ultimatum to the Tsar, the empress convinced him to order his mother to leave the capital. Consequently, the Empress Dowager left Petrograd to live in the Mariyinsky Palace in Kiev the same year. She never again returned to Russia's capital. Empress Alexandra commented about her departure: “it’s much better Motherdear stays … at Kiev, where the climate is better and she can live as she wishes and hears less gossip”.

In Kiev, Maria engaged in the Red Cross and hospital work, and in September, the 50th anniversary of her arrival in Russia was celebrated with great festivities, during which she was visited by her son, Nicholas II, who came without his wife. Empress Alexandra wrote to the Tsar: “When you see Motherdear, you must rather sharply tell her how pained you are, that she listens to slander and does not stop it, as it makes mischief and others would be delighted, I am sure, to put her against me…” Maria did ask Nicholas II to remove both Rasputin and Alexandra from all political influence, but shortly after, Nicholas and Alexandra broke all contact with the Tsar's family.

When Rasputin was murdered, part of the Imperial relatives asked Maria to return to the capital and use the moment to replace Alexandra as the Tsar's political adviser. Maria refused, but she did admit that Alexandra should be removed from influence over state affairs: “Alexandra Feodorovna must be banished. Don’t know how but it must be done. Otherwise she might go completely mad. Let her enter a convent or just disappear.”

Revolution came to Russia in 1917, first with the February Revolution, then with Nicholas II's abdication on 15 March. After travelling from Kiev to meet with her deposed son, Nicholas II in Mogilev, Maria returned to the city, where she quickly realised how Kiev had changed and that her presence was no longer wanted. She was persuaded by her family there to travel to the Crimea by train with a group of other refugee Romanovs.

After a time living in one of the imperial residences in the Crimea, she received reports that her sons, her daughter-in-law and her grandchildren had been murdered. However, she publicly rejected the report as a rumour. On the day after the murder of the Tsar's family, Maria received a messenger from Nicky, "a touching man" who told of how difficult life was for her son's family in Yekaterinburg. "And nobody can help or liberate them - only God! My Lord save my poor, unlucky Nicky, help him in his hard ordeals!" In her diary she comforted herself: "I am sure they all got out of Russia and now the Bolsheviks are trying to hide the truth." She firmly held on to this conviction until her death. The truth was too painful for her to admit publicly. Her letters to her son and his family have since almost all been lost; but in one that survives, she wrote to Nicholas: "You know that my thoughts and prayers never leave you. I think of you day and night and sometimes feel so sick at heart that I believe I cannot bear it any longer. But God is merciful. He will give us strength for this terrible ordeal." Maria's daughter Olga Alexandrovna commented further on the matter, "Yet I am sure that deep in her heart my mother had steeled herself to accept the truth some years before her death."

Despite the overthrow of the monarchy in 1917, the former Empress Dowager Maria at first refused to leave Russia. Only in 1919, at the urging of her sister, Queen Dowager Alexandra, did she begrudgingly depart, fleeing Crimea over the Black Sea to London. King George V sent the warship HMS "Marlborough" to retrieve his aunt. The party of 17 Romanovs included her daughter the Grand Duchess Xenia and five of Xenia's sons plus six dogs and a canary.

After a brief stay in the British base in Malta, they travelled to England on the British ship the "Lord Nelson", and she stayed with her sister, Alexandra. Although Queen Alexandra never treated her sister badly and they spent time together at Marlborough House in London and at Sandringham House in Norfolk, Maria, as a deposed empress dowager, felt that she was now "number two," in contrast to her sister, a popular queen dowager, and she eventually returned to her native Denmark. After living briefly with her nephew, King Christian X, in a wing of the Amalienborg Palace, she chose her holiday villa Hvidøre near Copenhagen as her new permanent home.

There were many Russian émigrées in Copenhagen who continued to regard her as the Empress and often asked her for help. The All-Russian Monarchical Assembly held in 1921 offered her the "locum tenens" of the Russian throne but she declined with the evasive answer "Nobody saw Nicky killed" and therefore there was a chance her son was still alive. She rendered financial support to Nikolai Sokolov, who studied the circumstances of the death of the Tsar's family, but they never met. The Grand Duchess Olga sent a telegram to Paris cancelling an appointment because it would have been too difficult for the old and sick woman to hear the terrible story of her son and his family.

In November 1925, Maria's favourite sister, Queen Alexandra, died. That was the last loss that she could bear. "She was ready to meet her Creator," wrote her son-in-law, Grand Duke Alexander Mikhailovich, about Maria's last years. On 13 October 1928 at Hvidøre near Copenhagen, in a house she had once shared with her sister Queen Alexandra, Maria died at the age of 80, having outlived four of her six children. Following services in Copenhagen's Russian Orthodox Alexander Nevsky Church, the Empress was interred at Roskilde Cathedral.

In 2005, Queen Margrethe II of Denmark and President Vladimir Putin of Russia and their respective governments agreed that the Empress's remains should be returned to St. Petersburg in accordance with her wish to be interred next to her husband. A number of ceremonies took place from 23 to 28 September 2006. The funeral service, attended by high dignitaries, including the Crown Prince and Crown Princess of Denmark and Prince and Princess Michael of Kent, did not pass without some turbulence. The crowd around the coffin was so great that a young Danish diplomat fell into the grave before the coffin was interred. On 26 September 2006, a statue of Maria Feodorovna was unveiled near her favourite Cottage Palace in Peterhof. Following a service at Saint Isaac's Cathedral, she was interred next to her husband Alexander III in the Peter and Paul Cathedral on 28 September 2006, 140 years after her first arrival in Russia and almost 78 years after her death.

Tsar Alexander III and Maria Feodorovna had four sons and two daughters:




</doc>
<doc id="20541" url="https://en.wikipedia.org/wiki?curid=20541" title="Montauban">
Montauban

Montauban (, ; ) is a commune in the Tarn-et-Garonne department in the Occitanie region in southern France. It is the capital of the department and lies north of Toulouse. Montauban is the most populated town in Tarn-et-Garonne, and the sixth most populated of Occitanie behind Toulouse, Montpellier, Nîmes, Perpignan and Béziers. In 2013, there were 57,921 inhabitants, called "Montalbanais". The town has been classified "Ville d’art et d’histoire" (City of art and history) since 2015.

The town, built mainly of a reddish brick, stands on the right bank of the Tarn at its confluence with the Tescou.

Montauban is the second oldest (after Mont-de-Marsan) of the "bastides" of southern France. Its foundation dates from 1144 when Count Alphonse Jourdain of Toulouse, granted it a liberal charter. The inhabitants were drawn chiefly from Montauriol, a village which had grown up around the neighbouring monastery of St Théodard.

In the 13th century the town suffered much from the ravages of the Albigensian war and from the Inquisition, but by 1317 it had recovered sufficiently to be chosen by John XXII as the head of a diocese of which the basilica of St Théodard became the cathedral.
In 1360, under the Treaty of Brétigny, it was ceded to the English; they were expelled by the inhabitants in 1414. In 1560 the bishops and magistrates embraced Protestantism, expelled the monks, and demolished the cathedral. Ten years later it became one of the four Huguenot strongholds under the Peace of Saint-Germain, and formed a small independent republic. It was the headquarters of the Huguenot rebellion of 1621, and successfully withstood an 86-day siege by Louis XIII. 

Because Montauban was a Protestant town, it resisted and held its position against the royal power, refusing to give allegiance to the Catholic King. To scare off the King's opponents and speed up the end of the siege, 400 cannonballs were fired, but Montauban resisted and the royal army was vanquished. Saint Jacques church is still marked by the cannonballs, and every year in September, the city celebrates "les 400 coups" (the 400 shots), which has become a common phrase in French.

Montauban did not submit to royal authority until after the fall of La Rochelle in 1629, when its fortifications were destroyed by Cardinal Richelieu. The Protestants again suffered persecution later in the century, as Louis XIV began to persecute Protestants by sending troops to their homes (dragonnades) and then in 1685 revoked the Edict of Nantes, which had granted the community tolerance.

During World War II, Leonardo da Vinci's "Mona Lisa" was briefly hidden in a secret vault behind a wine cellar at Montauban.

Montauban's climate is temperate and subtropical (borderline "Csa"/"Csb" in the Köppen climate classification). Temperatures are rather mild in winter and hot in summer. The town experienced severe droughts in 2003, 2006, 2012 and 2015. On August 31, 2015, the Tarn-et-Garonne area was particularly struck by a wave of violent storms. These storms, accompanied by very strong winds, created a tornado, which caused considerable damage in a large part of the department. Montauban was particularly affected, with winds measured between 130 and 150 kilometers per hour (a record) in the city center.

Its fortifications have been replaced by boulevards beyond which extend numerous suburbs, while on the left bank of the Tarn is the suburb of Villebourbon, which is connected to the town by a remarkable bridge of the early 14th century. This bridge is known as "Pont Vieux" (i.e. "Old Bridge"). King Philip the Fair of France officially launched the building of the bridge in 1303 while on a tour to Toulouse. The project took 30 years to complete, and the bridge was inaugurated in 1335. The main architects were Étienne de Ferrières and Mathieu de Verdun. It is a pink brick structure over in length, but while its fortified towers have disappeared, it is otherwise in a good state of preservation. The bridge was designed to resist the violent floods of the Tarn, and indeed it successfully withstood the two terrible millennial floods of 1441 and 1930. The bridge is a straight level bridge, which is quite unusual for Medieval Europe, where lack of technological skills meant that most bridges were of the humpback type.
The "Musée Ingres", on the site of a castle of the Counts of Toulouse and once the residence of the bishops of Montauban, stands at the east end of the bridge. It belongs chiefly to the 17th century, but some portions are much older, notably an underground chamber known as the Hall of the Black Prince ("Salle du Prince Noir"). It comprises most of the work (including his "Jesus among the Teachers of the Law") of Jean Ingres, the celebrated painter, whose birth in Montauban is commemorated by an elaborate monument. It is the largest museum of Ingres paintings in the world. The museum also contains some sculptures by famous sculptor Antoine Bourdelle, another native of Montauban, as well as collections of antiquities (Greek vases) and 18th and 19th ceramics.

The "Place Nationale" is a square of the 17th century, entered at each corner by gateways giving access to a large open space surrounded by pink brick houses supported by double rows of arcades.

The "préfecture" is located in the palace built by the "intendant" of Montauban (the equivalent of a "préfet" before the French Revolution), and is a large elegant 18th century mansion, built of pink bricks and white stone, with a steep roof of blue gray slates, in a style combining northern and southern French styles of architecture.

The chief churches of Montauban are the cathedral, remarkable only for the possession of the "Vow of Louis XIII", one of the masterpieces of Ingres, and the church of St Jacques (14th and 15th centuries), dedicated to Saint James of Compostela, the façade of which is surmounted by a handsome octagonal tower, the base of which is in Romanesque style, while the upper levels, built later, are in Gothic style. Montauban:

The commercial importance of Montauban is due rather to its trade in agricultural produce, horses, game and poultry, than to its industries, which include nursery-gardening, cloth-weaving, cloth-dressing, flour-milling, wood-sawing, and the manufacture of furniture, silk-gauze and straw hats.

However, due to the proximity of Toulouse and the cheaper cost of industrial grounds, more and more mechanical products are being manufactured there.

Population:

Aire urbaine:

The town is a railway junction, and the station Gare de Montauban-Ville-Bourbon offers connections with Toulouse, Bordeaux, Paris, Brive-la-Gaillarde, Marseille and several regional destinations. Montauban communicates with the Garonne via the Canal de Montech.

Founded in 1144 by the Comte de Toulouse, the town of Montauban has some particularities: its center's red brick streets intersect at right angles and meet at the National Square (Place Nationale) which is ranked among the most beautiful squares of France. Some buildings and architectural complexes are distinguished, such as "le Musée Ingres", "la Place Nationale", "le Pont vieux", "L’église Saint Jacques", " la Cathédrale Notre Dame", « l’Ancien Collège des Jésuites », « le Muséum ».

The town is home of the rugby union club US Montauban. The team gained promotion from the Pro D2 competition for the 2006–07 Top 14 season. The whole town supports rugby, but the athletic club is also very efficient and national results have been regular since 2007. Some athletes in Montauban's athletic club are international athletes. Every year, since 2004, the Rene Arcuset cross country race has been organized in the city.

In the movie "Les Tontons Flingueurs" a French classic by Georges Lautner, shot and released in 1963, Lino Ventura's character is a businessman from Montauban. Called to Paris for a personal case, he is nicknamed by Bernard Blier’s character "Le gugusse de Montauban" (the guy from Montauban.) The "gugusse" will later answer: "one should never leave Montauban". Recently, a round-about in the center of the town was renamed "Tonton Flingueurs’ round-about" and placards with drawings of the actors have been displayed.

Montauban was the birthplace of:

Montauban was the deathplace of:

Montauban is the seat of a bishop and a court of assize. It has tribunals of first instance and of commerce, a chamber of commerce and a board of trade arbitration, lycées and a training college, schools of commerce and viticulture, a branch of the Bank of France, and a faculty of Protestant theology.







</doc>
<doc id="20542" url="https://en.wikipedia.org/wiki?curid=20542" title="Rail transport modelling">
Rail transport modelling

Railway modelling (UK, Australia and Ireland) or model railroading (US and Canada) is a hobby in which rail transport systems are modelled at a reduced scale.

The scale models include locomotives, rolling stock, streetcars, tracks, signalling and landscapes including: countryside, roads, bridges, buildings, vehicles, urban landscape, model figures, lights, and features such as rivers, hills, tunnels, and canyons.

The earliest model railways were the 'carpet railways' in the 1840s. Electric trains appeared around the start of the 20th century, but these were crude likenesses. Model trains today are more realistic, in addition to being much more technologically advanced. Today modellers create model railway layouts, often recreating real locations and periods throughout history. The world's oldest working model railway is a model designed to train signalmen on the Lancashire and Yorkshire Railway. It is located in the National Railway Museum, York, England and dates back to 1912. It remained in use until 1995. The model was built as a training exercise by apprentices of the company's Horwich works and supplied with rolling stock by Bassett-Lowke.

Involvement ranges from possession of a train set to spending hours and large sums of money on a large and exacting model of a railroad and the scenery through which it passes, called a "layout". Hobbyists, called "railway modellers" or "model railroaders", may maintain models large enough to ride (see "Live steam, Ridable miniature railway" and "Backyard railroad").

Modellers may collect model trains, building a landscape for the trains to pass through. They may also operate their own railroad in miniature. For some modellers, the goal of building a layout is to eventually run it as if it were a real railroad (if the layout is based on the fancy of the builder) or as the real railroad did (if the layout is based on a prototype). If modellers choose to model a prototype, they may reproduce track-by-track reproductions of the real railroad in miniature, often using prototype track diagrams and historic maps.

Layouts vary from a circle or oval of track to realistic reproductions of real places modelled to scale. Probably the largest model landscape in the UK is in the Pendon Museum in Oxfordshire, UK, where an EM gauge (same 1∶76.2 scale as 00 but with more accurate track gauge) model of the Vale of White Horse in the 1930s is under construction. The museum also houses one of the earliest scenic models – the Madder Valley layout built by John Ahern. This was built in the late 1930s to late 1950s and brought in realistic modelling, receiving coverage on both sides of the Atlantic in the magazines "Model Railway News" and "Model Railroader". Bekonscot in Buckinghamshire is the oldest model village and includes a model railway, dating from the 1930s. The world's largest model railroad in H0 scale is the "Miniatur Wunderland" in Hamburg, Germany. The largest live steam layout, with of track is 'Train Mountain' in Chiloquin, Oregon, U.S.
Operations form an important aspect of rail transport modelling with many layouts being dedicated to emulating the operational aspects of a working railway. These layouts can become extremely complex with multiple routes, movement patterns and timetabled operation. The British outline model railway of Banbury Connections is one of the world's most complicated model railways.

Model railroad clubs exist where enthusiasts meet. Clubs often display models for the public. One specialist branch concentrates on larger scales and gauges, commonly using track gauges from . Models in these scales are usually hand-built and powered by live steam, or diesel-hydraulic, and the engines are often powerful enough to haul dozens of human passengers.

The Tech Model Railroad Club (TMRC) at MIT in the 1950s pioneered automatic control of track-switching by using telephone relays.

The oldest society is 'The Model Railway Club' (established 1910), near Kings Cross, London, UK. As well as building model railways, it has 5,000 books and periodicals. Similarly, 'The Historical Model Railway Society' at Butterley, near Ripley, Derbyshire specialises in historical matters and has archives available to members and non-members.

The words "scale" and "gauge" seem at first interchangeable but their meanings are different. "Scale" is the model's measurement as a proportion to the original, while "gauge" is the measurement between the rails.

The size of engines depends on the scale and can vary from tall for the largest ridable live steam scales such as 1∶4, down to matchbox size for the smallest: Z-scale (1∶220) or T scale (1∶450). A typical HO (1∶87) engine is tall, and long. The most popular scales are: G scale, Gauge 1, O scale, S scale, HO scale (in Britain, the similar OO), TT scale, and N scale (1∶160 in the United States, but 1∶148 in the UK). HO and OO are the most popular. Popular narrow-gauge scales include Sn3, HOn3 and Nn3, which are the same in scale as S, HO and N except with a narrower spacing between the tracks (in these examples, a scale instead of the standard gauge).

The largest common scale is 1∶8, with 1∶4 sometimes used for park rides. G scale (Garden, 1∶24 scale) is most popular for backyard modelling. It is easier to fit a G scale model into a garden and keep scenery proportional to the trains. Gauge 1 and Gauge 3 are also popular for gardens. O, S, HO, and N scale are more often used indoors.

At first, model railways were not to scale. Aided by trade associations such as the National Model Railroad Association (NMRA) and "Normen Europäischer Modellbahnen" (NEM), manufacturers and hobbyists soon arrived at "de facto" standards for interchangeability, such as gauge, but trains were only a rough approximation to the real thing. Official scales for the gauges were drawn up but not at first rigidly followed and not necessarily correctly proportioned for the gauge chosen. 0 (zero) gauge trains, for instance, operate on track too widely spaced in the United States as the scale is accepted as 1∶48 whereas in Britain 0 gauge uses a ratio of 43.5∶1 or 7 mm/1 foot and the gauge is near to correct. British OO standards operate on track significantly too narrow. The 4 mm/1 foot scale on a gauge corresponds to a track gauge of , (undersized). gauge corresponds to standard gauge in H0 (half-0) 3.5 mm/1 foot or 1∶87. This arose due to British locomotives and rolling stock being smaller than those found elsewhere, leading to an increase in scale to enable H0 scale mechanisms to be used. Most commercial scales have standards that include wheel flanges that are too deep, wheel treads that are too wide, and rail tracks that are too large. In H0 scale, the rail heights are codes 100, 87, 53

Later, modellers became dissatisfied with inaccuracies and developed standards in which everything is correctly scaled. These are used by modellers but have not spread to mass-production because the inaccuracies and overscale properties of the commercial scales ensure reliable operation and allow for shortcuts necessary for cost control. The finescale standards include the UK's P4, and the even finer S4, which uses track dimensions scaled from the prototype. This 4 mm:1 ft modelling uses wheels or less wide running on track with a gauge of . Check-rail and wing-rail clearances are similarly accurate.

A compromise of P4 and OO is 'EM' which uses a gauge of with more generous tolerances than P4 for check clearances. It gives a better appearance than OO though pointwork is not as close to reality as P4. It suits many where time and improved appearance are important. There is a small following of finescale OO which uses the same 16.5mm gauge as OO, but with the finer scale wheels and smaller clearances as used with EM- it is essentially 'EM-minus-1.7mm.'

Many groups build modules, which are sections of layouts, and can be joined together to form a larger layout, for meetings or for special occasions. For each kind of module system, there is an interface standard, so that modules made by different participants may be connected, even if they have never been connected before. Many of these module types are listed in the Layout standards organizations section of this article.

In addition to different scales, there are also different types of couplers for connecting cars, which are not compatible with each other.

In HO, the Americans standardized on horn-hook, or X2F couplers. Horn hook couplers have largely given way to a design known as a working knuckle coupler which was popularized by the Kadee Quality Products Co., and which has subsequently been emulated by a number of other manufactures in recent years. Working knuckle couplers are a closer approximation to the "automatic" couplers used on the prototype there and elsewhere. Also in HO, the European manufacturers have standardized, but on a coupler mount, not a coupler: many varieties of coupler can be plugged in (and out) of the NEM coupler box. None of the popular couplers has any resemblance to the prototype three-link chains generally used on the continent.

For British modellers, whose most popular scale is OO, the normal coupler is a tension-lock coupler, which, again has no pretence of replicating the usual prototype three-link chain couplers. Bachmann and more recently Hornby have begun to offer models fitted with NEM coupler pockets. This theoretically enables modellers of British railways to substitute any other NEM362 coupler, though many Bachmann models place the coupler pocket at the wrong height. A fairly common alternative is to use representations of chain couplings as found on the prototype, though these require large radius curves to be used to avoid derailments.

Other scales have similar ranges of non-compatible couplers available. In all scales couplers can be exchanged, with varying degrees of difficulty.

Some modellers pay attention to landscaping their layout, creating a fantasy world or modelling an actual location, often historic. Landscaping is termed "scenery building" or "scenicking".

Constructing scenery involves preparing a sub-terrain using a wide variety of building materials, including (but not limited to) screen wire, a lattice of cardboard strips, or carved stacks of expanded polystyrene (styrofoam) sheets. A scenery base is applied over the sub-terrain; typical base include casting plaster, plaster of Paris, hybrid paper-pulp (papier-mâché) or a lightweight foam/fiberglass/bubblewrap composite as in Geodesic Foam Scenery.
The scenery base is covered with substitutes for ground cover, which may be Static Grass or scatter. "Scatter" or "flock" is a substance used in the building of dioramas and model railways to simulate the effect of grass, poppies, fireweed, track ballast and other scenic ground cover. Scatter used to simulate track ballast is usually fine-grained ground granite. Scatter which simulates coloured grass is usually tinted sawdust, wood chips or ground foam. Foam or natural lichen or commercial scatter materials can be used to simulate shrubbery. An alternative to scatter, for grass, is static grass which uses static electricity to make its simulated grass actually stand up.

Buildings and structures can be purchased as kits, or built from cardboard, balsa wood, basswood, other soft woods, paper, or polystyrene or other plastic. Trees can be fabricated from materials such as Western sagebrush, candytuft, and caspia, to which adhesive and model foliage are applied; or they can be bought ready-made from specialist manufacturers. Water can be simulated using polyester casting resin, polyurethane, or rippled glass. Rocks can be cast in plaster or in plastic with a foam backing. Castings can be painted with stains to give colouring and shadows.

"Weathering" refers to making a model look used and exposed to weather by simulating dirt and wear on real vehicles, structures and equipment. Most models come out of the box looking new, because unweathered finishes are easier to produce. Also, the wear a freight car or building undergoes depends not only on age but where it is used. Rail cars in cities accumulate grime from building and automobile exhaust and graffiti, while cars in deserts may be subjected to sandstorms which etch or strip paint. A model that is weathered would not fit as many layouts as a pristine model which can be weathered by its purchaser.

There are many weather techniques that include, but are not limited to, painting (by either drybrushing or an airbrush), sanding, breaking, and even the use of chemicals to cause corrosion. Some processes become very creative depending on the skill of the modeller. For instance several steps may be taken to create a rusting effect to ensure not only proper colouring, but also proper texture and lustre.

Weathering purchased models is common, at the least, weathering aims to reduce the plastic-like finish of scale models. The simulation of grime, rust, dirt, and wear adds realism. Some modellers simulate fuel stains on tanks, or corrosion on battery boxes. In some cases, evidence of accidents or repairs may be added, such as dents or freshly painted replacement parts, and weathered models can be nearly indistinguishable from their prototypes when photographed appropriately.

Static diorama models or 'push along' scale models are a branch of model railways for unpowered locomotives, examples are Lone Star and Airfix models. Powered model railways are now generally operated by low voltage direct current (DC) electricity supplied via the tracks, but there are exceptions, such as Märklin and Lionel Corporation, which use alternating current (AC). Modern Digital Command Control (DCC) systems use alternating current. Other locomotives, particularly large models can use steam. Steam and clockwork driven engines are still sought by collectors.

Most early models for the toy market were powered by clockwork and controlled by levers on the locomotive. Although this made control crude the models were large and robust enough that handling the controls was practical. Various manufacturers introduced slowing and stopping tracks that could trigger levers on the locomotive and allow station stops.

Early electrical models used a three-rail system with the wheels resting on a metal track with metal sleepers that conducted power and a middle rail which provided power to a skid under the locomotive. This made sense at the time as models were metal and conductive. Modern plastics were not available and insulation was a problem. In addition the notion of accurate models had yet to evolve and toy trains and track were crude tinplate. A variation on the three-rail system, Trix Twin, allowed two trains to be independently controlled on one track, before the advent of Digital Command Control.

As accuracy became important some systems adopted two-rail power in which the wheels were isolated from each other and the rails carried the positive and negative supply with the right rail carrying the positive potential.

Other systems such as Märklin instead used fine metal studs to replace the central rail, allowing existing three-rail models to use more realistic track.

Where the model is of an electric locomotive, it may be supplied by overhead lines, like the full-size locomotive. Before Digital Command Control became available, this was one way of controlling two trains separately on the same track. The electric-outline model would be supplied by the overhead wire and the other model could be supplied by one of the running rails. The other running rail would act as a common return.

Early electric trains ran on trackside batteries because few homes in the late 19th century and early 20th century had electricity. Today, inexpensive train sets running on batteries are again common but regarded as toys and seldom used by hobbyists. Batteries located in the model often power garden railway and larger scale systems because of the difficulty in obtaining reliable power supply through the outdoor rails. The high power consumption and current draw of large scale garden models is more easily and safely met with internal rechargeable batteries. Most large scale battery powered models use radio control.

Engines powered by live steam are often built in large outdoor gauges of and , are also available in Gauge 1, G scale, 16 mm scale and can be found in O and OO/HO. Hornby Railways produce live steam locomotives in OO, based on designs first arrived at by an amateur modeller. Other modellers have built live steam models in HO/OO, OO9 and N, and there is one in Z in Australia.

Occasionally gasoline-electric models, patterned after real diesel-electric locomotives, come up among hobbyists and companies like Pilgrim Locomotive Works have sold such locomotives. Large-scale petrol-mechanical and petrol-hydraulic models are available but unusual and pricier than the electrically powered versions.

Modern manufacturing techniques mean mass-produced models achieve a high degree of precision and realism. In the past this was not the case and scratch building was very common. Simple models are made using cardboard engineering techniques. More sophisticated models can be made using a combination of etched sheets of brass and low temperature castings. Parts that need machining, such as wheels and couplings are purchased.

Etched kits are still popular, still accompanied by low temperature castings. These kits produce models that are not covered by the major manufacturers or in scales that are not in mass production. Laser machining techniques have extended this ability to thicker materials for scale steam and other locomotive types. Scratch builders may also make silicone rubber moulds of the parts they create, and cast them in various plastic resins (see Resin casting), or plasters. This may be done to save duplication of effort, or to sell to others. Resin "craftsman kits" are also available for a wide range of prototypes.

The first clockwork (spring-drive) and live steam locomotives ran until out of power, with no way for the operator to stop and restart the locomotive or vary its speed. The advent of electric trains, which appeared commercially in the 1890s, allowed control of the speed by varying the current or voltage. As trains began to be powered by transformers and rectifiers more sophisticated throttles appeared, and soon trains powered by AC contained mechanisms to change direction or go into neutral gear when the operator cycled the power. Trains powered by DC can change direction by reversing polarity.

Electricity permits control by dividing the layout into isolated blocks, where trains can be slowed or stopped by lowering or cutting power to a block. Dividing a layout into blocks permits operators to run more than one train with less risk of a fast train catching and hitting a slow train. Blocks can also trigger signals or other accessories, adding realism or whimsy. Three-rail systems often insulate one of the common rails on a section of track, and use a passing train to complete the circuit and activate an accessory.

Many layout builders are choosing digital operation of their layouts rather than the more traditional DC design. The industry standard command system is Digital Command Control (DCC). The advantages to DCC are that track voltage is constant (usually in the range of 20 volts AC) and the command throttle sends a signal to small circuit cards, or decoders, hidden inside the piece of equipment which control several functions of an individual locomotive, including speed, direction of travel, lights, smoke and various sound effects. This allows more realistic operation in that the modeller can operate independently several locomotives on the same stretch of track. Less common closed proprietary systems also exist. Several manufacturers offer software that can provide computer-control of DCC layouts.

In large scales, particularly for garden railways, radio control and DCC in the garden have become popular.

Several organizations exist to set standardizations for connectability between individual layout sections (commonly called "modules"). This is so several (or hundreds, given enough space and power) people or groups can bring together their own modules, connect them together with as little trouble as possible, and operate their trains. Despite different design and operation philosophies, different organizations have similar goals; standardized ends to facilitate connection with other modules built to the same specifications, standardized electricals, equipment, curve radii.




</doc>
<doc id="20544" url="https://en.wikipedia.org/wiki?curid=20544" title="Morphophonology">
Morphophonology

Morphophonology (also morphophonemics or morphonology) is the branch of linguistics that studies the interaction between morphological and phonological or phonetic processes. Its chief focus is the sound changes that take place in morphemes (minimal meaningful units) when they combine to form words.

Morphophonological analysis often involves an attempt to give a series of formal rules that successfully predict the regular sound changes occurring in the morphemes of a given language. Such a series of rules converts a theoretical underlying representation into a surface form that is actually heard. The units of which the underlying representations of morphemes are composed are sometimes called morphophonemes. The surface form produced by the morphophonological rules may consist of phonemes (which are then subject to ordinary phonological rules to produce speech sounds or "phones"), or else the morphophonological analysis may bypass the phoneme stage and produce the phones itself.

When morphemes combine, they influence each other's sound structure (whether analyzed at a phonetic or phonemic level), resulting in different variant pronunciations for the same morpheme. Morphophonology attempts to analyze these processes. A language's morphophonological structure is generally described with a series of rules which, ideally, can predict every morphophonological alternation that takes place in the language.

An example of a morphophonological alternation in English is provided by the plural morpheme, written as "-s" or "-es". Its pronunciation varies among , , and , as in "cats", "dogs", and "horses" respectively. A purely phonological analysis would most likely assign to these three endings the phonemic representations , , . On a morphophonological level, however, they may all be considered to be forms of the underlying object , which is a morphophoneme. The different forms it takes are dependent on the segment at the end of the morpheme to which it attaches: the dependencies are described by morphophonological rules. (The behaviour of the English past tense ending "-ed" is similar: it can be pronounced , or , as in "hoped", "bobbed" and "added".)

The plural suffix "-s" can also influence the form taken by the preceding morpheme, as in the case of the words "leaf" and "knife", which end with in the singular/but have in the plural ("leaves", "knives"). On a morphophonological level, the morphemes may be analyzed as ending in a morphophoneme , which becomes voiced when a voiced consonant (in this case the of the plural ending) is attached to it. The rule may be written symbolically as -> [α] / [α]. This expression is called Alpha Notation in which α can be + (positive value) or − (negative value).

Common conventions to indicate a morphophonemic rather than phonemic representation include double slashes (⫽  ⫽) (as above, implying that the transcription is 'more phonemic than simply phonemic'). This is the only convention consistent with the IPA. Other conventions include pipes (|  |), double pipes (‖  ‖) and curly brackets ({  }). Braces, from a convention in set theory, tend to be used when the phonemes are all listed, as in {s, z, ᵻz} for the English plural morpheme.

For instance, the English word "cats" may be transcribed phonetically as , phonemically as and morphophonemically as , if the plural is argued to be underlyingly , assimilating to after a voiceless nonsibilant. The tilde ~ may indicate morphological alternation, as in for "kneel~knelt" (the plus sign '+' indicates a morpheme boundary).

Inflected and agglutinating languages may have extremely complicated systems of morphophonemics. Examples of complex morphophonological systems include:

Until the 1950s, many phonologists assumed that neutralizing rules generally applied before allophonic rules. Thus phonological analysis was split into two parts: a morphophonological part, where neutralizing rules were developed to derive phonemes from morphophonemes; and a purely phonological part, where phones were derived from the phonemes. Since the 1960s (in particular with the work of the generative school, such as Chomsky and Halle's "The Sound Pattern of English") many linguists have moved away from making such a split, instead regarding the surface phones as being derived from the underlying morphophonemes (which may be referred to using various terminology) through a single system of (morpho)phonological rules.

The purpose of both phonemic and morphophonemic analysis is to produce simpler underlying descriptions for what appear on the surface to be complicated patterns. In purely phonemic analysis the data is just a set of words in a language, while for the purposes of morphophonemic analysis the words must be considered in grammatical paradigms to take account of the underlying morphemes. It is postulated that morphemes are recorded in the speaker's "lexicon" in an invariant (morphophonemic) form, which, in a given environment, is converted by rules into a surface form. The analyst attempts to present as completely as possible a system of underlying units (morphophonemes) and a series of rules that act on them, so as to produce surface forms consistent with the linguistic data.

The isolation form of a morpheme is the form in which that morpheme appears in isolation (when it is not subject to the effects of any other morpheme). In the case of a bound morpheme, such as the English past tense ending "-ed", it is generally not possible to identify an isolation form since such a morpheme does not occur in isolation.

It is often reasonable to assume that the isolation form of a morpheme provides its underlying representation. For example, in some varieties of American English, "plant" is pronounced , while "planting" is , where the morpheme "plant-" appears in the form . Here, the underlying form can be assumed to be , corresponding to the isolation form, since rules can be set up to derive the reduced form from this (but it would be difficult or impossible to set up rules that would derive the isolation form from an underlying ).

That is not always the case, however; the isolation form itself is sometimes subject to neutralization that does not apply to some other instances of the morpheme. For example, the French word "petit" ("small") is pronounced in isolation without the final [t] sound, but in certain derived forms (such as the feminine "petite"), the [t] is heard. If the isolation form were adopted as the underlying form, the information that there is a final "t" would be lost, and it would then be difficult to explain the appearance of the "t" in the inflected forms. Similar considerations apply to languages with final obstruent devoicing, in which the isolation form undergoes loss of voicing contrast, but other forms may not.

If the grammar of a language is assumed to have two rules, rule A and rule B, with A ordered before B, a given derivation may cause the application of rule A to create the environment for rule B to apply, which was not present before the application of rule A. Both rules then are in a "feeding relationship".

If rule A is ordered before B in the derivation in which rule A destroys the environment to which rule B applies, both rules are in a "bleeding order".

If A is ordered before B, and B creates an environment in which A could have applied, B is then said to counterfeed A, and the relationship is "counterfeeding".

If A is ordered before B, there is a "counterbleeding" relationship if B destroys the environment that A applies to and has already applied and so B has missed its chance to bleed A.

"Conjunctive ordering" is the ordering that ensures that all rules are applied in a derivation before the surface representation occurs. Rules applied in a feeding relationship are said to be "conjunctively ordered".

"Disjunctive ordering" is a rule that applies and prevents the other rule from applying in the surface representation. Such rules have a bleeding relationship and are said to be "disjunctively ordered".

The principle behind alphabetic writing systems is that the letters (graphemes) represent phonemes. However, many orthographies based on such systems have correspondences between graphemes and phonemes that are not exact, and it is sometimes the case that certain spellings better represent a word's morphophonological structure rather than the purely-phonological structure. An example is that the English plural morpheme is written "-s", regardless of whether it is pronounced or : "cats and "dogs, not "dogz".

The above example involves active morphology (inflection), and morphophonemic spellings are common in this context in many languages. Another type of spelling that can be described as morphophonemic is the kind that reflects the etymology of words. Such spellings are particularly common in English; examples include science" vs. "unconscious" , prejudice" vs. prequel" , sign signature" , nation" vs. nationalism" , and special" vs. species" .

For more detail on this topic, see Phonemic orthography, particularly the section on Morphophonemic features.



</doc>
<doc id="20545" url="https://en.wikipedia.org/wiki?curid=20545" title="Mirror">
Mirror

A mirror is a smooth or polished surface that returns an image by reflection. Technically, a mirror or reflector is an object such that each narrow beam of light that incides on its surface bounces (is reflected) in a single direction. This property, called specular reflection, distinguishes a mirror from objects that scatter light in many directions (such as flat-white paint), let it pass through them (such as a lens or prism), or absorb it.

Most mirrors behave as such only for certain ranges of wavelength, direction, and polarization of the incident light; most commonly for visible light, but also for other regions of the electromagnetic spectrum from X-rays to radio waves. A mirror will generally reflect only a fraction of the incident light; even the best mirrors may scatter, absorb, or transmit a small portion of it. If the mirror's width is only a few times the wavelength of the light, a significant part of the light will also be diffracted instead. An object that is a mirror when examined at a small scale (like a bearing ball) may seem to be scattering light when examined at a larger scale. 

When looking at a mirror, one will see a mirror image or reflected image of objects in the environment, formed by light emitted or scattered by them and reflected by the mirror towards one's eyes. This effect gives the illusion that those objects are behind the mirror, or (sometimes) in front of it. A plane mirror will yield a real-looking undistorted image, while a curved mirror may distort, magnify, or reduce the image in various ways.

A mirror is commonly used for inspecting oneself, such as during personal grooming; hence the old-fashioned name looking glass. This use, which dates from the Prehistory, overlaps with uses in decoration and architecture. Mirrors are also used to view other items that are not directly visible because of obstructions; examples include rear-view mirrors in vehicles, security mirrors in or around buildings, and dentist's mirrors. Mirrors are also used in optical and scientific apparatus such as telescopes, lasers, cameras, periscopes, and industrial machinery.

The terms "mirror" and "reflector" can be used for devices that reflect other types of radiation according to the same laws. An acoustic mirror reflects sound waves, and may be used for applications such as directional microphones, atmospheric studies, sonar, and sea floor mapping. An atomic mirror reflects matter waves, and can be used for atomic interferometry and atomic holography.

The first mirrors used by humans were most likely pools of dark, still water, or water collected in a primitive vessel of some sort. The requirements for making a good mirror are a surface with a very high degree of flatness (preferably but not necessarily with high reflectivity), and a surface roughness smaller than the wavelength of the light. 

The earliest manufactured mirrors were pieces of polished stone such as obsidian, a naturally occurring volcanic glass. Examples of obsidian mirrors found in Anatolia (modern-day Turkey) have been dated to around 6000 BC. Mirrors of polished copper were crafted in Mesopotamia from 4000 BC, and in ancient Egypt from around 3000 BC. Polished stone mirrors from Central and South America date from around 2000 BC onwards.

By the Bronze Age most cultures were using mirrors made from polished discs of bronze, copper, silver, or other metals. In China, bronze mirrors were manufactured from around 2000 BC, some of the earliest bronze and copper examples being produced by the Qijia culture. Such metal mirrors remained the norm through to Greco-Roman Antiquity and throughout the Middle Ages in Europe. During the Roman Empire silver mirrors were in wide use even by maidservants. 

Speculum metal is a highly reflective alloy of copper and tin that has been used for mirrors until a couple of centuries ago. Such mirrors may have originated in China and India. Mirrors of speculum metal or any precious metal were hard to produce and were only owned by the wealthy. 

Common metal mirrors tarnished and required frequent polishing. Bronze mirrors had low reflectivity and poor color rendering, and stone mirrors were much worse in this regard. These defects explain the New Testament reference in 1 Corinthians 13 to seeing "as in a mirror, darkly."

The Greek philosopher Socrates, of "know thyself" fame, urged young people to look at themselves in mirrors so that, if they were beautiful, they would become worthy of their beauty, and if they were ugly, they would know how to hide their disgrace through learning.

Glass began to be used for mirrors in the 1st century CE, with the development of soda-lime glass and glass blowing. The Roman scholar Pliny the Elder claims that artisans in Sidon (modern-day Lebanon) were producing glass mirrors coated with lead or gold leaf in the back. The metal provided good reflectivity, and the glass provided a smooth surface and protected the metal from scrathes and tarnishing. However, there is no archeological evidence of glass mirrors before the third century. 

These early glass mirrors were made by blowing a glass bubble, and then cutting off a small circular section from 10 to 20 cm in diameter. Their surface was either concave or convex, and imperfections tended to distort the image. Lead-coated mirrors were very thin to prevent cracking by the heat of the molten metal. Due to their poor quality, high cost, and small size, solid-metal mirrors, primarily of steel, remained in common use until the late nineteenth century.

Silver-coated metal mirrors were developed in China as early as 500 CE. The bare metal was coated with an amalgam, then heated it until the mercury boiled away.

The evolution of glass mirrors in the Middle Ages followed improvements in glassmaking technology. Glassmakers in France made flat glass plates by blowing glass bubbles, spinning them rapidly to flatten them, and cutting rectangles out of them. A better method, developed in Germany and perfected in Venice by the 16th century, was to blow a cylinder of glass, cut off the ends, slice it along its length, and unroll it onto a flat hot plate. Venetian glassmakers also adopted lead glass for mirrors, because of its crystal-clarity and its easier workability. By the 11th century, glass mirrors were being produced in Moorish Spain.

During the early European Renaissance, a fire-gilding technique developed to produce an even and highly reflective tin coating for glass mirrors. The back of the glass was coated with a tin-mercury amalgam, and the mercury was then evaporated by heating the piece. This process caused less thermal shock to the glass than the older molten-lead method. The date and location of the discovery is unknown, but by the 16th century Venice was a center of mirror production using this technique. These Venetian mirrors were up to square. 

For a century, Venice retained the monopoly of the tin amalgam technique. Venetian mirrors in richly decorated frames served as luxury decorations for palaces throughout Europe, and were very expensive. For example, in the late seventeenth century, the Countess de Fiesque was reported to have traded an entire wheat farm for a mirror, considering it a bargain. However, by the end of that century the secret was leaked through to industrial espionage. French workshops succeeded in large-scale industrialization of the process, eventually making mirrors affordable to the masses, in spite of the toxicity of mercury's vapor.

The invention of the ribbon machine in the late Industrial Revolution allowed modern glass panes to be produced in bulk. The Saint-Gobain factory, founded by royal initiative in France, was an important manufacturer, and Bohemian and German glass, often rather cheaper, was also important.

The invention of the silvered-glass mirror is credited to German chemist Justus von Liebig in 1835. His wet deposition process involved the deposition of a thin layer of metallic silver onto glass through the chemical reduction of silver nitrate. This silvering process was adapted for mass manufacturing and led to the greater availability of affordable mirrors.

Currently mirrors are often produced by the wet deposition of silver, or sometimes nickel or chromium (the latter used most often in automotive mirrors) via electroplating directly onto the glass substrate.

Glass mirrors for optical instruments are usually produced by vacuum deposition methods. These techniques can be traced to observations in the 1920s and 1930s that metal was being ejected from electrodes in gas discharge lamps and condensed on the glass walls forming a mirror-like coating. The phenomenon, called sputtering, was developed into an industrial metal-coating method with the development of semiconductor technology in the 1970s. 

A similar phenomenon had been observed with incandescent light bulbs: the metal in the hot filament would slowly sublimate and condense on the bulb's walls. This phenomenon was developed into the method of evaporation coating by Pohl and Pringsheim in 1912. John D. Strong used evaporation coating to make the first aluminum-coated telescope mirrors in the 1930s. The first dielectric mirror was created in 1937 by Auwarter using evaporated rhodium.

The metal coating of glass mirrors is usually protected from abrasion and corrosion by a layer of paint applied over it. Mirrors for optical instruments often have the metal layer on the front face, so that the light does not have to cross the glass twice. In these mirrors, the metal may be protected by a thin transparent coating of anon-metallic (dielectric) material. The first metallic mirror to be enhanced with a dielectric coating of silicon dioxide was created by Hass in 1937. In 1939 at the Schott Glass company, Walter Geffcken invented the first dielectric mirrors to use multilayer coatings.

The Greek in Classical Antiquity were familiar with the use of mirrors to concentrate light. Parabolic mirrors were described and studied by the mathematician Diocles in his work "On Burning Mirrors". Ptolemy conducted a number of experiments with curved polished iron mirrors, and discussed plane, convex spherical, and concave spherical mirrors in his "Optics". 

Parabolic mirrors were also described by the Caliphate mathematician Ibn Sahl in the tenth century. The scholar Ibn al-Haytham discussed concave and convex mirrors in both cylindrical and spherical geometries, carried out a number of experiments with mirrors, and solved the problem of finding the point on a convex mirror at which a ray coming from one point is reflected to another point.

Mirrors can be classified in many ways; including by shape, support and reflective materials, manufacturing methods, and intended application.

Typical mirror shapes are planar, convex, and concave. 

The surface of curved mirrors is often a part of a sphere, for ease of fabrication. Mirrors that are meant to precisely concentrate parallel rays of light into a point are usually made in the shape of a paraboloid of revolution instead; they are used in telescopes (form radio waves to X-rays), in antennas to communicate with broadcast satellites, and in solar furnaces. A segmented mirror, consisting of multiple flat or curved mirrors, properly placed and oriented, may be used instead.

Mirrors that are intended to concentrate sunlight onto a long pipe may be a circular cylinder or of a parabolic cylinder.

The most common structural material for mirrors is glass, due to its transparency, ease of fabrication, rigidity, hardness, and ability to take a smooth finish.

The most common mirrors consist of a plate of transparent glass, with a thin reflective layer on the back (the side opposite to the incident and reflected light) backed by a coating that protects that layer against abrasion, tarnishing, and corrosion. The glass is usually soda-lime glass, but lead glass may be used for decorative effects, and other transparent materials may be used for specific applications.

A plate of transparent plastic may be used instead of glass, for lighter weight or impact resistance. Alternatively, a flexible transparent plastic film may be bonded to the front and/or back surface of the mirror, to prevent injuries in case the mirror is broken. Lettering or decorative designs may be printed on the front face of the glass, or formed on the reflective layer. The front surface may have an anti-reflection coating.

Mirrors which are reflective on the front surface (the same side of the incident and reflected light) may be made of any rigid material. The supporting material does not need to be transparent, but telescope mirrors often use glass anyway. Often a protective transparent coating is added on top of the reflecting layer, to protect it against abrasion, tarnishing, and corrosion, or to absorb certain wavelengths.

Thin flexible plastic mirrors are sometimes used for safety, since they cannot shatter or produce sharp flakes. Their flatness is achieved by stretching them on a rigid frame. These usually consist of a layer of evaporated aluminum between two thin layers of transparent plastic.

In common mirrors, the reflective layer is usually some metal like silver, tin, nickel, or chromium, deposited by a wet process; or aluminum, deposited by sputtering or evaporation in vacuum. The reflective layer may also be made of one or more layers of transparent materials with suitable indices of refraction.

The structural material may be a metal, in which case the reflecting layer may be just the surface of the same. Metal concave dishes are often used to reflect infrared light (such as in space heaters) or microwaves (as in satellite TV antennas). Some telescopes, such as the Sky Mirror and liquid metal telescopes, as well as mirrors for high-power laser cutting, also use all-metal mirrors.

Mirrors that reflect only part of the light, while transmitting some of the rest, can be made with very thin metal layers or suitable combinations of dielectric layers. They are typically used as beamsplitters. A dichroic mirror, in particular, has surface that reflects certain wavelengths of light, while letting other wavelengths pass through. A cold mirror is a dichroic mirror that efficiently reflects the entire visible light spectrum while transmitting infrared wavelengths. A hot mirror is the opposite: it reflects infrared light while transmitting visible light. Dichroic mirrors are often used as filters to remove undesired components of the light in cameras and measuring instruments.

An active mirror will produce reflected beams that have more power than the incident beams. They are used to make disk lasers. The amplification is typically over a narrow range of wavelengths, and requires an external source of power.

In X-ray telescopes, the X-rays incide on a highly precise metal surface at almost grazing angles, and only a small fraction of the rays are reflected. In flying relativistic mirrors conceived for X-ray lasers, the reflecting surface is a spherical shockwave (wake wave) created in a low-density plasma by a very intense laser-pulse, and moving at an extremely high velocity.

A phase-conjugating mirror uses nonlinear optics to reverse the phase difference between incident beams. Such mirrors may be used, for example, for combination and self-guiding of laser beams and correction of atmospheric distortions in imaging systems.

When a sufficiently narrow beam of light is reflected at a point of a surface, the surface's normal direction formula_1 will be the bisector of the angle formed by the two beams at that point. That is, the direction vector formula_2 towards the incident beams's source, the normal vector formula_1, and direction vector formula_4 of the reflected beam will be coplanar, and the angle between formula_1 and formula_4 will be equal to the angle of incidence between formula_1 and formula_2, but of opposite sign. 

This property can be explained by the physics of an electromagnetic plane wave that is incident to a flat surface that is electrically conductive or where the speed of light changes abruptly, as between two materials with different indices of refraction. 




More specifically, a concave parabolic mirror (whose surface is a part of a paraboloid of revolution) will reflect rays that are parallel to its axis into rays that pass through its focus. Conversely, a parabolic concave mirror will reflect any ray that comes from its focus towards a direction parallel to its axis. If a concave mirror surface is a part of a prolate ellipsoid, it will reflect any ray coming from one focus toward the other focus.

A convex parabolic mirror, on the other hand, will reflect rays that are parallel to its axis into rays that seem to emanate from the focus of the surface, behind the mirror. Conversely, it will reflect incoming rays that converge toward that point into rays that are parallel to the axis. A convex mirror that is part of a prolate ellipsoid will reflect rays that converge towards one focus into divergent rays that seem to emanate from the other focus.

Spherical mirrors do not reflect parallel rays to rays that converge to or diverge from a single point, or vice-versa, due to spherical aberration. However, a spherical mirror whose diameter is sufficiently small compared to the sphere's radius will behave very similarly to a parabolic mirror whose axis goes through the mirror's center and the center of that sphere; so that spherical mirrors can substitute for parabolic ones in many applications.

A similar aberration occurs with parabolic mirrors when the incident rays are parallel among themselves but not parallel to the mirror's axis, or are divergent from a point that is not the focus — as when trying to form an image of an objet that is near the mirror or spans a wide angle as seen from it. However, this aberration can be sufficiently small if the object image is sufficiently far from the mirror and spans a sufficiently small angle around its axis.

Objects viewed in a (plane) mirror will appear laterally inverted (e.g., if one raises one's right hand, the image's left hand will appear to go up in the mirror), but not vertically inverted (in the image a person's head still appears above their body). However, a mirror does not usually "swap" left and right any more than it swaps top and bottom. A mirror typically reverses the forward/backward axis. To be precise, it reverses the object in the direction perpendicular to the mirror surface (the normal). Because left and right are defined relative to front-back and top-bottom, the "flipping" of front and back results in the perception of a left-right reversal in the image. (If you stand side-on to a mirror, the mirror really does reverse your left and right, because that's the direction perpendicular to the mirror.)

Looking at an image of oneself with the front-back axis flipped results in the perception of an image with its left-right axis flipped. When reflected in the mirror, your right hand remains directly opposite your real right hand, but it is perceived as the left hand of your image. When a person looks into a mirror, the image is actually front-back reversed, which is an effect similar to the hollow-mask illusion. Notice that a mirror image is fundamentally different from the object and cannot be reproduced by simply rotating the object.

For things that may be considered as two-dimensional objects (like text), front-back reversal cannot usually explain the observed reversal. In the same way that text on a piece of paper appears reversed if held up to a light and viewed from behind, text held facing a mirror will appear reversed, because the observer is behind the text. Another way to understand the reversals observed in images of objects that are effectively two-dimensional is that the inversion of left and right in a mirror is due to the way human beings turn their bodies. To turn from viewing the side of the object facing the mirror to view the reflection in the mirror requires the observer to look in the opposite direction. To look in another direction, human beings turn their heads about a vertical axis. This causes a left-right reversal in the image but not an up-down reversal.

The reflectivity of a mirror is determined by the percentage of reflected light per the total of the incident light. The reflectivity may vary with wavelength. All or a portion of the light not reflected is absorbed by the mirror, while in some cases a portion may also transmit through. Although some small portion of the light will be absorbed by the coating, the reflectivity is usually higher for first-surface mirrors, eliminating both reflection and absorption losses from the substrate. The reflectivity is often determined by the type and thickness of the coating. When the thickness of the coating is sufficient to prevent transmission, all of the losses occur due to absorption. Aluminum is harder, less expensive, and more resistant to tarnishing than silver, and will reflect 85 to 90% of the light in the visible to near-ultraviolet range, but experiences a drop in its reflectance between 800 and 900 nm. Gold is very soft and easily scratched, costly, yet does not tarnish. Gold is greater than 96% reflective to near and far-infrared light between 800 and 12000 nm, but poorly reflects visible light with wavelengths shorter than 600 nm (yellow). Silver is expensive, soft, and quickly tarnishes, but has the highest reflectivity in the visual to near-infrared of any metal. Silver can reflect up to 98 or 99% of light to wavelengths as long as 2000 nm, but loses nearly all reflectivity at wavelengths shorter than 350 nm. Dielectric mirrors can reflect greater than 99.99% of light, but only for a narrow range of wavelengths, ranging from a bandwidth of only 10 nm to as wide as 100 nm for tunable lasers. However, dielectric coatings can also enhance the reflectivity of metallic coatings and protect them from scratching or tarnishing. Dielectric materials are typically very hard and relatively cheap, however the number of coats needed generally makes it an expensive process. In mirrors with low tolerances, the coating thickness may be reduced to save cost, and simply covered with paint to absorb transmission.

Surface quality, or surface accuracy, measures the deviations from a perfect, ideal surface shape. Increasing the surface quality reduces distortion, artifacts, and aberration in images, and helps increase coherence, collimation, and reduce unwanted divergence in beams. For plane mirrors, this is often described in terms of flatness, while other surface shapes are compared to an ideal shape. The surface quality is typically measured with items like interferometers or optical flats, and are usually measured in wavelengths of light (λ). These deviations can be much larger or much smaller than the surface roughness. A normal household-mirror made with float glass may have flatness tolerances as low as 9–14λ per inch (25.4 mm), equating to a deviation of 5600 through 8800 nanometers from perfect flatness. Precision ground and polished mirrors intended for lasers or telescopes may have tolerances as high as λ/50 (1/50 of the wavelength of the light, or around 12 nm) across the entire surface. The surface quality can be affected by factors such as temperature changes, internal stress in the substrate, or even bending effects that occur when combining materials with different coefficients of thermal expansion, similar to a bimetallic strip.

Surface roughness describes the texture of the surface, often in terms of the depth of the microscopic scratches left by the polishing operations. Surface roughness determines how much of the reflection is specular and how much diffuses, controlling how sharp or blurry the image will be.

For perfectly specular reflection, the surface roughness must be kept smaller than the wavelength of the light. Microwaves, which sometimes have a wavelength greater than an inch (~25 mm) can reflect specularly off a metal screen-door, continental ice-sheets, or desert sand, while visible light, having wavelengths of only a few hundred nanometers (a few hundred-thousandths of an inch), must meet a very smooth surface to produce specular reflection. For wavelengths that are approaching or are even shorter than the diameter of the atoms, such as X-rays, specular reflection can only be produced by surfaces that are at a grazing incidence from the rays.

Surface roughness is typically measured in microns, wavelength, or grit size, with ~80,000–100,000 grit or ~½λ–¼λ being "optical quality".

Transmissivity is determined by the percentage of light transmitted per the incident light. Transmissivity is usually the same from both first and second surfaces. The combined transmitted and reflected light, subtracted from the incident light, measures the amount absorbed by both the coating and substrate. For transmissive mirrors, such as one-way mirrors, beam splitters, or laser output couplers, the transmissivity of the mirror is an important consideration. The transmissivity of metallic coatings are often determined by their thickness. For precision beam-splitters or output couplers, the thickness of the coating must be kept at very high tolerances to transmit the proper amount of light. For dielectric mirrors, the thickness of the coat must always be kept to high tolerances, but it is often more the number of individual coats that determine the transmissivity. For the substrate, the material used must also have good transmissivity to the chosen wavelengths. Glass is a suitable substrate for most visible-light applications, but other substrates such as zinc selenide or synthetic sapphire may be used for infrared or ultraviolet wavelengths.

Wedge errors are caused by the deviation of the surfaces from perfect parallelism. An optical wedge is the angle formed between two plane-surfaces (or between the principle planes of curved surfaces) due to manufacturing errors or limitations, causing one edge of the mirror to be slightly thicker than the other. Nearly all mirrors and optics with parallel faces have some slight degree of wedge, which is usually measured in seconds or minutes of arc. For first-surface mirrors, wedges can introduce alignment deviations in mounting hardware. For second-surface or transmissive mirrors, wedges can have a prismatic effect on the light, deviating its trajectory or, to a very slight degree, its color, causing chromatic and other forms of aberration. In some instances, a slight wedge is desirable, such as in certain laser systems where stray reflections from the uncoated surface are better dispersed than reflected back through the medium.

Surface defects are small-scale, discontinuous imperfections in the surface smoothness. Surface defects are larger (in some cases much larger) than the surface roughness, but only affect small, localized portions of the entire surface. These are typically found as scratches, digs, pits (often from bubbles in the glass), sleeks (scratches from prior, larger grit polishing operations that were not fully removed by subsequent polishing grits), edge chips, or blemishes in the coating. These defects are often an unavoidable side-effect of manufacturing limitations, both in cost and machine precision. If kept low enough, in most applications these defects will rarely have any adverse effect, unless the surface is located at an image plane where they will show up directly. For applications that require extremely low scattering of light, extremely high reflectance, or low absorption due to high energy-levels that could destroy the mirror, such as lasers or Fabry-Perot interferometers, the surface defects must be kept to a minimum.

Mirrors are usually manufactured by either polishing a naturally reflective material, such as speculum metal, or by applying a reflective coating to a suitable polished substrate.

In some applications, generally those that are cost-sensitive or that require great durability, such as for mounting in a prison cell, mirrors may be made from a single, bulk material such as polished metal. However, metals consist of small crystals (grains) separated by grain boundaries that may prevent the surface from attaining optical smoothness and uniform reflectivity. 

The coating of glass with a reflective layer of a metal is generally called "silvering", even though the metal may not be silver. Currently the main processes are electroplating, "wet" chemical deposition, and vacuum deposition Front-coated metal mirrors achieve reflectivities of 90–95% when new.

Applications requiring higher reflectivity or greater durability, where wide bandwidth is not essential, use dielectric coatings, which can achieve reflectivities as high as 99.997% over a limited range of wavelengths. Because they are often chemically stable and do not conduct electricity, dielectric coatings are almost always applied by methods of vacuum deposition, and most commonly by evaporation deposition. Because the coatings are usually transparent, absorption losses are negligible. Unlike with metals, the reflectivity of the individual dielectric-coatings is a function of Snell's law known as the Fresnel equations, determined by the difference in refractive index between layers. Therefore, the thickness and index of the coatings can be adjusted to be centered on any wavelength. Vacuum deposition can be achieved in a number of ways, including sputtering, evaporation deposition, arc deposition, reactive-gas deposition, and ion plating, among many others.

Mirrors can be manufactured to a wide range of engineering tolerances, including reflectivity, surface quality, surface roughness, or transmissivity, depending on the desired application. These tolerances can range from wide, such as found in a normal household-mirror, to extremely narrow, like those used in lasers or telescopes. Tightening the tolerances allows better and more precise imaging or beam transmission over longer distances. In imaging systems this can help reduce anomalies (artifacts), distortion or blur, but at a much higher cost. Where viewing distances are relatively close or high precision is not a concern, wider tolerances can be used to make effective mirrors at affordable costs.

Mirrors are commonly used as aids to personal grooming. They may range from small sizes, good to carry with oneself, to full body sized; they may be handheld, mobile, fixed or adjustable. A classic example of the latter is the cheval glass, which may be tilted.




With the sun as light source, a mirror can be used to signal by variations in the orientation of the mirror. The signal can be used over long distances, possibly up to on a clear day. This technique was used by Native American tribes and numerous militaries to transmit information between distant outposts.

Mirrors can also be used for search to attract the attention of search and rescue parties. Specialized type of mirrors are available and are often included in military survival kits.

Microscopic mirrors are a core element of many of the largest high-definition televisions and video projectors. A common technology of this type is Texas Instruments' DLP. A DLP chip is a postage stamp-sized microchip whose surface is an array of millions of microscopic mirrors. The picture is created as the individual mirrors move to either reflect light toward the projection surface (pixel on), or toward a light absorbing surface (pixel off).

Other projection technologies involving mirrors include LCoS. Like a DLP chip, LCoS is a microchip of similar size, but rather than millions of individual mirrors, there is a single mirror that is actively shielded by a liquid crystal matrix with up to millions of pixels. The picture, formed as light, is either reflected toward the projection surface (pixel on), or absorbed by the activated LCD pixels (pixel off). LCoS-based televisions and projectors often use 3 chips, one for each primary color.

Large mirrors are used in rear projection televisions. Light (for example from a DLP as mentioned above) is "folded" by one or more mirrors so that the television set is compact.

Mirrors are integral parts of a solar power plant. The one shown in the adjacent picture uses concentrated solar power from an array of parabolic troughs.

Telescopes and other precision instruments use "front silvered" or first surface mirrors, where the reflecting surface is placed on the front (or first) surface of the glass (this eliminates reflection from glass surface ordinary back mirrors have). Some of them use silver, but most are aluminium, which is more reflective at short wavelengths than silver.
All of these coatings are easily damaged and require special handling.
They reflect 90% to 95% of the incident light when new.
The coatings are typically applied by vacuum deposition.
A protective overcoat is usually applied before the mirror is removed from the vacuum, because the coating otherwise begins to corrode as soon as it is exposed to oxygen and humidity in the air. "Front silvered" mirrors have to be resurfaced occasionally to keep their quality. There are optical mirrors such as mangin mirrors that are "second surface mirrors" (reflective coating on the rear surface) as part of their optical designs, usually to correct optical aberrations.

The reflectivity of the mirror coating can be measured using a reflectometer and for a particular metal it will be different for different wavelengths of light. This is exploited in some optical work to make cold mirrors and hot mirrors. A cold mirror is made by using a transparent substrate and choosing a coating material that is more reflective to visible light and more transmissive to infrared light.

A hot mirror is the opposite, the coating preferentially reflects infrared. Mirror surfaces are sometimes given thin film overcoatings both to retard degradation of the surface and to increase their reflectivity in parts of the spectrum where they will be used. For instance, aluminum mirrors are commonly coated with silicon dioxide or magnesium fluoride. The reflectivity as a function of wavelength depends on both the thickness of the coating and on how it is applied.

For scientific optical work, dielectric mirrors are often used. These are glass (or sometimes other material) substrates on which one or more layers of dielectric material are deposited, to form an optical coating. By careful choice of the type and thickness of the dielectric layers, the range of wavelengths and amount of light reflected from the mirror can be specified. The best mirrors of this type can reflect >99.999% of the light (in a narrow range of wavelengths) which is incident on the mirror. Such mirrors are often used in lasers.

In astronomy, adaptive optics is a technique to measure variable image distortions and adapt a deformable mirror accordingly on a timescale of milliseconds, to compensate for the distortions.

Although most mirrors are designed to reflect visible light, surfaces reflecting other forms of electromagnetic radiation are also called "mirrors". The mirrors for other ranges of electromagnetic waves are used in
optics and astronomy. Mirrors for radio waves (sometimes known as reflectors) are important elements of radio telescopes.

Two or more mirrors aligned exactly parallel and facing each other can give an infinite regress of reflections, called an infinity mirror effect. Some devices use this to generate multiple reflections:

It has been said that Archimedes used a large array of mirrors to burn Roman ships during an attack on Syracuse. This has never been proven or disproved. On the TV show "MythBusters", a team from MIT tried to recreate the famous "Archimedes Death Ray". They were unsuccessful at starting a fire on the ship. Previous attempts to light the boat on fire using only the bronze mirrors available in Archimedes' time were unsuccessful, and the time taken to ignite the craft would have made its use impractical, resulting in the "MythBusters" team deeming the myth "busted". It was however found that the mirrors made it very difficult for the passengers of the targeted boat to see, likely helping to cause their defeat, which may have been the origin of the myth. (See solar power tower for a practical use of this technique.)

Due to its location in a steep-sided valley, the Italian town of Viganella gets no direct sunlight for seven weeks each winter. In 2006 a €100,000 computer-controlled mirror, 8×5 m, was installed to reflect sunlight into the town's piazza. In early 2007 the similarly situated village of Bondo, Switzerland, was considering applying this solution as well. In 2013, mirrors were installed to reflect sunlight into the town square in the Norwegian town of Rjukan. Mirrors can be used to produce enhanced lighting effects in greenhouses or conservatories.

Mirrors are a popular design theme in architecture, particularly with late modern and post-modernist high-rise buildings in major cities. Early examples include the Campbell Center in Dallas, which opened in 1972, and the John Hancock Tower in Boston.

More recently, two skyscrapers designed by architect Rafael Viñoly, the Vdara in Las Vegas and 20 Fenchurch Street in London, have experienced unusual problems due to their concave curved glass exteriors acting as respectively cylindrical and spherical reflectors for sunlight. In 2010, the Las Vegas Review Journal reported that sunlight reflected off the Vdara's south-facing tower could singe swimmers in the hotel pool, as well as melting plastic cups and shopping bags; employees of the hotel referred to the phenomenon as the "Vdara death ray", aka the "fryscraper." In 2013, sunlight reflecting off 20 Fenchurch Street melted parts of a Jaguar car parked nearby and scorching or igniting the carpet of a nearby barber shop. This building had been nicknamed the "walkie-talkie" because its shape was supposedly similar to a certain model of two-way radio; but after its tendency to overheat surrounding objects became known, the nickname changed to the "walkie-scorchie."

Painters depicting someone gazing into a mirror often also show the person's reflection. This is a kind of abstraction—in most cases the angle of view is such that the person's reflection should not be visible. Similarly, in movies and still photography an actor or actress is often shown ostensibly looking at him- or herself in the mirror, and yet the reflection faces the camera. In reality, the actor or actress sees only the camera and its operator in this case, not their own reflection. In the psychology of perception, this is known as the Venus effect.

The mirror is the central device in some of the greatest of European paintings:

Mirrors have been used by artists to create works and hone their craft:

Mirrors are sometimes necessary to fully appreciate art work:

Contemporary anamorphic artist Jonty Hurwitz uses cylindrical mirrors to project distorted sculptures.

Some other contemporary artists use mirrors as the material of art:

In the Middle Ages mirrors existed in various shapes for multiple uses. Mostly they were used as an accessory for personal hygiene but also as tokens of courtly love, made from ivory in the ivory carving centers in Paris, Cologne and the Southern Netherlands. They also had their uses in religious contexts as they were integrated in a special form of pilgrims badges or pewter/lead mirror boxes since the late 14th century. Burgundian ducal inventories show us that the dukes owned a mass of mirrors or objects with mirrors, not only with religious iconography or inscriptions, but combined with reliquaries, religious paintings or other objects that were distinctively used for personal piety. Considering mirrors in paintings and book illumination as depicted artifacts and trying to draw conclusions about their functions from their depicted setting, one of these functions is to be an aid in personal prayer to achieve self-knowledge and knowledge of God, in accord with contemporary theological sources. E.g. the famous Arnolfini-Wedding by Jan van Eyck shows a constellation of objects that can be recognized as one which would allow a praying man to use them for his personal piety: the mirror surrounded by scenes of the Passion to reflect on it and on oneself, a rosary as a device in this process, the veiled and cushioned bench to use as a prie-dieu, and the abandoned shoes that point in the direction in which the praying man kneeled. The metaphorical meaning of depicted mirrors is complex and many-layered, e.g. as an attribute of Mary, the “speculum sine macula”, or as attributes of scholarly and theological wisdom and knowledge as they appear in book illuminations of different evangelists and authors of theological treatises. Depicted mirrors – orientated on the physical properties of a real mirror – can be seen as metaphors of knowledge and reflection and are thus able to remind the beholder to reflect and get to know himself. The mirror may function simultaneously as a symbol and a device of a moral appeal. That is also the case if it is shown in combination with virtues and vices, a combination which also occurs more frequently in the 15th century: The moralizing layers of mirror metaphors remind the beholder to examine himself thoroughly according to his own virtuous or vicious life. This is all the more true if the mirror is combined with iconography of death. Not only is Death as a corpse or skeleton holding the mirror for the still living personnel of paintings, illuminations and prints, but the skull appears on the convex surfaces of depicted mirrors, showing the painted and real beholder his future face.

Mirrors are frequently used in interior decoration and as ornaments:



Mirrors play a powerful role in cultural literature.

Only a few animal species have been shown to have the ability to recognize themselves in a mirror, most of them mammals. Experiments have found that the following animals can pass the mirror test:








</doc>
