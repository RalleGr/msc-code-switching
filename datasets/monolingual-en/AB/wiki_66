<doc id="19609" url="https://en.wikipedia.org/wiki?curid=19609" title="Memory leak">
Memory leak

In computer science, a memory leak is a type of resource leak that occurs when a computer program incorrectly manages memory allocations in a way that memory which is no longer needed is not released. A memory leak may also happen when an object is stored in memory but cannot be accessed by the running code. A memory leak has symptoms similar to a number of other problems and generally can only be diagnosed by a programmer with access to the programs' source code.

A space leak occurs when a computer program uses more memory than necessary. In contrast to memory leaks, where the leaked memory is never released, the memory consumed by a space leak is released, but later than expected. 

Because they can exhaust available system memory as an application runs, memory leaks are often the cause of or a contributing factor to software aging.

A memory leak reduces the performance of the computer by reducing the amount of available memory. Eventually, in the worst case, too much of the available memory may become allocated and all or part of the system or device stops working correctly, the application fails, or the system slows down vastly due to thrashing.

Memory leaks may not be serious or even detectable by normal means. In modern operating systems, normal memory used by an application is released when the application terminates. This means that a memory leak in a program that only runs for a short time may not be noticed and is rarely serious.

Much more serious leaks include those:


The following example, written in pseudocode, is intended to show how a memory leak can come about, and its effects, without needing any programming knowledge. The program in this case is part of some very simple software designed to control an elevator. This part of the program is run whenever anyone inside the elevator presses the button for a floor.

The memory leak would occur if the floor number requested is the same floor that the elevator is on; the condition for releasing the memory would be skipped. Each time this case occurs, more memory is leaked.

Cases like this wouldn't usually have any immediate effects. People do not often press the button for the floor they are already on, and in any case, the elevator might have enough spare memory that this could happen hundreds or thousands of times. However, the elevator will eventually run out of memory. This could take months or years, so it might not be discovered despite thorough testing.

The consequences would be unpleasant; at the very least, the elevator would stop responding to requests to move to another floor (such as when an attempt is made to call the elevator or when someone is inside and presses the floor buttons). If other parts of the program need memory (a part assigned to open and close the door, for example), then someone may be trapped inside, or if no one is in, then no one would be able to use the elevator since the software cannot open the door.

The memory leak lasts until the system is reset. For example: if the elevator's power were turned off or in a power outage, the program would stop running. When power was turned on again, the program would restart and all the memory would be available again, but the slow process of memory leak would restart together with the program, eventually prejudicing the correct running of the system.

The leak in the above example can be corrected by bringing the 'release' operation outside of the conditional:

Memory leaks are a common error in programming, especially when using languages that have no built in automatic garbage collection, such as C and C++. Typically, a memory leak occurs because dynamically allocated memory has become unreachable. The prevalence of memory leak bugs has led to the development of a number of debugging tools to detect unreachable memory. "BoundsChecker", "Deleaker", "IBM Rational Purify", "Valgrind", "Parasoft Insure++", "Dr. Memory" and "memwatch" are some of the more popular memory debuggers for C and C++ programs. "Conservative" garbage collection capabilities can be added to any programming language that lacks it as a built-in feature, and libraries for doing this are available for C and C++ programs. A conservative collector finds and reclaims most, but not all, unreachable memory.

Although the memory manager can recover unreachable memory, it cannot free memory that is still reachable and therefore potentially still useful. Modern memory managers therefore provide techniques for programmers to semantically mark memory with varying levels of usefulness, which correspond to varying levels of "reachability". The memory manager does not free an object that is strongly reachable. An object is strongly reachable if it is reachable either directly by a strong reference or indirectly by a chain of strong references. (A "strong reference" is a reference that, unlike a weak reference, prevents an object from being garbage collected.) To prevent this, the developer is responsible for cleaning up references after use, typically by setting the reference to null once it is no longer needed and, if necessary, by deregistering any event listeners that maintain strong references to the object.

In general, automatic memory management is more robust and convenient for developers, as they don't need to implement freeing routines or worry about the sequence in which cleanup is performed or be concerned about whether or not an object is still referenced. It is easier for a programmer to know when a reference is no longer needed than to know when an object is no longer referenced. However, automatic memory management can impose a performance overhead, and it does not eliminate all of the programming errors that cause memory leaks.

RAII, short for Resource Acquisition Is Initialization, is an approach to the problem commonly taken in C++, D, and Ada. It involves associating scoped objects with the acquired resources, and automatically releasing the resources once the objects are out of scope. Unlike garbage collection, RAII has the advantage of knowing when objects exist and when they do not. Compare the following C and C++ examples:
/* C version */

void f(int n)

// C++ version

void f(int n)

The C version, as implemented in the example, requires explicit deallocation; the array is dynamically allocated (from the heap in most C implementations), and continues to exist until explicitly freed.

The C++ version requires no explicit deallocation; it will always occur automatically as soon as the object codice_1 goes out of scope, including if an exception is thrown. This avoids some of the overhead of garbage collection schemes. And because object destructors can free resources other than memory, RAII helps to prevent the leaking of input and output resources accessed through a handle, which mark-and-sweep garbage collection does not handle gracefully. These include open files, open windows, user notifications, objects in a graphics drawing library, thread synchronisation primitives such as critical sections, network connections, and connections to the Windows Registry or another database.

However, using RAII correctly is not always easy and has its own pitfalls. For instance, if one is not careful, it is possible to create dangling pointers (or references) by returning data by reference, only to have that data be deleted when its containing object goes out of scope.

D uses a combination of RAII and garbage collection, employing automatic destruction when it is clear that an object cannot be accessed outside its original scope, and garbage collection otherwise.

More modern garbage collection schemes are often based on a notion of reachability – if you don't have a usable reference to the memory in question, it can be collected. Other garbage collection schemes can be based on reference counting, where an object is responsible for keeping track of how many references are pointing to it. If the number goes down to zero, the object is expected to release itself and allow its memory to be reclaimed. The flaw with this model is that it doesn't cope with cyclic references, and this is why nowadays most programmers are prepared to accept the burden of the more costly mark and sweep type of systems.

The following Visual Basic code illustrates the canonical reference-counting memory leak:
Dim A, B
Set A = CreateObject("Some.Thing")
Set B = CreateObject("Some.Thing")
' At this point, the two objects each have one reference,

Set A.member = B
Set B.member = A
' Now they each have two references.

Set A = Nothing ' You could still get out of it...

Set B = Nothing ' And now you've got a memory leak!

End

In practice, this trivial example would be spotted straight away and fixed. In most real examples, the cycle of references spans more than two objects, and is more difficult to detect.

A well-known example of this kind of leak came to prominence with the rise of AJAX programming techniques in web browsers in the lapsed listener problem. JavaScript code which associated a DOM element with an event handler, and failed to remove the reference before exiting, would leak memory (AJAX web pages keep a given DOM alive for a lot longer than traditional web pages, so this leak was much more apparent).

If a program has a memory leak and its memory usage is steadily increasing, there will not usually be an immediate symptom. Every physical system has a finite amount of memory, and if the memory leak is not contained (for example, by restarting the leaking program) it will eventually cause problems.

Most modern consumer desktop operating systems have both main memory which is physically housed in RAM microchips, and secondary storage such as a hard drive. Memory allocation is dynamic – each process gets as much memory as it requests. Active pages are transferred into main memory for fast access; inactive pages are pushed out to secondary storage to make room, as needed. When a single process starts consuming a large amount of memory, it usually occupies more and more of main memory, pushing other programs out to secondary storage – usually significantly slowing performance of the system. Even if the leaking program is terminated, it may take some time for other programs to swap back into main memory, and for performance to return to normal.

When all the memory on a system is exhausted (whether there is virtual memory or only main memory, such as on an embedded system) any attempt to allocate more memory will fail. This usually causes the program attempting to allocate the memory to terminate itself, or to generate a segmentation fault. Some programs are designed to recover from this situation (possibly by falling back on pre-reserved memory). The first program to experience the out-of-memory may or may not be the program that has the memory leak.

Some multi-tasking operating systems have special mechanisms to deal with an out-of-memory condition, such as killing processes at random (which may affect "innocent" processes), or killing the largest process in memory (which presumably is the one causing the problem). Some operating systems have a per-process memory limit, to prevent any one program from hogging all of the memory on the system. The disadvantage to this arrangement is that the operating system sometimes must be re-configured to allow proper operation of programs that legitimately require large amounts of memory, such as those dealing with graphics, video, or scientific calculations.

If the memory leak is in the kernel, the operating system itself will likely fail. Computers without sophisticated memory management, such as embedded systems, may also completely fail from a persistent memory leak.

Publicly accessible systems such as web servers or routers are prone to denial-of-service attacks if an attacker discovers a sequence of operations which can trigger a leak. Such a sequence is known as an exploit.

A "sawtooth" pattern of memory utilization may be an indicator of a memory leak within an application, particularly if the vertical drops coincide with reboots or restarts of that application. Care should be taken though because garbage collection points could also cause such a pattern and would show a healthy usage of the heap.

Note that constantly increasing memory usage is not necessarily evidence of a memory leak. Some applications will store ever increasing amounts of information in memory (e.g. as a cache). If the cache can grow so large as to cause problems, this may be a programming or design error, but is not a memory leak as the information remains nominally in use. In other cases, programs may require an unreasonably large amount of memory because the programmer has assumed memory is always sufficient for a particular task; for example, a graphics file processor might start by reading the entire contents of an image file and storing it all into memory, something that is not viable where a very large image exceeds available memory.

To put it another way, a memory leak arises from a particular kind of programming error, and without access to the program code, someone seeing symptoms can only guess that there "might" be a memory leak. It would be better to use terms such as "constantly increasing memory use" where no such inside knowledge exists.

The following C function deliberately leaks memory by losing the pointer to the allocated memory. The leak can be said to occur as soon as the pointer 'a' goes out of scope, i.e. when function_which_allocates() returns without freeing 'a'.

void function_which_allocates(void) {

int main(void) {




</doc>
<doc id="19614" url="https://en.wikipedia.org/wiki?curid=19614" title="Molecular orbital">
Molecular orbital

In chemistry, a molecular orbital is a mathematical function describing the location and wave-like behavior of an electron in a molecule. This function can be used to calculate chemical and physical properties such as the probability of finding an electron in any specific region. The term "orbital" was introduced by Robert S. Mulliken in 1932 as an abbreviation for "one-electron orbital wave function". At an elementary level, it is used to describe the "region" of space in which the function has a significant amplitude. In an isolated atom, the orbital electrons' location is determined by functions called atomic orbitals. When multiple atoms combine chemically into a molecule, the electrons' locations are determined by the molecule as a whole, so the atomic orbitals combine to form molecular orbitals. The electrons from the constituent atoms occupy the molecular orbitals. Mathematically, molecular orbitals are an approximate solution to the Schrodinger equation for the electrons in the field of the molecule's atomic nuclei. They are usually constructed by combining atomic orbitals or hybrid orbitals from each atom of the molecule, or other molecular orbitals from groups of atoms. They can be quantitatively calculated using the Hartree–Fock or self-consistent field (SCF) methods.

Molecular orbitals are of three types: "bonding orbitals" which have an energy lower than the energy of the atomic orbitals which formed them, and thus promote the chemical bonds which hold the molecule together; "antibonding orbitals" which have an energy higher than the energy of their constituent atomic orbitals, and so oppose the bonding of the molecule, and "nonbonding orbitals" which have the same energy as their constituent atomic orbitals and thus have no effect on the bonding of the molecule.

A molecular orbital (MO) can be used to represent the regions in a molecule where an electron occupying that orbital is likely to be found. Molecular orbitals are approximate solutions to the Schrodinger equation for the electrons in the electric field of the molecule's atomic nuclei. However calculating the orbitals directly from this equation is far too intractable a problem. Instead they are obtained from the combination of atomic orbitals, which predict the location of an electron in an atom. A molecular orbital can specify the electron configuration of a molecule: the spatial distribution and energy of one (or one pair of) electron(s). Most commonly a MO is represented as a linear combination of atomic orbitals (the LCAO-MO method), especially in qualitative or very approximate usage. They are invaluable in providing a simple model of bonding in molecules, understood through molecular orbital theory.
Most present-day methods in computational chemistry begin by calculating the MOs of the system. A molecular orbital describes the behavior of one electron in the electric field generated by the nuclei and some average distribution of the other electrons. In the case of two electrons occupying the same orbital, the Pauli principle demands that they have opposite spin. Necessarily this is an approximation, and highly accurate descriptions of the molecular electronic wave function do not have orbitals (see configuration interaction).

Molecular orbitals are, in general, delocalized throughout the entire molecule. Moreover, if the molecule has symmetry elements, its nondegenerate molecular orbitals are either symmetric or antisymmetric with respect to any of these symmetries. In other words, application of a symmetry operation S (e.g., a reflection, rotation, or inversion) to molecular orbital ψ results in the molecular orbital being unchanged or reversing its mathematical sign: Sψ = ±ψ. In planar molecules, for example, molecular orbitals are either symmetric (sigma) or antisymmetric (pi) with respect to reflection in the molecular plane. If molecules with degenerate orbital energies are also considered, a more general statement that molecular orbitals form bases for the irreducible representations of the molecule's symmetry group holds. The symmetry properties of molecular orbitals means that delocalization is an inherent feature of molecular orbital theory and makes it fundamentally different from (and complementary to) valence bond theory, in which bonds are viewed as localized electron pairs, with allowance for resonance to account for delocalization.

In contrast to these symmetry-adapted "canonical" molecular orbitals, localized molecular orbitals can be formed by applying certain mathematical transformations to the canonical orbitals. The advantage of this approach is that the orbitals will correspond more closely to the "bonds" of a molecule as depicted by a Lewis structure. As a disadvantage, the energy levels of these localized orbitals no longer have physical meaning. (The discussion in the rest of this article will focus on canonical molecular orbitals. For further discussions on localized molecular orbitals, see: natural bond orbital and sigma-pi and equivalent-orbital models.)

Molecular orbitals arise from allowed interactions between atomic orbitals, which are allowed if the symmetries (determined from group theory) of the atomic orbitals are compatible with each other. Efficiency of atomic orbital interactions is determined from the overlap (a measure of how well two orbitals constructively interact with one another) between two atomic orbitals, which is significant if the atomic orbitals are close in energy. Finally, the number of molecular orbitals formed must be equal to the number of atomic orbitals in the atoms being combined to form the molecule.

For an imprecise, but qualitatively useful, discussion of the molecular structure, the molecular orbitals can be obtained from the "Linear combination of atomic orbitals molecular orbital method" ansatz. Here, the molecular orbitals are expressed as linear combinations of atomic orbitals.

Molecular orbitals were first introduced by Friedrich Hund and Robert S. Mulliken in 1927 and 1928. The linear combination of atomic orbitals or "LCAO" approximation for molecular orbitals was introduced in 1929 by Sir John Lennard-Jones. His ground-breaking paper showed how to derive the electronic structure of the fluorine and oxygen molecules from quantum principles. This qualitative approach to molecular orbital theory is part of the start of modern quantum chemistry.
Linear combinations of atomic orbitals (LCAO) can be used to estimate the molecular orbitals that are formed upon bonding between the molecule's constituent atoms. Similar to an atomic orbital, a Schrödinger equation, which describes the behavior of an electron, can be constructed for a molecular orbital as well. Linear combinations of atomic orbitals, or the sums and differences of the atomic wavefunctions, provide approximate solutions to the Hartree–Fock equations which correspond to the independent-particle approximation of the molecular Schrödinger equation. For simple diatomic molecules, the wavefunctions obtained are represented mathematically by the equations

where formula_3 and formula_4 are the molecular wavefunctions for the bonding and antibonding molecular orbitals, respectively, formula_5 and formula_6 are the atomic wavefunctions from atoms a and b, respectively, and formula_7 and formula_8 are adjustable coefficients. These coefficients can be positive or negative, depending on the energies and symmetries of the individual atomic orbitals. As the two atoms become closer together, their atomic orbitals overlap to produce areas of high electron density, and, as a consequence, molecular orbitals are formed between the two atoms. The atoms are held together by the electrostatic attraction between the positively charged nuclei and the negatively charged electrons occupying bonding molecular orbitals.

When atomic orbitals interact, the resulting molecular orbital can be of three types: bonding, antibonding, or nonbonding.

Bonding MOs:

Antibonding MOs: 
Nonbonding MOs: 

The type of interaction between atomic orbitals can be further categorized by the molecular-orbital symmetry labels σ (sigma), π (pi), δ (delta), φ (phi), γ (gamma) etc. These are the Greek letters corresponding to the atomic orbitals s, p, d, f and g respectively. The number of nodal planes containing the internuclear axis between the atoms concerned is zero for σ MOs, one for π, two for δ, three for φ and four for γ.

A MO with σ symmetry results from the interaction of either two atomic s-orbitals or two atomic p-orbitals. An MO will have σ-symmetry if the orbital is symmetric with respect to the axis joining the two nuclear centers, the internuclear axis. This means that rotation of the MO about the internuclear axis does not result in a phase change. A σ* orbital, sigma antibonding orbital, also maintains the same phase when rotated about the internuclear axis. The σ* orbital has a nodal plane that is between the nuclei and perpendicular to the internuclear axis.

A MO with π symmetry results from the interaction of either two atomic p orbitals or p orbitals. An MO will have π symmetry if the orbital is asymmetric with respect to rotation about the internuclear axis. This means that rotation of the MO about the internuclear axis will result in a phase change. There is one nodal plane containing the internuclear axis, if real orbitals are considered.

A π* orbital, pi antibonding orbital, will also produce a phase change when rotated about the internuclear axis. The π* orbital also has a second nodal plane between the nuclei.

A MO with δ symmetry results from the interaction of two atomic d or d orbitals. Because these molecular orbitals involve low-energy d atomic orbitals, they are seen in transition-metal complexes. A δ bonding orbital has two nodal planes containing the internuclear axis, and a δ* antibonding orbital also has a third nodal plane between the nuclei.

Theoretical chemists have conjectured that higher-order bonds, such as phi bonds corresponding to overlap of f atomic orbitals, are possible. There is as of 2005 only one known example of a molecule purported to contain a phi bond (a U−U bond, in the molecule U).

For molecules that possess a center of inversion (centrosymmetric molecules) there are additional labels of symmetry that can be applied to molecular orbitals.
Centrosymmetric molecules include:

Non-centrosymmetric molecules include:
If inversion through the center of symmetry in a molecule results in the same phases for the molecular orbital, then the MO is said to have gerade (g) symmetry, from the German word for even.
If inversion through the center of symmetry in a molecule results in a phase change for the molecular orbital, then the MO is said to have ungerade (u) symmetry, from the German word for odd.
For a bonding MO with σ-symmetry, the orbital is σ (s' + s<nowiki>"</nowiki> is symmetric), while an antibonding MO with σ-symmetry the orbital is σ, because inversion of s' – s<nowiki>"</nowiki> is antisymmetric.
For a bonding MO with π-symmetry the orbital is π because inversion through the center of symmetry for would produce a sign change (the two p atomic orbitals are in phase with each other but the two lobes have opposite signs), while an antibonding MO with π-symmetry is π because inversion through the center of symmetry for would not produce a sign change (the two p orbitals are antisymmetric by phase).

The qualitative approach of MO analysis uses a molecular orbital diagram to visualize bonding interactions in a molecule. In this type of diagram, the molecular orbitals are represented by horizontal lines; the higher a line the higher the energy of the orbital, and degenerate orbitals are placed on the same level with a space between them. Then, the electrons to be placed in the molecular orbitals are slotted in one by one, keeping in mind the Pauli exclusion principle and Hund's rule of maximum multiplicity (only 2 electrons, having opposite spins, per orbital; place as many unpaired electrons on one energy level as possible before starting to pair them). For more complicated molecules, the wave mechanics approach loses utility in a qualitative understanding of bonding (although is still necessary for a quantitative approach). 
Some properties:

The general procedure for constructing a molecular orbital diagram for a reasonably simple molecule can be summarized as follows:

1. Assign a point group to the molecule.

2. Look up the shapes of the SALCs.

3. Arrange the SALCs of each molecular fragment in increasing order of energy, first noting whether they stem from "s", "p", or "d" orbitals 
(and put them in the order "s" < "p" < "d"), and then their number of internuclear nodes.

4. Combine SALCs of the same symmetry type from the two fragments, and from N SALCs form N molecular orbitals.

5. Estimate the relative energies of the molecular orbitals from considerations of overlap and relative energies of the parent orbitals, and draw the levels on a molecular orbital energy level diagram (showing the origin of the orbitals).

6. Confirm, correct, and revise this qualitative order by carrying out a molecular orbital calculation by using commercial software.

Molecular orbitals are said to be degenerate if they have the same energy. For example, in the homonuclear diatomic molecules of the first ten elements, the molecular orbitals derived from the p and the p atomic orbitals result in two degenerate bonding orbitals (of low energy) and two degenerate antibonding orbitals (of high energy).

When the energy difference between the atomic orbitals of two atoms is quite large, one atom's orbitals contribute almost entirely to the bonding orbitals, and the other atom's orbitals contribute almost entirely to the antibonding orbitals. Thus, the situation is effectively that one or more electrons have been transferred from one atom to the other. This is called an (mostly) ionic bond.

The bond order, or number of bonds, of a molecule can be determined by combining the number of electrons in bonding and antibonding molecular orbitals. A pair of electrons in a bonding orbital creates a bond, whereas a pair of electrons in an antibonding orbital negates a bond. For example, N, with eight electrons in bonding orbitals and two electrons in antibonding orbitals, has a bond order of three, which constitutes a triple bond.

Bond strength is proportional to bond order—a greater amount of bonding produces a more stable bond—and bond length is inversely proportional to it—a stronger bond is shorter.

There are rare exceptions to the requirement of molecule having a positive bond order. Although Be has a bond order of 0 according to MO analysis, there is experimental evidence of a highly unstable Be molecule having a bond length of 245 pm and bond energy of 10 kJ/mol.

The highest occupied molecular orbital and lowest unoccupied molecular orbital are often referred to as the HOMO and LUMO, respectively. The difference of the energies of the HOMO and LUMO is called the HOMO-LUMO gap. This notion is often the matter of confusion in literature and should be considered with caution. Its value is usually located between the fundamental gap (difference between ionization potential and electron affinity) and the optical gap. In addition, HOMO-LUMO gap can be related to a bulk material band gap or transport gap, which is usually much smaller than fundamental gap.

Homonuclear diatomic MOs contain equal contributions from each atomic orbital in the basis set. This is shown in the homonuclear diatomic MO diagrams for H, He, and Li, all of which containing symmetric orbitals.

As a simple MO example, consider the electrons in a hydrogen molecule, H (see molecular orbital diagram), with the two atoms labelled H' and H". The lowest-energy atomic orbitals, 1s' and 1s", do not transform according to the symmetries of the molecule. However, the following symmetry adapted atomic orbitals do:

The symmetric combination (called a bonding orbital) is lower in energy than the basis orbitals, and the antisymmetric combination (called an antibonding orbital) is higher. Because the H molecule has two electrons, they can both go in the bonding orbital, making the system lower in energy (hence more stable) than two free hydrogen atoms. This is called a covalent bond. The bond order is equal to the number of bonding electrons minus the number of antibonding electrons, divided by 2. In this example, there are 2 electrons in the bonding orbital and none in the antibonding orbital; the bond order is 1, and there is a single bond between the two hydrogen atoms.

On the other hand, consider the hypothetical molecule of He with the atoms labeled He' and He". As with H, the lowest energy atomic orbitals are the 1s' and 1s", and do not transform according to the symmetries of the molecule, while the symmetry adapted atomic orbitals do. The symmetric combination—the bonding orbital—is lower in energy than the basis orbitals, and the antisymmetric combination—the antibonding orbital—is higher. Unlike H, with two valence electrons, He has four in its neutral ground state. Two electrons fill the lower-energy bonding orbital, σ(1s), while the remaining two fill the higher-energy antibonding orbital, σ*(1s). Thus, the resulting electron density around the molecule does not support the formation of a bond between the two atoms; without a stable bond holding the atoms together, the molecule would not be expected to exist. Another way of looking at it is that there are two bonding electrons and two antibonding electrons; therefore, the bond order is 0 and no bond exists (the molecule has one bound state supported by the Van der Waals potential).

Dilithium Li is formed from the overlap of the 1s and 2s atomic orbitals (the basis set) of two Li atoms. Each Li atom contributes three electrons for bonding interactions, and the six electrons fill the three MOs of lowest energy, σ(1s), σ*(1s), and σ(2s). Using the equation for bond order, it is found that dilithium has a bond order of one, a single bond.

Considering a hypothetical molecule of He, since the basis set of atomic orbitals is the same as in the case of H, we find that both the bonding and antibonding orbitals are filled, so there is no energy advantage to the pair. HeH would have a slight energy advantage, but not as much as H + 2 He, so the molecule is very unstable and exists only briefly before decomposing into hydrogen and helium. In general, we find that atoms such as He that have full energy shells rarely bond with other atoms. Except for short-lived Van der Waals complexes, there are very few noble gas compounds known.

While MOs for homonuclear diatomic molecules contain equal contributions from each interacting atomic orbital, MOs for heteronuclear diatomics contain different atomic orbital contributions. Orbital interactions to produce bonding or antibonding orbitals in heteronuclear diatomics occur if there is sufficient overlap between atomic orbitals as determined by their symmetries and similarity in orbital energies.

In hydrogen fluoride HF overlap between the H 1s and F 2s orbitals is allowed by symmetry but the difference in energy between the two atomic orbitals prevents them from interacting to create a molecular orbital. Overlap between the H 1s and F 2p orbitals is also symmetry allowed, and these two atomic orbitals have a small energy separation. Thus, they interact, leading to creation of σ and σ* MOs and a molecule with a bond order of 1. Since HF is a non-centrosymmetric molecule, the symmetry labels g and u do not apply to its molecular orbitals.

To obtain quantitative values for the molecular energy levels, one needs to have molecular orbitals that are such that the configuration interaction (CI) expansion converges fast towards the full CI limit. The most common method to obtain such functions is the Hartree–Fock method, which expresses the molecular orbitals as eigenfunctions of the Fock operator. One usually solves this problem by expanding the molecular orbitals as linear combinations of Gaussian functions centered on the atomic nuclei (see linear combination of atomic orbitals and basis set (chemistry)). The equation for the coefficients of these linear combinations is a generalized eigenvalue equation known as the Roothaan equations, which are in fact a particular representation of the Hartree–Fock equation. There are a number of programs in which quantum chemical calculations of MOs can be performed, including Spartan and HyperChem.

Simple accounts often suggest that experimental molecular orbital energies can be obtained by the methods of ultra-violet photoelectron spectroscopy for valence orbitals and X-ray photoelectron spectroscopy for core orbitals. This, however, is incorrect as these experiments measure the ionization energy, the difference in energy between the molecule and one of the ions resulting from the removal of one electron. Ionization energies are linked approximately to orbital energies by Koopmans' theorem. While the agreement between these two values can be close for some molecules, it can be very poor in other cases.


</doc>
<doc id="19615" url="https://en.wikipedia.org/wiki?curid=19615" title="Systems Concepts">
Systems Concepts

Systems Concepts (now the SC Group) is a company co-founded by Stewart Nelson and Mike Levitt focused on making hardware products related to the DEC PDP-10 series of computers. One of its major products was the SA-10, an interface which allowed PDP-10s to be connected to disk and tape drives designed for use with the channel interfaces of IBM mainframes.

Later, Systems Concepts attempted to produce a compatible replacement for the DEC PDP-10 computers. "Mars" was the code name for a family of PDP-10-compatible computers built by Systems Concepts, including the initial SC-30M, the smaller SC-25, and the slower SC-20. These machines were marvels of engineering design; although not much slower than the unique Foonly F-1, they were physically smaller and consumed less power than the much slower DEC KS10 or Foonly F-2, F-3, or F-4 machines. They were also completely compatible with the DEC KL10, and ran all KL10 binaries (including the operating system) with no modifications at about 2-3 times faster than a KL10.

When DEC cancelled the Jupiter project in 1983, Systems Concepts hoped to sell their machine to customers with a software investment in PDP-10s. Their spring 1984 announcement generated excitement in the PDP-10 world. TOPS-10 was running on the Mars by the summer of 1984, and TOPS-20 by early fall. However, people at Systems Concepts were better at designing machines than at mass-producing or selling them; the company continually improved the design, but lost credibility as delivery dates continued to slip. They also overpriced; believing they were competing with the KL10 and VAX 8600 and not startups such as Sun Microsystems building workstations with comparable power at a fraction of the price. By the time SC shipped the first SC-30M to Stanford University in late 1985, most customers had already abandoned the PDP-10, usually for VMS or Unix systems. Nevertheless, a number were purchased by CompuServe, which depended on PDP-10s to run its online service and was eager to move to newer but fully compatible systems. CompuServe's demand for the computers outpaced Systems Concepts' ability to produce them, so CompuServe licensed the design and built SC-designed computers itself.
Other companies that purchased the SC-30 machines included Telmar, Reynolds and Reynolds, The Danish National Railway.

Peter Samson was director of marketing and program development.

SC later designed the SC-40, released in 1993, a faster follow-on to the SC-30M and SC-25. It can perform up to 8 times as fast as a DEC KL-10, and it also supports more physical memory, a larger virtual address space, and more modern input/output devices. These systems were also used at CompuServe.

In 1985, the company contracted to engineer and produce a PC-based cellular automata system for Tommaso Toffoli of MIT, called the CAM-6. The CAM-6 was a 2-card "sandwich" that plugged into an IBM PC slot and ran cellular automata rules at a 60 Hz update rate. Toffoli provided Forth-based software to operate the card. The production problems that plagued the company's computer products were demonstrated here as well, and only a few boards were produced.

Systems Concepts remains in business, having changed its name to the SC Group when it moved from California to Nevada.


</doc>
<doc id="19616" url="https://en.wikipedia.org/wiki?curid=19616" title="Messiah">
Messiah

In Abrahamic religions, a messiah or messias (; , ) is a saviour or liberator of a group of people.
The concepts of "mashiach", messianism, and of a Messianic Age originated in Judaism, and in the Hebrew Bible; a "mashiach" (messiah) is a king or High Priest traditionally anointed with holy anointing oil. Messiahs were not exclusively Jewish: the Book of Isaiah refers to Cyrus the Great, king of the Achaemenid Empire, as a messiah for his decree to rebuild the Jerusalem Temple.

"Ha mashiach" (המשיח, 'the Messiah', 'the anointed one'), often referred to as "" (מלך המשיח 'King Messiah'), is to be a human leader, physically descended from the paternal Davidic line through King David and King Solomon. He is thought to accomplish predetermined things in only one future arrival, including the unification of the tribes of Israel, the gathering of all Jews to "Eretz Israel", the rebuilding of the Temple in Jerusalem, the ushering in of a Messianic Age of global universal peace, and the annunciation of the world to come.

In Christianity, the Messiah is called the Christ, from , translating the Hebrew word of the same meaning. The concept of the Messiah in Christianity originated from the Messiah in Judaism. However, unlike the concept of the Messiah in Judaism, Jesus is considered by Christians additionally to be the Son of God. Christ became the accepted Christian designation and title of Jesus of Nazareth, because Christians believe that the messianic prophecies in the Old Testament were fulfilled in his mission, death, and resurrection. These specifically include the prophecies of him being descended from the Davidic line, and being declared King of the Jews which happened on the day of his crucifixion.
They believe that Christ will fulfill the rest of the messianic prophecies, specifically that he will usher in a Messianic Age and the world to come at his Second Coming. Some Christian denominations, such as Catholicism, instead believe in amillenialist theology, but the Catholic Church has not adopted this term.

In Islam, Jesus was a prophet and the "Masîḥ" (مسيح), the Messiah sent to the Israelites, and he will return to Earth at the end of times, along with the "Mahdi", and defeat "al-Masih ad-Dajjal", the false Messiah.
In Ahmadiyya theology, these prophecies concerning the Mahdi and the second coming of Jesus have been fulfilled in Mirza Ghulam Ahmad (1835–1908), the founder of the Ahmadiyya Movement, and the terms 'Messiah' and 'Mahdi' are synonyms for one and the same person.

In Chabad messianism, Yosef Yitzchak Schneersohn (r. 1920–1950), sixth "Rebbe" (spiritual leader) of Chabad Lubavitch, and Menachem Mendel Schneerson (1902–1994), seventh "Rebbe" of Chabad, are Messiah claimants. Resembling early Christianity, the deceased Schneerson is believed to be the Messiah among some adherents of the Chabad movement; his second coming is believed to be imminent.

Messiah (; in modern Jewish texts in English spelled "Mashiach"; , , , , , , ) literally means "anointed one". In Hebrew, the Messiah is often referred to as מלך המשיח ("" in the Tiberian vocalization, , literally meaning "the Anointed King".)

The Greek Septuagint version of the Old Testament renders all thirty-nine instances of the Hebrew word for "anointed" ("Mašíaḥ") as Χριστός ("Khristós"). The New Testament records the Greek transliteration Μεσσίας, "Messias" twice in John.

"al-Masīḥ" (proper name, ) is the Arabic word for messiah. In modern Arabic, it is used as one of the many titles of Jesus. "Masīḥ" is used by Arab Christians as well as Muslims, and is written as "Yasūʿ al-Masih" (يسوع المسيح) by Arab Christians or "ʿĪsā al-Masīḥ" (عيسى المسيح) by Muslims. The word "al-Masīḥ" literally means "the anointed", "the traveller", or the "one who cures by caressing".

The literal translation of the Hebrew word "mashiach" (messiah) is "anointed", which refers to a ritual of consecrating someone or something by putting holy oil upon it. It is used throughout the Hebrew Bible in reference to a wide variety of individuals and objects; for example, kings, priests and prophets, the altar in the Temple, vessels, unleavened bread, and even a non-Jewish king (Cyrus the Great).

In Jewish eschatology, the term came to refer to a future Jewish king from the Davidic line, who will be "anointed" with holy anointing oil, to be king of God's kingdom, and rule the Jewish people during the Messianic Age. In Judaism, the Messiah is not considered to be God or a pre-existent divine Son of God. He is considered to be a great political leader that has descended from King David. That is why he is referred to as Messiah ben David, which means "Messiah, son of David". The messiah, in Judaism, is considered to be a great, charismatic leader that is well oriented with the laws that are followed in Judaism. He will be the one who will not "judge by what his eyes see" or "decide by what his ears hear".

Belief in the eventual coming of a future messiah is a fundamental part of Judaism, and is one of Maimonides' 13 Principles of Faith.

Maimonides describes the identity of the Messiah in the following terms:
Even though the eventual coming of the messiah is a strongly upheld belief in Judaism, trying to predict the actual time when the messiah will come is an act that is frowned upon. These kinds of actions are thought to weaken the faith the people have in the religion. So in Judaism, there is no specific time when the messiah comes. Rather, it is the acts of the people that determines when the messiah comes. It is said that the messiah would come either when the world needs his coming the most (when the world is so sinful and in desperate need of saving by the messiah) or deserves it the most (when genuine goodness prevails in the world).

A common modern rabbinic interpretation is that there is a "potential" messiah in every generation. The Talmud, which often uses stories to make a moral point ("aggadah"), tells of a highly respected rabbi who found the Messiah at the gates of Rome and asked him, "When will you finally come?" He was quite surprised when he was told, "Today." Overjoyed and full of anticipation, the man waited all day. The next day he returned, disappointed and puzzled, and asked, "You said messiah would come 'today' but he didn't come! What happened?" The Messiah replied, "Scripture says, 'Today, if you will but hearken to his voice.'"

A Kabbalistic tradition within Judaism is that the commonly discussed messiah who will usher in a period of freedom and peace, Messiah ben David, will be preceded by Messiah ben Joseph, who will gather the children of Israel around him, lead them to Jerusalem. After overcoming the hostile powers in Jerusalem, Messiah ben Joseph, will reestablish the Temple-worship and set up his own dominion. Then Armilus, according to one group of sources, or Gog and Magog, according to the other, will appear with their hosts before Jerusalem, wage war against Messiah ben Joseph, and slay him. His corpse, according to one group, will lie unburied in the streets of Jerusalem; according to the other, it will be hidden by the angels with the bodies of the Patriarchs, until Messiah ben David comes and brings him back to life.

Yosef Yitzchak Schneersohn (r. 1920 - 1950), sixth "Rebbe" (spiritual leader) of Chabad Lubavitch, and Menachem Mendel Schneerson (1902 - 1994), seventh "Rebbe" of Chabad, are messiah claimants.

As per Chabad-Lubavitch messianism, Menachem Mendel Schneerson openly declared his deceased father-in-law, the former 6th "Rebbe" of Chabad Lubavitch, to be the Messiah. He published about Yosef Yitzchak Schneersohn to be ""Atzmus u'mehus alein vi er hat zich areingeshtalt in a guf"" (Yiddish and English for: "Essence and Existence [of God] which has placed itself in a body"). The gravesite of his deceased father-in-law Yosef Yitzchak Schneersohn, known as "the "Ohel"", became a central point of focus for Menachem Mendel Schneerson's prayers and supplications.

Regarding the deceased Menachem Mendel Schneerson, a later Chabad Halachic ruling claims that it was "incumbent on every single Jew to heed the Rebbe's words and believe that he is indeed King Moshiach, who will be revealed imminently". Outside of Chabad messianism, in Judaism, there is no basis to these claims. If anything, this resembles the faith in the resurrection of Jesus and his second coming in early Christianity.

Still today, the deceased rabbi Menachem Mendel Schneerson is believed to be the Messiah among adherents of the Chabad movement, and his second coming is believed to be imminent. He is venerated and invocated to by thousands of visitors and letters each year at two bamot-tombs ("Ohel"), especially in a pilgrimage each year on the anniversary of his death.

The Greek translation of Messiah is "khristos" (), anglicized as "Christ", and Christians commonly refer to Jesus as either the "Christ" or the "Messiah". Christians believe that messianic prophecies were fulfilled in the mission, death, and resurrection of Jesus and that he will return to fulfill the rest of messianic prophecies.

The majority of historical and mainline Christian theologies consider Jesus to be the Son of God and God the Son, a concept of the Messiah fundamentally different from the Jewish and Islamic concepts. In each of the four New Testament Gospels, the only literal anointing of Jesus is conducted by a woman. In the Gospels of Mark, Matthew, and John, this anointing occurs in Bethany, outside Jerusalem. In the Gospel of Luke, the anointing scene takes place at an indeterminate location, but the context suggests it to be in Galilee, or even a separate anointing altogether.

While the term "messiah" does appear in Islam, the meaning is different from that found in Christianity and Judaism. "Though Islam shares many of the beliefs and characteristics of the two Semitic/Abrahamic/monotheistic religions which preceded it, the idea of messianism, which is of central importance in Judaism and Christianity, is alien to Islam as represented by the Qur'an."

The Quran identifies Jesus (Isa) as the messiah ("Masih"), who will one day return to earth. At the time of the second coming, "according to Islamic tradition, Jesus will come again and exercise his power of healing. He will forever destroy falsehood, as embodied in the Daj-jal, the great falsifier, the anti-Christ. Then God will reign forever."

Jesus is one of the most important prophets in the Islamic tradition, along with Noah, Abraham, Moses, and Muhammad. Unlike Christians, Muslims see Jesus as a prophet, but not as God himself or the son of God. Like all other prophets, Jesus is an ordinary man, who receives revelations from God. According to religious scholar Mona Siddiqui, in Islam, "Prophecy allows God to remain veiled and there is no suggestion in the Qur'an that God wishes to reveal of himself just yet. Prophets guarantee interpretation of revelation and that God's message will be understood." Prophecy in human form does not represent the true powers of God, contrary to the way Jesus is depicted in mainstream Christianity.

The Quran states that Isa, the Son of Maryam (Arabic: "Isa ibn Maryam"), is the messiah and prophet sent to the Children of Israel. The birth of Isa is described in Quran sura 19 verses 1–33, and sura 4 verse 171 explicitly states Isa as the Son of Maryam. Sunni Muslims believe Isa is alive in Heaven and did not die in the crucifixion. According to religious scholar Mahmoud Ayoub, "Jesus' close proximity or nearness (qurb) to God is affirmed in the Qur'anic insistence that Jesus did not die, but was taken up to God and remains with God" The Quran in sura 4 verse 157-158 states that: "They did not kill him, nor did they crucify him, but they thought they did" since the messiah was "made to resemble him to them."

It is believed that Isa will return to Earth to defeat the Masih ad-Dajjal (false Messiah), a figure similar to the Antichrist in Christianity, who will emerge shortly before "Yawm al-Qiyāmah" ("the Day of Resurrection"). The "Mahdi" will come shortly before the second coming of Jesus. After he has destroyed ad-Dajjal, his final task will be to become leader of the Muslims. Isa will unify the Muslim "Ummah" (the followers of Islam) under the common purpose of worshipping Allah alone in pure Islam, thereby ending divisions and deviations by adherents. Mainstream Muslims believe that at that time Isa will dispel Christian and Jewish claims about him.

A "hadith" in Abu Dawud () says:

The Prophet said: There is no prophet between me and him, that is, Isa. He will descend (to the earth). When you see him, recognise him: a man of medium height, reddish fair, wearing two light yellow garments, looking as if drops were falling down from his head though it will not be wet. He will fight the people for the cause of Islam. He will break the cross, kill swine, and abolish jizyah. Allah will perish all religions except Islam. He will destroy the Antichrist and will live on the earth for forty years and then he will die. The Muslims will pray over him.
Both Sunni and Shia Muslims agree that al-Mahdi will arrive first, and after him, Isa. Isa will proclaim al-Mahdi as the Islamic community leader. A war will be fought—the Dajjal against al-Mahdi and Isa. This war will mark the approach of the coming of the Last Day. After Isa slays al-Dajjāl at the Gate of Lud, he will bear witness and reveal that Islam is indeed the true and last word from God to humanity as Yusuf Ali's translation reads: "And there is none of the People of the Book but must believe in him before his death; and on the Day of Judgment he will be a witness against them." A "hadith" in Sahih Bukhari says:
"Allah's Apostle said "How will you be when the son of Mariam descends among you and your Imam is from among you?" "

The Quran denies the crucifixion of Jesus, claiming that he was neither killed nor crucified. The Quran also emphasizes the difference between Allah (God in Arabic) and the Messiah:
"Those who say that Allah is the Messiah, son of Mary, are unbelievers. The Messiah said: "O Children of Israel, worship Allah, my Lord and your Lord... unbelievers too are those who have said that Allah is the third of three... the Messiah, son of Mary, was only a Messenger before whom other Messengers had gone."

Twelver Shi'i Islam, which significantly values and revolves around the 12 spiritual leaders called Imams, differs significantly from the beliefs of Sunni Islam. Unlike Sunni Islam, "Messianism is an essential part of religious belief and practice for almost all Shi'a Muslims." Shi'i Islam believes that the last Imam will return again, with the return of Jesus. According to religious scholar Mona Sidique, "Shi'is are acutely aware of the existence everywhere of the twelfth Imam, who disappeared in 874. Shi'i piety teaches that the hidden Imam will return with Jesus Christ to set up the messianic kingdom before the final Judgement Day, when all humanity will stand before God. There is some controversy as to the identity of this imam. There are sources that underscore how the Shia sect agrees with the Jews and Christians that Imam Mehdi (al-Mahdi) is another name for Elijah, whose return prior to the arrival of the Messiah was prophesied in the Old Testament. 

The Imams and Fatima will have a direct impact on the judgements rendered that day. This will represent the ultimate intercession." There is debate on whether Shi'i Muslims should accept the death of Jesus. Religious scholar Mahmou Ayoub argues "Modern Shi'i thinkers have allowed the possibility that Jesus died and only his spirit was taken up to heaven." Conversely, religious scholar Mona Siddiqui argues that Shi'i thinkers believe Jesus was "neither crucified nor slain." She also argues that Shi'i Muslims believe that the twelfth imam did not die, but "was taken to God to return in God's time," and "will return at the end of history to establish the kingdom of God on earth as the expected Mahdi."

According to Ahmadiyya thought, Messiahship is a phenomenon through which a special emphasis is given on the transformation of a people by way of offering to suffer for the sake of God instead of giving suffering (i.e. refraining from revenge). Ahmadis believe that this special emphasis was given through the person of Jesus and Mirza Ghulam Ahmad (1835–1908) among others.

Ahmadis hold that the prophesied eschatological figures of Christianity and Islam, the Messiah and Mahdi, were, in fact, to be fulfilled in one person who was to represent all previous prophets.

Numerous hadith are presented by the Ahmadis in support of their view, such as one from Sunan Ibn Majah, which says, "There is No Mahdi other than Jesus son of Mary."

Ahmadis believe that the prophecies concerning the Mahdi and the second coming of Jesus have been fulfilled in Mirza Ghulam Ahmad (1835–1908), the founder of the Ahmadiyya Movement. Unlike mainstream Muslims, the Ahmadis do not believe that Jesus is alive in heaven, but that he survived the crucifixion and migrated towards the east where he died a natural death and that Ghulam Ahmad was only the promised spiritual second coming and likeness of Jesus, the promised Messiah and Mahdi. He also claimed to have appeared in the likeness of Krishna and that his advent fulfilled certain prophecies found in Hindu scriptures. He stated that the founder of Sikhism was a Muslim saint, who was a reflection of the religious challenges he perceived to be occurring. Ghulam Ahmad wrote "Barahin-e-Ahmadiyya", in 1880, which incorporated Indian, Sufi, Islamic and Western aspects in order to give life to Islam in the face of the British Raj, Protestant Christianity, and rising Hinduism. He later declared himself the Promised Messiah and the Mahdi following Divine revelations in 1891. Ghulam Ahmad argued that Jesus had appeared 1300 years after the formation of the Muslim community and stressed the need for a current Messiah, in turn claiming that he himself embodied both the Mahdi and the Messiah. Ghulam Ahmad was supported by Muslims who especially felt oppressed by Christian and Hindu missionaries.


The following works include the concept of a messiah as a leader of a cause or liberator of a people:


Footnotes
Citations



</doc>
<doc id="19617" url="https://en.wikipedia.org/wiki?curid=19617" title="Margaret Mead">
Margaret Mead

Margaret Mead (December 16, 1901 – November 15, 1978) was an American cultural anthropologist who featured frequently as an author and speaker in the mass media during the 1960s and 1970s. She earned her bachelor's degree at Barnard College in New York City and her MA and PhD degrees from Columbia University. Mead served as President of the American Association for the Advancement of Science in 1975.

Mead was a communicator of anthropology in modern American and Western culture and was often controversial as an academic. Her reports detailing the attitudes towards sex in South Pacific and Southeast Asian traditional cultures influenced the 1960s sexual revolution. She was a proponent of broadening sexual conventions within the context of Western cultural traditions.

Margaret Mead, the first of five children, was born in Philadelphia, but raised in nearby Doylestown, Pennsylvania. Her father, Edward Sherwood Mead, was a professor of finance at the Wharton School of the University of Pennsylvania, and her mother, Emily (née Fogg) Mead, was a sociologist who studied Italian immigrants. Her sister Katharine (1906–1907) died at the age of nine months. This was a traumatic event for Mead, who had named the girl, and thoughts of her lost sister permeated her daydreams for many years. Her family moved frequently, so her early education was directed by her grandmother until, at age 11, she was enrolled by her family at Buckingham Friends School in Lahaska, Pennsylvania. Her family owned the Longland farm from 1912 to 1926. Born into a family of various religious outlooks, she searched for a form of religion that gave an expression of the faith that she had been formally acquainted with, Christianity. In doing so, she found the rituals of the Episcopal Church to fit the expression of religion she was seeking. Mead studied one year, 1919, at DePauw University, then transferred to Barnard College where she found anthropology mired in "the stupid underbrush of nineteenth century arguments."

Mead earned her bachelor's degree from Barnard in 1923, then began studying with professor Franz Boas and Ruth Benedict at Columbia University, earning her master's degree in 1924. Mead set out in 1925 to do fieldwork in Samoa. In 1926, she joined the American Museum of Natural History, New York City, as assistant curator. She received her PhD from Columbia University in 1929.

Before departing for Samoa, Mead had a short affair with the linguist Edward Sapir, a close friend of her instructor Ruth Benedict. But Sapir's conservative ideas about marriage and the woman's role were unacceptable to Mead, and as Mead left to do field work in Samoa the two separated permanently. Mead received news of Sapir's remarriage while living in Samoa, where, on a beach, she later burned their correspondence.

Mead was married three times. After a six-year engagement, she married her first husband (1923–1928) American Luther Cressman, a theology student at the time who eventually became an anthropologist. Between 1925 and 1926 she was in Samoa returning wherefrom on the boat she met Reo Fortune, a New Zealander headed to Cambridge, England, to study psychology. They were married in 1928, after Mead's divorce from Cressman. Mead dismissively characterized her union with her first husband as "my student marriage" in her 1972 autobiography "Blackberry Winter", a sobriquet with which Cressman took vigorous issue. Mead's third and longest-lasting marriage (1936–1950) was to the British anthropologist Gregory Bateson, with whom she had a daughter, Mary Catherine Bateson, who would also become an anthropologist.

Mead's pediatrician was Benjamin Spock, whose subsequent writings on child rearing incorporated some of Mead's own practices and beliefs acquired from her ethnological field observations which she shared with him; in particular, breastfeeding on the baby's demand rather than a schedule. She readily acknowledged that Gregory Bateson was the husband she loved the most. She was devastated when he left her, and she remained his loving friend ever after, keeping his photograph by her bedside wherever she traveled, including beside her hospital deathbed. 

Mead also had an exceptionally close relationship with Ruth Benedict, one of her instructors. In her memoir about her parents, "With a Daughter's Eye", Mary Catherine Bateson implies that the relationship between Benedict and Mead was partly sexual. Mead never openly identified herself as lesbian or bisexual. In her writings, she proposed that it is to be expected that an individual's sexual orientation may evolve throughout life.

She spent her last years in a close personal and professional collaboration with anthropologist Rhoda Metraux, with whom she lived from 1955 until her death in 1978. Letters between the two published in 2006 with the permission of Mead's daughter clearly express a romantic relationship.

Mead had two sisters and a brother, Elizabeth, Priscilla, and Richard. Elizabeth Mead (1909–1983), an artist and teacher, married cartoonist William Steig, and Priscilla Mead (1911–1959) married author Leo Rosten. Mead's brother, Richard, was a professor. Mead was also the aunt of Jeremy Steig.

During World War II, Mead served as executive secretary of the National Research Council's Committee on Food Habits. She served as curator of ethnology at the American Museum of Natural History from 1946 to 1969. She was elected a Fellow of the American Academy of Arts and Sciences in 1948. She taught at The New School and Columbia University, where she was an adjunct professor from 1954 to 1978 and was a professor of anthropology and chair of the Division of Social Sciences at Fordham University's Lincoln Center campus from 1968 to 1970, founding their anthropology department. In 1970, she joined the faculty of the University of Rhode Island as a Distinguished Professor of Sociology and Anthropology.

Following Ruth Benedict's example, Mead focused her research on problems of child rearing, personality, and culture. She served as president of the Society for Applied Anthropology in 1950 and of the American Anthropological Association in 1960. In the mid-1960s, Mead joined forces with communications theorist Rudolf Modley, jointly establishing an organization called Glyphs Inc., whose goal was to create a universal graphic symbol language to be understood by any members of culture, no matter how primitive. In the 1960s, Mead served as the Vice President of the New York Academy of Sciences. She held various positions in the American Association for the Advancement of Science, notably president in 1975 and chair of the executive committee of the board of directors in 1976. She was a recognizable figure in academia, usually wearing a distinctive cape and carrying a walking-stick.

Mead was featured on two record albums published by Folkways Records. The first, released in 1959, "An Interview With Margaret Mead," explored the topics of morals and anthropology. In 1971, she was included in a compilation of talks by prominent women, "But the Women Rose, Vol.2: Voices of Women in American History".

She is credited with the term "semiotics", making it a noun.

In later life, Mead was a mentor to many young anthropologists and sociologists, including Jean Houston.

In 1976, Mead was a key participant at UN Habitat I, the first UN forum on human settlements.

Mead died of pancreatic cancer on November 15, 1978, and is buried at Trinity Episcopal Church Cemetery, Buckingham, Pennsylvania.

In the foreword to "Coming of Age in Samoa", Mead's advisor, Franz Boas, wrote of its significance:

Courtesy, modesty, good manners, conformity to definite ethical standards are universal, but what constitutes courtesy, modesty, very good manners, and definite ethical standards is not universal. It is instructive to know that standards differ in the most unexpected ways.

Mead's findings suggested that the community ignores both boys and girls until they are about 15 or 16. Before then, children have no social standing within the community. Mead also found that marriage is regarded as a social and economic arrangement where wealth, rank, and job skills of the husband and wife are taken into consideration.
In 1983, five years after Mead had died, New Zealand anthropologist Derek Freeman published "Margaret Mead and Samoa: The Making and Unmaking of an Anthropological Myth", in which he challenged Mead's major findings about sexuality in Samoan society. Freeman's book was controversial in its turn: later in 1983 a special session of Mead's supporters in the American Anthropological Association (to which Freeman was not invited) declared it to be "poorly written, unscientific, irresponsible and misleading."

In 1999, Freeman published another book, "The Fateful Hoaxing of Margaret Mead: A Historical Analysis of Her Samoan Research", including previously unavailable material. In his obituary in "The New York Times", John Shaw stated that his thesis, though upsetting many, had by the time of his death generally gained widespread acceptance. Recent work has nonetheless challenged his critique. A frequent criticism of Freeman is that he regularly misrepresented Mead's research and views. In a 2009 evaluation of the debate, anthropologist Paul Shankman concluded that:

There is now a large body of criticism of Freeman's work from a number of perspectives in which Mead, Samoa, and anthropology appear in a very different light than they do in Freeman's work. Indeed, the immense significance that Freeman gave his critique looks like 'much ado about nothing' to many of his critics.

While nurture-oriented anthropologists are more inclined to agree with Mead's conclusions, there are other non-anthropologists who take a nature-oriented approach following Freeman's lead, among them Harvard psychologist Steven Pinker, biologist Richard Dawkins, evolutionary psychologist David Buss, science writer Matt Ridley and classicist Mary Lefkowitz. The philosopher Peter Singer has also criticized Mead in his book "A Darwinian Left", where he states that "Freeman compiles a convincing case that Mead had misunderstood Samoan customs".

In 1996, author Martin Orans examined Mead's notes preserved at the Library of Congress, and credits her for leaving all of her recorded data available to the general public. Orans point out that Freeman's basic criticisms, that Mead was duped by ceremonial virgin Fa'apua'a Fa'amu (who later swore to Freeman that she had played a joke on Mead) were equivocal for several reasons: first, Mead was well aware of the forms and frequency of Samoan joking; second, she provided a careful account of the sexual restrictions on ceremonial virgins that corresponds to Fa'apua'a Fa'auma'a's account to Freeman, and third, that Mead's notes make clear that she had reached her conclusions about Samoan sexuality before meeting Fa'apua'a Fa'amu. Orans points out that Mead's data support several different conclusions, and that Mead's conclusions hinge on an interpretive, rather than positivist, approach to culture. Orans goes on to point out, concerning Mead's work elsewhere, that her own notes do not support her published conclusive claims. However, there are still those who claim Mead was hoaxed, including Peter Singer and zoologist David Attenborough. Evaluating Mead's work in Samoa from a positivist stance, Martin Orans' assessment of the controversy was that Mead did not formulate her research agenda in scientific terms, and that "her work may properly be damned with the harshest scientific criticism of all, that it is 'not even wrong'."

The Intercollegiate Review , published by the Intercollegiate Studies Institute which promotes conservative thought on college campuses, listed the book as No. 1 on its "The Fifty Worst Books of the Century" list.

Another influential book by Mead was "Sex and Temperament in Three Primitive Societies". This became a major cornerstone of the feminist movement, since it claimed that females are dominant in the Tchambuli (now spelled Chambri) Lake region of the Sepik basin of Papua New Guinea (in the western Pacific) without causing any special problems. The lack of male dominance may have been the result of the Australian administration's outlawing of warfare. According to contemporary research, males are dominant throughout Melanesia (although some believe that female witches have special powers). Others have argued that there is still much cultural variation throughout Melanesia, and especially in the large island of New Guinea. Moreover, anthropologists often overlook the significance of networks of political influence among females. The formal male-dominated institutions typical of some areas of high population density were not, for example, present in the same way in Oksapmin, West Sepik Province, a more sparsely populated area. Cultural patterns there were different from, say, Mt. Hagen. They were closer to those described by Mead.

Mead stated that the Arapesh people, also in the Sepik, were pacifists, although she noted that they do on occasion engage in warfare. Her observations about the sharing of garden plots among the Arapesh, the egalitarian emphasis in child rearing, and her documentation of predominantly peaceful relations among relatives are very different from the "big man" displays of dominance that were documented in more stratified New Guinea cultures—e.g. by Andrew Strathern. They are a different cultural pattern.

In brief, her comparative study revealed a full range of contrasting gender roles:

Deborah Gewertz (1981) studied the Chambri (called Tchambuli by Mead) in 1974–1975 and found no evidence of such gender roles. Gewertz states that as far back in history as there is evidence (1850s) Chambri men dominated over the women, controlled their produce and made all important political decisions. In later years there has been a diligent search for societies in which women dominate men, or for signs of such past societies, but none have been found (Bamberger, 1974). Jessie Bernard criticised Mead's interpretations of her findings, arguing that Mead was biased in her descriptions due to use of subjective descriptions. Bernard argues that while Mead claimed the Mundugumor women were temperamentally identical to men, her reports indicate that there were in fact sex differences; Mundugumor women hazed each other less than men hazed each other, they made efforts to make themselves physically desirable to others, married women had fewer affairs than married men, women were not taught to use weapons, women were used less as hostages and Mundugumor men engaged in physical fights more often than women. Conversely, the Arapesh were also described as equal in temperament, yet Bernard states that Mead's own writings indicate that men fought physically over women, yet women did not fight physically over men, despite the two being supposedly equal in temperament. The Arapesh also seemed to have some conception of sex differences in temperament, as they would sometimes describe a woman as acting like a particularly quarrelsome man. Bernard also questioned if the behaviour of men and women in these societies differed as much from Western behaviour as Mead claimed it did, arguing that some of her descriptions could be equally descriptive of a Western context.

Despite its feminist roots, Mead's work on women and men was also criticized by Betty Friedan 
on the basis that it contributes to infantilizing women.

In 1926, there was much debate about race and intelligence. Mead felt the methodologies involved in the experimental psychology research supporting arguments of racial superiority in intelligence were substantially flawed. In "The Methodology of Racial Testing: Its Significance for Sociology" Mead proposes that there are three problems with testing for racial differences in intelligence. First, there are concerns with the ability to validly equate one's test score with what Mead refers to as "racial admixture" or how much "Negro or Indian blood" an individual possesses. She also considers whether this information is relevant when interpreting IQ scores. Mead remarks that a genealogical method could be considered valid if it could be "subjected to extensive verification". In addition, the experiment would need a steady control group to establish whether racial admixture was actually affecting intelligence scores. Next, Mead argues that it is difficult to measure the effect that social status has on the results of a person's intelligence test. By this she meant that environment (i.e., family structure, socioeconomic status, exposure to language) has too much influence on an individual to attribute inferior scores solely to a physical characteristic such as race. Lastly, Mead adds that language barriers sometimes create the biggest problem of all. Similarly, Stephen J. Gould finds three main problems with intelligence testing, in his 1981 book "The Mismeasure of Man", that relate to Mead's view of the problem of determining whether there are indeed racial differences in intelligence.

In 1929 Mead and Fortune visited Manus, now the northern-most province of Papua New Guinea, travelling there by boat from Rabaul. She amply describes her stay there in her autobiography and it is mentioned in her 1984 biography by Jane Howard. On Manus she studied the Manus people of the south coast village of Peri. "Over the next five decades Mead would come back oftener to Peri than to any other field site of her career.

Mead has been credited with persuading the American Jewish Committee to sponsor a project to study European Jewish villages, "shtetls", in which a team of researchers would conduct mass interviews with Jewish immigrants living in New York City. The resulting book, widely cited for decades, allegedly created the Jewish mother stereotype, a mother intensely loving but controlling to the point of smothering, and engendering guilt in her children through the suffering she professed to undertake for their sakes.

Mead worked for the RAND Corporation, a U.S. Air Force military funded private research organization, from 1948 to 1950 to study Russian culture and attitudes toward authority.

As an Anglican Christian, Mead played a considerable part in the drafting of the 1979 American Episcopal Book of Common Prayer.

After her death, Mead's Samoan research was criticized by anthropologist Derek Freeman, who published a book that argued against many of Mead's conclusions. Freeman argued that Mead had misunderstood Samoan culture when she argued that Samoan culture did not place many restrictions on youths' sexual explorations. Freeman argued instead that Samoan culture prized female chastity and virginity and that Mead had been misled by her female Samoan informants. Freeman's critique was met with a considerable backlash and harsh criticism from the anthropology community, whereas it was received enthusiastically by communities of scientists who believed that sexual mores were more or less universal across cultures. Some anthropologists who studied Samoan culture argued in favor of Freeman's findings and contradicted those of Mead, whereas others argued that Freeman's work did not invalidate Mead's work because Samoan culture had been changed by the integration of Christianity in the decades between Mead's and Freeman's fieldwork periods. While Mead was careful to shield the identity of all her subjects for confidentiality Freeman was able to find and interview one of her original participants, and Freeman reported that she admitted to having wilfully misled Mead. She said that she and her friends were having fun with Mead and telling her stories.

On the whole, anthropologists have rejected the notion that Mead's conclusions rested on the validity of a single interview with a single person, finding instead that Mead based her conclusions on the sum of her observations and interviews during her time in Samoa, and that the status of the single interview did not falsify her work. Some anthropologists have however maintained that even though Freeman's critique was invalid, Mead's study was not sufficiently scientifically rigorous to support the conclusions she drew.

In her 2015 book "Galileo's Middle Finger", Alice Dreger argues that Freeman's accusations were unfounded and misleading. A detailed review of the controversy by Paul Shankman, published by the University of Wisconsin Press in 2009, supports the contention that Mead's research was essentially correct, and concludes that Freeman cherry-picked his data and misrepresented both Mead and Samoan culture.

In 1976, Mead was inducted into the National Women's Hall of Fame.

On January 19, 1979, President Jimmy Carter announced that he was awarding the Presidential Medal of Freedom posthumously to Mead. UN Ambassador Andrew Young presented the award to Mead's daughter at a special program honoring Mead's contributions, sponsored by the American Museum of Natural History, where she spent many years of her career. The citation read:

In 1979, the Supersisters trading card set was produced and distributed; one of the cards featured Mead's name and picture.

The 2014 novel "Euphoria" by Lily King is a fictionalized account of Mead's love/marital relationships with fellow anthropologists Reo Fortune and Gregory Bateson in pre-WWII New Guinea.

In addition, there are several schools named after Mead in the United States: a junior high school in Elk Grove Village, Illinois, an elementary school in Sammamish, Washington and another in Sheepshead Bay, Brooklyn, New York.

The USPS issued a stamp of face value 32¢ on May 28, 1998, as part of the Celebrate the Century stamp sheet series.

In the 1967 musical "Hair", her name is given to a tranvestite 'tourist' disturbing the show with the song 'My Conviction'.

Note: See also "Margaret Mead: The Complete Bibliography 1925–1975", Joan Gordan, ed., The Hague: Mouton.








</doc>
<doc id="19620" url="https://en.wikipedia.org/wiki?curid=19620" title="Michael Palin">
Michael Palin

Sir Michael Edward Palin (; born 5 May 1943) is an English actor, comedian, writer and television presenter. He was a member of the comedy group Monty Python. Since 1980 he has made a number of travel documentaries.

Palin wrote most of his comedic material with fellow Python member Terry Jones. Before Monty Python, they had worked on other shows such as the "Ken Dodd Show", "The Frost Report", and "Do Not Adjust Your Set". Palin appeared in some of the most famous Python sketches, including "Argument Clinic", "Dead Parrot sketch", "The Lumberjack Song", "The Spanish Inquisition", "Bicycle Repair Man" and "The Fish-Slapping Dance". He also regularly played a Gumby.

Palin continued to work with Jones after Python, co-writing "Ripping Yarns". He has also appeared in several films directed by fellow Python Terry Gilliam and made notable appearances in other films such as "A Fish Called Wanda" (1988), for which he won the BAFTA Award for Best Actor in a Supporting Role. In a 2005 poll to find "The Comedians' Comedian", he was voted the 30th favourite by fellow comedians and comedy insiders. After Python, he began a new career as a travel writer and travel documentarian. His journeys have taken him across the world, including the North and South Poles, the Sahara, the Himalayas, Eastern Europe, Brazil, and in 2018, he visited North Korea; documenting his visit to the isolated country in a series broadcast on Channel 5.

Having been awarded a CBE for services to television in the 2000 New Year Honours, Palin received a knighthood in the 2019 New Year Honours for services to travel, culture and geography. From 2009–12 he was the president of the Royal Geographical Society. On 12 May 2013, Palin was made a BAFTA fellow, the highest honour that is conferred by the organisation.

Palin was born in Ranmoor, Sheffield, the second child and only son of Edward Moreton Palin (1900–1977) and Mary Rachel Lockhart (née Ovey; 1903–1990). His father was a Shrewsbury and Cambridge-educated engineer working for a steel firm. His maternal grandfather, Lieutenant-Colonel Richard Lockhart Ovey, DSO, was High Sheriff of Oxfordshire in 1927. He was educated at Birkdale and Shrewsbury School. His sister Angela was nine years older than he was. Despite the age gap the two had a close relationship until her suicide in 1987. He has ancestral roots in Letterkenny, County Donegal.

When he was five years old, Palin had his first acting experience at Birkdale playing Martha Cratchit in a school performance of "A Christmas Carol". At the age of 10, Palin, still interested in acting, made a comedy monologue and read a Shakespeare play to his mother while playing all the parts. After leaving Shrewsbury in 1962, he went on to read modern history at Brasenose College, Oxford. With fellow student Robert Hewison he performed and wrote, for the first time, comedy material at a university Christmas party. Terry Jones, also a student in Oxford, saw that performance and began writing together with Hewison and Palin. In the same year Palin joined the Brightside and Carbrook Co-operative Society Players and first gained fame when he won an acting award at a Co-op drama festival. He also performed and wrote in the Oxford Revue (called the Et ceteras) with Jones.

In 1966, Palin married Helen Gibbins, whom he first met in 1959 on holiday in Southwold in Suffolk. This meeting was later fictionalised in Palin's teleplay for the 1987 BBC television drama "East of Ipswich". The couple have three children and four grandchildren. Daughter Rachel is a BBC TV director, whose work includes "". Son William is Director of Conservation at the Old Royal Naval College, Greenwich, London and oversaw the 2018–19 restoration of the Painted Hall. A photograph of William as a baby briefly appeared in "Monty Python and the Holy Grail" as "Sir Not-appearing-in-this-film". His nephew is the theatre designer Jeremy Herbert. Palin is an agnostic.

After finishing university in 1965 Palin became a presenter on a comedy pop show called "Now!" for the television contractor Television Wales and the West. At the same time Palin was contacted by Jones, who had left university a year earlier, for assistance in writing a theatrical documentary about sex through the ages. Although this project was eventually abandoned, it brought Palin and Jones together as a writing duo and led them to write comedy for various BBC programmes, such as "The Ken Dodd Show", "The Billy Cotton Bandshow", and "The Illustrated Weekly Hudd". They collaborated in writing lyrics for an album by Barry Booth called "Diversions". They were also in the team of writers working for "The Frost Report", whose other members included Frank Muir, Barry Cryer, Marty Feldman, Ronnie Barker, Ronnie Corbett, Dick Vosburgh and future Monty Python members Graham Chapman, John Cleese and Eric Idle.

Although the members of Monty Python had already encountered each other over the years, "The Frost Report" was the first time all the British members of Monty Python (its sixth member, Terry Gilliam, was at that time an American citizen) worked together. During the run of "The Frost Report" the Palin/Jones team contributed material to two shows starring John Bird: "The Late Show" and "A Series of Birds". For "A Series of Birds" the Palin/Jones team had their first experience of writing narrative instead of the short sketches they were accustomed to conceiving.

Following "The Frost Report" the Palin/Jones team worked both as actors and writers on the show "Twice a Fortnight" with Graeme Garden, Bill Oddie and Jonathan Lynn, and the successful children's comedy show "Do Not Adjust Your Set" with Idle and David Jason. The show also featured musical numbers by the Bonzo Dog Doo-Dah Band, including future Monty Python musical collaborator Neil Innes. The animations for "Do Not Adjust Your Set" were made by Terry Gilliam. Eager to work with Palin sans Jones, Cleese later asked him to perform in "How to Irritate People" together with Chapman and Tim Brooke-Taylor. The Palin/Jones team were reunited for "The Complete and Utter History of Britain".

On the strength of their work on "The Frost Report" and other programmes, Cleese and Chapman had been offered a show by the BBC, but Cleese was reluctant to do a two-man show for various reasons, among them Chapman's reputedly difficult personality. During this period Cleese contacted Palin about doing the show that would ultimately become "Monty Python's Flying Circus". At the same time the success of "Do Not Adjust Your Set" had led Palin, Jones, Idle and Gilliam to be offered their own series and, while it was still in production, Palin agreed to Cleese's proposal and brought along Idle, Jones and Gilliam. Thus the formation of the Monty Python troupe has been referred to as a result of Cleese's desire to work with Palin and the chance circumstances that brought the other four members into the fold.

Palin played various roles in "Monty Python", which ranged from manic enthusiasm (such as the lumberjack of "The Lumberjack Song", or Herbert Anchovy, host of the game show "Blackmail") to unflappable calmness (such as the Dead parrot vendor or Cheese Shop proprietor). As a straight man he was often a foil to the rising ire of characters portrayed by Cleese. He also played timid, socially inept characters such as Arthur Putey, the man who sits quietly as a marriage counsellor (Eric Idle) makes love to his wife (Carol Cleveland), and Mr Anchovy, a chartered accountant who wants to become a lion tamer. He appeared as the "It's" man (a Robinson Crusoe-type castaway with torn clothes and a long, unkempt beard) at the beginning of most episodes. He also frequently played a Gumby, a character Palin said "had these moronic views that were expressed with extraordinary force."

Palin frequently co-wrote sketches with Terry Jones and also initiated the "Spanish Inquisition sketch", which included the catchphrase "Nobody expects the Spanish Inquisition!" He also composed songs with Jones including "The Lumberjack Song", "Every Sperm Is Sacred" and "Spam". His solo musical compositions included "Decomposing Composers" and "Finland".

After the "Monty Python" television series ended in 1974, the Palin/Jones team worked on "Ripping Yarns", an intermittent television comedy series broadcast over three years from 1976. They had earlier collaborated on the play "Secrets" from the BBC series "Black and Blue" in 1973. He starred as Dennis the Peasant in Terry Gilliam's 1977 film "Jabberwocky". Palin also appeared in "All You Need Is Cash" (1978) as Eric Manchester (based on Derek Taylor), the press agent for the Rutles. In 1980, Palin co-wrote "Time Bandits" with Terry Gilliam. He also acted in the film.

In 1982, Palin wrote and starred in "The Missionary", co-starring Maggie Smith. In it, he plays the Reverend Charles Fortescue, who is recalled from Africa to aid prostitutes. He co-starred with Maggie Smith again in the 1984 comedy film "A Private Function". In 1984, he reunited with Terry Gilliam to appear in "Brazil". He appeared in the comedy film "A Fish Called Wanda", for which he won the BAFTA Award for Best Actor in a Supporting Role. Cleese reunited the main cast almost a decade later to make "Fierce Creatures". After filming for "Fierce Creatures" finished, Palin went on a travel journey for a BBC documentary and, returning a year later, found that the end of "Fierce Creatures" had failed at test screenings and had to be reshot.

After "Fierce Creatures" and a small part in "The Wind in the Willows", a film directed by and starring Terry Jones, it would be twenty more years until Palin's next film role, as Soviet politician Vyacheslav Molotov in the 2017 satirical black comedy "The Death of Stalin". Palin also appeared with John Cleese in his documentary, "The Human Face". Palin was cast in a supporting role in the Tom Hanks and Meg Ryan romantic comedy "You've Got Mail", but his role was eventually cut entirely.

Palin has also appeared in serious drama. In 1991 Palin appeared in a film, "American Friends", he wrote based upon a real event in the life of his great-grandfather, a fellow at St John's College, Oxford. In that same year he also played the part of a headmaster in Alan Bleasdale's Channel 4 drama series "GBH". In 1994, Palin narrated the English language audiobook version of "Esio Trot" by children's author Roald Dahl.

In 1997, Palin had a small cameo role in Australian soap opera "Home and Away". He played an English surfer with a fear of sharks, who interrupts a conversation between two main characters to ask whether there were any sharks in the sea. This was filmed while he was in Australia for the "Full Circle" series, with a segment about the filming of the role featuring in the series. In November 2005, he appeared in the "John Peel's Record Box" documentary.

In 2013, Palin appeared in a First World War drama titled "The Wipers Times" written by Ian Hislop and Nick Newman. At the Cannes Film Festival in 2016, it was announced that Palin was set to star alongside Adam Driver in Terry Gilliam's "The Man Who Killed Don Quixote". Palin, however, dropped out of the film after it ran into a financial problem.

While speaking at the Edinburgh International Film Festival, Palin announced that he was presenting the two-part documentary "Michael Palin in North Korea" to be broadcast on the British television network Channel 5. The documentary was broadcast in September 2018, in two one-hour segments on Channel 5 in the UK and in a single two-hour programme on National Geographic in the United States. It was broadcast again by Channel 5, in a single two-hour programme in December 2018.

In July 2019, Palin performed a one-man stage show at the Torch Theatre, Milford Haven, Wales, about the loss of HMS "Erebus" during the third Franklin expedition, which is recounted in his book "Erebus: The Story of a Ship".

Palin assisted Campaign for Better Transport and others with campaigns on sustainable transport, particularly those relating to urban areas, and has been president of the campaign since 1986. On 2 January 2011, he became the first person to sign the UK-based Campaign for Better Transport's Fair Fares Now campaign. In July 2015, he signed an open letter and gave an interview to support "a strong BBC at the centre of British life" at a time the government was reviewing the corporation's size and activities.

In July 2010, Palin sent a message of support for the Dongria Kondh tribe of India, who are resisting mining on their land by the company Vedanta Resources. Palin said, "I've been to the Nyamgiri Hills in Orissa and seen the forces of money and power that Vedanta Resources have arrayed against a people who have occupied their land for thousands of years, who husband the forest sustainably and make no great demands on the state or the government. The tribe I visited simply want to carry on living in the villages that they and their ancestors have always lived in".

Palin's first travel documentary was episode 4 of the 1980 BBC Television series "Great Railway Journeys of the World", entitled "Confessions of a Trainspotter". Throughout the hour long show, Palin humorously reminisces about his childhood hobby of train spotting while he travels throughout the UK by train from London to the Kyle of Lochalsh, via Manchester, York, Newcastle upon Tyne, Edinburgh and Inverness. He rides vintage railway lines and trains including the "Flying Scotsman". At the Kyle of Lochalsh, Palin bought the station's long metal platform sign and is seen lugging it back to London with him.

In 1994, Palin travelled through Ireland for the same series, entitled "Derry to Kerry". In a quest for family roots, he attempted to trace his great grandmother – Brita Gallagher – who set sail from Ireland years ago during the Great Famine (1845–1849), bound for a new life in Burlington, New Jersey. The series is a trip along the Palin family line.

Starting in 1989, Palin appeared as presenter in a series of travel programmes made for the BBC. It was after the veteran TV globetrotter Alan Whicker and journalist Miles Kington turned down presenting the first of these, "Around the World in 80 Days with Michael Palin", that gave Palin the opportunity to present his first and subsequent travel shows. These programmes have been broadcast worldwide in syndication, and were also sold on VHS tape and later on DVD:


Following each trip, Palin wrote a book about his travels, providing information and insights not included in the TV programme. Each book is illustrated with photographs by Basil Pao, the stills photographer who was on the team. (Exception: the first book, "Around the World in 80 Days", contains some pictures by Pao but most are by other photographers.)

All seven of these books were also made available as audio books, and all of them are read by Palin himself. "Around the World in 80 Days" and "Hemingway Adventure" are unabridged, while the other four books were made in both abridged and unabridged versions.

For four of the trips a photography book was made by Pao, each with an introduction written by Palin. These are large coffee-table style books with pictures printed on glossy paper. The majority of the pictures are of various people encountered on the trip, as informal portraits or showing them engaged in some interesting activity. Some of the landscape photos are displayed as two-page spreads.

Palin's travel programmes are responsible for a phenomenon termed the "Palin effect": areas of the world that he has visited suddenly become popular tourist attractions – for example, the significant increase in the number of tourists interested in Peru after Palin visited Machu Picchu. In a 2006 survey of "15 of the world's top travel writers" by "The Observer", Palin named Peru's Pongo de Mainique (canyon below the Machu Picchu) his "favourite place in the world".

Palin notes in his book of "Around the World in 80 Days" that the final leg of his journey could originally have taken him and his crew on one of the trains involved in the Clapham Junction rail crash, but they arrived ahead of schedule and caught an earlier train.

In recent years, Palin has written and presented occasional documentary programmes on artists that interest him. The first, on Scottish painter Anne Redpath, was "Palin on Redpath" in 1997. In "The Bright Side of Life" (2000), Palin continued on a Scottish theme, looking at the work of the Scottish Colourists. Two further programmes followed on European painters; "Michael Palin and the Ladies Who Loved Matisse" (2004) and "Michael Palin and the Mystery of Hammershøi" (2005), about the French artist Henri Matisse and Danish artist Vilhelm Hammershøi respectively. The DVD "Michael Palin on Art" contains all these documentaries except for the Matisse programme.

In November 2008, Palin presented a First World War documentary about Armistice Day, 11 November 1918, when thousands of soldiers lost their lives in battle after the war had officially ended. Palin filmed on the battlefields of Northern France and Belgium for the programme, called the "Last Day of World War One", produced for the BBC's "Timewatch" series.

Palin was instrumental in setting up the Michael Palin Centre for Stammering Children in 1993. Also in 1993, each member of Monty Python had an asteroid named after them. Palin's is Asteroid 9621 Michaelpalin. In 2003, inside the Globe a commemorative stone was placed – Palin has his own stone, to mark donors to the theatre, but it is misspelled as "Michael Pallin". The story goes that John Cleese paid for the stone, and mischievously insisted on misspelling his name.

In honour of his achievements as a traveller, especially rail travel, Palin has two British trains named after him. In 2002, Virgin Trains' new £5 million high speed Super Voyager train number 221130 was named "Michael Palin" it carries his name externally and a plaque is located adjacent to the onboard shop with information on Palin and his many journeys. Also, National Express East Anglia named a British Rail Class 153 (unit number 153335) after him. (He is also a model railway enthusiast.)
In 2008, he received the James Joyce Award of the Literary and Historical Society in Dublin. In recognition of his services to the promotion of geography, Palin was awarded the Livingstone Medal of the Royal Scottish Geographical Society in March 2009, along with a Fellowship of this Society (FRGS). In June 2013, he was similarly honoured in Canada with a gold medal for achievements in geography by the Royal Canadian Geographical Society. In June 2009, Palin was elected for a three-year term as President of the Royal Geographical Society. Because of his self-described "amenable, conciliatory character" Michael Palin has been referred to as unofficially "Britain's Nicest Man." In a 2018 poll for Yorkshire Day he was named the greatest Yorkshireman ever, ahead of Sean Bean and Patrick Stewart.

In September 2013, Moorlands School, Leeds named one of their school houses "Palin" after him. The University of St Andrews awarded Palin an honorary Doctor of Science degree during their June 2017 graduation ceremonies, with the degree recognising his contribution to the public's understanding of contemporary geography. He joins his fellow Pythons John Cleese and Terry Jones in receiving an honorary degree from the Fife institution. In October 2018, the Royal Canadian Geographical Society awarded Palin the first Louie Kamookak Medal for advances in geography, for his book on the history of the polar exploration vessel HMS "Erebus".

He was appointed a Commander of the Order of the British Empire (CBE) in the 2000 New Year Honours. Palin was appointed a Knight Commander of the Order of St Michael and St George (KCMG) in the 2019 New Year Honours for "services to travel, culture and geography". Palin is the only member of the Monty Python team to receive a knighthood. (John Cleese had turned down a CBE in 1996, calling it "too silly", and declined a life peerage in 1999).

In 2017 the British Library acquired Palin's archive consisting of project files relating to his work, notebooks, and his personal diaries. The papers in the archive relate to his work with "Monty Python", his later TV work, and his children's and humorous books.

All his travel books can also be read at no charge, complete and unabridged, on his website.














</doc>
<doc id="19622" url="https://en.wikipedia.org/wiki?curid=19622" title="Materials science">
Materials science

The interdisciplinary field of materials science, also commonly termed materials science and engineering, is the design and discovery of new materials, particularly solids. The intellectual origins of materials science stem from the Enlightenment, when researchers began to use analytical thinking from chemistry, physics, and engineering to understand ancient, phenomenological observations in metallurgy and mineralogy. Materials science still incorporates elements of physics, chemistry, and engineering. As such, the field was long considered by academic institutions as a sub-field of these related fields. Beginning in the 1940s, materials science began to be more widely recognized as a specific and distinct field of science and engineering, and major technical universities around the world created dedicated schools for its study. 
Many of the most pressing scientific problems humans currently face are due to the limits of available materials and how they are used. Thus, breakthroughs in materials science are likely to affect the future of technology significantly.

Materials scientists emphasize understanding how the history of a material (its "processing") influences its structure, and thus the material's properties and performance. The understanding of processing-structure-properties relationships is called the . This paradigm is used to advance understanding in a variety of research areas, including nanotechnology, biomaterials, and metallurgy. Materials science is also an important part of forensic engineering and failure analysis investigating materials, products, structures or components which fail or do not function as intended, causing personal injury or damage to property. Such investigations are key to understanding, for example, the causes of various aviation accidents and incidents.

The material of choice of a given era is often a defining point. Phrases such as Stone Age, Bronze Age, Iron Age, and Steel Age are historic, if arbitrary examples. Originally deriving from the manufacture of ceramics and its putative derivative metallurgy, materials science is one of the oldest forms of engineering and applied science. Modern materials science evolved directly from metallurgy, which itself evolved from mining and (likely) ceramics and earlier from the use of fire. A major breakthrough in the understanding of materials occurred in the late 19th century, when the American scientist Josiah Willard Gibbs demonstrated that the thermodynamic properties related to atomic structure in various phases are related to the physical properties of a material. Important elements of modern materials science were products of the Space Race: the understanding and engineering of the metallic alloys, and silica and carbon materials, used in building space vehicles enabling the exploration of space. Materials science has driven, and been driven by, the development of revolutionary technologies such as rubbers, plastics, semiconductors, and biomaterials.

Before the 1960s (and in some cases decades after), many eventual "materials science" departments were "metallurgy" or "ceramics engineering" departments, reflecting the 19th and early 20th century emphasis on metals and ceramics. The growth of materials science in the United States was catalyzed in part by the Advanced Research Projects Agency, which funded a series of university-hosted laboratories in the early 1960s "to expand the national program of basic research and training in the materials sciences." The field has since broadened to include every class of materials, including ceramics, polymers, semiconductors, magnetic materials, biomaterials, and nanomaterials, generally classified into three distinct groups: ceramics, metals, and polymers. The prominent change in materials science during the recent decades is active usage of computer simulations to find new materials, predict properties, and understand phenomena.

A material is defined as a substance (most often a solid, but other condensed phases can be included) that is intended to be used for certain applications. There are a myriad of materials around us—they can be found in anything from buildings to spacecraft. Materials can generally be further divided into two classes: crystalline and non-crystalline. The traditional examples of materials are metals, semiconductors, ceramics and polymers. New and advanced materials that are being developed include nanomaterials, biomaterials, and energy materials to name a few.

The basis of materials science involves studying the structure of materials, and relating them to their properties. Once a materials scientist knows about this structure-property correlation, they can then go on to study the relative performance of a material in a given application. The major determinants of the structure of a material and thus of its properties are its constituent chemical elements and the way in which it has been processed into its final form. These characteristics, taken together and related through the laws of thermodynamics and kinetics, govern a material's microstructure, and thus its properties.

As mentioned above, structure is one of the most important components of the field of materials science. Materials science examines the structure of materials from the atomic scale, all the way up to the macro scale. Characterization is the way materials scientists examine the structure of a material. This involves methods such as diffraction with X-rays, electrons, or neutrons, and various forms of spectroscopy and chemical analysis such as Raman spectroscopy, energy-dispersive spectroscopy (EDS), chromatography, thermal analysis, electron microscope analysis, etc. Structure is studied at various levels, as detailed below.

This deals with the atoms of the materials, and how they are arranged to give molecules, crystals, etc. Much of the electrical, magnetic and chemical properties of materials arise from this level of structure. The length scales involved are in angstroms(Å).
The chemical bonding and atomic arrangement (crystallography) are fundamental to studying the properties and behavior of any material.

To obtain a full understanding of the material structure and how it relates to its properties, the materials scientist must study how the different atoms, ions and molecules are arranged and bonded to each other. This involves the study and use of quantum chemistry or quantum physics. Solid-state physics, solid-state chemistry and physical chemistry are also involved in the study of bonding and structure.

Crystallography is the science that examines the arrangement of atoms in crystalline solids. Crystallography is a useful tool for materials scientists. In single crystals, the effects of the crystalline arrangement of atoms is often easy to see macroscopically, because the natural shapes of crystals reflect the atomic structure. Further, physical properties are often controlled by crystalline defects. The understanding of crystal structures is an important prerequisite for understanding crystallographic defects. Mostly, materials do not occur as a single crystal, but in polycrystalline form, i.e., as an aggregate of small crystals with different orientations. Because of this, the powder diffraction method, which uses diffraction patterns of polycrystalline samples with a large number of crystals, plays an important role in structural determination.
Most materials have a crystalline structure, but some important materials do not exhibit regular crystal structure. Polymers display varying degrees of crystallinity, and many are completely noncrystalline. Glass, some ceramics, and many natural materials are amorphous, not possessing any long-range order in their atomic arrangements. The study of polymers combines elements of chemical and statistical thermodynamics to give thermodynamic and mechanical descriptions of physical properties.

Nanostructure deals with objects and structures that are in the 1–100 nm range. In many materials, atoms or molecules agglomerate together to form objects at the nanoscale. This causes many interesting electrical, magnetic, optical, and mechanical properties.

In describing nanostructures it is necessary to differentiate between the number of dimensions on the nanoscale. Nanotextured surfaces have "one dimension" on the nanoscale, i.e., only the thickness of the surface of an object is between 0.1 and 100 nm. Nanotubes have "two dimensions" on the nanoscale, i.e., the diameter of the tube is between 0.1 and 100 nm; its length could be much greater. Finally, spherical nanoparticles have "three dimensions" on the nanoscale, i.e., the particle is between 0.1 and 100 nm in each spatial dimension. The terms nanoparticles and ultrafine particles (UFP) often are used synonymously although UFP can reach into the micrometre range. The term 'nanostructure' is often used when referring to magnetic technology. Nanoscale structure in biology is often called ultrastructure.

Materials which atoms and molecules form constituents in the nanoscale (i.e., they form nanostructure) are called nanomaterials. Nanomaterials are subject of intense research in the materials science community due to the unique properties that they exhibit.

Microstructure is defined as the structure of a prepared surface or thin foil of material as revealed by a microscope above 25× magnification. It deals with objects from 100 nm to a few cm. The microstructure of a material (which can be broadly classified into metallic, polymeric, ceramic and composite) can strongly influence physical properties such as strength, toughness, ductility, hardness, corrosion resistance, high/low temperature behavior, wear resistance, and so on. Most of the traditional materials (such as metals and ceramics) are microstructured.

The manufacture of a perfect crystal of a material is physically impossible. For example, any crystalline material will contain defects such as precipitates, grain boundaries (Hall–Petch relationship), vacancies, interstitial atoms or substitutional atoms. The microstructure of materials reveals these larger defects, so that they can be studied, with significant advances in simulation resulting in exponentially increasing understanding of how defects can be used to enhance material properties.

Macrostructure is the appearance of a material in the scale millimeters to meters—it is the structure of the material as seen with the naked eye.

Materials exhibit myriad properties, including the following.
The properties of a material determine its usability and hence its engineering application.

Synthesis and processing involves the creation of a material with the desired micro-nanostructure. From an engineering standpoint, a material cannot be used in industry if no economical production method for it has been developed. Thus, the processing of materials is vital to the field of materials science.

Different materials require different processing or synthesis methods. For example, the processing of metals has historically been very important and is studied under the branch of materials science named "physical metallurgy". Also, chemical and physical methods are also used to synthesize other materials such as polymers, ceramics, thin films, etc. As of the early 21st century, new methods are being developed to synthesize nanomaterials such as graphene.

Thermodynamics is concerned with heat and temperature and their relation to energy and work. It defines macroscopic variables, such as internal energy, entropy, and pressure, that partly describe a body of matter or radiation. It states that the behavior of those variables is subject to general constraints common to all materials. These general constraints are expressed in the four laws of thermodynamics. Thermodynamics describes the bulk behavior of the body, not the microscopic behaviors of the very large numbers of its microscopic constituents, such as molecules. The behavior of these microscopic particles is described by, and the laws of thermodynamics are derived from, statistical mechanics.

The study of thermodynamics is fundamental to materials science. It forms the foundation to treat general phenomena in materials science and engineering, including chemical reactions, magnetism, polarizability, and elasticity. It also helps in the understanding of phase diagrams and phase equilibrium.

Chemical kinetics is the study of the rates at which systems that are out of equilibrium change under the influence of various forces. When applied to materials science, it deals with how a material changes with time (moves from non-equilibrium to equilibrium state) due to application of a certain field. It details the rate of various processes evolving in materials including shape, size, composition and structure. Diffusion is important in the study of kinetics as this is the most common mechanism by which materials undergo change.

Kinetics is essential in processing of materials because, among other things, it details how the microstructure changes with application of heat.

Materials science is a highly active area of research. Together with materials science departments, physics, chemistry, and many engineering departments are involved in materials research. Materials research covers a broad range of topics – the following non-exhaustive list highlights a few important research areas.

Nanomaterials describe, in principle, materials of which a single unit is sized (in at least one dimension) between 1 and 1000 nanometers (10 meter) but is usually 1–100 nm.

Nanomaterials research takes a materials science-based approach to nanotechnology, using advances in materials metrology and synthesis which have been developed in support of microfabrication research. Materials with structure at the nanoscale often have unique optical, electronic, or mechanical properties.

The field of nanomaterials is loosely organized, like the traditional field of chemistry, into organic (carbon-based) nanomaterials such as fullerenes, and inorganic nanomaterials based on other elements, such as silicon. Examples of nanomaterials include fullerenes, carbon nanotubes, nanocrystals, etc.

A biomaterial is any matter, surface, or construct that interacts with biological systems. The study of biomaterials is called "bio materials science". It has experienced steady and strong growth over its history, with many companies investing large amounts of money into developing new products. Biomaterials science encompasses elements of medicine, biology, chemistry, tissue engineering, and materials science.

Biomaterials can be derived either from nature or synthesized in a laboratory using a variety of chemical approaches using metallic components, polymers, bioceramics, or composite materials. They are often intended or adapted for medical applications, such as biomedical devices which perform, augment, or replace a natural function. Such functions may be benign, like being used for a heart valve, or may be bioactive with a more interactive functionality such as hydroxylapatite-coated hip implants. Biomaterials are also used every day in dental applications, surgery, and drug delivery. For example, a construct with impregnated pharmaceutical products can be placed into the body, which permits the prolonged release of a drug over an extended period of time. A biomaterial may also be an autograft, allograft or xenograft used as an organ transplant material.

Semiconductors, metals, and ceramics are used today to form highly complex systems, such as integrated electronic circuits, optoelectronic devices, and magnetic and optical mass storage media. These materials form the basis of our modern computing world, and hence research into these materials is of vital importance.

Semiconductors are a traditional example of these types of materials. They are materials that have properties that are intermediate between conductors and insulators. Their electrical conductivities are very sensitive to the concentration of impurities, which allows the use of doping to achieve desirable electronic properties. Hence, semiconductors form the basis of the traditional computer.

This field also includes new areas of research such as superconducting materials, spintronics, metamaterials, etc. The study of these materials involves knowledge of materials science and solid-state physics or condensed matter physics.

With continuing increases in computing power, simulating the behavior of materials has become possible. This enables materials scientists to understand behavior and mechanisms, design new materials, and explain properties formerly poorly understood. Efforts surrounding Integrated computational materials engineering are now focusing on combining computational methods with experiments to drastically reduce the time and effort to optimize materials properties for a given application. This involves simulating materials at all length scales, using methods such as density functional theory, molecular dynamics, Monte Carlo, dislocation dynamics, phase field, finite element, and many more.

Radical materials advances can drive the creation of new products or even new industries, but stable industries also employ materials scientists to make incremental improvements and troubleshoot issues with currently used materials. Industrial applications of materials science include materials design, cost-benefit tradeoffs in industrial production of materials, processing methods (casting, rolling, welding, ion implantation, crystal growth, thin-film deposition, sintering, glassblowing, etc.), and analytic methods (characterization methods such as electron microscopy, X-ray diffraction, calorimetry, nuclear microscopy (HEFIB), Rutherford backscattering, neutron diffraction, small-angle X-ray scattering (SAXS), etc.).

Besides material characterization, the material scientist or engineer also deals with extracting materials and converting them into useful forms. Thus ingot casting, foundry methods, blast furnace extraction, and electrolytic extraction are all part of the required knowledge of a materials engineer. Often the presence, absence, or variation of minute quantities of secondary elements and compounds in a bulk material will greatly affect the final properties of the materials produced. For example, steels are classified based on 1/10 and 1/100 weight percentages of the carbon and other alloying elements they contain. Thus, the extracting and purifying methods used to extract iron in a blast furnace can affect the quality of steel that is produced.

Another application of material science is the structures of ceramics and glass typically associated with the most brittle materials. Bonding in ceramics and glasses uses covalent and ionic-covalent types with SiO (silica or sand) as a fundamental building block. Ceramics are as soft as clay or as hard as stone and concrete. Usually, they are crystalline in form. Most glasses contain a metal oxide fused with silica. At high temperatures used to prepare glass, the material is a viscous liquid. The structure of glass forms into an amorphous state upon cooling. Windowpanes and eyeglasses are important examples. Fibers of glass are also available. Scratch resistant Corning Gorilla Glass is a well-known example of the application of materials science to drastically improve the properties of common components. Diamond and carbon in its graphite form are considered to be ceramics.

Engineering ceramics are known for their stiffness and stability under high temperatures, compression and electrical stress. Alumina, silicon carbide, and tungsten carbide are made from a fine powder of their constituents in a process of sintering with a binder. Hot pressing provides higher density material. Chemical vapor deposition can place a film of a ceramic on another material. Cermets are ceramic particles containing some metals. The wear resistance of tools is derived from cemented carbides with the metal phase of cobalt and nickel typically added to modify properties.

Filaments are commonly used for reinforcement in composite materials.

Another application of materials science in industry is making composite materials. These are structured materials composed of two or more macroscopic phases. Applications range from structural elements such as steel-reinforced concrete, to the thermal insulating tiles which play a key and integral role in NASA's Space Shuttle thermal protection system which is used to protect the surface of the shuttle from the heat of re-entry into the Earth's atmosphere. One example is reinforced Carbon-Carbon (RCC), the light gray material which withstands re-entry temperatures up to and protects the Space Shuttle's wing leading edges and nose cap. RCC is a laminated composite material made from graphite rayon cloth and impregnated with a phenolic resin. After curing at high temperature in an autoclave, the laminate is pyrolized to convert the resin to carbon, impregnated with furfural alcohol in a vacuum chamber, and cured-pyrolized to convert the furfural alcohol to carbon. To provide oxidation resistance for reuse ability, the outer layers of the RCC are converted to silicon carbide.

Other examples can be seen in the "plastic" casings of television sets, cell-phones and so on. These plastic casings are usually a composite material made up of a thermoplastic matrix such as acrylonitrile butadiene styrene (ABS) in which calcium carbonate chalk, talc, glass fibers or carbon fibers have been added for added strength, bulk, or electrostatic dispersion. These additions may be termed reinforcing fibers, or dispersants, depending on their purpose.

Polymers are chemical compounds made up of a large number of identical components linked together like chains. They are an important part of materials science. Polymers are the raw materials (the resins) used to make what are commonly called plastics and rubber. Plastics and rubber are really the final product, created after one or more polymers or additives have been added to a resin during processing, which is then shaped into a final form. Plastics which have been around, and which are in current widespread use, include polyethylene, polypropylene, polyvinyl chloride (PVC), polystyrene, nylons, polyesters, acrylics, polyurethanes, and polycarbonates and also rubbers which have been around are natural rubber, styrene-butadiene rubber, chloroprene, and butadiene rubber. Plastics are generally classified as "commodity", "specialty" and "engineering" plastics.

Polyvinyl chloride (PVC) is widely used, inexpensive, and annual production quantities are large. It lends itself to a vast array of applications, from artificial leather to electrical insulation and cabling, packaging, and containers. Its fabrication and processing are simple and well-established. The versatility of PVC is due to the wide range of plasticisers and other additives that it accepts. The term "additives" in polymer science refers to the chemicals and compounds added to the polymer base to modify its material properties.

Polycarbonate would be normally considered an engineering plastic (other examples include PEEK, ABS). Such plastics are valued for their superior strengths and other special material properties. They are usually not used for disposable applications, unlike commodity plastics.

Specialty plastics are materials with unique characteristics, such as ultra-high strength, electrical conductivity, electro-fluorescence, high thermal stability, etc.

The dividing lines between the various types of plastics is not based on material but rather on their properties and applications. For example, polyethylene (PE) is a cheap, low friction polymer commonly used to make disposable bags for shopping and trash, and is considered a commodity plastic, whereas medium-density polyethylene (MDPE) is used for underground gas and water pipes, and another variety called ultra-high-molecular-weight polyethylene (UHMWPE) is an engineering plastic which is used extensively as the glide rails for industrial equipment and the low-friction socket in implanted hip joints.

The study of metal alloys is a significant part of materials science. Of all the metallic alloys in use today, the alloys of iron (steel, stainless steel, cast iron, tool steel, alloy steels) make up the largest proportion both by quantity and commercial value. Iron alloyed with various proportions of carbon gives low, mid and high carbon steels. An iron-carbon alloy is only considered steel if the carbon level is between 0.01% and 2.00%. For the steels, the hardness and tensile strength of the steel is related to the amount of carbon present, with increasing carbon levels also leading to lower ductility and toughness. Heat treatment processes such as quenching and tempering can significantly change these properties, however. Cast Iron is defined as an iron–carbon alloy with more than 2.00% but less than 6.67% carbon. Stainless steel is defined as a regular steel alloy with greater than 10% by weight alloying content of Chromium. Nickel and Molybdenum are typically also found in stainless steels.

Other significant metallic alloys are those of aluminium, titanium, copper and magnesium. Copper alloys have been known for a long time (since the Bronze Age), while the alloys of the other three metals have been relatively recently developed. Due to the chemical reactivity of these metals, the electrolytic extraction processes required were only developed relatively recently. The alloys of aluminium, titanium and magnesium are also known and valued for their high strength-to-weight ratios and, in the case of magnesium, their ability to provide electromagnetic shielding. These materials are ideal for situations where high strength-to-weight ratios are more important than bulk cost, such as in the aerospace industry and certain automotive engineering applications.

The study of semiconductors is a significant part of materials science. A semiconductor is a material that has a resistivity between a metal and insulator. Its electronic properties can be greatly altered through intentionally introducing impurities or doping. From these semiconductor materials, things such as diodes, transistors, light-emitting diodes (LEDs), and analog and digital electric circuits can be built, making them materials of interest in industry. Semiconductor devices have replaced thermionic devices (vacuum tubes) in most applications. Semiconductor devices are manufactured both as single discrete devices and as integrated circuits (ICs), which consist of a number—from a few to millions—of devices manufactured and interconnected on a single semiconductor substrate.

Of all the semiconductors in use today, silicon makes up the largest portion both by quantity and commercial value. Monocrystalline silicon is used to produce wafers used in the semiconductor and electronics industry. Second to silicon, gallium arsenide (GaAs) is the second most popular semiconductor used. Due to its higher electron mobility and saturation velocity compared to silicon, it is a material of choice for high-speed electronics applications. These superior properties are compelling reasons to use GaAs circuitry in mobile phones, satellite communications, microwave point-to-point links and higher frequency radar systems. Other semiconductor materials include germanium, silicon carbide, and gallium nitride and have various applications.

Materials science evolved—starting from the 1950s—because it was recognized that to create, discover and design new materials, one had to approach it in a unified manner. Thus, materials science and engineering emerged in many ways: renaming and/or combining existing metallurgy and ceramics engineering departments; splitting from existing solid state physics research (itself growing into condensed matter physics); pulling in relatively new polymer engineering and polymer science; recombining from the previous, as well as chemistry, chemical engineering, mechanical engineering, and electrical engineering; and more.

The field of materials science and engineering is important both from a scientific perspective, as well as towards applications. Materials are of the utmost importance for engineers (or other applied fields), as the usage of the appropriate materials is crucial when designing systems. As a result, materials science is an increasingly important part of an engineer's education.

The field is inherently interdisciplinary, and the materials scientists/engineers must be aware and make use of the methods of the physicist, chemist and engineer. Thus, there remain close relationships with these fields. Conversely, many physicists, chemists and engineers find themselves working in materials science due to the significant overlaps between the fields.

The main branches of materials science stem from the three main classes of materials: ceramics, metals, and polymers. 

There are additionally broadly applicable, materials independent, endeavors.

Further, there are relatively broad focuses across materials on specific phenomena.







</doc>
<doc id="19623" url="https://en.wikipedia.org/wiki?curid=19623" title="Mitsubishi A6M Zero">
Mitsubishi A6M Zero

The Mitsubishi A6M "Zero" is a long-range fighter aircraft formerly manufactured by Mitsubishi Aircraft Company, a part of Mitsubishi Heavy Industries, and operated by the Imperial Japanese Navy from 1940 to 1945. The A6M was designated as the , or the Mitsubishi A6M Rei-sen. The A6M was usually referred to by its pilots as the "Reisen" (, zero fighter), "0" being the last digit of the imperial year 2600 (1940) when it entered service with the Imperial Navy. The official Allied reporting name was "Zeke", although the name "Zero" (from Type 0) was used colloquially by the Allies as well.

The Zero is considered to have been the most capable carrier-based fighter in the world when it was introduced early in World War II, combining excellent maneuverability and very long range. The Imperial Japanese Navy Air Service (IJNAS) also frequently used it as a land-based fighter.

In early combat operations, the Zero gained a reputation as a dogfighter, achieving an outstanding kill ratio of 12 to 1, but by mid-1942 a combination of new tactics and the introduction of better equipment enabled Allied pilots to engage the Zero on generally equal terms. By 1943, due to inherent design weaknesses, such as a lack of hydraulic ailerons and rudder rendering it extremely unmaneuverable at high speeds, and an inability to equip it with a more powerful aircraft engine, the Zero gradually became less effective against newer Allied fighters. By 1944, with opposing Allied fighters approaching its levels of maneuverability and consistently exceeding its firepower, armor, and speed, the A6M had largely become outdated as a fighter aircraft. However, as design delays and production difficulties hampered the introduction of newer Japanese aircraft models, the Zero continued to serve in a front-line role until the end of the war in the Pacific. During the final phases, it was also adapted for use in "kamikaze" operations. Japan produced more Zeros than any other model of combat aircraft during the war.

The Mitsubishi A5M fighter was just entering service in early 1937, when the Imperial Japanese Navy (IJN) started looking for its eventual replacement. On October 5, 1937, it issued "Planning Requirements for the Prototype 12-shi Carrier-based Fighter", sending it to Nakajima and Mitsubishi. Both firms started preliminary design work while they awaited more definitive requirements to be handed over in a few months.

Based on the experiences of the A5M in China, the IJN sent out updated requirements in October calling for a speed of at and a climb to in 9.5 minutes. With drop tanks, it wanted an endurance of two hours at normal power, or six to eight hours at economical cruising speed. Armament was to consist of two 20 mm cannons, two 7.7 mm (.303 in) machine guns and two bombs. A complete radio set was to be mounted in all aircraft, along with a radio direction finder for long-range navigation. The maneuverability was to be at least equal to that of the A5M, while the wingspan had to be less than to allow for use on aircraft carriers.

Nakajima's team considered the new requirements unachievable and pulled out of the competition in January. Mitsubishi's chief designer, Jiro Horikoshi, thought that the requirements could be met, but only if the aircraft were made as light as possible. Every possible weight-saving measure was incorporated into the design. Most of the aircraft was built of a new top-secret aluminium alloy developed by Sumitomo Metal Industries in 1936. Called "extra super duralumin" (ESD), it was lighter, stronger and more ductile than other alloys (e.g. 24S alloy) used at the time, but was prone to corrosive attack, which made it brittle. This detrimental effect was countered with an anti-corrosion coating applied after fabrication. No armour protection was provided for the pilot, engine or other critical points of the aircraft, and self-sealing fuel tanks, which were becoming common at the time, were not used. This made the Zero lighter, more maneuverable, and the longest-ranged single-engine fighter of World War II, which made it capable of searching out an enemy hundreds of kilometres away, bringing it to battle, then returning to its base or aircraft carrier. However, that tradeoff in weight and construction also made it prone to catching fire and exploding when struck by enemy fire.

With its low-wing cantilever monoplane layout, retractable, wide-set conventional landing gear and enclosed cockpit, the Zero was one of the most modern carrier-based aircraft in the world at the time of its introduction. It had a fairly high-lift, low-speed wing with very low wing loading. This, combined with its light weight, resulted in a very low stalling speed of well below . This was the main reason for its phenomenal maneuverability, allowing it to out-turn any Allied fighter of the time. Early models were fitted with servo tabs on the ailerons after pilots complained that control forces became too heavy at speeds above . They were discontinued on later models after it was found that the lightened control forces were causing pilots to overstress the wings during vigorous maneuvers.

It has been claimed that the Zero's design showed a clear influence from British and American fighter aircraft and components exported to Japan in the 1930s, and in particular on the American side, the Vought V-143 fighter. Chance Vought had sold the prototype for this aircraft and its plans to Japan in 1937. Eugene Wilson, president of Vought, claimed that when shown a captured Zero in 1943, he found that "There on the floor was the Vought V 142 or just the spitting image of it, Japanese-made", while the "power-plant installation was distinctly Chance Vought, the wheel stowage into the wing roots came from Northrop, and the Japanese designers had even copied the Navy inspection stamp from Pratt & Whitney type parts." While the sale of the V-143 was fully legal, Wilson later acknowledged the conflicts of interest that can arise whenever military technology is exported. Counterclaims maintain that there was no significant relationship between the V-143 (which was an unsuccessful design that had been rejected by the U.S. Army Air Corps and several export customers) and the Zero, with only a superficial similarity in layout.

The Zero resembled the 1937 British Gloster F.5/34. Performance of the Gloster F.5/34 was comparable to that of early model Zeros, with its dimensions and appearance remarkably close to the Zero. Gloster had a relationship with the Japanese between the wars, with Nakajima building the carrier-based plane, the Gloster Gambet, under license. However allegations about the Zero being a copy have been discredited by some authors.

The A6M is usually known as the "Zero" from its Japanese Navy type designation, Type 0 carrier fighter ("Rei shiki Kanjō sentōki", ), taken from the last digit of the Imperial year 2600 (1940) when it entered service. In Japan, it was unofficially referred to as both "Rei-sen" and "Zero-sen"; Japanese pilots most commonly called it "Zero-sen," where "sen" is the first syllable of "sentōki," Japanese for "fighter plane".

In the official designation "A6M", the "A" signified a carrier-based fighter, "6" meant that it was the sixth such model built for the Imperial Navy, and "M" indicated Mitsubishi as the manufacturer.

The official Allied code name was "Zeke", in keeping with the practice of giving male names to Japanese fighters, female names to bombers, bird names to gliders, and tree names to trainers. "Zeke" was part of the first batch of "hillbilly" code names assigned by Captain Frank T. McCoy of Nashville, Tennessee (assigned to the Allied Technical Air Intelligence Unit (ATAIU) at Eagle Farm Airport in Australia), who wanted quick, distinctive, easy-to-remember names. The Allied code for Japanese aircraft was introduced in 1942, and McCoy chose "Zeke" for the "Zero". Later, two variants of the fighter received their own code names. The Nakajima A6M2-N floatplane version of the Zero was called "Rufe", and the A6M3-32 variant was initially called "Hap". General "Hap" Arnold, commander of the USAAF, objected to that name, however, so it was changed to "Hamp".

The first Zeros (pre-series of 15 A6M2) went into operation with the 12th Rengo Kōkūtai in July 1940. On 13 September 1940, the Zeros scored their first air-to-air victories when 13 A6M2s led by Lieutenant Saburo Shindo attacked 27 Soviet-built Polikarpov I-15s and I-16s of the Chinese Nationalist Air Force, shooting down all the fighters without loss to themselves. By the time they were redeployed a year later, the Zeros had shot down 99 Chinese aircraft (266 according to other sources).

At the time of the attack on Pearl Harbor, 521 Zeros were active in the Pacific, 328 in first-line units. The carrier-borne Model 21 was the type encountered by the Americans. Its tremendous range of over allowed it to range farther from its carrier than expected, appearing over distant battlefronts and giving Allied commanders the impression that there were several times as many Zeros as actually existed.

The Zero quickly gained a fearsome reputation. Thanks to a combination of unsurpassed maneuverability – compared to contemporary Axis fighters – and excellent firepower, it easily disposed of Allied aircraft sent against it in the Pacific in 1941. It proved a difficult opponent even for the Supermarine Spitfire. "The RAF pilots were trained in methods that were excellent against German and Italian equipment but suicide against the acrobatic Japs", as Lt.Gen. Claire Lee Chennault had to notice. Although not as fast as the British fighter, the Mitsubishi fighter could out-turn the Spitfire with ease, sustain a climb at a very steep angle, and stay in the air for three times as long.

Allied pilots soon developed tactics to cope with the Zero. Due to its extreme agility, engaging a Zero in a traditional, turning dogfight was likely to be fatal. It was better to swoop down from above in a high-speed pass, fire a quick burst, then climb quickly back up to altitude. A short burst of fire from heavy machine guns or cannon was often enough to bring down the fragile Zero. These tactics were regularly employed by Grumman F4F Wildcat fighters during Guadalcanal defense through high-altitude ambush, which was possible due to early warning system consisted of Coastwatchers and radar. Such "boom-and-zoom" tactics were also successfully used in the China Burma India Theater (CBI) by the "Flying Tigers" of the American Volunteer Group (AVG) against similarly maneuverable Japanese Army aircraft such as the Nakajima Ki-27 "Nate" and Nakajima Ki-43 "Oscar". AVG pilots were trained by their commander Claire Chennault to exploit the advantages of their P-40s, which were very sturdy, heavily armed, generally faster in a dive and level flight at low altitude, with a good rate of roll.

Another important maneuver was Lieutenant Commander John S. "Jimmy" Thach's "Thach Weave", in which two fighters would fly about apart. If a Zero latched onto the tail of one of the fighters, the two aircraft would turn toward each other. If the Zero followed his original target through the turn, he would come into a position to be fired on by the target's wingman. This tactic was first used to good effect during the Battle of Midway and later over the Solomon Islands.

Many highly experienced Japanese aviators were lost in combat, resulting in a progressive decline in quality, which became a significant factor in Allied successes. Unexpected heavy losses of pilots at the Battles of the Coral Sea and Midway dealt the Japanese carrier air force a blow from which it never fully recovered.
Throughout the Battle of Midway Allied pilots expressed a high level of dissatisfaction with the Grumman F4F Wildcat. The Commanding Officer of noted:
In contrast, Allied fighters were designed with ruggedness and pilot protection in mind. The Japanese ace Saburō Sakai described how the toughness of early Grumman aircraft was a factor in preventing the Zero from attaining total domination: 

When the powerfully armed Lockheed P-38 Lightning, armed with four "light barrel" AN/M2 .50 cal. Browning machine guns and one 20 mm autocannon, and the Grumman F6F Hellcat and Vought F4U Corsair, each with six AN/M2 heavy calibre Browning guns, appeared in the Pacific theater, the A6M, with its low-powered engine and lighter armament, was hard-pressed to remain competitive. In combat with an F6F or F4U, the only positive thing that could be said of the Zero at this stage of the war was that, in the hands of a skillful pilot, it could maneuver as well as most of its opponents. Nonetheless, in competent hands, the Zero could still be deadly.

Due to shortages of high-powered aviation engines and problems with planned successor models, the Zero remained in production until 1945, with over 10,000 of all variants produced.

The American military discovered many of the A6M's unique attributes when they recovered a largely intact specimen of an A6M2, the Akutan Zero, on Akutan Island in the Aleutians. During an air raid over Dutch Harbor on June 4, 1942, one A6M fighter was hit by ground-based anti-aircraft fire. Losing oil, Flight Petty Officer Tadayoshi Koga attempted an emergency landing on Akutan Island about northeast of Dutch Harbor, but his Zero flipped over on soft ground in a sudden crash-landing. Koga died instantly of head injuries (his neck was broken by the tremendous impact), but his wingmen hoped he had survived and so went against Japanese doctrine to destroy disabled Zeros. The relatively-undamaged fighter was found over a month later by an American salvage team and was shipped to Naval Air Station North Island, where testing flights of the repaired A6M revealed both strengths and deficiencies in design and performance.

The experts who evaluated the captured Zero found that the plane weighed about fully loaded, some lighter than the F4F Wildcat, the standard United States Navy fighter of the time. The A6M's airframe was "built like a fine watch"; the Zero was constructed with flush rivets, and even the guns were flush with the wings. The instrument panel was a "marvel of simplicity… with no superfluities to distract [the pilot]". What most impressed the experts was that the Zero's fuselage and wings were constructed in one piece, unlike the American method that built them separately and joined the two parts together. The Japanese method was much slower, but resulted in a very strong structure and improved close maneuverability.

American test pilots found that the Zero's controls were "very light" at , but stiffened at faster speeds (above ) to safeguard against wing failure. The Zero could not keep up with Allied aircraft in high-speed maneuvers, and its low "never exceed speed" (V) made it vulnerable in a dive. Testing also revealed that the Zero could not roll as quickly to the right as it could to the left, which could be exploited. While stable on the ground despite its light weight, the aircraft was designed purely for the attack role, emphasizing long range, maneuverability, and firepower at the expense of protection of its pilot. Most lacked self-sealing tanks and armor plating.

Captain Eric Brown, the Chief Naval Test Pilot of the Royal Navy, recalled being impressed by the Zero during tests of captured aircraft. "I don't think I have ever flown a fighter that could match the rate of turn of the Zero. The Zero had ruled the roost totally and was the finest fighter in the world until mid-1943."

The first two A6M1 prototypes were completed in March 1939, powered by the Mitsubishi Zuisei 13 engine with a two-blade propeller. It first flew on 1 April, and passed testing within a remarkably short period. By September, it had already been accepted for Navy testing as the A6M1 Type 0 Carrier Fighter, with the only notable change being a switch to a three-bladed propeller to cure a vibration problem.

While the navy was testing the first two prototypes, they suggested that the third be fitted with the Nakajima Sakae 12 engine instead. Mitsubishi had its own engine of this class in the form of the Kinsei, so they were somewhat reluctant to use the Sakae. Nevertheless, when the first A6M2 was completed in January 1940, the Sakae's extra power pushed the performance of the Zero well past the original specifications.

The new version was so promising that the Navy had 15 built and shipped to China before they had completed testing. They arrived in Manchuria in July 1940, and first saw combat over Chungking in August. There they proved to be completely untouchable by the Polikarpov I-16s and I-153s that had been such a problem for the A5Ms when in service. In one encounter, 13 Zeros shot down 27 I-15s and I-16s in under three minutes without loss. After hearing of these reports, the navy immediately ordered the A6M2 into production as the Type 0 Carrier Fighter, Model 11.
Reports of the Zero's performance filtered back to the US slowly. There they were dismissed by most military officials, who thought it was impossible for the Japanese to build such an aircraft.

After the delivery of the 65th aircraft, a further change was worked into the production lines, which introduced folding wingtips to allow them to fit on aircraft carriers. The resulting Model 21 would become one of the most produced versions early in the war. A feature was the improved range with wing tank and drop tank. When the lines switched to updated models, 740 Model 21s had been completed by Mitsubishi, and another 800 by Nakajima. Two other versions of the Model 21 were built in small numbers, the Nakajima-built A6M2-N "Rufe" floatplane (based on the Model 11 with a slightly modified tail), and the A6M2-K two-seat trainer of which a total of 508 were built by Hitachi and the Sasebo Naval Air Arsenal.

In 1941, Nakajima introduced the Sakae 21 engine, which used a two-speed supercharger for better altitude performance, and increased power to . A prototype Zero with the new engine was first flown on July 15, 1941.

The new Sakae was slightly heavier and somewhat longer due to the larger supercharger, which moved the center of gravity too far forward on the existing airframe. To correct for this, the engine mountings were cut back by to move the engine toward the cockpit. This had the side effect of reducing the size of the main fuselage fuel tank (located between the engine and the cockpit) from to . The cowling was redesigned to enlarge the cowl flaps, revise the oil cooler air intake, and move the carburetor air intake to the upper half of the cowling.

The wings were redesigned to reduce span, eliminate the folding tips, and square off the wingtips. The inboard edge of the aileron was moved outboard by one rib, and the wing fuel tanks were enlarged accordingly to . The two 20 mm wing cannon were upgraded from the Type 99 Mark l to the Type 99 Mark II, which required a bulge in the sheet metal of the wing below each cannon. The wings also included larger ammunition boxes and thus allowing 100 rounds per cannon.

The Sakae 21 engine and other changes increased maximum speed by only compared to the Model 21, but sacrificed nearly of range. Nevertheless, the navy accepted the type and it entered production in April 1942.

The shorter wing span led to better roll, and the reduced drag allowed the diving speed to be increased to . On the downside, turning and range, which were the strengths of the Model 21, suffered due to smaller ailerons, decreased lift and greater fuel consumption. The shorter range proved a significant limitation during the Solomons Campaign, during which Zeros based at Rabaul had to travel nearly to their maximum range to reach Guadalcanal and return. Consequently, the Model 32 was unsuited to that campaign and was used mainly for shorter range offensive missions and interception.

The appearance of the redesigned A6M3-32 prompted the US to assign the Model 32 a new code name, "Hap". This name was short-lived, as a protest from USAAF Commanding General Henry "Hap" Arnold forced a change to "Hamp". Soon after, it was realized that it was simply a new model of the "Zeke" and was termed "Zeke 32".

This variant was flown by only a small number of units, and only 343 were built.

In order to correct the deficiencies of the Model 32, a new version with folding wingtips and redesigned wing was introduced. The fuel tanks were moved to the outer wings, fuel lines for a drop tank were installed under each wing and the internal fuel capacity was increased to . More importantly, it regained its capabilities for long operating ranges, similar to the previous A6M2 Model 21, which was vastly shortened by the Model 32.

However, before the new design type was accepted formally by the Navy, the A6M3 Model 22 already stood ready for service in December 1942. Approximately 560 aircraft of the new type had been produced in the meantime by Mitsubishi Jukogyo K.K.

According to a theory, the very late production Model 22 might have had wings similar to the shortened, rounded-tip wing of the Model 52. One plane of such arrangement was photographed at Lakunai Airfield ("Rabaul East") in the second half of 1943, and has been published widely in a number of Japanese books. While the engine cowling is the same of previous Model 32 and 22, the theory proposes that the plane is an early production Model 52. 

The Model 32, 22, 22 kou, 52, 52 kou and 52 otsu were all powered by the Nakajima ("Sakae") engine. That engine kept its designation in spite of changes in the exhaust system for the Model 52.

Mitsubishi is unable to state with certainty that it ever used the designation "A6M4" or model numbers for it. However, "A6M4" does appear in a translation of a captured Japanese memo from a Naval Air Technical Arsenal, titled Quarterly Report on Research Experiments, dated 1 October 1942. It mentions a "cross-section of the A6M4 intercooler" then being designed. Some researchers believe "A6M4" was applied to one or two prototype planes fitted with an experimental turbo-supercharged Sakae engine designed for high altitude. Mitsubishi's involvement in the project was probably quite limited or nil; the unmodified Sakae engine was made by Nakajima. The design and testing of the turbo-supercharger was the responsibility of the First Naval Air [Technical] Arsenal (, "") at Yokosuka. At least one photo of a prototype plane exists. It shows a turbo unit mounted in the forward left fuselage.

Lack of suitable alloys for use in the manufacture of a turbo-supercharger and its related ducting caused numerous ruptures, resulting in fires and poor performance. Consequently, further development of a turbo-supercharged A6M was cancelled. The lack of acceptance by the navy suggests that the navy did not bestow model number 41 or 42 formally, although it appears that the arsenal did use the designation "A6M4". The prototype engines nevertheless provided useful experience for future engine designs.

Sometimes considered as the most effective variant, the Model 52 was developed to again shorten the wings to increase speed and dispense with the folding wing mechanism. In addition, ailerons, aileron trim tab and flaps were revised. Produced first by Mitsubishi, most Model 52s were made by Nakajima. The prototype was made in June 1943 by modifying an A6M3 and was first flown in August 1943. The first Model 52 is said in the handling manual to have production number 3904, which apparently refers to the prototype.

Research by Mr. Bunzo Komine published by Mr. Kenji Miyazaki states that aircraft 3904 through 4103 had the same exhaust system and cowl flaps as on the Model 22. This is partially corroborated by two wrecks researched by Mr. Stan Gajda and Mr. L. G. Halls, production number 4007 and 4043, respectively. (The upper cowling was slightly redesigned from that of the Model 22.)
An early production A6M5 Zero with non separated exhaust, with an A6M3 Model 22 in the background.
A new exhaust system provided an increment of thrust by aiming the stacks aft and distributing them around the forward fuselage. The new exhaust system required "notched" cowl flaps and heat shields just aft of the stacks. (Note, however, that the handling manual translation states that the new style of exhaust commenced with number 3904. Whether this is correct, indicates retrofitting intentions, refers to the prototype but not to all subsequent planes, or is in error is not clear.) From production number 4274, the wing fuel tanks received carbon dioxide fire extinguishers. From number 4354, the radio became the Model 3, aerial Mark 1, and at that point it is said the antenna mast was shortened slightly. Through production number 4550, the lowest exhaust stacks were approximately the same length as those immediately above them. This caused hot exhaust to burn the forward edge of the landing gear doors and heat the tires. Therefore, from number 4551 Mitsubishi began to install shorter bottom stacks. Nakajima manufactured the Model 52 at its Koizumi plant in Gunma Prefecture. The A6M5 had a maximum speed of ) at and reached that altitude in 7:01 minutes.

Subsequent variants included:

Some Model 21 and 52 aircraft were converted to "bakusen" (fighter-bombers) by mounting a bomb rack and bomb in place of the centerline drop tank.

Perhaps seven Model 52 planes were ostensibly converted into A6M5-K two-seat trainers. Mass production was contemplated by Hitachi, but not undertaken.

The A6M6 was developed to use the Sakae 31a engine, featuring water-methanol engine boost and self-sealing wing tanks. During preliminary testing, its performance was considered unsatisfactory due to the additional engine power failing to materialize and the unreliability of the fuel injection system. Testing continued on the A6M6 but the end of war stopped further development. Only one prototype was produced.

The A6M7 was the last variant to see service. It was designed to meet a requirement by the Navy for a dedicated attack/dive bomber version that could operate from smaller aircraft carriers or according to another source, replace the obsolete Aichi D3A. The A6M7 had considerable design changes compared to previous attempts to make the A6M suitable for dive bombing. This included a reinforced vertical stabilizer, a special bomb rack, provision of two 350 litre drop tanks and fixed bomb/rocket swing stoppers on the underside of the wings. It was also given a new powerplant, the Sakae-31 engine, producing 1,130hp on take-off. The A6M7 had a similar armament layout to the A6M5c with the exception of the bomb centreline bomb rack, capable of carrying 250kg or 500kg bombs. Entering production in May 1945,
the A6M7 was also used in the special attack role.

Similar to the A6M6 but with the Sakae (now out of production) replaced by the Mitsubishi Kinsei 62 engine with , 60% more powerful than the engine of the A6M2. This resulted in an extensively modified cowling and nose for the aircraft. The carburetor intake was much larger, a long duct like that on the Nakajima B6N Tenzan was added, and a large spinner—like that on the Yokosuka D4Y Suisei with the Kinsei 62—was mounted. The armament consisted of two 13.2 mm (.52 in) Type 3 machine guns and two 20 mm (.80 in) Type 99 cannons in the wings. In addition, the Model 64 was modified to carry two drop tanks on either wing in order to permit the mounting of a bomb on the underside of the fuselage. Two prototypes were completed in April 1945 but the chaotic situation of Japanese industry and the end of the war obstructed the start of the ambitious program of production for 6,300 A6M8s, only the two prototypes being completed and flown.

Not included:



Like many surviving World War II Japanese aircraft, most surviving Zeros are made up of parts from multiple airframes. As a result, some are referred to by conflicting manufacturer serial numbers. In other cases, such as those recovered after decades in a wrecked condition, they have been reconstructed to the point that the majority of their structure is made up of modern parts. All of this means the identities of survivors can be difficult to confirm.

Most flying Zeros have had their engines replaced with similar American units. Only one, the Planes of Fame Museum's A6M5, has the original Sakae engine.

The rarity of flyable Zeros accounts for the use of single-seat North American T-6 Texans, with heavily modified fuselages and painted in Japanese markings, as substitutes for Zeros in the films "Tora! Tora! Tora!", "The Final Countdown", and many other television and film depictions of the aircraft, such as "Baa Baa Black Sheep" (renamed "Black Sheep Squadron"). One Model 52 was used during the production of "Pearl Harbor".











[[Category:Carrier-based aircraft|Mitsubishi A6M Zero]]
[[Category:1930s Japanese fighter aircraft]]
[[Category:Mitsubishi aircraft]]
[[Category:Attack on Pearl Harbor]]
[[Category:World War II Japanese fighter aircraft]]
[[Category:Articles containing video clips]]
[[Category:Single-engined tractor aircraft]]
[[Category:Low-wing aircraft]]
[[Category:Aircraft first flown in 1939]]
[[Category:Retractable conventional landing gear]]

</doc>
<doc id="19624" url="https://en.wikipedia.org/wiki?curid=19624" title="May 27">
May 27





</doc>
<doc id="19626" url="https://en.wikipedia.org/wiki?curid=19626" title="Monasticism">
Monasticism

Monasticism (from Ancient Greek , , from , , 'alone') or monkhood, is a religious way of life in which one renounces worldly pursuits to devote oneself fully to spiritual work. Monastic life plays an important role in many Christian churches, especially in the Catholic and Orthodox traditions as well as in other faiths such as Buddhism, Hinduism and Jainism. In other religions monasticism is criticized and not practiced, as in Islam and Zoroastrianism, or plays a marginal role, as in modern Judaism. Women pursuing a monastic life are generally called "nuns", "religious" or "sisters" or rarely, Canonesses, while monastic men are called "monks", "friars" or "brothers". 

Many monastics live in abbeys, convents, monasteries or priories to separate themselves from the secular world, unless they are in mendicant or missionary orders. Titles for monastics differ between the Christian denominations. In Roman Catholicism and Anglicanism, monks and nuns are addressed as Brother (or Father, if ordained to the priesthood) or Mother/Sister, while in Eastern Orthodoxy, they are addressed as Father or Mother.

The Sangha or community of ordained Buddhist bhikkhus ("beggar" or "one who lives by alms".) and original bhikkhunis (nuns) was founded by Gautama Buddha during his lifetime over 2500 years ago. This communal monastic lifestyle grew out of the lifestyle of earlier sects of wandering ascetics, some of whom the Buddha had studied under. It was initially fairly eremitic or reclusive in nature. Bhikkhus and bhikkunis were expected to live with a minimum of possessions, which were to be voluntarily provided by the lay community. Lay followers also provided the daily food that bhikkhus required, and provided shelter for bhikkhus when they needed it.
After the Parinibbana (Final Passing) of the Buddha, the Buddhist monastic order developed into a primarily cenobitic or communal movement. The practice of living communally during the rainy vassa season, prescribed by the Buddha, gradually grew to encompass a settled monastic life centered on life in a community of practitioners. Most of the modern disciplinary rules followed by bhikkhus and bhikkhunis — as encoded in the Patimokkha — relate to such an existence, prescribing in great detail proper methods for living and relating in a community of bhikkhus or bhikkhunis. The number of rules observed varies with the order; Theravada bhikkhus follow around 227 rules, the Vinaya. There are a larger number of rules specified for bhikkhunis (nuns).

The Buddhist monastic order consists of the male bhikkhu assembly and the female bhikkhuni assembly. Initially consisting only of males, it grew to include females after the Buddha's stepmother, Mahaprajapati, asked for and received permission to live as an ordained practitioner.

Bhikkhus and bhikkhunis are expected to fulfill a variety of roles in the Buddhist community. First and foremost, they are expected to preserve the doctrine and discipline now known as Buddhism. They are also expected to provide a living example for the laity, and to serve as a "field of merit" for lay followers—providing laymen and women with the opportunity to earn merit by giving gifts and support to the bhikkhus. In return for the support of the laity, bhikkhus and bhikkhunis are expected to live an austere life focused on the study of Buddhist doctrine, the practice of meditation, and the observance of good moral character.

A bhikkhu (the term in the Pali language) or bhikshu (in Sanskrit), first ordains as a "Samanera" (novice). Novices often ordain at a young age, but generally no younger than eight. Samaneras live according to the Ten Precepts, but are not responsible for living by the full set of monastic rules. Higher ordination, conferring the status of a full Bhikkhu, is given only to men who are aged 20 or older. Bhikkhunis follow a similar progression, but are required to live as Samaneras for longer periods of time- typically five years.

The disciplinary regulations for bhikkhus and bhikkhunis are intended to create a life that is simple and focused, rather than one of deprivation or severe asceticism. However, celibacy is a fundamental part of this form of monastic discipline.

Monasticism in Christianity, which provides the origins of the words "monk" and "monastery", comprises several diverse forms of religious living. It began to develop early in the history of the Church, but is not mentioned in the scriptures. It has come to be regulated by religious rules (e.g. the Rule of St Basil, the Rule of St Benedict) and, in modern times, the Church law of the respective apostolic Christian churches that have forms of monastic living.

The Christian monk embraces the monastic life as a vocation from God. His objective is to imitate the life of Christ as far as possible in preparation for attaining eternal life after death. 

In 4th century Egypt, Christians felt called to a more reclusive or eremitic form of living (in the spirit of the "Desert Theology" for the purpose of spiritual renewal and return to God). Saint Anthony the Great is cited by Athanasius as one of the early "Hermit monks". Especially in the Middle East, eremitic monasticism continued to be common until the decline of Syriac Christianity in the late Middle Ages.

Around 318 Saint Pachomius started to organize his many followers in what was to become the first Christian cenobitic or communal monastery. Soon, similar institutions were established throughout the Egyptian desert as well as the rest of the eastern half of the Roman Empire. Notable monasteries in the East include:


In the West, the most significant development occurred when the rules for monastic communities were written down, the Rule of St Basil being credited with having been the first. The precise dating of the Rule of the Master is problematic. It has been argued that it antedates the Rule of Saint Benedict created by Benedict of Nursia for his monastery in Monte Cassino, Italy (c. 529), and the other Benedictine monasteries he had founded as part of the Order of St Benedict. It would become the most common rule throughout the Middle Ages and is still in use today. The Augustinian Rule, due to its brevity, has been adopted by various communities, chiefly the Canons Regular. Around the 12th century, the Franciscan, Carmelite, Dominican, Servite Order (see Servants of Mary) and Augustinian mendicant orders chose to live in city convents among the people instead of being secluded in monasteries. St. Augustine's Monastery, founded in 1277 in Erfurt, Germany is regarded by many historians and theologians as the "cradle of the Reformation", as it is where Martin Luther lived as a monk from 1505 to 1511.

Today new expressions of Christian monasticism, many of which are ecumenical, are developing in various places such as the Bose Monastic Community in Italy, the Monastic Fraternities of Jerusalem throughout Europe, the New Skete, the Anglo-Celtic Society of Nativitists, the Taizé Community in France, and the mainly Evangelical Protestant New Monasticism.

In their quest to attain the spiritual goal of life, some Hindus choose the path of monasticism (Sannyasa). Monastics commit themselves to a life of simplicity, celibacy, detachment from worldly pursuits, and the contemplation of God. A Hindu monk is called a s"anyāsī, sādhu", or "swāmi". A nun is called a "sanyāsini", "sādhvi", or "swāmini". Such renunciates are accorded high respect in Hindu society, because their outward renunciation of selfishness and worldliness serves as an inspiration to householders who strive for "mental" renunciation. Some monastics live in monasteries, while others wander from place to place, trusting in God alone to provide for their physical needs. It is considered a highly meritorious act for a lay devotee to provide sadhus with food or other necessaries. Sādhus are expected to treat all with respect and compassion, whether a person may be poor or rich, good or wicked. They are also expected to be indifferent to praise, blame, pleasure, and pain. A sādhu can typically be recognized by his ochre-colored clothing. Generally, Vaisnava monks shave their heads except for a small patch of hair on the back of the head, while Saivite monks let their hair and beard grow uncut.

A "sadhu's" vow of renunciation typically forbids him from:

Islam forbids the practice of monasticism. In Sunni Islam, one example is Uthman bin Maz'oon; one of the companions of Muhammad. He was married to Khawlah bint Hakim, both being two of the earliest converts to Islam. There is a Sunni narration that, out of religious devotion, Uthman bin Maz'oon decided to dedicate himself to night prayers and take a vow of chastity from his wife. His wife got upset and spoke to Muhammad about this. Muhammad reminded Uthman that he himself, as the Prophet, also had a family life, and that Uthman had a responsibility to his family and should not adopt monasticism as a form of religious practice.

Muhammad told his companions to ease their burden and avoid excess. According to some Sunni hadiths, in a message to some companions who wanted to put an end to their sexual life, pray all night long or fast continuously, Muhammad said: “Do not do that! Fast on some days and eat on others. Sleep part of the night, and stand in prayer another part. For your body has rights upon you, your eyes have a right upon you, your wife has a right upon you, your guest has a right upon you.” Muhammad once exclaimed, repeating it three times: “Woe to those who exaggerate [who are too strict]!” And, on another occasion, Muhammad said: “Moderation, moderation! For only with moderation will you succeed.”

Monasticism is also mentioned in four places in the following verses of Qur'an:

They have taken as lords beside Allah their rabbis and their monks and the Messiah son of Mary, when they were bidden to worship only One God. There is no god save Him. Be He glorified from all that they ascribe as partner (unto Him)!

O ye who believe! Lo! many of the (Jewish) rabbis and the (Christian) monks devour the wealth of mankind wantonly and debar (men) from the way of Allah. They who hoard up gold and silver and spend it not in the way of Allah, unto them give tidings (O Muhammad) of a painful doom

Thou wilt find the most vehement of mankind in hostility to those who believe (to be) the Jews and the idolaters. And thou wilt find the nearest of them in affection to those who believe (to be) those who say: Lo! We are Christians. That is because there are among them priests and monks, and because they are not proud.

In Jainism, monasticism is encouraged and respected. Rules for monasticism are rather strict. A Jain ascetic has neither a permanent home nor any possessions, wandering barefoot from place to place except during the months of Chaturmas. The quality of life they lead is difficult because of the many constraints placed on them. They don't use a vehicle for commuting and always commute barefoot from one place to another, irrespective of the distance. They don't possess any materialistic things and also don't use the basic services like that of a phone, electricity etc. They don't prepare food and live only on what people offer them.

Judaism does not encourage the monastic ideal of celibacy and poverty. To the contrary—all of the Torah's Commandments are a means of sanctifying the physical world. As further disseminated through the teachings of the Yisrael Ba'al Shem Tov, the pursuit of permitted physical pleasures is encouraged as a means to "serve God with joy" (Deut. 28:47).

However, until the Destruction of the Second Temple, about two thousand years ago, taking Nazirite vows was a common feature of the religion. Nazirite Jews (in Hebrew: נזיר) abstained from grape products, haircuts, and contact with the dead. However, they did not withdraw from general society, and they were permitted to marry and own property; moreover, in most cases a Nazirite vow was for a specified time period and not permanent. In Modern Hebrew, the term "Nazir" is most often used to refer to non-Jewish monastics.

Unique among Jewish communities is the monasticism of the Beta Israel of Ethiopia, a practice believed to date to the 15th century.

A form of asceticism was practiced by some individuals in pre–World War II European Jewish communities. Its principal expression was "prishut", the practice of a married Talmud student going into self-imposed exile from his home and family to study in the kollel of a different city or town. This practice was associated with, but not exclusive to, the Perushim.

The Essenes (in Modern but not in Ancient Hebrew: , "Isiyim"; Greek: Εσσηνοι, Εσσαιοι, or Οσσαιοι; "Essēnoi", "Essaioi", or "Ossaioi") were a Jewish sect that flourished from the 2nd century BC to AD 100 which some scholars claim seceded from the Zadokite priests. Being much fewer in number than the Pharisees and the Sadducees (the other two major sects at the time), the Essenes lived in various cities but congregated in communal life dedicated to asceticism, voluntary poverty, daily immersion (in mikvah), and abstinence from worldly pleasures, including (for some groups) marriage. Many separate but related religious groups of that era shared similar mystic, eschatological, messianic, and ascetic beliefs. These groups are collectively referred to by various scholars as the "Essenes". Josephus records that Essenes existed in large numbers, and thousands lived throughout Roman Judaea.

The Essenes have gained fame in modern times as a result of the discovery of an extensive group of religious documents known as the Dead Sea Scrolls, which are commonly believed to be the Essenes' library—although there is no proof that the Essenes wrote them. These documents include multiple preserved copies of the Hebrew Bible which were untouched from as early as 300 years before Christ until their discovery in 1946. Some scholars, however, dispute the notion that the Essenes wrote the Dead Sea Scrolls. Rachel Elior, a prominent Israeli scholar, even questions the existence of the Essenes.

Throughout the centuries Taoism developed its own extensive monastic traditions and practices. Particularly well known is the White Cloud Monastery in Beijing, which houses a rare complete copy of the "Daozang", a major Taoist Scripture.






</doc>
<doc id="19629" url="https://en.wikipedia.org/wiki?curid=19629" title="May 10">
May 10





</doc>
<doc id="19631" url="https://en.wikipedia.org/wiki?curid=19631" title="May 17">
May 17





</doc>
<doc id="19632" url="https://en.wikipedia.org/wiki?curid=19632" title="May 19">
May 19




</doc>
<doc id="19633" url="https://en.wikipedia.org/wiki?curid=19633" title="March 3">
March 3





</doc>
<doc id="19635" url="https://en.wikipedia.org/wiki?curid=19635" title="March 15">
March 15

In the Roman calendar, March 15 was known as the Ides of March.




</doc>
<doc id="19636" url="https://en.wikipedia.org/wiki?curid=19636" title="Mathematical logic">
Mathematical logic

Mathematical logic is a subfield of mathematics exploring the applications of formal logic to mathematics. It bears close connections to metamathematics, the foundations of mathematics, and theoretical computer science. The unifying themes in mathematical logic include the study of the expressive power of formal systems and the deductive power of formal proof systems.
Mathematical logic is often divided into the fields of set theory, model theory, recursion theory, and proof theory. These areas share basic results on logic, particularly first-order logic, and definability. In computer science (particularly in the ACM Classification) mathematical logic encompasses additional topics not detailed in this article; see Logic in computer science for those.

Since its inception, mathematical logic has both contributed to, and has been motivated by, the study of foundations of mathematics. This study began in the late 19th century with the development of axiomatic frameworks for geometry, arithmetic, and analysis. In the early 20th century it was shaped by David Hilbert's program to prove the consistency of foundational theories. Results of Kurt Gödel, Gerhard Gentzen, and others provided partial resolution to the program, and clarified the issues involved in proving consistency. Work in set theory showed that almost all ordinary mathematics can be formalized in terms of sets, although there are some theorems that cannot be proven in common axiom systems for set theory. Contemporary work in the foundations of mathematics often focuses on establishing which parts of mathematics can be formalized in particular formal systems (as in reverse mathematics) rather than trying to find theories in which all of mathematics can be developed.

The "Handbook of Mathematical Logic" in 1977 makes a rough division of contemporary mathematical logic into four areas:
Each area has a distinct focus, although many techniques and results are shared among multiple areas. The borderlines amongst these fields, and the lines separating mathematical logic and other fields of mathematics, are not always sharp. Gödel's incompleteness theorem marks not only a milestone in recursion theory and proof theory, but has also led to Löb's theorem in modal logic. The method of forcing is employed in set theory, model theory, and recursion theory, as well as in the study of intuitionistic mathematics.

The mathematical field of category theory uses many formal axiomatic methods, and includes the study of categorical logic, but category theory is not ordinarily considered a subfield of mathematical logic. Because of its applicability in diverse fields of mathematics, mathematicians including Saunders Mac Lane have proposed category theory as a foundational system for mathematics, independent of set theory. These foundations use toposes, which resemble generalized models of set theory that may employ classical or nonclassical logic.

Mathematical logic emerged in the mid-19th century as a subfield of mathematics, reflecting the confluence of two traditions: formal philosophical logic and mathematics (Ferreirós 2001, p. 443). "Mathematical logic, also called 'logistic', 'symbolic logic', the 'algebra of logic', and, more recently, simply 'formal logic', is the set of logical theories elaborated in the course of the last [nineteenth] century with the aid of an artificial notation and a rigorously deductive method." Before this emergence, logic was studied with rhetoric, with "calculationes", through the syllogism, and with philosophy. The first half of the 20th century saw an explosion of fundamental results, accompanied by vigorous debate over the foundations of mathematics.

Theories of logic were developed in many cultures in history, including China, India, Greece and the Islamic world. Greek methods, particularly Aristotelian logic (or term logic) as found in the "Organon", found wide application and acceptance in Western science and mathematics for millennia. The Stoics, especially Chrysippus, began the development of predicate logic. In 18th-century Europe, attempts to treat the operations of formal logic in a symbolic or algebraic way had been made by philosophical mathematicians including Leibniz and Lambert, but their labors remained isolated and little known.

In the middle of the nineteenth century, George Boole and then Augustus De Morgan presented systematic mathematical treatments of logic. Their work, building on work by algebraists such as George Peacock, extended the traditional Aristotelian doctrine of logic into a sufficient framework for the study of foundations of mathematics (Katz 1998, p. 686).

Charles Sanders Peirce built upon the work of Boole to develop a logical system for relations and quantifiers, which he published in several papers from 1870 to 1885.
Gottlob Frege presented an independent development of logic with quantifiers in his "Begriffsschrift", published in 1879, a work generally considered as marking a turning point in the history of logic. Frege's work remained obscure, however, until Bertrand Russell began to promote it near the turn of the century. The two-dimensional notation Frege developed was never widely adopted and is unused in contemporary texts.

From 1890 to 1905, Ernst Schröder published "Vorlesungen über die Algebra der Logik" in three volumes. This work summarized and extended the work of Boole, De Morgan, and Peirce, and was a comprehensive reference to symbolic logic as it was understood at the end of the 19th century.

Concerns that mathematics had not been built on a proper foundation led to the development of axiomatic systems for fundamental areas of mathematics such as arithmetic, analysis, and geometry.
In logic, the term "arithmetic" refers to the theory of the natural numbers. Giuseppe Peano (1889) published a set of axioms for arithmetic that came to bear his name (Peano axioms), using a variation of the logical system of Boole and Schröder but adding quantifiers. Peano was unaware of Frege's work at the time. Around the same time Richard Dedekind showed that the natural numbers are uniquely characterized by their induction properties. Dedekind (1888) proposed a different characterization, which lacked the formal logical character of Peano's axioms. Dedekind's work, however, proved theorems inaccessible in Peano's system, including the uniqueness of the set of natural numbers (up to isomorphism) and the recursive definitions of addition and multiplication from the successor function and mathematical induction.
In the mid-19th century, flaws in Euclid's axioms for geometry became known (Katz 1998, p. 774). In addition to the independence of the parallel postulate, established by Nikolai Lobachevsky in 1826 (Lobachevsky 1840), mathematicians discovered that certain theorems taken for granted by Euclid were not in fact provable from his axioms. Among these is the theorem that a line contains at least two points, or that circles of the same radius whose centers are separated by that radius must intersect. Hilbert (1899) developed a complete set of axioms for geometry, building on previous work by Pasch (1882). The success in axiomatizing geometry motivated Hilbert to seek complete axiomatizations of other areas of mathematics, such as the natural numbers and the real line. This would prove to be a major area of research in the first half of the 20th century.

The 19th century saw great advances in the theory of real analysis, including theories of convergence of functions and Fourier series. Mathematicians such as Karl Weierstrass began to construct functions that stretched intuition, such as nowhere-differentiable continuous functions. Previous conceptions of a function as a rule for computation, or a smooth graph, were no longer adequate. Weierstrass began to advocate the arithmetization of analysis, which sought to axiomatize analysis using properties of the natural numbers. The modern (ε, δ)-definition of limit and continuous functions was already developed by Bolzano in 1817 (Felscher 2000), but remained relatively unknown.
Cauchy in 1821 defined continuity in terms of infinitesimals (see Cours d'Analyse, page 34). In 1858, Dedekind proposed a definition of the real numbers in terms of Dedekind cuts of rational numbers (Dedekind 1872), a definition still employed in contemporary texts.

Georg Cantor developed the fundamental concepts of infinite set theory. His early results developed the theory of cardinality and proved that the reals and the natural numbers have different cardinalities (Cantor 1874). Over the next twenty years, Cantor developed a theory of transfinite numbers in a series of publications. In 1891, he published a new proof of the uncountability of the real numbers that introduced the diagonal argument, and used this method to prove Cantor's theorem that no set can have the same cardinality as its powerset. Cantor believed that every set could be well-ordered, but was unable to produce a proof for this result, leaving it as an open problem in 1895 (Katz 1998, p. 807).

In the early decades of the 20th century, the main areas of study were set theory and formal logic. The discovery of paradoxes in informal set theory caused some to wonder whether mathematics itself is inconsistent, and to look for proofs of consistency.

In 1900, Hilbert posed a famous list of 23 problems for the next century. The first two of these were to resolve the continuum hypothesis and prove the consistency of elementary arithmetic, respectively; the tenth was to produce a method that could decide whether a multivariate polynomial equation over the integers has a solution. Subsequent work to resolve these problems shaped the direction of mathematical logic, as did the effort to resolve Hilbert's "Entscheidungsproblem", posed in 1928. This problem asked for a procedure that would decide, given a formalized mathematical statement, whether the statement is true or false.

Ernst Zermelo (1904) gave a proof that every set could be well-ordered, a result Georg Cantor had been unable to obtain. To achieve the proof, Zermelo introduced the axiom of choice, which drew heated debate and research among mathematicians and the pioneers of set theory. The immediate criticism of the method led Zermelo to publish a second exposition of his result, directly addressing criticisms of his proof (Zermelo 1908a). This paper led to the general acceptance of the axiom of choice in the mathematics community.

Skepticism about the axiom of choice was reinforced by recently discovered paradoxes in naive set theory. Cesare Burali-Forti (1897) was the first to state a paradox: the Burali-Forti paradox shows that the collection of all ordinal numbers cannot form a set. Very soon thereafter, Bertrand Russell discovered Russell's paradox in 1901, and Jules Richard (1905) discovered Richard's paradox.

Zermelo (1908b) provided the first set of axioms for set theory. These axioms, together with the additional axiom of replacement proposed by Abraham Fraenkel, are now called Zermelo–Fraenkel set theory (ZF). Zermelo's axioms incorporated the principle of limitation of size to avoid Russell's paradox.

In 1910, the first volume of "Principia Mathematica" by Russell and Alfred North Whitehead was published. This seminal work developed the theory of functions and cardinality in a completely formal framework of type theory, which Russell and Whitehead developed in an effort to avoid the paradoxes. "Principia Mathematica" is considered one of the most influential works of the 20th century, although the framework of type theory did not prove popular as a foundational theory for mathematics (Ferreirós 2001, p. 445).

Fraenkel (1922) proved that the axiom of choice cannot be proved from the axioms of Zermelo's set theory with urelements. Later work by Paul Cohen (1966) showed that the addition of urelements is not needed, and the axiom of choice is unprovable in ZF. Cohen's proof developed the method of forcing, which is now an important tool for establishing independence results in set theory.

Leopold Löwenheim (1915) and Thoralf Skolem (1920) obtained the Löwenheim–Skolem theorem, which says that first-order logic cannot control the cardinalities of infinite structures. Skolem realized that this theorem would apply to first-order formalizations of set theory, and that it implies any such formalization has a countable model. This counterintuitive fact became known as Skolem's paradox.

In his doctoral thesis, Kurt Gödel (1929) proved the completeness theorem, which establishes a correspondence between syntax and semantics in first-order logic. Gödel used the completeness theorem to prove the compactness theorem, demonstrating the finitary nature of first-order logical consequence. These results helped establish first-order logic as the dominant logic used by mathematicians.

In 1931, Gödel published "On Formally Undecidable Propositions of Principia Mathematica and Related Systems", which proved the incompleteness (in a different meaning of the word) of all sufficiently strong, effective first-order theories. This result, known as Gödel's incompleteness theorem, establishes severe limitations on axiomatic foundations for mathematics, striking a strong blow to Hilbert's program. It showed the impossibility of providing a consistency proof of arithmetic within any formal theory of arithmetic. Hilbert, however, did not acknowledge the importance of the incompleteness theorem for some time.

Gödel's theorem shows that a consistency proof of any sufficiently strong, effective axiom system cannot be obtained in the system itself, if the system is consistent, nor in any weaker system. This leaves open the possibility of consistency proofs that cannot be formalized within the system they consider. Gentzen (1936) proved the consistency of arithmetic using a finitistic system together with a principle of transfinite induction. Gentzen's result introduced the ideas of cut elimination and proof-theoretic ordinals, which became key tools in proof theory. Gödel (1958) gave a different consistency proof, which reduces the consistency of classical arithmetic to that of intuitionistic arithmetic in higher types.

Alfred Tarski developed the basics of model theory.

Beginning in 1935, a group of prominent mathematicians collaborated under the pseudonym Nicolas Bourbaki to publish "Éléments de mathématique", a series of encyclopedic mathematics texts. These texts, written in an austere and axiomatic style, emphasized rigorous presentation and set-theoretic foundations. Terminology coined by these texts, such as the words "bijection", "injection", and "surjection", and the set-theoretic foundations the texts employed, were widely adopted throughout mathematics.

The study of computability came to be known as recursion theory or computability theory, because early formalizations by Gödel and Kleene relied on recursive definitions of functions. When these definitions were shown equivalent to Turing's formalization involving Turing machines, it became clear that a new concept – the computable function – had been discovered, and that this definition was robust enough to admit numerous independent characterizations. In his work on the incompleteness theorems in 1931, Gödel lacked a rigorous concept of an effective formal system; he immediately realized that the new definitions of computability could be used for this purpose, allowing him to state the incompleteness theorems in generality that could only be implied in the original paper.

Numerous results in recursion theory were obtained in the 1940s by Stephen Cole Kleene and Emil Leon Post. Kleene (1943) introduced the concepts of relative computability, foreshadowed by Turing (1939), and the arithmetical hierarchy. Kleene later generalized recursion theory to higher-order functionals. Kleene and Georg Kreisel studied formal versions of intuitionistic mathematics, particularly in the context of proof theory.

At its core, mathematical logic deals with mathematical concepts expressed using formal logical systems. These systems, though they differ in many details, share the common property of considering only expressions in a fixed formal language. The systems of propositional logic and first-order logic are the most widely studied today, because of their applicability to foundations of mathematics and because of their desirable proof-theoretic properties. Stronger classical logics such as second-order logic or infinitary logic are also studied, along with Non-classical logics such as intuitionistic logic.

First-order logic is a particular formal system of logic. Its syntax involves only finite expressions as well-formed formulas, while its semantics are characterized by the limitation of all quantifiers to a fixed domain of discourse.

Early results from formal logic established limitations of first-order logic. The Löwenheim–Skolem theorem (1919) showed that if a set of sentences in a countable first-order language has an infinite model then it has at least one model of each infinite cardinality. This shows that it is impossible for a set of first-order axioms to characterize the natural numbers, the real numbers, or any other infinite structure up to isomorphism. As the goal of early foundational studies was to produce axiomatic theories for all parts of mathematics, this limitation was particularly stark.

Gödel's completeness theorem (Gödel 1929) established the equivalence between semantic and syntactic definitions of logical consequence in first-order logic. It shows that if a particular sentence is true in every model that satisfies a particular set of axioms, then there must be a finite deduction of the sentence from the axioms. The compactness theorem first appeared as a lemma in Gödel's proof of the completeness theorem, and it took many years before logicians grasped its significance and began to apply it routinely. It says that a set of sentences has a model if and only if every finite subset has a model, or in other words that an inconsistent set of formulas must have a finite inconsistent subset. The completeness and compactness theorems allow for sophisticated analysis of logical consequence in first-order logic and the development of model theory, and they are a key reason for the prominence of first-order logic in mathematics.

Gödel's incompleteness theorems (Gödel 1931) establish additional limits on first-order axiomatizations. The first incompleteness theorem states that for any consistent, effectively given (defined below) logical system that is capable of interpreting arithmetic, there exists a statement that is true (in the sense that it holds for the natural numbers) but not provable within that logical system (and which indeed may fail in some non-standard models of arithmetic which may be consistent with the logical system). For example, in every logical system capable of expressing the Peano axioms, the Gödel sentence holds for the natural numbers but cannot be proved.

Here a logical system is said to be effectively given if it is possible to decide, given any formula in the language of the system, whether the formula is an axiom, and one which can express the Peano axioms is called "sufficiently strong." When applied to first-order logic, the first incompleteness theorem implies that any sufficiently strong, consistent, effective first-order theory has models that are not elementarily equivalent, a stronger limitation than the one established by the Löwenheim–Skolem theorem. The second incompleteness theorem states that no sufficiently strong, consistent, effective axiom system for arithmetic can prove its own consistency, which has been interpreted to show that Hilbert's program cannot be reached.

Many logics besides first-order logic are studied. These include infinitary logics, which allow for formulas to provide an infinite amount of information, and higher-order logics, which include a portion of set theory directly in their semantics.

The most well studied infinitary logic is formula_1. In this logic, quantifiers may only be nested to finite depths, as in first-order logic, but formulas may have finite or countably infinite conjunctions and disjunctions within them. Thus, for example, it is possible to say that an object is a whole number using a formula of formula_1 such as

Higher-order logics allow for quantification not only of elements of the domain of discourse, but subsets of the domain of discourse, sets of such subsets, and other objects of higher type. The semantics are defined so that, rather than having a separate domain for each higher-type quantifier to range over, the quantifiers instead range over all objects of the appropriate type. The logics studied before the development of first-order logic, for example Frege's logic, had similar set-theoretic aspects. Although higher-order logics are more expressive, allowing complete axiomatizations of structures such as the natural numbers, they do not satisfy analogues of the completeness and compactness theorems from first-order logic, and are thus less amenable to proof-theoretic analysis.

Another type of logics are s that allow inductive definitions, like one writes for primitive recursive functions.

One can formally define an extension of first-order logic — a notion which encompasses all logics in this section because they behave like first-order logic in certain fundamental ways, but does not encompass all logics in general, e.g. it does not encompass intuitionistic, modal or fuzzy logic. 

Lindström's theorem implies that the only extension of first-order logic satisfying both the compactness theorem and the downward Löwenheim–Skolem theorem is first-order logic.

Modal logics include additional modal operators, such as an operator which states that a particular formula is not only true, but necessarily true. Although modal logic is not often used to axiomatize mathematics, it has been used to study the properties of first-order provability (Solovay 1976) and set-theoretic forcing (Hamkins and Löwe 2007).

Intuitionistic logic was developed by Heyting to study Brouwer's program of intuitionism, in which Brouwer himself avoided formalization. Intuitionistic logic specifically does not include the law of the excluded middle, which states that each sentence is either true or its negation is true. Kleene's work with the proof theory of intuitionistic logic showed that constructive information can be recovered from intuitionistic proofs. For example, any provably total function in intuitionistic arithmetic is computable; this is not true in classical theories of arithmetic such as Peano arithmetic.

Algebraic logic uses the methods of abstract algebra to study the semantics of formal logics. A fundamental example is the use of Boolean algebras to represent truth values in classical propositional logic, and the use of Heyting algebras to represent truth values in intuitionistic propositional logic. Stronger logics, such as first-order logic and higher-order logic, are studied using more complicated algebraic structures such as cylindric algebras.

Set theory is the study of sets, which are abstract collections of objects. Many of the basic notions, such as ordinal and cardinal numbers, were developed informally by Cantor before formal axiomatizations of set theory were developed. The first such axiomatization, due to Zermelo (1908b), was extended slightly to become Zermelo–Fraenkel set theory (ZF), which is now the most widely used foundational theory for mathematics.

Other formalizations of set theory have been proposed, including von Neumann–Bernays–Gödel set theory (NBG), Morse–Kelley set theory (MK), and New Foundations (NF). Of these, ZF, NBG, and MK are similar in describing a cumulative hierarchy of sets. New Foundations takes a different approach; it allows objects such as the set of all sets at the cost of restrictions on its set-existence axioms. The system of Kripke–Platek set theory is closely related to generalized recursion theory.

Two famous statements in set theory are the axiom of choice and the continuum hypothesis. The axiom of choice, first stated by Zermelo (1904), was proved independent of ZF by Fraenkel (1922), but has come to be widely accepted by mathematicians. It states that given a collection of nonempty sets there is a single set "C" that contains exactly one element from each set in the collection. The set "C" is said to "choose" one element from each set in the collection. While the ability to make such a choice is considered obvious by some, since each set in the collection is nonempty, the lack of a general, concrete rule by which the choice can be made renders the axiom nonconstructive. Stefan Banach and Alfred Tarski (1924) showed that the axiom of choice can be used to decompose a solid ball into a finite number of pieces which can then be rearranged, with no scaling, to make two solid balls of the original size. This theorem, known as the Banach–Tarski paradox, is one of many counterintuitive results of the axiom of choice.

The continuum hypothesis, first proposed as a conjecture by Cantor, was listed by David Hilbert as one of his 23 problems in 1900. Gödel showed that the continuum hypothesis cannot be disproven from the axioms of Zermelo–Fraenkel set theory (with or without the axiom of choice), by developing the constructible universe of set theory in which the continuum hypothesis must hold. In 1963, Paul Cohen showed that the continuum hypothesis cannot be proven from the axioms of Zermelo–Fraenkel set theory (Cohen 1966). This independence result did not completely settle Hilbert's question, however, as it is possible that new axioms for set theory could resolve the hypothesis. Recent work along these lines has been conducted by W. Hugh Woodin, although its importance is not yet clear (Woodin 2001).

Contemporary research in set theory includes the study of large cardinals and determinacy. Large cardinals are cardinal numbers with particular properties so strong that the existence of such cardinals cannot be proved in ZFC. The existence of the smallest large cardinal typically studied, an inaccessible cardinal, already implies the consistency of ZFC. Despite the fact that large cardinals have extremely high cardinality, their existence has many ramifications for the structure of the real line. "Determinacy" refers to the possible existence of winning strategies for certain two-player games (the games are said to be "determined"). The existence of these strategies implies structural properties of the real line and other Polish spaces.

Model theory studies the models of various formal theories. Here a theory is a set of formulas in a particular formal logic and signature, while a model is a structure that gives a concrete interpretation of the theory. Model theory is closely related to universal algebra and algebraic geometry, although the methods of model theory focus more on logical considerations than those fields.

The set of all models of a particular theory is called an elementary class; classical model theory seeks to determine the properties of models in a particular elementary class, or determine whether certain classes of structures form elementary classes.

The method of quantifier elimination can be used to show that definable sets in particular theories cannot be too complicated. Tarski (1948) established quantifier elimination for real-closed fields, a result which also shows the theory of the field of real numbers is decidable. (He also noted that his methods were equally applicable to algebraically closed fields of arbitrary characteristic.) A modern subfield developing from this is concerned with o-minimal structures.

Morley's categoricity theorem, proved by Michael D. Morley (1965), states that if a first-order theory in a countable language is categorical in some uncountable cardinality, i.e. all models of this cardinality are isomorphic, then it is categorical in all uncountable cardinalities.

A trivial consequence of the continuum hypothesis is that a complete theory with less than continuum many nonisomorphic countable models can have only countably many. Vaught's conjecture, named after Robert Lawson Vaught, says that this is true even independently of the continuum hypothesis. Many special cases of this conjecture have been established.

Recursion theory, also called computability theory, studies the properties of computable functions and the Turing degrees, which divide the uncomputable functions into sets that have the same level of uncomputability. Recursion theory also includes the study of generalized computability and definability. Recursion theory grew from the work of Rózsa Péter, Alonzo Church and Alan Turing in the 1930s, which was greatly extended by Kleene and Post in the 1940s.

Classical recursion theory focuses on the computability of functions from the natural numbers to the natural numbers. The fundamental results establish a robust, canonical class of computable functions with numerous independent, equivalent characterizations using Turing machines, λ calculus, and other systems. More advanced results concern the structure of the Turing degrees and the lattice of recursively enumerable sets.

Generalized recursion theory extends the ideas of recursion theory to computations that are no longer necessarily finite. It includes the study of computability in higher types as well as areas such as hyperarithmetical theory and α-recursion theory.

Contemporary research in recursion theory includes the study of applications such as algorithmic randomness, computable model theory, and reverse mathematics, as well as new results in pure recursion theory.

An important subfield of recursion theory studies algorithmic unsolvability; a decision problem or function problem is algorithmically unsolvable if there is no possible computable algorithm that returns the correct answer for all legal inputs to the problem. The first results about unsolvability, obtained independently by Church and Turing in 1936, showed that the Entscheidungsproblem is algorithmically unsolvable. Turing proved this by establishing the unsolvability of the halting problem, a result with far-ranging implications in both recursion theory and computer science.

There are many known examples of undecidable problems from ordinary mathematics. The word problem for groups was proved algorithmically unsolvable by Pyotr Novikov in 1955 and independently by W. Boone in 1959. The busy beaver problem, developed by Tibor Radó in 1962, is another well-known example.

Hilbert's tenth problem asked for an algorithm to determine whether a multivariate polynomial equation with integer coefficients has a solution in the integers. Partial progress was made by Julia Robinson, Martin Davis and Hilary Putnam. The algorithmic unsolvability of the problem was proved by Yuri Matiyasevich in 1970 (Davis 1973).

Proof theory is the study of formal proofs in various logical deduction systems. These proofs are represented as formal mathematical objects, facilitating their analysis by mathematical techniques. Several deduction systems are commonly considered, including Hilbert-style deduction systems, systems of natural deduction, and the sequent calculus developed by Gentzen.

The study of constructive mathematics, in the context of mathematical logic, includes the study of systems in non-classical logic such as intuitionistic logic, as well as the study of predicative systems. An early proponent of predicativism was Hermann Weyl, who showed it is possible to develop a large part of real analysis using only predicative methods (Weyl 1918).

Because proofs are entirely finitary, whereas truth in a structure is not, it is common for work in constructive mathematics to emphasize provability. The relationship between provability in classical (or nonconstructive) systems and provability in intuitionistic (or constructive, respectively) systems is of particular interest. Results such as the Gödel–Gentzen negative translation show that it is possible to embed (or "translate") classical logic into intuitionistic logic, allowing some properties about intuitionistic proofs to be transferred back to classical proofs.

Recent developments in proof theory include the study of proof mining by Ulrich Kohlenbach and the study of proof-theoretic ordinals by Michael Rathjen.

"Mathematical logic has been successfully applied not only to mathematics and its foundations (G. Frege, B. Russell, D. Hilbert, P. Bernays, H. Scholz, R. Carnap, S. Lesniewski, T. Skolem), but also to physics (R. Carnap, A. Dittrich, B. Russell, C. E. Shannon, A. N. Whitehead, H. Reichenbach, P. Fevrier), to biology (J. H. Woodger, A. Tarski), to psychology (F. B. Fitch, C. G. Hempel), to law and morals (K. Menger, U. Klug, P. Oppenheim), to economics (J. Neumann, O. Morgenstern), to practical questions (E. C. Berkeley, E. Stamm), and even to metaphysics (J. [Jan] Salamucha, H. Scholz, J. M. Bochenski). Its applications to the history of logic have proven extremely fruitful (J. Lukasiewicz, H. Scholz, B. Mates, A. Becker, E. Moody, J. Salamucha, K. Duerr, Z. Jordan, P. Boehner, J. M. Bochenski, S. [Stanislaw] T. Schayer, D. Ingalls)." "Applications have also been made to theology (F. Drewnowski, J. Salamucha, I. Thomas)."

The study of computability theory in computer science is closely related to the study of computability in mathematical logic. There is a difference of emphasis, however. Computer scientists often focus on concrete programming languages and feasible computability, while researchers in mathematical logic often focus on computability as a theoretical concept and on noncomputability.

The theory of semantics of programming languages is related to model theory, as is program verification (in particular, model checking). The Curry–Howard isomorphism between proofs and programs relates to proof theory, especially intuitionistic logic. Formal calculi such as the lambda calculus and combinatory logic are now studied as idealized programming languages.

Computer science also contributes to mathematics by developing techniques for the automatic checking or even finding of proofs, such as automated theorem proving and logic programming.

Descriptive complexity theory relates logics to computational complexity. The first significant result in this area, Fagin's theorem (1974) established that NP is precisely the set of languages expressible by sentences of existential second-order logic.

In the 19th century, mathematicians became aware of logical gaps and inconsistencies in their field. It was shown that Euclid's axioms for geometry, which had been taught for centuries as an example of the axiomatic method, were incomplete. The use of infinitesimals, and the very definition of function, came into question in analysis, as pathological examples such as Weierstrass' nowhere-differentiable continuous function were discovered.

Cantor's study of arbitrary infinite sets also drew criticism. Leopold Kronecker famously stated "God made the integers; all else is the work of man," endorsing a return to the study of finite, concrete objects in mathematics. Although Kronecker's argument was carried forward by constructivists in the 20th century, the mathematical community as a whole rejected them. David Hilbert argued in favor of the study of the infinite, saying "No one shall expel us from the Paradise that Cantor has created."

Mathematicians began to search for axiom systems that could be used to formalize large parts of mathematics. In addition to removing ambiguity from previously naive terms such as function, it was hoped that this axiomatization would allow for consistency proofs. In the 19th century, the main method of proving the consistency of a set of axioms was to provide a model for it. Thus, for example, non-Euclidean geometry can be proved consistent by defining "point" to mean a point on a fixed sphere and "line" to mean a great circle on the sphere. The resulting structure, a model of elliptic geometry, satisfies the axioms of plane geometry except the parallel postulate.

With the development of formal logic, Hilbert asked whether it would be possible to prove that an axiom system is consistent by analyzing the structure of possible proofs in the system, and showing through this analysis that it is impossible to prove a contradiction. This idea led to the study of proof theory. Moreover, Hilbert proposed that the analysis should be entirely concrete, using the term "finitary" to refer to the methods he would allow but not precisely defining them. This project, known as Hilbert's program, was seriously affected by Gödel's incompleteness theorems, which show that the consistency of formal theories of arithmetic cannot be established using methods formalizable in those theories. Gentzen showed that it is possible to produce a proof of the consistency of arithmetic in a finitary system augmented with axioms of transfinite induction, and the techniques he developed to do so were seminal in proof theory.

A second thread in the history of foundations of mathematics involves nonclassical logics and constructive mathematics. The study of constructive mathematics includes many different programs with various definitions of "constructive". At the most accommodating end, proofs in ZF set theory that do not use the axiom of choice are called constructive by many mathematicians. More limited versions of constructivism limit themselves to natural numbers, number-theoretic functions, and sets of natural numbers (which can be used to represent real numbers, facilitating the study of mathematical analysis). A common idea is that a concrete means of computing the values of the function must be known before the function itself can be said to exist. 

In the early 20th century, Luitzen Egbertus Jan Brouwer founded intuitionism as a part of philosophy of mathematics . This philosophy, poorly understood at first, stated that in order for a mathematical statement to be true to a mathematician, that person must be able to "intuit" the statement, to not only believe its truth but understand the reason for its truth. A consequence of this definition of truth was the rejection of the law of the excluded middle, for there are statements that, according to Brouwer, could not be claimed to be true while their negations also could not be claimed true. Brouwer's philosophy was influential, and the cause of bitter disputes among prominent mathematicians. Later, Kleene and Kreisel would study formalized versions of intuitionistic logic (Brouwer rejected formalization, and presented his work in unformalized natural language). With the advent of the BHK interpretation and Kripke models, intuitionism became easier to reconcile with classical mathematics.









</doc>
<doc id="19637" url="https://en.wikipedia.org/wiki?curid=19637" title="Molecular nanotechnology">
Molecular nanotechnology

Molecular nanotechnology (MNT) is a technology based on the ability to build structures to complex, atomic specifications by means of mechanosynthesis. This is distinct from nanoscale materials. Based on Richard Feynman's vision of miniature factories using nanomachines to build complex products (including additional nanomachines), this advanced form of nanotechnology (or "molecular manufacturing") would make use of positionally-controlled mechanosynthesis guided by molecular machine systems. MNT would involve combining physical principles demonstrated by biophysics, chemistry, other nanotechnologies, and the molecular machinery of life with the systems engineering principles found in modern macroscale factories.
While conventional chemistry uses inexact processes obtaining inexact results, and biology exploits inexact processes to obtain definitive results, molecular nanotechnology would employ original definitive processes to obtain definitive results. The desire in molecular nanotechnology would be to balance molecular reactions in positionally-controlled locations and orientations to obtain desired chemical reactions, and then to build systems by further assembling the products of these reactions.

A roadmap for the development of MNT is an objective of a broadly based technology project led by Battelle (the manager of several U.S. National Laboratories) and the Foresight Institute. The roadmap was originally scheduled for completion by late 2006, but was released in January 2008. The Nanofactory Collaboration is a more focused ongoing effort involving 23 researchers from 10 organizations and 4 countries that is developing a practical research agenda specifically aimed at positionally-controlled diamond mechanosynthesis and diamondoid nanofactory development. In August 2005, a task force consisting of 50+ international experts from various fields was organized by the Center for Responsible Nanotechnology to study the societal implications of molecular nanotechnology.

One proposed application of MNT is so-called smart materials. This term refers to any sort of material designed and engineered at the nanometer scale for a specific task. It encompasses a wide variety of possible commercial applications. One example would be materials designed to respond differently to various molecules; such a capability could lead, for example, to artificial drugs which would recognize and render inert specific viruses. Another is the idea of self-healing structures, which would repair small tears in a surface naturally in the same way as self-sealing tires or human skin.

A MNT nanosensor would resemble a smart material, involving a small component within a larger machine that would react to its environment and change in some fundamental, intentional way. A very simple example: a photosensor might passively measure the incident light and discharge its absorbed energy as electricity when the light passes above or below a specified threshold, sending a signal to a larger machine. Such a sensor would supposedly cost less and use less power than a conventional sensor, and yet function usefully in all the same applications — for example, turning on parking lot lights when it gets dark.

While smart materials and nanosensors both exemplify useful applications of MNT, they pale in comparison with the complexity of the technology most popularly associated with the term: the replicating nanorobot.

MNT nanofacturing is popularly linked with the idea of swarms of coordinated nanoscale robots working together, a popularization of an early proposal by K. Eric Drexler in his 1986 discussions of MNT, but superseded in 1992. In this early proposal, sufficiently capable nanorobots would construct more nanorobots in an artificial environment containing special molecular building blocks.

Critics have doubted both the feasibility of self-replicating nanorobots and the feasibility of control if self-replicating nanorobots could be achieved: they cite the possibility of mutations removing any control and favoring reproduction of mutant pathogenic variations. Advocates address the first doubt by pointing out that the first macroscale autonomous machine replicator, made of Lego blocks, was built and operated experimentally in 2002. While there are sensory advantages present at the macroscale compared to the limited sensorium available at the nanoscale, proposals for positionally controlled nanoscale mechanosynthetic fabrication systems employ dead reckoning of tooltips combined with reliable reaction sequence design to ensure reliable results, hence a limited sensorium is no handicap; similar considerations apply to the positional assembly of small nanoparts. Advocates address the second doubt by arguing that bacteria are (of necessity) evolved to evolve, while nanorobot mutation could be actively prevented by common error-correcting techniques. Similar ideas are advocated in the Foresight Guidelines on Molecular Nanotechnology, and a map of the 137-dimensional replicator design space recently published by Freitas and Merkle provides numerous proposed methods by which replicators could, in principle, be safely controlled by good design.

However, the concept of suppressing mutation raises the question: How can design evolution occur at the nanoscale without a process of random mutation and deterministic selection? Critics argue that MNT advocates have not provided a substitute for such a process of evolution in this nanoscale arena where conventional sensory-based selection processes are lacking. The limits of the sensorium available at the nanoscale could make it difficult or impossible to winnow successes from failures. Advocates argue that design evolution should occur deterministically and strictly under human control, using the conventional engineering paradigm of modeling, design, prototyping, testing, analysis, and redesign.

In any event, since 1992 technical proposals for MNT do not include self-replicating nanorobots, and recent ethical guidelines put forth by MNT advocates prohibit unconstrained self-replication.

One of the most important applications of MNT would be medical nanorobotics or nanomedicine, an area pioneered by Robert Freitas in numerous books and papers. The ability to design, build, and deploy large numbers of medical nanorobots would, at a minimum, make possible the rapid elimination of disease and the reliable and relatively painless recovery from physical trauma. Medical nanorobots might also make possible the convenient correction of genetic defects, and help to ensure a greatly expanded lifespan. More controversially, medical nanorobots might be used to augment natural human capabilities. One study has reported on how conditions like tumors, arteriosclerosis, blood clots leading to stroke, accumulation of scar tissue and localized pockets of infection can possibly be addressed by employing medical nanorobots.

Another proposed application of molecular nanotechnology is "utility fog" — in which a cloud of networked microscopic robots (simpler than assemblers) would change its shape and properties to form macroscopic objects and tools in accordance with software commands. Rather than modify the current practices of consuming material goods in different forms, utility fog would simply replace many physical objects.

Yet another proposed application of MNT would be phased-array optics (PAO). However, this appears to be a problem addressable by ordinary nanoscale technology. PAO would use the principle of phased-array millimeter technology but at optical wavelengths. This would permit the duplication of any sort of optical effect but virtually. Users could request holograms, sunrises and sunsets, or floating lasers as the mood strikes. PAO systems were described in BC Crandall's "Nanotechnology: Molecular Speculations on Global Abundance" in the Brian Wowk article "Phased-Array Optics."

Molecular manufacturing is a potential future subfield of nanotechnology that would make it possible to build complex structures at atomic precision. Molecular manufacturing requires significant advances in nanotechnology, but once achieved could produce highly advanced products at low costs and in large quantities in nanofactories weighing a kilogram or more. When nanofactories gain the ability to produce other nanofactories production may only be limited by relatively abundant factors such as input materials, energy and software.

The products of molecular manufacturing could range from cheaper, mass-produced versions of known high-tech products to novel products with added capabilities in many areas of application. Some applications that have been suggested are advanced smart materials, nanosensors, medical nanorobots and space travel. Additionally, molecular manufacturing could be used to cheaply produce highly advanced, durable weapons, which is an area of special concern regarding the impact of nanotechnology. Being equipped with compact computers and motors these could be increasingly autonomous and have a large range of capabilities.

According to Chris Phoenix and Mike Treder from the Center for Responsible Nanotechnology as well as Anders Sandberg from the Future of Humanity Institute molecular manufacturing is the application of nanotechnology that poses the most significant global catastrophic risk. Several nanotechnology researchers state that the bulk of risk from nanotechnology comes from the potential to lead to war, arms races and destructive global government. Several reasons have been suggested why the availability of nanotech weaponry may with significant likelihood lead to unstable arms races (compared to e.g. nuclear arms races): (1) A large number of players may be tempted to enter the race since the threshold for doing so is low; (2) the ability to make weapons with molecular manufacturing will be cheap and easy to hide; (3) therefore lack of insight into the other parties' capabilities can tempt players to arm out of caution or to launch preemptive strikes; (4) molecular manufacturing may reduce dependency on international trade, a potential peace-promoting factor; (5) wars of aggression may pose a smaller economic threat to the aggressor since manufacturing is cheap and humans may not be needed on the battlefield.

Since self-regulation by all state and non-state actors seems hard to achieve, measures to mitigate war-related risks have mainly been proposed in the area of international cooperation. International infrastructure may be expanded giving more sovereignty to the international level. This could help coordinate efforts for arms control. International institutions dedicated specifically to nanotechnology (perhaps analogously to the International Atomic Energy Agency IAEA) or general arms control may also be designed. One may also jointly make differential technological progress on defensive technologies, a policy that players should usually favour. The Center for Responsible Nanotechnology also suggest some technical restrictions. Improved transparency regarding technological capabilities may be another important facilitator for arms-control.

A grey goo is another catastrophic scenario, which was proposed by Eric Drexler in his 1986 book "Engines of Creation", has been analyzed by Freitas in "Some Limits to Global Ecophagy by Biovorous Nanoreplicators, with Public Policy Recommendations" and has been a theme in mainstream media and fiction. This scenario involves tiny self-replicating robots that consume the entire biosphere using it as a source of energy and building blocks. Nanotech experts including Drexler now discredit the scenario. According to Chris Phoenix a "So-called grey goo could only be the product of a deliberate and difficult engineering process, not an accident". With the advent of nano-biotech, a different scenario called green goo has been forwarded. Here, the malignant substance is not nanobots but rather self-replicating biological organisms engineered through nanotechnology.

Nanotechnology (or molecular nanotechnology to refer more specifically to the goals discussed here) will let us continue the historical trends in manufacturing right up to the fundamental limits imposed by physical law. It will let us make remarkably powerful molecular computers. It will let us make materials over fifty times lighter than steel or aluminium alloy but with the same strength. We'll be able to make jets, rockets, cars or even chairs that, by today's standards, would be remarkably light, strong, and inexpensive. Molecular surgical tools, guided by molecular computers and injected into the blood stream could find and destroy cancer cells or invading bacteria, unclog arteries, or provide oxygen when the circulation is impaired.

Nanotechnology will replace our entire manufacturing base with a new, radically more precise, radically less expensive, and radically more flexible way of making products. The aim is not simply to replace today's computer chip making plants, but also to replace the assembly lines for cars, televisions, telephones, books, surgical tools, missiles, bookcases, airplanes, tractors, and all the rest. The objective is a pervasive change in manufacturing, a change that will leave virtually no product untouched. Economic progress and military readiness in the 21st Century will depend fundamentally on maintaining a competitive position in nanotechnology.

Despite the current early developmental status of nanotechnology and molecular nanotechnology, much concern surrounds MNT's anticipated impact on economics and on law. Whatever the exact effects, MNT, if achieved, would tend to reduce the scarcity of manufactured goods and make many more goods (such as food and health aids) manufacturable.

MNT should make possible nanomedical capabilities able to cure any medical condition not already cured by advances in other areas. Good health would be common, and poor health of any form would be as rare as smallpox and scurvy are today. Even cryonics would be feasible, as cryopreserved tissue could be fully repaired.

Molecular nanotechnology is one of the technologies that some analysts believe could lead to a technological singularity, in which technological growth has accelerated to the point of having unpredictable effects. Some effects could be beneficial, while others could be detrimental, such as the utilization of molecular nanotechnology by an unfriendly artificial general intelligence. 
Some feel that molecular nanotechnology would have daunting risks. It conceivably could enable cheaper and more destructive conventional weapons. Also, molecular nanotechnology might permit weapons of mass destruction that could self-replicate, as viruses and cancer cells do when attacking the human body. Commentators generally agree that, in the event molecular nanotechnology were developed, its self-replication should be permitted only under very controlled or "inherently safe" conditions.

A fear exists that nanomechanical robots, if achieved, and if designed to self-replicate using naturally occurring materials (a difficult task), could consume the entire planet in their hunger for raw materials, or simply crowd out natural life, out-competing it for energy (as happened historically when blue-green algae appeared and outcompeted earlier life forms). Some commentators have referred to this situation as the "grey goo" or "ecophagy" scenario. K. Eric Drexler considers an accidental "grey goo" scenario extremely unlikely and says so in later editions of "Engines of Creation".

In light of this perception of potential danger, the Foresight Institute, founded by Drexler, has prepared a set of guidelines for the ethical development of nanotechnology. These include the banning of free-foraging self-replicating pseudo-organisms on the Earth's surface, at least, and possibly in other places.

The feasibility of the basic technologies analyzed in "Nanosystems" has been the subject of a formal scientific review by U.S. National Academy of Sciences, and has also been the focus of extensive debate on the internet and in the popular press.

In 2006, U.S. National Academy of Sciences released the report of a study of molecular manufacturing as part of a longer report, "A Matter of Size: Triennial Review of the National Nanotechnology Initiative" The study committee reviewed the technical content of "Nanosystems", and in its conclusion states that no current theoretical analysis can be considered definitive regarding several questions of potential system performance, and that optimal paths for implementing high-performance systems cannot be predicted with confidence. It recommends experimental research to advance knowledge in this area:

A section heading in Drexler's "Engines of Creation" reads "Universal Assemblers", and the following text speaks of multiple types of assemblers which, collectively, could hypothetically "build almost anything that the laws of nature allow to exist." Drexler's colleague Ralph Merkle has noted that, contrary to widespread legend, Drexler never claimed that assembler systems could build absolutely any molecular structure. The endnotes in Drexler's book explain the qualification "almost": "For example, a delicate structure might be designed that, like a stone arch, would self-destruct unless all its pieces were already in place. If there were no room in the design for the placement and removal of a scaffolding, then the structure might be impossible to build. Few structures of practical interest seem likely to exhibit such a problem, however."

In 1992, Drexler published "Nanosystems: Molecular Machinery, Manufacturing, and Computation", a detailed proposal for synthesizing stiff covalent structures using a table-top factory. Diamondoid structures and other stiff covalent structures, if achieved, would have a wide range of possible applications, going far beyond current MEMS technology. An outline of a path was put forward in 1992 for building a table-top factory in the absence of an assembler. Other researchers have begun advancing tentative, alternative proposed paths for this in the years since Nanosystems was published.

In 2004 Richard Jones wrote Soft Machines (nanotechnology and life), a book for lay audiences published by Oxford University. In this book he describes radical nanotechnology (as advocated by Drexler) as a deterministic/mechanistic idea of nano engineered machines that does not take into account the nanoscale challenges such as wetness, stickiness, Brownian motion, and high viscosity. He also explains what is soft nanotechnology or more appropriately biomimetic nanotechnology which is the way forward, if not the best way, to design functional nanodevices that can cope with all the problems at a nanoscale. One can think of soft nanotechnology as the development of nanomachines that uses the lessons learned from biology on how things work, chemistry to precisely engineer such devices and stochastic physics to model the system and its natural processes in detail.

Several researchers, including Nobel Prize winner Dr. Richard Smalley (1943–2005), attacked the notion of universal assemblers, leading to a rebuttal from Drexler and colleagues, and eventually to an exchange of letters. Smalley argued that chemistry is extremely complicated, reactions are hard to control, and that a universal assembler is science fiction. Drexler and colleagues, however, noted that Drexler never proposed universal assemblers able to make absolutely anything, but instead proposed more limited assemblers able to make a very wide variety of things. They challenged the relevance of Smalley's arguments to the more specific proposals advanced in "Nanosystems". Also, Smalley argued that nearly all of modern chemistry involves reactions that take place in a solvent (usually water), because the small molecules of a solvent contribute many things, such as lowering binding energies for transition states. Since nearly all known chemistry requires a solvent, Smalley felt that Drexler's proposal to use a high vacuum environment was not feasible. However, Drexler addresses this in Nanosystems by showing mathematically that well designed catalysts can provide the effects of a solvent and can fundamentally be made even more efficient than a solvent/enzyme reaction could ever be. It is noteworthy that, contrary to Smalley's opinion that enzymes require water, "Not only do enzymes work vigorously in anhydrous organic media, but in this unnatural milieu they acquire remarkable properties such as greatly enhanced stability, radically altered substrate and enantiomeric specificities, molecular memory, and the ability to catalyse unusual reactions."

For the future, some means have to be found for MNT design evolution at the nanoscale which mimics the process of biological evolution at the molecular scale. Biological evolution proceeds by random variation in ensemble averages of organisms combined with culling of the less-successful variants and reproduction of the more-successful variants, and macroscale engineering design also proceeds by a process of design evolution from simplicity to complexity as set forth somewhat satirically by John Gall: "A complex system that works is invariably found to have evolved from a simple system that worked. . . . A complex system designed from scratch never works and can not be patched up to make it work. You have to start over, beginning with a system that works." A breakthrough in MNT is needed which proceeds from the simple atomic ensembles which can be built with, e.g., an STM to complex MNT systems via a process of design evolution. A handicap in this process is the difficulty of seeing and manipulation at the nanoscale compared to the macroscale which makes deterministic selection of successful trials difficult; in contrast biological evolution proceeds via action of what Richard Dawkins has called the "blind watchmaker"
comprising random molecular variation and deterministic reproduction/extinction.

At present in 2007 the practice of nanotechnology embraces both stochastic approaches (in which, for example, supramolecular chemistry creates waterproof pants) and deterministic approaches wherein single molecules (created by stochastic chemistry) are manipulated on substrate surfaces (created by stochastic deposition methods) by deterministic methods comprising nudging them with STM or AFM probes and causing simple binding or cleavage reactions to occur. The dream of a complex, deterministic molecular nanotechnology remains elusive. Since the mid-1990s, thousands of surface scientists and thin film technocrats have latched on to the nanotechnology bandwagon and redefined their disciplines as nanotechnology. This has caused much confusion in the field and has spawned thousands of "nano"-papers on the peer reviewed literature. Most of these reports are extensions of the more ordinary research done in the parent fields.

The feasibility of Drexler's proposals largely depends, therefore, on whether designs like those in "Nanosystems" could be built in the absence of a universal assembler to build them and would work as described. Supporters of molecular nanotechnology frequently claim that no significant errors have been discovered in "Nanosystems" since 1992. Even some critics concede that "Drexler has carefully considered a number of physical principles underlying the 'high level' aspects of the nanosystems he proposes and, indeed, has thought in some detail" about some issues.

Other critics claim, however, that "Nanosystems" omits important chemical details about the low-level 'machine language' of molecular nanotechnology. They also claim that much of the other low-level chemistry in "Nanosystems" requires extensive further work, and that Drexler's higher-level designs therefore rest on speculative foundations. Recent such further work by Freitas and Merkle is aimed at strengthening these foundations by filling the existing gaps in the low-level chemistry.

Drexler argues that we may need to wait until our conventional nanotechnology improves before solving these issues: "Molecular manufacturing will result from a series of advances in molecular machine systems, much as the first Moon landing resulted from a series of advances in liquid-fuel rocket systems. We are now in a position like that of the British Interplanetary Society of the 1930s which described how multistage liquid-fueled rockets could reach the Moon and pointed to early rockets as illustrations of the basic principle." However, Freitas and Merkle argue that a focused effort to achieve diamond mechanosynthesis (DMS) can begin now, using existing technology, and might achieve success in less than a decade if their "direct-to-DMS approach is pursued rather than a more circuitous development approach that seeks to implement less efficacious nondiamondoid molecular manufacturing technologies before progressing to diamondoid".

To summarize the arguments against feasibility: First, critics argue that a primary barrier to achieving molecular nanotechnology is the lack of an efficient way to create machines on a molecular/atomic scale, especially in the absence of a well-defined path toward a self-replicating assembler or diamondoid nanofactory. Advocates respond that a preliminary research path leading to a diamondoid nanofactory is being developed.

A second difficulty in reaching molecular nanotechnology is design. Hand design of a gear or bearing at the level of atoms might take a few to several weeks. While Drexler, Merkle and others have created designs of simple parts, no comprehensive design effort for anything approaching the complexity of a Model T Ford has been attempted. Advocates respond that it is difficult to undertake a comprehensive design effort in the absence of significant funding for such efforts, and that despite this handicap much useful design-ahead has nevertheless been accomplished with new software tools that have been developed, e.g., at Nanorex.

In the latest report "A Matter of Size: Triennial Review of the National Nanotechnology Initiative" put out by the National Academies Press in December 2006 (roughly twenty years after Engines of Creation was published), no clear way forward toward molecular nanotechnology could yet be seen, as per the conclusion on page 108 of that report: "Although theoretical calculations can be made today, the eventually attainable
range of chemical reaction cycles, error rates, speed of operation, and thermodynamic
efficiencies of such bottom-up manufacturing systems cannot be reliably
predicted at this time. Thus, the eventually attainable perfection and complexity of
manufactured products, while they can be calculated in theory, cannot be predicted
with confidence. Finally, the optimum research paths that might lead to systems
which greatly exceed the thermodynamic efficiencies and other capabilities of
biological systems cannot be reliably predicted at this time. Research funding that
is based on the ability of investigators to produce experimental demonstrations
that link to abstract models and guide long-term vision is most appropriate to
achieve this goal." This call for research leading to demonstrations is welcomed by groups such as the Nanofactory Collaboration who are specifically seeking experimental successes in diamond mechanosynthesis. The "Technology Roadmap for Productive Nanosystems" aims to offer additional constructive insights.

It is perhaps interesting to ask whether or not most structures consistent with physical law can in fact be manufactured. Advocates assert that to achieve most of the vision of molecular manufacturing it is not necessary to be able to build "any structure that is compatible with natural law." Rather, it is necessary to be able to build only a sufficient (possibly modest) subset of such structures—as is true, in fact, of any practical manufacturing process used in the world today, and is true even in biology. In any event, as Richard Feynman once said, "It is scientific only to say what's more likely or less likely, and not to be proving all the time what's possible or impossible."

There is a growing body of peer-reviewed theoretical work on synthesizing diamond by mechanically removing/adding hydrogen atoms and depositing carbon atoms (a process known as mechanosynthesis). This work is slowly permeating the broader nanoscience community and is being critiqued. For instance, Peng et al. (2006) (in the continuing research effort by Freitas, Merkle and their collaborators) reports that the most-studied mechanosynthesis tooltip motif (DCB6Ge) successfully places a C carbon dimer on a C(110) diamond surface at both 300 K (room temperature) and 80 K (liquid nitrogen temperature), and that the silicon variant (DCB6Si) also works at 80 K but not at 300 K. Over 100,000 CPU hours were invested in this latest study. The DCB6 tooltip motif, initially described by Merkle and Freitas at a Foresight Conference in 2002, was the first complete tooltip ever proposed for diamond mechanosynthesis and remains the only tooltip motif that has been successfully simulated for its intended function on a full 200-atom diamond surface.

The tooltips modeled in this work are intended to be used only in carefully controlled environments (e. g., vacuum). Maximum acceptable limits for tooltip translational and rotational misplacement errors are reported in Peng et al. (2006) -- tooltips must be positioned with great accuracy to avoid bonding the dimer incorrectly. Peng et al. (2006) reports that increasing the handle thickness from 4 support planes of C atoms above the tooltip to 5 planes decreases the resonance frequency of the entire structure from 2.0 THz to 1.8 THz. More importantly, the vibrational footprints of a DCB6Ge tooltip mounted on a 384-atom handle and of the same tooltip mounted on a similarly constrained but much larger 636-atom "crossbar" handle are virtually identical in the non-crossbar directions. Additional computational studies modeling still bigger handle structures are welcome, but the ability to precisely position SPM tips to the requisite atomic accuracy has been repeatedly demonstrated experimentally at low temperature, or even at room temperature constituting a basic existence proof for this capability.

Further research to consider additional tooltips will require time-consuming computational chemistry and difficult laboratory work.

A working nanofactory would require a variety of well-designed tips for different reactions, and detailed analyses of placing atoms on more complicated surfaces. Although this appears a challenging problem given current resources, many tools will be available to help future researchers: Moore's law predicts further increases in computer power, semiconductor fabrication techniques continue to approach the nanoscale, and researchers grow ever more skilled at using proteins, ribosomes and DNA to perform novel chemistry.






</doc>
<doc id="19638" url="https://en.wikipedia.org/wiki?curid=19638" title="Microelectromechanical systems">
Microelectromechanical systems

Microelectromechanical systems (MEMS), also written as micro-electro-mechanical systems (or microelectronic and microelectromechanical systems) and the related micromechatronics and microsystems constitute the technology of microscopic devices, particularly those with moving parts. They merge at the nanoscale into nanoelectromechanical systems (NEMS) and nanotechnology. MEMS are also referred to as micromachines in Japan and microsystem technology (MST) in Europe.

MEMS are made up of components between 1 and 100 micrometers in size (i.e., 0.001 to 0.1 mm), and MEMS devices generally range in size from 20 micrometres to a millimetre (i.e., 0.02 to 1.0 mm), although components arranged in arrays (e.g., digital micromirror devices) can be more than 1000 mm. 
They usually consist of a central unit that processes data (an integrated circuit chip such as microprocessor) and several components that interact with the surroundings (such as microsensors). Because of the large surface area to volume ratio of MEMS, forces produced by ambient electromagnetism (e.g., electrostatic charges and magnetic moments), and fluid dynamics (e.g., surface tension and viscosity) are more important design considerations than with larger scale mechanical devices. MEMS technology is distinguished from molecular nanotechnology or molecular electronics in that the latter must also consider surface chemistry.

The potential of very small machines was appreciated before the technology existed that could make them (see, for example, Richard Feynman's famous 1959 lecture There's Plenty of Room at the Bottom). MEMS became practical once they could be fabricated using modified semiconductor device fabrication technologies, normally used to make electronics. These include molding and plating, wet etching (KOH, TMAH) and dry etching (RIE and DRIE), electrical discharge machining (EDM), and other technologies capable of manufacturing small devices.

MEMS technology has roots in the silicon revolution, which can be traced back to two important silicon semiconductor inventions from 1959: the monolithic integrated circuit (IC) chip by Robert Noyce at Fairchild Semiconductor, and the MOSFET (metal-oxide-semiconductor field-effect transistor, or MOS transistor) by Mohamed M. Atalla and Dawon Kahng at Bell Labs. MOSFET scaling, the miniaturisation of MOSFETs on IC chips, led to the miniaturisation of electronics (as predicted by Moore's law and Dennard scaling). This laid the foundations for the miniaturisation of mechanical systems, with the development of micromachining technology based on silicon semiconductor technology, as engineers began realizing that silicon chips and MOSFETs could interact and communicate with the surroundings and process things such as chemicals, motions and light. One of the first silicon pressure sensors was isotropically micromachined by Honeywell in 1962.

An early example of a MEMS device is the resonant-gate transistor, an adaptation of the MOSFET, developed by Harvey C. Nathanson in 1965. Another early example is the resonistor, an electromechanical monolithic resonator patented by Raymond J. Wilfinger between 1966 and 1971. During the 1970s to early 1980s, a number of MOSFET microsensors were developed for measuring physical, chemical, biological and environmental parameters.

There are two basic types of MEMS switch technology: capacitive and ohmic. A capacitive MEMS switch is developed using a moving plate or sensing element, which changes the capacitance. Ohmic switches are controlled by electrostatically controlled cantilevers. Ohmic MEMS switches can fail from metal fatigue of the MEMS actuator (cantilever) and contact wear, since cantilevers can deform over time.

The fabrication of MEMS evolved from the process technology in semiconductor device fabrication, i.e. the basic techniques are deposition of material layers, patterning by photolithography and etching to produce the required shapes.

Silicon is the material used to create most integrated circuits used in consumer electronics in the modern industry. The economies of scale, ready availability of inexpensive high-quality materials, and ability to incorporate electronic functionality make silicon attractive for a wide variety of MEMS applications. Silicon also has significant advantages engendered through its material properties. In single crystal form, silicon is an almost perfect Hookean material, meaning that when it is flexed there is virtually no hysteresis and hence almost no energy dissipation. As well as making for highly repeatable motion, this also makes silicon very reliable as it suffers very little fatigue and can have service lifetimes in the range of billions to trillions of cycles without breaking. Semiconductor nanostructures based on silicon are gaining increasing importance in the field of microelectronics and MEMS in particular. Silicon nanowires, fabricated through the thermal oxidation of silicon, are of further interest in electrochemical conversion and storage, including nanowire batteries and photovoltaic systems.

Even though the electronics industry provides an economy of scale for the silicon industry, crystalline silicon is still a complex and relatively expensive material to produce. Polymers on the other hand can be produced in huge volumes, with a great variety of material characteristics. MEMS devices can be made from polymers by processes such as injection molding, embossing or stereolithography and are especially well suited to microfluidic applications such as disposable blood testing cartridges.

Metals can also be used to create MEMS elements. While metals do not have some of the advantages displayed by silicon in terms of mechanical properties, when used within their limitations, metals can exhibit very high degrees of reliability. Metals can be deposited by electroplating, evaporation, and sputtering processes. Commonly used metals include gold, nickel, aluminium, copper, chromium, titanium, tungsten, platinum, and silver.

The nitrides of silicon, aluminium and titanium as well as silicon carbide and other ceramics are increasingly applied in MEMS fabrication due to advantageous combinations of material properties. AlN crystallizes in the wurtzite structure and thus shows pyroelectric and piezoelectric properties enabling sensors, for instance, with sensitivity to normal and shear forces. TiN, on the other hand, exhibits a high electrical conductivity and large elastic modulus, making it possible to implement electrostatic MEMS actuation schemes with ultrathin beams. Moreover, the high resistance of TiN against biocorrosion qualifies the material for applications in biogenic environments. The figure shows an electron-microscopic picture of a MEMS biosensor with a 50 nm thin bendable TiN beam above a TiN ground plate. Both can be driven as opposite electrodes of a capacitor, since the beam is fixed in electrically isolating side walls. When a fluid is suspended in the cavity its viscosity may be derived from bending the beam by electrical attraction to the ground plate and measuring the bending velocity. 

One of the basic building blocks in MEMS processing is the ability to deposit thin films of material with a thickness anywhere between one micrometre, to about 100 micrometres. The NEMS process is the same, although the measurement of film deposition ranges from a few nanometres to one micrometre. There are two types of deposition processes, as follows.

Physical vapor deposition ("PVD") consists of a process in which a material is removed from a target, and deposited on a surface. Techniques to do this include the process of sputtering, in which an ion beam liberates atoms from a target, allowing them to move through the intervening space and deposit on the desired substrate, and evaporation, in which a material is evaporated from a target using either heat (thermal evaporation) or an electron beam (e-beam evaporation) in a vacuum system.

Chemical deposition techniques include chemical vapor deposition (CVD), in which a stream of source gas reacts on the substrate to grow the material desired. This can be further divided into categories depending on the details of the technique, for example LPCVD (low-pressure chemical vapor deposition) and PECVD (plasma-enhanced chemical vapor deposition).

Oxide films can also be grown by the technique of thermal oxidation, in which the (typically silicon) wafer is exposed to oxygen and/or steam, to grow a thin surface layer of silicon dioxide.

Patterning in MEMS is the transfer of a pattern into a material.

Lithography in MEMS context is typically the transfer of a pattern into a photosensitive material by selective exposure to a radiation source such as light. A photosensitive material is a material that experiences a change in its physical properties when exposed to a radiation source. If a photosensitive material is selectively exposed to radiation (e.g. by masking some of the radiation) the pattern of the radiation on the material is transferred to the material exposed, as the properties of the exposed and unexposed regions differs.

This exposed region can then be removed or treated providing a mask for the underlying substrate. Photolithography is typically used with metal or other thin film deposition, wet and dry etching. Sometimes, photolithography is used to create structure without any kind of post etching. One example is SU8 based lens where SU8 based square blocks are generated. Then the photoresist is melted to form a semi-sphere which acts as a lens.

Electron beam lithography (often abbreviated as e-beam lithography) is the practice of scanning a beam of electrons in a patterned fashion across a surface covered with a film (called the resist), ("exposing" the resist) and of selectively removing either exposed or non-exposed regions of the resist ("developing"). The purpose, as with photolithography, is to create very small structures in the resist that can subsequently be transferred to the substrate material, often by etching. It was developed for manufacturing integrated circuits, and is also used for creating nanotechnology architectures.

The primary advantage of electron beam lithography is that it is one of the ways to beat the diffraction limit of light and make features in the nanometer range. This form of maskless lithography has found wide usage in photomask-making used in photolithography, low-volume production of semiconductor components, and research & development.

The key limitation of electron beam lithography is throughput, i.e., the very long time it takes to expose an entire silicon wafer or glass substrate. A long exposure time leaves the user vulnerable to beam drift or instability which may occur during the exposure. Also, the turn-around time for reworking or re-design is lengthened unnecessarily if the pattern is not being changed the second time.

It is known that focused-ion beam lithography has the capability of writing extremely fine lines (less than 50 nm line and space has been achieved) without proximity effect. However, because the writing field in ion-beam lithography is quite small, large area patterns must be created by stitching together the small fields.

Ion track technology is a deep cutting tool with a resolution limit around 8 nm applicable to radiation resistant minerals, glasses and polymers. It is capable of generating holes in thin films without any development process. Structural depth can be defined either by ion range or by material thickness. Aspect ratios up to several 10 can be reached. The technique can shape and texture materials at a defined inclination angle. Random pattern, single-ion track structures and aimed pattern consisting of individual single tracks can be generated.

X-ray lithography is a process used in electronic industry to selectively remove parts of a thin film. It uses X-rays to transfer a geometric pattern from a mask to a light-sensitive chemical photoresist, or simply "resist", on the substrate. A series of chemical treatments then engraves the produced pattern into the material underneath the photoresist.

A simple way to carve or create patterns on the surface of nanodiamonds without damaging them could lead to a new photonic devices.

Diamond patterning is a method of forming diamond MEMS. It is achieved by the lithographic application of diamond films to a substrate such as silicon. The patterns can be formed by selective deposition through a silicon dioxide mask, or by deposition followed by micromachining or focused ion beam milling.

There are two basic categories of etching processes: wet etching and dry etching. In the former, the material is dissolved when immersed in a chemical solution. In the latter, the material is sputtered or dissolved using reactive ions or a vapor phase etchant.

Wet chemical etching consists in selective removal of material by dipping a substrate into a solution that dissolves it. The chemical nature of this etching process provides a good selectivity, which means the etching rate of the target material is considerably higher than the mask material if selected carefully.

Etching progresses at the same speed in all directions. Long and narrow holes in a mask will produce v-shaped grooves in the silicon. The surface of these grooves can be atomically smooth if the etch is carried out correctly, with dimensions and angles being extremely accurate.

Some single crystal materials, such as silicon, will have different etching rates depending on the crystallographic orientation of the substrate. This is known as anisotropic etching and one of the most common examples is the etching of silicon in KOH (potassium hydroxide), where Si <111> planes etch approximately 100 times slower than other planes (crystallographic orientations). Therefore, etching a rectangular hole in a (100)-Si wafer results in a pyramid shaped etch pit with 54.7° walls, instead of a hole with curved sidewalls as with isotropic etching.

Hydrofluoric acid is commonly used as an aqueous etchant for silicon dioxide (, also known as BOX for SOI), usually in 49% concentrated form, 5:1, 10:1 or 20:1 BOE (buffered oxide etchant) or BHF (Buffered HF). They were first used in medieval times for glass etching. It was used in IC fabrication for patterning the gate oxide until the process step was replaced by RIE.

Hydrofluoric acid is considered one of the more dangerous acids in the cleanroom. It penetrates the skin upon contact and it diffuses straight to the bone. Therefore, the damage is not felt until it is too late.

Electrochemical etching (ECE) for dopant-selective removal of silicon is a common method to automate and to selectively control etching. An active p-n diode junction is required, and either type of dopant can be the etch-resistant ("etch-stop") material. Boron is the most common etch-stop dopant. In combination with wet anisotropic etching as described above, ECE has been used successfully for controlling silicon diaphragm thickness in commercial piezoresistive silicon pressure sensors. Selectively doped regions can be created either by implantation, diffusion, or epitaxial deposition of silicon.

Xenon difluoride () is a dry vapor phase isotropic etch for silicon originally applied for MEMS in 1995 at University of California, Los Angeles. Primarily used for releasing metal and dielectric structures by undercutting silicon, has the advantage of a stiction-free release unlike wet etchants. Its etch selectivity to silicon is very high, allowing it to work with photoresist, , silicon nitride, and various metals for masking. Its reaction to silicon is "plasmaless", is purely chemical and spontaneous and is often operated in pulsed mode. Models of the etching action are available, and university laboratories and various commercial tools offer solutions using this approach.

Modern VLSI processes avoid wet etching, and use plasma etching instead. Plasma etchers can operate in several modes by adjusting the parameters of the plasma. Ordinary plasma etching operates between 0.1 and 5 Torr. (This unit of pressure, commonly used in vacuum engineering, equals approximately 133.3 pascals.) The plasma produces energetic free radicals, neutrally charged, that react at the surface of the wafer. Since neutral particles attack the wafer from all angles, this process is isotropic.

Plasma etching can be isotropic, i.e., exhibiting a lateral undercut rate on a patterned surface approximately the same as its downward etch rate, or can be anisotropic, i.e., exhibiting a smaller lateral undercut rate than its downward etch rate. Such anisotropy is maximized in deep reactive ion etching. The use of the term anisotropy for plasma etching should not be conflated with the use of the same term when referring to orientation-dependent etching.

The source gas for the plasma usually contains small molecules rich in chlorine or fluorine. For instance, carbon tetrachloride () etches silicon and aluminium, and trifluoromethane etches silicon dioxide and silicon nitride. A plasma containing oxygen is used to oxidize ("ash") photoresist and facilitate its removal.

Ion milling, or sputter etching, uses lower pressures, often as low as 10−4 Torr (10 mPa). It bombards the wafer with energetic ions of noble gases, often Ar+, which knock atoms from the substrate by transferring momentum. Because the etching is performed by ions, which approach the wafer approximately from one direction, this process is highly anisotropic. On the other hand, it tends to display poor selectivity. Reactive-ion etching (RIE) operates under conditions intermediate between sputter and plasma etching (between 10–3 and 10−1 Torr). Deep reactive-ion etching (DRIE) modifies the RIE technique to produce deep, narrow features.

In reactive-ion etching (RIE), the substrate is placed inside a reactor, and several gases are introduced. A plasma is struck in the gas mixture using an RF power source, which breaks the gas molecules into ions. The ions accelerate towards, and react with, the surface of the material being etched, forming another gaseous material. This is known as the chemical part of reactive ion etching. There is also a physical part, which is similar to the sputtering deposition process. If the ions have high enough energy, they can knock atoms out of the material to be etched without a chemical reaction. It is a very complex task to develop dry etch processes that balance chemical and physical etching, since there are many parameters to adjust. By changing the balance it is possible to influence the anisotropy of the etching, since the chemical part is isotropic and the physical part highly anisotropic the combination can form sidewalls that have shapes from rounded to vertical.
Deep RIE (DRIE) is a special subclass of RIE that is growing in popularity. In this process, etch depths of hundreds of micrometres are achieved with almost vertical sidewalls. The primary technology is based on the so-called "Bosch process", named after the German company Robert Bosch, which filed the original patent, where two different gas compositions alternate in the reactor. Currently there are two variations of the DRIE. The first variation consists of three distinct steps (the original Bosch process) while the second variation only consists of two steps.

In the first variation, the etch cycle is as follows:

(i) isotropic etch;
(ii) passivation;
(iii) anisoptropic etch for floor cleaning.

In the 2nd variation, steps (i) and (iii) are combined.

Both variations operate similarly. The creates a polymer on the surface of the substrate, and the second gas composition ( and ) etches the substrate. The polymer is immediately sputtered away by the physical part of the etching, but only on the horizontal surfaces and not the sidewalls. Since the polymer only dissolves very slowly in the chemical part of the etching, it builds up on the sidewalls and protects them from etching. As a result, etching aspect ratios of 50 to 1 can be achieved. The process can easily be used to etch completely through a silicon substrate, and etch rates are 3–6 times higher than wet etching.

After preparing a large number of MEMS devices on a silicon wafer, individual dies have to be separated, which is called die preparation in semiconductor technology. For some applications, the separation is preceded by wafer backgrinding in order to reduce the wafer thickness. Wafer dicing may then be performed either by sawing using a cooling liquid or a dry laser process called stealth dicing.

Bulk micromachining is the oldest paradigm of silicon-based MEMS. The whole thickness of a silicon wafer is used for building the micro-mechanical structures. Silicon is machined using various etching processes. Anodic bonding of glass plates or additional silicon wafers is used for adding features in the third dimension and for hermetic encapsulation. Bulk micromachining has been essential in enabling high performance pressure sensors and accelerometers that changed the sensor industry in the 1980s and 90's.

Surface micromachining uses layers deposited on the surface of a substrate as the structural materials, rather than using the substrate itself. Surface micromachining was created in the late 1980s to render micromachining of silicon more compatible with planar integrated circuit technology, with the goal of combining MEMS and integrated circuits on the same silicon wafer. The original surface micromachining concept was based on thin polycrystalline silicon layers patterned as movable mechanical structures and released by sacrificial etching of the underlying oxide layer. Interdigital comb electrodes were used to produce in-plane forces and to detect in-plane movement capacitively. This MEMS paradigm has enabled the manufacturing of low cost accelerometers for e.g. automotive air-bag systems and other applications where low performance and/or high g-ranges are sufficient. Analog Devices has pioneered the industrialization of surface micromachining and has realized the co-integration of MEMS and integrated circuits.

To control the size of micro and nano-scale components, the use of so-called etchless processes is often applied. This approach to MEMS fabrication relies mostly on the oxidation of silicon, as described by the Deal-Grove model. Thermal oxidation processes are used to produced diverse silicon structures with highly precise dimensional control. Devices including optical frequency combs, and silicon MEMS pressure sensors, have been produced through the use of thermal oxidation processes to fine-tune silicon structures in one or two dimensions. Thermal oxidation is of particular value in the fabrication of silicon nanowires, which are widely employed in MEMS systems as both mechanical and electrical components.

Both bulk and surface silicon micromachining are used in the industrial production of sensors, ink-jet nozzles, and other devices. But in many cases the distinction between these two has diminished. A new etching technology, deep reactive-ion etching, has made it possible to combine good performance typical of bulk micromachining with comb structures and in-plane operation typical of surface micromachining. While it is common in surface micromachining to have structural layer thickness in the range of 2 µm, in HAR silicon micromachining the thickness can be from 10 to 100 µm. The materials commonly used in HAR silicon micromachining are thick polycrystalline silicon, known as epi-poly, and bonded silicon-on-insulator (SOI) wafers although processes for bulk silicon wafer also have been created (SCREAM). Bonding a second wafer by glass frit bonding, anodic bonding or alloy bonding is used to protect the MEMS structures. Integrated circuits are typically not combined with HAR silicon micromachining.

Some common commercial applications of MEMS include:


The global market for micro-electromechanical systems, which includes products such as automobile airbag systems, display systems and inkjet cartridges totaled $40 billion in 2006 according to Global MEMS/Microsystems Markets and Opportunities, a research report from SEMI and Yole Development and is forecasted to reach $72 billion by 2011.

Companies with strong MEMS programs come in many sizes. Larger firms specialize in manufacturing high volume inexpensive components or packaged solutions for end markets such as automobiles, biomedical, and electronics. Smaller firms provide value in innovative solutions and absorb the expense of custom fabrication with high sales margins. Both large and small companies typically invest in R&D to explore new MEMS technology.

The market for materials and equipment used to manufacture MEMS devices topped $1 billion worldwide in 2006. Materials demand is driven by substrates, making up over 70 percent of the market, packaging coatings and increasing use of chemical mechanical planarization (CMP). While MEMS manufacturing continues to be dominated by used semiconductor equipment, there is a migration to 200 mm lines and select new tools, including etch and bonding for certain MEMS applications.



</doc>
<doc id="19639" url="https://en.wikipedia.org/wiki?curid=19639" title="Marvin Minsky">
Marvin Minsky

Marvin Lee Minsky (August 9, 1927 – January 24, 2016) was an American cognitive scientist concerned largely with research of artificial intelligence (AI), co-founder of the Massachusetts Institute of Technology's AI laboratory, and author of several texts concerning AI and philosophy.

Minsky received many accolades and honors, such as the 1969 Turing Award.

Marvin Lee Minsky was born in New York City, to an eye surgeon father, Henry, and to a mother, Fannie (Reiser), who was a Zionist activist. His family was Jewish. He attended the Ethical Culture Fieldston School and the Bronx High School of Science. He later attended Phillips Academy in Andover, Massachusetts. He then served in the US Navy from 1944 to 1945. He received a B.A. in mathematics from Harvard University in 1950 and a Ph.D. in mathematics from Princeton University in 1954. His doctoral dissertation was titled "Theory of neural-analog reinforcement systems and its application to the brain-model problem." He was a Junior Fellow of the Harvard Society of Fellows from 1954–1957.

He was on the MIT faculty from 1958 to his death. He joined the staff at MIT Lincoln Laboratory in 1958, and a year later he and John McCarthy initiated what is, , named the MIT Computer Science and Artificial Intelligence Laboratory. He was the Toshiba Professor of Media Arts and Sciences, and professor of electrical engineering and computer science.

Minsky's inventions include the first head-mounted graphical display (1963) and the confocal microscope (1957, a predecessor to today's widely used confocal laser scanning microscope). He developed, with Seymour Papert, the first Logo "turtle". Minsky also built, in 1951, the first randomly wired neural network learning machine, SNARC.

In 1962, Minsky published a (7,4) Turing machine and proved it universal. It was the simplest known universal Turing machine until Stephen Wolfram's (2,3) Turing machine was proven to be universal in 2007.

Minsky wrote the book "Perceptrons" (with Seymour Papert), attacking the work of Frank Rosenblatt, which became the foundational work in the analysis of artificial neural networks. This book is the center of a controversy in the history of AI, as some claim it to have had great importance in discouraging research of neural networks in the 1970s, and contributing to the so-called "AI winter". He also founded several other AI models. His book "A framework for representing knowledge" created a new paradigm in programming. While his "Perceptrons" is now more a historical than practical book, the theory of frames is in wide use. Minsky also wrote of the possibility that extraterrestrial life may think like humans, permitting communication.

In the early 1970s, at the MIT Artificial Intelligence Lab, Minsky and Papert started developing what came to be known as the Society of Mind theory. The theory attempts to explain how what we call intelligence could be a product of the interaction of non-intelligent parts. Minsky says that the biggest source of ideas about the theory came from his work in trying to create a machine that uses a robotic arm, a video camera, and a computer to build with children's blocks. In 1986, Minsky published "The Society of Mind", a comprehensive book on the theory which, unlike most of his previously published work, was written for the general public.
In November 2006, Minsky published "The Emotion Machine", a book that critiques many popular theories of how human minds work and suggests alternative theories, often replacing simple ideas with more complex ones. Recent drafts of the book are freely available from his webpage.

Minsky was an adviser on Stanley Kubrick's movie ; one of the movie's characters, Victor Kaminski, was named in Minsky's honor. Minsky is mentioned explicitly in Arthur C. Clarke's derivative novel of the same name, where he is portrayed as achieving a crucial break-through in artificial intelligence in the then-future 1980s, paving the way for HAL 9000 in the early 21st century:

In 1952, Minsky married pediatrician Gloria Rudisch; together they had three children. Minsky was a talented improvisational pianist who published musings on the relations between music and psychology.

Minsky was an atheist, a signatory to the Scientists' Open Letter on Cryonics.

He was a critic of the Loebner Prize for conversational robots, and argued that a fundamental difference between humans and machines was that while humans are machines, they are machines in which intelligence emerges from the interplay of the many unintelligent but semi-autonomous agents that comprise the brain. He argued that "somewhere down the line, some computers will become more intelligent than most people," but that it was very hard to predict how fast progress would be. He cautioned that an artificial superintelligence designed to solve an innocuous mathematical problem might decide to assume control of Earth's resources to build supercomputers to help achieve its goal, but believed that such negative scenarios are "hard to take seriously" because he felt confident that AI would go through a lot of testing before being deployed.

In January 2016 Minsky died of a cerebral hemorrhage, at the age of 88. Minsky was a member of Alcor Life Extension Foundation's Scientific Advisory Board. Alcor will neither confirm nor deny whether Minsky was cryonically preserved.

Minsky received a $100,000 research grant from Jeffrey Epstein in 2002, four years before Epstein's first arrest for sex offenses; it was the first from Epstein to MIT. Minsky received no further research grants from him.

Minsky organized two academic symposia on Epstein's private island Little Saint James, one in 2002 and another in 2011, after Epstein was a registered sex offender. Virginia Guiffre testified in a 2015 deposition in her defamation lawsuit against Epstein associate Ghislaine Maxwell that Maxwell directed her to have sex with Minsky among others. There is no independent corroboration of the sex allegations, and there has been no lawsuit against Minsky's estate. Minsky's widow, Gloria Rudisch, says that he could not have had sex with any of the women at Epstein's residences, as they were always together during all of the visits to Epstein's residences.


Minsky won the Turing Award (the greatest distinction in computer science) in 1969, the Golden Plate Award of the American Academy of Achievement in 1982, the Japan Prize in 1990, the IJCAI Award for Research Excellence for 1991, and the Benjamin Franklin Medal from the Franklin Institute for 2001. In 2006, he was inducted as a Fellow of the Computer History Museum "for co-founding the field of artificial intelligence, creating early neural networks and robots, and developing theories of human and machine cognition." In 2011, Minsky was inducted into IEEE Intelligent Systems' AI Hall of Fame for the "significant contributions to the field of AI and intelligent systems". In 2014, Minsky won the Dan David Prize for "Artificial Intelligence, the Digital Mind". He was also awarded with the 2013 BBVA Foundation Frontiers of Knowledge Award in the Information and Communication Technologies category.

Minsky was affiliated with the following organizations:





</doc>
<doc id="19640" url="https://en.wikipedia.org/wiki?curid=19640" title="Milton Friedman">
Milton Friedman

Milton Friedman (; July 31, 1912 – November 16, 2006) was an American economist who received the 1976 Nobel Memorial Prize in Economic Sciences for his research on consumption analysis, monetary history and theory and the complexity of stabilization policy. With George Stigler and others, Friedman was among the intellectual leaders of the Chicago school of economics, a neoclassical school of economic thought associated with the work of the faculty at the University of Chicago that rejected Keynesianism in favor of monetarism until the mid-1970s, when it turned to new classical macroeconomics heavily based on the concept of rational expectations. Several students and young professors who were recruited or mentored by Friedman at Chicago went on to become leading economists, including Gary Becker, Robert Fogel, Thomas Sowell and Robert Lucas Jr.

Friedman's challenges to what he later called "naive Keynesian" theory began with his 1950s reinterpretation of the consumption function. During the 1960s he became the main advocate opposing Keynesian government policies, and described his approach (along with mainstream economics) as using "Keynesian language and apparatus" yet rejecting its "initial" conclusions. He theorized that there existed a "natural" rate of unemployment and argued that unemployment below this rate would cause inflation to accelerate. He argued that the Phillips curve was in the long run vertical at the "natural rate" and predicted what would come to be known as stagflation. Friedman promoted an alternative macroeconomic viewpoint known as "monetarism" and argued that a steady, small expansion of the money supply was the preferred policy. His ideas concerning monetary policy, taxation, privatization and deregulation influenced government policies, especially during the 1980s. His monetary theory influenced the Federal Reserve's response to the global financial crisis of 2007–2008.

Friedman was an advisor to Republican President Ronald Reagan and Conservative British Prime Minister Margaret Thatcher. His political philosophy extolled the virtues of a free market economic system with minimal intervention. He once stated that his role in eliminating conscription in the United States was his proudest accomplishment. In his 1962 book "Capitalism and Freedom", Friedman advocated policies such as a volunteer military, freely floating exchange rates, abolition of medical licenses, a negative income tax and school vouchers and opposed the war on drugs. His support for school choice led him to found the Friedman Foundation for Educational Choice, later renamed EdChoice.

Friedman's works include monographs, books, scholarly articles, papers, magazine columns, television programs, and lectures, and cover a broad range of economic topics and public policy issues. His books and essays have had global influence, including in former communist states. A survey of economists ranked Friedman as the second-most popular economist of the 20th century, following only John Maynard Keynes, and "The Economist" described him as "the most influential economist of the second half of the 20th century ... possibly of all of it".

Friedman was born in Brooklyn, New York on July 31, 1912. His parents, Sára Ethel (née Landau) and Jenő Saul Friedman, were Jewish immigrants from Beregszász in Carpathian Ruthenia, Kingdom of Hungary (now Berehove in Ukraine). They both worked as dry goods merchants. Shortly after his birth, the family relocated to Rahway, New Jersey. In his early teens, Friedman was injured in a car accident, which scarred his upper lip. A talented student, Friedman graduated from Rahway High School in 1928, just before his 16th birthday. He was awarded a competitive scholarship to Rutgers University (then a private university receiving limited support from the State of New Jersey, e.g., for such scholarships). He specialized in mathematics and economics, and became influenced by two economics professors, Arthur F. Burns and Homer Jones, who convinced him that modern economics could help end the Great Depression.

Friedman graduated in 1932, and initially intended to become an actuary. But he was offered two scholarships to do graduate work, one in mathematics at Brown University and the other in economics at the University of Chicago. Friedman chose the second, earning a Master of Arts degree in 1933. He was strongly influenced by Jacob Viner, Frank Knight, and Henry Simons. At Chicago Friedman met his future wife, economist Rose Director.

During the 1933–1934 academic year he had a fellowship at Columbia University, where he studied statistics with statistician and economist Harold Hotelling. He was back in Chicago for the 1934–1935 academic year, working as a research assistant for Henry Schultz, who was then working on "Theory and Measurement of Demand". That year, Friedman formed what would prove to be lifelong friendships with George Stigler and W. Allen Wallis.

Friedman was unable to find academic employment, so in 1935 followed his friend W. Allen Wallis to Washington, D.C., where Franklin D. Roosevelt's New Deal was "a lifesaver" for many young economists. At this stage, Friedman said that he and his wife "regarded the job-creation programs such as the WPA, CCC, and PWA appropriate responses to the critical situation," but not "the price- and wage-fixing measures of the National Recovery Administration and the Agricultural Adjustment Administration." Foreshadowing his later ideas, he believed price controls interfered with an essential signaling mechanism to help resources be used where they were most valued. Indeed, Friedman later concluded that all government intervention associated with the New Deal was "the wrong cure for the wrong disease," arguing that the money supply should simply have been expanded, instead of contracted. Later, Friedman and his colleague Anna Schwartz wrote "A Monetary History of the United States, 1867–1960", which argued that the Great Depression was caused by a severe monetary contraction due to banking crises and poor policy on the part of the Federal Reserve. Robert J. Shiller describes the book as the "most influential account" of the Great Depression.

During 1935, he began working for the National Resources Planning Board, which was then working on a large consumer budget survey. Ideas from this project later became a part of his "Theory of the Consumption Function". Friedman began employment with the National Bureau of Economic Research during autumn 1937 to assist Simon Kuznets in his work on professional income. This work resulted in their jointly authored publication "Incomes from Independent Professional Practice", which introduced the concepts of permanent and transitory income, a major component of the Permanent Income Hypothesis that Friedman worked out in greater detail in the 1950s. The book hypothesizes that professional licensing artificially restricts the supply of services and raises prices.

During 1940, Friedman was appointed as an assistant professor teaching Economics at the University of Wisconsin–Madison, but encountered antisemitism in the Economics department and returned to government service. From 1941 to 1943 Friedman worked on wartime tax policy for the federal government, as an advisor to senior officials of the United States Department of the Treasury. As a Treasury spokesman during 1942 he advocated a Keynesian policy of taxation. He helped to invent the payroll withholding tax system, since the federal government needed money to fund the war. He later said, "I have no apologies for it, but I really wish we hadn't found it necessary and I wish there were some way of abolishing withholding now."

In 1940, Friedman accepted a position at the University of Wisconsin–Madison, but left because of differences with faculty regarding United States involvement in World War II. Friedman believed the United States should enter the war. In 1943, Friedman joined the Division of War Research at Columbia University (headed by W. Allen Wallis and Harold Hotelling), where he spent the rest of World War II working as a mathematical statistician, focusing on problems of weapons design, military tactics, and metallurgical experiments.

In 1945, Friedman submitted "Incomes from Independent Professional Practice" (co-authored with Kuznets and completed during 1940) to Columbia as his doctoral dissertation. The university awarded him a PhD in 1946. Friedman spent the 1945–1946 academic year teaching at the University of Minnesota (where his friend George Stigler was employed). On February 12, 1945, his son, David D. Friedman was born.

In 1946, Friedman accepted an offer to teach economic theory at the University of Chicago (a position opened by departure of his former professor Jacob Viner to Princeton University). Friedman would work for the University of Chicago for the next 30 years. There he contributed to the establishment of an intellectual community that produced a number of Nobel Prize winners, known collectively as the Chicago school of economics.

At that time, Arthur F. Burns, who was then the head of the National Bureau of Economic Research, asked Friedman to rejoin the Bureau's staff. He accepted the invitation, and assumed responsibility for the Bureau's inquiry into the role of money in the business cycle. As a result, he initiated the "Workshop in Money and Banking" (the "Chicago Workshop"), which promoted a revival of monetary studies. During the latter half of the 1940s, Friedman began a collaboration with Anna Schwartz, an economic historian at the Bureau, that would ultimately result in the 1963 publication of a book co-authored by Friedman and Schwartz, "A Monetary History of the United States, 1867–1960".

Friedman spent the 1954–1955 academic year as a Fulbright Visiting Fellow at Gonville and Caius College, Cambridge. At the time, the Cambridge economics faculty was divided into a Keynesian majority (including Joan Robinson and Richard Kahn) and an anti-Keynesian minority (headed by Dennis Robertson). Friedman speculated that he was invited to the fellowship, because his views were unacceptable to both of the Cambridge factions. Later his weekly columns for "Newsweek" magazine (1966–84) were well read and increasingly influential among political and business people, and helped earn the magazine a Gerald Loeb Special Award in 1968. From 1968 to 1978, he and Paul Samuelson participated in the Economics Cassette Series, a biweekly subscription series where the economist would discuss the days' issues for about a half-hour at a time.

One of Milton Friedman's most popular works, "A Theory of the Consumption Function", challenged traditional Keynesian viewpoints about the household. This work was originally published in 1957 by Princeton University Press, and it reanalysed the relationship displayed "between aggregate consumption or aggregate savings and aggregate income." Keynes believed that people would modify their household consumption expenditures to relate to their existing income levels. Friedman's research introduced the term "permanent income" to the world, which was the average of a household's expected income over several years, and he also developed the permanent income hypothesis. Milton Friedman's research changed how economists interpreted the consumption function, and his work pushed the idea that current income was not the only factor that affected people's adjustment household consumption expenditures. Instead, expected income levels also affected how households would change their consumption expenditures. Friedman's contributions strongly influenced research on consumer behavior, and he further defined how to predict consumption smoothing, which contradicts Keynes' marginal propensity to consume. Although this work presented many controversial points of view that differed from existing viewpoints established by Keynes, "A Theory of the Consumption Function" helped Friedman gain respect in the field of economics.

His "Capitalism and Freedom" brought him national and international attention outside academia. It was published in 1962 by the University of Chicago Press and consists of essays that used non-mathematical economic models to explore issues of public policy. It sold over 400,000 copies in the first eighteen years and more than half a million since 1962. It has been translated into eighteen languages. Friedman talks about the need to move to a classically liberal society, that free markets would help nations and individuals in the long-run and fix the efficiency problems currently faced by the United States and other major countries of the 1950s and 1960s. He goes through the chapters specifying a specific issue in each respective chapter from the role of government and money supply to social welfare programs to a special chapter on occupational licensure. Friedman concludes "Capitalism and Freedom" with his "classical liberal" (more accurately, libertarian) stance, that government should stay out of matters that do not need and should only involve itself when absolutely necessary for the survival of its people and the country. He recounts how the best of a country's abilities come from its free markets while its failures come from government intervention.

In 1977, at the age of 65, Friedman retired from the University of Chicago after teaching there for 30 years. He and his wife moved to San Francisco, where he became a visiting scholar at the Federal Reserve Bank of San Francisco. From 1977 on, he was affiliated with the Hoover Institution at Stanford University. During the same year, Friedman was approached by the Free To Choose Network and asked to create a television program presenting his economic and social philosophy.

The Friedmans worked on this project for the next three years, and during 1980, the ten-part series, titled "Free to Choose", was broadcast by the Public Broadcasting Service (PBS). The companion book to the series (co-authored by Milton and his wife, Rose Friedman), also titled "Free To Choose", was the bestselling nonfiction book of 1980 and has since been translated into 14 languages.

Friedman served as an unofficial adviser to Ronald Reagan during his 1980 presidential campaign, and then served on the President's Economic Policy Advisory Board for the rest of the Reagan Administration. Ebenstein says Friedman was "the 'guru' of the Reagan administration." In 1988 he received the National Medal of Science and Reagan honored him with the Presidential Medal of Freedom.

Friedman is known now as one of the most influential economists of the 20th century. Throughout the 1980s and 1990s, Friedman continued to write editorials and appear on television. He made several visits to Eastern Europe and to China, where he also advised governments. He was also for many years a Trustee of the Philadelphia Society.

According to a 2007 article in "Commentary" magazine, his "parents were moderately observant Jews, but Friedman, after an intense burst of childhood piety, rejected religion altogether." He described himself as an agnostic. Friedman wrote extensively of his life and experiences, especially in 1998 in his memoirs with his wife, Rose, titled "Two Lucky People".

Friedman died of heart failure at the age of 94 years in San Francisco on November 16, 2006. He was still a working economist performing original economic research; his last column was published in "The Wall Street Journal" the day after his death. He was survived by his wife (who died on August 18, 2009) and their two children, David, known for the anarcho-capitalist book "The Machinery of Freedom", and bridge expert Jan Martel.

Friedman was best known for reviving interest in the money supply as a determinant of the nominal value of output, that is, the quantity theory of money. Monetarism is the set of views associated with modern quantity theory. Its origins can be traced back to the 16th-century School of Salamanca or even further; however, Friedman's contribution is largely responsible for its modern popularization. He co-authored, with Anna Schwartz, "A Monetary History of the United States, 1867–1960" (1963), which was an examination of the role of the money supply and economic activity in the U.S. history. A striking conclusion of their research regarded the way in which money supply fluctuations contribute to economic fluctuations. Several regression studies with David Meiselman during the 1960s suggested the primacy of the money supply over investment and government spending in determining consumption and output. These challenged a prevailing, but largely untested, view on their relative importance. Friedman's empirical research and some theory supported the conclusion that the short-run effect of a change of the money supply was primarily on output but that the longer-run effect was primarily on the price level.

Friedman was the main proponent of the monetarist school of economics. He maintained that there is a close and stable association between inflation and the money supply, mainly that inflation could be avoided with proper regulation of the monetary base's growth rate. He famously used the analogy of "dropping money out of a helicopter", in order to avoid dealing with money injection mechanisms and other factors that would overcomplicate his models.

Friedman's arguments were designed to counter the popular concept of cost-push inflation, that the increased general price level at the time was the result of increases in the price of oil, or increases in wages; as he wrote:

Friedman rejected the use of fiscal policy as a tool of demand management; and he held that the government's role in the guidance of the economy should be restricted severely. Friedman wrote extensively on the Great Depression, and he termed the 1929–1933 period the Great Contraction. He argued that the Depression had been caused by an ordinary financial shock whose duration and seriousness were greatly increased by the subsequent contraction of the money supply caused by the misguided policies of the directors of the Federal Reserve.

This theory was put forth in "A Monetary History of the United States", and the chapter on the Great Depression was then published as a stand-alone book entitled "The Great Contraction, 1929–1933". Both books are still in print from Princeton University Press, and some editions include as an appendix a speech at a University of Chicago event honoring Friedman in which Ben Bernanke made this statement:

Let me end my talk by abusing slightly my status as an official representative of the Federal Reserve. I would like to say to Milton and Anna: Regarding the Great Depression, you're right. We did it. We're very sorry. But thanks to you, we won't do it again.

Friedman also argued for the cessation of government intervention in currency markets, thereby spawning an enormous literature on the subject, as well as promoting the practice of freely floating exchange rates. His close friend George Stigler explained, "As is customary in science, he did not win a full victory, in part because research was directed along different lines by the theory of rational expectations, a newer approach developed by Robert Lucas, also at the University of Chicago." The relationship between Friedman and Lucas, or new classical macroeconomics as a whole, was highly complex. The Friedmanian Phillips curve was an interesting starting point for Lucas, but he soon realized that the solution provided by Friedman was not quite satisfactory. Lucas elaborated a new approach in which rational expectations were presumed instead of the Friedmanian adaptive expectations. Due to this reformulation, the story in which the theory of the new classical Phillips curve was embedded radically changed. This modification, however, had a significant effect on Friedman's own approach, so, as a result, the theory of the Friedmanian Phillips curve also changed. Moreover, new classical Neil Wallace, who was a graduate student at the University of Chicago between 1960 and 1963, regarded Friedman's theoretical courses as a mess. This evaluation clearly indicates the broken relationship between Friedmanian monetarism and new classical macroeconomics.

Friedman was also known for his work on the consumption function, the permanent income hypothesis (1957), which Friedman himself referred to as his best scientific work. This work contended that rational consumers would spend a proportional amount of what they perceived to be their permanent income. Windfall gains would mostly be saved. Tax reductions likewise, as rational consumers would predict that taxes would have to increase later to balance public finances. Other important contributions include his critique of the Phillips curve and the concept of the natural rate of unemployment (1968). This critique associated his name, together with that of Edmund Phelps, with the insight that a government that brings about greater inflation cannot permanently reduce unemployment by doing so. Unemployment may be temporarily lower, if the inflation is a surprise, but in the long run unemployment will be determined by the frictions and imperfections of the labor market.

Friedman's essay "The Methodology of Positive Economics" (1953) provided the epistemological pattern for his own subsequent research and to a degree that of the Chicago School. There he argued that economics as "science" should be free of value judgments for it to be objective. Moreover, a useful economic theory should be judged not by its descriptive realism but by its simplicity and fruitfulness as an engine of prediction. That is, students should measure the accuracy of its predictions, rather than the 'soundness of its assumptions'. His argument was part of an ongoing debate among such statisticians as Jerzy Neyman, Leonard Savage, and Ronald Fisher.

However, despite being an advocate of the free market, Milton Friedman believed that the government had two crucial roles. In an interview with Phil Donahue, Milton Friedman argued that "the two basic functions of a government are to protect the nation against foreign enemy, and to protect citizens against its fellows.”. He also, admitted that although privatisation of national defence could reduce the overall cost, he has not yet thought of a way to make this privatisation possible.

One of his most famous contributions to statistics is sequential sampling. Friedman did statistical work at the Division of War Research at Columbia, where he and his colleagues came up with the technique. It became, in the words of "The New Palgrave Dictionary of Economics", "the standard analysis of quality control inspection". The dictionary adds, "Like many of Friedman's contributions, in retrospect it seems remarkably simple and obvious to apply basic economic ideas to quality control; that, however, is a measure of his genius."

Although Friedman concluded the government does have a role in the monetary system he was critical of the Federal Reserve due to its poor performance and felt it should be abolished. He was opposed to Federal Reserve policies, even during the so-called 'Volcker shock' that was labeled 'monetarist'. Friedman believed that the Federal Reserve System should ultimately be replaced with a computer program. He favored a system that would automatically buy and sell securities in response to changes in the money supply.

The proposal to constantly grow the money supply at a certain predetermined amount every year has become known as Friedman's k-percent rule. There is debate about the effectiveness of a theoretical money supply targeting regime. The Fed's inability to meet its money supply targets from 1978–1982 has led some to conclude it is not a feasible alternative to more conventional inflation and interest rate targeting. Towards the end of his life, Friedman expressed doubt about the validity of targeting the quantity of money.

Idealistically, Friedman actually favored the principles of the 1930s Chicago plan, which would have ended fractional reserve banking and, thus, private money creation. It would force banks to have 100% reserves backing deposits, and instead place money creation powers solely in the hands of the US Government. This would make targeting money growth more possible, as endogenous money created by fractional reserve lending would no longer be a major issue.

Friedman was a strong advocate for floating exchange rates throughout the entire Bretton-Woods period. He argued that a flexible exchange rate would make external adjustment possible and allow countries to avoid balance of payments crises. He saw fixed exchange rates as an undesirable form of government intervention. The case was articulated in an influential 1953 paper, "The Case for Flexible Exchange Rates", at a time, when most commentators regarded the possibility of floating exchange rates as a fantasy.

In his 1955 article "The Role of Government in Education" Friedman proposed supplementing publicly operated schools with privately run but publicly funded schools through a system of school vouchers. Reforms similar to those proposed in the article were implemented in, for example, Chile in 1981 and Sweden in 1992. In 1996, Friedman, together with his wife, founded the Friedman Foundation for Educational Choice to advocate school choice and vouchers. In 2016, the Friedman Foundation changed its name to EdChoice to honor the Friedmans' desire to have the educational choice movement live on without their names attached to it after their deaths.

While Walter Oi is credited with establishing the economic basis for a volunteer military, Friedman was a proponent, stating that the draft was "inconsistent with a free society."
In "Capitalism and Freedom", he argued that conscription is inequitable and arbitrary, preventing young men from shaping their lives as they see fit. During the Nixon administration he headed the committee to research a conversion to paid/volunteer armed force. He would later state that his role in eliminating the conscription in the United States was his proudest accomplishment. Friedman did, however, believe that the introduction of a system of universal military training as a reserve in cases of war-time could be justified.
But opposed its implementation in the United States, describing it as a “monstrosity”.

Biographer Lanny Ebenstein noted a drift over time in Friedman's views from an interventionist to a more cautious foreign policy. He supported US involvement in the Second World War and initially supported a hard-line against Communism, but moderated over time. However, Friedman did state in a 1995 interview that he was an anti-interventionist. He opposed the Gulf War and the Iraq War. In a spring 2006 interview, Friedman said that the US's stature in the world had been eroded by the Iraq War, but that it might be improved if Iraq were to become a peaceful and independent country.

Friedman was an economic advisor and speech writer in Barry Goldwater's presidential campaign in 1964. He was an advisor to California governor Ronald Reagan, and was active in Reagan's presidential campaigns. He served as a member of President Reagan's Economic Policy Advisory Board starting in 1981. In 1988, he received the Presidential Medal of Freedom and the National Medal of Science. He said that he was a libertarian philosophically, but a member of the U.S. Republican Party for the sake of "expediency" ("I am a libertarian with a small 'l' and a Republican with a capital 'R.' And I am a Republican with a capital 'R' on grounds of expediency, not on principle.") But, he said, "I think the term classical liberal is also equally applicable. I don't really care very much what I'm called. I'm much more interested in having people thinking about the ideas, rather than the person."

Friedman was supportive of the state provision of some public goods that private businesses are not considered as being able to provide. However, he argued that many of the services performed by government could be performed better by the private sector. Above all, if some public goods are provided by the state, he believed that they should not be a legal monopoly where private competition is prohibited; for example, he wrote:

In 1962, Friedman criticized Social Security in his book "Capitalism and Freedom", arguing that it had created welfare dependency. However, in the penultimate chapter of the same book, Friedman argued that while capitalism had greatly reduced the extent of poverty in absolute terms, "poverty is in part a relative matter, [and] even in [wealthy Western] countries, there are clearly many people living under conditions that the rest of us label as poverty." Friedman also noted that while private charity could be one recourse for alleviating poverty and cited late 19th century Britain and the United States as exemplary periods of extensive private charity and eleemosynary activity, he made the following point:

Friedman argued further that other advantages of the negative income tax were that it could fit directly into the tax system, would be less costly, and would reduce the administrative burden of implementing a social safety net. Friedman reiterated these arguments 18 years later in "Free to Choose", with the additional proviso that such a reform would only be satisfactory if it replaced the current system of welfare programs rather than augment it. According to economist Robert H. Frank, writing in "The New York Times", Friedman's views in this regard were grounded in a belief that while "market forces ... accomplish wonderful things", they "cannot ensure a distribution of income that enables all citizens to meet basic economic needs".

Friedman also supported libertarian policies such as legalization of drugs and prostitution. During 2005, Friedman and more than 500 other economists advocated discussions regarding the economic benefits of the legalization of marijuana.

Friedman was also a supporter of gay rights. He never specifically supported same-sex marriage, instead saying "I do not believe there should be any discrimination against gays."

Friedman favored immigration, saying "legal and illegal immigration has a very positive impact on the U.S. economy." However, he suggested that immigrants ought not to have access to the welfare system. Friedman stated that immigration from Mexico had been a "good thing", in particular illegal immigration. Friedman argued that illegal immigration was a boon because they "take jobs that most residents of this country are unwilling to take, they provide employers with workers of a kind they cannot get" and they do not use welfare. In "Free to Choose", Friedman wrote:

No arbitrary obstacles should prevent people from achieving those positions for which their talents fit them and which their values lead them to seek. Not birth, nationality, color, religion, sex, nor any other irrelevant characteristic should determine the opportunities that are open to a person — only his abilities.

Michael Walker of the Fraser Institute and Friedman hosted a series of conferences from 1986 to 1994. The goal was to create a clear definition of economic freedom and a method for measuring it. Eventually this resulted in the first report on worldwide economic freedom, "Economic Freedom in the World". This annual report has since provided data for numerous peer-reviewed studies and has influenced policy in several nations.

Along with sixteen other distinguished economists he opposed the Copyright Term Extension Act, and signed on to an amicus brief filed in "Eldred v. Ashcroft". Friedman jokingly described it as a "no-brainer".

Friedman argued for stronger basic legal (constitutional) protection of economic rights and freedoms to further promote industrial-commercial growth and prosperity and buttress democracy and freedom and the rule of law generally in society.

George H. Nash, a leading historian of American conservatism, says that by "the end of the 1960s he was probably the most highly regarded and influential conservative scholar in the country, and one of the few with an international reputation." Friedman allowed the libertarian Cato Institute to use his name for its biannual Milton Friedman Prize for Advancing Liberty beginning in 2001. A Friedman Prize was given to the late British economist Peter Bauer in 2002, Peruvian economist Hernando de Soto in 2004, Mart Laar, former Estonian Prime Minister in 2006 and a young Venezuelan student Yon Goicoechea in 2008. His wife Rose, sister of Aaron Director, with whom he initiated the Friedman Foundation for Educational Choice, served on the international selection committee. 

Friedman was also a recipient of the Nobel Memorial Prize in Economics.

Upon Friedman's death, Harvard President Lawrence Summers called him "The Great Liberator", saying "... any honest Democrat will admit that we are now all Friedmanites." He said Friedman's great popular contribution was "in convincing people of the importance of allowing free markets to operate."

Stephen Moore, a member of the editorial forward of "The Wall Street Journal", said in 2013: "Quoting the most-revered champion of free-market economics since Adam Smith has become a little like quoting the Bible." He adds, "There are sometimes multiple and conflicting interpretations."

Friedman won the Nobel Memorial Prize in Economic Sciences, the sole recipient for 1976, "for his achievements in the fields of consumption analysis, monetary history and theory and for his demonstration of the complexity of stabilization policy."

Friedman once said: "If you want to see capitalism in action, go to Hong Kong." He wrote in 1990 that the Hong Kong economy was perhaps the best example of a free market economy.

One month before his death, he wrote the article "Hong Kong Wrong—What would Cowperthwaite say?" in "The Wall Street Journal", criticizing Donald Tsang, the Chief Executive of Hong Kong, for abandoning "positive noninterventionism." Tsang later said he was merely changing the slogan to "big market, small government", where small government is defined as less than 20% of GDP. In a debate between Tsang and his rival Alan Leong before the 2007 Hong Kong Chief Executive election, Leong introduced the topic and jokingly accused Tsang of angering Friedman to death.

During 1975, two years after the military coup that brought military dictator President Augusto Pinochet to power and ended the government of Salvador Allende, the economy of Chile experienced a severe crisis. Friedman and Arnold Harberger accepted an invitation of a private Chilean foundation to visit Chile and speak on principles of economic freedom. He spent seven days in Chile giving a series of lectures at the Universidad Católica de Chile and the (National) University of Chile. One of the lectures was entitled "The Fragility of Freedom" and according to Friedman, "dealt with precisely the threat to freedom from a centralized military government."

In a letter to Pinochet of April 21, 1975, Friedman considered the "key economic problems of Chile are clearly ... inflation and the promotion of a healthy social market economy". He stated that "There is only one way to end inflation: by drastically reducing the rate of increase of the quantity of money ..." and that "... cutting government spending is by far and away the most desirable way to reduce the fiscal deficit, because it ... strengthens the private sector thereby laying the foundations for "healthy" economic growth". As to how rapidly inflation should be ended, Friedman felt that "for Chile where inflation is raging at 10–20% a month ... gradualism is not feasible. It would involve so painful an operation over so long a period that the "patient" would not survive." Choosing "a brief period of higher unemployment ..." was the lesser evil.. and that "the experience of Germany, ... of Brazil ..., of the post-war adjustment in the U.S. ... all argue for shock treatment". In the letter Friedman recommended to deliver the shock approach with "... a package to eliminate the surprise and to relieve acute distress" and "... for definiteness let me sketch the contents of a package proposal ... to be taken as illustrative" although his knowledge of Chile was "too limited to enable [him] to be precise or comprehensive". He listed a "sample proposal" of 8 monetary and fiscal measures including "the removal of as many as obstacles as possible that now hinder the private market. For example, suspend ... the present law against discharging employees". He closed, stating "Such a shock program could end inflation in months". His letter suggested that cutting spending to reduce the fiscal deficit would result in less transitional unemployment than raising taxes.

Sergio de Castro, a Chilean Chicago School graduate, became the nation's Minister of Finance in 1975. During his six-year tenure, foreign investment increased, restrictions were placed on striking and labor unions, and GDP rose yearly. A foreign exchange program was created between the Catholic University of Chile and the University of Chicago. Many other Chicago School alumni were appointed government posts during and after the Pinochet years; others taught its economic doctrine at Chilean universities. They became known as the Chicago Boys.

Friedman did not criticize Pinochet's dictatorship at the time, nor the assassinations, illegal imprisonments, torture, or other atrocities that were well known by then.
In 1976 Friedman defended his unofficial adviser position with: "I do not consider it as evil for an economist to render technical economic advice to the Chilean Government, any more than I would regard it as evil for a physician to give technical medical advice to the Chilean Government to help end a medical plague."

Friedman defended his activity in Chile on the grounds that, in his opinion, the adoption of free market policies not only improved the economic situation of Chile but also contributed to the amelioration of Pinochet's rule and to the eventual transition to a democratic government during 1990. That idea is included in "Capitalism and Freedom", in which he declared that economic freedom is not only desirable in itself but is also a necessary condition for political freedom. In his 1980 documentary "Free to Choose", he said the following: "Chile is not a politically free system, and I do not condone the system. But the people there are freer than the people in Communist societies because government plays a smaller role. ... The conditions of the people in the past few years has been getting better and not worse. They would be still better to get rid of the junta and to be able to have a free democratic system." In 1984, Friedman stated that he has "never refrained from criticizing the political system in Chile." In 1991 he said: "I have nothing good to say about the political regime that Pinochet imposed. It was a terrible political regime. The real miracle of Chile is not how well it has done economically; the real miracle of Chile is that a military junta was willing to go against its principles and support a free market regime designed by principled believers in a free market. ... In Chile, the drive for political freedom, that was generated by economic freedom and the resulting economic success, ultimately resulted in a referendum that introduced political democracy. Now, at long last, Chile has all three things: political freedom, human freedom and economic freedom. Chile will continue to be an interesting experiment to watch to see whether it can keep all three or whether, now that it has political freedom, that political freedom will tend to be used to destroy or reduce economic freedom." He stressed that the lectures he gave in Chile were the same lectures he later gave in China and other socialist states.

During the 2000 PBS documentary "The Commanding Heights" (based on the book), Friedman continued to argue that "free markets would undermine [Pinochet's] political centralization and political control.", and that criticism over his role in Chile missed his main contention that freer markets resulted in freer people, and that Chile's unfree economy had caused the military government. Friedman advocated for free markets which undermined "political centralization and political control".

Friedman visited Iceland during the autumn of 1984, met with important Icelanders and gave a lecture at the University of Iceland on the "tyranny of the "status quo"." He participated in a lively television debate on August 31, 1984, with socialist intellectuals, including Ólafur Ragnar Grímsson, who later became the president of Iceland. When they complained that a fee was charged for attending his lecture at the university and that, hitherto, lectures by visiting scholars had been free-of-charge, Friedman replied that previous lectures had not been free-of-charge in a meaningful sense: lectures always have related costs. What mattered was whether attendees or non-attendees covered those costs. Friedman thought that it was fairer that only those who attended paid. In this discussion Friedman also stated that he did not receive any money for delivering that lecture.

Although Friedman never visited Estonia, his book "Free to Choose" exercised a great influence on that nation's then 32-year-old prime minister, Mart Laar, who has claimed that it was the only book on economics he had read before taking office. Laar's reforms are often credited with responsibility for transforming Estonia from an impoverished Soviet Republic to the "Baltic Tiger." A prime element of Laar's program was introduction of the flat tax. Laar won the 2006 Milton Friedman Prize for Advancing Liberty, awarded by the Cato Institute.

After 1950 Friedman was frequently invited to lecture in Britain, and by the 1970s his ideas had gained widespread attention in conservative circles. For example, he was a regular speaker at the Institute of Economic Affairs (IEA), a libertarian think tank. Conservative politician Margaret Thatcher closely followed IEA programs and ideas, and met Friedman there in 1978. He also strongly influenced Keith Joseph, who became Thatcher's senior advisor on economic affairs, as well as Alan Walters and Patrick Minford, two other key advisers. Major newspapers, including the "Daily Telegraph," "The Times," and "The Financial Times" all promulgated Friedman's monetarist ideas to British decision-makers. Friedman's ideas strongly influenced Thatcher and her allies when she became Prime Minister in 1979.

After his death a number of obituaries and articles were written in Friedman's honor, citing him as one of the most important and influential economists of the post-war era. Milton Friedman's somewhat controversial legacy in America remains strong within the conservative movement. However, some journalists and economists like Noah Smith and Scott Sumner have argued Friedman's academic legacy has been buried under his political philosophy and misinterpreted by modern conservatives.

Econometrician David Hendry criticized part of Friedman's and Anna Schwartz's 1982 "Monetary Trends". When asked about it during an interview with Icelandic TV in 1984, Friedman said that the criticism referred to a different problem from that which he and Schwartz had tackled, and hence was irrelevant, and pointed out the lack of consequential peer review amongst econometricians on Hendry's work. In 2006, Hendry said that Friedman was guilty of "serious errors" of misunderstanding that meant "the t-ratios he reported for UK money demand were overstated by nearly 100 per cent", and said that, in a paper published in 1991 with Neil Ericsson, he had refuted "almost every empirical claim ... made about UK money demand" by Friedman and Schwartz. A 2004 paper updated and confirmed the validity of the Hendry–Ericsson findings through 2000.

Although Keynesian Nobel laureate Paul Krugman praised Friedman as a "great economist and a great man" after Friedman's death in 2006, and acknowledged his many, widely accepted contributions to empirical economics, Krugman had been, and remains, a prominent critic of Friedman. Krugman has written that "he slipped all too easily into claiming both that markets always work and that only markets work. It's extremely hard to find cases in which Friedman acknowledged the possibility that markets could go wrong, or that government intervention could serve a useful purpose." Others agree Friedman was not open enough to the possibility of market inefficiencies. Economist Noah Smith argues that while Friedman made many important contributions to economic theory not all of his ideas relating to macroeconomics have entirely held up over the years and that too few people are willing to challenge them.

Political scientist C.B. Macpherson disagreed with Friedman's historical assessment of economic freedom leading to political freedom, suggesting that political freedom actually gave way to economic freedom for property-owning elites. He also challenged the notion that markets efficiently allocated resources and rejected Friedman's definition of liberty. Friedman's positivist methodological approach to economics has also been critiqued and debated. Finnish economist Uskali Mäki has argued some of his assumptions were unrealistic and vague.

In her book "The Shock Doctrine", author and social activist Naomi Klein criticized Friedman's economic liberalism, identifying it with the principles that guided the economic restructuring that followed the military coups in countries such as Chile and Argentina. Based on their assessments of the extent to which what she describes as neoliberal policies contributed to income disparities and inequality, both Klein and Noam Chomsky have suggested that the primary role of what they describe as neoliberalism was as an ideological cover for capital accumulation by multinational corporations.

Because of his involvement with the Pinochet government, there were international protests when Friedman was awarded the Nobel Prize in 1976. Friedman was accused of supporting the military dictatorship in Chile because of the relation of economists of the University of Chicago to Pinochet, and a controversial seven-day trip he took to Chile during March 1975 (less than two years after the coup that ended with the death of President Salvador Allende). Friedman answered that he was never an adviser to the dictatorship, but only gave some lectures and seminars on inflation, and met with officials, including Augusto Pinochet, while in Chile.

Chilean economist Orlando Letelier asserted that Pinochet's dictatorship resorted to oppression because of popular opposition to Chicago School policies in Chile. After a 1991 speech on drug legalisation, Friedman answered a question on his involvement with the Pinochet regime, saying that he was never an advisor to Pinochet (also mentioned in his 1984 Iceland interview), but that a group of his students at the University of Chicago were involved in Chile's economic reforms. Friedman credited these reforms with high levels of economic growth and with the establishment of democracy that has subsequently occurred in Chile. In October 1988, after returning from a lecture tour of China during which he had met with Zhao Ziyang, General Secretary of the Communist Party of China, Friedman wrote to "The Stanford Daily" asking if he should anticipate a similar "avalanche of protests for having been willing to give advice to so evil a government? And if not, why not?"






</doc>
<doc id="19641" url="https://en.wikipedia.org/wiki?curid=19641" title="Mass media">
Mass media

Mass media refers to a diverse array of media technologies that reach a large audience via mass communication. The technologies through which this communication takes place include a variety of outlets.

Broadcast media transmit information electronically via media such as films, radio, recorded music, or television. Digital media comprises both Internet and mobile mass communication. Internet media comprise such services as email, social media sites, websites, and Internet-based radio and television. Many other mass media outlets have an additional presence on the web, by such means as linking to or running TV ads online, or distributing QR Codes in outdoor or print media to direct mobile users to a website. In this way, they can use the easy accessibility and outreach capabilities the Internet affords, as thereby easily broadcast information throughout many different regions of the world simultaneously and cost-efficiently. Outdoor media transmit information via such media as AR advertising; billboards; blimps; flying billboards (signs in tow of airplanes); placards or kiosks placed inside and outside buses, commercial buildings, shops, sports stadiums, subway cars, or trains; signs; or skywriting. Print media transmit information via physical objects, such as books, comics, magazines, newspapers, or pamphlets. Event organizing and public speaking can also be considered forms of mass media.

The organizations that control these technologies, such as movie studios, publishing companies, and radio and television stations, are also known as the mass media.

In the late 20th century, mass media could be classified into eight mass media industries: books, the Internet, magazines, movies, newspapers, radio, recordings, and television. The explosion of digital communication technology in the late 20th and early 21st centuries made prominent the question: what forms of media should be classified as "mass media"? For example, it is controversial whether to include cell phones, computer games (such as MMORPGs), and video games in the definition. In the 2000s, a classification called the "seven mass media" became popular. In order of introduction, they are:


Each mass medium has its own content types, creative artists, technicians, and business models. For example, the Internet includes blogs, podcasts, web sites, and various other technologies built atop the general distribution network. The sixth and seventh media, Internet and mobile phones, are often referred to collectively as digital media; and the fourth and fifth, radio and TV, as broadcast media. Some argue that video games have developed into a distinct mass form of media.

While a telephone is a two-way communication device, mass media communicates to a large group. In addition, the telephone has transformed into a cell phone which is equipped with Internet access. A question arises whether this makes cell phones a mass medium or simply a device used to access a mass medium (the Internet). There is currently a system by which marketers and advertisers are able to tap into satellites, and broadcast commercials and advertisements directly to cell phones, unsolicited by the phone's user. This transmission of mass advertising to millions of people is another form of mass communication.

Video games may also be evolving into a mass medium. Video games (for example massively multiplayer online role-playing games (MMORPGs), such as "RuneScape") provide a common gaming experience to millions of users across the globe and convey the same messages and ideologies to all their users. Users sometimes share the experience with one another by playing online. Excluding the Internet however, it is questionable whether players of video games are sharing a common experience when they play the game individually. It is possible to discuss in great detail the events of a video game with a friend one has never played with, because the experience is identical to each. The question, then, is whether this is a form of mass communication.

Five characteristics of mass communication have been identified by sociologist John Thompson of Cambridge University:

The term "mass media" is sometimes erroneously used as a synonym for "mainstream media". Mainstream media are distinguished from alternative media by their content and point of view. Alternative media are also "mass media" outlets in the sense that they use technology capable of reaching many people, even if the audience is often smaller than the mainstream.

In common usage, the term "mass" denotes not that a given number of individuals receives the products, but rather that the products are available in principle to a plurality of recipients.

The sequencing of content in a broadcast is called a schedule. With all technological endeavours a number of technical terms and slang have developed. Please see the list of broadcasting terms for a glossary of terms used.

Radio and television programs are distributed over frequency bands which are highly regulated in the United States. Such regulation includes determination of the width of the bands, range, licensing, types of receivers and transmitters used, and acceptable content.

Cable television programs are often broadcast simultaneously with radio and television programs, but have a more limited audience. By coding signals and requiring a cable converter box at individual recipients' locations, cable also enables subscription-based channels and pay-per-view services.

A broadcasting organisation may broadcast several programs simultaneously, through several channels (frequencies), for example BBC One and Two. On the other hand, two or more organisations may share a channel and each use it during a fixed part of the day, such as the Cartoon Network/Adult Swim. Digital radio and digital television may also transmit multiplexed programming, with several channels compressed into one ensemble.

When broadcasting is done via the Internet the term webcasting is often used. In 2004, a new phenomenon occurred when a number of technologies combined to produce podcasting. Podcasting is an asynchronous broadcast/narrowcast medium. Adam Curry and his associates, the "Podshow", are principal proponents of podcasting.

The term 'film' encompasses motion pictures as individual projects, as well as the field in general. The name comes from the photographic film (also called filmstock), historically the primary medium for recording and displaying motion pictures. Many other terms for film exist, such as "motion pictures" (or just "pictures" and "picture"), "the silver screen", "photoplays", "the cinema", "picture shows", "flicks", and most common, "movies".

Films are produced by recording people and objects with cameras, or by creating them using animation techniques or special effects. Films comprise a series of individual frames, but when these images are shown in rapid succession, an illusion of motion is created. Flickering between frames is not seen because of an effect known as persistence of vision, whereby the eye retains a visual image for a fraction of a second after the source has been removed. Also of relevance is what causes the perception of motion: a psychological effect identified as beta movement.

Film is considered by many to be an important art form; films entertain, educate, enlighten, and inspire audiences. Any film can become a worldwide attraction, especially with the addition of dubbing or subtitles that translate the film message. Films are also artifacts created by specific cultures, which reflect those cultures, and, in turn, affect them.

A video game is a computer-controlled game in which a video display, such as a monitor or television, is the primary feedback device. The term "computer game" also includes games which display only text (and which can, therefore, theoretically be played on a teletypewriter) or which use other methods, such as sound or vibration, as their primary feedback device, but there are very few new games in these categories. There always must also be some sort of input device, usually in the form of button/joystick combinations (on arcade games), a keyboard and mouse/trackball combination (computer games), a controller (console games), or a combination of any of the above. Also, more esoteric devices have been used for input, e.g., the player's motion. Usually there are rules and goals, but in more open-ended games the player may be free to do whatever they like within the confines of the virtual universe.

In common usage, an "arcade game" refers to a game designed to be played in an establishment in which patrons pay to play on a per-use basis. A "computer game" or "PC game" refers to a game that is played on a personal computer. A "Console game" refers to one that is played on a device specifically designed for the use of such, while interfacing with a standard television set. A "video game" (or "videogame") has evolved into a catchall phrase that encompasses the aforementioned along with any game made for any other device, including, but not limited to, advanced calculators, mobile phones, PDAs, etc.

Sound recording and reproduction is the electrical or mechanical re-creation or amplification of sound, often as music. This involves the use of audio equipment such as microphones, recording devices, and loudspeakers. From early beginnings with the invention of the phonograph using purely mechanical techniques, the field has advanced with the invention of electrical recording, the mass production of the 78 record, the magnetic wire recorder followed by the tape recorder, the vinyl LP record. The invention of the compact cassette in the 1960s, followed by Sony's Walkman, gave a major boost to the mass distribution of music recordings, and the invention of digital recording and the compact disc in 1983 brought massive improvements in ruggedness and quality. The most recent developments have been in digital audio players.

An album is a collection of related audio recordings, released together to the public, usually commercially.

The term record album originated from the fact that 78 RPM Phonograph disc records were kept together in a book resembling a photo album. The first collection of records to be called an "album" was Tchaikovsky's "Nutcracker Suite", release in April 1909 as a four-disc set by Odeon records. It retailed for 16 shillings – about £15 in modern currency.

A music video (also promo) is a short film or video that accompanies a complete piece of music, most commonly a song. Modern music videos were primarily made and used as a marketing device intended to promote the sale of music recordings. Although the origins of music videos go back much further, they came into their own in the 1980s, when Music Television's format was based on them. In the 1980s, the term "rock video" was often used to describe this form of entertainment, although the term has fallen into disuse.

Music videos can accommodate all styles of filmmaking, including animation, live action films, documentaries, and non-narrative, abstract film.

The Internet (also known simply as "the Net" or less precisely as "the Web") is a more interactive medium of mass media, and can be briefly described as "a network of networks". Specifically, it is the worldwide, publicly accessible network of interconnected computer networks that transmit data by packet switching using the standard Internet Protocol (IP). It consists of millions of smaller domestic, academic, business, and governmental networks, which together carry various information and services, such as email, online chat, file transfer, and the interlinked web pages and other documents of the World Wide Web.

Contrary to some common usage, the Internet and the World Wide Web are not synonymous: the Internet is the system of interconnected "computer networks", linked by copper wires, fiber-optic cables, wireless connections etc.; the Web is the contents, or the interconnected "documents", linked by hyperlinks and URLs. The World Wide Web is accessible through the Internet, along with many other services including e-mail, file sharing and others described below.

Toward the end of the 20th century, the advent of the World Wide Web marked the first era in which most individuals could have a means of exposure on a scale comparable to that of mass media. Anyone with a web site has the potential to address a global audience, although serving to high levels of web traffic is still relatively expensive. It is possible that the rise of peer-to-peer technologies may have begun the process of making the cost of bandwidth manageable. Although a vast amount of information, imagery, and commentary (i.e. "content") has been made available, it is often difficult to determine the authenticity and reliability of information contained in web pages (in many cases, self-published). The invention of the Internet has also allowed breaking news stories to reach around the globe within minutes. This rapid growth of instantaneous, decentralized communication is often deemed likely to change mass media and its relationship to society.

"Cross-media" means the idea of distributing the same message through different media channels. A similar idea is expressed in the news industry as "convergence". Many authors understand cross-media publishing to be the ability to publish in both print and on the web without manual conversion effort. An increasing number of wireless devices with mutually incompatible data and screen formats make it even more difficult to achieve the objective "create once, publish many".

The Internet is quickly becoming the center of mass media. Everything is becoming accessible via the internet. Rather than picking up a newspaper, or watching the 10 o'clock news, people can log onto the internet to get the news they want, when they want it. For example, many workers listen to the radio through the Internet while sitting at their desk.

Even the education system relies on the Internet. Teachers can contact the entire class by sending one e-mail. They may have web pages on which students can get another copy of the class outline or assignments. Some classes have class blogs in which students are required to post weekly, with students graded on their contributions.

Blogging, too, has become a pervasive form of media. A blog is a website, usually maintained by an individual, with regular entries of commentary, descriptions of events, or interactive media such as images or video. Entries are commonly displayed in reverse chronological order, with most recent posts shown on top. Many blogs provide commentary or news on a particular subject; others function as more personal online diaries. A typical blog combines text, images and other graphics, and links to other blogs, web pages, and related media. The ability for readers to leave comments in an interactive format is an important part of many blogs. Most blogs are primarily textual, although some focus on art (artlog), photographs (photoblog), sketchblog, videos (vlog), music (MP3 blog), audio (podcasting) are part of a wider network of social media. Microblogging is another type of blogging which consists of blogs with very short posts.

RSS is a format for syndicating news and the content of news-like sites, including major news sites like Wired, news-oriented community sites like Slashdot, and personal blogs. It is a family of Web feed formats used to publish frequently updated content such as blog entries, news headlines, and podcasts. An RSS document (which is called a "feed" or "web feed" or "channel") contains either a summary of content from an associated web site or the full text. RSS makes it possible for people to keep up with web sites in an automated manner that can be piped into special programs or filtered displays.

A podcast is a series of digital-media files which are distributed over the Internet using syndication feeds for playback on portable media players and computers. The term podcast, like broadcast, can refer either to the series of content itself or to the method by which it is syndicated; the latter is also called podcasting. The host or author of a podcast is often called a podcaster.

Mobile phones were introduced in Japan in 1979 but became a mass media only in 1998 when the first downloadable ringing tones were introduced in Finland. Soon most forms of media content were introduced on mobile phones, tablets and other portable devices, and today the total value of media consumed on mobile vastly exceeds that of internet content, and was worth over 31 billion dollars in 2007 (source Informa). The mobile media content includes over 8 billion dollars worth of mobile music (ringing tones, ringback tones, truetones, MP3 files, karaoke, music videos, music streaming services etc.); over 5 billion dollars worth of mobile gaming; and various news, entertainment and advertising services. In Japan mobile phone books are so popular that five of the ten best-selling printed books were originally released as mobile phone books.

Similar to the internet, mobile is also an interactive media, but has far wider reach, with 3.3 billion mobile phone users at the end of 2007 to 1.3 billion internet users (source ITU). Like email on the internet, the top application on mobile is also a personal messaging service, but SMS text messaging is used by over 2.4 billion people. Practically all internet services and applications exist or have similar cousins on mobile, from search to multiplayer games to virtual worlds to blogs. Mobile has several unique benefits which many mobile media pundits claim make mobile a more powerful media than either TV or the internet, starting with mobile being permanently carried and always connected. Mobile has the best audience accuracy and is the only mass media with a built-in payment channel available to every user without any credit cards or PayPal accounts or even an age limit. Mobile is often called the 7th Mass Medium and either the fourth screen (if counting cinema, TV and PC screens) or the third screen (counting only TV and PC).

A magazine is a periodical publication containing a variety of articles, generally financed by advertising or purchase by readers.

Magazines are typically published weekly, biweekly, monthly, bimonthly or quarterly, with a date on the cover that is in advance of the date it is actually published. They are often printed in color on coated paper, and are bound with a soft cover.

Magazines fall into two broad categories: consumer magazines and business magazines. In practice, magazines are a subset of , distinct from those periodicals produced by scientific, artistic, academic or special interest publishers which are subscription-only, more expensive, narrowly limited in circulation, and often have little or no advertising.

Magazines can be classified as:

A newspaper is a publication containing news and information and advertising, usually printed on low-cost paper called newsprint. It may be general or special interest, most often published daily or weekly. The most important function of newspapers is to inform the public of significant events. Local newspapers inform local communities and include advertisements from local businesses and services, while national newspapers tend to focus on a theme, which can be exampled with "The Wall Street Journal" as they offer news on finance and business related-topics. The first printed newspaper was published in 1605, and the form has thrived even in the face of competition from technologies such as radio and television. Recent developments on the Internet are posing major threats to its business model, however. Paid circulation is declining in most countries, and advertising revenue, which makes up the bulk of a newspaper's income, is shifting from print to online; some commentators, nevertheless, point out that historically new media such as radio and television did not entirely supplant existing.

The internet has challenged the press as an alternative source of information and opinion but has also provided a new platform for newspaper organizations to reach new audiences. According to the World Trends Report, between 2012 and 2016, print newspaper circulation continued to fall in almost all regions, with the exception of Asia and the Pacific, where the dramatic increase in sales in a few select countries has offset falls in historically strong Asian markets such as Japan and the Republic of Korea. Most notably, between 2012 and 2016, India's print circulation grew by 89 per cent.

Outdoor media is a form of mass media which comprises billboards, signs, placards placed inside and outside commercial buildings/objects like shops/buses, flying billboards (signs in tow of airplanes), blimps, skywriting, AR Advertising. Many commercial advertisers use this form of mass media when advertising in sports stadiums. Tobacco and alcohol manufacturers used billboards and other outdoor media extensively. However, in 1998, the Master Settlement Agreement between the US and the tobacco industries prohibited the billboard advertising of cigarettes. In a 1994 Chicago-based study, Diana Hackbarth and her colleagues revealed how tobacco- and alcohol-based billboards were concentrated in poor neighbourhoods. In other urban centers, alcohol and tobacco billboards were much more concentrated in African-American neighborhoods than in white neighborhoods.

Mass media encompasses much more than just news, although it is sometimes misunderstood in this way. It can be used for various purposes:

Journalism is the discipline of collecting, analyzing, verifying and presenting information regarding current events, trends, issues and people. Those who practice journalism are known as journalists.

News-oriented journalism is sometimes described as the "first rough draft of history" (attributed to Phil Graham), because journalists often record important events, producing news articles on short deadlines. While under pressure to be first with their stories, news media organizations usually edit and proofread their reports prior to publication, adhering to each organization's standards of accuracy, quality and style. Many news organizations claim proud traditions of holding government officials and institutions accountable to the public, while media critics have raised questions about holding the press itself accountable to the standards of professional journalism.

Public relations is the art and science of managing communication between an organization and its key publics to build, manage and sustain its positive image. Examples include:

Publishing is the industry concerned with the production of literature or information – the activity of making information available for public view. In some cases, authors may be their own publishers.

Traditionally, the term refers to the distribution of printed works such as books and newspapers. With the advent of digital information systems and the Internet, the scope of publishing has expanded to include websites, blogs, and the like.

As a business, publishing includes the development, marketing, production, and distribution of newspapers, magazines, books, literary works, musical works, software, other works dealing with information.

Publication is also important as a legal concept; (1) as the process of giving formal notice to the world of a significant intention, for example, to marry or enter bankruptcy, and; (2) as the essential precondition of being able to claim defamation; that is, the alleged libel must have been published.

A software publisher is a publishing company in the software industry between the developer and the distributor. In some companies, two or all three of these roles may be combined (and indeed, may reside in a single person, especially in the case of shareware).

Software publishers often license software from developers with specific limitations, such as a time limit or geographical region. The terms of licensing vary enormously, and are typically secret.

Developers may use publishers to reach larger or foreign markets, or to avoid focussing on marketing. Or publishers may use developers to create software to meet a market need that the publisher has identified.

A YouTuber is anyone who has made their fame from creating and promoting videos on the public video-sharing site, YouTube. Many YouTube celebrities have made a profession from their site through sponsorships, advertisements, product placement, and network support.

The history of mass media can be traced back to the days when dramas were performed in various ancient cultures. This was the first time when a form of media was "broadcast" to a wider audience. The first dated printed book known is the "Diamond Sutra", printed in China in 868 AD, although it is clear that books were printed earlier. Movable clay type was invented in 1041 in China. However, due to the slow spread of literacy to the masses in China, and the relatively high cost of paper there, the earliest printed mass-medium was probably European popular prints from about 1400. Although these were produced in huge numbers, very few early examples survive, and even most known to be printed before about 1600 have not survived. The term "mass media" was coined with the creation of print media, which is notable for being the first example of mass media, as we use the term today. This form of media started in Europe in the Middle Ages.

Johannes Gutenberg's invention of the printing press allowed the mass production of books to sweep the nation. He printed the first book, a Latin Bible, on a printing press with movable type in 1453. The invention of the printing press gave rise to some of the first forms of mass communication, by enabling the publication of books and newspapers on a scale much larger than was previously possible. The invention also transformed the way the world received printed materials, although books remained too expensive really to be called a mass-medium for at least a century after that. Newspapers developed from about 1612, with the first example in English in 1620; but they took until the 19th century to reach a mass-audience directly. The first high-circulation newspapers arose in London in the early 1800s, such as The Times, and were made possible by the invention of high-speed rotary steam printing presses, and railroads which allowed large-scale distribution over wide geographical areas. The increase in circulation, however, led to a decline in feedback and interactivity from the readership, making newspapers a more one-way medium.

The phrase "the media" began to be used in the 1920s. The notion of "mass media" was generally restricted to print media up until the post-Second World War, when radio, television and video were introduced. The audio-visual facilities became very popular, because they provided both information and entertainment, because the colour and sound engaged the viewers/listeners and because it was easier for the general public to passively watch TV or listen to the radio than to actively read. In recent times, the Internet become the latest and most popular mass medium. Information has become readily available through websites, and easily accessible through search engines. One can do many activities at the same time, such as playing games, listening to music, and social networking, irrespective of location. Whilst other forms of mass media are restricted in the type of information they can offer, the internet comprises a large percentage of the sum of human knowledge through such things as Google Books. Modern day mass media includes the internet, mobile phones, blogs, podcasts and RSS feeds.

During the 20th century, the growth of mass media was driven by technology, including that which allowed much duplication of material. Physical duplication technologies such as printing, record pressing and film duplication allowed the duplication of books, newspapers and movies at low prices to huge audiences. Radio and television allowed the electronic duplication of information for the first time. Mass media had the economics of linear replication: a single work could make money. An example of Riel and Neil's theory. proportional to the number of copies sold, and as volumes went up, unit costs went down, increasing profit margins further. Vast fortunes were to be made in mass media. In a democratic society, the media can serve the electorate about issues regarding government and corporate entities (see Media influence). Some consider the concentration of media ownership to be a threat to democracy.

Between 1985 and 2018 about 76,720 deals have been announced in the Media industry. This sums up to an overall value of around 5,634 bil USD. There have been three major waves of M&A in the Mass Media Sector (2000, 2007 and 2015), while the most active year in terms of numbers was 2007 with around 3,808 deals. The U.S. is the most prominent country in Media M&A with 41 of the top 50 deals having an acquiror from the United States.

The largest deal in history was the acquisition of Time Warner by America Online Inc for 164,746.86 mil USD.

Limited-effects theory, originally tested in the 1940s and 1950s, considers that because people usually choose what media to interact with based on what they already believe, media exerts a negligible influence. Class-dominant theory argues that the media reflects and projects the view of a minority elite, which controls it. Culturalist theory, which was developed in the 1980s and 1990s, combines the other two theories and claims that people interact with media to create their own meanings out of the images and messages they receive. This theory states that audience members play an active, rather than passive role in relation to mass media.

There is an article that argues 90 percent of all mass media including radio broadcast networks and programing, video news, sports entertainment, and others are owned by 6 major companies (GE, News-Corp, Disney, Viacom, Time Warner, and CBS). According to Morris Creative Group, these six companies made over 200 billion dollars in revenue in 2010. More diversity is brewing among many companies, but they have recently merged to form an elite which have the power to control the narrative of stories and alter people's beliefs. In the new media-driven age we live in, marketing has more value than ever before because of the various ways it can be implemented. Advertisements can convince citizens to purchase a specific product or have consumers avoid a particular product. The definition of what is acceptable by society can be heavily dictated by the media in regards to the amount of attention it receives.

The documentary "Super Size Me" describes how companies like McDonald's have been sued in the past, the plaintiffs claiming that it was the fault of their liminal and subliminal advertising that "forced" them to purchase the product. The Barbie and Ken dolls of the 1950s are sometimes cited as the main cause for the obsession in modern-day society for women to be skinny and men to be buff. After the attacks of 9/11, the media gave extensive coverage of the event and exposed Osama Bin Laden's guilt for the attack, information they were told by the authorities. This shaped the public opinion to support the war on terrorism, and later, the war on Iraq. A main concern is that due to this extreme power of the mass media, portraying inaccurate information could lead to an immense public concern. In his book The Commercialization of American Culture, Matthew P. McAllister says that "a well-developed media system, informing and teaching its citizens, helps democracy move toward its ideal state."

In 1997, J. R. Finnegan Jr. and K. Viswanath identified three main effects or functions of mass media:

Since the 1950s, when cinema, radio and TV began to be the primary or the only source of information for a larger and larger percentage of the population, these media began to be considered as central instruments of mass control. Up to the point that it emerged the idea that when a country has reached a high level of industrialization, the country itself "belongs to the person who controls communications."

Mass media play a significant role in shaping public perceptions on a variety of important issues, both through the information that is dispensed through them, and through the interpretations they place upon this information. They also play a large role in shaping modern culture, by selecting and portraying a particular set of beliefs, values, and traditions (an entire way of life), as reality. That is, by portraying a certain interpretation of reality, they shape reality to be more in line with that interpretation. Mass media also play a crucial role in the spread of civil unrest activities such as anti-government demonstrations, riots, and general strikes. That is, the use of radio and television receivers has made the unrest influence among cities not only by the geographic location of cities, but also by proximity within the mass media distribution networks.

Mass media sources, through theories like framing and agenda-setting, can affect the scope of a story as particular facts and information are highlighted (Media influence). This can directly correlate with how individuals may perceive certain groups of people, as the only media coverage a person receives can be very limited and may not reflect the whole story or situation; stories are often covered to reflect a particular perspective to target a specific demographic.

According to Stephen Balkaran, an Instructor of Political Science and African American Studies at Central Connecticut State University, mass media has played a large role in the way white Americans perceive African-Americans. The media focus on African-American in the contexts of crime, drug use, gang violence, and other forms of anti-social behavior has resulted in a distorted and harmful public perception of African-Americans.

In his 1999 article "Mass Media and Racism", Balkaran states: "The media has played a key role in perpetuating the effects of this historical oppression and in contributing to African-Americans' continuing status as second-class citizens". This has resulted in an uncertainty among white Americans as to what the genuine nature of African-Americans really is. Despite the resulting racial divide, the fact that these people are undeniably American has "raised doubts about the white man's value system". This means that there is a somewhat "troubling suspicion" among some Americans that their white America is tainted by the black influence. Mass media as well as propaganda tend to reinforce or introduce stereotypes to the general public.

Lack of local or specific topical focus is a common criticism of mass media. A mass news media outlet is often forced to cover national and international news due to it having to cater for and be relevant for a wide demographic. As such, it has to skip over many interesting or important local stories because they simply do not interest the large majority of their viewers. An example given by the website WiseGeek is that "the residents of a community might view their fight against development as critical, but the story would only attract the attention of the mass media if the fight became controversial or if precedents of some form were set".

The term "mass" suggests that the recipients of media products constitute a vast sea of passive, undifferentiated individuals. This is an image associated with some earlier critiques of "mass culture" and mass society which generally assumed that the development of mass communication has had a largely negative impact on modern social life, creating a kind of bland and homogeneous culture which entertains individuals without challenging them. However, interactive digital media have also been seen to challenge the read-only paradigm of earlier broadcast media.

Whilst some refer to the mass media as "opiate of the masses", others argue that is a vital aspect of human societies. By understanding mass media, one is then able to analyse and find a deeper understanding of one's population and culture. This valuable and powerful ability is one reason why the field of media studies is popular. As WiseGeek says, "watching, reading, and interacting with a nation's mass media can provide clues into how people think, especially if a diverse assortment of mass media sources are perused".

Since the 1950s, in the countries that have reached a high level of industrialization, the mass media of cinema, radio and TV have a key role in political power.

Contemporary research demonstrates an increasing level of concentration of media ownership, with many media industries already highly concentrated and dominated by a small number of firms.

When the study of mass media began the media was compiled of only mass media which is a very different media system than the social media empire of the 21st-century experiences. With this in mind, there are critiques that mass media no longer exists, or at least that it doesn't exist in the same form as it once did. This original form of mass media put filters on what the general public would be exposed to in regards to "news" something that is harder to do in a society of social media.

Theorist Lance Bennett explains that excluding a few major events in recent history, it is uncommon for a group big enough to be labeled a mass, to be watching the same news via the same medium of mass production. Bennett's critique of 21st Century mass media argues that today it is more common for a group of people to be receiving different news stories, from completely different sources, and thus, mass media has been re-invented. As discussed above, filters would have been applied to original mass medias when the journalists decided what would or wouldn't be printed.

Social Media is a large contributor to the change from mass media to a new paradigm because through social media what is mass communication and what is interpersonal communication is confused. Interpersonal/niche communication is an exchange of information and information in a specific genre. In this form of communication, smaller groups of people are consuming news/information/opinions. In contrast, mass media in its original form is not restricted by genre and it is being consumed by the masses.






</doc>
<doc id="19643" url="https://en.wikipedia.org/wiki?curid=19643" title="Mahabharata">
Mahabharata

The Mahābhārata (, ; , "", ) is one of the two major Sanskrit epics of ancient India, the other being the "Rāmāyaṇa". It narrates the struggle between two groups of cousins in the Kurukshetra War and the fates of the Kaurava and the Pāṇḍava princes and their successors.

It also contains philosophical and devotional material, such as a discussion of the four "goals of life" or "puruṣārtha" (12.161). Among the principal works and stories in the "Mahābhārata" are the "Bhagavad Gita", the story of Damayanti, story of Savitri and Satyavan, story of Kacha and Devyani, the story of Ṛṣyasringa and an abbreviated version of the "Rāmāyaṇa", often considered as works in their own right.

Traditionally, the authorship of the "Mahābhārata" is attributed to Vyāsa. There have been many attempts to unravel its historical growth and compositional layers. The bulk of the "Mahābhārata" was probably compiled between the 3rd century BCE and the 3rd century CE, with the oldest preserved parts not much older than around 400 BCE. The original events related by the epic probably fall between the 9th and 8th centuries BCE. The text probably reached its final form by the early Gupta period (c. 4th century CE).

The "Mahābhārata" is the longest epic poem known and has been described as "the longest poem ever written". Its longest version consists of over 100,000 "śloka" or over 200,000 individual verse lines (each shloka is a couplet), and long prose passages. At about 1.8 million words in total, the "Mahābhārata" is roughly ten times the length of the "Iliad" and the "Odyssey" combined, or about four times the length of the "Rāmāyaṇa". W. J. Johnson has compared the importance of the "Mahābhārata" in the context of world civilization to that of the Bible, the works of William Shakespeare, the works of Homer, Greek drama, or the Quran. Within the Indian tradition it is sometimes called the fifth Veda.

The epic is traditionally ascribed to the sage Vyāsa, who is also a major character in the epic. Vyāsa described it as being "itihāsa" (Sanskrit: इतिहास, meaning "history"). He also describes the Guru-shishya parampara, which traces all great teachers and their students of the Vedic times.

The first section of the Mahābhārata states that it was Ganesha who wrote down the text to Vyasa's dictation.

The epic employs the story within a story structure, otherwise known as frametales, popular in many Indian religious and non-religious works. It is first recited at "Takshashila" by the sage Vaiśampāyana, a disciple of Vyāsa, to the King Janamejaya who was the great-grandson of the Pāṇḍava prince Arjuna. The story is then recited again by a professional storyteller named Ugraśrava Sauti, many years later, to an assemblage of sages performing the 12-year sacrifice for the king Saunaka Kulapati in the Naimiśa Forest.

The text was described by some early 20th-century Indologists as unstructured and chaotic. Hermann Oldenberg supposed that the original poem must once have carried an immense "tragic force" but dismissed the full text as a "horrible chaos." Moritz Winternitz ("Geschichte der indischen Literatur" 1909) considered that "only unpoetical theologists and clumsy scribes" could have lumped the parts of disparate origin into an unordered whole.

Research on the Mahābhārata has put an enormous effort into recognizing and dating layers within the text. Some elements of the present Mahābhārata can be traced back to Vedic times. The background to the Mahābhārata suggests the origin of the epic occurs "after the very early Vedic period" and before "the first Indian 'empire' was to rise in the third century B.C." That this is "a date not too far removed from the 8th or 9th century B.C." is likely. Mahābhārata started as an orally-transmitted tale of the charioteer bards. It is generally agreed that "Unlike the Vedas, which have to be preserved letter-perfect, the epic was a popular work whose reciters would inevitably conform to changes in language and style," so the earliest 'surviving' components of this dynamic text are believed to be no older than the earliest 'external' references we have to the epic, which may include an allusion in Panini's 4th century BCE grammar Aṣṭādhyāyī 4:2:56. It is estimated that the Sanskrit text probably reached something of a "final form" by the early Gupta period (about the 4th century CE). Vishnu Sukthankar, editor of the first great critical edition of the "Mahābhārata", commented: "It is useless to think of reconstructing a fluid text in a literally original shape, on the basis of an archetype and a "stemma codicum". What then is possible? Our objective can only be to reconstruct "the oldest form of the text which it is possible to reach" on the basis of the manuscript material available." That manuscript evidence is somewhat late, given its material composition and the climate of India, but it is very extensive.

The Mahābhārata itself (1.1.61) distinguishes a core portion of 24,000 verses: the "Bhārata" proper, as opposed to additional secondary material, while the "Aśvalāyana Gṛhyasūtra" (3.4.4) makes a similar distinction. At least three redactions of the text are commonly recognized: "Jaya" (Victory) with 8,800 verses attributed to Vyāsa, "Bhārata" with 24,000 verses as recited by Vaiśampāyana, and finally the Mahābhārata as recited by Ugraśrava Sauti with over 100,000 verses. However, some scholars, such as John Brockington, argue that "Jaya" and "Bharata" refer to the same text, and ascribe the theory of "Jaya" with 8,800 verses to a misreading of a verse in Ā"diparvan" (1.1.81).
The redaction of this large body of text was carried out after formal principles, emphasizing the numbers 18 and 12. The addition of the latest parts may be dated by the absence of the "Anuśāsana-parva" and the "Virāta parva" from the "Spitzer manuscript". The oldest surviving Sanskrit text dates to the Kushan Period (200 CE).

According to what one character says at Mbh. 1.1.50, there were three versions of the epic, beginning with "Manu" (1.1.27), "Astika" (1.3, sub-parva 5) or "Vasu" (1.57), respectively. These versions would correspond to the addition of one and then another 'frame' settings of dialogues. The "Vasu" version would omit the frame settings and begin with the account of the birth of Vyasa. The "astika" version would add the "sarpasattra" and "aśvamedha" material from Brahmanical literature, introduce the name "Mahābhārata", and identify Vyāsa as the work's author. The redactors of these additions were probably Pāñcarātrin scholars who according to Oberlies (1998) likely retained control over the text until its final redaction. Mention of the Huna in the "Bhīṣma-parva" however appears to imply that this parva may have been edited around the 4th century.
The Ādi-parva includes the snake sacrifice ("sarpasattra") of Janamejaya, explaining its motivation, detailing why all snakes in existence were intended to be destroyed, and why in spite of this, there are still snakes in existence. This "sarpasattra" material was often considered an independent tale added to a version of the Mahābhārata by "thematic attraction" (Minkowski 1991), and considered to have a particularly close connection to Vedic (Brahmana) literature. The Pañcavimśa Brahmana (at 25.15.3) enumerates the officiant priests of a "sarpasattra" among whom the names Dhṛtarāṣtra and Janamejaya, two main characters of the "Mahābhārata"'s "sarpasattra", as well as Takṣaka, the name of a snake in the "Mahābhārata", occur.

The "Suparṇākhyāna", a late Vedic period poem considered to be among the "earliest traces of epic poetry in India," is an older, shorter precursor to the expanded legend of Garuda that is included in the "Āstīka Parva", within the "Ādi Parva" of the "Mahābhārata".

The earliest known references to the Mahābhārata and its core "Bhārata" date to the "Aṣṭādhyāyī" (sutra 6.2.38) of Pāṇini ("fl." 4th century BCE) and in the "Aśvalāyana Gṛhyasūtra" (3.4.4). This may mean the core 24,000 verses, known as the "Bhārata", as well as an early version of the extended "Mahābhārata", were composed by the 4th century BCE. A report by the Greek writer Dio Chrysostom (c. 40 – c. 120 CE) about Homer's poetry being sung even in India seems to imply that the "Iliad" had been translated into Sanskrit. However, Indian scholars have, in general, taken this as evidence for the existence of a Mahābhārata at this date, whose episodes Dio or his sources identify with the story of the "Iliad".

Several stories within the Mahābhārata took on separate identities of their own in Classical Sanskrit literature. For instance, Abhijñānaśākuntala by the renowned Sanskrit poet Kālidāsa (c. 400 CE), believed to have lived in the era of the Gupta dynasty, is based on a story that is the precursor to the "Mahābhārata". Urubhaṅga, a Sanskrit play written by Bhāsa who is believed to have lived before Kālidāsa, is based on the slaying of Duryodhana by the splitting of his thighs by Bhīma.

The copper-plate inscription of the Maharaja Sharvanatha (533–534 CE) from Khoh (Satna District, Madhya Pradesh) describes the Mahābhārata as a "collection of 100,000 verses" ("śata-sahasri saṃhitā").

The division into 18 parvas is as follows:
The historicity of the Kurukshetra War is unclear. Many historians estimate the date of the Kurukshetra war to Iron Age India of the 10th century BCE. The setting of the epic has a historical precedent in Iron Age (Vedic) India, where the Kuru kingdom was the center of political power during roughly 1200 to 800 BCE. A dynastic conflict of the period could have been the inspiration for the "Jaya", the foundation on which the Mahābhārata corpus was built, with a climactic battle eventually coming to be viewed as an epochal event.

Puranic literature presents genealogical lists associated with the Mahābhārata narrative. The evidence of the Puranas is of two kinds. Of the first kind, there is the direct statement that there were 1015 (or 1050) years between the birth of Parikshit (Arjuna's grandson) and the accession of Mahapadma Nanda (400-329 BCE), which would yield an estimate of about 1400 BCE for the Bharata battle. However, this would imply improbably long reigns on average for the kings listed in the genealogies.
Of the second kind are analyses of parallel genealogies in the Puranas between the times of Adhisimakrishna (Parikshit's great-grandson) and Mahapadma Nanda. Pargiter accordingly estimated 26 generations by averaging 10 different dynastic lists and, assuming 18 years for the average duration of a reign, arrived at an estimate of 850 BCE for Adhisimakrishna, and thus approximately 950 BCE for the Bharata battle.
B. B. Lal used the same approach with a more conservative assumption of the average reign to estimate a date of 836 BCE, and correlated this with archaeological evidence from Painted Grey Ware (PGW) sites, the association being strong between PGW artifacts and places mentioned in the epic. John Keay confirms this and also gives 950 BCE for the Bharata battle.

Attempts to date the events using methods of archaeoastronomy have produced, depending on which passages are chosen and how they are interpreted, estimates ranging from the late 4th to the mid-2nd millennium BCE. The late 4th-millennium date has a precedent in the calculation of the Kaliyuga epoch, based on planetary conjunctions, by Aryabhata (6th century). Aryabhata's date of 18 February 3102 BCE for Mahābhārata war has become widespread in Indian tradition. Some sources mark this as the disappearance of Krishna from earth. The Aihole inscription of Pulikeshi II, dated to Saka 556 = 634 CE, claims that 3735 years have elapsed since the Bharata battle, putting the date of Mahābhārata war at 3137 BCE.
Another traditional school of astronomers and historians, represented by Vriddha-Garga, Varahamihira (author of the "Brhatsamhita") and Kalhana (author of the "Rajatarangini"), place the Bharata war 653 years after the Kaliyuga epoch, corresponding to 2449 BCE.

The core story of the work is that of a dynastic struggle for the throne of Hastinapura, the kingdom ruled by the Kuru clan. The two collateral branches of the family that participate in the struggle are the Kaurava and the Pandava. Although the Kaurava is the senior branch of the family, Duryodhana, the eldest Kaurava, is younger than Yudhishthira, the eldest Pandava. Both Duryodhana and Yudhishthira claim to be first in line to inherit the throne.

The struggle culminates in the great battle of Kurukshetra, in which the Pandavas are ultimately victorious. The battle produces complex conflicts of kinship and friendship, instances of family loyalty and duty taking precedence over what is right, as well as the converse.

The Mahābhārata itself ends with the death of Krishna, and the subsequent end of his dynasty and ascent of the Pandava brothers to heaven. It also marks the beginning of the Hindu age of Kali Yuga, the fourth and final age of humankind, in which great values and noble ideas have crumbled, and people are heading towards the complete dissolution of right action, morality and virtue.

King Janamejaya's ancestor Shantanu, the king of Hastinapura, has a short-lived marriage with the goddess Ganga and has a son, Devavrata (later to be called Bhishma, a great warrior), who becomes the heir apparent. Many years later, when King Shantanu goes hunting, he sees Satyavati, the daughter of the chief of fisherman, and asks her father for her hand. Her father refuses to consent to the marriage unless Shantanu promises to make any future son of Satyavati the king upon his death. To resolve his father's dilemma, Devavrata agrees to relinquish his right to the throne. As the fisherman is not sure about the prince's children honouring the promise, Devavrata also takes a vow of lifelong celibacy to guarantee his father's promise.

Shantanu has two sons by Satyavati, Chitrāngada and Vichitravirya. Upon Shantanu's death, Chitrangada becomes king. He lives a very short uneventful life and dies. Vichitravirya, the younger son, rules Hastinapura. Meanwhile, the King of Kāśī arranges a swayamvara for his three daughters, neglecting to invite the royal family of Hastinapur. In order to arrange the marriage of young Vichitravirya, Bhishma attends the swayamvara of the three princesses Amba, Ambika and Ambalika, uninvited, and proceeds to abduct them. Ambika and Ambalika consent to be married to Vichitravirya.

The oldest princess Amba, however, informs Bhishma that she wishes to marry king of Shalva whom Bhishma defeated at their swayamvara. Bhishma lets her leave to marry king of Shalva, but Shalva refuses to marry her, still smarting at his humiliation at the hands of Bhishma. Amba then returns to marry Bhishma but he refuses due to his vow of celibacy. Amba becomes enraged and becomes Bhishma's bitter enemy, holding him responsible for her plight. Later she is reborn to King Drupada as Shikhandi (or Shikhandini) and causes Bhishma's fall, with the help of Arjuna, in the battle of Kurukshetra.

When Vichitravirya dies young without any heirs, Satyavati asks her first son Vyasa to father children with the widows. The eldest, Ambika, shuts her eyes when she sees him, and so her son Dhritarashtra is born blind. Ambalika turns pale and bloodless upon seeing him, and thus her son Pandu is born pale and unhealthy (the term Pandu may also mean 'jaundiced'). Due to the physical challenges of the first two children, Satyavati asks Vyasa to try once again. However, Ambika and Ambalika send their maid instead, to Vyasa's room. Vyasa fathers a third son, Vidura, by the maid. He is born healthy and grows up to be one of the wisest characters in the "Mahabharata". He serves as Prime Minister (Mahamantri or Mahatma) to King Pandu and King Dhritarashtra.

When the princes grow up, Dhritarashtra is about to be crowned king by Bhishma when Vidura intervenes and uses his knowledge of politics to assert that a blind person cannot be king. This is because a blind man cannot control and protect his subjects. The throne is then given to Pandu because of Dhritarashtra's blindness. Pandu marries twice, to Kunti and Madri. Dhritarashtra marries Gandhari, a princess from Gandhara, who blindfolds herself for the rest of her life so that she may feel the pain that her husband feels. Her brother Shakuni is enraged by this and vows to take revenge on the Kuru family. One day, when Pandu is relaxing in the forest, he hears the sound of a wild animal. He shoots an arrow in the direction of the sound. However the arrow hits the sage Kindama, who was in engaged in a sexual act in the guise of a deer. He curses Pandu that if he engages in a sexual act, he will die. Pandu then retires to the forest along with his two wives, and his brother Dhritarashtra rules thereafter, despite his blindness.

Pandu's older queen Kunti, however, had been given a boon by Sage Durvasa that she could invoke any god using a special mantra. Kunti uses this boon to ask Dharma the god of justice, Vayu the god of the wind, and Indra the lord of the heavens for sons. She gives birth to three sons, Yudhishthira, Bhima, and Arjuna, through these gods. Kunti shares her mantra with the younger queen Madri, who bears the twins Nakula and Sahadeva through the Ashwini twins. However, Pandu and Madri indulge in sex, and Pandu dies. Madri commits Sati out of remorse. Kunti raises the five brothers, who are from then on usually referred to as the Pandava brothers.

Dhritarashtra has a hundred sons through Gandhari, all born after the birth of Yudhishthira. These are the Kaurava brothers, the eldest being Duryodhana, and the second Dushasana. Other Kaurava brothers were Vikarna and Sukarna. The rivalry and enmity between them and the Pandava brothers, from their youth and into manhood, leads to the Kurukshetra war.

After the deaths of their mother (Madri) and father (Pandu), the Pandavas and their mother Kunti return to the palace of Hastinapur. Yudhishthira is made Crown Prince by Dhritarashtra, under considerable pressure from his courtiers. Dhritarashtra wanted his own son Duryodhana to become king and lets his ambition get in the way of preserving justice.

Shakuni, Duryodhana and Dushasana plot to get rid of the Pandavas. Shakuni calls the architect Purochana to build a palace out of flammable materials like lac and ghee. He then arranges for the Pandavas and the Queen Mother Kunti to stay there, with the intention of setting it alight. However, the Pandavas are warned by their wise uncle, Vidura, who sends them a miner to dig a tunnel. They are able to escape to safety and go into hiding. During this time Bhima marries a demoness Hidimbi and has a son Ghatotkacha. Back in Hastinapur, the Pandavas and Kunti are presumed dead.

Whilst they were in hiding the Pandavas learn of a swayamvara which is taking place for the hand of the Pāñcāla princess Draupadī. The Pandavas disguised as Brahmins come to witness the event. Meanwhile, Krishna who has already befriended Draupadi, tells her to look out for Arjuna (though now believed to be dead). The task was to string a mighty steel bow and shoot a target on the ceiling, which was the eye of a moving artificial fish, while looking at its reflection in oil below. In popular versions, after all the princes fail, many being unable to lift the bow, Karna proceeds to the attempt but is interrupted by Draupadi who refuses to marry a suta (this has been excised from the Critical Edition of Mahabharata as later interpolation). After this the swayamvara is opened to the Brahmins leading Arjuna to win the contest and marry Draupadi. The Pandavas return home and inform their meditating mother that Arjuna has won a competition and to look at what they have brought back. Without looking, Kunti asks them to share whatever Arjuna has won amongst themselves, thinking it to be alms. Thus, Draupadi ends up being the wife of all five brothers.

After the wedding, the Pandava brothers are invited back to Hastinapura. The Kuru family elders and relatives negotiate and broker a split of the kingdom, with the Pandavas obtaining and demanding only a wild forest inhabited by Takshaka, the king of snakes and his family. Through hard work the Pandavas are able to build a new glorious capital for the territory at Indraprastha.

Shortly after this, Arjuna elopes with and then marries Krishna's sister, Subhadra. Yudhishthira wishes to establish his position as king; he seeks Krishna's advice. Krishna advises him, and after due preparation and the elimination of some opposition, Yudhishthira carries out the "rājasūya yagna" ceremony; he is thus recognised as pre-eminent among kings.

The Pandavas have a new palace built for them, by Maya the Danava. They invite their Kaurava cousins to Indraprastha. Duryodhana walks round the palace, and mistakes a glossy floor for water, and will not step in. After being told of his error, he then sees a pond, and assumes it is not water and falls in. Bhima, Arjun, the twins and the servants laugh at him. In popular adaptations, this insult is wrongly attributed to Draupadi, even though in the Sanskrit epic, it was the Pandavas (except Yudhishthira) who had insulted Duryodhana. Enraged by the insult, and jealous at seeing the wealth of the Pandavas, Duryodhana decides to host a dice-game at Shakuni's suggestion.

Shakuni, Duryodhana's uncle, now arranges a dice game, playing against Yudhishthira with loaded dice. In the dice game, Yudhishthira loses all his wealth, then his kingdom. Yudhishthira then gambles his brothers, himself, and finally his wife into servitude. The jubilant Kauravas insult the Pandavas in their helpless state and even try to disrobe Draupadi in front of the entire court, but Draupadi's disrobe is prevented by Krishna, who miraculously make her dress endless, therefore it couldn't be removed.

Dhritarashtra, Bhishma, and the other elders are aghast at the situation, but Duryodhana is adamant that there is no place for two crown princes in Hastinapura. Against his wishes Dhritarashtra orders for another dice game. The Pandavas are required to go into exile for 12 years, and in the 13th year, they must remain hidden. If they are discovered by the Kauravas in the 13th year of their exile, then they will be forced into exile for another 12 years.

The Pandavas spend thirteen years in exile; many adventures occur during this time. The Pandavas acquire many divine weapons, given by gods, during this period. They also prepare alliances for a possible future conflict. They spend their final year in disguise in the court of king Virata, and they are discovered just after the end of the year.

At the end of their exile, they try to negotiate a return to Indraprastha with Krishna as their emissary. However, this negotiation fails, because Duryodhana objected that they were discovered in the 13th year of their exile and the return of their kingdom was not agreed. Then the Pandavas fought the Kauravas, claiming their rights over Indraprastha.

The two sides summon vast armies to their help and line up at Kurukshetra for a war. The kingdoms of Panchala, Dwaraka, Kasi, Kekaya, Magadha, Matsya, Chedi, Pandyas, Telinga, and the Yadus of Mathura and some other clans like the Parama Kambojas were allied with the Pandavas. The allies of the Kauravas included the kings of Pragjyotisha, Anga, Kekaya, Sindhudesa (including Sindhus, Sauviras and Sivis), Mahishmati, Avanti in Madhyadesa, Madra, Gandhara, Bahlika people, Kambojas and many others. Before war being declared, Balarama had expressed his unhappiness at the developing conflict and leaves to go on pilgrimage; thus he does not take part in the battle itself. Krishna takes part in a non-combatant role, as charioteer for Arjuna.

Before the battle, Arjuna, noticing that the opposing army includes his own cousins and relatives, including his grandfather Bhishma and his teacher Drona, has grave doubts about the fight. He falls into despair and refuses to fight. At this time, Krishna reminds him of his duty as a Kshatriya to fight for a righteous cause in the famous Bhagavad Gita section of the epic.

Though initially sticking to chivalrous notions of warfare, both sides soon adopt dishonourable tactics. At the end of the 18-day battle, only the Pandavas, Satyaki, Kripa, Ashwatthama, Kritavarma, Yuyutsu and Krishna survive. Yudhisthir becomes King of Hastinapur and Gandhari curses Krishna that the downfall of his clan is imminent.

After "seeing" the carnage, Gandhari, who had lost all her sons, curses Krishna to be a witness to a similar annihilation of his family, for though divine and capable of stopping the war, he had not done so. Krishna accepts the curse, which bears fruit 36 years later.

The Pandavas, who had ruled their kingdom meanwhile, decide to renounce everything. Clad in skins and rags they retire to the Himalaya and climb towards heaven in their bodily form. A stray dog travels with them. One by one the brothers and Draupadi fall on their way. As each one stumbles, Yudhishthira gives the rest the reason for their fall (Draupadi was partial to Arjuna, Nakula and Sahadeva were vain and proud of their looks, and Bhima and Arjuna were proud of their strength and archery skills, respectively). Only the virtuous Yudhishthira, who had tried everything to prevent the carnage, and the dog remain. The dog reveals himself to be the god Yama (also known as Yama Dharmaraja), and then takes him to the underworld where he sees his siblings and wife. After explaining the nature of the test, Yama takes Yudhishthira back to heaven and explains that it was necessary to expose him to the underworld because (Rajyante narakam dhruvam) any ruler has to visit the underworld at least once. Yama then assures him that his siblings and wife would join him in heaven after they had been exposed to the underworld for measures of time according to their vices.

Arjuna's grandson Parikshit rules after them and dies bitten by a snake. His furious son, Janamejaya, decides to perform a snake sacrifice ("sarpasattra") in order to destroy the snakes. It is at this sacrifice that the tale of his ancestors is narrated to him.

The Mahābhārata mentions that Karna, the Pandavas, Draupadi and Dhritarashtra's sons eventually ascended to svarga and "attained the state of the gods", and banded together – "serene and free from anger".

The "Mahābhārata" offers one of the first instances of theorizing about "dharmayuddha", "just war", illustrating many of the standards that would be debated later across the world. In the story, one of five brothers asks if the suffering caused by war can ever be justified. A long discussion ensues between the siblings, establishing criteria like "proportionality" (chariots cannot attack cavalry, only other chariots; no attacking people in distress), "just means" (no poisoned or barbed arrows), "just cause" (no attacking out of rage), and fair treatment of captives and the wounded.

Between 1919 and 1966, scholars at the Bhandarkar Oriental Research Institute, Pune, compared the various manuscripts of the epic from India and abroad and produced the "Critical Edition" of the "Mahabharata", on 13,000 pages in 19 volumes, followed by the "Harivamsha" in another two volumes and six index volumes. This is the text that is usually used in current Mahābhārata studies for reference. This work is sometimes called the "Pune" or "Poona" edition of the "Mahabharata".

Many regional versions of the work developed over time, mostly differing only in minor details, or with verses or subsidiary stories being added. These include the Tamil street theatre, terukkuttu and kattaikkuttu, the plays of which use themes from the Tamil language versions of "Mahabharata", focusing on Draupadi.

Outside the Indian subcontinent, in Indonesia, a version was developed in ancient Java as Kakawin Bhāratayuddha in the 11th century under the patronage of King Dharmawangsa (990–1016) and later it spread to the neighboring island of Bali, which remains a Hindu majority island today. It has become the fertile source for Javanese literature, dance drama (wayang wong), and wayang shadow puppet performances. This Javanese version of the Mahābhārata differs slightly from the original Indian version. For example, Draupadi is only wed to Yudhishthira, not to all the Pandava brothers; this might demonstrate ancient Javanese opposition to polyandry. The author later added some female characters to be wed to the Pandavas, for example, Arjuna is described as having many wives and consorts next to Subhadra. Another difference is that Shikhandini does not change her sex and remains a woman, to be wed to Arjuna, and takes the role of a warrior princess during the war. Another twist is that Gandhari is described as antagonistic character who hates the Pandavas: her hate is out of jealousy because during Gandhari's swayamvara, she was in love with Pandu but was later wed to his blind elder brother instead, whom she did not love, so she blindfolded herself as protest. Another notable difference is the inclusion of the Punakawans, the clown servants of the main characters in the storyline. These characters include Semar, Petruk, Gareng and Bagong, who are much-loved by Indonesian audiences. There are also some spin-off episodes developed in ancient Java, such as Arjunawiwaha composed in 11th century.

A Kawi version of the "Mahabharata", of which eight of the eighteen "parvas" survive, is found on the Indonesian island of Bali. It has been translated into English by Dr. I. Gusti Putu Phalgunadi.

A Persian translation of "Mahabharata", titled "Razmnameh", was produced at Akbar's orders, by Faizi and ʽAbd al-Qadir Badayuni in the 18th century.

The first complete English translation was the Victorian prose version by Kisari Mohan Ganguli, published between 1883 and 1896 (Munshiram Manoharlal Publishers) and by M. N. Dutt (Motilal Banarsidass Publishers). Most critics consider the translation by Ganguli to be faithful to the original text. The complete text of Ganguli's translation is in the public domain and is available online.

Another English prose translation of the full epic, based on the "Critical Edition", is in progress, published by University of Chicago Press. It was initiated by Indologist J. A. B. van Buitenen (books 1–5) and, following a 20-year hiatus caused by the death of van Buitenen, is being continued by D. Gitomer of DePaul University (book 6), J. L. Fitzgerald of Brown University (books 11–13) and Wendy Doniger of the University of Chicago (books 14–18).

An early poetry translation by Romesh Chunder Dutt and published in 1898 condenses the main themes of the Mahābhārata into English verse. A later poetic "transcreation" (author's own description) of the full epic into English, done by the poet P. Lal, is complete, and in 2005 began being published by Writers Workshop, Calcutta. The P. Lal translation is a non-rhyming verse-by-verse rendering, and is the only edition in any language to include all slokas in all recensions of the work (not just those in the "Critical Edition"). The completion of the publishing project is scheduled for 2010. Sixteen of the eighteen volumes are now available.

A project to translate the full epic into English prose, translated by various hands, began to appear in 2005 from the Clay Sanskrit Library, published by New York University Press. The translation is based not on the "Critical Edition" but on the version known to the commentator Nīlakaṇṭha. Currently available are 15 volumes of the projected 32-volume edition.

Indian economist Bibek Debroy has also begun an unabridged English translation in ten volumes. Volume 1: Adi Parva was published in March 2010.

Many condensed versions, abridgements and novelistic prose retellings of the complete epic have been published in English, including works by Ramesh Menon, William Buck, R. K. Narayan, C. Rajagopalachari, K. M. Munshi, Krishna Dharma, Romesh C. Dutt, Bharadvaja Sarma, John D. Smith and Sharon Maas.

Bhasa, the 2nd- or 3rd-century CE Sanskrit playwright, wrote two plays on episodes in the "Marabharata", "Urubhanga" ("Broken Thigh"), about the fight between Duryodhana and Bhima, while "Madhyamavyayoga" ("The Middle One") set around Bhima and his son, Ghatotkacha. The first important play of 20th century was "Andha Yug" ("The Blind Epoch"), by Dharamvir Bharati, which came in 1955, found in "Mahabharat", both an ideal source and expression of modern predicaments and discontent. Starting with Ebrahim Alkazi, it was staged by numerous directors. V. S. Khandekar's Marathi novel, "Yayati" (1960), and Girish Karnad's debut play "Yayati" (1961) are based on the story of King Yayati found in the "Mahabharat". Bengali writer and playwright, Buddhadeva Bose wrote three plays set in Mahabharat, "Anamni Angana", "Pratham Partha" and "Kalsandhya". Pratibha Ray wrote an award winning novel entitled Yajnaseni from Draupadi's perspective in 1984. Later, Chitra Banerjee Divakaruni wrote a similar novel entitled "" in 2008. Gujarati poet Chinu Modi has written long narrative poetry "Bahuk" based on character Bahuka. Krishna Udayasankar, a Singapore-based Indian author, has written several novels which are modern-day retellings of the epic, most notably the Aryavarta Chronicles Series. Suman Pokhrel wrote a solo play based on Ray's novel by personalizing and taking Draupadi alone in the scene.

Amar Chitra Katha published a 1,260-page comic book version of the "Mahabharata".

In Indian cinema, several film versions of the epic have been made, dating back to 1920. The Mahābhārata was also reinterpreted by Shyam Benegal in Kalyug. Prakash Jha directed 2010 film Raajneeti was partially inspired by the "Mahabharata". A 2013 animated adaptation holds the record for India's most expensive animated film.

In 1988, B. R. Chopra created a television series named "Mahabharat." It was directed by Ravi Chopra, and was televised on India's national television (Doordarshan). The same year as "Mahabharat" was being shown on Doordarshan, that same company's other television show, "Bharat Ek Khoj", also directed by Shyam Benegal, showed a 2-episode abbreviation of the "Mahabharata", drawing from various interpretations of the work, be they sung, danced, or staged. In the Western world, a well-known presentation of the epic is Peter Brook's nine-hour play, which premiered in Avignon in 1985, and its five-hour movie version "The Mahābhārata" (1989). In the late 2013 "Mahabharat" was televised on STAR Plus. It was produced by Swastik Productions Pvt.

Uncompleted projects on the Mahābhārata include one by Rajkumar Santoshi, and a theatrical adaptation planned by Satyajit Ray.

Every year in the Garhwal region of Uttarakhand, villagers perform the "Pandav Lila", a ritual re-enactment of episodes from the "Mahabharata" through dancing, singing and recitation. The "lila" is a cultural highlight of the year and is usually performed between November and February. Folk instruments of the region, dhol, damau and two long trumpets bhankore, accompany the action. The actors, who are amateurs not professionals, often break into a spontaneous dance when they are "possessed" by the spirits of their characters.

Jain versions of Mahābhārata can be found in the various Jain texts like "Harivamsapurana" (the story of Harivamsa) "Trisastisalakapurusa Caritra" (Hagiography of 63 Illustrious persons), "Pandavacaritra" (lives of Pandavas) and "Pandavapurana" (stories of Pandavas). From the earlier canonical literature, "Antakrddaaśāh" (8th cannon) and "Vrisnidasa" ("upangagama" or secondary canon) contain the stories of Neminatha (22nd Tirthankara), Krishna and Balarama. Prof. Padmanabh Jaini notes that, unlike in the Hindu Puranas, the names Baladeva and Vasudeva are not restricted to Balarama and Krishna in Jain puranas. Instead they serve as names of two distinct class of mighty brothers, who appear nine times in each half of time cycles of the Jain cosmology and rule the half the earth as half-chakravartins. Jaini traces the origin of this list of brothers to the Jinacharitra by Bhadrabahu swami (4th–3rd century BCE). According to Jain cosmology Balarama, Krishna and Jarasandha are the ninth and the last set of Baladeva, Vasudeva, and Partivasudeva. The main battle is not the Mahabharata, but the fight between Krishna and Jarasandha (who is killed by Krishna). Ultimately, the Pandavas and Balarama take renunciation as Jain monks and are reborn in heavens, while on the other hand Krishna and Jarasandha are reborn in hell. In keeping with the law of karma, Krishna is reborn in hell for his exploits (sexual and violent) while Jarasandha for his evil ways. Prof. Jaini admits a possibility that perhaps because of his popularity, the Jain authors were keen to rehabilitate Krishna. The Jain texts predict that after his karmic term in hell is over sometime during the next half time-cycle, Krishna will be reborn as a Jain Tirthankara and attain liberation. Krishna and Balrama are shown as contemporaries and cousins of 22nd Tirthankara, Neminatha. According to this story, Krishna arranged young Neminath's marriage with Rajamati, the daughter of Ugrasena, but Neminatha, empathizing with the animals which were to be slaughtered for the marriage feast, left the procession suddenly and renounced the world.

In the "Bhagavad Gita", Krishna explains to Arjuna his duties as a warrior and prince and elaborates on different Yogic and Vedantic philosophies, with examples and analogies. This has led to the Gita often being described as a concise guide to Hindu philosophy and a practical, self-contained guide to life. In more modern times, Swami Vivekananda, Netaji Subhas Chandra Bose, Bal Gangadhar Tilak, Mahatma Gandhi and many others used the text to help inspire the Indian independence movement.






</doc>
<doc id="19644" url="https://en.wikipedia.org/wiki?curid=19644" title="Mein Kampf">
Mein Kampf

Mein Kampf (; "My Struggle" or "My Fight") is a 1925 autobiographical manifesto by Nazi Party leader Adolf Hitler. The work describes the process by which Hitler became antisemitic and outlines his political ideology and future plans for Germany. Volume 1 of "Mein Kampf" was published in 1925 and Volume 2 in 1926. The book was edited first by Emil Maurice, then by Hitler's deputy Rudolf Hess.

Hitler began "Mein Kampf" while imprisoned for what he considered to be "political crimes" following his failed Putsch in Munich in November 1923. Although he received many visitors initially, he soon devoted himself entirely to the book. As he continued, he realized that it would have to be a two-volume work, with the first volume scheduled for release in early 1925. The governor of Landsberg noted at the time that "he [Hitler] hopes the book will run into many editions, thus enabling him to fulfill his financial obligations and to defray the expenses incurred at the time of his trial." After slow initial sales, the book became a bestseller in Germany following Hitler's rise to power in 1933.

After Hitler's death, copyright of "Mein Kampf" passed to the state government of Bavaria, which refused to allow any copying or printing of the book in Germany. In 2016, following the expiration of the copyright held by the Bavarian state government, "Mein Kampf" was republished in Germany for the first time since 1945, which prompted public debate and divided reactions from Jewish groups.

Hitler originally wanted to call his forthcoming book "Viereinhalb Jahre (des Kampfes) gegen Lüge, Dummheit und Feigheit", or "Four and a Half Years (of Struggle) Against Lies, Stupidity and Cowardice". Max Amann, head of the Franz Eher Verlag and Hitler's publisher, is said to have suggested the much shorter ""Mein Kampf"" or ""My Struggle"".

The arrangement of chapters is as follows:


In "Mein Kampf", Hitler used the main thesis of "the Jewish peril", which posits a Jewish conspiracy to gain world leadership. The narrative describes the process by which he became increasingly antisemitic and militaristic, especially during his years in Vienna. He speaks of not having met a Jew until he arrived in Vienna, and that at first his attitude was liberal and tolerant. When he first encountered the antisemitic press, he says, he dismissed it as unworthy of serious consideration. Later he accepted the same antisemitic views, which became crucial to his program of national reconstruction of Germany.

"Mein Kampf" has also been studied as a work on political theory. For example, Hitler announces his hatred of what he believed to be the world's two evils: Communism and Judaism.

In the book Hitler blamed Germany's chief woes on the parliament of the Weimar Republic, the Jews, and Social Democrats, as well as Marxists, though he believed that Marxists, Social Democrats, and the parliament were all working for Jewish interests. He announced that he wanted to completely destroy the parliamentary system, believing it to be corrupt in principle, as those who reach power are inherent opportunists.

While historians dispute the exact date Hitler decided to exterminate the Jewish people, few place the decision before the mid-1930s. First published in 1925, "Mein Kampf" shows Hitler's personal grievances and his ambitions for creating a New Order. Hitler also wrote that "The Protocols of the Elders of Zion", a fabricated text which purported to expose the Jewish plot to control the world, was an authentic document. This later became a part of the Nazi propaganda effort to justify persecution and annihilation of the Jews.

The historian Ian Kershaw points out that several passages in "Mein Kampf" are undeniably of a genocidal nature. Hitler wrote "the nationalization of our masses will succeed only when, aside from all the positive struggle for the soul of our people, their international poisoners are exterminated", and he suggested that, "If at the beginning of the war and during the war twelve or fifteen thousand of these Hebrew corrupters of the nation had been subjected to poison gas, such as had to be endured in the field by hundreds of thousands of our very best German workers of all classes and professions, then the sacrifice of millions at the front would not have been in vain."

The racial laws to which Hitler referred resonate directly with his ideas in "Mein Kampf". In the first edition, Hitler stated that the destruction of the weak and sick is far more humane than their protection. Apart from this allusion to humane treatment, Hitler saw a purpose in destroying "the weak" in order to provide the proper space and purity for the "strong".

In the chapter "Eastern Orientation or Eastern Policy", Hitler argued that the Germans needed Lebensraum in the East, a "historic destiny" that would properly nurture the German people. Hitler believed that "the organization of a Russian state formation was not the result of the political abilities of the Slavs in Russia, but only a wonderful example of the state-forming efficacy of the German element in an inferior race."

In "Mein Kampf" Hitler openly stated the future German expansion in the East, foreshadowing Generalplan Ost:

Although Hitler originally wrote "Mein Kampf" mostly for the followers of National Socialism, it grew in popularity after he rose to power. (Two other books written by party members, Gottfried Feder's "Breaking The Interest Slavery" and Alfred Rosenberg's "The Myth of the Twentieth Century," have since lapsed into comparative literary obscurity.) Hitler had made about 1.2 million Reichsmarks from the income of the book by 1933 (), when the average annual income of a teacher was about 4,800 Marks (). He accumulated a tax debt of 405,500 Reichsmark (very roughly in 2015 1.1 million GBP, 1.4 million EUR, 1.5 million USD) from the sale of about 240,000 copies before he became chancellor in 1933 (at which time his debt was waived).

Hitler began to distance himself from the book after becoming chancellor of Germany in 1933. He dismissed it as "fantasies behind bars" that were little more than a series of articles for the "Völkischer Beobachter", and later told Hans Frank that "If I had had any idea in 1924 that I would have become Reich chancellor, I never would have written the book." Nevertheless, "Mein Kampf" was a bestseller in Germany during the 1930s. During Hitler's years in power, the book was in high demand in libraries and often reviewed and quoted in other publications. It was given free to every newlywed couple and every soldier fighting at the front. By 1939 it had sold 5.2 million copies in eleven languages. By the end of the war, about 10 million copies of the book had been sold or distributed in Germany.

"Mein Kampf", in essence, lays out the ideological program Hitler established for the German revolution, by identifying the Jews and "Bolsheviks" as racially and ideologically inferior and threatening, and "Aryans" and National Socialists as racially superior and politically progressive. Hitler's revolutionary goals included expulsion of the Jews from Greater Germany and the unification of German peoples into one Greater Germany. Hitler desired to restore German lands to their greatest historical extent, real or imagined.

Due to its racist content and the historical effect of Nazism upon Europe during World War II and the Holocaust, it is considered a highly controversial book. Criticism has not come solely from opponents of Nazism. Italian Fascist dictator and Nazi ally Benito Mussolini was also critical of the book, saying that it was "a boring tome that I have never been able to read" and remarking that Hitler's beliefs, as expressed in the book, were "little more than commonplace clichés".

The German journalist Konrad Heiden, an early critic of the Nazi Party, observed that the content of "Mein Kampf" is essentially a political argument with other members of the Nazi Party who had appeared to be Hitler's friends, but whom he was actually denouncing in the book's content – sometimes by not even including references to them.

The American literary theorist and philosopher Kenneth Burke wrote a 1939 rhetorical analysis of the work, "The Rhetoric of Hitler's "Battle"", which revealed an underlying message of aggressive intent.

The American journalist John Gunther said in 1940 that compared to the autobiographies such as Leon Trotsky's "My Life" or Henry Adams's "The Education of Henry Adams", "Mein Kampf" was "vapid, vain, rhetorical, diffuse, prolix." However, he added that "it is a powerful and moving book, the product of great passionate feeling". He suggested that the book exhausted curious German readers, but its "ceaseless repetition of the argument, left impregnably in their minds, fecund and germinating".

In March 1940, British writer George Orwell reviewed a then-recently published uncensored translation of "Mein Kampf" for "The New English Weekly". Orwell suggested that the force of Hitler's personality shone through the often "clumsy" writing, capturing the magnetic allure of Hitler for many Germans. In essence, Orwell notes, Hitler offers only visions of endless struggle and conflict in the creation of "a horrible brainless empire" that "stretch[es] to Afghanistan or thereabouts". He wrote, "Whereas Socialism, and even capitalism in a more grudging way, have said to people 'I offer you a good time,' Hitler has said to them, 'I offer you struggle, danger, and death,' and as a result a whole nation flings itself at his feet." Orwell's review was written in the aftermath of the 1939 Molotov–Ribbentrop Pact, when Hitler made peace with USSR after more than a decade of vitriolic rhetoric and threats between the two nations; with the pact in place, Orwell believed, England was now facing a risk of Nazi attack and the UK must not underestimate the appeal of Hitler's ideas.

In his 1943 book "The Menace of the Herd", Austrian scholar Erik von Kuehnelt-Leddihn described Hitler's ideas in "Mein Kampf" and elsewhere as "a veritable "reductio ad absurdum" of 'progressive' thought" and betraying "a curious lack of original thought" that shows Hitler offered no innovative or original ideas but was merely "a "virtuoso" of commonplaces which he may or may not repeat in the guise of a 'new discovery.'" Hitler's stated aim, Kuehnelt-Leddihn writes, is to quash individualism in furtherance of political goals:

In his "The Second World War", published in several volumes in the late 1940s and early 1950s, Winston Churchill wrote that he felt that after Hitler's ascension to power, no other book than "Mein Kampf" deserved more intensive scrutiny.

The critic George Steiner has suggested that "Mein Kampf" can be seen as one of several books that resulted from the crisis of German culture following Germany's defeat in World War I, comparable in this respect to the philosopher Ernst Bloch's "The Spirit of Utopia" (1918), the historian Oswald Spengler's "The Decline of the West" (1918), the theologian Franz Rosenzweig's "The Star of Redemption" (1921), the theologian Karl Barth's "The Epistle to the Romans" (1922), and the philosopher Martin Heidegger's "Being and Time" (1927).

While Hitler was in power (1933–1945), "Mein Kampf" came to be available in three common editions. The first, the "Volksausgabe" or People's Edition, featured the original cover on the dust jacket and was navy blue underneath with a gold swastika eagle embossed on the cover. The "Hochzeitsausgabe", or Wedding Edition, in a slipcase with the seal of the province embossed in gold onto a parchment-like cover was given free to marrying couples. In 1940, the "Tornister-Ausgabe", or Knapsack Edition, was released. This edition was a compact, but unabridged, version in a red cover and was released by the post office, available to be sent to loved ones fighting at the front. These three editions combined both volumes into the same book.

A special edition was published in 1939 in honour of Hitler's 50th birthday. This edition was known as the "Jubiläumsausgabe", or Anniversary Issue. It came in both dark blue and bright red boards with a gold sword on the cover. This work contained both volumes one and two. It was considered a deluxe version, relative to the smaller and more common "Volksausgabe".

The book could also be purchased as a two-volume set during Hitler's rule, and was available in soft cover and hardcover. The soft cover edition contained the original cover (as pictured at the top of this article). The hardcover edition had a leather spine with cloth-covered boards. The cover and spine contained an image of three brown oak leaves.

At the time of his suicide, Hitler's official place of residence was in Munich, which led to his entire estate, including all rights to "Mein Kampf", changing to the ownership of the state of Bavaria. The government of Bavaria, in agreement with the federal government of Germany, refused to allow any copying or printing of the book in Germany. It also opposed copying and printing in other countries, but with less success. As per German copyright law, the entire text entered the public domain on 1 January 2016, 70 years after the author's death.

Owning and buying the book in Germany is not an offence. Trading in old copies is lawful as well, unless it is done in such a fashion as to "promote hatred or war." In particular, the unmodified edition is not covered by §86 StGB that forbids dissemination of means of propaganda of unconstitutional organizations, since it is a "pre-constitutional work" and as such cannot be opposed to the free and democratic basic order, according to a 1979 decision of the Federal Court of Justice of Germany. Most German libraries carry heavily commented and excerpted versions of "Mein Kampf." In 2008, Stephan Kramer, secretary-general of the Central Council of Jews in Germany, not only recommended lifting the ban, but volunteered the help of his organization in editing and annotating the text, saying that it is time for the book to be made available to all online.

A variety of restrictions or special circumstances apply in other countries.

In 1934, the French government unofficially sponsored the publication of an unauthorized translation. It was meant as a warning and included a critical introduction by Marshal Lyautey ("Every Frenchman must read this book"). It was published by far-right publisher Fernand Sorlot in an agreement with the activists of LICRA who bought 5000 copies to be offered to "influential people"; however, most of them treated the book as a casual gift and did not read it. The Nazi regime unsuccessfully tried to have it forbidden. Hitler, as the author, and Eher-Verlag, his German publisher, had to sue for copyright infringement in the Commercial Court of France. Hitler's lawsuit succeeded in having all copies seized, the print broken up, and having an injunction against booksellers offering any copies. However, a large quantity of books had already been shipped and stayed available undercover by Sorlot.

In 1938, Hitler licensed for France an authorized edition by Fayard, translated by François Dauture and Georges Blond, lacking the threatening tone against France of the original. The French edition was 347 pages long, while the original title was 687 pages, and it was titled "Ma doctrine" ("My doctrine").

After the war, Fernand Sorlot re-edited, re-issued, and continued to sell the work, without permission from the state of Bavaria, to which the author's rights had defaulted.

In the 1970s, the rise of the extreme right in France along with the growing of Holocaust denial works, placed the "Mein Kampf" under judicial watch and in 1978, LICRA entered a complaint in the courts against the publisher for inciting antisemitism. Sorlot received a "substantial fine" but the court also granted him the right to continue publishing the work, provided certain warnings and qualifiers accompany the text.

On 1 January 2016, seventy years after the author's death, "Mein Kampf" entered the public domain in France.

A new edition was published in 2017 by Fayard, now part of the Groupe Hachette, with a critical introduction, just as the edition published in 2018 in Germany by the "Institut für Zeitgeschichte", the Institute of Contemporary History based in Munich.

Since its first publication in India in 1928, "Mein Kampf" has gone through hundreds of editions and sold over 100,000 copies.

On 5 May 1995 a translation of "Mein Kampf" released by a small Latvian publishing house "Vizītkarte" began appearing in bookstores, provoking a reaction from Latvian authorities, who confiscated the approximately 2,000 copies that had made their way to the bookstores and charged director of the publishing house Pēteris Lauva with offences under anti-racism law. Currently the publication of "Mein Kampf" is forbidden in Latvia.

In April 2018 a number of Russian-language news sites (Baltnews, Zvezda, Sputnik, Komsomolskaya Pravda and Komprava among others) reported that Adolf Hitler had allegedly become more popular in Latvia than Harry Potter, referring to a Latvian online book trading platform ibook.lv, where "Mein Kampf" had appeared at the No. 1 position in "The Most Current Books in 7 Days" list.

In research done by Polygraph.info who called the claim "false", ibook.lv was only the 878th most popular website and 149th most popular shopping site in Latvia at the time, according to Alexa Internet. In addition to that, the website only had 4 copies on sale by individual users and no users wishing to purchase the book. Owner of ibook.lv pointed out that the book list is not based on actual deals, but rather page views, of which 70% in the case of "Mein Kampf" had come from anonymous and unregistered users she believed could be fake users. Ambassador of Latvia to the Russian Federation Māris Riekstiņš responded to the story by tweeting "everyone, who wishes to know what books are actually bought and read in Latvia, are advised to address the largest book stores @JanisRoze; @valtersunrapa; @zvaigzneabc". BBC also acknowledged the story was fake news, adding that in the last three years "Mein Kampf" had been requested for borrowing for only 139 times across all libraries in Latvia, in comparison with around 25,000 requests for books about Harry Potter.

In the Netherlands the sale of "Mein Kampf" had been forbidden since World War II. In September 2018, however, Dutch publisher Prometheus officially released an academic edition of the 2016 German translation with comprehensive introductions and annotations by Dutch historians. It marks the first time the book is widely available to the general public in the Netherlands since World War II.

In the Russian Federation, "Mein Kampf" has been published at least three times since 1992; the Russian text is also available on websites. In 2006 the Public Chamber of Russia proposed banning the book. In 2009 St. Petersburg's branch of the Russian Ministry of Internal Affairs requested to remove an annotated and hyper-linked Russian translation of the book from a historiography website. On 13 April 2010, it was announced that "Mein Kampf" is outlawed on grounds of extremism promotion.

"Mein Kampf" has been reprinted several times since 1945; in 1970, 1992, 2002 and 2010. In 1992 the Government of Bavaria tried to stop the publication of the book, and the case went to the Supreme Court of Sweden which ruled in favour of the publisher, stating that the book is protected by copyright, but that the copyright holder is unidentified (and not the State of Bavaria) and that the original Swedish publisher from 1934 had gone out of business. It therefore refused the Government of Bavaria's claim.
The only translation changes came in the 1970 edition, but they were only linguistic, based on a new Swedish standard.

"Mein Kampf" was widely available and growing in popularity in Turkey, even to the point where it became a bestseller, selling up to 100,000 copies in just two months in 2005. Analysts and commentators believe the popularity of the book to be related to a rise in nationalism and anti-U.S. sentiment. A columnist in Shalom stated this was a result of "what is happening in the Middle East, the Israeli-Palestinian problem and the war in Iraq." Doğu Ergil, a political scientist at Ankara University, said both far-right ultranationalists and extremist Islamists had found common ground - "not on a common agenda for the future, but on their anxieties, fears and hate".

In the United States, "Mein Kampf" can be found at many community libraries and can be bought, sold and traded in bookshops. The U.S. government seized the copyright in September 1942 during the Second World War under the Trading with the Enemy Act and in 1979, Houghton Mifflin, the U.S. publisher of the book, bought the rights from the government pursuant to . More than 15,000 copies are sold a year. In 2016, Houghton Mifflin Harcourt reported that it was having difficulty finding a charity that would accept profits from the sales of its version of "Mein Kampf", which it had promised to donate.

In 1999, the Simon Wiesenthal Center documented that the book was available in Germany via major online booksellers such as Amazon and Barnes & Noble. After a public outcry, both companies agreed to stop those sales to addresses in Germany. In March 2020 Amazon banned sales of new and second-hand copies of "Mein Kampf", and several other Nazi publications, on its platform. The book remains available on Barnes and Noble's website. It is also available in various languages, including German, at the Internet Archive. One of the first complete English translations was published by James Vincent Murphy in 1939. The Murphy translation of the book is freely available on Project Gutenberg Australia.

On 3 February 2010, the Institute of Contemporary History (IfZ) in Munich announced plans to republish an annotated version of the text, for educational purposes in schools and universities, in 2015. The book had last been published in Germany in 1945. The IfZ argued that a republication was necessary to get an authoritative annotated edition by the time the copyright ran out, which might open the way for neo-Nazi groups to publish their own versions. The Bavarian Finance Ministry opposed the plan, citing respect for victims of the Holocaust. It stated that permits for reprints would not be issued, at home or abroad. This would also apply to a new annotated edition. There was disagreement about the issue of whether the republished book might be banned as Nazi propaganda. The Bavarian government emphasized that even after expiration of the copyright, "the dissemination of Nazi ideologies will remain prohibited in Germany and is punishable under the penal code". However, the Bavarian Science Minister Wolfgang Heubisch supported a critical edition, stating in 2010 that, "Once Bavaria's copyright expires, there is the danger of charlatans and neo-Nazis appropriating this infamous book for themselves".

On 12 December 2013 the Bavarian government cancelled its financial support for an annotated edition. IfZ, which was preparing the translation, announced that it intended to proceed with publication after the copyright expired. The IfZ scheduled an edition of "Mein Kampf" for release in 2016.

Richard Verber, vice-president of the Board of Deputies of British Jews, stated in 2015 that the board trusted the academic and educational value of republishing. "We would, of course, be very wary of any attempt to glorify Hitler or to belittle the Holocaust in any way", Verber declared to "The Observer". "But this is not that. I do understand how some Jewish groups could be upset and nervous, but it seems it is being done from a historical point of view and to put it in context".

An annotated edition of "Mein Kampf" was published in Germany in January 2016 and sold out within hours on Amazon's German site. The book's publication led to public debate in Germany, and divided reactions from Jewish groups, with some supporting, and others opposing, the decision to publish. German officials had previously said they would limit public access to the text amid fears that its republication could stir neo-Nazi sentiment. Some bookstores stated that they would not stock the book. Dussmann, a Berlin bookstore, stated that one copy was available on the shelves in the history section, but that it would not be advertised and more copies would be available only on order. By January 2017, the German annotated edition had sold over 85,000 copies.

After the party's poor showing in the 1928 elections, Hitler believed that the reason for his loss was the public's misunderstanding of his ideas. He then retired to Munich to dictate a sequel to "Mein Kampf" to expand on its ideas, with more focus on foreign policy.

Only two copies of the 200-page manuscript were originally made, and only one of these was ever made public. The document was neither edited nor published during the Nazi era and remains known as "Zweites Buch", or "Second Book". To keep the document strictly secret, in 1935 Hitler ordered that it be placed in a safe in an air raid shelter. It remained there until being discovered by an American officer in 1945.

The authenticity of the document found in 1945 has been verified by Josef Berg, a former employee of the Nazi publishing house Eher Verlag, and Telford Taylor, a former brigadier general of the United States Army Reserve and Chief Counsel at the Nuremberg war-crimes trials.

In 1958, the "Zweites Buch" was found in the archives of the United States by American historian Gerhard Weinberg. Unable to find an American publisher, Weinberg turned to his mentor – Hans Rothfels at the Institute of Contemporary History in Munich, and his associate Martin Broszat – who published "Zweites Buch" in 1961. A pirated edition was published in English in New York in 1962. The first authoritative English edition was not published until 2003 ("Hitler's Second Book: The Unpublished Sequel to Mein Kampf," ).








</doc>
<doc id="19645" url="https://en.wikipedia.org/wiki?curid=19645" title="Morpheus (disambiguation)">
Morpheus (disambiguation)

Morpheus is a god associated with sleep and dreams. In Ovid's "Metamorphoses" he is the son of Sleep, who appears in dreams in human form. Morpheus may also refer to:






</doc>
<doc id="19648" url="https://en.wikipedia.org/wiki?curid=19648" title="May 26">
May 26







</doc>
<doc id="19649" url="https://en.wikipedia.org/wiki?curid=19649" title="MVS">
MVS

Multiple Virtual Storage, more commonly called MVS, was the most commonly used operating system on the System/370 and System/390 IBM mainframe computers. IBM developed MVS, along with OS/VS1 and SVS, as a successor to OS/360. It is unrelated to IBM's other mainframe operating system lines, e.g., VSE, VM, TPF.

First released in 1974, MVS was extended by program products with new names multiple times:

At first IBM described MVS as simply a new release of OS/VS2, but it was, in fact a major rewrite. OS/VS2 release 1 was an upgrade of OS/360 MVT that retained most of the original code and, like MVT, was mainly written in assembly language. The MVS core was almost entirely written in Assembler XF, although a few modules were written in PL/S, but not the performance-sensitive ones, in particular not the Input/Output Supervisor (IOS). IBM's use of "OS/VS2" emphasized upwards compatibility: application programs that ran under MVT did not even need recompiling to run under MVS. The same Job Control Language files could be used unchanged; utilities and other non-core facilities like TSO ran unchanged. IBM and users almost unanimously called the new system MVS from the start, and IBM continued to use the term "MVS" in the naming of later "major" versions such as MVS/XA.

OS/360 MFT (Multitasking with a Fixed number of Tasks) provided multitasking: several memory partitions, each of a fixed size, were set up when the operating system was installed and when the operator redefined them. For example, there could be a small partition, two medium partitions, and a large partition. If there were two large programs ready to run, one would have to wait until the other finished and vacated the large partition.

OS/360 MVT (Multitasking with a Variable number of Tasks) was an enhancement that further refined memory use. Instead of using fixed-size memory partitions, MVT allocated memory to regions for job steps as needed, provided enough "contiguous" physical memory was available. This was a significant advance over MFT's memory management, but had some weaknesses: if a job allocated memory dynamically (as most sort programs and database management systems do), the programmers had to estimate the job's maximum memory requirement and pre-define it for MVT. A job step that contained a mix of small and large programs wasted memory while the small programs ran. Most seriously, memory could become fragmented, i.e., the memory not used by current jobs could be divided into uselessly small chunks between the areas used by current jobs, and the only remedy was to wait some current jobs finished before starting any new ones.

In the early 1970s IBM sought to mitigate these difficulties by introducing virtual memory (which IBM called "virtual storage"), which allowed programs to request address spaces larger than physical memory. The original implementations had a single virtual address space, shared by all jobs. OS/VS1 was OS/360 MFT within a single virtual address space; OS/VS2 SVS was OS/360 MVT within a single virtual address space. So OS/VS1 and SVS in principle had the same disadvantages as MFT and MVT, but the impacts were less severe because jobs could request much larger address spaces and the requests came out of a 16 MB pool even if physical storage was smaller.
In the mid-1970s IBM introduced MVS, which not only supported virtual storage that was larger than the available real storage, as did SVS, but also allowed an indefinite number of applications to run in different address spaces. Two concurrent programs might try to access the same virtual memory address, but the virtual memory system redirected these requests to different areas of physical memory. Each of these address spaces consisted of three areas: an operating system (one instance shared by all jobs), an application area unique for each application, and a shared virtual area used for various purposes, including inter-job communication. IBM promised that application areas would always be at least 8 MB. This made MVS the perfect solution for business problems that resulted from the need to run more applications.

MVS maximized processing potential by providing multiprogramming and multiprocessing capabilities. Like its MVT and OS/VS2 SVS predecessors, MVS supported multiprogramming; program instructions and associated data are scheduled by a control program and given processing cycles. Unlike a single-programming operating system, these systems maximize the use of the processing potential by dividing processing cycles among the instructions associated with several different concurrently running programs. This way, the control program does not have to wait for the I/O operation to complete before proceeding. By executing the instructions for multiple programs, the computer is able to switch back and forth between active and inactive programs.

Early editions of MVS (mid-1970s) were among the first of the IBM OS series to support multiprocessor configurations, though the M65MP variant of OS/360 running on 360 Models 65 and 67 had provided limited multiprocessor support. The 360 Model 67 had also hosted the multiprocessor capable TSS/360, MTS and CP-67 operating systems. Because multiprocessing systems can execute instructions simultaneously, they offer greater processing power than single-processing system. As a result, MVS was able to address the business problems brought on by the need to process large amounts of data.

Multiprocessing systems are either loosely coupled, which means that each computer has access to a common workload, or tightly coupled, which means that the computers share the same real storage and are controlled by a single copy of the operating system. MVS retained both the loosely coupled multiprocessing of Attached Support Processor (ASP) and the tightly coupled multiprocessing of OS/360 Model 65 Multiprocessing. In tightly coupled systems, two CPUs shared concurrent access to the same memory (and copy of the operating system) and peripherals, providing greater processing power and a degree of graceful degradation if one CPU failed. In loosely coupled configurations each of a group of processors (single and / or tightly coupled) had its own memory and operating system but shared peripherals and the operating system component JES3 allowed managing the whole group from one console. This provided greater resilience and let operators decide which processor should run which jobs from a central job queue. MVS JES3 gave users the opportunity to network together two or more data processing systems via shared disks and Channel-to-Channel Adapters (CTCA's). This capability eventually became available to JES2 users as Multi-Access SPOOL (MAS).

MVS originally supported 24-bit addressing (i.e., up to 16 MB). As the underlying hardware progressed, it supported 31-bit (XA and ESA; up to 2048 MB) and now (as z/OS) 64-bit addressing. The most significant motives for the rapid upgrade to 31-bit addressing were the growth of large transaction-processing networks, mostly controlled by CICS, which ran in a single address space—and the DB2 relational database management system needed more than 8 MB of application address space to run efficiently. (Early versions were configured into two address spaces that communicated via the shared virtual area, but this imposed a significant overhead since all such communications had transmit via the operating system.)

The main user interfaces to MVS are: Job Control Language (JCL), which was originally designed for batch processing but from the 1970s onwards was also used to start and allocate resources to long-running interactive jobs such as CICS; and TSO (Time Sharing Option), the interactive time-sharing interface, which was mainly used to run development tools and a few end-user information systems. ISPF is a TSO application for users on 3270-family terminals (and later, on VM as well), which allows the user to accomplish the same tasks as TSO's command line but in a menu and form oriented manner, and with a full screen editor and file browser. TSO's basic interface is command line, although facilities were added later for form-driven interfaces).

MVS took a major step forward in fault-tolerance, built on the earlier STAE facility, that IBM called "software recovery". IBM decided to do this after years of practical real-world experience with MVT in the business world. System failures were now having major impacts on customer businesses, and IBM decided to take a major design jump, to assume that despite the very best software development and testing techniques, that 'problems WILL occur.' This profound assumption was pivotal in adding great percentages of fault-tolerance code to the system and likely contributed to the system's success in tolerating software and hardware failures. Statistical information is hard to come by to prove the value of these design features (how can you measure 'prevented' or 'recovered' problems?), but IBM has, in many dimensions, enhanced these fault-tolerant software recovery and rapid problem resolution features, over time.

This design specified a hierarchy of error-handling programs, in system (kernel/'privileged') mode, called Functional Recovery Routines, and in user ('task' or 'problem program') mode, called "ESTAE" (Extended Specified Task Abnormal Exit routines) that were invoked in case the system detected an error (actually, hardware processor or storage error, or software error). Each recovery routine made the 'mainline' function reinvokable, captured error diagnostic data sufficient to debug the causing problem, and either 'retried' (reinvoke the mainline) or 'percolated' (escalated error processing to the next recovery routine in the hierarchy).

Thus, with each error the system captured diagnostic data, and attempted to perform a repair and keep the system up. The worst thing possible was to take down a user address space (a 'job') in the case of unrepaired errors. Though it was an initial design point, it was not until the most recent MVS version (z/OS), that recovery program was not only guaranteed its own recovery routine, but each recovery routine now has its own recovery routine. This recovery structure was embedded in the basic MVS control program, and programming facilities are available and used by application program developers and 3rd party developers.

Practically, the MVS software recovery made problem debugging both easier and more difficult. Software recovery requires that programs leave 'tracks' of where they are and what they are doing, thus facilitating debugging—but the fact that processing progresses despite an error can overwrite the tracks. Early data capture at the time of the error maximizes debugging, and facilities exist for the recovery routines (task and system mode, both) to do this.

IBM included additional criteria for a major software problem that required IBM service. If a mainline component failed to initiate software recovery, that was considered a valid reportable failure. Also, if a recovery routine failed to collect significant diagnostic data such that the original problem was solvable by data collected by that recovery routine, IBM standards dictated that this fault was reportable and required repair. Thus, IBM standards, when rigorously applied, encouraged continuous improvement.

IBM continued to support the major serviceability tool Dynamic Support System (DSS) that it had introduced in OS/VS1 and OS/VS2 Release 1. This interactive facility could be invoked to initiate a session to create diagnostic procedures, or invoke already-stored procedures. The procedures trapped special events, such as the loading of a program, device I/O, system procedure calls, and then triggered the activation of the previously defined procedures. These procedures, which could be invoked recursively, allowed for reading and writing of data, and alteration of instruction flow. Program Event Recording hardware was used. 
IBM dropped support for DSS with Selectable Unit 7 (SU7), an update to OS/VS2 Release 3.7 required by the program product OS/VS2 MVS/System Extensions (MVS/SE), Program Number 5740-XEl. The User group SHARE passed a requirement that IBM reinstate DSS, and IBM provided a PTF to allow use of DSS after MVS/SE was installed.

IBM again dropped support for DSS with SU64, an update to OS/VS2 Release 3.8 required by Release 2 of MVS/SE. 

Program-Event Recording (PER) exploitation was performed by the enhancement of the diagnostic SLIP command with the introduction of the PER support (SLIP/Per) in SU 64/65 (1978).

Multiple copies of MVS (or other IBM operating systems) could share the
same machine if that machine was controlled by VM/370. In this case VM/370 was the real operating system, and regarded the "guest" operating systems as applications with unusually high privileges. As a result of later hardware enhancements one instance of an operating system (either MVS, or VM with guests, or other) could also occupy a Logical Partition (LPAR) instead of an entire physical system.

Multiple MVS instances can be organized and collectively administered in a structure called a "systems complex" or "sysplex", introduced in September, 1990. Instances interoperate through a software component called a Cross-system Coupling Facility (XCF) and a hardware component called a "Hardware Coupling Facility" (CF or Integrated Coupling Facility, ICF, if co-located on the same mainframe hardware). Multiple sysplexes can be joined via standard network protocols such as IBM's proprietary Systems Network Architecture (SNA) or, more recently, via TCP/IP. The z/OS operating system (MVS' most recent descendant) also has native support to execute POSIX and Single UNIX Specification applications. The support began with MVS/SP V4R3, and IBM has obtained UNIX 95 certification for z/OS V1R2 and later.

The system is typically used in business and banking, and applications are often written in COBOL. COBOL programs were traditionally used with transaction processing systems like IMS and CICS. For a program running in CICS, special EXEC CICS statements are inserted in the COBOL source code. A preprocessor (translator) replaces those EXEC CICS statements with the appropriate COBOL code to call CICS before the program is compiled — not altogether unlike SQL used to call DB2. Applications can also be written in other languages such as C, C++, Java, assembly language, FORTRAN, BASIC, RPG, and REXX. Language support is packaged as a common component called "Language Environment" or "LE" to allow uniform debugging, tracing, profiling, and other language independent functions.

MVS systems are traditionally accessed by 3270 terminals or by PCs running 3270 emulators. However, many mainframe applications these days have custom web or GUI interfaces. The z/OS operating system has built-in support for TCP/IP. System management, done in the past with a 3270 terminal, is now done through the Hardware Management Console (HMC) and, increasingly, Web interfaces. Operator consoles are provided through 2074 emulators, so you are unlikely to see any S/390 or zSeries processor with a real 3270 connected to it.

The native character encoding scheme of MVS and its peripherals is EBCDIC, but the TR instruction made it easy to translate to other 7- and 8-bit codes. Over time IBM added hardware-accelerated services to perform translation to and between larger codes, hardware-specific service for Unicode transforms and software support of, e.g., ASCII, ISO/IEC 8859, UTF-8, UTF-16, and UTF-32. The software translation services take source and destination code pages as inputs.

Files, other than Unix files, are properly called data sets in MVS. Names of those files are organized in "catalogs" that are VSAM files themselves.

Data set names (DSNs, mainframe term for filenames) are organized in a hierarchy whose levels are separated with dots, e.g. "DEPT01.SYSTEM01.FILE01". Each level in the hierarchy can be up to eight characters long. The total filename length is a maximum of 44 characters including dots. By convention, the components separated by the dots are used to organize files similarly to directories in other operating systems. For example, there were utility programs that performed similar functions to those of Windows Explorer (but without the GUI and usually in batch processing mode) - adding, renaming or deleting new elements and reporting all the contents of a specified element. However, unlike in many other systems, these levels are not usually actual directories but just a naming convention (like the original Macintosh File System, where folder hierarchy was an illusion maintained by the Finder). TSO supports a default prefix for files (similar to a "current directory" concept), and RACF supports setting up access controls based on filename patterns, analogous to access controls on directories on other platforms.

As with other members of the OS family, MVS' data sets were record-oriented. MVS inherited three main types from its predecessors:
Sequential and ISAM datasets could store either fixed-length or variable length records, and all types could occupy more than one disk volume.

All of these are based on the VTOC disk structure.

Early IBM database management systems used various combinations of ISAM and BDAM datasets - usually BDAM for the actual data storage and ISAM for indexes.

In the early 1970s IBM's virtual memory operating systems introduced a new file management component, VSAM, which provided similar facilities:

These VSAM formats became the basis of IBM's database management systems, IMS/VS and DB2 - usually ESDS for the actual data storage and KSDS for indexes.

VSAM also included a catalog component used for MVS' master catalog.

Partitioned data sets (PDS) were sequential data sets subdivided into "members" that could each be processed as sequential files in their own right (like a folder in a hierarchical file system). The most important use of PDSes was for program libraries - system administrators used the main PDS as a way to allocate disk space to a project and the project team then created and edited the members. Other uses of PDSs were libraries of frequently used job control procedures (PROCs), and “copy books” of programming language statements such as record definitions used by several programs.

Generation Data Groups (GDGs) are groups of like named data sets, which can be referenced by absolute generation number, or by an offset from the most recent generation. They were originally designed to support grandfather-father-son backup procedures - if a file was modified, the changed version became the new "son", the previous "son" became the "father", the previous "father" became the "grandfather" and the previous "grandfather" was deleted. But one could set up GDGs with more than 3 generations and some applications used GDGs to collect data from several sources and feed the information to one program - each collecting program created a new generation of the file and the final program read the whole group as a single sequential file (by not specifying a generation in the JCL).

Modern versions of MVS (e.g., z/OS) use datasets as containers for Unix filesystems along with facilities for partially integrating them. That is, Unix programs using fopen() can access an MVS dataset and a user can allocate a Unix file as though it were a dataset, with some restrictions. The Hierarchical File System (HFS) (not to be confused with Apple's Hierarchical File System) uses a unique type of dataset, while the newer z/OS File System (zFS) (not to be confused with Sun's ZFS) uses a VSAM Linear Data Set (LDS).

Programs running on network-connected computers (such as the AS/400) can use local data management interfaces to transparently create, manage, and access VSAM record-oriented files by using client-server products implemented according to Distributed Data Management Architecture (DDM). DDM is also the base architecture for the MVS DB2 server that implements Distributed Relational Database Architecture (DRDA).

In addition to new functionality that IBM added with releases and sub-releases of OS/VS2, IBM provided a number of free Incremental Change Releases (ICRs) and Selectable Units (SUs) and chargeable program products and field developed programs that IBM eventually bundled as part of z/OS. These include:

MVS has now evolved into z/OS; older MVS releases are no longer supported by IBM and, since 2007, only 64-bit z/OS releases are supported. z/OS supports running older 24-bit and 31-bit MVS applications alongside newer 64-bit applications.

MVS releases up to 3.8j (24-bit, released in 1981) were freely available and it is now possible to run the MVS 3.8j release in mainframe emulators for free.

MVS/370 is a generic term for all versions of the MVS operating system prior to MVS/XA. The System/370 architecture, at the time MVS was released, supported only 24-bit virtual addresses, so the MVS/370 operating system architecture is based on a 24-bit address. Because of this 24-bit address length, programs running under MVS/370 are each given 16 MB of contiguous virtual storage.

MVS/XA, or Multiple Virtual Storage/Extended Architecture, was a version of MVS that supported the 370-XA architecture, which expanded addresses from 24 bits to 31 bits, providing a 2 gigabyte addressable memory area. It also supported a 24-bit legacy addressing mode for older 24-bit applications (i.e. those that stored a 24-bit address in the lower 24 bits of a 32-bit word and utilized the upper 8 bits of that word for other purposes).

MVS/ESA: MVS Enterprise System Architecture. Version of MVS, first introduced as MVS/SP Version 3 in February 1988. Replaced by/renamed as OS/390 late 1995 and subsequently as z/OS.

MVS/ESA OpenEdition: upgrade to Version 4 Release 3 of MVS/ESA announced February 1993 with support for POSIX and other standards. While the initial release only had National Institute of Standards and Technology (NIST) certification for Federal Information Processing Standard (FIPS) 151 compliance, subsequent releases were certified at higher levels and by other organizations, e.g. X/Open and its successor, The Open Group. It included about 1 million new lines of code, which provide an API shell, utilities, and an extended user interface. Works with a hierarchical file system provided by DFSMS (Data Facility System Managed Storage). The shell and utilities are based on Mortice Kerns' InterOpen products. Independent specialists estimate that it was over 80% open systems-compliant—more than most Unix systems. DCE2 support announced February 1994, and many application development tools in March 1995. From mid 1995, as all of the open features became a standard part of vanilla MVS/ESA SP Version 5 Release 1, IBM stopped distinguishing OpenEdition from the operating system. Under OS/390 V2R6 it became UNIX System Services, and has kept that name under z/OS.

In late 1995 IBM bundled MVS with several program products and changed the name from MVS/ESA to OS/390.

The current level of MVS is marketed as z/OS.

Japanese mainframe manufacturers Fujitsu and Hitachi both repeatedly and illegally obtained IBM's MVS source code and internal documentation in one of the 20th century's most famous cases of industrial espionage. Fujitsu relied heavily on IBM's code in its MSP mainframe operating system, and likewise Hitachi did the same for its VOS3 operating system. MSP and VOS3 were heavily marketed in Japan, where they still hold a substantial share of the mainframe installed base, but also to some degree in other countries, notably Australia. Even IBM's bugs and documentation misspellings were faithfully copied. IBM cooperated with the U.S. Federal Bureau of Investigation in a sting operation, reluctantly supplying Fujitsu and Hitachi with proprietary MVS and mainframe hardware technologies during the course of multi-year investigations culminating in the early 1980s—investigations which implicated senior company managers and even some Japanese government officials. Amdahl, however, was not involved in Fujitsu's theft of IBM's intellectual property. Any communications from Amdahl to Fujitsu were through "Amdahl Only Specifications" which were scrupulously cleansed of any IBM IP or any references to IBM's IP.

Subsequent to the investigations, IBM reached multimillion-dollar settlements with both Fujitsu and Hitachi, collecting substantial fractions of both companies' profits for many years. Reliable reports indicate that the settlements exceeded US$500,000,000.

The three companies have long since amicably agreed to many joint business ventures. For example, in 2000 IBM and Hitachi collaborated on developing the IBM z900 mainframe model.

Because of this historical copying, MSP and VOS3 are properly classified as "forks" of MVS, and many third party software vendors with MVS-compatible products were able to produce MSP- and VOS3-compatible versions with little or no modification.

When IBM introduced its 64-bit z/Architecture mainframes in the year 2000, IBM also introduced the 64-bit z/OS operating system, the direct successor to OS/390 and MVS. Fujitsu and Hitachi opted not to license IBM's z/Architecture for their quasi-MVS operating systems and hardware systems, and so MSP and VOS3, while still nominally supported by their vendors, maintain most of MVS's 1980s architectural limitations to the present day. Since z/OS still supports MVS-era applications and technologies—indeed, z/OS still contains most of MVS's code, albeit greatly enhanced and improved over decades of evolution—applications (and operational procedures) running on MSP and VOS3 can move to z/OS much more easily than to other operating systems.





</doc>
<doc id="19652" url="https://en.wikipedia.org/wiki?curid=19652" title="Monoid">
Monoid

In abstract algebra, a branch of mathematics, a monoid is an algebraic structure with a single associative binary operation and an identity element.

Monoids are semigroups with identity. They occur in several branches of mathematics. 

For example, the functions from a set into itself form a monoid with respect to function composition. More generally, in category theory, the morphisms of an object to itself form a monoid, and, conversely, a monoid may be viewed as a category with a single object. 

In computer science and computer programming, the set of strings built from a given set of characters is a free monoid. Transition monoids and syntactic monoids are used in describing finite-state machines. Trace monoids and history monoids provide a foundation for process calculi and concurrent computing.

In theoretical computer science, the study of monoids is fundamental for automata theory (Krohn–Rhodes theory), and formal language theory (star height problem).

See Semigroup for the history of the subject, and some other general properties of monoids.

Suppose that "S" is a set and • is some binary operation , then "S" with • is a monoid if it satisfies the following two axioms:


In other words, a monoid is a semigroup with an identity element. It can also be thought of as a magma with associativity and identity. The identity element of a monoid is unique. For this reason the identity is regarded as a constant, i. e. 0-ary (or nullary) operation. The monoid therefore is characterized by specification of the triple ("S", • , "e").

Depending on the context, the symbol for the binary operation may be omitted, so that the operation is denoted by juxtaposition; for example, the monoid axioms may be written formula_1 and formula_2. This notation does not imply that it is numbers being multiplied.

A monoid in which each element has an inverse is a group.

A submonoid of a monoid is a subset "N" of "M" that is closed under the monoid operation and contains the identity element "e" of "M". Symbolically, "N" is a submonoid of "M" if , whenever , and . "N" is thus a monoid under the binary operation inherited from "M".

A subset "S" of "M" is said to be a generator of "M" if "M" is the smallest set containing "S" that is closed under the monoid operation, or equivalently "M" is the result of applying the finitary closure operator to "S". If there is a generator of "M" that has finite cardinality, then "M" is said to be finitely generated. Not every set "S" will generate a monoid, as the generated structure may lack an identity element.

A monoid whose operation is commutative is called a commutative monoid (or, less commonly, an abelian monoid). Commutative monoids are often written additively. Any commutative monoid is endowed with its algebraic preordering , defined by if there exists "z" such that . An order-unit of a commutative monoid "M" is an element "u" of "M" such that for any element "x" of "M", there exists "v" in the set generated by "u" such that . This is often used in case "M" is the positive cone of a partially ordered abelian group "G", in which case we say that "u" is an order-unit of "G".

A monoid for which the operation is commutative for some, but not all elements is a trace monoid; trace monoids commonly occur in the theory of concurrent computation.


In a monoid, one can define positive integer powers of an element "x" : "x" = "x", and "x" = "x" • ... • "x" ("n" times) for "n" > 1 . The rule of powers "x" = "x" • "x" is obvious.

From the definition of a monoid, one can show that the identity element "e" is unique. Then, for any "x", one can set "x" = "e" and the rule of powers is still true with nonnegative exponents.

It is possible to define invertible elements: an element "x" is called invertible if there exists an element "y" such that and . The element "y" is called the inverse of "x". If "y" and "z" are inverses of "x", then by associativity . Thus inverses, if they exist, are unique.

If "y" is the inverse of "x", one can define negative powers of "x" by setting and ("n" times) for . And the rule of exponents is still verified for all integers . This is why the inverse of "x" is usually written . The set of all invertible elements in a monoid "M", together with the operation •, forms a group. In that sense, every monoid contains a group (possibly only the trivial group consisting of only the identity).

However, not every monoid sits inside a group. For instance, it is perfectly possible to have a monoid in which two elements "a" and "b" exist such that holds even though "b" is not the identity element. Such a monoid cannot be embedded in a group, because in the group we could multiply both sides with the inverse of "a" and would get that , which isn't true. A monoid has the cancellation property (or is cancellative) if for all "a", "b" and "c" in "M", always implies and always implies . A commutative monoid with the cancellation property can always be embedded in a group via the Grothendieck construction. That is how the additive group of the integers (a group with operation +) is constructed from the additive monoid of natural numbers (a commutative monoid with operation + and cancellation property). However, a non-commutative cancellative monoid need not be embeddable in a group.

If a monoid has the cancellation property and is "finite", then it is in fact a group. Proof: Fix an element "x" in the monoid. Since the monoid is finite, for some . But then, by cancellation we have that where "e" is the identity. Therefore, , so "x" has an inverse.

The right- and left-cancellative elements of a monoid each in turn form a submonoid (i.e. obviously include the identity and not so obviously are closed under the operation). This means that the cancellative elements of any commutative monoid can be extended to a group.

It turns out that requiring the cancellative property in a monoid is not required to perform the Grothendieck construction – commutativity is sufficient. However, if the original monoid has an absorbing element then its Grothendieck group is the trivial group. Hence the homomorphism is, in general, not injective.

An inverse monoid is a monoid where for every "a" in "M", there exists a unique "a" in "M" such that and . If an inverse monoid is cancellative, then it is a group.

In the opposite direction, a zerosumfree monoid is an additively written monoid in which implies that and : equivalently, that no element other than zero has an additive inverse.

Let "M" be a monoid, with the binary operation denoted by • and the identity element denoted by "e". Then a (left) "M"-act (or left act over "M") is a set "X" together with an operation which is compatible with the monoid structure as follows:
This is the analogue in monoid theory of a (left) group action. Right "M"-acts are defined in a similar way. A monoid with an act is also known as an operator monoid. Important examples include transition systems of semiautomata. A transformation semigroup can be made into an operator monoid by adjoining the identity transformation.

A homomorphism between two monoids and is a function such that
where "e" and "e" are the identities on "M" and "N" respectively. Monoid homomorphisms are sometimes simply called monoid morphisms.

Not every semigroup homomorphism between monoids is a monoid homomorphism, since it may not map the identity to the identity of the target monoid, even though the identity is the identity of the image of homomorphism. So, a monoid homomorphism is a semigroup homomorphism between monoids that maps the identity of the first monoid to the identity of the second monoid (the latter condition must not be omitted).

In contrast, a semigroup homomorphism between groups is always a group homomorphism, as it necessarily preserves the identity (because, in a group, the identity is the only element such that ).

A bijective monoid homomorphism is called a monoid isomorphism. Two monoids are said to be isomorphic if there is a monoid isomorphism between them.

Monoids may be given a presentation, much in the same way that groups can be specified by means of a group presentation. One does this by specifying a set of generators Σ, and a set of relations on the free monoid Σ. One does this by extending (finite) binary relations on Σ to monoid congruences, and then constructing the quotient monoid, as above.

Given a binary relation , one defines its symmetric closure as . This can be extended to a symmetric relation by defining if and only if and for some strings with . Finally, one takes the reflexive and transitive closure of "E", which is then a monoid congruence.

In the typical situation, the relation "R" is simply given as a set of equations, so that formula_13. Thus, for example,
is the equational presentation for the bicyclic monoid, and

is the plactic monoid of degree 2 (it has infinite order). Elements of this plactic monoid may be written as formula_16 for integers "i", "j", "k", as the relations show that "ba" commutes with both "a" and "b".

Monoids can be viewed as a special class of categories. Indeed, the axioms required of a monoid operation are exactly those required of morphism composition when restricted to the set of all morphisms whose source and target is a given object. That is,
More precisely, given a monoid , one can construct a small category with only one object and whose morphisms are the elements of "M". The composition of morphisms is given by the monoid operation •.

Likewise, monoid homomorphisms are just functors between single object categories. So this construction gives an equivalence between the category of (small) monoids Mon and a full subcategory of the category of (small) categories Cat. Similarly, the category of groups is equivalent to another full subcategory of Cat.

In this sense, category theory can be thought of as an extension of the concept of a monoid. Many definitions and theorems about monoids can be generalised to small categories with more than one object. For example, a quotient of a category with one object is just a quotient monoid.

Monoids, just like other algebraic structures, also form their own category, Mon, whose objects are monoids and whose morphisms are monoid homomorphisms.

There is also a notion of monoid object which is an abstract definition of what is a monoid in a category. A monoid object in Set is just a monoid.

In computer science, many abstract data types can be endowed with a monoid structure. In a common pattern, a sequence of elements of a monoid is "folded" or "accumulated" to produce a final value. For instance, many iterative algorithms need to update some kind of "running total" at each iteration; this pattern may be elegantly expressed by a monoid operation. Alternatively, the associativity of monoid operations ensures that the operation can be parallelized by employing a prefix sum or similar algorithm, in order to utilize multiple cores or processors efficiently.

Given a sequence of values of type "M" with identity element formula_17 and associative operation formula_18, the "fold" operation is defined as follows:

In addition, any data structure can be 'folded' in a similar way, given a serialization of its elements. For instance, the result of "folding" a binary tree might differ depending on pre-order vs. post-order tree traversal.

An application of monoids in computer science is so-called MapReduce programming model (see Encoding Map-Reduce As A Monoid With Left Folding). MapReduce, in computing, consists of two or three operations. Given a dataset, "Map" consists of mapping arbitrary data to elements of a specific monoid. "Reduce" consists of folding those elements, so that in the end we produce just one element.

For example, if we have a multiset, in a program it is represented as a map from elements to their numbers. Elements are called keys in this case. The number of distinct keys may be too big, and in this case the multiset is being sharded. To finalize reduction properly, the "Shuffling" stage regroups the data among the nodes. If we do not need this step, the whole Map/Reduce consists of mapping and reducing; both operation are parallelizable, the former due to its element-wise nature, the latter due to associativity of the monoid.

A complete monoid is a commutative monoid equipped with an infinitary sum operation formula_20 for any index set such that:

and

A continuous monoid is an ordered commutative monoid in which every directed set has a least upper bound compatible with the monoid operation:

These two concepts are closely related: a continuous monoid is a complete monoid in which the infinitary sum may be defined as

where the supremum on the right runs over all finite subsets of and each sum on the right is a finite sum in the monoid.





</doc>
<doc id="19653" url="https://en.wikipedia.org/wiki?curid=19653" title="May 31">
May 31





</doc>
<doc id="19654" url="https://en.wikipedia.org/wiki?curid=19654" title="May 30">
May 30





</doc>
<doc id="19655" url="https://en.wikipedia.org/wiki?curid=19655" title="May 23">
May 23





</doc>
<doc id="19659" url="https://en.wikipedia.org/wiki?curid=19659" title="May 16">
May 16




</doc>
<doc id="19660" url="https://en.wikipedia.org/wiki?curid=19660" title="May 22">
May 22

[1967]] – Egypt closes the Straits of Tiran to Israeli shipping.



</doc>
<doc id="19662" url="https://en.wikipedia.org/wiki?curid=19662" title="Mean value theorem">
Mean value theorem

In mathematics, the mean value theorem states, roughly, that for a given planar arc between two endpoints, there is at least one point at which the tangent to the arc is parallel to the secant through its endpoints.

This theorem is used to prove statements about a function on an interval starting from local hypotheses about derivatives at points of the interval.

More precisely, if formula_1 is a continuous function on the closed interval formula_2 and differentiable on the open interval formula_3, then there exists a point formula_4 in formula_3 such that the tangent at c is parallel to the secant line through the endpoints formula_6 and formula_7, that is,
It is one of the most important results in real analysis.

A special case of this theorem was first described by Parameshvara (1370–1460), from the Kerala School of Astronomy and Mathematics in India, in his commentaries on Govindasvāmi and Bhāskara II. A restricted form of the theorem was proved by Michel Rolle in 1691; the result was what is now known as Rolle's theorem, and was proved only for polynomials, without the techniques of calculus. The mean value theorem in its modern form was stated and proved by Augustin Louis Cauchy in 1823.

Let formula_8 be a continuous function on the closed interval formula_2 , and differentiable on the open interval formula_3, where <math>a. Then there exists some formula_4 in formula_3 such that

The mean value theorem is a generalization of Rolle's theorem, which assumes formula_14, so that the right-hand side above is zero.

The mean value theorem is still valid in a slightly more general setting. One only needs to assume that formula_8 is continuous on formula_2 , and that for every formula_17 in formula_3 the limit

exists as a finite number or equals formula_20 or formula_21 . If finite, that limit equals formula_22 . An example where this version of the theorem applies is given by the real-valued cube root function mapping formula_23 , whose derivative tends to infinity at the origin.

Note that the theorem, as stated, is false if a differentiable function is complex-valued instead of real-valued. For example, define formula_24 for all real formula_17 . Then
while formula_27 for any real formula_17 .

These formal statements are also known as Lagrange's Mean Value Theorem.

The expression formula_29 gives the slope of the line joining the points formula_6 and formula_7 , which is a chord of the graph of formula_1 , while formula_22 gives the slope of the tangent to the curve at the point formula_34 . Thus the mean value theorem says that given any chord of a smooth curve, we can find a point lying between the end-points of the chord such that the tangent at that point is parallel to the chord. The following proof illustrates this idea.

Define formula_35 , where formula_36 is a constant. Since formula_1 is continuous on formula_2 and differentiable on formula_3 , the same is true for formula_40 . We now want to choose formula_36 so that formula_40 satisfies the conditions of Rolle's theorem. Namely

By Rolle's theorem, since formula_40 is differentiable and formula_45 , there is some formula_4 in formula_3 for which formula_48 , and it follows from the equality formula_35 that,

Assume that "f" is a continuous, real-valued function, defined on an arbitrary interval "I" of the real line. If the derivative of "f" at every interior point of the interval "I" exists and is zero, then "f" is constant in the interior.

Proof: Assume the derivative of "f" at every interior point of the interval "I" exists and is zero. Let ("a", "b") be an arbitrary open interval in "I". By the mean value theorem, there exists a point "c" in ("a","b") such that

This implies that "f"("a") = "f"("b"). Thus, "f" is constant on the interior of "I" and thus is constant on "I" by continuity. (See below for a multivariable version of this result.)

Remarks: 

Cauchy's mean value theorem, also known as the extended mean value theorem, is a generalization of the mean value theorem. It states: If functions "f" and "g" are both continuous on the closed interval ["a", "b"], and differentiable on the open interval ("a", "b"), then there exists some "c" ∈ ("a", "b"), such that

Of course, if and if , this is equivalent to:

Geometrically, this means that there is some tangent to the graph of the curve

which is parallel to the line defined by the points ("f"("a"), "g"("a")) and ("f"("b"), "g"("b")). However Cauchy's theorem does not claim the existence of such a tangent in all cases where ("f"("a"), "g"("a")) and ("f"("b"), "g"("b")) are distinct points, since it might be satisfied only for some value "c" with , in other words a value for which the mentioned curve is stationary; in such points no tangent to the curve is likely to be defined at all. An example of this situation is the curve given by

which on the interval [−1, 1] goes from the point (−1, 0) to (1, 0), yet never has a horizontal tangent; however it has a stationary point (in fact a cusp) at .

Cauchy's mean value theorem can be used to prove l'Hôpital's rule. The mean value theorem is the special case of Cauchy's mean value theorem when .

The proof of Cauchy's mean value theorem is based on the same idea as the proof of the mean value theorem.



Assume that formula_59 and formula_60 are differentiable functions on formula_3 that are continuous on formula_2. Define

There exists formula_64 such that formula_65.

Notice that
and if we place formula_67, we get Cauchy's mean value theorem. If we place formula_67 and formula_69 we get Lagrange's mean value theorem.

The proof of the generalization is quite simple: each of formula_70 and formula_71 are determinants with two identical rows, hence formula_72. The Rolle's theorem implies that there exists formula_73 such that formula_74.

The mean value theorem generalizes to real functions of multiple variables. The trick is to use parametrization to create a real function of one variable, and then apply the one-variable theorem.

Let formula_75 be an open convex subset of formula_76 , and let formula_77 be a differentiable function. Fix points formula_78 , and define formula_79 . Since formula_40 is a differentiable function in one variable, the mean value theorem gives:

for some formula_4 between 0 and 1. But since formula_83 and formula_84 , computing formula_85 explicitly we have:

where formula_87 denotes a gradient and formula_88 a dot product. Note that this is an exact analog of the theorem in one variable (in the case formula_89 this "is" the theorem in one variable). By the Cauchy–Schwarz inequality, the equation gives the estimate:

In particular, when the partial derivatives of formula_1 are bounded, formula_1 is Lipschitz continuous (and therefore uniformly continuous). Note that formula_1 is not assumed to be continuously differentiable or continuous on the closure of formula_75 . However, in order to use the chain rule to compute formula_95, we really do need to know that formula_1 is differentiable on formula_75; the existence of the formula_98 and formula_99 partial derivatives by itself is not sufficient for the theorem to be true .

As an application of the above, we prove that formula_1 is constant if formula_75 is open and connected and every partial derivative of formula_1 is 0. Pick some point formula_103 , and let formula_104 . We want to show formula_105 for every formula_106 . For that, let formula_107 . Then "E" is closed and nonempty. It is open too: for every formula_108 ,

for every formula_110 in some neighborhood of formula_17 . (Here, it is crucial that formula_17 and formula_110 are sufficiently close to each other.) Since formula_75 is connected, we conclude formula_115 .

The above arguments are made in a coordinate-free manner; hence, they generalize to the case when formula_75 is a subset of a Banach space.

There is no exact analog of the mean value theorem for vector-valued functions.

In "Principles of Mathematical Analysis," Rudin gives an inequality which can be applied to many of the same situations to which the mean value theorem is applicable in the one dimensional case:

Theorem. "For a continuous vector-valued function formula_117 differentiable on formula_3, there exists formula_119 such that formula_120."

Jean Dieudonné in his classic treatise "Foundations of Modern Analysis" discards the mean value theorem and replaces it by mean inequality as the proof is not constructive and one cannot find the mean value and in applications one only needs mean inequality. Serge Lang in "Analysis I "uses the mean value theorem, in integral form, as an instant reflex but this use requires the continuity of the derivative. If one uses the Henstock–Kurzweil integral one can have the mean value theorem in integral form without the additional assumption that derivative should be continuous as every derivative is Henstock–Kurzweil integrable. The problem is roughly speaking the following: If "f" : "U" → R is a differentiable function (where "U" ⊂ R is open) and if "x" + "th", "x, h" ∈ R, "t" ∈ [0, 1] is the line segment in question (lying inside "U"), then one can apply the above parametrization procedure to each of the component functions "f" ("i" = 1, ..., "m") of "f" (in the above notation set "y" = "x" + "h"). In doing so one finds points "x" + "th" on the line segment satisfying

But generally there will not be a "single" point "x" + "t*h" on the line segment satisfying

for all "i" "simultaneously". For example, define:

Then formula_124, but formula_125 and formula_126 are never simultaneously zero as formula_17 ranges over formula_128.

However a certain type of generalization of the mean value theorem to vector-valued functions is obtained as follows: Let "f" be a continuously differentiable real-valued function defined on an open interval "I", and let "x" as well as "x" + "h" be points of "I". The mean value theorem in one variable tells us that there exists some "t*" between 0 and 1 such that

On the other hand, we have, by the fundamental theorem of calculus followed by a change of variables,

Thus, the value "f′"("x" + "t*h") at the particular point "t*" has been replaced by the mean value

This last version can be generalized to vector valued functions:

Proof. Let "f", ..., "f" denote the components of "f" and define:

Then we have

The claim follows since "Df" is the matrix consisting of the components formula_135

Proof. Let "u" in R denote the value of the integral
Now we have (using the Cauchy–Schwarz inequality):
Now cancelling the norm of "u" from both ends gives us the desired inequality.

Proof. From Lemma 1 and 2 it follows that

Let "f" : ["a", "b"] → R be a continuous function. Then there exists "c" in ["a", "b"] such that

Since the mean value of "f" on ["a", "b"] is defined as

we can interpret the conclusion as "f" achieves its mean value at some "c" in ("a", "b").

In general, if "f" : ["a", "b"] → R is continuous and "g" is an integrable function that does not change sign on ["a", "b"], then there exists "c" in ("a", "b") such that

Suppose "f" : ["a", "b"] → R is continuous and "g" is a nonnegative integrable function on ["a", "b"]. By the extreme value theorem, there exists "m" and "M" such that for each "x" in ["a", "b"], formula_144 and formula_145. Since "g" is nonnegative,

Now let

If formula_148, we're done since

means

so for any "c" in ("a", "b"),

If "I" ≠ 0, then

By the intermediate value theorem, "f" attains every value of the interval ["m", "M"], so for some "c" in ["a", "b"]

that is,

Finally, if "g" is negative on ["a", "b"], then

and we still get the same result as above.

QED

There are various slightly different theorems called the second mean value theorem for definite integrals. A commonly found version is as follows:

Here formula_157 stands for formula_158, the existence of which follows from the conditions. Note that it is essential that the interval ("a", "b"] contains "b". A variant not having this requirement is:

If the function formula_75 returns a multi-dimensional vector, then the MVT for integration is not true, even if the domain of formula_75 is also multi-dimensional.

For example, consider the following 2-dimensional function defined on an formula_162-dimensional cube:

Then, by symmetry it is easy to see that the mean value of formula_75 over its domain is (0,0):

However, there is no point in which formula_166, because formula_167 everywhere.

Let "X" and "Y" be non-negative random variables such that E["X"] < E["Y"] < ∞ and formula_168 (i.e. "X" is smaller than "Y" in the usual stochastic order). Then there exists an absolutely continuous non-negative random variable "Z" having probability density function

Let "g" be a measurable and differentiable function such that E["g"("X")], E["g"("Y")] < ∞, and let its derivative "g′" be measurable and Riemann-integrable on the interval ["x", "y"] for all "y" ≥ "x" ≥ 0. Then, E["g′"("Z")] is finite and

As noted above, the theorem does not hold for differentiable complex-valued functions. Instead, a generalization of the theorem is stated such:

Let "f" : Ω → C be a holomorphic function on the open convex set Ω, and let "a" and "b" be distinct points in Ω. Then there exist points "u", "v" on "L" (the line segment from "a" to "b") such that

Where Re() is the Real part and Im() is the Imaginary part of a complex-valued function.




</doc>
<doc id="19664" url="https://en.wikipedia.org/wiki?curid=19664" title="Mallow">
Mallow

Mallow or mallows may refer to:









</doc>
<doc id="19665" url="https://en.wikipedia.org/wiki?curid=19665" title="Marc Bloch">
Marc Bloch

Marc Léopold Benjamin Bloch (; ; 6 July 1886 – 16 June 1944) was a French historian. A founding member of the Annales School of French social history, he specialised in medieval history and published widely on Medieval France over the course of his career. As an academic, he worked at the University of Strasbourg (1920 to 1936), the University of Paris (1936 to 1939), and the University of Montpellier (1941 to 1944).

Born in Lyon to an Alsatian Jewish family, Bloch was raised in Paris, where his father—the classical historian Gustave Bloch—worked at Sorbonne University. Bloch was educated at various Parisian lycées and the École Normale Supérieure, and from an early age was affected by the anti-semitism of the Dreyfus affair. During the First World War, he served in the French Army and fought at the First Battle of the Marne and the Somme. After the war, he was awarded his doctorate in 1918 and became a lecturer at the University of Strasbourg. There, he formed an intellectual partnership with modern historian Lucien Febvre. Together they founded the Annales School and began publishing the journal "Annales d'histoire économique et sociale" in 1929. Bloch was a modernist in his historiographical approach, and repeatedly emphasised the importance of a multidisciplinary engagement towards history, particularly blending his research with that on geography, sociology and economics, which was his subject when he was offered a post at the University of Paris in 1936.

During the Second World War Bloch volunteered for service, and was a logistician during the Phoney War. Involved in the Battle of Dunkirk and spending a brief time in Britain, he unsuccessfully attempted to secure passage to the United States. Back in France, where his ability to work was curtailed by new anti-Semitic regulations, he applied for and received one of the few permits available allowing Jews to continue working in the French university system. He had to leave Paris, and complained that the Nazi German authorities looted his apartment and stole his books; he was also forced to relinquish his position on the editorial board of "Annales". Bloch worked in Montpellier until November 1942 when Germany invaded Vichy France. He then joined the French Resistance, acting predominantly as a courier and translator. In 1944, he was captured in Lyon and executed by firing squad. Several works—including influential studies like "The Historian's Craft" and "Strange Defeat"—were published posthumously.

His historical studies and his death as a member of the Resistance together made Bloch highly regarded by generations of post-war French historians; he came to be called "the greatest historian of all time". By the end of the 20th century, historians were making a more sober assessment of Bloch's abilities, influence, and legacy, arguing that there were flaws to his approach.

Marc Bloch was born in Lyon on 6 July 1886, one of two children to Gustave and Sarah Bloch, née Ebstein. Bloch's family were Alsatian Jews: secular, liberal and loyal to the French Republic. They "struck a balance", says the historian Carole Fink, between both "fierce Jacobin patriotism and the antinationalism of the left". His family had lived in Alsace for five generations under French rule. In 1871, France was forced to cede the region to Germany following its defeat in the Franco-Prussian War. The year after Bloch's birth, his father was appointed professor of Roman History at the Sorbonne, and the family moved to Paris—"the glittering capital of the Third Republic". Marc had a brother, Louis Constant Alexandre, seven years his senior. The two were close, although Bloch later described Louis as being occasionally somewhat intimidating. The Bloch family lived at 72, Rue d'Alésia, in the 14th arrondissement of Paris. Gustave began teaching Marc history while he was still a boy, with a secular, rather than Jewish, education intended to prepare him for a career in professional French society. Bloch's later close collaborator, Lucien Febvre, visited the Bloch family at home in 1902; although the reason for Febvre's visit is now unknown, he later wrote of Bloch that "from this fleeting meeting, I have kept the memory of a slender adolescent with eyes brilliant with intelligence and timid cheeks—a little lost then in the radiance of his older brother, future doctor of great prestige".

Bloch's biographer Karen Stirling ascribed significance to the era in which Bloch was born: the middle of the French Third Republic, so "after those who had founded it and before the generation that would aggressively challenge it". When Bloch was nine-years-old, the Dreyfus affair broke out in France. As the first major display of political antisemitism in Europe, it was probably a formative event of Bloch's youth, along with, more generally, the atmosphere of "fin de siècle" Paris. Bloch was 11 when Émile Zola published "J'Accuse…!", his indictment of the French establishment's antisemitism and corruption. Bloch was greatly affected by the Dreyfus affair, but even more affected was nineteenth-century France generally, and his father's employer, the École Normale Supérieure, saw existing divides in French society reinforced in every debate. Gustave Bloch was closely involved in the Dreyfusard movement and his son agreed with the cause.

Bloch was educated at the prestigious Lycée Louis-le-Grand for three years, where he was consistently head of his class and won prizes in French, history, Latin and natural history. He passed his "baccalauréat", in Letters and Philosophy, in July 1903, being graded "trés bien" (very good). The following year, he received a scholarship and undertook postgraduate study there for the École normale supérieure (ÉNS) (where his father had been appointed "maître de conferences" in 1887). His father had been nicknamed "le Méga" by his students at the ÉNS and the moniker "Microméga" was bestowed upon Bloch. Here he was taught history by Christian Pfister and Charles Seignobos, who led a relatively new school of historical thought which saw history as broad themes punctuated by tumultuous events. Another important influence on Bloch from this period was his father's contemporary, the sociologist Émile Durkheim, who pre-figured Bloch's own later emphasis on cross-disciplinary research. The same year, Bloch visited England; he later recalled being struck more by the number of homeless people on the Victoria Embankment than the new Entente Cordiale relationship between the two countries.

The Dreyfus affair had soured Bloch's views of the French Army, and he considered it laden with "snobbery, anti-semitism and anti-republicanism". National service had been made compulsory for all French adult males in 1905, with an enlistment term of two years. Bloch joined the 46th Infantry Regiment based at Pithiviers from 1905 to 1906.

By this time, changes were taking place in French academia. In Bloch's own speciality of history, attempts were being made at instilling a more scientific methodology. In other, newer departments such a sociology, efforts were made at establishing an independent identity. Bloch graduated in 1908 with degrees in both geography and history (Davies notes, given Bloch's later divergent interests, the significance of the two qualifications). He had a high respect for historical geography, then a speciality of French historiography, as practised by his tutor Vidal de la Blache whose "Tableau de la géographie" Bloch had studied at the ÉNS, and Lucien Gallois. Bloch applied unsuccessfully for a fellowship at the "Fondation Thiers". As a result, he travelled to Germany in 1909 where he studied demography under Karl Bücher in Leipzig and religion under Adolf Harnack in Berlin; he did not, however, particularly socialise with fellow students while in Germany. He returned to France the following year and again applied to the "Fondation", this time successfully. Bloch researched the medieval Île-de-France in preparation for his thesis. This research was Bloch's first focus on rural history. His parents had moved house and now resided at the Avenue d'Orleans, not far from Bloch's quarters.

Bloch's research at the Fondation—especially his research into the Capetian kings—laid the groundwork for his career. He began by creating maps of the Paris area illustrating where serfdom had thrived and where it had not. He also investigated the nature of serfdom, the culture of which, he discovered, was founded almost completely on custom and practice. His studies of this period formed Bloch into a mature scholar and first brought him into contact with other disciplines whose relevance he was to emphasise for most of his career. Serfdom as a topic was so broad that he touched on commerce, currency, popular religion, the nobility, as well as art, architecture and literature. His doctoral thesis—a study of 10th-century French serfdom—was titled "Rois et Serfs, un Chapitre d'Histoire Capétienne". Although it helped mould Bloch's ideas for the future, it did not, says Bryce Loyn, give any indication of the originality of thought that Bloch would later be known for, and was not vastly different to what others had written on the subject. Following his graduation, he taught at two lycées, first in Montpelier, a minor university town of 66,000 inhabitants. With Bloch working over 16 hours a week on his classes, there was little time for him to work on his thesis. He also taught at the University of Amiens. While there, he wrote a review of Febvre's first book, "Histoire de Franche-Comté". Bloch intended to turn his thesis into a book, but the First World War intervened.

Both Marc and Louis Bloch volunteered for service in the French Army. Although the Dreyfus Affair had soured Bloch's views of the French Army, he later wrote that his criticisms were only of the officers; he "had respect only for the men". Bloch was one of over 800 ÉNS students who enlisted; 239 were to be killed in action. On 2 August 1914 he was assigned to the 272nd Reserve Regiment. Within eight days he was stationed on the Belgian border where he fought in the Battle of the Meuse later that month. His regiment took part in the general retreat on the 25th, and the following day they were in Barricourt, in the Argonne. The march westward continued towards the River Marne—with a temporary recuperative halt in Termes—which they reached in early September. During the First Battle of the Marne, Bloch's troop was responsible for the assault and capture of Florent before advancing on La Gruerie. Bloch led his troop with shouts of "Forward the 18th!" They suffered heavy casualties: 89 men were either missing or known to be dead. Bloch enjoyed the early days of the war; like most of his generation, he had expected a short but glorious conflict. Gustave Bloch remained in France, wishing to be close to his sons at the front.

Except for two months in hospital followed by another three recuperating, he spent the war in the infantry; he joined as a sergeant and rose to become the head of his section. Bloch kept a war diary from his enlistment. Very detailed in the first few months, it rapidly became more general in its observations. However, says the historian Daniel Hochedez, Bloch was aware of his role as both a "witness and narrator" to events and wanted as detailed a basis for his historiographical understanding as possible. The historian Rees Davies notes that although Bloch served in the war with "considerable distinction", it had come at the worst possible time both for his intellectual development and his study of medieval society.

For the first time in his life, Bloch later wrote, he worked and lived alongside people he had never had close contact with before, such as shop workers and labourers, with whom he developed a great camaraderie. It was a completely different world to the one he was used to, being "a world where differences were settled not by words but by bullets". His experiences made him rethink his views on history, and influenced his subsequent approach to the world in general. He was particularly moved by the collective psychology he witnessed in the trenches. He later declared he knew of no better men than "the men of the Nord and the Pas de Calais" with whom he had spent four years in close quarters. His few references to the French generals were sparse and sardonic.

Apart from the Marne, Bloch fought at the battles of the Somme, the Argonne, and the final German assault on Paris. He survived the war, which he later described as having been an "honour" to have served through. He had, however, lost many friends and colleagues. Among the closest of them, all killed in action, were: Maxime David (died 1914), Antoine-Jules Bianconi (died 1915) and Ernest babut (died 1916). Bloch himself was wounded twice and decorated for courage, receiving the Croix de Guerre and the Légion d'Honneur. He had joined as a non-commissioned officer, received an officer's commission after the Marne, and had been promoted to warrant officer and finally a captain in the fuel service, ("Service des essences)" before the war ended. He was clearly, says Loyn, both a good and a brave soldier; he later wrote, "I know only one way to persuade a troop to brave danger: brave it yourself".

While on front-line service, Bloch contracted severe arthritis which required him to retire regularly to the thermal baths of Aix-les-Bains for treatment. He later remembered very little of the historical events he found himself in, writing only that his memories were "a discontinuous series of images, vivid in themselves, but badly arranged, like a reel of motion picture film containing some large gaps and some reversals of certain scenes". Bloch later described the war, in a detached style, as having been a "gigantic social experience, of unbelievable richness". For example, he had a habit of noting the different coloured smoke that different shells made — percussion bombs had black smoke, timed bombs were brown. He also remembered both the "friends killed at our side ... of the intoxication which had taken hold of us when we saw the enemy in flight". He also considered it to have been "four years of fighting idleness". Following the Armistice in November 1918, Bloch was demobilised on 13 March 1919.

The war was fundamental in re-arranging Bloch's approach to history, although he never acknowledged it as a turning point. In the years following the war, a disillusioned Bloch rejected the ideas and the traditions that had formed his scholarly training. He rejected the political and biographical history which up until that point was the norm, along with what the historian George Huppert has described as a "laborious cult of facts" that accompanied it. In 1920, with the opening of the University of Strasbourg, Bloch was appointed "chargé de cours" (assistant lecturer) of medieval history. Alsace-Lorraine had been returned to France with the Treaty of Versailles; the status of the region was a contentious political issue in Strasbourg, its capital, which had a large German population. Bloch, however, refused to take either side in the debate; indeed, he appears to have avoided politics entirely. Under Wilhelmine Germany, Strasbourg had rivalled Berlin as a centre for intellectual advancement, and the University of Strasbourg possessed the largest academic library in the world. Thus, says Stephan R. Epstein of the London School of Economics, "Bloch's unrivalled knowledge of the European Middle Ages was ... built on and around the French University of Strasbourg's inherited German treasures". Bloch also taught French to the few German students who were still at the Centre d'Études Germaniques at the University of Mainz during the Occupation of the Rhineland. He refrained from taking a public position when France occupied the Ruhr in 1923 over Germany's perceived failure to pay war reparations.

Bloch began working energetically, and later said that the most productive years of his life were spent at Strasbourg. In his teaching, his delivery was halting. His approach sometimes appeared cold and distant—caustic enough to be upsetting—but conversely, he could be also both charismatic and forceful. Durkheim died in 1917, but the movement he began against the "smugness" that pervaded French intellectual thinking continued. Bloch had been greatly influenced by him, as Durkheim also considered the connections between historians and sociologists to be greater than their differences. Not only did he openly acknowledge Durkheim's influence, but Bloch "repeatedly seized any opportunity to reiterate" it, according to R. C. Rhodes.

At Strasbourg he again met Febvre, who was now a leading historian of the 16th century. Modern and medieval seminars were adjacent to each other at Strasbourg, and attendance often overlapped. Their meeting has been called a "germinal event for 20th-century historiography", and they were to work closely together for the rest of Bloch's life. Febvre was some years older than Bloch and was probably a great influence on him. They lived in the same area of Strasbourg and became kindred spirits, often going on walking trips across the Vosges and other excursions.

Bloch's fundamental views on the nature and purpose of the study of history were established by 1920. That same year he defended, and subsequently published, his thesis. It was not as extensive a work as had been intended due to the war. There was a provision in French further education for doctoral candidates for whom the war had interrupted their research to submit only a small portion of the full-length thesis usually required. It sufficed, however, to demonstrate his credentials as a medievalist in the eyes of his contemporaries. He began publishing articles in Henri Berr's "Revue de Synthèse Historique". Bloch also published his first major work, "Les Rois Thaumaturges", which he later described as ""ce gros enfant"" (this big child). In 1928, Bloch was invited to lecture at the Institute for the Comparative Study of Civilizations in Oslo. Here he first expounded publicly his theories on total, comparative history: "it was a compelling plea for breaking out of national barriers that circumscribed historical research, for jumping out of geographical frameworks, for escaping from a world of artificiality, for making both horizontal and vertical comparisons of societies, and for enlisting the assistance of other disciplines".

His Oslo lecture, called "Towards a Comparative History of Europe", formed the basis of his next book, "Les Caractères Originaux de l'Histoire Rurale Française". In the same year he founded the historical journal "Annales" with Febvre. One of its aims was to counteract the administrative school of history, which Davies says had "committed the arch error of emptying history of human element". As Bloch saw it, it was his duty to correct that tendency. Both Bloch and Febvre were keen to refocus French historical scholarship on social rather than political history and to promote the use of sociological techniques. The journal avoided narrative history almost completely.

The inaugural issue of the "Annales" stated the editors' basic aims: to counteract the arbitrary and artificial division of history into periods, to re-unite history and social science as a single body of thought, and to promote the acceptance of all other schools of thought into historiography. As a result, the "Annales" often contained commentary on contemporary, rather than exclusively historical, events. Editing the journal led to Bloch forming close professional relationships with scholars in different fields across Europe. The "Annales" was the only academic journal to boast a preconceived methodological perspective. Neither Bloch nor Febvre wanted to present a neutral facade. During the decade it published it maintained a staunchly left-wing position. Henri Pirenne, a Belgian historian who wrote comparative history, closely supported the new journal. Before the war he had acted in an unofficial capacity as a conduit between French and German schools of historiography. Fernand Braudel—who was himself to become an important member of the Annales School after the Second World War—later described the journal's management as being a chief executive officer—Bloch—with a minister of foreign affairs—Febvre.

The comparative method allowed Bloch to discover instances of uniqueness within aspects of society, and he advocated it as a new kind of history. According to Bryce Lyon, Braudel and Febvre, "promising to perform all the burdensome tasks" themselves, asked Pirenne to become editor-in-chief of "Annales" to no avail. Pirenne remained a strong supporter, however, and had an article published in the first volume in 1929. He became close friends with both Bloch and Febvre. He was particularly influential on Bloch, who later said that Pirenne's approach should be the model for historians and that "at the time his country was fighting beside mine for justice and civilisation, wrote in captivity a history of Europe". The three men kept up a regular correspondence until Pirenne's death in 1935. In 1923, Bloch attended the inaugural meeting of the International Congress on Historical Studies (ICHS) in Brussels, which was opened by Pirenne. Bloch was a prolific reviewer for "Annales", and during the 1920s and 1930s he contributed over 700 reviews. These were both criticisms of specific works, but more generally, represented his own fluid thinking during this period. The reviews demonstrate the extent to which he shifted his thinking on particular subjects.

In 1930, both keen to make a move to Paris, Febvre and Bloch applied to the "École pratique des hautes études" for a position: both failed. Three years later Febvre was elected to the Collège de France. He moved to Paris, and in doing so, says Fink, became all the more aloof. This placed a strain on Bloch's and his relations, although they communicated regularly by letter and much of their correspondence has been preserved. In 1934, Bloch was invited to speak at the London School of Economics. There he met Eileen Power, R. H. Tawney and Michael Postan, among others. While in London, he was asked to write a section of the "Cambridge Economic History of Europe"; at the same time, he also attempted to foster interest in the "Annales" among British historians. He later told Febvre in some ways he felt he had a closer affinity with academic life in England than that of France. For example, in comparing the "Bibliothèque Nationale" with the British Museum, he said that

During this period he supported the Popular Front politically. Although he did not believe it would do any good, he signed Alain's—Émile Chartier's pseudonym—petition against Paul Boncour's Militarisation laws in 1935. While he was opposed to the growth of European fascism, he also objected to "demagogic appeals to the masses" to fight it, as the Communist Party was doing. Febvre and Bloch were both firmly on the left, although with different emphases. Febvre, for example, was more militantly Marxist than Bloch, while the latter criticised both the pacifist left and corporate trade unionism.

In 1934, Étienne Gilson sponsored Bloch's candidacy for a chair at the Collège de France. The College, says the historian Eugen Weber, was Bloch's "dream" appointment, although one never to be realised, as it was one of the few (possibly the only) institutions in France where personal research was central to lecturing. Camille Jullian had died the previous year, and his position was now available. While he had lived, Julian had wished for his chair to go to one of his students, Albert Grenier, and after his death, his colleagues generally agreed with him. However, Gilson proposed that not only should Bloch be appointed, but that the position be redesignated the study of comparative history. Bloch, says Weber, enjoyed and welcomed new schools of thought and ideas, but mistakenly believed the College should do so also. The College did not. The contest between Bloch and Grenier was not just the struggle for one post between two historians, but the path that historiography within the College would take for the next generation. To complicate the situation further, the country was in both political and economic crises, and the College had had its budget slashed by 10%. No matter who filled it, this made another new chair financially unviable. By the end of the year, and with further retirements, the College had lost four professors: it could replace only one, and Bloch was not appointed. Bloch personally suspected his failure was due to anti-Semitism and Jewish quotas. At the time, Febvre blamed it on a distrust of Bloch's approach to scholarship by the academic establishment, although Epstein has argued that this could not have been an over-riding fear as Bloch's next appointment indicated.

Henri Hauser retired from the Sorbonne in 1936, and his chair in economic history was up for appointment. Bloch—"distancing himself from the encroaching threat of Nazi Germany"—applied and was approved for his position. This was a more demanding position than the one he had applied for at the College. Weber has suggested Bloch was appointed because unlike at the College, he had not come into conflict with many faculty members. Weber researched the archives of the College in 1991 and discovered that Bloch had indicated an interest in working there as early as 1928, even though that would have meant him being appointed to the chair in numismatics rather than history. In a letter to the recruitment board written the same year, Bloch indicated that although he was not officially applying, he felt that "this kind of work (which he claimed to be alone in doing) deserves to have its place one day in our great foundation of free scientific research". H. Stuart Hughes says of Bloch's Sorbonne appointment: "In another country, it might have occasioned surprise that a medievalist like Bloch should have been named to such a chair with so little previous preparation. In France it was only to be expected: no one else was better qualified". His first lecture was on the theme of never-ending history, a process, a never to be finished thing. Davies says his years at the Sorbonne were to be "the most fruitful" of Bloch's career, and according to Epstein he was by now the most significant French historian of his age. In 1936, Friedman says he considered using Marx in his teachings, with the intention of bringing "some fresh air" into the Sorbonne.

The same year, Bloch and his family visited Venice, where they were chaperoned by the Italian historian Gino Luzzatto. During this period they were living in the Sèvres – Babylone area of Paris, next to the Hôtel Lutetia.

By now, "Annales" was being published six times a year to keep on top of current affairs, however, its "outlook was gloomy". In 1938, the publishers withdrew support and, experiencing financial hardship, the journal moved to cheaper offices, raised its prices and returned to publishing quarterly. Febvre increasingly opposed the direction Bloch wanted to take the journal. Febvre wanted it to be a "journal of ideas", whereas Bloch saw it as a vehicle for the exchange of information to different areas of scholarship.

By early 1939, war was known to be imminent. Bloch, in spite of his age, which automatically exempted him, had a reserve commission for the army holding the rank of captain. He had already been mobilised twice in false alarms. In August 1939, he and his wife Simonne intended to travel to the ICHS in Bucharest. In autumn 1939, just before the outbreak of war, Bloch published the first volume of "Feudal Society".

On 24 August 1939, at the age of 53, Bloch was mobilised for a third time, now as a fuel supply officer. He was responsible for the mobilisation of the French Army's massive motorised units. This involved him undertaking such a detailed assessment of the French fuel supply that he later wrote he was able to "count petrol tins and ration every drop" of fuel he obtained. During the first few months of the war, called the Phoney War, he was stationed in Alsace. He possessed none of the eager patriotism with which he had approached the First World War. Instead, Carole Fink suggests that because Bloch felt himself to have been discriminated against, he had "begun to distance himself intellectually and emotionally from his comrades and leaders". Back in Strasbourg, his main duty was the evacuation of civilians to behind the Maginot Line. Further transfers occurred, and Bloch was re-stationed to Molsheim, Saverne, and eventually to the 1st Army headquarters in Picardy, where he joined the Intelligence Department, in liaison with the British.

Bloch was largely bored between 1939 and May 1940 as he often had little work to do. To pass the time and occupy himself, he decided to begin writing a history of France. To this end, he purchased notebooks and began to work out a structure for the work. Although never completed, the pages he managed to write, "in his cold, poorly lit rooms", eventually became the kernel of "The Historian's Craft". At one point he expected to be invited to neutral Belgium to deliver a series of lectures in Liège. These never took place, however, disappointing Bloch very much; he had planned to speak on Belgian neutrality. He also turned down the opportunity to travel to Oslo as an attaché to the French Military Mission there. He was considered an excellent candidate for the position due to his fluency in Norwegian and knowledge of the country. Bloch considered it and came close to accepting; ultimately, though, it was too far from his family, whom he rarely saw enough of in any case. Some academics had escaped France for The New School in New York City, and the School also invited Bloch. He refused, possibly because of difficulties in obtaining visas: the US government would not grant visas to every member of his family.

In May 1940, the German army outflanked the French and forced them to withdraw. Facing capture in Rennes, Bloch disguised himself in civilian clothes and lived under German occupation for a fortnight before returning to his family at their country home in Fougères. He fought at the Battle of Dunkirk in May–June 1940 and was evacuated to England with the British Expeditionary Force on the requisitioned steamer MV "Royal Daffodil", which he later described as taking place "under golden skies coloured by the black and fawn smoke". Before the evacuation, Bloch ordered the immediate burning of fuel supplies. Although he could have remained in Britain, he chose to return to France the day he arrived because his family was still there.

Bloch felt that the French Army lacked the "esprit de corps" or "fervent fraternity" of the French Army in the First World War. He saw the French generals of 1940 as behaving as unimaginatively as Joseph Joffre had in the first war. He did not, however, believe that the earlier war was an indication of how the next would progress: "no two successive wars", he wrote in 1940, "are ever the same war".

To Bloch, France collapsed because her generals failed to capitalise on the best qualities humanity possessed—character and intelligence—because of their own "sluggish and intractable" progress since the First World War. He was horrified by the defeat which, Carole Fink has suggested, he saw as being worse, for both France and the world, than her previous defeats at Waterloo and Sedan. Bloch understood the reasons for France's sudden defeat: not in the rumours of British betrayal, communist fifth columns or fascist plots, but in her failure to motorise, and perhaps more importantly, her failure to understand what motorisation meant. He understood that it was the latter that allowed the French army to become bogged down in Belgium, and this had been compounded by the French army's slow retreat. He wrote in "Strange Defeat" that a fast, motorised retreat might have saved the army.

Two-thirds of France was occupied by Germany. Bloch, one of the only elderly academics to volunteer, was demobilised soon after Philippe Pétain's government signed the Armistice of 22 June 1940 forming Vichy France in the remaining southern-third of the country. Bloch moved south, where in January 1941, he applied for and received one of only ten exemptions to the ban on employing Jewish academics the Vichy government made. This was probably due to Bloch's pre-eminence in the field of history. He was allowed to work at the "University of Strasbourg-in-exile", the universities of Clermont-Ferrand, and Montpellier. The latter, further south, was beneficial to his wife's health, which was in decline. The dean of faculty at Montpellier was Augustin Fliche, an ecclesiastical historian of the Middle Ages, who, according to Weber, "made no secret of his antisemitism". He disliked Bloch further for having once given him a poor review. Fliche not only opposed Bloch's transfer to Montpellier but made his life uncomfortable when he was there. The Vichy government was attempting to promote itself as a return to traditional French values. Bloch condemned this as propaganda; the rural idyll that Vichy said it would return France to was impossible, he said, "because the idyllic, docile peasant life of the French right had never existed".

Bloch's professional relationship with Febvre was also under strain. The Nazis wanted French editorial boards to be stripped of Jews in accordance with German racial policies; Bloch advocated disobedience, while Febvre was passionate about the survival of "Annales" at any cost. He believed that it was worth making concessions to keep the journal afloat and to keep France's intellectual life alive. Bloch rejected out of hand any suggestion that he should, in his words, "fall into line". Febvre also asked Bloch to resign as joint-editor of the journal. Febvre feared that Bloch's involvement, as a Jew in Nazi-occupied France, would hinder the journal's distribution. Bloch, forced to accede, turned the "Annales" over to the sole editorship of Febvre, who then changed the journal's name to "Mélanges d'Histoire Sociale". Bloch was forced to write for it under the pseudonym Marc Fougères. The journal's bank account was also in Bloch's name; this too had to go. Henri Hauser supported Febvre's position, and Bloch was offended when Febvre intimated that Hauser had more to lose than both of them. This was because, whereas Bloch had been allowed to retain his research position, Hauser had not. Bloch interpreted Febvre's comment as implying that Bloch was not a victim. Bloch, alluding to his ethnicity, replied that the difference between them was that, whereas he feared for his children because of their Jewishness, Febvre's children were in no more danger than any other man in the country.

The Annalist historian André Burguière suggests Febvre did not really understand the position Bloch, or any French Jew, was in. Already damaged by this disagreement, Bloch's and Febvre's relationship declined further when the former had been forced to leave his library and papers in his Paris apartment following his move to Vichy. He had attempted to have them transported to his Creuse residence, but the Nazis—who had made their headquarters in the hotel next to Bloch's apartment—looted his rooms and confiscated his library in 1942. Bloch held Febvre responsible for the loss, believing he could have done more to prevent it.

Bloch's mother had recently died, and his wife was ill; furthermore, although he was permitted to work and live, he faced daily harassment. On 18 March 1941, Bloch made his will in Clermont-Ferrand. The Polish social historian Bronisław Geremek suggests that this document hints at Bloch in some way foreseeing his death, as he emphasised that nobody had the right to avoid fighting for their country. In March 1942 Bloch and other French academics such as Georges Friedmann and Émile Benveniste, refused to join or condone the establishment of the Union Générale des Israelites des France by the Vichy government, a group intended to include all Jews in France, both of birth and immigration.

In November 1942, as part of an operation known as Case Anton, the German Army crossed the demarcation line and occupied the territory previously under direct Vichy rule. This was the catalyst for Bloch's decision to join the French Resistance sometime between late 1942 and March 1943. Bloch was careful not to join simply because of his ethnicity or the laws that were passed against it. As Burguière has pointed out, and Bloch would have known, taking such a position would effectively "indict all Jews who did not join". Burguière has pinpointed Bloch's motive for joining the Resistance in his characteristic refusal to mince his words or play half a role. Bloch had previously expressed the view that "there can be no salvation where there is not some sacrifice". He sent his family away, and returned to Lyon to join the underground.

In spite of knowing a number of "francs-tireurs" around Lyon, Bloch still found it difficult to join them because of his age. Although the Resistance recruited heavily among university lecturers—and indeed, Bloch's alma mater, the École Normale Superieur, provided it with many members—he commented in exasperation to Simonne that he "didn't know it is so difficult to offer one's life". The French historian and philosopher François Dosse quotes a member of the "franc-tireurs" active with Bloch as later describing how "that eminent professor came to put himself at our command simply and modestly". Bloch used his professional and military skills on their behalf, writing propaganda for them and organising their supplies and materiel, becoming a regional organiser. Bloch also joined the "Mouvements Unis de la Résistance" (Unified Resistance Movement, or MUR), section R1, and edited the underground newsletter, "Cahiers Politique". He went under various pseudonyms: Arpajon, Chevreuse, Narbonne. Often on the move, Bloch used archival research as his excuse for travelling. The journalist-turned-resistance fighter Georges Altman later told how he knew Bloch as, although originally "a man, made for the creative silence of gentle study, with a cabinet full of books" was now "running from street to street, deciphering secret letters in some Lyonaisse Resistance garret"; all Bloch's notes were kept in code. For the first time, suggests Lyon, Bloch was forced to consider the role of the individual in history, rather than the collective; perhaps by then even realising he should have done so earlier.

Bloch was arrested at the Place de Pont, Lyon, during a major roundup by the Vichy "milice" on 8 March 1944, and handed over to Klaus Barbie of the Lyon Gestapo. Bloch was using the pseudonym "Maurice Blanchard", and in appearance was "an ageing gentleman, rather short, grey-haired, bespectacled, neatly dressed, holding a briefcase in one hand and a cane in the other". He was renting a room above a dressmakers on the rue des Quatre Chapeaux; the Gestapo raided the place the following day. It is possible Bloch had been denounced by a woman working in the shop. In any case, they found a radio transmitter and many papers. Bloch was imprisoned in Montluc prison, during which time his wife died. He was tortured with, for example, ice-cold baths which knocked him out. His ribs and a wrist were broken, which led to his being returned to his cell unconscious. He eventually caught bronchopneumonia and fell seriously ill. It was later claimed that he gave away no information to his interrogators, and while incarcerated taught French history to other inmates.

In the meantime, the allies had invaded Normandy on 6 June 1944. As a result, the Nazi regime was keen to evacuate and wanted to "liquidate their holdings" in France; this meant disposing of as many prisoners as they could. Between May and June 1944 around 700 prisoners were shot in scattered locations to avoid the risk of this becoming common knowledge and inviting Resistance reprisals around southern France. Among those killed was Bloch, one of a group of 26 Resistance prisoners picked out in Montluc and driven along the Saône towards Trévoux on the night of 16 June 1944. Driven to a field near Saint-Didier-de-Formans, they were shot by the Gestapo in groups of four. According to Lyon, Bloch spent his last moments comforting a 16-year-old beside him who was worried that the bullets might hurt. Bloch fell first, reputedly shouting ""Vive la France"" before being shot. A coup de grâce was delivered. One man managed to crawl away and later provided a detailed report of events; the bodies were discovered on 26 June. For some time Bloch's death was merely a "dark rumour" until it was confirmed to Febvre.

At his burial, his own words were read at the graveside. With them, Bloch proudly acknowledged his Jewish ancestry while denying religion in favour of his being foremost a Frenchman. He described himself as "a stranger to any formal religious belief as well as any supposed racial solidarity, I have felt myself to be, quite simply French before anything else". According to his instructions, no orthodox prayers were said over his grave, and on it was to be carved his epitaph "dilexi veritatem" ("I have loved the truth"). In 1977, his ashes were transferred from St-Didier to Fougeres and the gravestone was inscribed as he requested.

Febvre had not approved of Bloch's decision to join the Resistance, believing it to be a waste of his brain and talents, although, as Davies points out, "such a fate befell many other French intellectuals". Febvre continued publishing "Annales", ("if in a considerably modified form" comments Beatrice Gottlieb), dividing his time between his country château in the Franche-Comté and working at the École Normale in Paris. This caused some outrage, and, after liberation, when classes were returning to a degree of normality, he was booed by his students at the Sorbonne.

Bloch's first book was "L'Ile de France", published in 1913. A small book, Lyon calls it "light, readable and far from trivial", and showing the influence of H. J. Fleure in how Bloch combined discussion on geography, language and archaeology. It was translated into English in 1971. Davies says 1920's "Rois et Serfs", ("Kings and Serfs"), is a "long and rather meandering essay", although it had the potential to be Bloch's definitive monograph upon the single topic that "might have evoked his genius at his fullest", the transition from antiquity to the Middle Ages. Loyn also describes it as a "loose-knit monograph", and a program to move forward rather than a full-length academic text.

Bloch's most important early work—based on his doctoral dissertation—was published in 1924 as "Rois et Thaumaturges"; it was published in English as "The Royal Touch: Monarchy and Miracles in France and England" in 1973. Here he examined medieval belief in the royal touch, and the degree to which kings used such a belief for propaganda purposes. It was also the first example of Bloch's inter-disciplinary approach, as he used research from the fields of anthropology, medicine, psychology and iconography. It has been described as Bloch's first masterwork. It has a 500-page descriptive analysis of the medieval view of royalty effectively possessing supernatural powers. Verging on the antiquarian in his microscopic approach, and much influenced by the work of Raymond Crawfurd—who saw it as a "dubious if exotic" aspect of medicine, rather than history—Bloch makes diverse use of evidence from different disciplines and periods, assessing the King's Evil as far forward as the 19th century. The book had originally been inspired by discussions Bloch had with Louis, who acted as a medical consultant while his brother worked on it. Bloch concluded that the royal touch involved a degree of mass delusion among those who witnessed it.

1931 saw the publication of "Les caractéres originaux de l'histoire rurale francaise". In this—what Bloch called "mon petit livre"—he used both the traditional techniques of historiographical analysis(for example, scrutinising documents, manuscripts, accounts and rolls) and his newer, multi-faceted approach, with a heavy emphasis on maps as evidence. Bloch did not allow his new methods to detract from the former: he knew, says the historian Daniel Chirot, that the traditional methods of research were "the bread and butter of historical work. One had to do it well to be a minimally accepted historian". The first of "two classic works", says Hughes, and possibly his finest, studies the relationship between physical geographical location and the development of political institutions. Loyn has called Bloch's assessment of medieval French rural law great, but with the addendum that "he is not so good at describing ordinary human beings. He is no Eileen Power, and his peasants do not come to life as hers do". In this study, Chirot says Bloch "entirely abandoned the concept of linear history, and wrote, instead, from the present or near past into the distant past, and back towards the present". Febvre wrote the introduction to the book for its publication, and described the technique as "reading the past from the present", or what Bloch saw as starting with the known and moving into the unknown.

"La Société Féodale" was published in two volumes ("The Growth of Ties of Dependence", and "Social Classes and Political Organisation") in 1939, and was translated into English as "Feudal Society" in 1961. Bloch described the study as something of a sketch, although Stirling has called it his "most enduring work ... still a cornerstone of medieval curricula" in 2007 and representative of Bloch at the peak of his career. In "Feudal Society" he used research from the broadest range of disciplines to date to examine feudalism in the broadest possible way—most notably including a study of feudal Japan. He also compared areas where feudalism was imposed, rather than organically developed (such as England after the Norman conquest) and where it was never established (such as Scotland and Scandinavia). Bloch defined feudal society as, "from the peasants' point of view", politically fragmentary, where they are ruled by an aristocratic upper-class.

Daniel Chirot has described "The Royal Touch", "French Rural History" and "Feudal Society"—all of which concentrate on the French Middle Ages—as Bloch's most significant works. Conversely, his last two—"The Historian's Craft" and "Strange Defeat"—have been described as unrepresentative of his historical approach in that they discuss contemporary events in which Bloch was personally involved and without access to primary sources. "Strange Defeat" was uncompleted at the time of his death, and both were published posthumously in 1949. Davies has described "The Historian's Craft as" "beautifully sensitive and profound"; the book was written in response to his son, Étienne, asking his father, "what is history?". In his introduction, Bloch wrote to Febvre.

Likewise, "Strange Defeat", in the words of R. R. Davies, is a "damning and even intolerant analysis" of the long- and short-term reasons France fell in 1940. Bloch affirmed that the book was more than a personal memoir; rather, he intended it as a deposition and a testament. It contains—"uncomfortably and honestly"—Bloch's own self-appraisal:

Bloch emphasises failures in the French mindset: in the loss of morale of the soldiery and a failed education of the officers, effectively a failure of both character and intelligence on behalf of both. He condemns the "mania" for testing in education which, he felt, treated the testing as being an end in itself, draining generations of Frenchmen and Frenchwomen of originality and initiative or thirst for knowledge, and an "appreciation only of successful cheating and sheer luck". "Strange Defeat" has been called Bloch's autopsy of the France of the inter-war years.

A collection of essays was published in English in 1961 as "Land and Work in Medieval Europe". The long essay was a favoured medium of Bloch's, including, Davies says, "the famous essay on the water mill and the much-challenged one on the problem of gold in medieval Europe". In the former, Bloch saw one of the most important technological advances of the era, in the latter, the effective creation of a European currency. Although one of his best essays, according to Davies—"Liberté et servitude personelles au Moyen Age, particulement en France"—was not published when it could have been; this, he remarked was "an unpardonable omission".

Davies says Bloch was "no mean disputant" in historiographical debate, often reducing an opponent's argument to its most basic weaknesses. His approach was a reaction against the prevailing ideas within French historiography of the day which, when he was young, were still very much based on that of the German School, pioneered by Leopold von Ranke. Within French historiography this led to a forensic focus on administrative history as expounded by historians such as Ernest Lavisse. While he acknowledged his and his generation of historians' debt to their predecessors, he considered that they treated historical research as being little more meaningful than detective work. Bloch later wrote how, in his view, "There is no waste more criminal than that of erudition running ... in neutral gear, nor any pride more vainly misplaced than that in a tool valued as an end in itself". He believed it was wrong for historians to focus on the evidence rather than the human condition of whatever period they were discussing. Administrative historians, he said, understood every element of a government department without understanding anything of those who worked in it.

Bloch was very much influenced by Ferdinand Lot, who had already written comparative history, and by the work of Jules Michelet and Fustel de Coulanges with their emphasis on social history, Durkheim's sociological methodology, François Simiand's social economics, and Henri Bergson's philosophy of collectivism. Bloch's emphasis on using comparative history harked back to the Enlightenment, when writers such as Voltaire and Montesquieu decried the notion that history was a linear narrative of individuals and pushed for a greater use of philosophy in studying the past. Bloch condemned the "German-dominated" school of political economy, which he considered "analytically unsophisticated and riddled with distortions". Equally condemned were then-fashionable ideas on racial theories of national identity. Bloch believed that political history on its own could not explain deeper socioeconomics trends and influences.

Bloch did not see social history as being a separate field within historical research. Rather, he saw all aspects of history to be inherently a part of social history. By definition, all history was social history, an approach he and Febvre termed ""histoire totale"", not a focus on points of fact such as dates of battles, reigns, and changes of leaders and ministries, and a general confinement by the historian to what he can identify and verify. Bloch explained in a letter to Pirenne that, in Bloch's eyes, the historian's most important quality was the ability to be surprised by what he found—"I am more and more convinced of this", he said; "damn those of us who believe everything is normal!"Bloch identified two types of historical era: the generational era and the era of civilisation: these were defined by the speed with which they underwent change and development. In the latter type of period, which changed gradually, Bloch included physical, structural and psychological aspects of society, while the generational era could experience fundamental change over a relatively few generations. Bloch founded what modern French historians call the "regressive method" of historical scholarship. This method avoids the necessity of relying solely on historical documents as a source, by looking at the issues visible in later historical periods and drawing from them what they may have looked like centuries earlier. Davies says this was particularly useful in Bloch's study of village communities as "the strength of communal traditions often preserves earlier customs in a more or less fossilized state". Bloch studied peasant tools in museums, and in action, and discussed their use with the people themselves. He believed that in observing a plough or an annual harvest one was observing history, as more often than not both the technology and the technique were much the same as they had been hundreds of years earlier. However, the individuals themselves were not his focus, which was on "the collectivity, the community, the society". He wrote about the peasantry, rather than the individual peasant; says Lyon, "he roamed the provinces to become familiar with French agriculture over the long term, with the contours of peasant villages, with agrarian routine, its sounds and smells. Bloch claimed that both fighting alongside the peasantry in the war and his historical research into their history had shown him "the vigorous and unwearied quickness" of their minds.

Bloch described his area of study as the comparative history of European society and explained why he did not identify himself as a medievalist: "I refuse to do so. I have no interest in changing labels, nor in clever labels themselves, or those that are thought to be so." He did not leave a full study of his methodology, although it can be effectively reconstructed piecemeal. He believed that history was the "science of movement", but did not accept, for example, the aphorism that one could protect against the future by studying the past. His did not use a revolutionary approach to historiography; rather, he wished to combine the schools of thinking that preceded him into a new broad approach to history and, as he wrote in 1926, to bring to history "ce murmure qui n'était pas de la mort", ("the whisper that was not death'). He criticised what he called the "idol of the origins", where historians concentrate overly hard on the formation of something to the detriment of studying the thing itself.

Bloch's comparative history led him to tie his researches in with those of many other schools: social sciences, linguistics, philology, comparative literature, folklore, geography and agronomy. Similarly, he did not restrict himself to French history. At various points in his writings Bloch commented on medieval Corsican, Finnish, Japanese, Norwegian and Welsh history. R. R. Davies has compared Bloch's intelligence with what he calls that of "the Maitland of the 1890s", regarding his breadth of reading, use of language and multidisciplinary approach. Unlike Maitland, however, Bloch also wished to synthesise scientific history with narrative history. According to Stirling, he managed to achieve "an imperfect and volatile imbalance" between them. Bloch did not believe that it was possible to understand or recreate the past by the mere act of compiling facts from sources; rather, he described a source as a witness, "and like most witnesses", he wrote, "it rarely speaks until one begins to question it". Likewise, he viewed historians as detectives who gathered evidence and testimony, as "juges d'instruction" (examining magistrates) "charged with a vast enquiry of the past".

Bloch was not only interested in periods or aspects of history but in the importance of history as a subject, regardless of the period, of intellectual exercise. Davies writes, "he was certainly not afraid of repeating himself; and, unlike most English historians, he felt it his duty to reflect on the aims and purposes of history". Bloch considered it a mistake for the historian to confine himself overly rigidly to his own discipline. Much of his editorialising in "Annales" emphasised the importance of parallel evidence to be found in neighbouring fields of study, especially archaeology, ethnography, geography, literature, psychology, sociology, technology, air photography, ecology, pollen analysis and statistics. In Bloch's view, this provided not just for a broader field of study, but a far more comprehensive understanding of the past than would be possible from relying solely on historical sources. Bloch's favourite example of how technology impacts society was the watermill. This can be summed up as illustrating how it was known of but little used in the classical period; it became an economic necessity in the early medieval period; and finally, in the later Middle Ages it represented a scarce resource increasingly concentrated in the nobility's hands.

Bloch also emphasised the importance of geography in the study of history, and particularly in the study of rural history. He suggested that, fundamentally, they were the same subjects, although he criticised geographers for failing to take historical chronology or human agency into account. Using a farmer's field as an example, he described it as "fundamentally, a human work, built from generation to generation". Bloch also condemned the view that rural life was immobile. He believed that the Gallic farmer of the Roman period was inherently different to his 18th-century descendants, cultivating different plants, in a different way. He saw England and France's agricultural history as developing similarly, and, indeed, discovered an Enclosure Movement in France throughout the 15th, 16th and 17th centuries on the basis that it had been occurring in England in similar circumstances. Bloch also took a deep interest in the field of linguistics and their use of the comparative method. He believed that using the method in historical research could prevent the historian from ignoring the broader context in the course of his detailed local researches: "a simple application of the comparative method exploded the ethnic theories of historical institutions, beloved of so many German historians".

Bloch was not a tall man, being in height and an elegant dresser. Eugen Weber has described Bloch's handwriting as "impossible". He had expressive blue eyes, which could be "mischievous, inquisitive, ironic and sharp". Febvre later said that when he first met Bloch in 1902, he found a slender young man with "a timid face". Bloch was proud of his family's history of defending France: he later wrote, "My great-grandfather was a serving soldier in 1793; ... my father was one of the defenders of Strasbourg in 1870 ... I was brought up in the traditions of patriotism which found no more fervent champions than the Jews of the Alsatian exodus".

Bloch was a committed supporter of the Third Republic and politically left wing. He was not a Marxist, although he was impressed by Karl Marx himself, whom he thought was a great historian if possibly "an unbearable man" personally. He viewed contemporary politics as purely moral decisions to be made. He did not, however, let it enter into his work; indeed, he questioned the very idea of a historian studying politics. He believed that society should be governed by the young, and, although politically he was a moderate, he noted that revolutions generally promote the young over the old: "even the Nazis had done this, while the French had done the reverse, bringing to power a generation of the past". According to Epstein, following the First World War, Bloch presented a "curious lack of empathy and comprehension for the horrors of modern warfare", while John Lewis Gaddis has found Bloch's failure to condemn Stalinism in the 1930s "disturbing". Gaddis suggests that Bloch had ample evidence of Stalin's crimes and yet sought to shroud them in utilitarian calculations about the price of what he called 'progress'".

Although Bloch was very reserved—and later acknowledged that he had generally been old-fashioned and "timid" with women—he was good friends with Lucien Febvre and Christian Pfister. In July 1919 he married Simonne Vidal, a "cultivated and discreet, timid and energetic" woman, at a Jewish wedding. Her father was the "Inspecteur-Général de Ponts et Chaussées", and a very prosperous and influential man. Undoubtedly, says Friedman, his wife's family wealth allowed Bloch to focus on his research without having to depend on the income he made from it. Bloch was later to say he had found great happiness with her, and that he believed her to have also found it with him. They had six children together, four sons and two daughters. The eldest two were a daughter Alice, and a son, Étienne. As his father had done with him, Bloch took a great interest in his children's education, and regularly helped with their homework. He could, though, be "caustically critical" of his children, particularly Étienne. Bloch accused him in one of his wartime letters of having poor manners, being lazy and stubborn, and of being possessed occasionally by "evil demons". Regarding the facts of life, Bloch told Etienne to attempt always to avoid what Bloch termed "contaminated females".

Bloch was certainly agnostic, if not atheist, in matters of religion. His son Étienne later said of his father, "in his life as well as his writings not even the slightest trace of a supposed Jewish identity" can be found. "Marc Bloch was simply French". Some of his pupils believed him to be an Orthodox Jew, but Loyn says this is incorrect. While Bloch's Jewish roots were important to him, this was the result of the political tumult of the Dreyfuss years, said Loyn: that "it was only anti-semitism that made him want to affirm his Jewishness".

Bloch's brother Louis became a doctor, and eventually the head of the diphtheria section of the Hôpital des Enfants-Malades. Louis died prematurely in 1922. Their father died in March the following year. Following these deaths, Bloch took on responsibility for his ageing mother as well as his brother's widow and children. Eugen Weber has suggested that Bloch was probably a monomaniac who, in Bloch's own words, "abhorred falsehood". He also abhorred, as a result of both the Franco-Prussian war and more recently the First World War, German nationalism. This extended to that country's culture and scholarship, and is probably the reason he never debated with German historians. Indeed, in Bloch's later career, he rarely mentioned even those German historians with whom he must, professionally, have felt an affinity, such as Karl Lamprecht. Lyon says Lamprecht had denounced what he saw as the German obsession with political history and had focused on art and comparative history, thus "infuriat[ing] the "Rankianer"". Bloch once commented, on English historians, that "en Angleterre, rien qu'en Angleterre" ("in England, only England"). He was not, though, particularly critical of English historiography, and respected the long tradition of rural history in that country as well as more materially the government funding that went into historical research there.

It is possible, argues Weber, that had Bloch survived the war, he would have stood to be appointed Minister of Education in a post-war government and reformed the education system he had condemned for losing France the war in 1940. Instead, in 1948, his son Étienne offered the Archives Nationales his father's papers for repository, but they rejected the offer. As a result, the material was placed in the vaults of the École Normale Supérieure, "where it lay untouched for decades".

Intellectual historian Peter Burke named Bloch the leader of what he called the "French Historical Revolution", and Bloch became an icon for the post-war generation of new historians. Although he has been described as being, to some extent, the object of a cult in both England and France—"one of the most influential historians of the twentieth century" by Stirling, and "the greatest historian of modern times" by John H. Plumb—this is a reputation mostly acquired postmortem. Henry Loyn suggests it is also one which would have amused and amazed Bloch. According to Stirling, this posed a particular problem within French historiography when Bloch effectively had martyrdom bestowed upon him after the war, leading to much of his work being overshadowed by the last months of his life. This led to "indiscriminate heaps of praise under which he is now almost hopelessly buried". This is partly at least the fault of historians themselves, who have not critically re-examined Bloch's work but rather treat him as a fixed and immutable aspect of the historiographical background.

At the turn of the millennium "there is a woeful lack of critical engagement with Marc Bloch's writing in contemporary academic circles" according to Stirling. His legacy has been further complicated by the fact that the second generation of Annalists led by Fernand Braudel has "co-opted his memory", combining Bloch's academic work and Resistance involvement to create "a founding myth". The aspects of his life which made Bloch easy to beatify have been summed up by Henry Loyn as "Frenchman and Jew, scholar and soldier, staff officer and Resistance worker ... articulate on the present as well as the past".

The first critical biography of Bloch did not appear until Carole Fink's "Marc Bloch: A Life in History" was published in 1989. This, wrote S. R. Epstein, was the "professional, extensively researched and documented" story of Bloch's life, and, he commented, probably had to "overcome a strong sense of protectiveness among the guardians of Bloch's and the "Annales" memory". Since then, continuing scholarship—such as that by Stirling, who calls Bloch a visionary, although a "flawed" one—has been more critically objective of Bloch's recognisable weaknesses. For example, although he was a keen advocate for chronological precision and textual accuracy, his only major work in this area, a discussion of Osbert of Clare's "Life of Edward the Confessor", was subsequently "seriously criticised" by later experts in the field such as R. W. Southern and Frank Barlow; Epstein later suggested Bloch was "a mediocre theoretician but an adept artisan of method". Colleagues who worked with him occasionally complained that Bloch's manner could be "cold, distant, and both timid and hypocritical" due to the strong views he had held on the failure of the French education system. Bloch's reduction of the role of individuals, and their personal beliefs, in changing society or making history has been challenged. Even Febvre, reviewing "Feudal Society" on its post-war publication, suggested that Bloch had unnecessarily ignored the individual's role in societal development.

Bloch has also been accused of ignoring unanswered questions and presenting complete answers when they are perhaps not deserved, and of sometimes ignoring internal inconsistencies. Andrew Wallace-Hadrill has also criticised Bloch's division of the feudal period into two distinct times as artificial. He also says Bloch's theory on the transformation of blood-ties into feudal bonds do not match either the chronological evidence or what is known of the nature of the early family unit. Bloch seems to have occasionally ignored, whether accidentally or deliberately, important contemporaries in his field. Richard Lefebvre des Noëttes, for example, who founded the history of technology as a new discipline, built new harnesses from medieval illustrations, and drew histographical conclusions. Bloch, though, does not seem to have acknowledged the similarities between his and Lefebvre's approaches to physical research, even though he cited much earlier historians. Davies argued that there was a sociological aspect to Bloch's work which often neutralised the precision of his historical writing; as a result, he says, those of Bloch's works with a sociological conception, such as "Feudal Society", have not always "stood the test of time".

Comparative history, too, still proved controversial many years after Bloch's death, and Bryce Lyon has posited that, had Bloch survived the war, it is very likely that his views on history—already changing in the early years of the second war, just as they had done in the aftermath of the first—would have re-adjusted themselves against the very school he had founded. Stirling suggests what distinguished Bloch from his predecessors was that he effectively became a new kind of historian, who "strove primarily for transparency of methodology where his predecessors had striven for transparency of data" while continuously critiquing himself at the same time. Davies suggests his legacy lies not so much in the body of work he left behind him, which is not always as definitive as it has been made out to be, but the influence he had on "a whole generation of French historical scholarship". Bloch's emphasis on how rural and village society has been neglected by historians in favour of the lords and manorial courts that ruled them influenced later historians such as R. H. Hilton in the study of the economics of peasant society. Bloch's combination of economics, history and sociology was "forty years before it became fashionable", argues Daniel Chirot, which he says could make Bloch a founding father of post-war sociology scholarship.

The English-language journal "Past & Present", published by Oxford University Press, was a direct successor to the "Annales", suggests Loyn. Michel Foucault said of the Annales School, "what Bloch, Febvre and Braudel have shown for history, we can show, I believe, for the history of ideas". Bloch's influence spread beyond historiography after his death. In the 2007 French presidential election, Bloch was quoted many times. For example, candidates Nicolas Sarkozy and Marine Le Pen both cited Bloch's lines from "Strange Defeat": "there are two categories of Frenchmen who will never really grasp the significance of French history: those who refuse to be thrilled by the Consecration of our Kings at Reims, and those who can read unmoved the account of the Festival of Federation". In 1977, Bloch received a state reburial; streets schools and universities have been named after him, and the centennial of Bloch's birth was celebrated at a conference held in Paris in June 1986. It was attended academics of various disciplines, particularly historians and anthropologists.





</doc>
<doc id="19667" url="https://en.wikipedia.org/wiki?curid=19667" title="Michael Ventris">
Michael Ventris

Michael George Francis Ventris, (; 12 July 1922 – 6 September 1956) was an English architect, classicist and philologist who deciphered Linear B, the ancient Mycenaean Greek script. A student of languages, Ventris had pursued the decipherment as a personal vocation since his adolescence. After creating a new field of study, Ventris died in a car crash a few weeks before the publication, with John Chadwick, of "Documents in Mycenaean Greek".

Ventris was born into a traditional army family. His grandfather, Francis Ventris, was a major-general and Commander of British Forces in China. His father, Edward Francis Vereker Ventris, was a lieutenant-colonel in the Indian Army, who retired early due to ill health. Edward Ventris married Anna Dorothea Janasz (Dora), who was from a wealthy Jewish Polish paternal background. Michael Ventris was their only child.

The family moved to Switzerland for eight years, seeking a healthy environment for Colonel Ventris. Young Michael started school in Gstaad, where classes were taught in French and German. He soon was fluent in both languages and showing proficiency for Swiss German. He was capable of learning a language within a matter of weeks, which allowed him to acquire fluency in a dozen languages. His mother often spoke Polish to him, and he was fluent by the age of eight. At this time, he was reading Adolf Erman's "Die Hieroglyphen" in German.
In 1931, the Ventris family returned home. From 1931 to 1935 Ventris was sent to Bickley Hill School in Stowe. His parents divorced in 1935. At this time, he secured a scholarship to Stowe School. At Stowe he learned some Latin and Ancient Greek. He did not do outstanding work there - by then he was spending most of his spare time learning as much as he could about Linear B, some of his study time being spent under the covers at night with a flashlight. When he was not boarding at school, Ventris lived with his mother, before 1935 in coastal hotels, and then in the avant garde Berthold Lubetkin's Highpoint modernist apartments in Highgate. His mother's acquaintances, who frequented the house, included many sculptors, painters, and writers of the day. The money for her sophisticated lifestyle came from Polish estates.

Ventris's father died in 1938 and his mother Dora became administrator of the estate. With the German invasion of Poland in 1939, Mrs Ventris lost her private income, and in 1940 Dora's father died. Ventris lost his mother to clinical depression and an overdose of barbiturates. He never spoke of her, assuming instead an ebullient and energetic manner in whatever he decided to do, a trait which won him numerous friends. A friend of the family, Russian sculptor Naum Gabo, took Ventris under his wing. Ventris later said that Gabo was the most family he had ever had. It may have been at Gabo's house that he began the study of Russian. He decided on architecture as a career, and enrolled in the Architectural Association School of Architecture. There he met his wife-to-be Lois Knox-Niven, daughter of Lois Butler. Her social background was similar to Ventris's: her family was well-to-do, she had travelled in Europe, and she was interested in architecture. She was also popular and considered very beautiful.
Ventris did not complete his architecture studies, being conscripted in 1942. He chose the Royal Air Force (RAF). His preference was for navigator rather than pilot, and he completed the extensive training in the UK and Canada, to qualify early in 1944 and be commissioned. While training, he studied Russian intensively for several weeks, the purpose of which is not clear. He took part in the bombing of Germany, as aircrew on the Handley Page Halifax with No. 76 Squadron RAF, initially at RAF Breighton and then at RAF Holme-on-Spalding Moor. After the conclusion of the war he served out the rest of his term on the ground in Germany, for which he was chosen because of his knowledge of Russian. His duties are unclear. His friends assumed he was on intelligence duties, interpreting his denials as part of a legal gag. No evidence of such assignments has emerged in the decades since. There is also no evidence that he was ever part of any code-breaking unit, as was Chadwick, even though the public has readily believed this explanation of his genius and success with Linear B.

After the war he worked briefly in Sweden, learning enough Swedish to communicate with scholars. Then he came home to complete his architectural education with honours in 1948 and settled down with Lois working as an architect. He designed schools for the Ministry of Education. He and his wife personally designed their family home, 19 North End, Hampstead. Ventris and his wife had two children, a son, Nikki (1942–1984) and a daughter, Tessa (born 1946). Ventris continued with his efforts on Linear B, discovering in 1952 that it was an archaic form of Greek. He was awarded an OBE in 1955 for "services to Mycenaean paleography." In 1959 he was posthumously awarded the British Academy's Kenyon Medal.

In 1956 Ventris, who lived in Hampstead, died instantly in a late-night collision in Hatfield, Hertfordshire, with a parked truck while driving home, aged 34. The coroner's verdict was accidental death.

An English Heritage blue plaque commemorates Ventris at his home in Hampstead.

At the beginning of the 20th century, archaeologist Arthur Evans began excavating an ancient site at Knossos, on the island of Crete. In doing so he uncovered a great many clay tablets inscribed with two unknown scripts, Linear A and Linear B. Evans attempted to decipher both in the following decades, with little success.

In 1936, Evans hosted an exhibition on Cretan archaeology at Burlington House in London, home of the Royal Academy. It was the jubilee anniversary (50 years) of the British School of Archaeology in Athens, contemporaneous owners and managers of the Knossos site. Evans had given the site to them some years previously. Villa Ariadne, Evans's home there, was now part of the school. Boys from Stowe school were in attendance at one lecture and tour conducted by Evans himself at age 85. Ventris, 14 years old, was present and remembered Evans walking with a stick. The stick was undoubtedly the cane named Prodger which Evans carried all his life to assist him with his short-sightedness and night blindness. Evans held up tablets of the unknown scripts for the audience to see. During the interview period following the lecture, Ventris immediately confirmed that Linear B was as yet undeciphered, and determined to decipher it.

In 1940, the 18-year-old Ventris had an article "Introducing the Minoan Language" published in the "American Journal of Archaeology". Ventris's initial theory was that Etruscan and Linear B were related and that this might provide a key to decipherment. Although this proved incorrect, it was a link he continued to explore until the early 1950s.

Shortly after Evans died, Alice Kober noted that certain words in Linear B inscriptions had changing word endings — perhaps declensions in the manner of Latin or Greek. Using this clue, Ventris constructed a series of grids associating the symbols on the tablets with consonants and vowels. While "which" consonants and vowels these were remained mysterious, Ventris learned enough about the structure of the underlying language to begin guessing. Alice Kober was a classics professor at Brooklyn College and had done extensive work on Linear B. Ventris acknowledged her work as having made a significant contribution to his own work.

Shortly before World War II, American archaeologist Carl Blegen discovered a further 600 or so tablets of Linear B in the Mycenaean palace of Pylos. Photographs of these tablets by archaeologist Alison Frantz facilitated Ventris's later decipherment of the Linear B script.

Comparing the Linear B tablets discovered on the Greek mainland, and noting that certain symbol groups appeared only in the Cretan texts, Ventris made the inspired guess that those were place names on the island. This proved to be correct. Armed with the symbols he could decipher from this, Ventris soon unlocked much text and determined that the underlying language of Linear B was in fact Greek. This overturned Evans's theories of Minoan history by establishing that Cretan civilization, at least in the later periods associated with the Linear B tablets, had been part of Mycenean Greece.






</doc>
<doc id="19668" url="https://en.wikipedia.org/wiki?curid=19668" title="Maniac Mansion">
Maniac Mansion

Maniac Mansion is a 1987 graphic adventure video game developed and published by Lucasfilm Games. It follows teenage protagonist Dave Miller as he attempts to rescue his girlfriend Sandy Pantz from a mad scientist, whose mind has been enslaved by a sentient meteor. The player uses a point-and-click interface to guide Dave and two of his six playable friends through the scientist's mansion while solving puzzles and avoiding dangers. Gameplay is non-linear, and the game must be completed in different ways based on the player's choice of characters. Initially released for the Commodore 64 and Apple II, "Maniac Mansion" was Lucasfilm Games' first self-published product.

The game was conceived in 1985 by Ron Gilbert and Gary Winnick, who sought to tell a comedic story based on horror film and B-movie clichés. They mapped out the project as a paper-and-pencil game before coding commenced. While earlier adventure titles had relied on command lines, Gilbert disliked such systems, and he developed "Maniac Mansion"s simpler point-and-click interface as a replacement. To speed up production, he created a game engine called SCUMM, which was used in many later LucasArts titles. After its release, "Maniac Mansion" was ported to several platforms. A port for the Nintendo Entertainment System had to be reworked heavily, in response to complaints by Nintendo of America that the game was inappropriate for children.

"Maniac Mansion" was critically acclaimed: reviewers lauded its graphics, cutscenes, animation, and humor. Writer Orson Scott Card praised it as a step toward "computer games [becoming] a valid storytelling art". It influenced numerous graphic adventure titles, and its point-and-click interface became a standard feature in the genre. The game's success solidified Lucasfilm as a serious rival to adventure game studios such as Sierra On-Line. In 1990, "Maniac Mansion" was adapted into a three-season television series of the same name, written by Eugene Levy and starring Joe Flaherty. A sequel to the game, "Day of the Tentacle", was released in 1993.

"Maniac Mansion" is a graphic adventure game in which the player uses a point-and-click interface to guide characters through a two-dimensional game world and to solve puzzles. Fifteen action commands, such as "Walk To" and "Unlock", may be selected by the player from a menu on the screen's lower half. The player starts the game by choosing two out of six characters to accompany protagonist Dave Miller: Bernard, Jeff, Michael, Razor, Syd, and Wendy. Each character possesses unique abilities: for example, Syd and Razor can play musical instruments, while Bernard can repair appliances. The game may be completed with any combination of characters; but, since many puzzles are solvable only by certain characters, different paths must be taken based on the group's composition. "Maniac Mansion" features cutscenes, a word coined by Ron Gilbert, that interrupt gameplay to advance the story and inform the player about offscreen events.

The game takes place in the mansion of the fictional Edison family: Dr. Fred, a mad scientist; Nurse Edna, his wife; and their son Weird Ed. Living with the Edisons are two large, disembodied tentacles, one purple and the other green. The intro sequence shows that a sentient meteor crashed near the mansion twenty years earlier; it brainwashed the Edisons and directed Dr. Fred to obtain human brains for use in experiments. The game begins as Dave Miller prepares to enter the mansion to rescue his girlfriend, Sandy Pantz, who had been kidnapped by Dr. Fred. With the exception of the green tentacle, the mansion's inhabitants are hostile, and will throw the player characters into the dungeon—or, in some situations, kill them—if they see them. When a character dies, the player must choose a replacement from the unselected characters; and the game ends if all characters are killed. "Maniac Mansion" has five possible endings, based on which characters are chosen, which survive, and what the characters accomplish.

"Maniac Mansion" was conceived in 1985 when Lucasfilm Games employees Ron Gilbert and Gary Winnick were assigned to create an original game. Gilbert had been hired the previous year as a programmer for the game "Koronis Rift". He befriended Winnick over their similar tastes in humor, film, and television. Company management provided little oversight in the creation of "Maniac Mansion", a trend to which Gilbert credited the success of several of his games for Lucasfilm.

Gilbert and Winnick co-wrote and co-designed the project, but they worked separately as well: Gilbert on programming and Winnick on visuals. As both of them enjoyed B horror films, they decided to make a comedy-horror game set in a haunted house. They drew inspiration from a film whose name Winnick could not recall. He described it as "a ridiculous teen horror movie", in which teenagers inside a building were killed one by one without any thought of leaving. This film, combined with clichés from popular horror movies such as "Friday the 13th" and "A Nightmare on Elm Street", became the basis for the game's setting. Early work on the game progressed organically: according to Gilbert, "Very little was written down. Gary and I just talked and laughed a lot, and out it came." Lucasfilm Games relocated to the Stable House at Skywalker Ranch during "Maniac Mansion"s conception period, and the ranch's Main House was used as a model for the mansion. Several rooms from the Main House received exact reproductions in the game, such as a library with a spiral staircase and a media room with a large-screen TV and grand piano.

Story and characters were a primary concern for Gilbert and Winnick. The pair based the game's cast on friends, family members, acquaintances, and stereotypes. For example, Winnick's girlfriend Ray was the inspiration for Razor, while Dave and Wendy were based, respectively, on Gilbert and a fellow Lucasfilm employee named Wendy. According to Winnick, the Edison family was shaped after characters from EC Comics and Warren Publishing magazines. The sentient meteor that brainwashes Dr. Fred was inspired by a segment from the 1982 anthology film "Creepshow". A man-eating plant, similar to that of "Little Shop of Horrors", was included as well. The developers sought to strike a balance between tension and humor with the game's story.

Initially, Gilbert and Winnick struggled to choose a gameplay genre for "Maniac Mansion". While visiting relatives over Christmas, Gilbert saw his cousin play "King's Quest: Quest for the Crown", an adventure game by Sierra On-Line. Although he was a fan of text adventures, this was Gilbert's first experience with a graphic adventure, and he used the holiday to play the game and familiarize himself with the format. As a result, he decided to develop his and Winnick's ideas into a graphic adventure game.

"Maniac Mansion"s story and structure were designed before coding commenced. The project's earliest incarnation was a paper-and-pencil board game, in which the mansion's floor plan was used as a game board, and cards represented events and characters. Lines connected the rooms to illustrate pathways by which characters could travel. Strips of cellulose acetate were used to map out the game's puzzles by tracking which items worked together when used by certain characters. Impressed by the map's complexity, Winnick included it in the final game as a poster hung on a wall. Because each character contributes different skills and resources, the pair spent months working on the event combinations that could occur. This extended the game's production time beyond that of previous Lucasfilm Games projects, which almost led to Gilbert's firing. The game's dialogue, written by David Fox, was not created until after programming had begun.

Gilbert started programming "Maniac Mansion" in 6502 assembly language, but he quickly decided that the project was too large and complex for this method. He decided that a new game engine would have to be created. Its coding language was initially planned to be Lisp-inspired, but Gilbert opted for one similar to C, Yacc. Lucasfilm employee Chip Morningstar contributed the base code for the engine, which Gilbert then built on. Gilbert hoped to create a "system that could be used on many adventure games, cutting down the time it took to make them". "Maniac Mansion"s first six-to-nine months of production were dedicated largely to engine development. The game was developed around the Commodore 64 home computer, an 8-bit system with only 64 KB of memory. The team wanted to include scrolling screens, but as it was normally impossible to scroll bitmap graphics on the Commodore 64, they had to use lower-detail tile graphics. Winnick gave each character a large head made of three stacked sprites to make them recognizable.

Although Gilbert wrote much of the foundational code for "Maniac Mansion", the majority of the game's events were programmed by Lucasfilm employee David Fox. Fox was between projects and planned to work on the game only for a month, but he remained with the team for six months. With Gilbert, he wrote the characters' dialog and choreographed the action. Winnick's concept art inspired him to add new elements to the game: for example, Fox allowed the player to place a hamster inside the kitchen's microwave.

The team wanted to avoid punishing the player for applying everyday logic in "Maniac Mansion". Fox noted that one Sierra game features a scene in which the player, without prior warning, may encounter a game over screen simply by picking up a shard of glass. He characterized such game design as "sadistic", and he commented, "I know that in the real world I can successfully pick up a broken piece of mirror without dying". Because of the project's nonlinear puzzle design, the team struggled to prevent no-win scenarios, in which the player unexpectedly became unable to complete the game. As a result of this problem, Gilbert later explained, "We were constantly fighting against the desire just to rip out all the endings and just go with three characters, or even sometimes just one character". Lucasfilm Games had only one playtester, and many dead-ends went undetected as a result. Further playtesting was provided by Gilbert's uncle, to whom Gilbert mailed a floppy disk of the game's latest version each week.

The "Maniac Mansion" team wanted to retain the structure of a text-based adventure game, but without the standard command-line interface. Gilbert and Winnick were frustrated by the genre's text parsers and frequent game over screens. While in college, Gilbert had enjoyed "Colossal Cave Adventure" and the games of Infocom, but he disliked their lack of visuals. He found the inclusion of graphics in Sierra On-Line games, such as "King's Quest", to be a step in the right direction, but these games still require the player to type, and to guess which commands must be input. In response, Gilbert programmed a point-and-click graphical user interface that displays every possible command. Fox had made a similar attempt to streamline Lucasfilm's earlier "" and he conceived the entirety of "Maniac Mansion"s interface, according to Gilbert. Forty input commands were planned at first, but the number was gradually reduced to 12. Gilbert finished the "Maniac Mansion" engine—which he later named "Script Creation Utility for Maniac Mansion" (SCUMM)—after roughly one year of work. Although the game was designed for the Commodore 64, the SCUMM engine allowed it to be ported easily to other platforms.

After 18 to 24 months of development, "Maniac Mansion" debuted at the 1987 Consumer Electronics Show in Chicago. The game was released for the Commodore 64 and Apple II in October 1987. While previous Lucasfilm Games products had been published by outside companies, "Maniac Mansion" was self-published. This became a trend at Lucasfilm. The company hired Ken Macklin, an acquaintance of Winnick's, to design the game's packaging artwork. Gilbert and Winnick collaborated with the marketing department to design the back cover. The two also created an insert that includes hints, a backstory, and jokes. An MS-DOS port was released in early 1988, developed in part by Lucasfilm employees Aric Wilmunder and Brad Taylor. Ports for the Amiga, Atari ST and Nintendo Entertainment System (NES) followed, with the Amiga and Atari ST ports in 1989 and the NES port in 1990. The 16-bit versions of Maniac Mansion featured a copy protection system requiring the user to enter graphical symbols out of a code book included with the game. This was not present in the Commodore 64 and Apple versions due to lack of disk space, so those instead used an on-disk copy protection.

There were two separate versions of the game developed for the NES. The first port was handled and published by Jaleco only in Japan. Released on June 23, 1988, it featured characters redrawn in a cute art style and generally shrunken rooms. No scrolling is present, leading to rooms larger than a single screen to be displayed via flip-screens. Many of the background details are missing, and instead of a save feature a password, over 100 characters long, is required to save progress.
In September 1990 Jaleco released an American version of "Maniac Mansion" as the first NES title developed by Lucasfilm Games in cooperation with Realtime Associates. Generally, this port is regarded as being far closer to the original game than the Japanese effort.

Company management was occupied with other projects, and so the port received little attention until employee Douglas Crockford volunteered to direct it. The team used a modified version of the SCUMM engine called "NES SCUMM" for the port. According to Crockford, "[One] of the main differences between the NES and PCs is that the NES can do certain things much faster". The graphics had to be entirely redrawn to match the NES's display resolution. Tim Schafer, who later designed "Maniac Mansion"s sequel "Day of the Tentacle", received his first professional credit as a playtester for the NES version of "Maniac Mansion".

During "Maniac Mansion"s development for the Commodore 64, Lucasfilm had censored profanity in the script: for instance, the early line of dialogue "Don't be a shit head" became "Don't be a tuna head". Additional content was removed from the NES version to make it suitable for a younger audience, and to conform with Nintendo's policies. Jaleco USA president Howie Rubin warned Crockford about content to which Nintendo might object, such as the word "kill". After reading the NES Game Standards Policy for himself, Crockford suspected that further elements of "Maniac Mansion" could be problematic, and he sent a list of questionable content to Jaleco. When the company replied that the content was reasonable, Lucasfilm Games submitted "Maniac Mansion" for approval.

One month later, Nintendo of America contacted Lucasfilm Games to request the removal of offensive text and nude graphics. Crockford censored this content but attempted to leave the game's essence intact. For example, Nintendo wanted graffiti in one room—which provided an important hint to players—removed from the game. Unable to comply without simultaneously removing the hint, the team simply shortened it. Sexually suggestive and otherwise "graphic" dialogue was edited, including a remark from Dr. Fred about "pretty brains [being] sucked out". The nudity described by Nintendo encompassed a swimsuit calendar, a classical sculpture and a poster of a mummy in a Playmate pose. After a brief fight to keep the sculpture, the team ultimately removed all three. The phrase "NES SCUMM" in the credits sequence was censored as well.

Lucasfilm Games re-submitted the edited version of "Maniac Mansion" to Nintendo, which then manufactured 250,000 cartridges. Each cartridge was fitted with a battery-powered back-up to save data. Nintendo announced the port through its official magazine in early 1990, and it provided further coverage later that year. The ability to microwave a hamster remained in the game, which Crockford cited as an example of the censors' contradictory criteria. Nintendo later noticed it, and after the first batch of cartridges was sold, Jaleco was forced to remove the content from future shipments.

Late in development, Jaleco commissioned Realtime Associates to provide background music, which no previous version of "Maniac Mansion" had featured. Realtime Associates' founder and president David Warhol noted that "video games at that time had to have 'wall to wall' music". He brought in George "The Fat Man" Sanger and his band, along with David Hayes, to compose the score. Their goal was to create songs that suited each character, such as a punk rock theme for Razor, an electronic rock theme for Bernard and a version of Thin Lizzy's "The Boys Are Back in Town" for Dave Miller. Warhol translated their work into NES chiptune music.

According to Stuart Hunt of "Retro Gamer", "Maniac Mansion" received highly positive reviews from critics. Nevertheless, Ron Gilbert noted that "it wasn't a huge hit" commercially. In 2011, Hunt wrote that "as so often tends to be the way with cult classics, the popularity it saw was slow in coming."

Keith Farrell of "Compute!'s Gazette" was struck by "Maniac Mansion"s similarity to film, particularly in its use of cutscenes to impart "information or urgency". He lauded the game's graphics, animation and high level of detail. "Commodore User"s Bill Scolding and three reviewers from "Zzap!64" compared the game to "The Rocky Horror Picture Show". Further comparisons were drawn to "Psycho", "Friday the 13th", "The Texas Chain Saw Massacre", "The Addams Family" and "Scooby-Doo". Russ Ceccola of "Commodore Magazine" found the cutscenes to be creative and well made, and he commented that the "characters are distinctively Lucasfilm's, bringing facial expressions and personality to each individual character". In "Compute!", Orson Scott Card praised the game's humor, cinematic storytelling and lack of violence. He called it "compellingly good" and evidence of Lucasfilm's push "to make computer games a valid storytelling art".

German magazine "Happy-Computer" commended the point-and-click interface and likened it to that of "Uninvited" by ICOM Simulations. The publication highlighted "Maniac Mansion"s graphics, originality, and overall enjoyability: one of the writers called it the best adventure title yet released. "Happy-Computer" later reported that "Maniac Mansion" was the highest-selling video game in West Germany for three consecutive months. The game's humor received praise from "Zzap!64", whose reviewers called the point-and-click controls "tremendous" and the total package "innovative and polished". Shay Addams of "Questbusters: The Adventurer's Newsletter" preferred "Maniac Mansion"s interface to that of "Labyrinth: The Computer Game". He considered the game to be Lucasfilm's best, and he recommended it to Commodore 64 and Apple II users unable to run titles with better visuals, such as those from Sierra On-Line. A writer for "ACE" enjoyed the game's animation and depth, but he noted that fans of text-based adventures would dislike the game's simplicity. "Entertainment Weekly" picked the game as the #20 greatest game available in 1991, saying: "The graphics are merely okay and the music is Nintendo at its tinniest, but Maniac Mansion's plot is enough to overcome these faults. In this command-driven game — adapted from the computer hit — three buddies venture into a sinister haunted mansion and wind up juggling a bunch of wacky story lines."

Reviewing the MS-DOS and Atari ST ports, a critic from "The Games Machine" called "Maniac Mansion" "an enjoyable romp" that was structurally superior to later LucasArts adventure games. The writer noticed poor pathfinding and disliked the limited audio. Reviewers for "The Deseret News" lauded the audiovisuals and considered the product "wonderful fun". "Computer Gaming World"s Charles Ardai praised the game for attaining "the necessary and precarious balance between laughs and suspense that so many comic horror films and novels lack". Although he faulted the control system's limited options, he hailed it as "one of the most comfortable ever devised". Writing for "VideoGames & Computer Entertainment", Bill Kunkel and Joyce Worley stated that the game's plot and premise were typical of the horror genre; but they praised the interface and execution.

Reviewing "Maniac Mansion"s Amiga version four years after its release, Simon Byron of "The One Amiga" praised the game for retaining "charm and humour", but suggested that its art direction had become "tacky" compared to more recent titles. Stephen Bradly of "Amiga Format" found the game derivative, but he encountered "loads of visual humour" in it; and he added, "Strangely, it's quite compelling after a while." Michael Labiner of Germany's "Amiga Joker" considered "Maniac Mansion" to be one of the best adventure games for the system. He noted minor graphical flaws, such as a limited color palette, but he argued that the gameplay made up for such shortcomings. Writing for "Datormagazin" in Sweden, Ingela Palmér commented that the Amiga and Commodore 64 versions of "Maniac Mansion" were nearly identical. She criticized the graphics and gameplay of both releases but felt the game to be highly enjoyable regardless.

Reviewing the NES release, British magazine "Mean Machines" commended the game's presentation, playability, and replay value. The publication also noted undetailed graphics and "ear-bashing tunes". The magazine's Julian Rignall compared "Maniac Mansion" to the title "Shadowgate", but he preferred the former's controls and lack of "death-without-warning situations". Writers for Germany's "Video Games" referred to the NES version as a "classic". Co-reviewer Heinrich Lenhardt stated that "Maniac Mansion" was unlike any other NES adventure game, and that it was no less enjoyable than its home computer releases. Co-reviewer Winnie Forster found it to be "one of the most original representatives of the [adventure game] genre". In retrospective features, "Edge" magazine called the NES version "somewhat neutered" and "GamesTM" referred to it as "infamous" and "heavily censored".

Lucasfilm conceived the idea for a television adaptation of "Maniac Mansion", the rights to which were purchased by The Family Channel in 1990. The two companies collaborated with Atlantis Films to produce a sitcom named after the game, which debuted in September of that year. It aired on YTV in Canada and The Family Channel in the United States. Based in part on the video game, the series focuses on the Edison family's life and stars Joe Flaherty as Dr. Fred. Its writing staff was led by Eugene Levy. Gilbert later said that the premise of the series changed during production until it differed heavily from the game's original plot. Upon its debut, the adaptation received positive reviews from "Variety", "Entertainment Weekly" and the "Los Angeles Times". "Time" named it one of the year's best new series. Ken Tucker of "Entertainment Weekly" questioned the decision to air the series on The Family Channel, given Flaherty's subversive humor. Discussing the series in retrospect, Richard Cobbett of "PC Gamer" criticized its generic storylines and lack of relevance to the game. The series lasted for three seasons; sixty-six episodes were filmed.

In the early 1990s, LucasArts tasked Dave Grossman and Tim Schafer, both of whom had worked on the "Monkey Island" series, with designing a sequel to "Maniac Mansion". Gilbert and Winnick initially assisted with the project's writing. The team included voice acting and more detailed graphics, which Gilbert had originally envisioned for "Maniac Mansion". The first game's nonlinear design was discarded, and the team implemented a Chuck Jones-inspired visual style, alongside numerous puzzles based on time travel. Bernard and the Edison family were retained. The sequel "Day of the Tentacle" was released in 1993, and came with a fully playable copy of "Maniac Mansion" hidden as an easter egg within the game.

In 2010, the staff of "GamesTM" dubbed "Maniac Mansion" a "seminal" title that overhauled the gameplay of the graphic adventure genre. Removing the need to guess syntax allowed players to concentrate on the story and puzzles, which created a smoother and more enjoyable experience, according to the magazine. Eurogamer's Kristan Reed agreed: he believed that the design was "infinitely more elegant and intuitive" than its predecessors and that it freed players from "guessing-game frustration". Designer Dave Grossman, who worked on Lucasfilm Games' later "Day of the Tentacle" and "The Secret of Monkey Island", felt that "Maniac Mansion" had revolutionized the adventure game genre. Although 1985's "Uninvited" had featured a point-and-click interface, it was not influential. "Maniac Mansion"s implementation of the concept was widely imitated in other adventure titles. Writing in the game studies journal "Kinephanos", Jonathan Lessard argued that "Maniac Mansion" led a "Casual Revolution" in the late 1980s, which opened the adventure genre to a wider audience. Similarly, Christopher Buecheler of GameSpy called the game a contributor to its genre's subsequent critical adoration and commercial success.

Reed highlighted the "wonderfully ambitious" design of "Maniac Mansion", in reference to its writing, interface, and cast of characters. Game designer Sheri Graner Ray believed the game to challenge "damsel in distress" stereotypes through its inclusion of female protagonists. Conversely, writer Mark Dery argued that the goal of rescuing a kidnapped cheerleader reinforced negative gender roles. The Lucasfilm team built on their experiences from "Maniac Mansion" and became increasingly ambitious in subsequent titles. Gilbert admitted to making mistakes—such as the inclusion of no-win situations—in "Maniac Mansion", and he applied these lessons to future projects. For example, the game relies on timers rather than events to trigger cutscenes, which occasionally results in awkward transitions: Gilbert worked to avoid this flaw with the "Monkey Island" series. Because of "Maniac Mansion"s imperfections, Gilbert considers it his favorite of his games.

According to writers Mike and Sandie Morrison, Lucasfilm Games became "serious competition" in the adventure genre after the release of "Maniac Mansion". The game's success solidified Lucasfilm as one of the leading producers of adventure games: authors Rusel DeMaria and Johnny Wilson described it as a "landmark title" for the company. In their view, "Maniac Mansion"—along with "Space Quest: The Sarien Encounter" and "Leisure Suit Larry in the Land of the Lounge Lizards"—inaugurated a "new era of humor-based adventure games". This belief was shared by Reed, who wrote that "Maniac Mansion" "set in motion a captivating chapter in the history of gaming" that encompassed wit, invention, and style. The SCUMM engine was reused by Lucasfilm in eleven later titles; improvements were made to its code with each game. Over time, rival adventure game developers adopted this paradigm in their own software. "GamesTM" attributed the change to a desire to streamline production and create enjoyable games. Following his 1992 departure from LucasArts—a conglomeration of Lucasfilm Games, ILM and Skywalker Sound formed in 1990—Gilbert used SCUMM to create adventure games and "Backyard Sports" titles for Humongous Entertainment.

In 2011, Richard Cobbett summarized "Maniac Mansion" as "one of the most intricate and important adventure games ever made". "Retro Gamer" ranked it as one of the ten best Commodore 64 games in 2006, and IGN later named it one of the ten best LucasArts adventure games. Seven years after the NES version's debut, "Nintendo Power" named it the 61st best game ever. The publication dubbed it the 16th best NES title in 2008. The game's uniqueness and clever writing were praised by "Nintendo Power": in 2010, the magazine's Chris Hoffman stated that the game is "unlike anything else out there — a point-and-click adventure with an awesome sense of humor and multiple solutions to almost every puzzle." In its retrospective coverage, "Nintendo Power" several times noted the ability to microwave a hamster, which the staff considered to be an iconic scene. In March 2012, "Retro Gamer" listed the hamster incident as one of the "100 Classic Gaming Moments".

"Maniac Mansion" enthusiasts have drawn fan art of its characters, participated in tentacle-themed cosplay and produced a trailer for a fictitious film adaptation of the game. German fan Sascha Borisow created a fan game remake, titled "Maniac Mansion Deluxe", with enhanced audio and visuals. He used the Adventure Game Studio engine to develop the project, which he distributed free of charge on the Internet. By the end of 2004, the remake had over 200,000 downloads. A remake with three-dimensional graphics called "Meteor Mess" was created by the German developer Vampyr Games, and, as of 2011, another group in Germany is producing one with art direction similar to that of "Day of the Tentacle". Fans have created an episodic series of games based on "Maniac Mansion" as well. Gilbert has said that he would like to see an official remake, similar in its graphics and gameplay to "The Secret of Monkey Island: Special Edition" and "Monkey Island 2 Special Edition: LeChuck's Revenge". He also expressed doubts about its potential quality, in light of George Lucas's enhanced remakes of the original "Star Wars" trilogy. In December 2017, Disney, which gained rights to the LucasArts games following its acquisition of Lucasfilms, published "Maniac Mansion" running atop the ScummVM virtual machine to various digital storefronts.



</doc>
<doc id="19669" url="https://en.wikipedia.org/wiki?curid=19669" title="Marx Brothers">
Marx Brothers

The Marx Brothers were an American family comedy act that was successful in vaudeville, on Broadway, and in motion pictures from 1905 to 1949. Five of the Marx Brothers' thirteen feature films were selected by the American Film Institute (AFI) as among the top 100 comedy films, with two of them, "Duck Soup" (1933) and "A Night at the Opera" (1935), in the top fifteen. They are widely considered by critics, scholars, and fans to be among the greatest and most influential comedians of the 20th century. The brothers were included in AFI's 100 Years... 100 Stars list of the 25 greatest male stars of Classical Hollywood cinema, the only performers to be inducted collectively.

The brothers are almost universally known today by their stage names: Chico, Harpo, Groucho, Gummo, and Zeppo. There was a sixth brother, the first born, named Manfred (Mannie), who died aged seven months; Zeppo was given the middle name Manfred in his memory.

The core of the act was the three elder brothers: Chico, Harpo, and Groucho, each of whom developed a highly distinctive stage persona. After the group essentially disbanded in 1950, Groucho went on to begin a successful second career in television, while Harpo and Chico appeared less prominently. The two younger brothers, Gummo and Zeppo, never developed their stage characters to the same extent as the elder three. Both of them left the act to pursue business careers at which they were successful, and for a time ran a large theatrical agency through which they represented their brothers and others. Gummo was not in any of the movies; Zeppo appeared in the first five films in relatively straight (non-comedic) roles. The early performing lives of the brothers owed much to their mother Minnie Marx (the sister of vaudeville comic Al Shean), who acted as their manager until her death in 1929.

The Marx Brothers were born in New York City, the sons of Jewish immigrants from Germany and France. Their mother Miene "Minnie" Schoenberg (professionally known as Minnie Palmer, later the brothers' manager) was from Dornum in East Frisia, and their father Samuel ("Sam"; born Simon) Marx was a native of Mertzwiller, a small Alsacian village, and worked as a tailor. (His name was changed to Samuel Marx, and he was nicknamed "Frenchy".) The family lived in the Yorkville district of New York City's Upper East Side, centered in the Irish, German and Italian quarters. The brothers are best known by their stage names:

Another brother, Manfred ("Mannie"), the first-born son of Sam and Minnie, was born in 1886 and died in infancy:

Family lore told privately of the firstborn son, Manny, born in 1886 but surviving for only three months, and carried off by tuberculosis. Even some members of the Marx family wondered if he was pure myth. But Manfred can be verified. A death certificate of the Borough of Manhattan reveals that he died, aged seven months, on 17 July 1886, of enterocolitis, with "asthenia" contributing, i.e., probably a victim of influenza. He is buried at New York's Washington Cemetery, beside his grandmother, Fanny Sophie Schönberg (née Salomons), who died on 10 April 1901.

The Marx Brothers also had an older sister, actually a cousin, born in January 1885 who had been adopted by Minnie and Frenchie. Her name was Pauline, or "Polly". Groucho talked about her in his 1972 Carnegie Hall concert.

Minnie Marx came from a family of performers. Her mother was a yodeling harpist and her father a ventriloquist; both were funfair entertainers. Around 1880, the family emigrated to New York City, where Minnie married Sam in 1884. During the early 20th century, Minnie helped her younger brother Abraham Elieser Adolf Schönberg (stage name Al Shean) to enter show business; he became highly successful on vaudeville and Broadway as half of the musical comedy double act Gallagher and Shean, and this gave the brothers an entree to musical comedy, vaudeville and Broadway at Minnie's instigation. Minnie also acted as the brothers' manager, using the name Minnie Palmer so that agents did not realize that she was also their mother. All the brothers confirmed that Minnie Marx had been the head of the family and the driving force in getting the troupe launched, the only person who could keep them in order; she was said to be a hard bargainer with theatre management.

Gummo and Zeppo both became successful businessmen: Gummo gained success through his agency activities and a raincoat business, and Zeppo became a multi-millionaire through his engineering business.

The brothers were from a family of artists, and their musical talent was encouraged from an early age. Harpo was particularly talented, learning to play an estimated six different instruments throughout his career. He became a dedicated harpist, which gave him his nickname. Chico was an excellent pianist, Groucho a guitarist and singer, and Zeppo a vocalist.

They got their start in vaudeville, where their uncle Albert Schönberg performed as Al Shean of Gallagher and Shean. Groucho's debut was in 1905, mainly as a singer. By 1907, he and Gummo were singing together as "The Three Nightingales" with Mabel O'Donnell. The next year, Harpo became the fourth Nightingale and by 1910, the group briefly expanded to include their mother Minnie and their Aunt Hannah. The troupe was renamed "The Six Mascots".

One evening in 1912, a performance at the Opera House in Nacogdoches, Texas, was interrupted by shouts from outside about a runaway mule. The audience hurried out to see what was happening. Groucho was angered by the interruption and, when the audience returned, he made snide comments at their expense, including "Nacogdoches is full of roaches" and "the jackass is the flower of Tex-ass". Instead of becoming angry, the audience laughed. The family then realized that it had potential as a comic troupe. (However, in his autobiography "Harpo Speaks", Harpo Marx stated that the runaway mule incident occurred in Ada, Oklahoma. A 1930 article in the "San Antonio Express" newspaper stated that the incident took place in Marshall, Texas.)

The act slowly evolved from singing with comedy to comedy with music. The brothers' sketch "Fun in Hi Skule" featured Groucho as a German-accented teacher presiding over a classroom that included students Harpo, Gummo, and Chico. The last version of the school act was titled "Home Again" and was written by their uncle Al Shean. The "Home Again" tour reached Flint, Michigan in 1915, where 14-year-old Zeppo joined his four brothers for what is believed to be the only time that all five Marx Brothers appeared together on stage. Gummo then left to serve in World War I, reasoning that "anything is better than being an actor!" Zeppo replaced him in their final vaudeville years and in the jump to Broadway, and then to Paramount films.

During World War I, anti-German sentiments were common, and the family tried to conceal its German origin. Mother Minnie learned that farmers were excluded from the draft rolls, so she purchased a poultry farm near Countryside, Illinois — but the brothers soon found that chicken ranching was not in their blood. During this time, Groucho discontinued his "German" stage personality.

By this time, "The Four Marx Brothers" had begun to incorporate their unique style of comedy into their act and to develop their characters. Both Groucho's and Harpo's memoirs say that their now-famous on-stage personae were created by Al Shean. Groucho began to wear his trademark greasepaint mustache and to use a stooped walk. Harpo stopped speaking onstage and began to wear a red fright wig and carry a taxi-cab horn. Chico spoke with a fake Italian accent, developed off-stage to deal with neighborhood toughs, while Zeppo adopted the role of the romantic (and "peerlessly cheesy", according to James Agee) straight man.

The on-stage personalities of Groucho, Chico, and Harpo were said to have been based on their actual traits. Zeppo, on the other hand, was considered the funniest brother offstage, despite his straight stage roles. He was the youngest and had grown up watching his brothers, so he could fill in for and imitate any of the others when illness kept them from performing. "He was so good as Captain Spaulding [in "Animal Crackers"] that I would have let him play the part indefinitely, if they had allowed me to smoke in the audience," Groucho recalled. (Zeppo stood in for Groucho in the film version of "Animal Crackers". Groucho was unavailable to film the scene in which the Beaugard painting is stolen, so the script was contrived to include a power failure, which allowed Zeppo to play the Spaulding part in near-darkness.) In December 1917 the Marx brothers were noted in an advertisement playing in a musical comedy act "Home Again".

By the 1920s, the Marx Brothers had become one of America's favorite theatrical acts, with their sharp and bizarre sense of humor. They satirized high society and human hypocrisy, and they became famous for their improvisational comedy in free-form scenarios. A famous early instance was when Harpo arranged to chase a fleeing chorus girl across the stage during the middle of a Groucho monologue to see if Groucho would be thrown off. However, to the audience's delight, Groucho merely reacted by commenting, "First time I ever saw a taxi hail a passenger". When Harpo chased the girl back in the other direction, Groucho calmly checked his watch and ad-libbed, "The 9:20's right on time. You can set your watch by the Lehigh Valley."

The brothers' vaudeville act had made them stars on Broadway under Chico's management and with Groucho's creative direction—first with the musical revue "I'll Say She Is" (1924–1925) and then with two musical comedies: "The Cocoanuts" (1925–1926) and "Animal Crackers" (1928–1929). Playwright George S. Kaufman worked on the last two and helped sharpen the brothers' characterizations.

Out of their distinctive costumes, the brothers looked alike, even down to their receding hairlines. Zeppo could pass for a younger Groucho, and played the role of his son in "Horse Feathers". A scene in "Duck Soup" finds Groucho, Harpo, and Chico all appearing in the famous greasepaint eyebrows, mustache, and round glasses while wearing nightcaps. The three are indistinguishable, enabling them to carry off the "mirror scene" perfectly.

The stage names of the brothers (except Zeppo) were coined by monologist Art Fisher during a poker game in Galesburg, Illinois, based both on the brothers' personalities and Gus Mager's "Sherlocko the Monk", a popular comic strip of the day that included a supporting character named "Groucho". As Fisher dealt each brother a card, he addressed them, for the very first time, by the names they kept for the rest of their lives.

The reasons behind Chico's and Harpo's stage names are undisputed, and Gummo's is fairly well established. Groucho's and Zeppo's are far less clear. Arthur was named Harpo because he played the harp, and Leonard became Chico (pronounced "Chick-o") because he was, in the slang of the period, a "chicken chaser". ("Chickens"—later "chicks"—was period slang for women. "In England now," said Groucho, "they were called 'birds'.")

In his autobiography, Harpo explained that Milton became Gummo because he crept about the theater like a gumshoe detective. Other sources reported that Gummo was the family's hypochondriac, having been the sickliest of the brothers in childhood, and therefore wore rubber overshoes, called gumshoes, in all kinds of weather. Still others reported that Milton was the troupe's best dancer, and dance shoes tended to have rubber soles. Groucho stated that the source of the name was Gummo wearing galoshes. Whatever the details, the name relates to rubber-soled shoes.

The reason that Julius was named Groucho is perhaps the most disputed. There are three explanations:


I kept my money in a 'grouch bag'. This was a small chamois bag that actors used to wear around their neck to keep other hungry actors from pinching their dough. Naturally, you're going to think that's where I got my name from. But that's not so. Grouch bags were worn on manly chests long before there was a Groucho.


Herbert was not nicknamed by Art Fisher, since he did not join the act until Gummo had departed. As with Groucho, three explanations exist for Herbert's name "Zeppo":


Maxine Marx reported in "The Unknown Marx Brothers" that the brothers listed their "real" names (Julius, Leonard, Adolph, Milton, and Herbert) on playbills and in programs, and only used the nicknames behind the scenes, until Alexander Woollcott overheard them calling one another by the nicknames. He asked them why they used their real names publicly when they had such wonderful nicknames, and they replied, "That wouldn't be dignified." Woollcott answered with a belly laugh. Woollcott did not meet the Marx Brothers until the premiere of "I'll Say She Is", which was their first Broadway show, so this would mean that they used their real names throughout their vaudeville days, and that the name "Gummo" never appeared in print during his time in the act. Other sources reported that the Marx Brothers went by their nicknames during their vaudeville era, but briefly listed themselves by their given names when "I'll Say She Is" opened because they were worried that a Broadway audience would reject a vaudeville act if they were perceived as low class.

The Marx Brothers' stage shows became popular just as motion pictures were evolving to "talkies". They signed a contract with Paramount Pictures and embarked on their film career at Paramount's studios in New York City's Astoria section. Their first two released films (after an unreleased short silent film titled "Humor Risk") were adaptations of the Broadway shows "The Cocoanuts" (1929) and "Animal Crackers" (1930). Both were written by George S. Kaufman and Morrie Ryskind. Production then shifted to Hollywood, beginning with a short film that was included in Paramount's twentieth anniversary documentary, "The House That Shadows Built" (1931), in which they adapted a scene from "I'll Say She Is". Their third feature-length film, "Monkey Business" (1931), was their first movie not based on a stage production.

"Horse Feathers" (1932), in which the brothers satirized the American college system and Prohibition, was their most popular film yet, and won them the cover of "Time" magazine. It included a running gag from their stage work, in which Harpo produces a ludicrous array of props from inside his coat, including a wooden mallet, a fish, a coiled rope, a tie, a poster of a woman in her underwear, a cup of hot coffee, a sword; and, just after Groucho warns him that he "can't burn the candle at both ends," a candle burning at both ends.

During this period Chico and Groucho starred in a radio comedy series, "Flywheel, Shyster and Flywheel". Though the series was short lived, much of the material developed for it was used in subsequent films. The show's scripts and recordings were believed lost until copies of the scripts were found in the Library of Congress in the 1980s. After publication in a book they were performed with Marx Brothers impersonators for BBC Radio.

Their last Paramount film, "Duck Soup" (1933), directed by the highly regarded Leo McCarey, is the highest rated of the five Marx Brothers films on the American Film Institute's "100 years ... 100 Movies" list. It did not do as well financially as "Horse Feathers", but was the sixth-highest grosser of 1933. The film sparked a dispute between the Marxes and the village of Fredonia, New York. "Freedonia" was the name of a fictional country in the script, and the city fathers wrote to Paramount and asked the studio to remove all references to Freedonia because "it is hurting our town's image". Groucho fired back a sarcastic retort asking them to change the name of their town, because "it's hurting our picture."

After expiration of the Paramount contract Zeppo left the act to become an agent. He and brother Gummo went on to build one of the biggest talent agencies in Hollywood, helping the likes of Jack Benny and Lana Turner get their starts. Groucho and Chico did radio, and there was talk of returning to Broadway. At a bridge game with Chico, Irving Thalberg began discussing the possibility of the Marxes joining Metro-Goldwyn-Mayer. They signed, now billed as "Groucho, Chico, Harpo, Marx Bros."

Unlike the free-for-all scripts at Paramount, Thalberg insisted on a strong story structure that made the brothers more sympathetic characters, interweaving their comedy with romantic plots and non-comic musical numbers, and targeting their mischief-making at obvious villains. Thalberg was adamant that scripts include a "low point", where all seems lost for both the Marxes and the romantic leads. He instituted the innovation of testing the film's script before live audiences before filming began, to perfect the comic timing, and to retain jokes that earned laughs and replace those that did not. Thalberg restored Harpo's harp solos and Chico's piano solos, which had been omitted from "Duck Soup".

The first Marx Brothers/Thalberg film was "A Night at the Opera" (1935), a satire on the world of opera, where the brothers help two young singers in love by throwing a production of "Il Trovatore" into chaos. The film—including its famous scene where an absurd number of people crowd into a tiny stateroom on a ship—was a great success, and was followed two years later by an even bigger hit, "A Day at the Races" (1937), in which the brothers cause mayhem in a sanitarium and at a horse race. The film features Groucho and Chico's famous "Tootsie Frootsie Ice Cream" sketch. In a 1969 interview with Dick Cavett, Groucho said that the two movies made with Thalberg were the best that they ever produced. Despite the Thalberg films' success, the brothers left MGM in 1937; Thalberg had died suddenly on September 14, 1936, two weeks after filming began on "A Day at the Races", leaving the Marxes without an advocate at the studio.

After a short experience at RKO ("Room Service", 1938), the Marx Brothers returned to MGM and made three more films: "At the Circus" (1939), "Go West" (1940) and "The Big Store" (1941). Prior to the release of "The Big Store" the team announced they were retiring from the screen. Four years later, however, Chico persuaded his brothers to make two additional films, "A Night in Casablanca" (1946) and "Love Happy" (1949), to alleviate his severe gambling debts. Both pictures were released by United Artists.

From the 1940s onward Chico and Harpo appeared separately and together in nightclubs and casinos. Chico fronted a big band, the Chico Marx Orchestra (with 17-year-old Mel Tormé as a vocalist). Groucho made several radio appearances during the 1940s and starred in "You Bet Your Life", which ran from 1947 to 1961 on NBC radio and television. He authored several books, including "Groucho and Me" (1959), "Memoirs of a Mangy Lover" (1964) and "The Groucho Letters" (1967).

Groucho and Chico briefly appeared in a 1957 color short film promoting "The Saturday Evening Post" entitled "Showdown at Ulcer Gulch," directed by animator Shamus Culhane, Chico's son-in-law. Groucho, Chico, and Harpo worked together (in separate scenes) in "The Story of Mankind" (1957). In 1959, the three began production of "Deputy Seraph", a TV series starring Harpo and Chico as blundering angels, and Groucho (in every third episode) as their boss, the "Deputy Seraph." The project was abandoned when Chico was found to be uninsurable (and incapable of memorizing his lines) due to severe arteriosclerosis. On March 8 of that year, Chico and Harpo starred as bumbling thieves in "The Incredible Jewel Robbery", a half-hour pantomimed episode of the "General Electric Theater" on CBS. Groucho made a cameo appearance—uncredited, because of constraints in his NBC contract—in the last scene, and delivered the only line of dialogue ("We won't talk until we see our lawyer!").
According to a September 1947 article in "Newsweek", Groucho, Harpo, Chico and Zeppo all signed to appear as themselves in a biopic entitled "The Life and Times of the Marx Brothers". In addition to being a non-fiction biography of the Marxes, the film would have featured the brothers reenacting much of their previously unfilmed material from both their vaudeville and Broadway eras. The film, had it been made, would have been the first performance by the Brothers as a quartet since 1933.

The five brothers made only one television appearance together, in 1957, on an early incarnation of "The Tonight Show" called "Tonight! America After Dark", hosted by Jack Lescoulie. Five years later (October 1, 1962) after Jack Paar's tenure, Groucho made a guest appearance to introduce the "Tonight Show's" new host, Johnny Carson.

Around 1960, the acclaimed director Billy Wilder considered writing and directing a new Marx Brothers film. Tentatively titled "A Day at the U.N.", it was to be a comedy of international intrigue set around the United Nations building in New York. Wilder had discussions with Groucho and Gummo, but the project was put on hold because of Harpo's ill-health and abandoned when Chico died in 1961. He was 74. Three years later, on September 28, 1964, Harpo died at the age of 75 of a heart attack one day after heart surgery.

In 1966 Filmation produced a pilot for a Marx Brothers cartoon. Groucho's voice was supplied by Pat Harrington Jr. and other voices were done by Ted Knight and Joe Besser.

In 1969, audio excerpts of dialogue from all five of the Marx Brothers' Paramount films were collected and released on an LP album, "The Original Voice Tracks from Their Greatest Movies", by Decca Records. The excerpts were interspersed with voice-over introductions by disc jockey and voice actor Gary Owens. The album was praised by "Billboard" as "a program of zany antics"; the magazine highlighted the excerpts of Groucho, who was "way ahead of his time in spoofing the 'establishment,' [and] at his hilarious biting best with his film soundtrack one-line zingers on his love life, his son, politics, big business, society, etc." "Village Voice" critic Robert Christgau was less enthusiastic, however, grading the LP a C-plus and recommending it only to fanatics of the comedy group. "This is the sort of record you buy out of duty and then never play, not because it's a comedy record but because it isn't funny out of context", Christgau wrote, while also expressing displeasure with the interspersing of small portions of "annoying music" and Owens's commentary throughout.

In 1970, the four Marx Brothers had a brief reunion of sorts in the animated ABC television special "The Mad, Mad, Mad Comedians", produced by Rankin-Bass animation (of "Rudolph the Red-Nosed Reindeer" fame). The special featured animated reworkings of various famous comedians' acts, including W. C. Fields, Jack Benny, George Burns, Henny Youngman, the Smothers Brothers, Flip Wilson, Phyllis Diller, Jack E. Leonard, George Jessel and the Marx Brothers. Most of the comedians provided their own voices for their animated counterparts, except for Fields and Chico Marx (both had died), and Zeppo Marx (who had left show business in 1933). Voice actor Paul Frees filled in for all three (no voice was needed for Harpo). The Marx Brothers' segment was a reworking of a scene from their Broadway play "I'll Say She Is", a parody of Napoleon that Groucho considered among the brothers' funniest routines. The sketch featured animated representations, if not the voices, of all four brothers. Romeo Muller is credited as having written special material for the show, but the script for the classic "Napoleon Scene" was probably supplied by Groucho.

On January 16, 1977, the Marx Brothers were inducted into the Motion Picture Hall of Fame. With the deaths of Gummo in April 1977, Groucho in August 1977, and Zeppo in November 1979, the brothers were gone. But their impact on the entertainment community continues well into the 21st century. Among famous comedians who have cited them as influences on their style have been Woody Allen, Alan Alda, Judd Apatow, Mel Brooks, John Cleese, Elliott Gould, Spike Milligan, Monty Python, Carl Reiner, as well as David Zucker, Jerry Zucker and Jim Abrahams. Comedian Frank Ferrante made impersonations of Groucho a career. Other celebrity fans of the comedy ensemble have been Antonin Artaud, The Beatles, Anthony Burgess, Alice Cooper, Robert Crumb, Salvador Dalí, Eugene Ionesco, George Gershwin (who dressed up as Groucho once), René Goscinny, Cédric Klapisch, J. D. Salinger and Kurt Vonnegut.

Salvador Dali once made a drawing depicting Harpo.

Peter Sellers imitates Groucho in "Let's Go Crazy" (1951).

In "The Way We Were" (1972) the main characters attend a party, dressed as the Marx Brothers. The real Groucho Marx also visited the set, of which a photograph was taken by David F. Smith.

In Woody Allen's "Take the Money and Run" (1969) Virgil's parents give an interview while wearing Groucho masks. "Annie Hall" (1977) starts off with a Groucho Marx joke, which is referred to again later. In "Manhattan" (1979), he names the Marx Brothers as the first thing that makes life worth living. In "Stardust Memories" there is a huge Groucho poster in the main character's flat. In Allen's film "Hannah and Her Sisters" (1986), Woody's character, after a suicide attempt, is inspired to go on living after seeing a revival showing of "Duck Soup". In "Everyone Says I Love You" (1996) (the title itself a reference to Groucho's famous song), Woody Allen and Goldie Hawn dress as Groucho for a Marx Brothers celebration in France, and the song "Hooray for Captain Spaulding", from "Animal Crackers", is performed, with various actors dressed as the brothers, striking poses famous to Marx fans. (The film itself is named after a song from "Horse Feathers", a version of which plays over the opening credits.) In "Mighty Aphrodite" Woody suggests Harpo and Groucho as names for his song.

In Terry Gilliam's "Brazil" (1985) a woman in a bathtub is watching "The Cocoanuts" when troops break into her house. In "Twelve Monkeys" (1996) the inmates of an insane asylum watch "Monkey Business" on TV.

In the 1989 film "Indiana Jones and the Last Crusade", Professor Henry Jones (Sean Connery) mails his diary to his son Indiana Jones (Harrison Ford) to keep it out of Nazi hands. When Indy misconstrues the purpose of being sent it and returns it to his father instead, his father berates him by saying "I should have mailed it to the Marx Brothers!"

In Rob Zombie's 2003 film "House of 1000 Corpses", the clown Captain Spaulding, as well as many other characters, are named after various Marx brothers characters. In the sequel, "The Devil’s Rejects ", a Marx Brothers expert is brought in to try to help the police get in to the minds of the fugitives who use their character names. 

In the Fleischer Brothers' "Betty Boop" cartoon "Betty in Blunderland" (1934) Betty sings "Everyone Says I Love You", a song owned by Paramount Pictures, which also owned Betty's cartoons as well as the Marx Brothers film it was taken from: "Horse Feathers".

The Marx Brothers have cameos in the Disney cartoons "The Bird Store" (1932), "Mickey's Gala Premier" (1932), "Mickey's Polo Team" (1936), "Mother Goose Goes Hollywood" (1938) and "The Autograph Hound" (1939). Dopey in "Snow White and the Seven Dwarfs" was inspired by Harpo's mute performances.

Tex Avery's cartoon "Hollywood Steps Out" (1941) features appearances by Harpo and Groucho. Bugs Bunny impersonated Groucho Marx in the 1947 cartoon "Slick Hare" (with Elmer Fudd dressing up as Harpo and chasing him with a cleaver), and in "Wideo Wabbit" (1956) he again impersonated Groucho hosting a TV show called "You Beat Your Wife", asking Elmer Fudd if he had stopped beating his wife.

Many television shows and movies have used Marx Brothers references. "Animaniacs" and "Tiny Toons", for example, have featured Marx Brothers jokes and skits.

The Genie imitates the Marx Brothers in "Aladdin and the King of Thieves".

An episode of "Histeria!" about Communism portrays Groucho and Chico, respectively, as Karl Marx and Friedrich Engels.

Harpo Marx appeared as himself on a 1955 episode of "I Love Lucy" in which first, he performed "Take Me Out to the Ball Game" on his harp, then, he and Lucille Ball reprised the mirror routine from "Duck Soup", with Lucy dressed up as Harpo. Lucy had worked with the Marxes when she appeared in a supporting role in an earlier Marx Brothers film, "Room Service". Chico once appeared on "I've Got a Secret" dressed up as Harpo; his secret was shown in a caption reading, "I'm pretending to be Harpo Marx (I'm Chico)".

Hawkeye Pierce (Alan Alda) on "M*A*S*H" occasionally put on a fake nose and glasses, and, holding a cigar, did a Groucho impersonation to amuse patients recovering from surgery. Early episodes also featured a singing and off-scene character named Captain Spaulding as a tribute.

In the second episode of "The Muppet Show" Kermit the Frog sings "Lydia the Tattooed Lady".

In the "Airwolf" episode "Condemned", four anti-virus formulae for a deadly plague were named after the four Marx Brothers.

In "All in the Family", Rob Reiner often did imitations of Groucho, and Sally Struthers dressed as Harpo in one episode in which she (as Gloria Stivic) and Rob (as Mike Stivic) were going to a Marx Brothers film festival, with Reiner dressing as Groucho.

Gabe Kaplan did many Groucho imitations on "Welcome Back, Kotter" and Robert Hegyes sometimes imitated both Chico and Harpo on the show.

In an episode of "The Mary Tyler Moore Show" Murray calls the new station owner at home late at night to complain when the song "Hooray for Captain Spaulding" is cut from a showing of "Animal Crackers" because of the new owners' policy to cut more and more from shows to sell more ad time, putting his job on the line.

In 1990 three puppets were made of Groucho, Harpo and Chico for the satirical TV show "Spitting Image". They were later used to portray the hunters in a 1994 TV production of "Peter and the Wolf", with Sting as narrator and puppets from the series as characters.

The Marx Brothers were spoofed in the second act of the Broadway Review "A Day in Hollywood/A Night in the Ukraine".

In the 1996 musical "By Jeeves", based on the Jeeves stories by P.G. Wodehouse, during "The Hallo Song", Gussie Fink-Nottle suggests "You're either Pablo Picasso", to which Cyrus Budge III replies "or maybe Harpo Marx!"

Jacques Brel's song "Le Gaz" was inspired by the cabin scene in "A Night at the Opera".

Rock band Queen named two of their albums after Marx Brothers films; "A Night at the Opera" (1975) and "A Day at the Races" (1976), and in Freddie Mercury's solo album "Mr. Bad Guy " in the song titled “Living on My Own” he sings; "I ain't got no time for no Monkey Business." In 2002 the band Blind Guardian would also name an album "A Night at the Opera".

Groucho Marx can be seen on the cover of "Alice Cooper's Greatest Hits" by Alice Cooper. English punk band The Damned named their single "There Ain't No Sanity Clause" (1980), in reference to a famous quote from "A Night at the Opera". On the 1988 album "Modern Lovers '88" by Modern Lovers there is a track called "When Harpo Played His Harp". The band Karl and the Marx Brothers takes their name from them.

Jack Kerouac wrote a poem "To Harpo Marx".

Ron Goulart wrote six books between 1998 and 2005 where Groucho Marx was a detective.

In the Vlasic Pickles commercials, the stork associated with the product holds a pickle the way Groucho held a cigar and, in a Groucho voice, says, "Now that's the best tastin' pickle I ever heard!" and bites into the pickle.

Films with the four Marx Brothers:

Films with the three Marx Brothers (post-Zeppo):

Solo endeavors:

In the 1974 Academy Awards telecast, Jack Lemmon presented Groucho with an honorary Academy Award to a standing ovation. The award was also on behalf of Harpo, Chico, and Zeppo, whom Lemmon mentioned by name. It was one of Groucho's final major public appearances. "I wish that Harpo and Chico could be here to share with me this great honor", he said, naming the two deceased brothers (Zeppo was still alive at the time and in the audience). Groucho also praised the late Margaret Dumont as a great straight woman who never understood any of his jokes.

The Marx Brothers were collectively named #20 on AFI's list of the Top 25 American male screen legends of Classic Hollywood. They are the only group to be so honored.

The "Sweathogs" of the ABC-TV series "Welcome Back Kotter" (John Travolta, Robert Hegyes, Lawrence Hilton-Jacobs, and Ron Palillo) patterned much of their on-camera banter in that series after the Marx Brothers. Series star Gabe Kaplan was reputedly a big Marx Brothers fan.





</doc>
<doc id="19672" url="https://en.wikipedia.org/wiki?curid=19672" title="May 28">
May 28





</doc>
<doc id="19673" url="https://en.wikipedia.org/wiki?curid=19673" title="MP3">
MP3

MP3 (formally MPEG-1 Audio Layer III or MPEG-2 Audio Layer III) is a coding format for digital audio. Originally defined as the third audio format of the MPEG-1 standard, it was retained and further extended—defining additional bit-rates and support for more audio channels—as the third audio format of the subsequent MPEG-2 standard. A third version, known as MPEG 2.5—extended to better support lower bit rates—is commonly implemented, but is not a recognized standard.

MP3 (or mp3) as a file format commonly designates files containing an elementary stream of MPEG-1 Audio or MPEG-2 Audio encoded data, without other complexities of the MP3 standard.

In regard to audio compression (the aspect of the standard most apparent to end-users, and for which it is best known), MP3 uses lossy data-compression to encode data using inexact approximations and the partial discarding of data. This allows a large reduction in file sizes when compared to uncompressed audio. The combination of small size and acceptable fidelity led to a boom in the distribution of music over the Internet in the mid- to late-1990s, with MP3 serving as an enabling technology at a time when bandwidth and storage were still at a premium. The MP3 format soon became associated with controversies surrounding copyright infringement, music piracy, and the file ripping/sharing services MP3.com and Napster, among others. With the advent of portable media players, a product category also including smartphones, MP3 support remains near-universal.

MP3 compression works by reducing (or approximating) the accuracy of certain components of sound that are considered (by psychoacoustic analysis) to be beyond the hearing capabilities of most humans. This method is commonly referred to as perceptual coding or as psychoacoustic modeling. The remaining audio information is then recorded in a space-efficient manner, using MDCT and FFT algorithms. Compared to CD-quality digital audio, MP3 compression can commonly achieve a 75 to 95% reduction in size. For example, an MP3 encoded at a constant bitrate of 128 kbit/s would result in a file approximately 9% of the size of the original CD audio. In the early 2000s, compact disc players increasingly adapted support for playback of MP3 files on data CDs.

The Moving Picture Experts Group (MPEG) designed MP3 as part of its MPEG-1, and later MPEG-2, standards. MPEG-1 Audio (MPEG-1 Part 3), which included MPEG-1 Audio Layer I, II and III, was approved as a committee draft for an ISO/IEC standard in 1991, finalised in 1992, and published in 1993 as ISO/IEC 11172-3:1993. An MPEG-2 Audio (MPEG-2 Part 3) extension with lower sample- and bit-rates was published in 1995 as ISO/IEC 13818-3:1995. It requires only minimal modifications to existing MPEG-1 decoders (recognition of the MPEG-2 bit in the header and addition of the new lower sample and bit rates).

The MP3 lossy audio-data compression algorithm takes advantage of a perceptual limitation of human hearing called auditory masking. In 1894 the American physicist Alfred M. Mayer reported that a tone could be rendered inaudible by another tone of lower frequency. In 1959 Richard Ehmer described a complete set of auditory curves regarding this phenomenon. Between 1967 and 1974, Eberhard Zwicker did work in the areas of tuning and masking of critical frequency-bands, which in turn built on the fundamental research in the area from Harvey Fletcher and his collaborators at Bell Labs.

Perceptual coding was first used for speech coding compression with linear predictive coding (LPC), which has origins in the work of Fumitada Itakura (Nagoya University) and Shuzo Saito (Nippon Telegraph and Telephone) in 1966. In 1978, Bishnu S. Atal and Manfred R. Schroeder at Bell Labs proposed an LPC speech codec, called adaptive predictive coding, that used a psychoacoustic coding-algorithm exploiting the masking properties of the human ear. Further optimisation by Schroeder and Atal with J.L. Hall was later reported in a 1979 paper. That same year, a psychoacoustic masking codec was also proposed by M. A. Krasner, who published and produced hardware for speech (not usable as music bit-compression), but the publication of his results in a relatively obscure Lincoln Laboratory Technical Report did not immediately influence the mainstream of psychoacoustic codec-development.

The discrete cosine transform (DCT), a type of transform coding for lossy compression, proposed by Nasir Ahmed in 1972, was developed by Ahmed with T. Natarajan and K. R. Rao in 1973; they published their results in 1974. This led to the development of the modified discrete cosine transform (MDCT), proposed by J. P. Princen, A. W. Johnson and A. B. Bradley in 1987, following earlier work by Princen and Bradley in 1986. The MDCT later became a core part of the MP3 algorithm.

Ernst Terhardt "et al." constructed an algorithm describing auditory masking with high accuracy in 1982. This work added to a variety of reports from authors dating back to Fletcher, and to the work that initially determined critical ratios and critical bandwidths.

In 1985 Atal and Schroeder presented code-excited linear prediction (CELP), an LPC-based perceptual speech-coding algorithm with auditory masking that achieved a significant compression ratio for its time. IEEE's refereed "Journal on Selected Areas in Communications" reported on a wide variety of (mostly perceptual) audio compression algorithms in 1988. The "Voice Coding for Communications" edition published in February 1988 reported on a wide range of established, working audio bit compression technologies, some of them using auditory masking as part of their fundamental design, and several showing real-time hardware implementations.

The genesis of the MP3 technology is fully described in a paper from Professor Hans Musmann, who chaired the ISO MPEG Audio group for several years. In December 1988, MPEG called for an audio coding standard. In June 1989, 14 audio coding algorithms were submitted. Because of certain similarities between these coding proposals, they were clustered into four development groups. The first group was MUSICAM, by Matsushita, CCETT, ITT and Philips. The second group was ASPEC, by AT&T, France Telecom, Fraunhofer Gesellschaft, Deutsche and Thomson-Brandt. The third group was ATAC, by Fujitsu, JVC, NEC and Sony. And the fourth group was SB-ADPCM, by NTT and BTRL.

The immediate predecessors of MP3 were "Optimum Coding in the Frequency Domain" (OCF), and Perceptual Transform Coding (PXFM). These two codecs, along with block-switching contributions from Thomson-Brandt, were merged into a codec called ASPEC, which was submitted to MPEG, and which won the quality competition, but that was mistakenly rejected as too complex to implement. The first practical implementation of an audio perceptual coder (OCF) in hardware (Krasner's hardware was too cumbersome and slow for practical use), was an implementation of a psychoacoustic transform coder based on Motorola 56000 DSP chips.

Another predecessor of the MP3 format and technology is to be found in the perceptual codec MUSICAM based on an integer arithmetics 32 sub-bands filterbank, driven by a psychoacoustic model. It was primarily designed for Digital Audio Broadcasting (digital radio) and digital TV, and its basic principles disclosed to the scientific community by CCETT (France) and IRT (Germany) in Atlanta during an IEEE-ICASSP conference in 1991, after having worked on MUSICAM with Matsushita and Philips since 1989.

This codec incorporated into a broadcasting system using COFDM modulation was demonstrated on air and on the field together with Radio Canada and CRC Canada during the NAB show (Las Vegas) in 1991. The implementation of the audio part of this broadcasting system was based on a two chips encoder (one for the subband transform, one for the psychoacoustic model designed by the team of G. Stoll (IRT Germany), later known as psychoacoustic model I) and a real time decoder using one Motorola 56001 DSP chip running an integer arithmetics software designed by Y.F. Dehery's team (CCETT, France). The simplicity of the corresponding decoder together with the high audio quality of this codec using for the first time a 48 kHz sampling frequency, a 20 bits/sample input format (the highest available sampling standard in 1991, compatible with the AES/EBU professional digital input studio standard) were the main reasons to later adopt the characteristics of MUSICAM as the basic features for an advanced digital music compression codec.

During the development of the MUSICAM encoding software, Stoll and Dehery's team made a thorough use of a set of high quality audio assessment material selected by a group of audio professionals from the European Broadcasting Union and later used as a reference for the assessment of music compression codecs. The subband coding technique was found to be efficient, not only for the perceptual coding of the high quality sound materials but especially for the encoding of critical percussive sound materials (drums, triangle, ..) due to the specific temporal masking effect of the MUSICAM sub-band filterbank (this advantage being a specific feature of short transform coding techniques).

As a doctoral student at Germany's University of Erlangen-Nuremberg, Karlheinz Brandenburg began working on digital music compression in the early 1980s, focusing on how people perceive music. He completed his doctoral work in 1989. MP3 is directly descended from OCF and PXFM, representing the outcome of the collaboration of Brandenburg—working as a postdoc at AT&T-Bell Labs with James D. Johnston ("JJ") of AT&T-Bell Labs—with the Fraunhofer Institute for Integrated Circuits, Erlangen (where he worked with Bernhard Grill and four other researchers – "The Original Six"), with relatively minor contributions from the MP2 branch of psychoacoustic sub-band coders. In 1990, Brandenburg became an assistant professor at Erlangen-Nuremberg. While there, he continued to work on music compression with scientists at the Fraunhofer Society's Heinrich Herz Institute (in 1993 he joined the staff of Fraunhofer HHI). The song "Tom's Diner" by Suzanne Vega was the first song used by Karlheinz Brandenburg to develop the MP3. Brandenburg adopted the song for testing purposes, listening to it again and again each time refining the scheme, making sure it did not adversely affect the subtlety of Vega's voice.

In 1991, there were two available proposals that were assessed for an MPEG audio standard: MUSICAM (Masking pattern adapted Universal Subband Integrated Coding And Multiplexing) and ASPEC (Adaptive Spectral Perceptual Entropy Coding). The MUSICAM technique, proposed by Philips (Netherlands), CCETT (France), the Institute for Broadcast Technology (Germany), and Matsushita (Japan), was chosen due to its simplicity and error robustness, as well as for its high level of computational efficiency. The MUSICAM format, based on sub-band coding, became the basis for the MPEG Audio compression format, incorporating, for example, its frame structure, header format, sample rates, etc.

While much of MUSICAM technology and ideas were incorporated into the definition of MPEG Audio Layer I and Layer II, the filter bank alone and the data structure based on 1152 samples framing (file format and byte oriented stream) of MUSICAM remained in the Layer III (MP3) format, as part of the computationally inefficient hybrid filter bank. Under the chairmanship of Professor Musmann of the Leibniz University Hannover, the editing of the standard was delegated to Leon van de Kerkhof (Netherlands), Gerhard Stoll (Germany), and Yves-François Dehery (France), who worked on Layer I and Layer II. ASPEC was the joint proposal of AT&T Bell Laboratories, Thomson Consumer Electronics, Fraunhofer Society and CNET. It provided the highest coding efficiency.

A working group consisting of van de Kerkhof, Stoll, Leonardo Chiariglione (CSELT VP for Media), Yves-François Dehery, Karlheinz Brandenburg (Germany) and James D. Johnston (United States) took ideas from ASPEC, integrated the filter bank from Layer II, added some of their own ideas such as the joint stereo coding of MUSICAM and created the MP3 format, which was designed to achieve the same quality at 128 kbit/s as MP2 at 192 kbit/s.

The algorithms for MPEG-1 Audio Layer I, II and III were approved in 1991 and finalized in 1992 as part of MPEG-1, the first standard suite by MPEG, which resulted in the international standard ISO/IEC 11172-3 (a.k.a. "MPEG-1 Audio" or "MPEG-1 Part 3"), published in 1993. Files or data streams conforming to this standard must handle sample rates of 48k, 44100 and 32k and continue to be supported by current MP3 players and decoders. Thus the first generation of MP3 defined interpretations of MP3 frame data structures and size layouts.

Further work on MPEG audio was finalized in 1994 as part of the second suite of MPEG standards, MPEG-2, more formally known as international standard ISO/IEC 13818-3 (a.k.a. "MPEG-2 Part 3" or backwards compatible "MPEG-2 Audio" or "MPEG-2 Audio BC"), originally published in 1995. MPEG-2 Part 3 (ISO/IEC 13818-3) defined 42 additional bit rates and sample rates for MPEG-1 Audio Layer I, II and III. The new sampling rates are exactly half that of those originally defined in MPEG-1 Audio. This reduction in sampling rate serves to cut the available frequency fidelity in half while likewise cutting the bitrate by 50%.
MPEG-2 Part 3 also enhanced MPEG-1's audio by allowing the coding of audio programs with more than two channels, up to 5.1 multichannel. An MP3 coded with MPEG-2 results in half of the bandwidth reproduction of MPEG-1 appropriate for piano and singing.

A third generation of "MP3" style data streams (files) extended the "MPEG-2" ideas and implementation but was named "MPEG-2.5" audio, since MPEG-3 already had a different meaning. This extension was developed at Fraunhofer IIS, the registered patent holders of MP3 by reducing the frame sync field in the MP3 header from 12 to 11 bits. As in the transition from MPEG-1 to MPEG-2, MPEG-2.5 adds additional sampling rates exactly half of those available using MPEG-2. It thus widens the scope of MP3 to include human speech and other applications yet requires only 25% of the bandwidth (frequency reproduction) possible using MPEG-1 sampling rates. While not an ISO recognized standard, MPEG-2.5 is widely supported by both inexpensive Chinese and brand name digital audio players as well as computer software based MP3 encoders (LAME), decoders (FFmpeg) and players (MPC) adding additional MP3 frame types. Each generation of MP3 thus supports 3 sampling rates exactly half that of the previous generation for a total of 9 varieties of MP3 format files. The sample rate comparison table between MPEG-1, 2 and 2.5 is given later in the article. MPEG-2.5 is supported by LAME (since 2000), Media Player Classic (MPC), iTunes, and FFmpeg.

MPEG-2.5 was not developed by MPEG (see above) and was never approved as an international standard. MPEG-2.5 is thus an unofficial or proprietary extension to the MP3 format. It is nonetheless ubiquitous and especially advantageous for low-bit rate human speech applications.
The ISO standard ISO/IEC 11172-3 (a.k.a. MPEG-1 Audio) defined three formats: the MPEG-1 Audio Layer I, Layer II and Layer III. The ISO standard ISO/IEC 13818-3 (a.k.a. MPEG-2 Audio) defined extended version of the MPEG-1 Audio: MPEG-2 Audio Layer I, Layer II and Layer III. MPEG-2 Audio (MPEG-2 Part 3) should not be confused with MPEG-2 AAC (MPEG-2 Part 7 – ISO/IEC 13818-7).

Compression efficiency of encoders is typically defined by the bit rate, because compression ratio depends on the bit depth and sampling rate of the input signal. Nevertheless, compression ratios are often published. They may use the Compact Disc (CD) parameters as references (44.1 kHz, 2 channels at 16 bits per channel or 2×16 bit), or sometimes the Digital Audio Tape (DAT) SP parameters (48 kHz, 2×16 bit). Compression ratios with this latter reference are higher, which demonstrates the problem with use of the term "compression ratio" for lossy encoders.

Karlheinz Brandenburg used a CD recording of Suzanne Vega's song "Tom's Diner" to assess and refine the MP3 compression algorithm. This song was chosen because of its nearly monophonic nature and wide spectral content, making it easier to hear imperfections in the compression format during playbacks. Some refer to Suzanne Vega as "The mother of MP3". This particular track has an interesting property in that the two channels are almost, but not completely, the same, leading to a case where Binaural Masking Level Depression causes spatial unmasking of noise artifacts unless the encoder properly recognizes the situation and applies corrections similar to those detailed in the MPEG-2 AAC psychoacoustic model. Some more critical audio excerpts (glockenspiel, triangle, accordion, etc.) were taken from the EBU V3/SQAM reference compact disc and have been used by professional sound engineers to assess the subjective quality of the MPEG Audio formats. LAME is the most advanced MP3 encoder. LAME includes a VBR variable bit rate encoding which uses a quality parameter rather than a bit rate goal. Later versions 2008+) support an n.nnn quality goal which automatically selects MPEG-2 or MPEG-2.5 sampling rates as appropriate for human speech recordings which need only 5512 Hz bandwidth resolution.

A reference simulation software implementation, written in the C language and later known as "ISO 11172-5", was developed (in 1991–1996) by the members of the ISO MPEG Audio committee in order to produce bit compliant MPEG Audio files (Layer 1, Layer 2, Layer 3). It was approved as a committee draft of ISO/IEC technical report in March 1994 and printed as document CD 11172-5 in April 1994. It was approved as a draft technical report (DTR/DIS) in November 1994, finalized in 1996 and published as international standard ISO/IEC TR 11172-5:1998 in 1998. The reference software in C language was later published as a freely available ISO standard. Working in non-real time on a number of operating systems, it was able to demonstrate the first real time hardware decoding (DSP based) of compressed audio. Some other real time implementation of MPEG Audio encoders and decoders were available for the purpose of digital broadcasting (radio DAB, television DVB) towards consumer receivers and set top boxes.

On 7 July 1994, the Fraunhofer Society released the first software MP3 encoder, called l3enc. The filename extension ".mp3" was chosen by the Fraunhofer team on 14 July 1995 (previously, the files had been named ".bit"). With the first real-time software MP3 player WinPlay3 (released 9 September 1995) many people were able to encode and play back MP3 files on their PCs. Because of the relatively small hard drives of the era (≈500–1000 MB) lossy compression was essential to store multiple albums' worth of music on a home computer as full recordings (as opposed to MIDI notation, or tracker files which combined notation with short recordings of instruments playing single notes). As sound scholar Jonathan Sterne notes, "An Australian hacker acquired l3enc using a stolen credit card. The hacker then reverse-engineered the software, wrote a new user interface, and redistributed it for free, naming it "thank you Fraunhofer"".

A hacker named SoloH discovered the source code of the "dist10" MPEG reference implementation shortly after the release on the servers of the University of Erlangen. He developed a higher-quality version and spread it on the internet. This code started the widespread CD ripping and digital music distribution as MP3 over the internet.

In the second half of the 1990s, MP3 files began to spread on the Internet, often via underground pirated song networks. The first known experiment in Internet distribution was organized in the early 1990s by the Internet Underground Music Archive, better known by the acronym IUMA. After some experiments using uncompressed audio files, this archive started to deliver on the native worldwide low speed Internet some compressed MPEG Audio files using the MP2 (Layer II) format and later on used MP3 files when the standard was fully completed. The popularity of MP3s began to rise rapidly with the advent of Nullsoft's audio player Winamp, released in 1997. In 1998, the first portable solid state digital audio player MPMan, developed by SaeHan Information Systems which is headquartered in Seoul, South Korea, was released and the Rio PMP300 was sold afterwards in 1998, despite legal suppression efforts by the RIAA.

In November 1997, the website mp3.com was offering thousands of MP3s created by independent artists for free. The small size of MP3 files enabled widespread peer-to-peer file sharing of music ripped from CDs, which would have previously been nearly impossible. The first large peer-to-peer filesharing network, Napster, was launched in 1999. The ease of creating and sharing MP3s resulted in widespread copyright infringement. Major record companies argued that this free sharing of music reduced sales, and called it "music piracy". They reacted by pursuing lawsuits against Napster (which was eventually shut down and later sold) and against individual users who engaged in file sharing.

Unauthorized MP3 file sharing continues on next-generation peer-to-peer networks. Some authorized services, such as Beatport, Bleep, Juno Records, eMusic, Zune Marketplace, Walmart.com, Rhapsody, the recording industry approved re-incarnation of Napster, and Amazon.com sell unrestricted music in the MP3 format.

An MP3 file is made up of MP3 frames, which consist of a header and a data block. This sequence of frames is called an elementary stream. Due to the "bit reservoir", frames are not independent items and cannot usually be extracted on arbitrary frame boundaries. The MP3 Data blocks contain the (compressed) audio information in terms of frequencies and amplitudes. The diagram shows that the MP3 Header consists of a sync word, which is used to identify the beginning of a valid frame. This is followed by a bit indicating that this is the MPEG standard and two bits that indicate that layer 3 is used; hence MPEG-1 Audio Layer 3 or MP3. After this, the values will differ, depending on the MP3 file. "ISO/IEC 11172-3" defines the range of values for each section of the header along with the specification of the header. Most MP3 files today contain ID3 metadata, which precedes or follows the MP3 frames, as noted in the diagram. The data stream can contain an optional checksum.

Joint stereo is done only on a frame-to-frame basis.

The MP3 encoding algorithm is generally split into four parts. Part 1 divides the audio signal into smaller pieces, called frames, and a modified discrete cosine transform (MDCT) filter is then performed on the output. Part 2 passes the sample into a 1024-point fast Fourier transform (FFT), then the psychoacoustic model is applied and another MDCT filter is performed on the output. Part 3 quantifies and encodes each sample, known as noise allocation, which adjusts itself in order to meet the bit rate and sound masking requirements. Part 4 formats the bitstream, called an audio frame, which is made up of 4 parts, the header, error check, audio data, and ancillary data.

The MPEG-1 standard does not include a precise specification for an MP3 encoder, but does provide example psychoacoustic models, rate loop, and the like in the non-normative part of the original standard.
MPEG-2 doubles the number of sampling rates which are supported and MPEG-2.5 adds 3 more. When this was written, the suggested implementations were quite dated. Implementers of the standard were supposed to devise their own algorithms suitable for removing parts of the information from the audio input. As a result, many different MP3 encoders became available, each producing files of differing quality. Comparisons were widely available, so it was easy for a prospective user of an encoder to research the best choice. Some encoders that were proficient at encoding at higher bit rates (such as LAME) were not necessarily as good at lower bit rates. Over time, LAME evolved on the SourceForge website until it became the de facto CBR MP3 encoder. Later an ABR mode was added. Work progressed on true variable bit rate using a quality goal between 0 and 10. Eventually numbers (such as -V 9.600) could generate excellent quality low bit rate voice encoding at only 41 kbit/s using the MPEG-2.5 extensions.

During encoding, 576 time-domain samples are taken and are transformed to 576 frequency-domain samples. If there is a transient, 192 samples are taken instead of 576. This is done to limit the temporal spread of quantization noise accompanying the transient. (See psychoacoustics.) Frequency resolution is limited by the small long block window size, which decreases coding efficiency. Time resolution can be too low for highly transient signals and may cause smearing of percussive sounds.

Due to the tree structure of the filter bank, pre-echo problems are made worse, as the combined impulse response of the two filter banks does not, and cannot, provide an optimum solution in time/frequency resolution. Additionally, the combining of the two filter banks' outputs creates aliasing problems that must be handled partially by the "aliasing compensation" stage; however, that creates excess energy to be coded in the frequency domain, thereby decreasing coding efficiency.

Decoding, on the other hand, is carefully defined in the standard. Most decoders are "bitstream compliant", which means that the decompressed output that they produce from a given MP3 file will be the same, within a specified degree of rounding tolerance, as the output specified mathematically in the ISO/IEC high standard document (ISO/IEC 11172-3). Therefore, comparison of decoders is usually based on how computationally efficient they are (i.e., how much memory or CPU time they use in the decoding process). Over time this concern has become less of an issue as CPU speeds transitioned from MHz to GHz. Encoder/decoder overall delay is not defined, which means there is no official provision for gapless playback. However, some encoders such as LAME can attach additional metadata that will allow players that can handle it to deliver seamless playback.

When performing lossy audio encoding, such as creating an MP3 data stream, there is a trade-off between the amount of data generated and the sound quality of the results. The person generating an MP3 selects a bit rate, which specifies how many kilobits per second of audio is desired. The higher the bit rate, the larger the MP3 data stream will be, and, generally, the closer it will sound to the original recording. With too low a bit rate, compression artifacts (i.e., sounds that were not present in the original recording) may be audible in the reproduction. Some audio is hard to compress because of its randomness and sharp attacks. When this type of audio is compressed, artifacts such as ringing or pre-echo are usually heard. A sample of applause or a triangle instrument with a relatively low bit rate provide good examples of compression artifacts. Most subjective testings of perceptual codecs tend to avoid using these types of sound materials, however, the artifacts generated by percussive sounds are barely perceptible due to the specific temporal masking feature of the 32 sub-band filterbank of Layer II on which the format is based.

Besides the bit rate of an encoded piece of audio, the quality of MP3 encoded sound also depends on the quality of the encoder algorithm as well as the complexity of the signal being encoded. As the MP3 standard allows quite a bit of freedom with encoding algorithms, different encoders do feature quite different quality, even with identical bit rates. As an example, in a public listening test featuring two early MP3 encoders set at about 128 kbit/s, one scored 3.66 on a 1–5 scale, while the other scored only 2.22. Quality is dependent on the choice of encoder and encoding parameters.

This observation caused a revolution in audio encoding. Early on bitrate was the prime and only consideration. At the time MP3 files were of the very simplest type: they used the same bit rate for the entire file: this process is known as Constant Bit Rate (CBR) encoding. Using a constant bit rate makes encoding simpler and less CPU intensive. However, it is also possible to create files where the bit rate changes throughout the file. These are known as Variable Bit Rate. The bit reservoir and VBR encoding were actually part of the original MPEG-1 standard. The concept behind them is that, in any piece of audio, some sections are easier to compress, such as silence or music containing only a few tones, while others will be more difficult to compress. So, the overall quality of the file may be increased by using a lower bit rate for the less complex passages and a higher one for the more complex parts. With some advanced MP3 encoders, it is possible to specify a given quality, and the encoder will adjust the bit rate accordingly. Users that desire a particular "quality setting" that is transparent to their ears can use this value when encoding all of their music, and generally speaking not need to worry about performing personal listening tests on each piece of music to determine the correct bit rate.

Perceived quality can be influenced by listening environment (ambient noise), listener attention, and listener training and in most cases by listener audio equipment (such as sound cards, speakers and headphones). Furthermore, sufficient quality may be achieved by a lesser quality setting for lectures and human speech applications and reduces encoding time and complexity. A test given to new students by Stanford University Music Professor Jonathan Berger showed that student preference for MP3-quality music has risen each year. Berger said the students seem to prefer the 'sizzle' sounds that MP3s bring to music.

An in-depth study of MP3 audio quality, sound artist and composer Ryan Maguire's project "The Ghost in the MP3" isolates the sounds lost during MP3 compression. In 2015, he released the track "moDernisT" (an anagram of "Tom's Diner"), composed exclusively from the sounds deleted during MP3 compression of the song "Tom's Diner", the track originally used in the formulation of the MP3 standard. A detailed account of the techniques used to isolate the sounds deleted during MP3 compression, along with the conceptual motivation for the project, was published in the 2014 Proceedings of the International Computer Music Conference.

Bitrate is the product of the sample rate and number of bits per sample used to encode the music. CD audio is 44100 samples per second. The number of bits per sample also depends on the number of audio channels. CD is stereo and 16 bits per channel. So, multiplying 44100 by 32 gives 1411200—the bitrate of uncompressed CD digital audio. MP3 was designed to encode this 1411 kbit/s data at 320 kbit/s or less. As less complex passages are detected by MP3 algorithms then lower bitrates may be employed. When using MPEG-2 instead of MPEG-1, MP3 supports only lower sampling rates (16000, 22050 or 24000 samples per second) and offers choices of bitrate as low as 8 kbit/s but no higher than 160 kbit/s. By lowering the sampling rate, MPEG-2 layer III removes all frequencies above half the new sampling rate that may have been present in the source audio.

As shown in these two tables, 14 selected bit rates are allowed in MPEG-1 Audio Layer III standard: 32, 40, 48, 56, 64, 80, 96, 112, 128, 160, 192, 224, 256 and 320 kbit/s, along with the 3 highest available sampling frequencies of 32, 44.1 and 48 kHz. MPEG-2 Audio Layer III also allows 14 somewhat different (and mostly lower) bit rates of 8, 16, 24, 32, 40, 48, 56, 64, 80, 96, 112, 128, 144, 160 kbit/s with sampling frequencies of 16, 22.05 and 24 kHz which are exactly half that of MPEG-1 MPEG-2.5 Audio Layer III frames are limited to only 8 bit rates of 8, 16, 24, 32, 40, 48, 56 and 64 kbit/s with 3 even lower sampling frequencies of 8, 11.025, and 12 kHz. On earlier systems that only support the MPEG-1 Audio Layer III standard, MP3 files with a bit rate below 32 kbit/s might be played back sped and pitched up.

Earlier systems also lack fast forwarding and rewinding playback controls on MP3.

MPEG-1 frames contain the most detail in 320 kbit/s mode, the highest allowable bit rate setting, with silence and simple tones still requiring 32 kbit/s. MPEG-2 frames can capture up to 12 kHz sound reproductions needed up to 160 kbit/s. MP3 files made with MPEG-2 don't have 20 kHz bandwidth because of the Nyquist–Shannon sampling theorem. Frequency reproduction is always strictly less than half of the sampling frequency, and imperfect filters require a larger margin for error (noise level versus sharpness of filter), so an 8 kHz sampling rate limits the maximum frequency to 4 kHz, while a 48 kHz sampling rate limits an MP3 to a maximum 24 kHz sound reproduction. MPEG-2 uses half and MPEG-2.5 only a quarter of MPEG-1 sample rates.

For the general field of human speech reproduction, a bandwidth of 5512 Hz is sufficient to produce excellent results (for voice) using the sampling rate of 11025 and VBR encoding from 44100 (standard) WAV file. English speakers average 41–42 kbit/s with -V 9.6 setting but this may vary with amount of silence recorded or the rate of delivery (wpm). Resampling to 12000 (6K bandwidth) is selected by the LAME parameter -V 9.4 Likewise -V 9.2 selects 16000 sample rate and a resultant 8K lowpass filtering. For more info see Nyquist – Shannon. Older versions of LAME and FFmpeg only support integer arguments for variable bit rate quality selection parameter. The n.nnn quality parameter (-V) is documented at lame.sourceforge.net but is only supported in LAME with the new style VBR variable bit rate quality selector—not average bit rate (ABR).

A sample rate of 44.1 kHz is commonly used for music reproduction, because this is also used for CD audio, the main source used for creating MP3 files. A great variety of bit rates are used on the Internet. A bit rate of 128 kbit/s is commonly used, at a compression ratio of 11:1, offering adequate audio quality in a relatively small space. As Internet bandwidth availability and hard drive sizes have increased, higher bit rates up to 320 kbit/s are widespread. Uncompressed audio as stored on an audio-CD has a bit rate of 1,411.2 kbit/s, (16 bit/sample × 44100 samples/second × 2 channels / 1000 bits/kilobit), so the bitrates 128, 160 and 192 kbit/s represent compression ratios of approximately 11:1, 9:1 and 7:1 respectively.

Non-standard bit rates up to 640 kbit/s can be achieved with the LAME encoder and the freeformat option, although few MP3 players can play those files. According to the ISO standard, decoders are only required to be able to decode streams up to 320 kbit/s. Early MPEG Layer III encoders used what is now called Constant Bit Rate (CBR). The software was only able to use a uniform bitrate on all frames in an MP3 file. Later more sophisticated MP3 encoders were able to use the bit reservoir to target an average bit rate selecting the encoding rate for each frame based on the complexity of the sound in that portion of the recording.

A more sophisticated MP3 encoder can produce variable bitrate audio. MPEG audio may use bitrate switching on a per-frame basis, but only layer III decoders must support it. VBR is used when the goal is to achieve a fixed level of quality. The final file size of a VBR encoding is less predictable than with constant bitrate. Average bitrate is a type of VBR implemented as a compromise between the two: the bitrate is allowed to vary for more consistent quality, but is controlled to remain near an average value chosen by the user, for predictable file sizes. Although an MP3 decoder must support VBR to be standards compliant, historically some decoders have bugs with VBR decoding, particularly before VBR encoders became widespread. The most evolved LAME MP3 encoder supports the generation of VBR, ABR, and even the older CBR MP3 formats.

Layer III audio can also use a "bit reservoir", a partially full frame's ability to hold part of the next frame's audio data, allowing temporary changes in effective bitrate, even in a constant bitrate stream. Internal handling of the bit reservoir increases encoding delay. There is no scale factor band 21 (sfb21) for frequencies above approx 16 kHz, forcing the encoder to choose between less accurate representation in band 21 or less efficient storage in all bands below band 21, the latter resulting in wasted bitrate in VBR encoding.

The ancillary data field can be used to store user defined data. The ancillary data is optional and the number of bits available is not explicitly given. The ancillary data is located after the Huffman code bits and ranges to where the next frame's main_data_begin points to. Encoder mp3PRO used ancillary data to encode extra information which could improve audio quality when decoded with its own algorithm.

A "tag" in an audio file is a section of the file that contains metadata such as the title, artist, album, track number or other information about the file's contents. The MP3 standards do not define tag formats for MP3 files, nor is there a standard container format that would support metadata and obviate the need for tags. However, several "de facto" standards for tag formats exist. As of 2010, the most widespread are ID3v1 and ID3v2, and the more recently introduced APEv2. These tags are normally embedded at the beginning or end of MP3 files, separate from the actual MP3 frame data. MP3 decoders either extract information from the tags, or just treat them as ignorable, non-MP3 junk data.

Playing and editing software often contains tag editing functionality, but there are also tag editor applications dedicated to the purpose. Aside from metadata pertaining to the audio content, tags may also be used for DRM. ReplayGain is a standard for measuring and storing the loudness of an MP3 file (audio normalization) in its metadata tag, enabling a ReplayGain-compliant player to automatically adjust the overall playback volume for each file. MP3Gain may be used to reversibly modify files based on ReplayGain measurements so that adjusted playback can be achieved on players without ReplayGain capability.

The basic MP3 decoding and encoding technology is patent-free in the European Union, all patents having expired there by 2012 at the latest. In the United States, the technology became substantially patent-free on 16 April 2017 (see below). MP3 patents expired in the US between 2007 and 2017. In the past, many organizations have claimed ownership of patents related to MP3 decoding or encoding. These claims led to a number of legal threats and actions from a variety of sources. As a result, uncertainty about which patents must have been licensed in order to create MP3 products without committing patent infringement in countries that allow software patents was a common feature of the early stages of adoption of the technology.

The initial near-complete MPEG-1 standard (parts 1, 2 and 3) was publicly available on 6 December 1991 as ISO CD 11172. In most countries, patents cannot be filed after prior art has been made public, and patents expire 20 years after the initial filing date, which can be up to 12 months later for filings in other countries. As a result, patents required to implement MP3 expired in most countries by December 2012, 21 years after the publication of ISO CD 11172.

An exception is the United States, where patents in force but filed prior to 8 June 1995 expire after the later of 17 years from the issue date or 20 years from the priority date. A lengthy patent prosecution process may result in a patent issuing much later than normally expected (see submarine patents). The various MP3-related patents expired on dates ranging from 2007 to 2017 in the United States. Patents for anything disclosed in ISO CD 11172 filed a year or more after its publication are questionable. If only the known MP3 patents filed by December 1992 are considered, then MP3 decoding has been patent-free in the US since 22 September 2015, when , which had a PCT filing in October 1992, expired. If the longest-running patent mentioned in the aforementioned references is taken as a measure, then the MP3 technology became patent-free in the United States on 16 April 2017, when , held and administered by Technicolor, expired. As a result, many free and open-source software projects, such as the Fedora operating system, have decided to start shipping MP3 support by default, and users will no longer have to resort to installing unofficial packages maintained by third party software repositories for MP3 playback or encoding.

Technicolor (formerly called Thomson Consumer Electronics) claimed to control MP3 licensing of the Layer 3 patents in many countries, including the United States, Japan, Canada and EU countries. Technicolor had been actively enforcing these patents. MP3 license revenues from Technicolor's administration generated about €100 million for the Fraunhofer Society in 2005. In September 1998, the Fraunhofer Institute sent a letter to several developers of MP3 software stating that a license was required to "distribute and/or sell decoders and/or encoders". The letter claimed that unlicensed products "infringe the patent rights of Fraunhofer and Thomson. To make, sell or distribute products using the [MPEG Layer-3] standard and thus our patents, you need to obtain a license under these patents from us." This led to the situation where the LAME MP3 encoder project could not offer its users official binaries that could run on their computer. The project's position was that as source code, LAME was simply a description of how an MP3 encoder "could" be implemented. Unofficially, compiled binaries were available from other sources.

Sisvel S.p.A. and its United States subsidiary Audio MPEG, Inc. previously sued Thomson for patent infringement on MP3 technology, but those disputes were resolved in November 2005 with Sisvel granting Thomson a license to their patents. Motorola followed soon after, and signed with Sisvel to license MP3-related patents in December 2005. Except for three patents, the US patents administered by Sisvel had all expired in 2015. The three exceptions are: , expired February 2017; , expired February 2017; and , expired 9 April 2017.

In September 2006, German officials seized MP3 players from SanDisk's booth at the IFA show in Berlin after an Italian patents firm won an injunction on behalf of Sisvel against SanDisk in a dispute over licensing rights. The injunction was later reversed by a Berlin judge, but that reversal was in turn blocked the same day by another judge from the same court, "bringing the Patent Wild West to Germany" in the words of one commentator. In February 2007, Texas MP3 Technologies sued Apple, Samsung Electronics and Sandisk in eastern Texas federal court, claiming infringement of a portable MP3 player patent that Texas MP3 said it had been assigned. Apple, Samsung, and Sandisk all settled the claims against them in January 2009.

Alcatel-Lucent has asserted several MP3 coding and compression patents, allegedly inherited from AT&T-Bell Labs, in litigation of its own. In November 2006, before the companies' merger, Alcatel sued Microsoft for allegedly infringing seven patents. On 23 February 2007, a San Diego jury awarded Alcatel-Lucent US $1.52 billion in damages for infringement of two of them. The court subsequently revoked the award, however, finding that one patent had not been infringed and that the other was not owned by Alcatel-Lucent; it was co-owned by AT&T and Fraunhofer, who had licensed it to Microsoft, the judge ruled. That defense judgment was upheld on appeal in 2008. See Alcatel-Lucent v. Microsoft for more information.

Other lossy formats exist. Among these, Advanced Audio Coding (AAC) is the most widely used, and was designed to be the successor to MP3. There also exist other lossy formats such as mp3PRO and MP2. They are members of the same technological family as MP3 and depend on roughly similar psychoacoustic models and MDCT algorithms. Whereas MP3 uses a hybrid coding approach that is part MDCT and part FFT, AAC is purely MDCT, significantly improving compression efficiency. Many of the basic patents underlying these formats are held by Fraunhofer Society, Alcatel-Lucent, Thomson Consumer Electronics, Bell, Dolby, LG Electronics, NEC, NTT Docomo, Panasonic, Sony Corporation, ETRI, JVC Kenwood, Philips, Microsoft, and NTT.

There are also open compression formats like Opus and Vorbis that are available free of charge and without any known patent restrictions. Some of the newer audio compression formats, such as AAC, WMA Pro and Vorbis, are free of some limitations inherent to the MP3 format that cannot be overcome by any MP3 encoder.

Besides lossy compression methods, lossless formats are a significant alternative to MP3 because they provide unaltered audio content, though with an increased file size compared to lossy compression. Lossless formats include FLAC (Free Lossless Audio Codec), Apple Lossless and many others.




</doc>
<doc id="19674" url="https://en.wikipedia.org/wiki?curid=19674" title="May 15">
May 15





</doc>
<doc id="19675" url="https://en.wikipedia.org/wiki?curid=19675" title="May 13">
May 13





</doc>
<doc id="19676" url="https://en.wikipedia.org/wiki?curid=19676" title="May 14">
May 14





</doc>
<doc id="19677" url="https://en.wikipedia.org/wiki?curid=19677" title="May 20">
May 20





</doc>
<doc id="19679" url="https://en.wikipedia.org/wiki?curid=19679" title="Mary Rose">
Mary Rose

The Mary Rose is a carrack-type warship of the English Tudor navy of King Henry VIII. She served for 33 years in several wars against France, Scotland, and Brittany. After being substantially rebuilt in 1536, she saw her last action on 1545. She led the attack on the galleys of a French invasion fleet, but sank in the Solent, the straits north of the Isle of Wight.

The wreck of the "Mary Rose" was discovered in 1971 and was raised on 11 October 1982 by the Mary Rose Trust in one of the most complex and expensive maritime salvage projects in history. The surviving section of the ship and thousands of recovered artefacts are of great value as a Tudor-era time capsule. The excavation and raising of the "Mary Rose" was a milestone in the field of maritime archaeology, comparable in complexity and cost to the raising of the 17th-century Swedish warship "Vasa" in 1961.

The finds include weapons, sailing equipment, naval supplies, and a wide array of objects used by the crew. Many of the artefacts are unique to the "Mary Rose" and have provided insights into topics ranging from naval warfare to the history of musical instruments. The remains of the hull have been on display at the Portsmouth Historic Dockyard since the mid-1980s while undergoing restoration. An extensive collection of well-preserved artefacts is on display at the Mary Rose Museum, built to display the remains of the ship and its artefacts.

The "Mary Rose" was one of the largest ships in the English navy through more than three decades of intermittent war, and she was one of the earliest examples of a purpose-built sailing warship. She was armed with new types of heavy guns that could fire through the recently invented gun-ports. She was substantially rebuilt in 1536 and was also one of the earliest ships that could fire a broadside, although the line of battle tactics had not yet been developed. Several theories have sought to explain the demise of the "Mary Rose", based on historical records, knowledge of 16th-century shipbuilding, and modern experiments. The precise cause of her sinking is still unclear because of conflicting testimonies and a lack of conclusive physical evidence.

In the late 15th century, England was still reeling from its dynastic wars first with France and then among its ruling families back on home soil. The great victories against France in the Hundred Years' War were in the past; only the small enclave of Calais in northern France remained of the vast continental holdings of the English kings. The War of the Roses—the civil war between the houses of York and Lancaster—had ended with Henry VII's establishment of the House of Tudor, the new ruling dynasty of England. The ambitious naval policies of Henry V were not continued by his successors, and from 1422 to 1509 only six ships were built for the crown. The marriage alliance between Anne of Brittany and Charles VIII of France in 1491, and his successor Louis XII in 1499, left England with a weakened strategic position on its southern flank. Despite this, Henry VII managed to maintain a comparatively long period of peace and a small but powerful core of a navy.

At the onset of the early modern period, the great European powers were France, the Holy Roman Empire and Spain. All three became involved in the War of the League of Cambrai in 1508. The conflict was initially aimed at the Republic of Venice but eventually turned against France. Through the Spanish possessions in the Low Countries, England had close economic ties with the Spanish Habsburgs, and it was the young Henry VIII's ambition to repeat the glorious martial endeavours of his predecessors. In 1509, six weeks into his reign, Henry married the Spanish princess Catherine of Aragon and joined the League, intent on certifying his historical claim as king of both England and France. By 1511 Henry was part of an anti-French alliance that included Ferdinand II of Aragon, Pope Julius II and Holy Roman emperor Maximilian.

The small navy that Henry VIII inherited from his father had only two sizeable ships, the carracks "Regent" and "Sovereign". Just months after his accession, two large ships were ordered: the "Mary Rose" and the "Peter Pomegranate" (later known as "Peter" after being rebuilt in 1536) of about 500 and 450 tons respectively. Which king ordered the building of the "Mary Rose" is unclear; although construction began during Henry VIII's reign, the plans for naval expansion could have been in the making earlier. Henry VIII oversaw the project and he ordered additional large ships to be built, most notably the "Henry Grace à Dieu" ("Henry by the Grace of God"), or "Great Harry" at more than 1000 tons burthen. By the 1520s the English state had established a "de facto" permanent "Navy Royal", the organizational ancestor of the modern Royal Navy.

The construction of the "Mary Rose" began in 1510 in Portsmouth and she was launched in July 1511. She was then towed to London and fitted with rigging and decking, and supplied with armaments. Other than the structural details needed to sail, stock and arm the "Mary Rose", she was also equipped with flags, banners and streamers (extremely elongated flags that were flown from the top of the masts) that were either painted or gilded.

Constructing a warship of the size of the "Mary Rose" was a major undertaking, requiring vast quantities of high-quality material. In the case of building a state-of-the-art warship, these materials were primarily oak. The total amount of timber needed for the construction can only be roughly calculated since only about one third of the ship still exists. One estimate for the number of trees is around 600 mostly large oaks, representing about 16 hectares (40 acres) of woodland. The huge trees that had been common in Europe and the British Isles in previous centuries were by the 16th century quite rare, which meant that timbers were brought in from all over southern England. The largest timbers used in the construction were of roughly the same size as those used in the roofs of the largest cathedrals in the high Middle Ages. An unworked hull plank would have weighed over 300 kg (660 lb), and one of the main deck beams would have weighed close to three-quarters of a tonne.

The common explanation for the ship's name was that it was inspired by Henry VIII's favourite sister, Mary Tudor, and the rose as the emblem of the Tudors. According to historians David Childs, David Loades and Peter Marsden, no direct evidence of naming the ship after the King's sister exists. It was far more common at the time to give ships pious Christian names, a long-standing tradition in Western Europe, or to associate them with their royal patrons. Names like "Grace Dieu" (Thank God) and "Holighost" (Holy Spirit) had been common since the 15th century and other Tudor navy ships had names like the "Regent" and "Three Ostrich Feathers" (referring to the crest of the Prince of Wales). The Virgin Mary is a more likely candidate for a namesake, and she was also associated with the Rosa Mystica (mystic rose). The name of the sister ship of the "Mary Rose", the "Peter Pomegranate", is believed to have been named in honour of Saint Peter, and the badge of the Queen Catharine of Aragon, a pomegranate. According to Childs, Loades and Marsden, the two ships, which were built around the same time, were named in honour of the king and queen, respectively.

The "Mary Rose" was substantially rebuilt in 1536. The 1536 rebuilding turned a ship of 500 tons into one of 700 tons, and added an entire extra tier of broadside guns to the old carrack-style structure. By consequence, modern research is based mostly on interpretations of the concrete physical evidence of this version of the "Mary Rose". The construction of the original design from 1509 is less known.

The "Mary Rose" was built according to the carrack-style with high "castles" in the bow and stern with a low waist of open decking in the middle. The shape of the hull has a so-called tumblehome form and reflected the use of the ship as a platform for heavy guns. Above the waterline, the hull gradually narrows to compensate for the weight of the guns and to make boarding more difficult. Since only part of the hull has survived, it is not possible to determine many of the basic dimensions with any great accuracy. The moulded breadth, the widest point of the ship roughly above the waterline, was about 12 metres (39 ft) and the keel about 32 metres (105 ft), although the ship's overall length is uncertain.

The hull had four levels separated by three decks. The terminology for these in the 16th century was still not standardised so the terms used here are those that were applied by the Mary Rose Trust. The "hold" lay furthest down in the ship, right above the bottom planking below the waterline. This is where the kitchen, or galley, was situated and the food was cooked. Directly aft of the galley was the mast step, a rebate in the centre-most timber of the keelson, right above the keel, which supported the main mast, and next to it the main bilge pump. To increase the stability of the ship, the hold was where the ballast was placed and much of the supplies were kept. Right above the hold was the "orlop", the lowest deck. Like the hold it was partitioned and was also used as a storage area for everything from food to spare sails.
Above the orlop lay the "main deck" which housed the heaviest guns. The side of the hull on the main deck level had seven gunports on each side fitted with heavy lids that would have been watertight when closed. This was also the highest deck that was caulked and waterproof. Along the sides of the main deck there were cabins under the forecastle and sterncastle which have been identified as belonging to the carpenter, barber-surgeon, pilot and possibly also the master gunner and some of the officers. The top deck in the hull structure was the "upper deck" (or weather deck) which was exposed to the elements in the waist. It was a dedicated fighting deck without any known partitions and a mix of heavy and light guns. Over the open waist the upper deck was entirely covered with a boarding net, a coarse netting that served as a defence measure against boarding. Though very little of the upper deck has survived, it has been suggested that it housed the main living quarters of the crew underneath the sterncastle. A drainage located in this area has been identified as a possible "piss-dale", a general urinal to complement the regular toilets that would probably have been located in the bow.

The castles of the "Mary Rose" had additional decks, but since virtually nothing of them survives, their design has had to be reconstructed from historical records. Contemporary ships of equal size were consistently listed as having three decks in both castles. Although speculative, this layout is supported by the illustration in the Anthony Roll and the gun inventories.

During the early stages of excavation of the wreck, it was believed that the ship had originally been built with clinker (or clench) planking, a technique where the hull consisted of overlapping planks that bore the structural strength of the ship. Cutting gunports into a clinker-built hull would have meant weakening the ship's structural integrity, and it was assumed that she was later rebuilt to accommodate a hull with carvel edge-to-edge planking with a skeletal structure to support a hull perforated with gunports. Later examination indicates that the clinker planking is not present throughout the ship; only the outer structure of the sterncastle is built with overlapping planking, though not with a true clinker technique.

Although only the lower fittings of the rigging survive, a 1514 inventory and the only known contemporary depiction of the ship from the Anthony Roll have been used to determine how the propulsion system of the "Mary Rose" was designed. Nine, or possibly ten, sails were flown from four masts and a bowsprit: the foremast and mainmast had two and three square sails respectively; the mizzen mast had a lateen sail and a small square sail and the bonaventure mizzen had at least one lateen sail, and possibly also a square sail, and the bowsprit flew a small square spritsail. According to the Anthony Roll illustration (see top of this section), the yards (the spars from which the sails were set) on the foremast and mainmast were also equipped with sheerhooks, twin curved blades sharpened on the inside, that were intended to cut an enemy ship's rigging during boarding actions.

The sailing capabilities of the "Mary Rose" were commented on by her contemporaries and were once even put to the test. In March 1513 a contest was arranged off The Downs, west of Kent, in which she raced against nine other ships. She won the contest, and Admiral Edward Howard described her enthusiastically as "the noblest ship of sayle [of any] gret ship, at this howr, that I trow [believe] be in Cristendom". Several years later, while sailing between Dover and The Downs, Vice-Admiral William Fitzwilliam noted that both the "Henry Grace à Dieu" and the "Mary Rose" performed very well, riding steadily in rough seas and that it would have been a "hard chose" between the two. Modern experts have been more sceptical to her sailing qualities, believing that ships at this time were almost incapable of sailing close against the wind, and describing the handling of the "Mary Rose" as being like "a wet haystack".

The "Mary Rose" represented a transitional ship design in naval warfare. Since ancient times, war at sea had been fought much like that on land: with melee weapons and bows and arrows, but on floating wooden platforms rather than battlefields. Though the introduction of guns was a significant change, it only slowly changed the dynamics of ship-to-ship combat. As guns became heavier and able to take more powerful gunpowder charges, they needed to be placed lower in the ship, closer to the water line. Gunports cut in the hull of ships had been introduced as early as 1501, only about a decade before the "Mary Rose" was built. This made broadsides, coordinated volleys from all the guns on one side of a ship, possible for the first time in history, at least in theory. Naval tactics throughout the 16th century and well into the 17th century focused on countering the oar-powered galleys that were armed with heavy guns in the bow, facing forwards, which were aimed by turning the entire ship against its target. Combined with inefficient gunpowder and the difficulties inherent in firing accurately from moving platforms, this meant that boarding remained the primary tactic for decisive victory throughout the 16th century.

As the "Mary Rose" was built and served during a period of rapid development of heavy artillery, her armament was a mix of old designs and innovations. The heavy armament was a mix of older-type wrought iron and cast bronze guns, which differed considerably in size, range and design. The large iron guns were made up of staves or bars welded into cylinders and then reinforced by shrinking iron hoops and breech loaded and equipped with simpler gun-carriages made from hollowed-out elm logs with only one pair of wheels, or without wheels entirely. The bronze guns were cast in one piece and rested on four-wheel carriages which were essentially the same as those used until the 19th century. The breech-loaders were cheaper to produce and both easier and faster to reload, but could take less powerful charges than cast bronze guns. Generally, the bronze guns used cast iron shot and were more suited to penetrate hull sides while the iron guns used stone shot that would shatter on impact and leave large, jagged holes, but both could also fire a variety of ammunition intended to destroy rigging and light structure or injure enemy personnel.

The majority of the guns were small iron guns with short range that could be aimed and fired by a single person. The two most common are the "bases", breech-loading swivel guns, most likely placed in the castles, and "hailshot pieces", small muzzle-loaders with rectangular bores and fin-like protrusions that were used to support the guns against the railing and allow the ship structure to take the force of the recoil. Though the design is unknown, there were two "top pieces" in a 1546 inventory (finished after the sinking) which were probably similar to a base, but placed in one or more of the fighting tops.

The ship went through several changes in her armament throughout her career, most significantly accompanying her "rebuilding" in 1536 (see below), when the number of anti-personnel guns was reduced and a second tier of carriage-mounted long guns fitted. There are three inventories that list her guns, dating to 1514, 1540 and 1546. Together with records from the armoury at the Tower of London, these show how the configuration of guns changed as gun-making technology evolved and new classifications were invented. In 1514, the armament consisted mostly of anti-personnel guns like the larger breech-loading iron "murderers" and the small "serpentines", "demi-slings" and stone guns. Only a handful of guns in the first inventory were powerful enough to hole enemy ships, and most would have been supported by the ship's structure rather than resting on carriages. The inventories of both the "Mary Rose" and the Tower had changed radically by 1540. There were now the new cast bronze "cannons", "demi-cannons", "culverins" and "sakers" and the wrought iron "port pieces" (a name that indicated they fired through ports), all of which required carriages, had longer range and were capable of doing serious damage to other ships. The analysis of the 1514 inventory combined with hints of structural changes in the ship both indicate that the gunports on the main deck were indeed a later addition.

Various types of ammunition could be used for different purposes: plain spherical shot of stone or iron smashed hulls, spiked bar shot and shot linked with chains would tear sails or damage rigging, and canister shot packed with sharp flints produced a devastating shotgun effect. Trials made with replicas of culverins and port pieces showed that they could penetrate wood the same thickness of the "Mary Rose's" hull planking, indicating a stand-off range of at least 90 m (295 ft). The port pieces proved particularly efficient at smashing large holes in wood when firing stone shot and were a devastating anti-personnel weapon when loaded with flakes or pebbles.

To defend against being boarded, "Mary Rose" carried large stocks of melee weapons, including pikes and bills; 150 of each kind were stocked on the ship according to the Anthony Roll, a figure confirmed roughly by the excavations. Swords and daggers were personal possessions and not listed in the inventories, but the remains of both have been found in great quantities, including the earliest dated example of a British basket-hilted sword.

A total of 250 longbows were carried on board, and 172 of these have so far been found, as well as almost 4,000 arrows, bracers (arm guards) and other archery-related equipment. Longbow archery in Tudor England was mandatory for all able adult men, and despite the introduction of field artillery and handguns, they were used alongside new missile weapons in great quantities. On the "Mary Rose", the longbows could only have been drawn and shot properly from behind protective panels in the open waist or from the top of the castles as the lower decks lacked sufficient headroom. There were several types of bows of various size and range. Lighter bows would have been used as "sniper" bows, while the heavier design could possibly have been used to shoot fire arrows.

The inventories of both 1514 and 1546 also list several hundred heavy darts and lime pots that were designed to be thrown onto the deck of enemy ships from the fighting tops, although no physical evidence of either of these weapon types has been identified. Of the 50 handguns listed in the Anthony Roll, the complete stocks of five matchlock muskets and fragments of another eleven have been found. They had been manufactured mainly in Italy, with some originating from Germany. Found in storage were several "gunshields", a rare type of firearm consisting of a wooden shield with a small gun fixed in the middle.

Throughout her 33-year career, the crew of the "Mary Rose" changed several times and varied considerably in size. It would have a minimal skeleton crew of 17 men or fewer in peacetime and when she was "laid up in ordinary" (in reserve). The average wartime manning would have been about 185 soldiers, 200 sailors, 20–30 gunners and an assortment of other specialists such as surgeons, trumpeters and members of the admiral's staff, for a total of 400–450 men. When taking part in land invasions or raids, such as in the summer of 1512, the number of soldiers could have swelled to just over 400 for a combined total of more than 700. Even with the normal crew size of around 400, the ship was quite crowded, and with additional soldiers would have been extremely cramped.
Little is known of the identities of the men who served on the "Mary Rose", even when it comes to the names of the officers, who would have belonged to the gentry. Two admirals and four captains (including Edward and Thomas Howard, who served both positions) are known through records, as well as a few ship masters, pursers, master gunners and other specialists. Forensic science has been used by artists to create reconstructions of faces of eight crew members, and the results were publicised in May 2013. In addition, researchers have extracted DNA from remains in the hopes of identifying origins of crew, and potentially living descendants.

Of the vast majority of the crewmen, soldiers, sailors and gunners alike, nothing has been recorded. The only source of information for these men has been through osteological analysis of the human bones found at the wrecksite. An approximate composition of some of the crew has been conjectured based on contemporary records. The "Mary Rose" would have carried a captain, a master responsible for navigation, and deck crew. There would also have been a purser responsible for handling payments, a boatswain, the captain's second in command, at least one carpenter, a pilot in charge of navigation, and a cook, all of whom had one or more assistants (mates). The ship was also staffed by a barber-surgeon who tended to the sick and wounded, along with an apprentice or mate and possibly also a junior surgeon. The only positively identified person who went down with the ship was Vice-Admiral George Carew. McKee, Stirland and several other authors have also named Roger Grenville, father of Richard Grenville of the Elizabethan-era "Revenge", captain during the final battle, although the accuracy of the sourcing for this has been disputed by maritime archaeologist Peter Marsden.

The bones of a total of 179 people were found during the excavations of the "Mary Rose", including 92 "fairly complete skeletons", more or less complete collections of bones associated with specific individuals. Analysis of these has shown that crew members were all male, most of them young adults. Some were no more than 11–13 years old, and the majority (81%) under 30. They were mainly of English origin and, according to archaeologist Julie Gardiner, they most likely came from the West Country; many following their aristocratic masters into maritime service. There were also a few people from continental Europe. An eyewitness testimony right after the sinking refers to a survivor who was a Fleming, and the pilot may very well have been French. Analysis of oxygen isotopes in teeth indicates that some were also of southern European origin. In general they were strong, well-fed men, but many of the bones also reveal tell-tale signs of childhood diseases and a life of grinding toil. The bones also showed traces of numerous healed fractures, probably the result of on-board accidents.

There are no extant written records of the make-up of the broader categories of soldiers and sailors, but since the "Mary Rose" carried some 300 longbows and several thousand arrows there had to be a considerable proportion of longbow archers. Examination of the skeletal remains has found that there was a disproportionate number of men with a condition known as "os acromiale", affecting their shoulder blades. This condition is known among modern elite archery athletes and is caused by placing considerable stress on the arm and shoulder muscles, particularly of the left arm that is used to hold the bow to brace against the pull on the bowstring. Among the men who died on the ship it was likely that some had practised using the longbow since childhood, and served on board as specialist archers.

A group of six skeletons was found grouped close to one of the 2-tonne bronze culverins on the main deck near the bow. Fusing of parts of the spine and ossification, the growth of new bone, on several vertebrae evidenced all but one of these crewmen to have been strong, well-muscled men who had been engaged in heavy pulling and pushing, the exception possibly being a "powder monkey" not involved in heavy work. These have been tentatively classified as members of a complete gun crew, all having died at their battle station.

The "Mary Rose" first saw battle in 1512, in a joint naval operation with the Spanish against the French. The English were to meet the French and Breton fleets in the English Channel while the Spanish attacked them in the Bay of Biscay and then attack Gascony. The 35-year-old Sir Edward Howard was appointed Lord High Admiral in April and chose the "Mary Rose" as his flagship. His first mission was to clear the seas of French naval forces between England to the northern coast of Spain to allow for the landing of supporting troops near the French border at Fuenterrabia. The fleet consisted of 18 ships, among them the large ships the "Regent" and the "Peter Pomegranate", carrying over 5,000 men. Howard's expedition led to the capture of twelve Breton ships and a four-day raiding tour of Brittany where English forces successfully fought against local forces and burned numerous settlements.

The fleet returned to Southampton in June where it was visited by King Henry. In August the fleet sailed for Brest where it encountered a joint, but ill-coordinated, French-Breton fleet at the battle of St. Mathieu. The English with one of the great ships in the lead (according to Marsden the "Mary Rose") battered the French ships with heavy gunfire and forced them to retreat. The Breton flagship "Cordelière" put up a fight and was boarded by the 1,000-ton "Regent". By accident or through the unwillingness of the Breton crew to surrender, the powder magazine of the "Cordelière" caught fire and blew up in a violent explosion, setting fire to the "Regent" and eventually sinking her. About 180 English crew members saved themselves by throwing themselves into the sea and only a handful of Bretons survived, only to be captured. The captain of the "Regent", 600 soldiers and sailors, the High Admiral of France and the steward of the town of Morlaix were killed in the incident, making it the focal point of several contemporary chronicles and reports. On , the English burnt 27 French ships, captured another five and landed forces near Brest to raid and take prisoners, but storms forced the fleet back to Dartmouth in Devon and then to Southampton for repairs.
In early 1513, the "Mary Rose" was once more chosen by Howard as the flagship for an expedition against the French. Before seeing action, she took part in a race against other ships where she was deemed to be one of the most nimble and the fastest of the great ships in the fleet (see details under "Sails and rigging"). On , Howard's force arrived off Brest only to see a small enemy force join with the larger force in the safety of Brest harbour and its fortifications. The French had recently been reinforced by a force of galleys from the Mediterranean, which sank one English ship and seriously damaged another. Howard landed forces near Brest, but made no headway against the town and was by now getting low on supplies. Attempting to force a victory, he took a small force of small oared vessels on a daring frontal attack on the French galleys on . Howard himself managed to reach the ship of French admiral, Prégent de Bidoux, and led a small party to board it. The French fought back fiercely and cut the cables that attached the two ships, separating Howard from his men. It left him at the mercy of the soldiers aboard the galley, who instantly killed him.

Demoralised by the loss of its admiral and seriously short of food, the fleet returned to Plymouth. Thomas Howard, elder brother of Edward, was assigned the new Lord Admiral, and was set to the task of arranging another attack on Brittany. The fleet was not able to mount the planned attack because of adverse winds and great difficulties in supplying the ships adequately and the "Mary Rose" took up winter quarters in Southampton. In August the Scots joined France in war against England, but were dealt a crushing defeat at the Battle of Flodden on 1513. A follow-up attack in early 1514 was supported by a naval force that included the "Mary Rose", but without any known engagements. The French and English mounted raids on each other throughout that summer, but achieved little, and both sides were by then exhausted. By autumn the war was over and a peace treaty was sealed by the marriage of Henry's sister, Mary, to French king Louis XII.

After the peace "Mary Rose" was placed in the reserves, "in ordinary". She was laid up for maintenance along with her sister ship the "Peter Pomegranate" in July 1514. In 1518 she received a routine repair and caulking, waterproofing with tar and oakum (old rope fibres) and was then assigned a small skeleton crew who lived on board the ship until 1522. She served briefly on a mission with other warships to "scour the seas" in preparation for Henry VIII's journey across the Channel to the summit with the French king Francis I at the Field of the Cloth of Gold in June 1520.

In 1522, England was once again at war with France because of a treaty with the Holy Roman Emperor Charles V. The plan was for an attack on two fronts with an English thrust in northern France. The "Mary Rose" participated in the escort transport of troops in June 1522, and by the Breton port of Morlaix was captured. The fleet sailed home and the "Mary Rose" berthed for the winter in Dartmouth. The war raged on until 1525 and saw the Scots join the French side. Though Charles Brandon came close to capturing Paris in 1523, there was little gained either against France or Scotland throughout the war. With the defeat of the French army and capture of Francis I by Charles V's forces at the Battle of Pavia on 1525, the war was effectively over without any major gains or major victories for the English side.

The "Mary Rose" was kept in reserve from 1522 to 1545. She was once more caulked and repaired in 1527 in a newly dug dock at Portsmouth and her longboat was repaired and trimmed. Little documentation about the "Mary Rose" between 1528 and 1539 exists. A document written by Thomas Cromwell in 1536 specifies that the "Mary Rose" and six other ships were "made new" during his service under the king, though it is unclear which years he was referring to and what "made new" actually meant. A later document from January 1536 by an anonymous author states that the "Mary Rose" and other ships were "new made", and dating of timbers from the ship confirms some type of repair being done in 1535 or 1536. This would have coincided with the controversial dissolution of the monasteries that resulted in a major influx of funds into the royal treasury. The nature and extent of this repair is unknown. Many experts, including Margaret Rule, the project leader for the raising of the "Mary Rose", have assumed that it meant a complete rebuilding from clinker planking to carvel planking, and that it was only after 1536 that the ship took on the form that it had when it sank and that was eventually recovered in the 20th century. Marsden has speculated that it could even mean that the "Mary Rose" was originally built in a style that was closer to 15th-century ships, with a rounded, rather than square, stern and without the main deck gunports.

Henry's complicated marital situation and his high-handed dissolution of the monasteries angered the Pope and Catholic rulers throughout Europe, which increased England's diplomatic isolation. In 1544 Henry had agreed to attack France together with Emperor Charles V, and English forces captured Boulogne at great cost in September, but soon England was left in the lurch after Charles had achieved his objectives and brokered a separate peace.

In May 1545, the French had assembled a large fleet in the estuary of the Seine with the intent to land troops on English soil. The estimates of the size of the fleet varied considerably; between 123 and 300 vessels according to French sources; and up to 226 sailing ships and galleys according to the chronicler Edward Hall. In addition to the massive fleet, 50,000 troops were assembled at Havre de Grâce (modern-day Le Havre). An English force of 160 ships and 12,000 troops under Viscount Lisle was ready at Portsmouth by early June, before the French were ready to set sail, and an ineffective pre-emptive strike was made in the middle of the month. In early July the huge French force under the command of Admiral Claude d'Annebault set sail for England and entered the Solent unopposed with 128 ships on . The English had around 80 ships with which to oppose the French, including the flagship "Mary Rose". But since they had virtually no heavy galleys, the vessels that were at their best in sheltered waters like the Solent, the English fleet promptly retreated into Portsmouth harbour.

The English were becalmed in port and unable to manoeuvre. On 1545, the French galleys advanced on the immobilised English fleet, and initially threatened to destroy a force of 13 small galleys, or "rowbarges", the only ships that were able to move against them without a wind. The wind picked up and the sailing ships were able to go on the offensive before the oared vessels were overwhelmed. Two of the largest ships, the "Henry Grace à Dieu" and the "Mary Rose", led the attack on the French galleys in the Solent.

Early in the battle something went wrong. While engaging the French galleys the "Mary Rose" suddenly heeled (leaned) heavily over to her starboard (right) side and water rushed in through the open gunports. The crew was powerless to correct the sudden imbalance, and could only scramble for the safety of the upper deck as the ship began to sink rapidly. As she leaned over, equipment, ammunition, supplies and storage containers shifted and came loose, adding to the general chaos. The massive port side brick oven in the galley collapsed completely and the huge 360-litre (90 gallon) copper cauldron was thrown onto the orlop deck above. Heavy guns came free and slammed into the opposite side, impeding escape or crushing men beneath them.

For those who were not injured or killed outright by moving objects, there was little time to reach safety, especially for the men who were manning the guns on the main deck or fetching ammunition and supplies in the hold. The companionways that connected the decks with one another would have become bottlenecks for fleeing men, something indicated by the positioning of many of the skeletons recovered from the wreck. What turned the sinking into a major tragedy was the anti-boarding netting that covered the upper decks in the waist (the midsection of the ship) and the sterncastle. With the exception of the men who were stationed in the tops in the masts, most of those who managed to get up from below deck were trapped under the netting; they would have been in view of the surface, and their colleagues above, but with little or no chance to break through, and were dragged down with the ship. Out of a crew of at least 400, fewer than 35 escaped, a casualty rate of over 90%.
Many accounts of the sinking have been preserved that describe the incident, but the only confirmed eyewitness account is the testimony of a surviving Flemish crewman written down by the Holy Roman Emperor's ambassador François van der Delft in a letter dated . According to the unnamed Fleming, the ship had fired all of its guns on one side and was turning to present the guns on the other side to the enemy ship, when she was caught in a strong gust of wind, heeled and took in water through the open gunports. In a letter to William Paget dated , former Lord High Admiral John Russel claimed that the ship had been lost because of "rechenes and great negligence". Three years after the sinking, the "Hall's Chronicle" gave the reason for the sinking as being caused by "to[o] much foly ... for she was laden with much ordinaunce, and the portes left open, which were low, & the great ordinaunce unbreached, so that when the ship should turne, the water entered, and sodainly she sanke."

Later accounts repeat the explanation that the ship heeled over while going about and that the ship was brought down because of the open gunports. A biography of Peter Carew, brother of George Carew, written by John Hooker sometime after 1575, gives the same reason for the sinking, but adds that insubordination among the crew was to blame. The biography claims that George Carew noted that the "Mary Rose" showed signs of instability as soon as her sails were raised. George's uncle Gawen Carew had passed by with his own ship the "Matthew Gonson" during the battle to inquire about the situation of his nephew's ship. In reply he was told "that he had a sorte of knaves whom he could not rule". Contrary to all other accounts, Martin du Bellay, a French cavalry officer who was present at the battle, stated that the "Mary Rose" had been sunk by French guns.

The most common explanation for the sinking among modern historians is that the ship was unstable for a number of reasons. When a strong gust of wind hit the sails at a critical moment, the open gunports proved fatal, the ship flooded and quickly foundered. Coates offered a variant of this hypothesis, which explains why a ship which served for several decades without sinking, and which even fought in actions in the rough seas off Brittany, unexpectedly foundered: the ship had accumulated additional weight over the years in service and finally become unseaworthy. That the ship was turning after firing all the cannons on one side has been questioned by Marsden after examination of guns recovered in both the 19th and 20th centuries; guns from both sides were found still loaded. This has been interpreted to mean that something else could have gone wrong since it is assumed that an experienced crew would not have failed to secure the gunports before making a potentially risky turn.

The most recent surveys of the ship indicate that the ship was modified late in her career and have lent support to the idea that the "Mary Rose" was altered too much to be properly seaworthy. Marsden has suggested that the weight of additional heavy guns would have increased her draught so much that the waterline was less than one metre (c. 3 feet) from the gunports on the main deck.

Peter Carew's claim of insubordination has been given support by James Watt, former Medical Director-General of the Royal Navy, based on records of an epidemic of dysentery in Portsmouth which could have rendered the crew incapable of handling the ship properly, while historian Richard Barker has suggested that the crew actually knew that the ship was an accident waiting to happen, at which they balked and refused to follow orders. Marsden has noted that the Carew biography is in some details inconsistent with the sequence of events reported by both French and English eyewitnesses. It also reports that there were 700 men on board, an unusually high number. The distance in time to the event it describes may mean that it was embellished to add a dramatic touch. The report of French galleys sinking the "Mary Rose" as stated by Martin du Bellay has been described as "the account of a courtesan" by naval historian Maurice de Brossard. Du Bellay and his two brothers were close to king Francis I and du Bellay had much to gain from portraying the sinking as a French victory. English sources, even if biased, would have nothing to gain from portraying the sinking as the result of crew incompetence rather than conceding a victory to the much-feared gun galleys.

Dominic Fontana, a geographer at the University of Portsmouth, has voiced support for du Bellay's version of the sinking based on the battle as it is depicted in the Cowdray Engraving, and modern GIS analysis of the modern scene of the battle. By plotting the fleets and calculating the conjectured final manoeuvres of the "Mary Rose", Fontana reached the conclusion that the ship had been hit low in the hull by the galleys and was destabilised after taking in water. He has interpreted the final heading of the ship straight due north as a failed attempt to reach the shallows at Spitbank only a few hundred metres away. This theory has been given partial support by Alexzandra Hildred, one of the experts who has worked with the "Mary Rose", though she has suggested that the close proximity to Spitbank could also indicate that the sinking occurred while trying to make a hard turn to avoid running aground.

In 2000, the Channel 4 television programme "What Sank the Mary Rose?" attempted to investigate the causes suggested for her sinking by means of experiments with scale models of the ship and metal weights to simulate the presence of troops on the upper decks. Initial tests showed that the ship was able to make the turn described by eyewitnesses without capsizing. In later tests, a fan was used to create a breeze similar to the one reported to have suddenly sprung up on the day of the sinking as the "Mary Rose" went to make the turn. As the model made the turn, the breeze in the upper works forced it to heel more than at calm, forcing the main deck gun ports below the waterline and foundering the model within a few seconds. The sequence of events closely followed what eyewitnesses had reported, particularly the suddenness with which the ship sank.

A salvage attempt was ordered by Secretary of State William Paget only days after the sinking, and Charles Brandon, the king's brother-in-law, took charge of practical details. The operation followed the standard procedure for raising ships in shallow waters: strong cables were attached to the sunken ship and fastened to two empty ships, or hulks. At low tide, the ropes were pulled taut with capstans. When the high tide came in, the hulks rose and with them the wreck. It would then be towed into shallower water and the procedure repeated until the whole ship could be raised completely.

A list of necessary equipment was compiled by 1 August and included, among other things, massive cables, capstans, pulleys, and 40 pounds of tallow for lubrication. The proposed salvage team comprised 30 Venetian mariners and a Venetian carpenter with 60 English sailors to serve them. The two ships to be used as hulks were "Jesus of Lübeck" and "Samson", each of 700 tons burthen and similar in size to the "Mary Rose". Brandon was so confident of success that he reassured the king that it would only be a matter of days before they could raise the "Mary Rose". The optimism proved unfounded. Since the ship had settled at a 60-degree angle to starboard much of it was stuck deep into the clay of the seabed. This made it virtually impossible to pass cables under the hull and required far more lifting power than if the ship had settled on a hard seabed. An attempt to secure cables to the main mast appears only to have resulted in its being snapped off.
The project was only successful in raising rigging, some guns and other items. At least two other salvage teams in 1547 and 1549 received payment for raising more guns from the wreck. Despite the failure of the first salvage operation, there was still lingering belief in the possibility of retrieving the "Mary Rose" at least until 1546, when she was presented as part of the illustrated list of English warships called the Anthony Roll. When all hope of raising the complete ship was finally abandoned is not known. It could have been after Henry VIII's death in January 1547 or even as late as 1549, when the last guns were brought up. The "Mary Rose" was remembered well into the reign of Elizabeth I, and according to one of the queen's admirals, William Monson (1569–1643), the wreck was visible from the surface at low tide in the late 16th century.

After the sinking, the partially buried wreck created a barrier at a right angle against the currents of the Solent. Two scour pits, large underwater ditches, formed on either side of the wreck while silt and seaweed was deposited inside the ship. A deep but narrow pit formed on the upward tilting port side, while a shallower, broader pit formed on the starboard side, which had mostly been buried by the force of the impact. The abrasive actions of sand and silt carried by the currents and the activity of fungi, bacteria and wood-boring crustaceans and molluscs, such as the "teredo" "shipworm", began to break down the structure of the ship. Eventually the exposed wooden structure was weakened and gradually collapsed. The timbers and contents of the port side were either deposited in the scour pits and remaining ship structure, or carried off by the currents. Following the collapse of the exposed parts of the ship, the site was levelled with the seabed and gradually covered by layers of sediment, concealing most of the remaining structure. During the 16th century, a hard layer of compacted clay and crushed shells formed over the ship, stabilising the site and sealing the Tudor-era deposits. Further layers of soft silt covered the site during the 18th and 19th centuries, but frequent changes in the tidal patterns and currents in the Solent occasionally exposed some of the timbers, leading to its accidental rediscovery in 1836 and aided in locating the wreck in 1971. After the ship had been raised it was determined that about 40% of the original structure had survived.

In mid-1836, a group of five fishermen caught their nets on timbers protruding from the bottom of the Solent. They contacted a diver to help them remove the hindrance, and on , Henry Abbinett became the first person to see the "Mary Rose" in almost 300 years. Later, two other professional divers, John Deane and William Edwards, were employed. Using a recently invented rubber suit and metal diving helmet, Deane and Edwards began to examine the wreck and salvage items from it. Along with an assortment of timbers and wooden objects, including several longbows, they brought up several bronze and iron guns, which were sold to the Board of Ordnance for over £220. Initially, this caused a dispute between Deane (who had also brought in his brother Charles into the project), Abbinett and the fishermen who had hired them. The matter was eventually settled by allowing the fishermen a share of the proceeds from the sale of the first salvaged guns, while Deane received exclusive salvage rights at the expense of Abbinett. The wreck was soon identified as the "Mary Rose" from the inscriptions of one of the bronze guns manufactured in 1537.
The identification of the ship led to significant public interest in the salvage operation, and caused a great demand for the objects which were brought up. Though many of the objects could not be properly conserved at the time and subsequently deteriorated, many were documented with pencil sketches and watercolour drawings which survive to this day. John Deane ceased working on the wreck in 1836, but returned in 1840 with new, more destructive methods. With the help of condemned bomb shells filled with gunpowder acquired from the Ordnance Board, he blasted his way into parts of the wreck. Fragments of bombs and traces of blasting craters were found during the modern excavations, but there was no evidence that Deane managed to penetrate the hard layer that had sealed off the Tudor levels. Deane reported retrieving a bilge pump and the lower part of the main mast, both of which would have been located inside the ship. The recovery of small wooden objects like longbows suggests that Deane did manage to penetrate the Tudor levels at some point, though this has been disputed by the excavation project leader Margaret Rule. Newspaper reports on Deane's diving operations in October 1840 report that the ship was clinker built, but since the sterncastle is the only part of the ship with this feature, an alternative explanation has been suggested: Deane did not penetrate the hard shelly layer that covered most of the ship, but only managed to get into remains of the sterncastle that today no longer exist. Despite the rough handling by Deane, the "Mary Rose" escaped the wholesale destruction by giant rakes and explosives that was the fate of other wrecks in the Solent (such as ).

The modern search for the "Mary Rose" was initiated by the Southsea branch of the British Sub-Aqua Club in 1965 as part of a project to locate shipwrecks in the Solent. The project was under the leadership of historian, journalist and amateur diver Alexander McKee. Another group led by Lieutenant-Commander Alan Bax of the Royal Navy, sponsored by the Committee for Nautical Archaeology in London, also formed a search team. Initially the two teams had differing views on where to find the wreck, but eventually joined forces. In February 1966 a chart from 1841 was found that marked the positions of the "Mary Rose" and several other wrecks. The charted position coincided with a trench (one of the scour pits) that had already been located by McKee's team, and a definite location was finally established at a position 3 km (1.9 mi) south of the entrance to Portsmouth Harbour () in water with a depth of (36 feet) at low tide. Diving on the site began in 1966 and a sonar scan by Harold Edgerton in 1967–68 revealed some type of buried feature. In 1970 a loose timber was located and on 1971, the first structural details of the buried hull were identified after they were partially uncovered by winter storms.

A major problem for the team from the start was that wreck sites in the UK lacked any legal protection from plunderers and treasure hunters. Sunken ships, once being moving objects, were legally treated as chattel and were awarded to those who could first raise them. The Merchant Shipping Act of 1894 also stipulated that any objects raised from a wreck should be auctioned off to finance the salvage operations, and there was nothing preventing anyone from "stealing" the wreck and making a profit. The problem was handled by forming an organisation, the Mary Rose Committee, aiming "to find, excavate, raise and preserve for all time such remains of the ship "Mary Rose" as may be of historical or archaeological interest".

To keep intruders at bay, the Committee arranged a lease of the seabed where the wreck lay from the Portsmouth authorities, thereby discouraging anyone from trespassing on the underwater property. In hindsight this was only a legalistic charade which had little chance of holding up in a court of law. In combination with secrecy as to the exact location of the wreck, it saved the project from interference. It was not until the passing of the Protection of Wrecks Act on 1973 that the "Mary Rose" was declared to be of national historic interest that enjoyed full legal protection from any disturbance by commercial salvage teams. Despite this, years after the passing of the 1973 act and the excavation of the ship, lingering conflicts with salvage legislation remained a threat to the "Mary Rose" project as "personal" finds such as chests, clothing and cooking utensils risked being confiscated and auctioned off.

Following the discovery of the wreck in 1971, the project became known to the general public and received increasing media attention. This helped bring in more donations and equipment, primarily from private sources. By 1974 the committee had representatives from the National Maritime Museum, the Royal Navy, the BBC and local organisations. In 1974 the project received royal patronage from Prince Charles, who participated in dives on the site. This attracted yet more publicity, and also more funding and assistance. The initial aims of the Mary Rose Committee were now more officially and definitely confirmed. The committee had become a registered charity in 1974, which made it easier to raise funds, and the application for excavation and raising of the ship had been officially approved by the UK government.

By 1978 the initial excavation work had uncovered a complete and coherent site with an intact ship structure and the orientation of the hull had been positively identified as being on an almost straight northerly heading with a 60-degree heel to starboard and a slight downward tilt towards the bow. As no records of English shipbuilding techniques used in vessels like the "Mary Rose" survive, excavation of the ship would allow for a detailed survey of her design and shed new light on the construction of ships of the era. A full excavation also meant removing the protective layers of silt that prevented the remaining ship structure from being destroyed through biological decay and the scouring of the currents; the operation had to be completed within a predetermined timespan of a few years or it risked irreversible damage. It was also considered desirable to recover and preserve the remains of the hull if possible. For the first time, the project was faced with the practical difficulties of actually raising, conserving and preparing the hull for public display.

To handle this new, considerably more complex and expensive task, it was decided that a new organisation was needed. The Mary Rose Trust, a limited charitable trust, with representatives from many organisations would handle the need for a larger operation and a large infusion of funds. In 1979 a new diving vessel was purchased to replace the previous 12 m (40 ft) catamaran "Roger Greenville" which had been used from 1971. The choice fell on the salvage vessel "Sleipner", the same craft that had been used as a platform for diving operations on the "Vasa". The project went from a team of only twelve volunteers working four months a year to over 50 individuals working almost around the clock nine months a year. In addition there were over 500 volunteer divers and a laboratory staff of about 70 that ran the shore base and conservation facilities. During the four diving seasons from 1979 to 1982 over 22,000 diving hours was spent on the site, an effort that amounted to 11.8-man-years.

Raising the "Mary Rose" meant overcoming a number of delicate problems that had never been encountered before. The raising of the Swedish warship "Vasa" 1959–61 was the only comparable precedent, but it had been a relatively straightforward operation since the hull was completely intact and rested upright on the seabed. It had been raised with basically the same methods as were in use in Tudor England: cables were slung under the hull and attached to two pontoons on either side of the ship which was then gradually raised and towed into shallower waters. Only one third of the "Mary Rose" was intact and she lay deeply embedded in mud. If the hull were raised in the traditional way, there was no guarantee that it would have enough structural strength to hold together out of water. Many suggestions for raising the ship were discarded, including the construction of a cofferdam around the wreck site, filling the ship with small buoyant objects (such as ping pong balls) or even pumping brine into the seabed and freezing it so that it would float and take the hull with it. After lengthy discussions it was decided in February 1980 that the hull would first be emptied of all its contents and strengthened with steel braces and frames. It would then be lifted to the surface with floating sheerlegs attached to nylon strops passing under the hull and transferred to a cradle. It was also decided that the ship would be recovered before the end of the diving season in 1982. If the wreck stayed uncovered any longer it risked irreversible damage from biological decay and tidal scouring.

During the last year of the operation, the massive scope of full excavation and raising was beginning to take its toll on those closely involved in the project. In May 1981, Alexander McKee voiced concerns about the method chosen for raising the timbers and openly questioned Margaret Rule's position as excavation leader. McKee felt ignored in what he viewed as a project where he had always played a central role, both as the initiator of the search for the "Mary Rose" and other ships in the Solent, and as an active member throughout the diving operations. He had several supporters who all pointed to the risk of the project's turning into an embarrassing failure if the ship were damaged during raising operations. To address these concerns it was suggested that the hull should be placed on top of a supporting steel cradle underwater. This would avoid the inherent risks of damaging the wooden structure if it were lifted out of the water without appropriate support. The idea of using nylon strops was also discarded in favour of drilling holes through the hull at 170 points and passing iron bolts through them to allow the attachment of wires connected to a lifting frame.

In the spring of 1982, after three intense seasons of archaeological underwater work, preparations began for raising the ship. The operation soon ran into problems: early on there were difficulties with the custom-made lifting equipment; divers on the project belonging to the Royal Engineers had to be pulled because of the outbreak of the Falklands War; and the method of lifting the hull had to be considerably altered as late as June. After the frame was properly attached to the hull, it was slowly jacked up on four legs to pull the ship off the seabed. The massive crane of the barge "Tog Mor" then moved the frame and hull, transferring them underwater to the specially designed cradle, which was padded with water-filled bags. On the morning of 1982, the final lift of the entire package of cradle, hull and lifting frame began. It was watched by the team, Prince Charles and other spectators in boats around the site. At 9:03 am, the first timbers of the "Mary Rose" broke the surface. A second set of bags under the hull was inflated with air, to cushion the waterlogged wood. Finally, the whole package was placed on a barge and taken to the shore. Though eventually successful, the operation was close to foundering on two occasions; first when one of the supporting legs of the lifting frame was bent and had to be removed and later when a corner of the frame, with "an unforgettable crunch", slipped more than a metre (3 feet) and came close to crushing part of the hull.

As one of the most ambitious and expensive projects in the history of maritime archaeology, the "Mary Rose" project broke new ground within this field in the UK. Besides becoming one of the first wrecks to be protected under the new Protection of Wrecks Act in 1973 it also created several new precedents. It was the first time that a British privately funded project was able to apply modern scientific standards fully and without having to auction off part of the findings to finance its activities; where previous projects often had to settle for just a partial recovery of finds, everything found in connection with the "Mary Rose" was recovered and recorded. The raising of the vessel made it possible to establish the first historic shipwreck museum in the UK to receive government accreditation and funding. The excavation of the "Mary Rose" wreck site proved that it was possible to achieve a level of exactness in underwater excavations comparable to those on dry land.

Throughout the 1970s, the "Mary Rose" was meticulously surveyed, excavated and recorded with the latest methods within the field of maritime archaeology. Working in an underwater environment meant that principles of land-based archaeology did not always apply. Mechanical excavators, airlifts and suction dredges were used in the process of locating the wreck, but as soon as it began to be uncovered in earnest, more delicate techniques were employed. Many objects from the "Mary Rose" had been well preserved in form and shape, but many were quite delicate, requiring careful handling. Artefacts of all sizes were supported with soft packing material, such as old plastic ice cream containers, and some of the arrows that were "soft like cream cheese" had to be brought up in special styrofoam containers. The airlifts that sucked up clay, sand and dirt off-site or to the surface were still used, but with much greater precision since they could potentially disrupt the site. The many layers of sediment that had accumulated on the site could be used to date artefacts in which they were found, and had to be recorded properly. The various types of accretions and remnants of chemicals with artefacts were essential clues to objects that had long since broken down and disappeared, and needed to be treated with considerable care.

The excavation and raising of the ship in the 1970s and early 1980s meant that diving operations ceased, even though modern scaffolding and part of the bow were left on the seabed. The pressure on conservators to treat tens of thousands of artefacts and the high costs of conserving, storing and displaying the finds and the ship meant that there were no funds available for diving. In 2002, the UK Ministry of Defence announced plans to build two new aircraft carriers. Because of the great size of the new vessels, the outlet from Portsmouth needed to be surveyed to make sure that they could sail no matter the tide. The planned route for the underwater channel ran close to the "Mary Rose" wrecksite, which meant that funding was supplied to survey and excavate the site once more. Even though the planned carriers were down-sized enough to not require alteration of Portsmouth outlet, the excavations had already exposed timbers and were completed in 2005. Among the most important finds was the ten-metre (32 feet) stem, the forward continuation of the keel, which provided more exact details about the original profile of the ship.

Over 26,000 artefacts and pieces of timber were raised along with remains of about half the crew members. The faces of some crew members have been reconstructed. Analysis of the crew skeletons shows many had suffered malnutrition, and had evidence of rickets, scurvy, and other deficiency diseases. Crew members also developed arthritis through the stresses on their joints from heavy lifting and maritime life generally, and suffered bone fractures.

As the ship was intended to function as a floating, self-contained community, it was stocked with victuals (food and drink) that could sustain its inhabitants for extended periods of time. The casks used for storage on the "Mary Rose" have been compared with those from a wreck of a trade vessel from the 1560s and have revealed that they were of better quality, more robust and reliable, an indication that supplies for the Tudor navy were given high priority, and their requirements set a high standard for cask manufacturing at the time.

As a miniature society at sea, the wreck of the "Mary Rose" held personal objects belonging to individual crew members. This included clothing, games, various items for spiritual or recreational use, and objects related to mundane everyday tasks such as personal hygiene, fishing, and sewing. The master carpenter's chest, for example, contained a backgammon set, a book, three plates, a sundial, and a tankard, goods suggesting he was relatively wealthy.

The ship carried several skilled craftsmen and was equipped for handling both routine maintenance and repairing extensive battle damage. In and around one of the cabins on the main deck under the sterncastle, archaeologists found a "collection of woodworking tools ... unprecedented in its range and size", consisting of eight chests of carpentry tools. Along with loose mallets and tar pots used for caulking, this variety of tools belonged to one or several of the carpenters employed on the "Mary Rose".

Many of the cannons and other weapons from the "Mary Rose" have provided invaluable physical evidence about 16th-century weapon technology. The surviving gunshields are almost all from the "Mary Rose", and the four small cast iron hailshot pieces are the only known examples of this type of weapon.

Animal remains have been found in the wreck of the "Mary Rose". These include the skeletons of a rat, a frog and a dog. The dog, a mongrel between eighteen months and two years in age, was found near the hatch to the ship's carpenter's cabin and is thought to have been brought aboard as a ratter. Nine barrels have been found to contain bones of cattle, indicating that they contained pieces of beef butchered and stored as ship's rations. The bones of pigs and fish, stored in baskets, have also been found.

Two fiddles, a bow, a still shawm or "doucaine", three three-hole pipes, and a tabor drum with a drumstick were found throughout the wreck. These would have been used for the personal enjoyment of the crew and to provide a rhythm to work on the rigging and turning the capstans on the upper decks. The tabor drum is the earliest known example of its kind and the drumstick of a previously unknown design. The tabor pipes are considerably longer than any known examples from the period. Their discovery proved that contemporary illustrations, previously viewed with some suspicion, were accurate depictions of the instruments. Before the discovery of the "Mary Rose" shawm, an early predecessor to the oboe, instrument historians had been puzzled by reference to "still shawms", or "soft" shawms, that were said to have a sound that was less shrill than earlier shawms. The still shawm disappeared from the musical scene in the 16th century, and the instrument found on the "Mary Rose" is the only surviving example. A reproduction has been made and played. Combined with a pipe and tabor, it provides a "very effective bass part" that would have produced "rich and full sound, which would have provided excellent music for dancing on board ship". Only a few other fiddle-type instruments from the 16th century exist, but none of them of the type found on the "Mary Rose". Reproductions of both fiddles have been made, though less is known of their design than the shawm since the neck and strings were missing.

In the remains of a small cabin in the bow of the ship and in a few other locations around the wreck was found the earliest dated set of navigation instruments in Europe found so far: compasses, divider calipers, a stick used for charting, protractors, sounding leads, tide calculators and a logreel, an instrument for calculating speed. Several of these objects are not only unique in having such an early, definite dating, but also because they pre-date written records of their use; protractors would have reasonably been used to measure bearings and courses on maps, but sea charts are not known to have been used by English navigators during the first half of the 16th century, compasses were not depicted on English ships until the 1560s, and the first mention of a logreel is from 1574.

The cabin located on the main deck underneath the sterncastle is thought to have belonged to the barber-surgeon. He was a trained professional who saw to the health and welfare of the crew and acted as the medical expert on board. The most important of these finds were found in an intact wooden chest which contained over 60 objects relating to the barber-surgeon's medical practice: the wooden handles of a complete set of surgical tools and several shaving razors (although none of the steel blades had survived), a copper syringe for wound irrigation and treatment of gonorrhoea, and even a skilfully crafted feeding bottle for feeding incapacitated patients. More objects were found around the cabin, such as earscoops, shaving bowls and combs. With this wide selection of tools and medicaments the barber-surgeon, along with one or more assistants, could set bone fractures, perform amputations and deal with other acute injuries, treat a number of diseases and provide crew members with a minimal standard of personal hygiene.

One of the first scientifically confirmed ratters was "Hatch" a terrier and whippet dog crossbreed who spent his short life on the Mary Rose. The dog, named Hatch by researchers, was discovered in 1981 during the underwater excavation of the ship. Hatch's main duty was to kill rats on board the ship. Based on the DNA work performed on Hatch's teeth, he was a young adult male, 18–24 months old, with a brown coat. Hatch's skeleton is on display in the Mary Rose Museum in Portsmouth Historic Dockyard.

Preservation of the "Mary Rose" and her contents was an essential part of the project from the start. Though many artefacts, especially those that were buried in silt, had been preserved, the long exposure to an underwater environment had rendered most of them sensitive to exposure to air after recovery. Archaeologists and conservators had to work in tandem from the start to prevent deterioration of the artefacts. After recovery, finds were placed in so-called passive storage, which would prevent any immediate deterioration before the active conservation which would allow them to be stored in an open-air environment. Passive storage depended on the type of material that the object was made of, and could vary considerably. Smaller objects from the most common material, wood, were sealed in polyethylene bags to preserve moisture. Timbers and other objects that were too large to be wrapped were stored in unsealed water tanks. Growth of fungi and microbes that could degrade wood were controlled by various techniques, including low-temperature storage, chemicals, and in the case of large objects, common pond snails that consumed wood-degrading organisms but not the wood itself.

Other organic materials such as leather, skin and textiles were treated similarly, by keeping them moist in tanks or sealed plastic containers. Bone and ivory was desalinated to prevent damage from salt crystallisation, as were glass, ceramic and stone. Iron, copper and copper alloy objects were kept moist in a sodium sesquicarbonate solution to prevent oxidisation and reaction with the chlorides that had penetrated the surface. Alloys of lead and pewter are inherently stable in the atmosphere and generally require no special treatment. Silver and gold were the only materials that required no special passive storage.
Conserving the hull of the "Mary Rose" was the most complicated and expensive task for the project. In 2002 a donation of from the Heritage Lottery Fund and equivalent monetary support from the Portsmouth City and Hampshire County Councils was needed to keep the work with conservation on schedule. During passive conservation, the ship structure could for practical reasons not be completely sealed, so instead it was regularly sprayed with filtered, recycled water that was kept at a temperature of 2 to 5 °C (35 to 41 °F) to keep it from drying out. Drying waterlogged wood that has been submerged for several centuries without appropriate conservation causes considerable shrinkage (20–50%) and leads to severe warping and cracking as water evaporates from the cellular structure of the wood. The substance polyethylene glycol (PEG) had been used before on archaeological wood, and was during the 1980s being used to conserve the "Vasa". After almost ten years of small-scale trials on timbers, an active three-phase conservation programme of the hull of the "Mary Rose" began in 1994. During the first phase, which lasted from 1994 to 2003, the wood was sprayed with low-molecular-weight PEG to replace the water in the cellular structure of the wood. From 2003 to 2010, a higher-molecular-weight PEG was used to strengthen the mechanical properties of the outer surface layers. The third phase consisted of a controlled air drying ending in 2016. Researchers are planning on using magnetic nanoparticles to remove iron in the ship's wood to reduce the production of harmful sulfuric acid that is causing deterioration.

After the decision to raise the "Mary Rose," discussions ensued as to where she would eventually go on permanent display. The east end of Portsea Island at Eastney emerged as an early alternative, but was rejected because of parking problems and the distance from the dockyard where she was originally built. Placing the ship next to the famous flagship of Horatio Nelson, HMS "Victory", at Portsmouth Historic Dockyard was proposed in July 1981. A group called the Maritime Preservation Society even suggested Southsea Castle, where Henry VIII had witnessed the sinking, as a final resting place and there was widespread scepticism to the dockyard location. At one point a county councillor even threatened to withdraw promised funds if the dockyard site became more than an interim solution. As costs for the project mounted, there was a debate in the Council chamber and in the local paper "The News" as to whether the money could be spent more appropriately. Although author David Childs writes that in the early 1980s "the debate was as a fiery one", the project was never seriously threatened because of the great symbolic importance of the "Mary Rose" to the naval history of both Portsmouth and England.

Since the mid-1980s, the hull of the "Mary Rose" has been kept in a covered dry dock while undergoing conservation. Although the hull has been open to the public for viewing, the need for keeping the ship saturated first with water and later a polyethylene glycol (PEG) solution meant that, before 2013, visitors were separated from the hull by a glass barrier. By 2007, the specially built ship hall had been visited by over seven million visitors since it first opened on 1983, just under a year after it was successfully raised.

A separate Mary Rose Museum was housed in a structure called No. 5 Boathouse near the ship hall and was opened to the public on 1984. containing displays explaining the history of the ship and a small number of conserved artefacts, from entire bronze cannons to household items. In September 2009 the temporary "Mary Rose" display hall was closed to visitors to facilitate construction of the new museum building, which opened to the public on 31 May 2013.

The new Mary Rose Museum was designed by architects Wilkinson Eyre, Perkins+Will and built by construction firm Warings. The construction has been challenging because the museum has been built over the ship in the dry dock which is a listed monument. During construction of the museum, conservation of the hull continued inside a sealed "hotbox". In April 2013 the polyethylene glycol sprays were turned off and the process of controlled airdrying began. In 2016 the "hotbox" was removed and for the first time since 1545, the ship was revealed dry. This new museum displays most of the artefacts recovered from within the ship in context with the conserved hull. Since opening it has been visited by over 500,000 people.






</doc>
<doc id="19680" url="https://en.wikipedia.org/wiki?curid=19680" title="Mario Kart">
Mario Kart

The first in the series, "Super Mario Kart", was launched in 1992 on the Super Nintendo Entertainment System to critical and commercial success.

With six "Mario Kart" games released on home consoles, three on portable handheld consoles, four arcade games co-developed with Namco and one for mobile phones, the "Mario Kart" series includes a total of fourteen entries. The latest game in the main series, "Mario Kart Tour", was released on iOS and Android in September 2019. The series has sold over 150 million copies worldwide to date.

The first title in the "Mario Kart" series, "Super Mario Kart," was released for the Super Nintendo Entertainment System in 1992. The development of the first game was overseen by Shigeru Miyamoto, the Japanese video game designer who created the original "Super Mario Bros." as well as many other successful games for Nintendo. Darran Jones of NowGamer suggests that the original success of "Super Mario Kart" was the result of including characters previously seen in "Mario Bros." games, while also being a new type of racing game.

In the "Mario Kart" series, players compete in go-kart races, controlling one of a selection of characters, typically from the "Mario" franchise. Up to twelve characters can compete in each race; the exact number varies between games.

One of the features of the series is the use of various power-up items obtained by driving into item boxes laid out on the course. These power-ups include mushrooms to give players a speed boost, Koopa Shells to be thrown at opponents, banana peels and fake item boxes that can be laid on the course as hazards. The type of weapon received from an item box is influenced by the player's current position in the race. For example, players lagging far behind may receive more powerful items, such as Bullet Bills which give the player a bigger speed boost depending on the place of the player, while the leader may only receive small defensive items, such as shells or bananas. Called rubber banding, this gameplay mechanism allows other players or computers a realistic chance to catch up to the leading player. They can also perform driving techniques during the race such as rocket starts, slipstreaming, and mini-turbos.

As the series has progressed, each new installment has introduced new gameplay elements, such as new circuits, items, modes, and playable characters. These changes include:

"Mario Kart" mainly features characters from the "Mario" franchise. The "Mario Kart Arcade GP" series features Bandai Namco characters such as Pac-Man. The DLC for "Mario Kart 8" added Link from "The Legend of Zelda", and Villager and Isabelle from "Animal Crossing". "Mario Kart 8 Deluxe" features 42 characters, including the Inklings from "Splatoon".

Many course themes recur throughout the series. Most are based on existing areas in the "Mario" franchise (Bowser's Castle being among the most prominent), but there are a number of courses that have not appeared elsewhere, yet still belong in the Mushroom Kingdom, such as Rainbow Road, which usually takes place above a city or in space. Each game in the series (following the original game) includes at least 16 original courses and up to 6 original battle arenas. Each game's tracks are divided into four "cups", or groups in which the player has to have the highest overall ranking to win and they are the Mushroom Cup, the Flower Cup, the Star Cup, and the Special Cup. Most courses can be done in three laps, except in the original game where all circuits required five laps to finish, seven in "Mario Kart: Double Dash!!" when racing on Baby Park, and two in "Double Dash!!" when racing on Wario Colosseum as well as in "Mario Kart Tour". The first game to feature courses from previous games was "Mario Kart: Super Circuit", which contained all of the tracks from the original Super NES game. Starting with "Mario Kart DS", each entry in the series has featured sixteen "nitro" (brand new courses introduced for said game) and 16 "retro" tracks (reappearing courses from previous "Mario Kart" games), spread across four cups each with four races. The four Retro Grand Prix cups are the Shell Cup, the Banana Cup, the Leaf Cup, and the Lightning Cup. In "Mario Kart 8", sixteen additional tracks are available across two downloadable packages, eight for each package downloaded, including seven retro courses, four original courses, and five courses based on other Nintendo franchises, including "Excitebike", "F-Zero", "The Legend of Zelda", and "Animal Crossing" divided into four additional cups; the Egg Cup, the Triforce Cup, the Crossing Cup, and the Bell Cup. "Mario Kart Tour" introduced courses from around the world including New York City, Tokyo, Paris, London, and Vancouver; as well as variants of courses where drivers race in reverse (R), with additional ramps and elevation (T), and a combination of the two (R/T).

Each installment features a variety of different modes. The following five modes recur most often in the series:




Several "Mario Kart"-related items appear in the "Super Smash Bros." series, with "Super Smash Bros. Brawl" in particular featuring a Mario Circuit stage based on Figure-8 Circuit from "Mario Kart DS", "Super Smash Bros. for Nintendo 3DS" featuring a Rainbow Road stage based on its appearance in "Mario Kart 7", "Super Smash Bros. for Wii U" featuring a Mario Circuit stage based on its appearance in "Mario Kart 8", along with the returning Mario Circuit stage from "Brawl," and "Super Smash Bros. Ultimate" featuring Spirits and songs based on the series along with the returning stages.

Certain courses from the series have also appeared in "F-Zero X", "Fortune Street", the "Mario & Sonic" series, "", and the "WarioWare" series. Various items from the series can also be seen in games such as "Nintendogs" and "Animal Crossing".

The "Mario Kart" series has had a range of merchandise released.

Among them are a slot car racer series based on "Mario Kart DS", which comes with Mario and Donkey Kong figures, while Wario and Luigi are available separately. A line of radio-controlled karts have also been marketed, with are controlled by Game Boy Advance-shaped controllers, and feature Mario, Donkey Kong, and Yoshi. There are additional, larger karts that depict the same trio and are radio-controlled by a GameCube-shape controller.

Japanese figurines of Mario, Luigi, Peach, Toad, Yoshi, Wario, Donkey Kong, and Bowser are also available for purchase as well as for "Mario Kart 64", figures of Mario, Luigi, Wario, Bowser, Donkey Kong, and Yoshi were made by Toybiz. There are also Sound Drops inspired by "Mario Kart Wii" with eight sounds taken from the game including the Spiny shell and the Item Box. A land-line telephone featuring Mario holding a lightning bolt while seated in his kart, has also been marketed.

K'Nex released "Mario Kart Wii" sets, with Mario, Luigi, Yoshi, Donkey Kong, and Bowser in karts and bikes, as well as tracks from the game. "Mario Kart 7" and "Mario Kart 8" K'Nex sets have also been released.

LINE has released an animated sticker set with 24 stickers based on "Mario Kart 8" and "Mario Kart 8 Deluxe".

Nintendo's own customer rewards program Club Nintendo released merchandise from the series as well. These included a "Mario Kart 8" soundtrack, a "Mario Kart Wii"-themed stopwatch, and three gold trophies modeled after those in "Mario Kart 7". Before Club Nintendo, a "Mario Kart 64" soundtrack was offered by mail.

In 2014, McDonald's released "Mario Kart 8" toys with Happy Meals. They featured eight of the characters in karts that were customizable with stickers.

In 2018, Monopoly Gamer features a "Mario Kart" themed board game with courses from "Mario Kart 8" serving as properties, ten playable characters as tokens, (Mario, Luigi, Peach, Toad, Donkey Kong, Shy Guy, Metal Mario, Rosalina, Bowser, and Yoshi) and a special die with power-ups taken from the series.

In 2019, Hot Wheels teamed up with "Mario Kart" to release cars and track sets based on the series.

The "Mario Kart" series has received acclaim from critics. "Nintendo Power" listed the series as being one of the greatest multiplayer experiences, citing the diversity in game modes as well as the entertainment value found.

"Guinness World Records" listed six records set by the "Mario Kart" series, including "First Console Kart Racing Game", "Best Selling Racing Game" and "Longest Running Kart Racing Franchise". "Guinness World Records" ranked the original "Super Mario Kart" number 1 on the list of top 50 console games of all time based on initial impact and lasting legacy. "Super Mario Kart" has been inducted into the World Video Game Hall of Fame in 2019.

Like the "Super Mario" series, the "Mario Kart" series has achieved successful sales with over 150 million copies sold in total. "Super Mario Kart" has sold 8.76 million copies and is the fourth best-selling game on the Super Nintendo Entertainment System console. "Mario Kart 64" is the second best-selling game for the Nintendo 64 (behind "Super Mario 64"), selling a total of 9.87 million copies. "" has sold 6.96 million copies. It is the second best-selling game on the GameCube (next to "Super Smash Bros. Melee"). "Mario Kart Wii" has achieved highly successful numbers, selling a total of 37.32 million copies. It is the best-selling installment in the series and is the second best-selling game for the Wii (next to "Wii Sports"). "Mario Kart 8", released for the Wii U, has shipped 1.2 million copies in North America and Europe combined on its first few days since launch, which was the console's fastest-selling game until the record was beaten by "Super Smash Bros. for Wii U". It sold a total of 8.45 million copies and is the Wii U's best-selling game. In contrast, the enhanced port for the Nintendo Switch system, "Mario Kart 8 Deluxe", has sold 459,000 units in the United States in one day of its launch, making it the fastest-selling game in the series to date. "Deluxe" sold a total of 24.77 million copies worldwide, outperforming the original Wii U version, and is the best-selling Nintendo Switch game of all time. Both versions sold a combined total of 33.22 million copies, making it the second best-selling game in the series.

In the portable entries, the series also performed outstanding sales. "", has sold a total of 5.9 million copies, making it the fourth best-selling game on the Game Boy Advance. The second portable game, "Mario Kart DS", has sold a total of 23.60 million copies. The third best-selling game for the Nintendo DS, it is also the best-selling portable game in the series. "Mario Kart 7", released for the Nintendo 3DS, has sold 18.71 million copies, and is the best-selling 3DS game as of March 2020.

In September 2016, Nintendo filed an objection against the Japanese company MariCar, which rents go-karts modified for use on public roads in Tokyo along with costumes resembling Nintendo characters. MariCar's English website warned customers not to throw "banana peels" or "red turtle shells". The service is popular with tourists.

Nintendo argued that the MariCar name was "intended to be mistaken for or confused with" "Mario Kart", citing games commonly known by abbreviations in Japan, such as "Pokémon" (for "Pocket Monsters") and Sumabura ("Super Smash Bros."). In January 2017, the Japan Patent Office dismissed the objection, ruling that MariCar was not widely recognized as an abbreviation of "Mario Kart".

In February 2017, Nintendo sued MariCar over copyright infringement for renting unauthorized costumes of Nintendo characters and using their pictures to promote its business. In September 2018, MariCar was ordered to stop using the characters and pay Nintendo ¥10 million in damages.

Universal Parks & Resorts and Nintendo have plans on a "Mario Kart" themed ride at Universal Studios Japan at their most recent announcement of the Super Nintendo World theme park. And they have plans to build this ride in Singapore, Orlando and California. Plans should be announced either at the first Nintendo Direct of 2020 or at the 2020 Summer Olympics in Tokyo, Japan. It was confirmed by Nintendo and Universal that their new theme park in Florida, Universal's Epic Universe, will be the home of Nintendo world in Florida.


</doc>
<doc id="19683" url="https://en.wikipedia.org/wiki?curid=19683" title="Module">
Module

Module, modular and modularity may refer to the concept of modularity. They may also refer to:









</doc>
<doc id="19684" url="https://en.wikipedia.org/wiki?curid=19684" title="May 21">
May 21





</doc>
<doc id="19688" url="https://en.wikipedia.org/wiki?curid=19688" title="Mind map">
Mind map

A mind map is a diagram used to visually organize information. A mind map is hierarchical and shows relationships among pieces of the whole. It is often created around a single concept, drawn as an image in the center of a blank page, to which associated representations of ideas such as images, words and parts of words are added. Major ideas are connected directly to the central concept, and other ideas branch out from those major ideas.

Mind maps can also be drawn by hand, either as "notes" during a lecture, meeting or planning session, for example, or as higher quality pictures when more time is available. Mind maps are considered to be a type of spider diagram. A similar concept in the 1970s was "idea sun bursting".

Although the term "mind map" was first popularized by British popular psychology author and television personality Tony Buzan, the use of diagrams that visually "map" information using branching and radial maps traces back centuries. These pictorial methods record knowledge and model systems, and have a long history in learning, brainstorming, memory, visual thinking, and problem solving by educators, engineers, psychologists, and others. Some of the earliest examples of such graphical records were developed by Porphyry of Tyros, a noted thinker of the 3rd century, as he graphically visualized the concept categories of Aristotle. Philosopher Ramon Llull (1235–1315) also used such techniques.

The semantic network was developed in the late 1950s as a theory to understand human learning and developed further by Allan M. Collins and M. Ross Quillian during the early 1960s. Mind maps are similar in structure to concept maps, developed by learning experts in the 1970s, but differ in that mind maps are simplified by focusing around a single central key concept.

Buzan's specific approach, and the introduction of the term "mind map", arose during a 1974 BBC TV series he hosted, called "Use Your Head". In this show, and companion book series, Buzan promoted his conception of radial tree, diagramming key words in a colorful, radiant, tree-like structure.

Buzan says the idea was inspired by Alfred Korzybski's general semantics as popularized in science fiction novels, such as those of Robert A. Heinlein and A. E. van Vogt. He argues that while "traditional" outlines force readers to scan left to right and top to bottom, readers actually tend to scan the entire page in a non-linear fashion. Buzan's treatment also uses then-popular assumptions about the functions of cerebral hemispheres in order to explain the claimed increased effectiveness of mind mapping over other forms of note making.


Cunningham (2005) conducted a user study in which 80% of the students thought "mindmapping helped them understand concepts and ideas in science". Other studies also report some subjective positive effects on the use of mind maps. Positive opinions on their effectiveness, however, were much more prominent among students of art and design than in students of computer and information technology, with 62.5% vs 34% (respectively) agreeing that they were able to understand concepts better with mind mapping software. Farrand, Hussain, and Hennessy (2002) found that spider diagrams (similar to concept maps) had limited, but significant, impact on memory recall in undergraduate students (a 10% increase over baseline for a 600-word text only) as compared to preferred study methods (a 6% increase over baseline). This improvement was only robust after a week for those in the diagram group and there was a significant decrease in motivation compared to the subjects' preferred methods of note taking. A meta study about concept mapping concluded that concept mapping is more effective than "reading text passages, attending lectures, and participating in class discussions". The same study also concluded that concept mapping is slightly more effective "than other constructive activities such as writing summaries and outlines". However, results were inconsistent, with the authors noting "significant heterogeneity was found in most subsets". In addition, they concluded that low-ability students may benefit more from mind mapping than high-ability students.

Joeran Beel and Stefan Langer conducted a comprehensive analysis of the content of mind maps. They analysed 19,379 mind maps from 11,179 users of the mind mapping applications (now ) and MindMeister. Results include that average users create only a few mind maps (mean=2.7), average mind maps are rather small (31 nodes) with each node containing about three words (median). However, there were exceptions. One user created more than 200 mind maps, the largest mind map consisted of more than 50,000 nodes and the largest node contained ~7,500 words. The study also showed that between different mind mapping applications (Docear vs MindMeister) significant differences exist related to how users create mind maps.

There have been some attempts to create mind maps automatically. Brucks & Schommer created mind maps automatically from full-text streams. Rothenberger et al. extracted the main story of a text and presented it as mind map. And there is a patent about automatically creating sub-topics in mind maps.

Mind-mapping software can be used to organize large amounts of information, combining spatial organization, dynamic hierarchical structuring and node folding. Software packages can extend the concept of mind-mapping by allowing individuals to map more than thoughts and ideas with information on their computers and the Internet, like spreadsheets, documents, Internet sites and images. It has been suggested that mind-mapping can improve learning/study efficiency up to 15% over conventional note-taking.


</doc>
<doc id="19690" url="https://en.wikipedia.org/wiki?curid=19690" title="Machine gun">
Machine gun

A machine gun is a fully automatic firearm designed for rapid, sustained fire. Other fully automatic weapons, such as assault rifles and submachine guns, are not designed for sustained fire, and not considered machine guns.

As a class of military rapid-fire guns, machine guns are fully automatic weapons designed to be used as support weapons and generally used when attached to a mount or fired from the ground on a bipod or tripod. Many machine guns also use belt feeding and open bolt operation, features not normally found on other weapons.

Unlike semi-automatic firearms, which require one trigger pull per round fired, a machine gun is designed to fire for as long as the trigger is held down. Nowadays the term is restricted to relatively heavy weapons, able to provide continuous or frequent bursts of automatic fire for as long as ammunition lasts. Machine guns are normally used against personnel, aircraft and light vehicles, or to provide suppressive fire, either directly or indirectly. They are commonly mounted on fast attack vehicles such as technicals to provide heavy mobile firepower, armored vehicles such as tanks for engaging targets too small to justify use of the primary weaponry or too fast to effectively engage with it, and on aircraft as defensive armament or for strafing ground targets, though on fighter aircraft true machine guns have mostly been supplanted by large-caliber rotary guns.

Some machine guns have in practice sustained fire almost continuously for hours; other automatic weapons overheat after less than a minute of use. Because they become very hot, the great majority of designs fire from an open bolt, to permit air cooling from the breech between bursts. They also usually have either a barrel cooling system, slow-heating heavyweight barrel, or removable barrels which allow a hot barrel to be replaced.

Although subdivided into "light", "medium", "heavy" or "general-purpose", even the lightest machine guns tend to be substantially larger and heavier than standard infantry arms. Medium and heavy machine guns are either mounted on a tripod or on a vehicle; when carried on foot, the machine gun and associated equipment (tripod, ammunition, spare barrels) require additional crew members.

Light machine guns are designed to provide mobile fire support to a squad and are typically air-cooled weapons fitted with a box magazine or drum and a bipod; they may use full-size rifle rounds, but modern examples often use intermediate rounds. Medium machine guns use full-sized rifle rounds and are designed to be used from fixed positions mounted on a tripod. Heavy machine gun is a term originating in World War I to describe heavyweight medium machine guns and persisted into World War II with Japanese Hotchkiss M1914 clones; today, however, it is used to refer to automatic weapons with a caliber of at least .50 in (12.7 mm) but less than 20 mm. A general-purpose machine gun is usually a lightweight medium machine gun that can either be used with a bipod and drum in the light machine gun role or a tripod and belt feed in the medium machine gun role.

Machine guns usually have simple iron sights, though the use of optics is becoming more common. A common aiming system for direct fire is to alternate solid ("ball") rounds and tracer ammunition rounds (usually one tracer round for every four ball rounds), so shooters can see the trajectory and "walk" the fire into the target, and direct the fire of other soldiers.

Many heavy machine guns, such as the Browning M2 .50 caliber machine gun, are accurate enough to engage targets at great distances. During the Vietnam War, Carlos Hathcock set the record for a long-distance shot at with a .50 caliber heavy machine gun he had equipped with a telescopic sight. This led to the introduction of .50 caliber anti-materiel sniper rifles, such as the Barrett M82.

Other automatic weapons are subdivided into several categories based on the size of the bullet used, whether the cartridge is fired from a closed bolt or an open bolt, and whether the action used is locked or is some form of blowback.

Fully automatic firearms using pistol-calibre ammunition are called machine pistols or submachine guns largely on the basis of size; those using shotgun cartridges are almost always referred to as automatic shotguns. The term personal defense weapon (PDW) is sometimes applied to weapons firing dedicated armor-piercing rounds which would otherwise be regarded as machine pistols or SMGs, but it is not particularly strongly defined and has historically been used to describe a range of weapons from ordinary SMGs to compact assault rifles. Selective fire rifles firing a full-power rifle cartridge from a closed bolt are called automatic rifles or battle rifles, while rifles that fire an intermediate cartridge are called assault rifles.

Assault rifles are a compromise between the size and weight of a pistol-calibre submachine gun and a full-size battle rifle, firing intermediate cartridges and allowing semi-automatic and burst or full-automatic fire options (selective fire), sometimes with both of the latter present.

Many machine guns are of the locked breech type, and follow this cycle:


The operation is basically the same for all locked breech automatic firearms, regardless of the means of activating these mechanisms. There are also multi-chambered formats, such as revolver cannon, and some automatic weapons, including many submachine guns, the Schwarzlose machine gun etc., that do not lock at all but instead use simple blowback or some type of delayed blowback.

Most modern machine guns are of the locking type, and of these, most utilize the principle of gas-operated reloading, which taps off some of the propellant gas from the fired cartridge, using its mechanical pressure to unlock the bolt and cycle the action. The Russian PK machine gun is an example. Another efficient and widely used format is the recoil actuated type, which uses the gun's recoil energy for the same purpose. Machine guns such as the M2 Browning and MG42, are of this second kind. A cam, lever or actuator absorbs part of the energy of the recoil to operate the gun mechanism.

An externally actuated weapon uses an external power source, such as an electric motor or hand crank, to move its mechanism through the firing sequence. Modern weapons of this type are often referred to as Gatling guns, after the original inventor (not only of the well-known hand-cranked 19th century proto-machine gun, but also of the first electrically-powered version). They have several barrels each with an associated chamber and action on a rotating carousel and a system of cams that load, cock, and fire each mechanism progressively as it rotates through the sequence; essentially each barrel is a separate bolt-action rifle using a common feed source. The continuous nature of the rotary action, and its relative immunity to overheating allow for an incredibly high cyclic rate of fire, often several thousand rounds per minute. Rotary guns are less prone to jamming than a gun operated by gas or recoil, as the external power source will eject misfired rounds with no further trouble, but this is not possible in the rare cases of self-powered rotary guns. Rotary designs are intrinsically comparatively bulky and expensive, and are therefore generally used with large rounds, 20 mm in diameter or more, often referred to as Rotary cannon – though the rifle-calibre Minigun is an exception to this. Whereas such weapons are highly reliable and formidably effective, one drawback is that the weight and size of the power source and driving mechanism makes them usually impractical for use outside of a vehicle or aircraft mount.

Revolver cannons, such as the Mauser MK 213, were developed in World War II by the Germans to provide high-caliber cannons with a reasonable rate of fire and reliability. In contrast to the rotary format, such weapons have a single barrel, and a recoil-operated carriage holding a revolving chamber with typically five chambers. As each round is fired, electrically, the carriage moves back rotating the chamber which also ejects the spent case, indexes the next live round to be fired with the barrel and loads the next round into the chamber. The action is very similar to that of the revolver pistols common in the 19th and 20th centuries, giving this type of weapon its name. A Chain gun is a specific, patented type of Revolver cannon, the name in this case deriving from its driving mechanism.

Firing a machine gun for prolonged periods produces large amounts of heat. In a worst-case scenario this may cause a cartridge to overheat and detonate even when the trigger is not pulled, potentially leading to damage or causing the gun to cycle its action and keep firing until it has exhausted its ammunition supply or jammed (this is known as "cooking off", distinct from "runaway fire" where the sear fails to re-engage when the trigger is released). To prevent this, some kind of cooling system is required. Early machine guns were often water-cooled; while this technology was very effective, (and was indeed one of the sources of the notorious efficiency of machine guns during the First World War ), the water jackets also added considerable weight to an already bulky design; they were also vulnerable to bullets themselves. Armour could of course be provided, and in WW I the Germans in particular often did this; but this added yet more weight to the guns. Air-cooled machine guns often feature quick-change barrels (often carried by a crew member), passive cooling fins, or in some designs forced-air cooling, such as that employed by the Lewis Gun. Advances in metallurgy and use of special composites in barrel liners allow for greater heat absorption and dissipation during firing. The higher the rate of fire, the more often barrels must be changed and allowed to cool. To minimize this, most air-cooled guns are fired only in short bursts or at a reduced rate of fire. Some designs – such as the many variants of the MG42 – are capable of rates of fire in excess of 1,200 rounds per minute. Gatling guns are capable of the fastest firing rates of all, partly because this format involves extra energy being injected into the system from outside, instead of depending on energy derived from the propellant contained within the cartridges, and partly because this design deals with the unwanted heat most efficiently – effectively quick-changing the barrel and chamber after every shot. The multiple guns that comprise a Gatling being a much larger bulk of metal than other, single-barreled guns, they are thus much slower to rise in temperature for a given amount of heat. Simultaneously they are much better at shedding the excess, as the extra barrels not only provide a larger surface area from which to dissipate it, but in the nature of the design are spun at very high speed, which has the benefit of producing enhanced air-cooling as a side-effect.

In weapons where the round seats and fires at the same time, mechanical timing is essential for operator safety, to prevent the round from firing before it is seated properly. Machine guns are controlled by one or more mechanical sears. When a sear is in place, it effectively stops the bolt at some point in its range of motion. Some sears stop the bolt when it is locked to the rear. Other sears stop the firing pin from going forward after the round is locked into the chamber. Almost all machine guns have a "safety" sear, which simply keeps the trigger from engaging.

The first successful machine-gun designs were developed in the mid-19th century. The key characteristic of modern machine guns, their relatively high rate of fire and more importantly mechanical loading, first appeared in the Model 1862 Gatling gun, which was adopted by the United States Navy. These weapons were still powered by hand; however, this changed with Hiram Maxim's idea of harnessing recoil energy to power reloading in his Maxim machine gun. Dr. Gatling also experimented with electric-motor-powered models; as discussed above, this externally powered machine reloading has seen use in modern weapons as well.

While technical use of the term "machine gun" has varied, the modern definition used by the Sporting Arms and Ammunition Manufacturers' Institute of America is "a fully automatic firearm that loads, fires and ejects continuously when the trigger is held to the rear until the ammunition is exhausted or pressure on the trigger is released." This definition excludes most early manually operated repeating arms such as volley guns and the Gatling gun.

The first known ancestors of multi-shot weapons were medieval organ guns, while the first to have the ability to fire multiple shots from a single barrel without a full manual reload were revolvers made in Europe in the late 1500s. One is a shoulder-gun-length weapon made in Nuremberg, Germany, circa 1580. Another is a revolving arquebus, produced by Hans Stopler of Nuremberg in 1597.

True repeating long arms were difficult to manufacture prior to the development of the unitary firearm cartridge; nevertheless, lever-action repeating rifles such as the Kalthoff repeater and Cookson repeater were made in small quantities in the 17th century.

Perhaps the earliest examples of predecessors to the modern machine gun are to be found in East Asia. According to the Wu-Pei-Chih, a booklet examining Chinese military equipment produced during the first quarter of the 17th century, the Chinese army had in its arsenal the 'Po-Tzu Lien-Chu-P'ao' or 'string-of-100-bullets cannon'. This was a repeating cannon fed by a hopper which fired its charges sequentially. Another repeating gun was produced by a Chinese commoner in the late 17th century. This weapon was also hopper-fed and never went into mass production.

In 1663 the first mention of the automatic principle of machine guns was in a paper presented to the Royal Society of England by an Englishman by the name of Palmer who described a volley gun capable of being operated by either recoil or gas.

Another early revolving gun was created by James Puckle, a London lawyer, who patented what he called "The Puckle Gun" on May 15, 1718. It was a design for a manually operated 1.25 in. (32 mm) caliber, flintlock cannon with a revolver cylinder able to fire 6–11 rounds before reloading by swapping out the cylinder, intended for use on ships. It was one of the earliest weapons to be referred to as a machine gun, being called such in 1722, though its operation does not match the modern usage of the term. According to Puckle, it was able to fire round bullets at Christians and square bullets at Turks. However, it was a commercial failure and was not adopted or produced in any meaningful quantity.

In 1777, Philadelphia gunsmith Joseph Belton offered the Continental Congress a "new improved gun", which was capable of firing up to twenty shots in five seconds; unlike older repeaters using complex lever-action mechanisms, it used a simpler system of superposed loads, and was loaded with a single large paper cartridge. Congress requested that Belton modify 100 flintlock muskets to fire eight shots in this manner, but rescinded the order when Belton's price proved too high.

In the early and mid-19th century, a number of rapid-firing weapons appeared which offered multi-shot fire, mostly volley guns. Volley guns (such as the Mitrailleuse) and double-barreled pistols relied on duplicating all parts of the gun, though the Nock gun used the otherwise-undesirable "chain fire" phenomenon (where multiple chambers are ignited at once) to propagate a spark from a single flintlock mechanism to multiple barrels. Pepperbox pistols also did away with needing multiple hammers but used multiple manually operated barrels. Revolvers further reduced this to only needing a pre-prepared cylinder and linked advancing the cylinder to cocking the hammer. However, these were still manually operated.

In the 1830s a machine gun was designed by a Swiss man called Jacob Steuble, who tried to sell it to the Russian, English and French governments. The English and Russian governments showed interest but the former refused to pay Steuble, who later sued them for this transgression, and the latter tried to imprison him. The French government showed interest at first and while they noted that mechanically there was nothing wrong with Steuble's invention they turned him down, stating that the machine both lacked novelty and could not be usefully employed by the army.

In 1847 a short description of a prototype electrically-ignited mechanical machine gun was published in Scientific American by a J.R. Nichols. The model described is small in scale and works by rotating a series of barrels vertically so that it is feeding at the top from a 'tube' or hopper and could be discharged immediately at any elevation after having received a charge, according to the author.

In 1848 an Italian by the name of Cesar Rosaglio announced his invention of a machine gun capable of being operated by a single man and firing 300 shots a minute or 12,000 in an hour after taking into account the time needed to reload the 'tanks' of ammunition.

In June 1851 a model of a 'war engine' allegedly capable of firing 10,000 ball cartridges in 10 minutes was demonstrated by a British inventor called Francis McGetrick.

In 1852 a rotary cannon using a unique form of wheellock ignition was designed by an Irish immigrant to America by the name of Delany.

In 1854 a British patent for a mechanically operated machine gun was filed by Henry Clarke. This weapon used multiple barrels arranged side by side, fed by a revolving cylinder that was in turn fed by hoppers, similar to the system used by Nichols. The gun could be fired by percussion or electricity, according to the author. Unlike other mechanically operated machine guns of the era, this weapon didn't use any form of self-contained cartridge, with firing being carried out by separate percussion caps. In the same year, water cooling was proposed for machine guns by Henry Bessemer, along with a water cleaning system, though he later abandoned this design. In his patent, Bessemer describes a hydropneumatic blowback-operated, fully automatic cannon. Part of the patent also refers to a steam-operated piston to be used with firearms but the bulk of the patent is spent detailing the former system.

In America, a patent for a machine gun type weapon was filed by John Andrus Reynolds in 1855. Another early American patent for a manually operated machine gun was filed by C. E. Barnes in 1856.

In France and Britain, a mechanically operated machine gun was patented in 1856 by the Frenchman Francois Julien. This weapon was a cannon that fed from a type of open-ended tubular magazine, only using rollers and an endless chain in place of springs.

The Agar Gun, otherwise known as a "coffee-mill gun" because of its resemblance to a coffee mill, was invented by Wilson Agar at the beginning of the US Civil War. The weapon featured mechanized loading using a hand crank linked to a hopper above the weapon. The weapon featured a single barrel and fired through the turning of the same crank; it operated using paper cartridges fitted with percussion caps and inserted into metal tubes which acted as chambers; it was therefore functionally similar to a revolver. The weapon was demonstrated to President Lincoln in 1861. He was so impressed with the weapon that he purchased 10 on the spot for $1,500 apiece. The Union Army eventually purchased a total of 54 of the weapons. However, due to antiquated views of the Ordnance Department the weapons, like its more famous counterpart the Gatling Gun, saw only limited use.

The Gatling gun, patented in 1861 by Richard Jordan Gatling, was the first to offer controlled, sequential fire with mechanical loading. The design's key features were machine loading of prepared cartridges and a hand-operated crank for sequential high-speed firing. It first saw very limited action in the American Civil War; it was subsequently improved and used in the Franco-Prussian war and North-West Rebellion. Many were sold to other armies in the late 19th century and continued to be used into the early 20th century, until they were gradually supplanted by Maxim guns. Early multi-barrel guns were approximately the size and weight of contemporary artillery pieces, and were often perceived as a replacement for cannon firing grapeshot or canister shot. The large wheels required to move these guns around required a high firing position, which increased the vulnerability of their crews. Sustained firing of gunpowder cartridges generated a cloud of smoke, making concealment impossible until smokeless powder became available in the late 19th century. Gatling guns were targeted by artillery they could not reach, and their crews were targeted by snipers they could not see. The Gatling gun was used most successfully to expand European colonial empires, since against poorly equipped indigenous armies it did not face such threats.

In 1870 a Lt. D. H. Friberg of the Swedish army patented a fully automatic recoil-operated firearm action and may have produced firing prototypes of a derived design around 1882: this was the forerunner to the 1907 Kjellman machine gun, though, due to rapid residue buildup from the use of black powder, Friberg's design was not a practical weapon.

Also in 1870, the Bavarian regiment of the Prussian army used a unique mitrailleuse style weapon in the Franco-Prussian war. The weapon was made up of four barrels placed side by side that replaced the manual loading of the French mitrailleuse with a mechanical loading system featuring a hopper containing 41 cartridges at the breech of each barrel. Although it was used effectively at times, mechanical difficulties hindered its operation and it was ultimately abandoned shortly after the war ended ().

The first practical self-powered machine gun was invented in 1884 by Sir Hiram Maxim. The Maxim machine gun used the recoil power of the previously fired bullet to reload rather than being hand-powered, enabling a much higher rate of fire than was possible using earlier designs such as the Nordenfelt and Gatling weapons. Maxim also introduced the use of water cooling, via a water jacket around the barrel, to reduce overheating. Maxim's gun was widely adopted, and derivative designs were used on all sides during the First World War. The design required fewer crew and was lighter and more usable than the Nordenfelt and Gatling guns. First World War combat experience demonstrated the military importance of the machine gun. The United States Army issued four machine guns per regiment in 1912, but that allowance increased to 336 machine guns per regiment by 1919.
Heavy guns based on the Maxim such as the Vickers machine gun were joined by many other machine weapons, which mostly had their start in the early 20th century such as the Hotchkiss machine gun. Submachine guns (e.g., the German MP 18) as well as lighter machine guns (the first light machine gun deployed in any significant number being the Madsen machine gun, with the Chauchat and Lewis gun soon following) saw their first major use in World War I, along with heavy use of large-caliber machine guns. The biggest single cause of casualties in World War I was actually artillery, but combined with wire entanglements, machine guns earned a fearsome reputation.

Another fundamental development occurring before and during the war was the incorporation by gun designers of machine gun auto-loading mechanisms into handguns, giving rise to semi-automatic pistols such as the Borchardt (1890s), automatic machine pistols and later submachine guns (such as the Beretta 1918).

Machine guns were mounted in aircraft for the first time in World War I. Immediately this raised a problem. The most effective position for guns in a single-seater fighter was clearly, for the purpose of aiming, directly in front of the pilot; but this placement would obviously result in bullets striking the moving propeller. Early solutions, aside from simply hoping that luck was on the pilot's side with an unsynchronized forward-firing gun, involved either aircraft with pusher props like the Vickers F.B.5, Royal Aircraft Factory F.E.2 and Airco DH.2, wing mounts like that of the Nieuport 10 and Nieuport 11 which avoided the propeller entirely, or armored propeller blades such as those mounted on the Morane-Saulnier L which would allow the propeller to deflect unsynchronized gunfire. By mid 1915, the introduction of a reliable gun synchronizer by the Imperial German Flying Corps made it possible to fire a closed-bolt machine gun forward through a spinning propeller by timing the firing of the gun to miss the blades. The Allies had no equivalent system until 1916 and their aircraft suffered badly as a result, a period known as the Fokker Scourge, after the Fokker Eindecker, the first German plane to incorporate the new technology.

As better materials became available following the First World War, light machine guns became more readily portable; designs such as the Bren light machine gun replaced bulky predecessors like the Lewis gun in the squad support weapon role, while the modern division between medium machine guns like the M1919 Browning machine gun and heavy machine guns like the Browning M2 became clearer. New designs largely abandoned water jacket cooling systems as both undesirable, due to a greater emphasis on mobile tactics and unnecessary, thanks to the alternative and superior technique of preventing overheating by swapping barrels.

The interwar years also produced the first widely used and successful general-purpose machine gun, the German MG 34. While this machine gun was equally able in the light and medium roles, it proved difficult to manufacture in quantity, and experts on industrial metalworking were called in to redesign the weapon for modern tooling, creating the MG 42. This weapon was simpler, cheaper to produce, fired faster, and replaced the MG 34 in every application except vehicle mounts, since the MG 42's barrel changing system could not be operated when it was mounted.

Experience with the MG42 led to the US issuing a requirement to replace the aging Browning Automatic Rifle with a similar weapon, which would also replace the M1919; simply using the MG42 itself was not possible, as the design brief required a weapon which could be fired from the hip or shoulder like the BAR. The resulting design, the M60 machine gun, was issued to troops during the Vietnam War.

As it became clear that a high-volume-of-fire weapon would be needed for fast-moving jet aircraft to reliably hit their opponents, Gatling's work with electrically powered weapons was recalled and the 20 mm M61 Vulcan designed; a miniaturized 7.62 mm version initially known as the "mini-Vulcan" and quickly shortened to "minigun" was soon in production for use on helicopters, where the volume of fire could compensate for the instability of the helicopter as a firing platform.

The most common interface on machine guns is a pistol grip and trigger. On earlier manual machine guns, the most common type was a hand crank. On externally powered machine guns, such as miniguns, an electronic button or trigger on a joystick is commonly used. Light machine guns often have a butt stock attached, while vehicle and tripod mounted machine guns usually have spade grips. In the late 20th century, scopes and other complex optics became more common as opposed to the more basic iron sights.

Loading systems in early manual machine guns were often from a hopper of loose (un-linked) cartridges. Manual-operated volley guns usually had to be reloaded manually all at once (each barrel reloaded by hand, or with a set of cartridges affixed to a plate that was inserted into the weapon). With hoppers, the rounds could often be added while the weapon was firing. This gradually changed to belt-fed types. Belts were either held in the open by the person, or in a bag or box. Some modern vehicle machine guns used linkless feed systems however.

Modern machine guns are commonly mounted in one of four ways. The first is a bipod – often these are integrated with the weapon. This is common on light machine guns and some medium machine guns. Another is a tripod, where the person holding it does not form a "leg" of support. Medium and heavy machine guns usually use tripods. On ships, vehicles and aircraft machine guns are usually mounted on a pintle mount – basically a steel post that is connected to the frame or body. Tripod and pintle mounts are usually used with spade grips. The last major mounting type is one that is disconnected from humans, as part of an armament system, such as a tank coaxial or part of an aircraft's armament. These are usually electrically fired and have complex sighting systems, for example, the US Helicopter Armament Subsystems.



</doc>
<doc id="19692" url="https://en.wikipedia.org/wiki?curid=19692" title="Monopoly (game)">
Monopoly (game)

Monopoly is a board game currently published by Hasbro. In the game, players roll two six-sided dice to move around the game board, buying and trading properties, and developing them with houses and hotels. Players collect rent from their opponents, with the goal being to drive them into bankruptcy. Money can also be gained or lost through Chance and Community Chest cards, and tax squares; players can end up in jail, which they cannot move from until they have met one of several conditions. The game has numerous house rules, and hundreds of different editions exist, as well as many spin-offs and related media. "Monopoly" has become a part of international popular culture, having been licensed locally in more than 103 countries and printed in more than 37 languages.

"Monopoly" is derived from "The Landlord's Game" created by Lizzie Magie in the United States in 1903 as a way to demonstrate that an economy which rewards wealth creation is better than one where monopolists work under few constraints, and to promote the economic theories of Henry George—in particular his ideas about taxation. It is important to note that The Landlord's Game had two sets of rules originally, one with taxation and another one mainly based on current rules. When Monopoly was first published by Parker Brothers in 1935, it did not include the less capitalistic taxation rule, which resulted in a more competitive game. Parker Brothers was eventually absorbed into Hasbro in 1991. The game is named after the economic concept of monopoly—the domination of a market by a single entity.

The history of "Monopoly" can be traced back to 1903, when American anti-monopolist Lizzie Magie created a game which she hoped would explain the single tax theory of Henry George. It was intended as an educational tool to illustrate the negative aspects of concentrating land in private monopolies. She took out a patent in 1904. Her game, "The Landlord's Game", was self-published, beginning in 1906.

Magie created two sets of rules: an anti-monopolist set in which all were rewarded when wealth was created, and a monopolist set in which the goal was to create monopolies and crush opponents.

Several variant board games, based on her concept, were developed from 1906 through the 1930s; they involved both the process of buying land for its development and the sale of any undeveloped property. Cardboard houses were added and rents increased as they were added to a property. Magie patented the game again in 1923.

According to an advertisement placed in "The Christian Science Monitor", Charles Todd of Philadelphia recalled the day in 1932 when his childhood friend, Esther Jones, and her husband Charles Darrow came to their house for dinner. After the meal, the Todds introduced Darrow to "The Landlord's Game", which they then played several times. The game was entirely new to Darrow, and he asked the Todds for a written set of the rules. After that night, Darrow went on to utilize this and distribute the game himself as "Monopoly".

Parker Brothers bought the game's copyrights from Darrow. When the company learned Darrow was not the sole inventor of the game, it bought the rights to Magie's patent for just $500.

Parker Brothers began marketing the game on November 5, 1935. Cartoonist F. O. Alexander contributed the design. U. S. patent number US 2026082 A was issued to Charles Darrow on December 31, 1935, for the game board design and was assigned to Parker Brothers Inc. The original version of the game in this format was based on the streets of Atlantic City, New Jersey.

In 1936, Parker Brothers began licensing the game for sale outside the United States. In 1941, the British Secret Intelligence Service had John Waddington Ltd., the licensed manufacturer of the game in the United Kingdom, create a special edition for World War II prisoners of war held by the Nazis. Hidden inside these games were maps, compasses, real money, and other objects useful for escaping. They were distributed to prisoners by fake charity organizations created by the British Secret Service.

In the Nazi-occupied Netherlands, the German government and its collaborators were displeased with Dutch people using Monopoly Game sets with American or British locales, and developed a version with Dutch locations. Since that version had in itself no specific pro-Nazi elements, it continued in use after the war, and formed the base for Monopoly games used in the Netherlands up to the present.

Economics professor Ralph Anspach published a game "Anti-Monopoly" in 1973, and was sued for trademark infringement by Parker Brothers in 1974. The case went to trial in 1976. Anspach won on appeals in 1979, as the 9th Circuit Court determined that the trademark "Monopoly" was generic and therefore unenforceable. The United States Supreme Court declined to hear the case, allowing the appellate court ruling to stand. This decision was overturned by the passage of Public Law 98–620 in 1984. With that law in place, Parker Brothers and its parent company, Hasbro, continue to hold valid trademarks for the game "Monopoly". However, "Anti-Monopoly" was exempted from the law and Anspach later reached a settlement with Hasbro and markets his game under license from them.

The research that Anspach conducted during the course of the litigation was what helped bring the game's history before Charles Darrow into the spotlight.

In 1991, Hasbro acquired Parker Bros. and thus "Monopoly". Before the Hasbro acquisition, Parker Bros. acted as a publisher only issuing two versions at a time, a regular and deluxe. Hasbro moved to create and license many other versions of "Monopoly" and sought public input in varying the game. A new wave of licensed products began in 1994, when Hasbro granted a license to USAopoly to begin publishing a San Diego Edition of "Monopoly", which has since been followed by more than a hundred more licensees including Winning Moves Games (since 1995) and Winning Solutions, Inc. (since 2000) in the United States.

In 2003, the company held a national tournament on a chartered train going from Chicago to Atlantic City (see ). Also in 2003, Hasbro sued the maker of Ghettopoly and won. In February 2005, the company sued RADGames over their Super Add-On accessory board game that fit in the center of the board. The judge initially issued an injunction on February 25, 2005, to halt production and sales before ruling in RADGames' favor in April 2005.

In 2008, the Speed Die was added to all regular Monopoly set. After polling their Facebook followers, Hasbro Gaming took the top house rules and added them to a House Rule Edition released in the Fall of 2014 and added them as optional rules in 2015. In January 2017, Hasbro invited Internet users to vote on a new set of game pieces, with this new regular edition to be issued in March 2017.

On May 1, 2018, the Monopoly Mansion hotel agreement was announced by Hasbro's managing director for South-East Asia, Hong Kong and Taiwan, Jenny Chew Yean Nee with M101 Holdings Sdn Bhd. M101 has the five-star, 225-room hotel, then under construction, located at the M101 Bukit Bintang in Kuala Lumpur and would have a 1920s Gatsby feel. M101's Sirocco Group would manage the hotel when it opens in 2019.

The "Monopoly" game-board consists of forty spaces containing twenty-eight properties—twenty-two streets (grouped into eight color groups), four railroads, and two utilities—three Chance spaces, three Community Chest spaces, a Luxury Tax space, an Income Tax space, and the four corner squares: GO, (In) Jail/Just Visiting, Free Parking, and Go to Jail.

There have since been some changes to the board. Not all of the Chance and Community Chest cards as shown in the 1935 patent were used in editions from 1936/1937 onwards. Graphics with the Mr. Monopoly character (then known as "Rich Uncle Pennybags") were added in that same time-frame. A graphic of a chest containing coins was added to the Community Chest spaces, as were the flat purchase prices of the properties. Traditionally, the Community Chest cards were yellow (although they were sometimes printed on blue stock) with no decoration or text on the back; the Chance cards were orange with no text or decoration on the back.

Hasbro commissioned a major graphic redesign to the U.S. Standard Edition of the game in 2008 along with some minor revisions. Among the changes: the colors of Mediterranean and Baltic Avenues changed from purple to brown, and the colors of the GO square changed from red to black. A flat $200 Income Tax was imposed (formerly the player's choice of $200 or 10% of their total holdings, which they could not calculate until after making their final decision). Originally the amount was $300 but was changed a year after the game's debut, and the Luxury Tax amount increased to $100 from $75. There were also changes to the Chance and Community Chest cards; for example, the "poor tax" and "grand opera opening" cards became "speeding fine" and "it is your birthday", respectively; though their effects remained the same; the player must pay only $50 instead of $150 for the school tax. In addition, a player now gets $50 instead of $45 for sale of stock, and the Advance to Illinois Avenue card now has the added text indicating a player collects $200 if they pass Go on the way there.
All the Chance and Community Chest cards received a graphic upgrade in 2008 as part of the graphic refresh of the game. Mr. Monopoly's classic line illustration was also now usually replaced by renderings of a 3D Mr. Monopoly model. The backs of the cards have their respective symbols, with Community Chest cards in blue, and Chance cards in orange.

Additionally, recent versions of "Monopoly" replace the dollar sign ($) with an M with two horizontal strokes through it.

In the U.S. versions shown below, the properties are named after locations in (or near) Atlantic City, New Jersey.
Atlantic City's Illinois Avenue was renamed Martin Luther King Jr. Blvd. in the 1980s. St. Charles Place no longer exists, as the Showboat Atlantic City was developed where it once ran.

Different versions have been created based on various current consumer interests such as: "Dog-opoly", "Cato-poly", "Bug-opoly", and TV/movie games among others.

It was not until 1995 that Parker Brothers acknowledged the misspelling of "Marvin Gardens", formally apologizing to the residents of Marven Gardens.

Short Line refers to the Shore Fast Line, a streetcar line that served Atlantic City. The B&O Railroad did not serve Atlantic City. A booklet included with the reprinted 1935 edition states that the four railroads that served Atlantic City in the mid-1930s were the Jersey Central, the Seashore Lines, the Reading Railroad, and the Pennsylvania Railroad.

The Baltimore & Ohio (now part of CSX) was the parent of the Reading. There is a tunnel in Philadelphia where track to the south was B. & O. and track to the north is Reading. The Central of N.J. did not have a track to Atlantic City but was the daughter of the Reading (and granddaughter of the B. & O.) Their track ran from the New York City area to Delaware Bay and some trains ran on the Reading-controlled track to Atlantic City.

The actual "Electric Company" and "Water Works" serving the city are respectively Atlantic City Electric Company (a subsidiary of Exelon) and the Atlantic City Municipal Utilities Authority.

In the 1930s, John Waddington Ltd. (Waddingtons) was a printing company in Leeds that had begun to branch out into packaging and the production of playing cards. Waddingtons had sent the card game "Lexicon" to Parker Brothers hoping to interest them in publishing the game in the United States. In a similar fashion, Parker Brothers sent over a copy of "Monopoly" to Waddingtons early in 1935 before the game had been put into production in the United States.

Victor Watson, the managing director of Waddingtons, gave the game to his son Norman, head of the card games division, to test over the weekend. Norman was impressed by the game and persuaded his father to call Parker Brothers on Monday morning – transatlantic calls then being almost unheard of. This call resulted in Waddingtons obtaining a license to produce and market the game outside the United States.

Watson felt that for the game to be a success in the United Kingdom, the American locations would have to be replaced, so Victor and his secretary, Marjory Phillips, went to London to scout out locations. The Angel, Islington is not a street in London but a building (and the name of the road intersection where it is located). It had been a coaching inn that stood on the Great North Road. By the 1930s, the inn had become a J. Lyons and Co. tea room (today The Co-operative Bank). Some accounts say that Marjory and Victor met at the Angel to discuss the selection and celebrated the fact by including it on the "Monopoly" board. In 2003, a plaque commemorating the naming was unveiled at the site by Victor Watson's grandson, who is also named Victor.

During World War II, the British Secret Service contacted Waddington (who could also print on silk) to make "Monopoly" sets that included escape maps, money, a compass and file, all hidden in copies of the game sent by fake POW relief charities to prisoners of war.

The standard British board, produced by Waddingtons, was for many years the version most familiar to people in countries in the Commonwealth (except Canada, where the U.S. edition with Atlantic City-area names was reprinted), although local variants of the board are now also found in several of these countries.

In 1998, Winning Moves procured the "Monopoly" license from Hasbro and created new UK city and regional editions with sponsored squares. Initially, in December 1998, the game was sold in just a few W H Smith stores, but demand was high, with almost fifty thousand games shipped in the four weeks leading to Christmas. Winning Moves still produces new annually.

The original income tax choice from the 1930s U.S. board is replaced by a flat rate on the UK board, and the $75 Luxury Tax space is replaced with the £100 Super Tax space, the same as the current German board. In 2008, the U.S. Edition was changed to match the UK and various European editions, including a flat $200 Income Tax value and an increased $100 Luxury Tax amount.

In cases where a national company produced the game, the $ (dollar) sign was replaced with the £ (pound), but the place names were unchanged.

Beginning in the U.K. in 2005, a revised version of the game, titled "Monopoly Here and Now", was produced, replacing game scenarios, properties, and tokens with newer equivalents. Similar boards were produced for Germany and France. Variants of these first editions appeared with Visa-branded debit cards taking the place of cash – the later U.S. "Electronic Banking" edition has unbranded debit cards.

The success of the first "Here and Now" editions prompted Hasbro U.S. to allow online voting for twenty-six landmark properties across the United States to take their places along the game-board. The popularity of this voting, in turn, led to the creation of similar websites, and secondary game-boards per popular vote to be created in the U.K., Canada, France, Germany, Australia, New Zealand, Ireland, and other nations.

In 2006, Winning Moves Games released the "", with a 30% larger game-board and revised game play. Other streets from Atlantic City (eight, one per color group) were included, along with a third "utility", the Gas Company. In addition, $1,000 denomination notes (first seen in Winning Moves' "Monopoly: The Card Game") are included. Game play is further changed with bus tickets (allowing non-dice-roll movement along one side of the board), a speed die (itself adopted into variants of the "Atlantic City standard edition"; see below), skyscrapers (after houses and hotels), and train depots that can be placed on the Railroad spaces.

This edition was adapted for the U.K. market in 2007, and is sold by Winning Moves U.K. After the initial U.S. release, critiques of some of the rules caused the company to issue revisions and clarifications on their website.

In September 2006, the U.S. edition of "Monopoly Here and Now" was released. This edition features top landmarks across the U.S. The properties were decided by votes over the Internet in the spring of 2006.

Monetary values are multiplied by 10,000 (e.g., one collects $2,000,000 instead of $200 for passing GO and pays that much for Income Tax (or 10% of their total, as this edition was launched prior to 2008), each player starts with $15,000,000 instead of $1,500, etc.). Also, the Chance and Community Chest cards are updated, the Railroads are replaced by Airports (Chicago O'Hare, Los Angeles International, New York City's JFK, and Atlanta's Hartsfield-Jackson), and the Utilities (Electric Company and Water Works) are replaced by Service Providers (Internet Service Provider and Cell Phone Service Provider). The houses and hotels are blue and silver, not green and red as in most editions of "Monopoly". The board uses the traditional U.S. layout; the cheapest properties are purple, not brown, and "Interest on Credit Card Debt" replaces "Luxury Tax".

Despite the updated Luxury Tax space, and the Income Tax space no longer using the 10% option, this edition uses paper "Monopoly" money, and not an electronic banking unit like the "Here and Now World Edition". However, a similar edition of "Monopoly", the "Electronic Banking" edition, does feature an electronic banking unit and bank cards, as well as a different set of tokens. Both "Here and Now" and "Electronic Banking" feature an updated set of tokens from the Atlantic City edition.

It is also notable that three states (California, Florida, and Texas) are represented by two cities each (Los Angeles and San Francisco, Miami and Orlando, and Dallas and Houston). No other state is represented by more than one city (not including the airports). One landmark, Texas Stadium, has been demolished and no longer exists. Another landmark, Jacobs Field, still exists, but was renamed Progressive Field in 2008.

In 2015, in honor of the game's 80th birthday, Hasbro held an online vote to determine which cities would make it into an updated version of the Here and Now edition of the game. This second edition is more a spin-off as the winning condition has changed to completing your passport instead of bankrupting your opponents. Community Chest is replaced with Here and Now cards while the Here and Now space replaced the railroads. Houses and hotels have been removed.

Hasbro released a World edition with the top voted cities from all around the world, as well as at least a Here & Now edition with the voted-on U.S. cities.

"Monopoly Empire" has uniquely branded tokens and places based on popular brands. Instead of buying properties, players buy popular brands one by one and slide their billboards onto their Empire towers. Instead of building houses and hotels, players collect rent from their rivals based on their tower height. How a player wins is by being the first player to fill his or her tower with billboards. Every space on the board is a brand name, including Xbox, Coca-Cola, McDonald's and Samsung.

Monopoly Token Madness

This version of Monopoly contains an extra eight "golden" tokens. That includes a penguin, a television, a race car, a Mr. Monopoly emoji, a rubber duck, a watch, a wheel and a bunny slipper.

Monopoly Jackpot

During the game, players travel around the gameboard buying properties and collecting rent. If they land on a Chance space, or roll the Chance icon on a die, they can spin the Chance spinner to try to make more money. Players may hit the "Jackpot", go bankrupt, or be sent to Jail. The player who has the most cash when the bank crashes wins.

Monopoly: Ultimate Banking Edition

In this version, there is no cash. The Monopoly Ultimate Banking game features an electronic ultimate banking piece with touch technology. Players can buy properties instantly and set rents by tapping. Each player has a bankcard and their cash is tracked by the Ultimate Banking unit. It can scan the game's property cards and boost or crash the market. Event cards and Location spaces replace Chance and Community Chest cards. On an Event Space, rents may be raised or lowered, a player may earn or lose money, or someone could be sent to Jail. Location Spaces allow players to pay and move to any property space on the gameboard.

Monopoly Voice Banking

In this version, there's no cash or cards. The Voice Banking game allows the player to respond with your voice with the Top Hat. The hat responds by purchasing properties, paying rent, and making buildings.

"Ms. Monopoly" is a version of the game released in 2019, in which female players earn more than male players.

"Monopoly Deal" is a card game derived from the board-game Monopoly introduced in 2008, produced and sold by Cartamundi under a license from Hasbro. Players attempt to collect three complete sets of cards representing the properties from the original board game, either by playing them directly, stealing them from other players, swapping cards with other players, or collecting them as rent for other properties they already own. The cards in the 110-card deck represent properties and wild cards, various denominations of Monopoly money used to pay rent, and special action cards which can either be played for their effects or banked as money instead.

All property deeds, houses, and hotels are held by the bank until bought by the players. A standard set of "Monopoly" pieces includes:

A deck of thirty-two Chance and Community Chest cards (sixteen each) which players draw when they land on the corresponding squares of the track, and follow the instructions printed on them.

A title deed for each property is given to a player to signify ownership, and specifies purchase price, mortgage value, the cost of building houses and hotels on that property, and the various rents depending on how developed the property is. Properties include:

The purchase price for properties varies from $60 to $400 on a U.S. Standard Edition set.

A pair of six-sided dice is included, with a "Speed Die" added for variation in 2007. The 1999 Millennium Edition featured two jewel-like dice which were the subject of a lawsuit from Michael Bowling, owner of dice maker Crystal Caste. Hasbro lost the suit in 2008 and had to pay $446,182 in royalties. Subsequent printings of the game reverted to normal six-sided dice.

32 houses and 12 hotels made of wood or plastic (the original and current "Deluxe Edition" have wooden houses and hotels; the current "base set" uses plastic buildings). Unlike money, houses and hotels have a finite supply. If no more are available, no substitute is allowed. In most editions, houses are green and hotels red.

Older U.S. standard editions of the game included a total of $15,140 in the following denominations:

Newer (September 2008 and later) U.S. editions provide a total of $20,580–30 of each denomination instead. The colors of some of the bills are also changed: $10s are now blue instead of yellow, $20s are a brighter green than before, and $50s are now purple instead of blue.

Each player begins the game with his or her token on the Go square, and $1,500 (or 1,500 of a localized currency) in play money ($2,500 with the Speed Die). Before September 2008, the money was divided with greater numbers of 20 and 10-dollar bills. Since then, the U.S. version has taken on the British version's initial cash distributions.

Although the U.S. version is indicated as allowing eight players, the cash distribution shown above is not possible with all eight players since it requires 32 $100 bills and 40 $1 bills. However, the amount of cash contained in the game is enough for eight players with a slight alteration of bill distribution.

Pre-Euro German editions of the game started with 30,000 "Spielmark" in eight denominations (abbreviated as "M."), and later used seven denominations of the "Deutsche Mark" ("DM."). In the classic Italian game, each player received L. 350,000 ($3500) in a two-player game, but L. 50,000 ($500) less for each player more than two. Only in a six-player game does a player receive the equivalent of $1,500. The classic Italian games were played with only four denominations of currency. Both Spanish editions (the Barcelona and Madrid editions) started the game with 150,000 in play money, with a breakdown identical to that of the American version.

According to the Parker Brothers rules, Monopoly money is theoretically unlimited; if the bank runs out of money it may issue as much as needed "by merely writing on any ordinary paper".
However, Hasbro's published Monopoly rules make no mention of this. Additional paper money can be bought at certain locations, notably game and hobby stores, or downloaded from various websites and printed and cut by hand. One such site has created a $1,000 bill; while a $1,000 bill can be found in "" and "Monopoly: The Card Game", both published by Winning Moves Games, this note is not a standard denomination for "classic" versions of Monopoly.

In several countries there is also a version of the game that features electronic banking. Instead of receiving paper money, each player receives a plastic bank card that is inserted into a calculator-like electronic device that keeps track of the player's balance.

Lizzie Magie originally created this game for children to learn how to add and subtract through the usage of paper money. However, now with the new innovations of credit cards implemented in these games, many consumers are worried that the purpose of the game is ruined.

Each player is represented by a small metal or plastic token that is moved around the edge of the board according to the roll of two six-sided dice. The number of tokens (and the tokens themselves) have changed over the history of the game with many appearing in special editions only, and some available with non-game purchases. After prints with wood tokens in 1937, a set of eight tokens was introduced. Two more were added in late 1937, and tokens changed again in 1942. During World War II, the game tokens were switched back to wood. Early localized editions of the standard edition (including some Canadian editions, which used the U.S. board layout) did not include pewter tokens but instead had generic wooden pawns identical to those that "Sorry!" had.

Many of the early tokens were created by companies such as Dowst Miniature Toy Company, which made metal charms and tokens designed to be used on charm bracelets. The battleship and cannon were also used briefly in the Parker Brothers war game "Conflict" (released in 1940), but after the game failed on the market, the premade pieces were recycled for "Monopoly" usage. By 1943, there were ten tokens which included the Battleship, Boot, Cannon, Horse and rider, Iron, Racecar, Scottie Dog, Thimble, Top hat, and Wheelbarrow. These tokens remained the same until the late 1990s, when Parker Brothers was sold to Hasbro.

In 1998, a Hasbro advertising campaign asked the public to vote on a new playing piece to be added to the set. The candidates were a "bag of money", a bi-plane, and a piggy bank. The bag ended up winning 51 percent of the vote compared to the other two which failed to go above 30%. This new token was added to the set in 1999 bringing the number of tokens to eleven. Another 1998 campaign poll asked people which monopoly token was their favorite. The most popular was the Race Car at 18% followed by the Dog (16%), Cannon (14%) and Top Hat (10%). The least favorite in the poll was the Wheelbarrow at 3% followed by Thimble (7%) and the Iron (7%). The "Cannon", and "Horse and rider" were both retired in 2000 with no new tokens taking their place. Another retirement came in 2007 with the sack of money that brought down the total token count to eight again.

In 2013, a similar promotional campaign was launched encouraging the public to vote on one of several possible new tokens to replace an existing one. The choices were a guitar, a diamond ring, a helicopter, a robot, and a cat. This new campaign was different than the one in 1998 as one piece was retired and replaced with a new one. Both were chosen by a vote that ran on Facebook from January 8 to February 5, 2013. The cat took the top spot with 31% of the vote over the iron which was replaced. In January 2017, Hasbro placed the line of tokens in the regular edition with another vote which included a total of 64 options. The eight playable tokens at the time included the Battleship, Boot, Cat, Racecar, Scottie Dog, Thimble, Top hat, and Wheelbarrow. By March 17, 2017, Hasbro retired three additional tokens, namely the thimble, wheelbarrow, and boot; these were replaced by a penguin, a Tyrannosaurus and a rubber duck.

Over the years Hasbro has released tokens for special or collector's editions of the game. One of the first tokens to come out included a Steam Locomotive which was only released in Deluxe Editions. A Director's Chair token was released in 2011 in limited edition copies of "". Shortly after the 2013 Facebook voting campaign, a limited-edition Golden Token set was released exclusively at various national retailers, such as Target in the U.S., and Tesco in the U.K.

The set contained the Battleship, Boot, Iron, Racecar, Scottie Dog, Thimble, Top hat and Wheelbarrow as well as the iron's potential replacements. These replacement tokens included the cat, the guitar, the diamond ring, the helicopter, and the robot. Hasbro released a 64-token limited edition set in 2017 called Monopoly Signature Token Collection to include all of the candidates that were not chosen in the vote held that year.

Players take turns in order with the initial player determined by chance before the game. A typical turn begins with the rolling of the dice and advancing a piece clockwise around the board the corresponding number of squares. If a player rolls doubles, they roll again after completing that portion of their turn. A player who rolls three consecutive sets of doubles on one turn has been "caught speeding" and is immediately sent to jail instead of moving the amount shown on the dice for the third roll.

A player who lands on or passes the Go space collects $200 from the bank. Players who land on either Income Tax or Luxury Tax pay the indicated amount to the bank. In older editions of the game, two options were given for Income Tax: either pay a flat fee of $200 or 10% of total net worth (including the current values of all the properties and buildings owned). No calculation could be made before the choice, and no latitude was given for reversing an unwise calculation. In 2008, the calculation option was removed from the official rules, and simultaneously the Luxury Tax was increased to $100 from its original $75. No reward or penalty is given for landing on Free Parking.

Properties can only be developed once a player owns all the properties in that color group. They then must be developed equally. A house must be built on each property of that color before a second can be built. Each property within a group must be within one house level of all the others within that group.

If a player lands on a Chance or Community Chest space, they draw the top card from the respective deck and follow its instructions. This may include collecting or paying money to the bank or another player or moving to a different space on the board. Two types of cards that involve jail, "Go to Jail" and "Get Out of Jail Free", are explained below.

A player is sent to jail for doing any of the following:

When a player is sent to jail, they move directly to the Jail space and their turn ends ("Do not pass Go. Do not collect $200."). If an ordinary dice roll (not one of the above events) ends with the player's token on the Jail corner, they are "Just Visiting" and can move ahead on their next turn without incurring any penalty.

If a player is in jail, they do not take a normal turn and must either pay a fine of $50 to be released, use a Chance or Community Chest Get Out of Jail Free card, or attempt to roll doubles on the dice. If a player fails to roll doubles, they lose their turn. Failing to roll doubles for three consecutive turns requires the player to either pay the $50 fine or use a Get Out of Jail Free card, after which they move ahead according to the total rolled. Players in jail may not buy properties directly from the bank since they are unable to move. They can engage all other transactions, such as mortgaging properties, selling/trading properties to other players, buying/selling houses and hotels, collecting rent, and bidding on property auctions. A player who rolls doubles to leave jail does not roll again; however, if the player pays the fine or uses a card to get out and then rolls doubles, they do take another turn.

If the player lands on an unowned property, whether street, railroad, or utility, they can buy the property for its listed purchase price. If they decline this purchase, the property is auctioned off by the bank to the highest bidder, including the player who declined to buy. If the property landed on is already owned and unmortgaged, they must pay the owner a given rent; the amount depends on whether the property is part of a set or its level of development.
When a player owns all the properties in a color group and none of them are mortgaged, they may develop them during their turn or in between other player's turns. Development involves buying miniature houses or hotels from the bank and placing them on the property spaces; this must be done uniformly across the group. That is, a second house cannot be built on any property within a group until all of them have one house. Once the player owns an entire group, they can collect double rent for any undeveloped properties within it. Although houses and hotels cannot be built on railroads or utilities, the given rent increases if a player owns more than one of either type. If there is a housing shortage (more demand for houses to be built than what remains in the bank), then a housing auction is conducted to determine who will get to purchase each house.

Properties can also be mortgaged, although all developments on a monopoly must be sold before any property of that color can be mortgaged or traded. The player receives half the purchase price from the bank for each mortgaged property. This must be repaid with 10% interest to clear the mortgage. Houses and hotels can be sold back to the bank for half their purchase price. Players cannot collect rent on mortgaged properties and may not give improved property away to others; however, trading mortgaged properties is allowed. The player receiving the mortgaged property must immediately pay the bank the mortgage price plus 10% or pay just the 10% amount and keep the property mortgaged; if the player chooses the latter, they must pay the 10% again when they pay off the mortgage.

A player who cannot pay what they owe is bankrupt and eliminated from the game. If the bankrupt player owes the bank, they must turn all their assets over to the bank, who then auctions off their properties (if they have any), except buildings. If the debt is owed to another player instead, all assets are given to that opponent, except buildings which must be returned to the bank. The new owner must either pay off any mortgages held by the bank on such properties received or pay a fee of 10% of the mortgaged value to the bank if they choose to leave the properties mortgaged. The winner is the remaining player left after all of the others have gone bankrupt.

If a player runs out of money but still has assets that can be converted to cash, they can do so by selling buildings, mortgaging properties, or trading with other players. To avoid bankruptcy the player must be able to raise enough cash to pay the full amount owed.

A player cannot choose to go bankrupt; if there is any way to pay what they owe, even by returning all their buildings at a loss, mortgaging all their real estate and giving up all their cash, even knowing they are likely going bankrupt the next time, they must do so.

From 1936, the rules booklet included with each Monopoly set contained a short section at the end providing rules for making the game shorter, including dealing out two Title Deed cards to each player before starting the game, by setting a time limit or by ending the game after the second player goes bankrupt. A later version of the rules included this variant, along with the time limit game, in the main rules booklet, omitting the last, the second bankruptcy method, as a third short game.

Many house rules have emerged for the game throughout its history. Well-known is the "Free Parking jackpot rule", where all the money collected from Income Tax, Luxury Tax, Chance and Community Chest goes to the center of the board instead of the bank. Many people add $500 to start each pile of Free Parking money, guaranteeing a minimum payout. When a player lands on Free Parking, they may take the money. Another rule is that if a player lands directly on Go, they collect double the amount, or $400, instead of $200. Since these rules provide additional cash to players regardless of their property management choices, they can lengthen the game considerably and limit the role of strategy.

Video game and computer game versions of "Monopoly" have options where popular house rules can be used. In 2014, Hasbro determined five popular house rules by public Facebook vote, and released a "House Rules Edition" of the board game. Rules selected include a "Free Parking" house rule without additional money and forcing players to traverse the board once before buying properties.

According to Jim Slater in "The Mayfair Set", the Orange property group is the best to own because players land on them more often, as a result of the Chance cards "Go to Jail", "Advance to St. Charles Place (Pall Mall)", "Advance to Reading Railroad (Kings Cross Station)" and "Go Back Three Spaces".

In all, during game play, Illinois Avenue (Trafalgar Square) (Red), New York Avenue (Vine Street) (Orange), B&O Railroad (Fenchurch Street Station), and Reading Railroad (Kings Cross Station) are the most frequently landed-upon properties. Mediterranean Avenue (Old Kent Road) (brown), Baltic Avenue (Whitechapel Road) (brown), Park Place (Park Lane) (blue), and Oriental Avenue (The Angel, Islington) (light blue) are the least-landed-upon properties. Among the property groups, the Railroads are most frequently landed upon, as no other group has four properties; Orange has the next highest frequency, followed by Red.

According to "Business Insider", the best way to get the most out of every property is through houses and hotels. In order to do so, the player must have all the corresponding properties of the color set. Three houses allows the player to make all the money they spent on the houses back and earn even more as players land on those properties.

Trading is a vital strategy in order to accumulate all the properties in a color set. Obtaining all the properties in a specific color set enables the player to buy houses and hotels which increase the rent another player has to pay when they land on the property. According to "Slate", players trade to speed up the process and secure a win. Building at least 3 houses on each property allows the player to break even once at least one player lands on this property.

One common criticism of "Monopoly" is that although it has carefully defined termination conditions, it may take an unlimited amount of time to reach them. Edward P. Parker, a former president of Parker Brothers, is quoted as saying, "We always felt that forty-five minutes was about the right length for a game, but "Monopoly" could go on for hours. Also, a game was supposed to have a definite end somewhere. In "Monopoly" you kept going around and around."

Hasbro states that the longest game of "Monopoly" ever played lasted 70 days.

Numerous add-ons have been produced for "Monopoly", sold independently from the game both before its commercialization and after, with three official ones discussed below:

The original "Stock Exchange" add-on was published by Capitol Novelty Co. of Rensselaer, New York in early 1936. It was marketed as an add-on for "Monopoly", "Finance", or "Easy Money" games. Shortly after Capitol Novelty introduced "Stock Exchange", Parker Brothers bought it from them then marketed their own, slightly redesigned, version as an add-on specifically for their "new" "Monopoly" game; the Parker Brothers version was available in June 1936. The Free Parking square is covered over by a new Stock Exchange space and the add-on included three Chance and three Community Chest cards directing the player to "Advance to Stock Exchange".

The "Stock Exchange" add-on was later redesigned and re-released in 1992 under license by Chessex, this time including a larger number of new Chance and Community Chest cards. This version included ten new Chance cards (five "Advance to Stock Exchange" and five other related cards) and eleven new Community Chest cards (five "Advance to Stock Exchange" and six other related cards; the regular Community Chest card "From sale of stock you get $45" is removed from play when using these cards). Many of the original rules applied to this new version (in fact, one optional play choice allows for playing in the original form by only adding the "Advance to Stock Exchange" cards to each deck).

A "Monopoly Stock Exchange Edition" was released in 2001 (although not in the U.S.), this time adding an electronic calculator-like device to keep track of the complex stock figures. This was a full edition, not just an add-on, that came with its own board, money and playing pieces. Properties on the board were replaced by companies on which shares could be floated, and offices and home offices (instead of houses and hotels) could be built.

Playmaster, another official add-on, released in 1982, is an electronic device that keeps track of all player movement and dice rolls as well as what properties are still available. It then uses this information to call random auctions and mortgages making it easier to free up cards of a color group. It also plays eight short tunes when key game functions occur; for example when a player lands on a railroad it plays "I've Been Working on the Railroad", and a police car's siren sounds when a player goes to Jail.

In 2009, Hasbro released two minigames that can be played as stand-alone games or combined with the "Monopoly" game. In "Get Out of Jail", the goal is to manipulate a spade under a jail cell to flick out various colored prisoners. The game can be used as an alternative to rolling doubles to get out of jail. In "Free Parking", players attempt to balance taxis on a wobbly board. The "Free Parking" add-on can also be used with the Monopoly game. When a player lands on the Free Parking, the player can take the Taxi Challenge, and if successful, can move to any space on the board.

First included in Winning Moves' "Monopoly: The Mega Edition" variant, this third, six-sided die is rolled with the other two, and accelerates game-play when in use. In 2007, Parker Brothers began releasing its standard version (also called the Speed Die Edition) of "Monopoly" with the same die (originally in blue, later in red). Its faces are: 1, 2, 3, two "Mr. Monopoly" sides, and a bus. The numbers behave as normal, adding to the other two dice, unless a "triple" is rolled, in which case the player can move to any space on the board. If "Mr. Monopoly" is rolled while there are unowned properties, the player advances forward to the nearest one. Otherwise, the player advances to the nearest property on which rent is owed. In the "Monopoly: Mega Edition", rolling the bus allows the player to take the regular dice move, then either take a bus ticket or move to the nearest draw card space.

Mega rules specifies that triples do not count as doubles for going to jail as the player does not roll again. Used in a regular edition, the bus (properly "get off the bus") allows the player to use only one of the two numbered dice or the sum of both, thus a roll of 1, 5, and bus would let the player choose between moving 1, 5, or 6 spaces. The Speed Die is used throughout the game in the "Mega Edition", while in the "Regular Edition" it is used by any player who has passed GO at least once. In these editions it remains optional, although use of the Speed Die was made mandatory for use in the 2009 U.S. and World Monopoly Championship, as well as the 2015 World Championship.

Parker Brothers and its licensees have also sold several spin-offs of "Monopoly". These are not add-ons, as they do not function as an addition to the "Monopoly" game, but are simply additional games with the flavor of "Monopoly":

Besides the many variants of the actual game (and the "Monopoly Junior" spin-off) released in either video game or computer game formats (e.g., Commodore 64, Macintosh, Windows-based PC, Game Boy, Game Boy Advance, Nintendo Entertainment System, iPad, Genesis, Super NES, etc.), two spin-off computer games have been created. An electronic hand-held version was marketed from 1997 to 2001.

"Monopoly"-themed slot machines and lotteries have been produced by WMS Gaming in conjunction with International Game Technology for land-based casinos.WagerWorks, who have the online rights to "Monopoly", have created online "Monopoly" themed games.

London's Gamesys Group have also developed "Monopoly"-themed gambling games. The British quiz machine brand itbox also supports a "Monopoly" trivia and chance game.

There was also a live, online version of "Monopoly". Six painted taxis drive around London picking up passengers. When the taxis reach their final destination, the region of London that they are in is displayed on the online board. This version takes far longer to play than board-game "Monopoly", with one game lasting 24 hours. Results and position are sent to players via e-mail at the conclusion of the game.

The "McDonald's Monopoly" game is a sweepstakes advertising promotion of McDonald's and Hasbro that has been offered in Argentina, Australia, Austria, Brazil, Canada, France, Germany, Hong Kong, Ireland, the Netherlands, New Zealand, Poland, Portugal, Romania, Russia, Singapore, South Africa, Spain, Switzerland, Taiwan, United Kingdom and United States.

A short-lived "Monopoly" game show aired on Saturday evenings from June 16 to September 1, 1990, on ABC. The show was produced by Merv Griffin and hosted by Mike Reilly. The show was paired with a summer-long "Super Jeopardy!" tournament, which also aired during this period on ABC.

From 2010 to 2014, The Hub aired the game show "Family Game Night" with Todd Newton. For the first two seasons, teams earned cash in the form of "Monopoly Crazy Cash Cards" from the "Monopoly Crazy Cash Corner", which was then inserted to the "Monopoly Crazy Cash Machine" at the end of the show. In addition, beginning with Season 2, teams won "Monopoly Party Packages" for winning the individual games. For Season 3, there was a Community Chest. Each card on Mr. Monopoly had a combination of three colors. Teams used the combination card to unlock the chest. If it was the right combination, they advanced to the Crazy Cash Machine for a brand-new car. For the show's fourth season, a new game was added called Monopoly Remix, featuring Park Place and Boardwalk, as well as Income Tax and Luxury Tax.

To honor the game's 80th anniversary, a game show in syndication on March 28, 2015, called "Monopoly Millionaires' Club" was launched. It was connected with a multi-state lottery game of the same name and hosted by comedian Billy Gardell from "Mike & Molly". The game show was filmed at the Rio All Suite Hotel and Casino and at Bally's Las Vegas in Las Vegas, with players having a chance to win up to $1,000,000. However, the lottery game connected with the game show (which provided the contestants) went through multiple complications and variations, and the game show last aired at the end of April 2016.

In November 2008, Ridley Scott was announced to direct Universal Pictures' film version of the game, based on a script written by Pamela Pettler. The film was being co-produced by Hasbro's Brian Goldner as part of a deal with Hasbro to develop movies based on the company's line of toys and games. The story was being developed by author Frank Beddor. However, Universal eventually halted development in February 2012 then opted out of the agreement and the rights reverted to Hasbro.

In October 2012, Hasbro announced a new partnership with production company Emmett/Furla Films, and said they would develop a live-action version of "Monopoly", along with Action Man and Hungry Hungry Hippos. Emmett/Furla/Oasis dropped out of the production of this satire version that was to be directed by Ridley Scott.

In July 2015, Hasbro announced that Lionsgate will distribute a "Monopoly" film with Andrew Niccol writing the film as a family-friendly action adventure film co-financed and produced by Lionsgate and Hasbro's Allspark Pictures.

In January 2019, it was announced that Allspark Pictures would now be producing an untitled "Monopoly" film in conjunction with Kevin Hart's company HartBeat Productions and The Story Company. Hart is attached to star in the film and Tim Story is attached to direct. No logline or writer for this iteration of the long-gestating project has been announced.

The documentary "", covering the history and players of the game, won an Audience Award for Best Documentary at the 2010 Anaheim International Film Festival. The film played theatrically in the U.S. beginning in March 2011 and was released on Amazon and iTunes on February 14, 2012. The television version of the film won four regional Emmy Awards from the Pacific Southwest Chapter of NATAS. The film is directed by Kevin Tostado and narrated by Zachary Levi.

Until 1999, U.S. entrants had to win a state/district/territory competition to represent that state/district/territory at the once every four year national championship. The 1999 U.S. National Tournament had 50 contestants - 49 State Champions (Oklahoma was not represented) and the reigning national champion.

Qualifying for the National Championship has been online since 2003. For the 2003 Championship, qualification was limited to the first fifty people who correctly completed an online quiz. Out of concerns that such methods of qualifying might not always ensure a competition of the best players, the 2009 Championship qualifying was expanded to include an online multiple-choice quiz (a score of 80% or better was required to advance); followed by an online five-question essay test; followed by a two-game online tournament at Pogo.com. The process was to have produced a field of 23 plus one: Matt McNally, the 2003 national champion, who received a bye and was not required to qualify. However, at the end of the online tournament, there was an eleven-way tie for the last six spots. The decision was made to invite all of those who had tied for said spots. In fact, two of those who had tied and would have otherwise been eliminated, Dale Crabtree of Indianapolis, Indiana, and Brandon Baker, of Tuscaloosa, Alabama, played in the final game and finished third and fourth respectively.

The 2009 "Monopoly" U.S. National Championship was held on April 14–15 in Washington, D.C. In his first tournament ever, Richard Marinaccio, an attorney from Sloan, New York (a suburb of Buffalo), prevailed over a field that included two previous champions to be crowned the 2009 U.S. National Champion. In addition to the title, Marinaccio took home $20,580—the amount of money in the bank of the board game—and competed in the 2009 World Championship in Las Vegas, Nevada, on October 21–22, where he finished in third place.

In 2015, Hasbro used a competition that was held solely online to determine who would be the U.S. representative to compete at the 2015 "Monopoly" World Championship. Interested players took a twenty-question quiz on "Monopoly" strategy and rules and submitted a hundred-word essay on how to win a "Monopoly" tournament. Hasbro then selected Brian Valentine of Washington, D.C., to be the U.S. representative.

Hasbro conducts a worldwide "Monopoly" tournament. The first "Monopoly" World Championships took place in Grossinger's Resort in New York, in November 1973, but they did not include competitors from outside the United States until 1975. It has been aired in the United States by ESPN. In 2009, forty-one players competed for the title of "Monopoly" World Champion and a cash prize of $20,580 (USD)—the total amount of Monopoly money in the current Monopoly set used in the tournament. The most recent World Championship took place September 2015 in Macau. Italian Nicolò Falcone defeated the defending world champion and players from twenty-six other countries. Monopoly Dreams at The Peak in Hong Kong has stated that it will be the site of the next world championship in March 2021.

Because "Monopoly" evolved in the public domain before its commercialization, "Monopoly" has seen many variant games. The game is licensed in 103 countries and printed in thirty-seven languages. Most of the variants are exact copies of the "Monopoly" games with the street names replaced with locales from a particular town, university, or fictional place. National boards have been released as well. Over the years, many specialty "Monopoly" editions, licensed by Parker Brothers/Hasbro, and produced by them, or their licensees (including USAopoly and Winning Moves Games) have been sold to local and national markets worldwide. Two well known "families" of -opoly like games, without licenses from Parker Brothers/Hasbro, have also been produced.

Several published games like "Monopoly" include:

Other unlicensed editions include: "BibleOpoly", "HomoNoPolis" and Petropolis, among others.

There have been a large number of localized editions, broken down here by region:

This list is of unauthorized, unlicensed games based on "Monopoly":
Ghettopoly
Middopoly
Memeopolis (Android app)

In 2008, Hasbro released "Monopoly Here and Now: The World Edition". This world edition features top locations of the world. The locations were decided by votes over the Internet. The result of the voting was announced on August 20, 2008.

Out of these, Gdynia is especially notable, as it is by far the smallest city of those featured and won the vote thanks to a spontaneous, large-scale mobilization of support started by its citizens. The new game uses its own currency unit, the Monopolonian (a game-based take on the Euro; designated by M). The game uses said unit in millions and thousands. As seen below, there is no dark purple color-group, as that is replaced by brown, as in the European version of the game.

It is also notable that three cities (Montreal, Toronto, and Vancouver) are from Canada and three other cities (Beijing, Hong Kong, and Shanghai) are from the People's Republic of China. No other countries are represented by more than one city.

Of the 68 cities listed on Hasbro Inc.'s website for the vote, Jerusalem was chosen as one of the 20 cities to be featured in the newest "Monopoly" World Edition. Before the vote took place, a Hasbro employee in the London office eliminated the country signifier "Israel" after the city, in response to pressure from pro-Palestinian advocacy groups. After the Israeli government protested, Hasbro Inc. issued a statement that read: "It was a bad decision, one that we rectified relatively quickly. This is a game. We never wanted to enter into any political debate. We apologize to our "Monopoly" fans."

A similar online vote was held in early 2015 for an updated version of the game. The resulting board should be released worldwide in late 2015. Lima, Peru won the vote and will hold the Boardwalk space.

Hasbro sells a "Deluxe Edition", which is mostly identical to the classic edition but has wooden houses and hotels and gold-toned tokens, including one token in addition to the standard eleven, a railroad locomotive. Other additions to the "Deluxe Edition" include a card carousel, which holds the title deed cards, and money printed with two colors of ink.

In 1978, retailer Neiman Marcus manufactured and sold an all-chocolate edition of "Monopoly" through its "Christmas Wish Book" for that year. The entire set was edible, including the money, dice, hotels, properties, tokens and playing board. The set retailed for $600.

In 2000, the FAO Schwarz store in New York City sold a custom version called "One-Of-A-Kind Monopoly" for $100,000. This special edition comes in a locking attaché case made with Napolino leather and lined in suede, and features include:

The "Guinness Book of World Records" states that a set worth $2,000,000 and made of 23-carat gold, with rubies and sapphires atop the chimneys of the houses and hotels, is the most expensive "Monopoly" set ever produced. This set was designed by artist Sidney Mobell to honor the game's 50th anniversary in 1985, and is now in the Smithsonian Institution.

"Wired" magazine believes "Monopoly" is a poorly designed game. Former Wall Streeter Derk Solko explains, "Monopoly has you grinding your opponents into dust. It's a very negative experience. It's all about cackling when your opponent lands on your space and you get to take all their money."

Most of the three to four-hour average playing time is spent waiting for other players to play their turn. "Board game enthusiasts disparagingly call this a 'roll your dice, move your mice' format."

The hobby-gaming community BoardGameGeek is especially critical. User reviews of Monopoly rank the game among the 20 worst games out of nearly 10,000 ranked in the database with an average rating of 4.36 out of 10 from over 25,000 reviews.


Notes
Bibliography 



</doc>
<doc id="19693" url="https://en.wikipedia.org/wiki?curid=19693" title="Max Steiner">
Max Steiner

Maximilian Raoul Steiner (May 10, 1888 – December 28, 1971) was an Austrian-born American music composer for theatre and films, as well as a conductor. He was a child prodigy who conducted his first operetta when he was twelve and became a full-time professional, either composing, arranging, or conducting, when he was fifteen.

Steiner worked in England, then Broadway, and in 1929, he moved to Hollywood, where he became one of the first composers to write music scores for films. He is referred to as "the father of film music", as Steiner played a major part in creating the tradition of writing music for films, along with composers Dimitri Tiomkin, Franz Waxman, Erich Wolfgang Korngold, Alfred Newman, Bernard Herrmann, and Miklós Rózsa.

Steiner composed over 300 film scores with RKO Pictures and Warner Bros., and was nominated for 24 Academy Awards, winning three: "The Informer" (1935); "Now, Voyager" (1942); and "Since You Went Away" (1944). Besides his Oscar-winning scores, some of Steiner's popular works include "King Kong" (1933), "Little Women" (1933), "Jezebel" (1938), and "Casablanca" (1942), though he did not compose its love theme, "As Time Goes By". In addition, Steiner scored "The Searchers" (1956), "A Summer Place" (1959), and "Gone with the Wind" (1939), which ranked second on the AFI's list of best American film scores, and is the film score for which he is best known.

He was also the first recipient of the Golden Globe Award for Best Original Score, which he won for his score for "Life with Father". Steiner was a frequent collaborator with some of the best known film directors in history, including Michael Curtiz, John Ford, and William Wyler, and scored many of the films with Errol Flynn, Bette Davis, Humphrey Bogart, and Fred Astaire. Many of his film scores are available as separate soundtrack recordings.

Max Steiner was born on May 10, 1888, in Austria-Hungary, as the only child in a wealthy business and theatrical family of Jewish heritage. He was named after his paternal grandfather, Maximilian Steiner (1839–1880), who was credited with first persuading Johann Strauss II to write for the theater, and was the influential manager of Vienna's historic Theater an der Wien. His parents were Marie Josefine/Mirjam (Hasiba) and Hungarian-Jewish (1858–1944, born in Temesvár, Kingdom of Hungary, Austrian Empire), a Viennese impresario, carnival exposition manager, and inventor, responsible for building the Wiener Riesenrad. His father encouraged Steiner's musical talent, and allowed him to conduct an American operetta at the age of twelve, "The Belle of New York", which allowed Steiner to gain early recognition by the operetta's author, Gustave Kerker. Steiner's mother Marie was a dancer in stage productions put on by his grandfather when she was young, but later became involved in the restaurant business. His godfather was the composer Richard Strauss who strongly influenced Steiner's future work. Steiner often credited his family for inspiring his early musical abilities. As early as six years old, Steiner was taking three or four piano lessons a week, yet often became bored of the lessons. Because of this, he would practice improvising on his own, his father encouraging him to write his music down. Steiner cited his early improvisation as an influence of his taste in music, particularly his interest in the music of Claude Debussy which was "avant garde" for the time. In his youth, he began his composing career through his work on marches for regimental bands and hit songs for a show put on by his father.

His parents sent Steiner to the Vienna University of Technology, but he expressed little interest in scholastic subjects. He enrolled in the Imperial Academy of Music in 1904, where, due to his precocious musical talents and private tutoring by Robert Fuchs, and Gustav Mahler, he completed a four-year course in only one year, winning himself a gold medal from the academy at the age of fifteen. He studied various instruments including piano, organ, violin, double bass, and trumpet. His preferred and best instrument was the piano, but he acknowledged the importance of being familiar with what the other instruments could do. He also had courses in harmony, counterpoint, and composition. Along with Mahler and Fuchs, he cited his teachers as Felix Weingartner and Edmund Eysler.

The music of Edmund Eysler was an early influence in the pieces of Max Steiner; however, one of his first introductions to operettas was by Franz Lehár who worked for a time as a military bandmaster for Steiner's father's theatre. Steiner paid tribute to Lehár through an operetta modeled after Lehár's "Die lustige Witwe" which Steiner staged in 1907 in Vienna. Eysler was well-known for his operettas though as critiqued by Richard Traubner, the libretti were poor, with a fairly simple style, the music often relying too heavily on the Viennese waltz style. As a result, when Steiner started writing pieces for the theater, he was interested in writing libretto as his teacher had, but had minimal success. However, many of his future film scores such as "Dark Victory" (1939), "In This Our Life" (1941), and "Now, Voyager" (1942) had frequent waltz melodies as influenced by Eysler. According to author of "Max Steiner's "Now, Voyager"" Kate Daubney, Steiner may also have been influenced by Felix Weingartner who conducted the Vienna Opera from 1908 to 1911. Although he took composition classes from Weingartner, as a young boy, Steiner always wanted to be a great conductor.

Between 1907 and 1914, Steiner traveled between Britain and Europe to work on theatrical productions. Steiner first entered the world of professional music when he was fifteen. He wrote and conducted the operetta "The Beautiful Greek Girl", but his father refused to stage it saying it was not good enough. Steiner took the composition to competing impresario Carl Tuschl who offered to produce it. Much to Steiner's pleasure, it ran in the Orpheum Theatre for a year. This led to opportunities to conduct other shows in various cities around the world, including Moscow and Hamburg. Upon returning to Vienna, Steiner found his father in bankruptcy. Having difficulties finding work, he moved to London (in part to follow an English showgirl he had met in Vienna). In London, he was invited to conduct Lehar's "The Merry Widow". He stayed in London for eight years conducting musicals at Daly's Theatre, the Adelphi, the Hippodrome, the London Pavilion, and the Blackpool Winter Gardens. Steiner married Beatrice Stilt on September 12, 1912. The exact date of their divorce is unknown.

In England, Steiner wrote and conducted theater productions and symphonies. But the beginning of World War I in 1914 led him to be interned as an enemy alien. Fortunately, he was befriended by the Duke of Westminster, who was a fan of his work, and was given exit papers to go to America, although his money was impounded. He arrived in New York City in December 1914, with only $32. Unable to find work, he resorted to menial jobs such as a copyist for Harms Music Publishing which quickly led him to jobs orchestrating stage musicals.

In New York, Max Steiner quickly acquired employment and worked for fifteen years as a musical director, arranger, orchestrator, and conductor of Broadway productions. These productions include operettas and musicals written by Victor Herbert, Jerome Kern, Vincent Youmans, and George Gershwin, among others. Steiner's credits include: "George White's Scandals" (1922) (director), "Peaches" (1923) (composer), and "Lady, Be Good" (1924) (conductor and orchestrator). At twenty-seven years old, Steiner became Fox Film's musical director in 1915. At the time, there was no specially written music for films and Steiner told studio founder William Fox his idea to write an original score for "The Bondman" (1916). Fox agreed and they put together a 110-piece orchestra to accompany the screenings. During his time working on Broadway, he married Audree van Lieu on April 27, 1927. They divorced on December 14, 1933. In 1927, Steiner orchestrated and conducted Harry Tierney's "Rio Rita". Tierney himself later requested RKO Pictures in Hollywood hire Steiner to work in their music production departments. William LeBaron, RKO's head of production, traveled to New York to watch Steiner conduct and was impressed by Steiner and his musicians, who each played several instruments. Eventually, Steiner became a Hollywood asset. Steiner's final production on Broadway was "Sons O' Guns" in 1929.

By request of Harry Tierney, RKO hired Max Steiner as an orchestrator and his first film job consisted of composing music for the main and end titles and occasional "on screen" music. According to Steiner, the general opinion of filmmakers during the time was that film music was a "necessary evil," and would often slow down production and release of the film after it was filmed. Steiner's first job was for the film "Dixiana"; however, after a while, RKO decided to let him go, feeling they were not using him. His agent found him a job as a musical director on an operetta in Atlantic City. Before he left RKO, they offered him a month to month contract as the head of the music department with promise of more work in the future and he agreed. Because the few composers in Hollywood were unavailable, Steiner composed his first film score for "Cimarron". The score was well received and was partially credited for the success of the film. He turned down several offers to teach film scoring technique in Moscow and Peking in order to stay in Hollywood. In 1932, Steiner was asked by David O. Selznick, the new producer at RKO, to add music to "Symphony of Six Million". Steiner composed a short segment; Selznick liked it so much that he asked him to compose the theme and underscoring for the entire picture. Selznick was proud of the film, feeling it gave a realistic view of Jewish family life and tradition. "Music until then had not been used very much for underscoring". Steiner "pioneered the use of original composition as background scoring for films". The successful scoring in "Symphony of Six Million" was a turning point for Steiner's career and for the film industry. After the underscoring of "Symphony of Six Million", a third to half of the success of most films was "attributed to the extensive use of music."

The score for "King Kong" (1933) became Steiner's breakthrough and represented a paradigm shift in the scoring of fantasy and adventure films. The score was an integral part of the film, because it added realism to an unrealistic film plot. The studio's bosses were initially skeptical about the need for an original score; however, since they disliked the film's contrived special effects, they let Steiner try to improve the film with music. The studio suggested using old tracks in order to save on the cost of the film; however, "King Kong" producer Merian C. Cooper asked Steiner to score the film and said he would pay for the orchestra. Steiner took advantage of this offer and used an eighty-piece orchestra, explaining the film "was made for music." According to Steiner, "it was the kind of film that allowed you to do anything and everything, from weird chords and dissonances to pretty melodies." Steiner additionally scored the wild tribal music which accompanied the ceremony to sacrifice Ann to Kong. He wrote the score in two weeks and the music recording cost around $50,000. The film became a "landmark of film scoring," as it showed the power music has to manipulate audience emotions. Steiner constructed the score on Wagnerian leitmotif principle, which calls for special themes for leading characters and concepts. The theme of the monster is recognizable as a descending three-note chromatic motif. After the death of King Kong, the Kong theme and the Fay Wray theme converge, underlining the "Beauty and the Beast" type relationship between the characters. The music in the film's finale helped express the tender feelings Kong had for the woman without the film having to explicitly state it. The majority of the music is heavy and loud, but some of the music is a bit lighter. For example, when the ship sails into Skull Island, Steiner keeps the music calm and quiet with a small amount of texture in the harps to help characterize the ship as it cautiously moves through the misty waters. Steiner received a bonus from his work, as Cooper credited 25 percent of the film's success to the film score. Before he died, Steiner admitted "King Kong" was one of his favorite scores.

"King Kong" quickly made Steiner one of the most respected names in Hollywood. He continued as RKO's music director for two more years, until 1936. Max married Louise Klos, a harpist, in 1936. They had a son, Ron, together and they divorced in 1946. Steiner composed, arranged and conducted another 55 films, including most of Fred Astaire and Ginger Rogers' dance musicals. Additionally, Steiner wrote a sonata used in Katharine Hepburn's first film, "Bill of Divorcement" (1932). RKO producers, including Selznick, often came to him when they had problems with films, treating him as if he were a music "doctor". Steiner was asked to compose a score for "Of Human Bondage" (1934), which originally lacked music. He added musical touches to significant scenes. Director John Ford called on Steiner to score his film, "The Lost Patrol" (1934), which lacked tension without music.

John Ford hired Steiner again to compose for his next film, "The Informer" (1935), before Ford began production of the film. Ford even asked his screenwriter to meet with Steiner during the writing phase to collaborate. This was unusual for Steiner who typically refused to compose a score from anything earlier than a rough cut of the film. Because Steiner scored the music before and during film production, Ford would sometimes shoot scenes in synchronization with the music Steiner composed rather than the usual practice of film composers synchronizing music to the film's scenes. Consequently, Steiner directly influenced the development of the protagonist, Gypo. Victor McLaglen, who played Gypo, rehearsed his walking in order to match the fumbling leitmotif Steiner had created for Gypo. This unique film production practice was successful; the film was nominated for six Academy Awards and won four, including Steiner's first Academy Award for Best Scoring. This score helped to exemplify Steiner's ability to encompass the essence of a film in a single theme. The main title of the film's soundtrack has three specific aspects. First, the heavy-march-like theme helps to describe the oppressive military and main character Gypo's inevitable downfall. Second, the character's theme is stern and sober and puts the audience into the correct mood for the film. Finally, the theme of the music contains some Irish folk song influences which serves to better characterize the Irish historical setting and influence of the film. The theme is not heard consistently throughout the film and serves rather as a framework for the other melodic motifs heard throughout different parts of the film.

The score for this film is made up of many different themes which characterize the different personages and situations in the film. Steiner helps portray the genuine love Katie has for the main character Gypo. In one scene, Katie calls after Gypo as a solo violin echos the falling cadence of her voice. In another scene, Gypo sees an advertisement for a steamship to America and instead of the advertisement, sees himself holding Katie's hand on the ship. Wedding bells are heard along with organ music and he sees Katie wearing a veil and holding a bouquet. In a later scene, the Katie theme plays as a drunk Gypo sees a beautiful woman at the bar, insinuating he had mistaken her for Katie. Other musical themes included in the film score are an Irish folk song on French horns for Frankie McPhilip, a warm string theme for Dan and Gallagher and Mary McPhillip, and a sad theme on English horn with harp for the blind man.The most important motif in the film is the theme of betrayal relating to how Gypo betrays his friend Frankie: the "blood-money" motif. The theme is heard as the Captain throws the money on the table after Frankie is killed. The theme is a four note descending tune on harp; the first interval is the tritone. As the men are deciding who will be the executioner, the motif is repeated quietly and perpetually to establish Gypo's guilt and the musical motif is synchronized with the dripping of water in the prison. As it appears in the end of the film, the theme is played at a fortissimo volume as Gypo staggers into the church, ending the climax with the clap of the cymbals, indicating Gypo's penitence, no longer needing to establish his guilt.

Silent film mannerisms are still seen in Steiner's composition such as when actions or consequences are accompanied by a "sforzato" chord immediately before it, followed by silence. An example of this is remarked in the part of the film when Frankie confronts Gypo looking at his reward for arrest poster. Steiner uses minor "Mickey Mousing" techniques in the film. Through this score, Steiner showed the potential of film music, as he attempted the show the internal struggles inside of Gypo's mind through the mixing of different themes such as the Irish "Circassian Circle," the "blood-money" motif, and Frankie's theme. The score concludes with an original "Sancta Maria" by Steiner. Some writers have erroneously referred to the cue as featuring Franz Schubert's "Ave Maria".

In 1937, Steiner was hired by Frank Capra to conduct Dimitri Tiomkin's score for "Lost Horizon" (1937) as a safeguard in case Steiner needed to rewrite the score by an inexperienced Tiomkin; however, according to Hugo Friedhofer, Tiomkin specifically asked for Steiner, preferring him over the film studio's then music director. Selznick set up his own production company in 1936 and recruited Steiner to write the scores for his next three films.

In April 1937, Steiner left RKO and signed a long-term contract with Warner Bros.; he would, however, continue to work for Selznick. The first film he scored for Warner Bros. was "The Charge of the Light Brigade" (1936). Steiner became a mainstay at Warner Bros., scoring 140 of their films over the next 30 years alongside Hollywood stars such as Bette Davis, Errol Flynn, Humphrey Bogart, and James Cagney. Steiner frequently worked with composer Hugo Friedhofer who was hired as an orchestrator for Warner Bros; Friedholfer would orchestrate more than 50 of Steiner's pieces during his career. In 1938, Steiner wrote and arranged the first "composed for film" piece, "Symphony Moderne" which a character plays on the piano and later plays as a theme in "Four Daughters" (1938) and is performed by a full orchestra in "Four Wives" (1939).
In 1939, Steiner was borrowed from Warner Bros. by Selznick to compose the score for "Gone with the Wind" (1939), which became one of Steiner's most notable successes. Steiner was the only composer Selznick considered for scoring the film. Steiner was given only three months to complete the score, despite composing twelve more film scores in 1939, more than he would in any other year of his career. Because Selznick was concerned Steiner wouldn't have enough time to finish the score, he had Franz Waxman write an additional score in the case the Steiner didn't finish. To meet the deadline, Steiner sometimes worked for 20-hours straight, assisted by doctor-administered Benzedrine to stay awake. When the film was released, it was the longest film score ever composed, nearly three hours. The composition consisted of 16 main themes and nearly 300 musical segments. Due to the score's length, Steiner had help from four orchestrators and arrangers, including Heinz Roemheld, to work on the score. Selznick had asked Steiner to use only pre-existing classical music to help cut down on cost and time, but Steiner tried to convince him that filling the picture with swatches of classic concert music or popular works would not be as effective as an original score, which could be used to heighten the emotional content of scenes. Steiner ignored Selznick's wishes and composed an entirely new score. Selznick's opinion about using original scoring may have changed due to the overwhelming reaction to the film, nearly all of which contained Steiner's music. A year later, he even wrote a letter emphasizing the value of original film scores. The most well known of Steiner's themes for the score is the "Tara" theme for the O'Hara family plantation. Steiner explains Scarlett's deep-founded love for her home is why "the 'Tara' theme begins and ends with the picture and permeates the entire score". The film went on to win ten Academy Awards, although not for Best Original Score, which instead went to Herbert Stothart for "The Wizard of Oz". The score of "Gone with the Wind" is ranked #2 by AFI as the second greatest American film score of all time.

"Now, Voyager" would be the film score for which Steiner would win his second Academy Award. Kate Daubney attributes the success of this score to Steiner's ability to "[balance] the scheme of thematic meaning with the sound of the music." Steiner used motifs and thematic elements in the music to emphasize the emotional development of the narrative. After finishing "Now, Voyager" (1942), Steiner was hired to score the music for "Casablanca" (1942). Steiner would typically wait until the film was edited before scoring it, and after watching "Casablanca", he decided the song "As Time Goes By" by Herman Hupfeld wasn't an appropriate addition to the movie and he wanted to replace it with a song of his own composition; however, Ingrid Bergman had just cut her hair short in preparation for filming "For Whom the Bell Tolls" (1943), so she couldn't re-film the section with Steiner's song. Stuck with "As Time Goes By," Steiner embraced the song and made it the center theme of his score. Steiner's score for "Casablanca" was nominated for the Academy Award for Best Scoring of a Dramatic or Comedy Picture, losing to "The Song of Bernadette" (1943). Steiner received his third and final Oscar in 1944 for "Since You Went Away" (1944). Steiner actually first composed the theme from "Since You Went Away" while helping counterbalance Franz Waxman's moody score for "Rebecca". Producer David O. Selznick liked the theme so much, he asked Steiner to include it in "Since You Went Away". In 1947, Max married Leonette Blair.

Steiner also found success with the film noir genre. "The Big Sleep", "Mildred Pierce", and "The Letter" were his best film noir scores of the 1940s. "The Letter" is set in Singapore, the tale of murder begins with the loud main musical theme during the credits, which sets the tense and violent mood of the film. The main theme characterizes Leslie, the main character, by her tragic passion. The main theme is heard in the confrontation between Leslie and the murdered man's wife in the Chinese shop. Steiner portrays this scene through the jangling of wind chimes which crescendos as the wife emerges through opium smoke. The jangling continues until the wife asks Leslie to take off her shawl, after which the theme blasts indicating the breaking point of emotions of these women. Steiner's score for "The Letter" was nominated for the 1941 Academy Award for Best Original Score, losing to Walt Disney's "Pinocchio". In the score for "The Big Sleep", Steiner uses musical thematic characterization for the characters in the film. The theme for Philip Marlowe (Humphrey Bogart) is beguiling and ironic, with a playful grace note at the end of the motif, portrayed mixed between major and minor. At the end of the film, his theme is played fully in major chords and finishes by abruptly ending the chord as the film terminates (this was an unusual film music practice in Hollywood at the time). According to Christopher Palmer, the love theme for Bogart's Philip and Lauren Bacall's Vivian is one of Steiner's strongest themes. Steiner uses the contrast of high strings and low strings and brass to emphasize Philip's feelings for Vivian opposed with the brutality of the criminal world.In 1947, Steiner scored a film noir Western, "Pursued".

Steiner had more success with the Western genre of film, writing the scores for over twenty large-scale Westerns, most with epic-inspiring scores "about empire building and progress," like "Dodge City" (1939), "The Oklahoma Kid" (1939), and "Virginia City" (1940). "Dodge City", starring Errol Flynn and Olivia de Havilland, is a good example of Steiner's handling of typical scenes of the Western genre. Steiner used a "lifting, loping melody" which reflected the movement and sounds of wagons, horses, and cattle. Steiner showed a love for combining Westerns and romance, as he did in "They Died with Their Boots On" (1941), also starring Flynn and de Havilland. "The Searchers" (1956) is, today, considered his greatest Western.

Although his contract ended in 1953, Steiner returned to Warner Bros. in 1958 and scored several films such as "Band of Angels", "Marjorie Morningstar", and "John Paul Jones", and later ventured into television. Steiner still preferred large orchestras and leitmotif techniques during this part of his career. Steiner's pace slowed significantly in the mid-1950s, and he began freelancing. In 1954, RCA Victor asked Steiner to prepare and conduct an orchestral suite of music from "Gone with the Wind" for a special LP, which was later issued on CD. There are also acetates of Steiner conducting the Warner Brothers studio orchestra in music from many of his film scores. Composer Victor Young and Steiner were good friends, and Steiner completed the film score for "China Gate", because Young had died before he could finish it. The credit frame reads: "Music by Victor Young, extended by his old friend, Max Steiner." There are numerous soundtrack recordings of Steiner's music as soundtracks, collections, and recordings by others. Steiner wrote into his seventies, ailing and near blind, but his compositions "revealed a freshness and fertility of invention." A theme for "A Summer Place" in 1959, written when Steiner was 71, became one of Warner Brothers' biggest hit-tunes for years and a re-recorded pop standard. This memorable instrumental theme spent nine weeks at #1 on the "Billboard" Hot 100 singles chart in 1960 (in an instrumental cover version by Percy Faith). Steiner continued to score films produced by Warner until the mid-sixties.

In 1963, Steiner began writing his autobiography. Although it was completed, it was never published, and is the only source available on Steiner's childhood. A copy of the manuscript resides with the rest of the Max Steiner Collection at Brigham Young University in Provo, Utah. Steiner scored his last piece in 1965; however, he claimed he would have scored more films had he been offered the opportunity. His lack of work in the last years of his life was due to Hollywood's decreased interest in his scores caused by new film producers and new taste in film music. Another contribution to his declining career was his failing eyesight and deteriorating health, which caused him to reluctantly retire. Tony Thomas cited Steiner's last few scores as, "a weak coda to a mighty career."

Steiner died of congestive heart failure in Hollywood, aged 83. He is entombed in the Great Mausoleum at Forest Lawn Memorial Park Cemetery in Glendale, California.

In the early days of sound, producers avoided underscoring music behind dialogue, feeling the audience would wonder where the music was coming from. As a result, Steiner noted, "They began to add a little music here and there to support love scenes or silent sequences." But in scenes where music might be expected, such as a nightclub, ballroom, or theater, the orchestra fit in more naturally and was used often. In order to justify the addition of music in scenes where it wasn't expected, music was integrated into the scene through characters or added more conspicuously. For example, a shepherd boy might play a flute along with the orchestra heard in the background, or a random, wandering violinist might follow around a couple during a love scene; however, because half of the music was recorded on the set, Steiner says it led to a great deal of inconvenience and cost when scenes were later edited, because the score would often be ruined. As recording technology improved during this period, he was able to record the music synced to the film and could change the score after the film was edited. Steiner explains his own typical method of scoring:

Steiner often followed his instincts and his own reasoning in creating film scores. For example, when he chose to go against Selznick's instruction to use classical music for "Gone with the Wind." Steiner stated:

Scores from the classics were sometimes harmful to a picture, especially when they drew unwanted attention to themselves by virtue of their familiarity. For example, films like "", "The Sting", and "Manhattan", had scores with recognizable tunes instead of having a preferred "subliminal" effect. Steiner, was among the first to acknowledge the need for original scores for each film.

Steiner felt knowing when to start and stop was the hardest part of proper scoring, since incorrect placement of music can speed up a scene meant to be slow and vice versa: "Knowing the difference is what makes a film composer." He also notes that many composers, contrary to his own technique, would fail to subordinate the music to the film:

Although some scholars cite Steiner as the inventor of the click track technique, he, along with Roy Webb were only the first to use the technique in film scoring. Carl W. Stalling and Scott Bradley used the technique first, in cartoon music. The click-track allows the composer to sync music and film together more precisely. The technique involves punching holes into the soundtrack film based on the mathematics of metronome speed. As the holes pass through a projector, the orchestra and conductor can hear the clicking sound through headphones, allowing them to record the music along the exact timing of the film. This technique allowed conductors and orchestras to match the music with perfection to the timing of the film, eliminating the previous necessity to cut off or stop music in the middle of recording as had been done previously. Popularized by Steiner in film music, this technique allowed Steiner to "catch the action," creating sounds for small details on screen. In fact, Steiner reportedly spent more of his time matching the action to the music than composing the melodies and motifs, as creating and composing came easy to him.

With Steiner's background in his European musical training largely consisting of operas and operettas and his experience with stage music, he brought with him a slew of old-fashioned techniques he contributed to the development of the Hollywood film score. Although Steiner has been called, "the man who invented modern film music," he himself claimed that, "the idea originated with Richard Wagner ... If Wagner had lived in this century, he would have been the No. 1 film composer." Wagner was the inventor of the leitmotif, and this influenced Steiner's composition. In his music, Steiner relied heavily on leitmotifs. He would also quote pre-existing, recognizable melodies in his scores, such as national anthems. Steiner was known and often criticized for his use of Mickey Mousing or "catching the action." This technique is characterized by the precise matching of music with the actions or gestures on screen. Steiner was criticized for using this technique too frequently. For example, in "Of Human Bondage", Steiner created a limping effect with his music whenever the clubfooted character walked.

One of the important principles that guided Steiner whenever possible was his rule: "Every character should have a theme." "Steiner creates a musical picture that tells us all we need to know about the character." To accomplish this, Steiner synchronized the music, the narrative action, and the leitmotif as a structural framework for his compositions.

A good example of how the characters and the music worked together is best exemplified by his score for "The Glass Menagerie" (1950):

Another film which exemplifies the synchronizing of character and music is "The Fountainhead" (1949):
The character of Roark, an idealist architect (played by Gary Cooper):

In the same way that Steiner created a theme for each character in a film, Steiner's music developed themes to express emotional aspects of general scenes which originally lacked emotional content. For example:


When adding a music score to a picture, Steiner used a "spotting process" in which he and the director of the film would watch the film in its entirety and discuss where underscoring of diegetic music would begin and end. Another technique Steiner used was the mixing of realistic and background music. For example, a character humming to himself is realistic music, and the orchestra might play his tune, creating a background music effect that ties into the film. Steiner was criticized for this technique as the awareness of the film music can ruin the narrative illusion of the film; however, Steiner understood the importance of letting the film take the spotlight, making the music, "subordinate..to the picture," stating that, "if it gets too decorative, it loses its emotional appeal." Before 1932, producers of sound films tried to avoid the use of background music, because viewers would wonder where the music was coming from. Steiner was known for writing using atmospheric music without melodic content for certain neutral scenes in music. Steiner designed a melodic motion to create normal-sounding music without taking too much attention away from the film. In contrast, Steiner sometimes used diegetic, or narrative based music, in order to emphasize certain emotions or contradict them. According to Steiner, there is, "no greater counterpoint ... than gay music underlying a tragic scene or vice versa."

Three of Max Steiner's scores won the Academy Award for Best Original Score: "The Informer" (1935), "Now, Voyager" (1942), and "Since You Went Away" (1944). Steiner received a certificate for "The Informer". He originally received plaques for "Now, Voyager" and "Since You Went Away", but those plaques were replaced with Academy Award statuettes in 1946. As an individual, Steiner was nominated for a total of 20 Academy Awards, and won two. Prior to 1939, the Academy recognized a studio's music department, rather than the individual composer, with a nomination in the scoring category. During this time, five of Steiner's scores including "The Lost Patrol" and "The Charge of the Light Brigade" were nominated, but the Academy does not consider these nominations to belong to Max Steiner himself. Consequently, even though Steiner's score for "The Informer" won the Academy Award in 1936, the Academy does not officially consider Steiner as the individual winner of the award, as Steiner accepted the award on behalf of RKO's music department of which he was the department head. Steiner's 20 nominations make him the third most nominated individual in the history of the scoring categories, behind John Williams and Alfred Newman.

The United States Postal Service issued its "American Music Series" stamps on September 16, 1999, to pay tribute to renowned Hollywood composers, including Steiner. After Steiner's death, Charles Gerhardt conducted the National Philharmonic Orchestra in an RCA Victor album of highlights from Steiner's career, titled "Now Voyager". He also won a Golden Globe for Best Original Score for "Life with Father" (1947). Additional selections of Steiner scores were included on other RCA classic film albums during the early 1970s. The quadraphonic recordings were later digitally remastered for Dolby surround sound and released on CD. In 1975, Steiner was honored with a star located at 1551 Vine Street on the Hollywood Walk of Fame for his contribution to motion pictures. In 1995, Steiner was inducted posthumously into the Songwriters Hall of Fame. In commemoration of Steiner's 100th birthday, a memorial plaque was unveiled by Helmut Zilk, then mayor of Vienna, in 1988 at Steiner's birthplace, the "Hotel Nordbahn" (now "Austria Classic Hotel Wien") on Praterstraße 72. In 1990, Steiner was one of the first to be recognized for Lifetime Achievement by an online awards site.

In Kurt London's "Film Music", London expressed the opinion that American film music was inferior to European film music because it lacked originality of composition; he cited the music of Steiner as an exception to the rule. Steiner, along with contemporaries Erich Wolfgang Korngold and Alfred Newman, set the style and forms of film music of the time period and for film scores to come. Known for their similar music styles, Roy Webb was also Steiner's contemporary and they were friends until Steiner's death. Webb's score for "Mighty Joe Young" was reminiscent of Steiner. James Bond composer John Barry cited Steiner as an influence of his work. James Newton Howard, who composed the score for the 2005 remake of "King Kong", stated that he was influenced by Steiner's score; his descending theme when Kong first appears is reminiscent of Steiner's score. In fact, during the tribal sacrifice scene of the 2005 version, the music playing is from Steiner's score of the same scene in the 1933 version. Composer of the "Star Wars" film score, John Williams cited Steiner as well as other European emigrant composers in the 1930s and 1940s "Golden Age" of film music as influences of his work. In fact, George Lucas wanted Williams to use the scores of Steiner and Korngold as influences for the music for "Star Wars", despite the rarity of grandiose film music and the lack of use of leitmotifs and full orchestrations during the 1970s.

Often compared to his contemporary Erich Wolfgang Korngold, his rival and friend at Warner Bros., the music of Steiner was often seen by critics as inferior to Korngold. Composer David Raksin stated that the music of Korngold was, "of a higher order with a much wider sweep;" however, according to William Darby and Jack Du Bois's "American Film Music", even though other film score composers may have produced greater individual scores than Steiner, no composer ever created as many "very good" ones as Steiner. Despite the inferiority of Steiner's individual scores, his influence was largely historical. Steiner was the one of the first composers to reintroduce music into films after the invention of talking films. Steiner's score for "King Kong" modeled the method of adding background music into a movie. Some of his contemporaries did not like his music. Miklós Rózsa criticized Steiner for his use of Mickey Mousing and did not like his music, but Rózsa conceded that Steiner had a successful career and had a good "melodic sense."

Now referred to as the "father of film music" or the "dean of film music," Steiner had written or arranged music for over three hundred films by the end of his career. George Korngold, son of Erich Korngold, produced the Classic Film Score Series albums which included the music of Steiner. Albert K. Bender established the Max Steiner Music Society with international membership, publishing journals and newsletters and a library of audio recordings. When the Steiner collection went to Brigham Young University in 1981, the organization disbanded. The Max Steiner Memorial Society was formed in the United Kingdom continue the work of the Max Steiner Music Society.

The American Film Institute respectively ranked Steiner's scores for "Gone with the Wind" (1939) and "King Kong" (1933) #2 and #13 on their list of the 25 greatest film scores. His scores for the following films were also nominated for the list:



</doc>
<doc id="19694" url="https://en.wikipedia.org/wiki?curid=19694" title="Mercury (planet)">
Mercury (planet)

Mercury is the smallest and innermost planet in the Solar System. Its orbit around the Sun takes 87.97 days, the shortest of all the planets in the Solar System. It is named after the Greek god Hermes (Ερμής), translated into Latin Mercurius Mercury, god of commerce, messenger of the gods, mediator between gods and mortals.

Like Venus, Mercury orbits the Sun within Earth's orbit as an inferior planet, and its apparent distance from the Sun as viewed from Earth never exceeds 28°. This proximity to the Sun means the planet can only be seen near the western horizon after sunset or eastern horizon before sunrise, usually in twilight. At this time, it may appear as a bright star-like object, but is often far more difficult to observe than Venus. The planet telescopically displays the complete range of phases, similar to Venus and the Moon, as it moves in its inner orbit relative to Earth, which recurs over its synodic period of approximately 116 days.

Mercury rotates in a way that is unique in the Solar System. It is tidally locked with the Sun in a 3:2 spin–orbit resonance, meaning that relative to the fixed stars, it rotates on its axis exactly three times for every two revolutions it makes around the Sun. As seen from the Sun, in a frame of reference that rotates with the orbital motion, it appears to rotate only once every two Mercurian years. An observer on Mercury would therefore see only one day every two Mercurian years.

Mercury's axis has the smallest tilt of any of the Solar System's planets (about degree). Its orbital eccentricity is the largest of all known planets in the Solar System; at perihelion, Mercury's distance from the Sun is only about two-thirds (or 66%) of its distance at aphelion. Mercury's surface appears heavily cratered and is similar in appearance to the Moon's, indicating that it has been geologically inactive for billions of years. Having almost no atmosphere to retain heat, it has surface temperatures that vary diurnally more than on any other planet in the Solar System, ranging from at night to during the day across the equatorial regions. The polar regions are constantly below . The planet has no known natural satellites.

Two spacecraft have visited Mercury: "" flew by in 1974 and 1975; and "MESSENGER", launched in 2004, orbited Mercury over 4,000 times in four years before exhausting its fuel and crashing into the planet's surface on April 30, 2015. The "BepiColombo" spacecraft is planned to arrive at Mercury in 2025.

Mercury appears to have a solid silicate crust and mantle overlying a solid, iron sulfide outer core layer, a deeper liquid core layer, and a solid inner core.

Mercury is one of four terrestrial planets in the Solar System, and is a rocky body like Earth. It is the smallest planet in the Solar System, with an equatorial radius of . Mercury is also smaller—albeit more massive—than the largest natural satellites in the Solar System, Ganymede and Titan. Mercury consists of approximately 70% metallic and 30% silicate material. Mercury's density is the second highest in the Solar System at 5.427 g/cm, only slightly less than Earth's density of 5.515 g/cm. If the effect of gravitational compression were to be factored out from both planets, the materials of which Mercury is made would be denser than those of Earth, with an uncompressed density of 5.3 g/cm versus Earth's 4.4 g/cm.

Mercury's density can be used to infer details of its inner structure. Although Earth's high density results appreciably from gravitational compression, particularly at the core, Mercury is much smaller and its inner regions are not as compressed. Therefore, for it to have such a high density, its core must be large and rich in iron.

Geologists estimate that Mercury's core occupies about 55% of its volume; for Earth this proportion is 17%. Research published in 2007 suggests that Mercury has a molten core. Surrounding the core is a mantle consisting of silicates. Based on data from the mission and Earth-based observation, Mercury's crust is estimated to be thick. One distinctive feature of Mercury's surface is the presence of numerous narrow ridges, extending up to several hundred kilometers in length. It is thought that these were formed as Mercury's core and mantle cooled and contracted at a time when the crust had already solidified.

Mercury's core has a higher iron content than that of any other major planet in the Solar System, and several theories have been proposed to explain this. The most widely accepted theory is that Mercury originally had a metal–silicate ratio similar to common chondrite meteorites, thought to be typical of the Solar System's rocky matter, and a mass approximately 2.25 times its current mass. Early in the Solar System's history, Mercury may have been struck by a planetesimal of approximately 1/6 that mass and several thousand kilometers across. The impact would have stripped away much of the original crust and mantle, leaving the core behind as a relatively major component. A similar process, known as the giant impact hypothesis, has been proposed to explain the formation of the Moon.

Alternatively, Mercury may have formed from the solar nebula before the Sun's energy output had stabilized. It would initially have had twice its present mass, but as the protosun contracted, temperatures near Mercury could have been between 2,500 and 3,500 K and possibly even as high as 10,000 K. Much of Mercury's surface rock could have been vaporized at such temperatures, forming an atmosphere of "rock vapor" that could have been carried away by the solar wind.

A third hypothesis proposes that the solar nebula caused drag on the particles from which Mercury was accreting, which meant that lighter particles were lost from the accreting material and not gathered by Mercury. Each hypothesis predicts a different surface composition, and there are two space missions set to make observations. "MESSENGER", which ended in 2015, found higher-than-expected potassium and sulfur levels on the surface, suggesting that the giant impact hypothesis and vaporization of the crust and mantle did not occur because potassium and sulfur would have been driven off by the extreme heat of these events. "BepiColombo", which will arrive at Mercury in 2025, will make observations to test these hypotheses. The findings so far would seem to favor the third hypothesis; however, further analysis of the data is needed.

Mercury's surface is similar in appearance to that of the Moon, showing extensive mare-like plains and heavy cratering, indicating that it has been geologically inactive for billions of years. Because knowledge of Mercury's geology had been based only on the 1975 flyby and terrestrial observations, it is the least understood of the terrestrial planets. As data from "MESSENGER" orbiter are processed, this knowledge will increase. For example, an unusual crater with radiating troughs has been discovered that scientists called "the spider". It was later named Apollodorus.

Albedo features are areas of markedly different reflectivity, as seen by telescopic observation. Mercury has dorsa (also called "wrinkle-ridges"), Moon-like highlands, montes (mountains), planitiae (plains), rupes (escarpments), and valles (valleys).

Names for features on Mercury come from a variety of sources. Names coming from people are limited to the deceased. Craters are named for artists, musicians, painters, and authors who have made outstanding or fundamental contributions to their field. Ridges, or dorsa, are named for scientists who have contributed to the study of Mercury. Depressions or fossae are named for works of architecture. Montes are named for the word "hot" in a variety of languages. Plains or planitiae are named for Mercury in various languages. Escarpments or rupēs are named for ships of scientific expeditions. Valleys or valles are named for abandoned cities, towns, or settlements of antiquity.

Mercury was heavily bombarded by comets and asteroids during and shortly following its formation 4.6 billion years ago, as well as during a possibly separate subsequent episode called the Late Heavy Bombardment that ended 3.8 billion years ago. During this period of intense crater formation, Mercury received impacts over its entire surface, facilitated by the lack of any atmosphere to slow impactors down. During this time Mercury was volcanically active; basins such as the Caloris Basin were filled by magma, producing smooth plains similar to the maria found on the Moon.

Data from the October 2008 flyby of "MESSENGER" gave researchers a greater appreciation for the jumbled nature of Mercury's surface. Mercury's surface is more heterogeneous than either Mars's or the Moon's, both of which contain significant stretches of similar geology, such as maria and plateaus.

Craters on Mercury range in diameter from small bowl-shaped cavities to multi-ringed impact basins hundreds of kilometers across. They appear in all states of degradation, from relatively fresh rayed craters to highly degraded crater remnants. Mercurian craters differ subtly from lunar craters in that the area blanketed by their ejecta is much smaller, a consequence of Mercury's stronger surface gravity. According to IAU rules, each new crater must be named after an artist that was famous for more than fifty years, and dead for more than three years, before the date the crater is named.

The largest known crater is , with a diameter of 1,550 km. The impact that created the Caloris Basin was so powerful that it caused lava eruptions and left a concentric ring over 2 km tall surrounding the impact crater. At the antipode of the Caloris Basin is a large region of unusual, hilly terrain known as the "Weird Terrain". One hypothesis for its origin is that shock waves generated during the Caloris impact traveled around Mercury, converging at the basin's antipode (180 degrees away). The resulting high stresses fractured the surface. Alternatively, it has been suggested that this terrain formed as a result of the convergence of ejecta at this basin's antipode.

Overall, about 15 impact basins have been identified on the imaged part of Mercury. A notable basin is the 400 km wide, multi-ring Tolstoj Basin that has an ejecta blanket extending up to 500 km from its rim and a floor that has been filled by smooth plains materials. Beethoven Basin has a similar-sized ejecta blanket and a 625 km diameter rim. Like the Moon, the surface of Mercury has likely incurred the effects of space weathering processes, including Solar wind and micrometeorite impacts.

There are two geologically distinct plains regions on Mercury. Gently rolling, hilly plains in the regions between craters are Mercury's oldest visible surfaces, predating the heavily cratered terrain. These inter-crater plains appear to have obliterated many earlier craters, and show a general paucity of smaller craters below about 30 km in diameter.

Smooth plains are widespread flat areas that fill depressions of various sizes and bear a strong resemblance to the lunar maria. Notably, they fill a wide ring surrounding the Caloris Basin. Unlike lunar maria, the smooth plains of Mercury have the same albedo as the older inter-crater plains. Despite a lack of unequivocally volcanic characteristics, the localisation and rounded, lobate shape of these plains strongly support volcanic origins. All the smooth plains of Mercury formed significantly later than the Caloris basin, as evidenced by appreciably smaller crater densities than on the Caloris ejecta blanket. The floor of the Caloris Basin is filled by a geologically distinct flat plain, broken up by ridges and fractures in a roughly polygonal pattern. It is not clear whether they are volcanic lavas induced by the impact, or a large sheet of impact melt.

One unusual feature of Mercury's surface is the numerous compression folds, or rupes, that crisscross the plains. As Mercury's interior cooled, it contracted and its surface began to deform, creating wrinkle ridges and lobate scarps associated with thrust faults. The scarps can reach lengths of 1000 km and heights of 3 km. These compressional features can be seen on top of other features, such as craters and smooth plains, indicating they are more recent. Mapping of the features has suggested a total shrinkage of Mercury's radius in the range of ~1 to 7 km. Small-scale thrust fault scarps have been found, tens of meters in height and with lengths in the range of a few km, that appear to be less than 50 million years old, indicating that compression of the interior and consequent surface geological activity continue to the present.

The Lunar Reconnaissance Orbiter discovered that similar small thrust faults exist on the Moon.

Images obtained by "MESSENGER" have revealed evidence for pyroclastic flows on Mercury from low-profile shield volcanoes. "MESSENGER" data has helped identify 51 pyroclastic deposits on the surface, where 90% of them are found within impact craters. A study of the degradation state of the impact craters that host pyroclastic deposits suggests that pyroclastic activity occurred on Mercury over a prolonged interval.

A "rimless depression" inside the southwest rim of the Caloris Basin consists of at least nine overlapping volcanic vents, each individually up to 8 km in diameter. It is thus a "compound volcano". The vent floors are at a least 1 km below their brinks and they bear a closer resemblance to volcanic craters sculpted by explosive eruptions or modified by collapse into void spaces created by magma withdrawal back down into a conduit. Scientists could not quantify the age of the volcanic complex system, but reported that it could be of the order of a billion years.

The surface temperature of Mercury ranges from at the most extreme places: 0°N, 0°W, or 180°W. It never rises above 180 K at the poles,
due to the absence of an atmosphere and a steep temperature gradient between the equator and the poles. The subsolar point reaches about 700 K during perihelion (0°W or 180°W), but only 550 K at aphelion (90° or 270°W).
On the dark side of the planet, temperatures average 110 K.
The intensity of sunlight on Mercury's surface ranges between 4.59 and 10.61 times the solar constant (1,370 W·m).

Although the daylight temperature at the surface of Mercury is generally extremely high, observations strongly suggest that ice (frozen water) exists on Mercury. The floors of deep craters at the poles are never exposed to direct sunlight, and temperatures there remain below 102 K; far lower than the global average. Water ice strongly reflects radar, and observations by the 70-meter Goldstone Solar System Radar and the VLA in the early 1990s revealed that there are patches of high radar reflection near the poles. Although ice was not the only possible cause of these reflective regions, astronomers think it was the most likely.

The icy regions are estimated to contain about 10–10 kg of ice, and may be covered by a layer of regolith that inhibits sublimation. By comparison, the Antarctic ice sheet on Earth has a mass of about 4 kg, and Mars's south polar cap contains about 10 kg of water. The origin of the ice on Mercury is not yet known, but the two most likely sources are from outgassing of water from the planet's interior or deposition by impacts of comets.

Mercury is too small and hot for its gravity to retain any significant atmosphere over long periods of time; it does have a tenuous surface-bounded exosphere containing hydrogen, helium, oxygen, sodium, calcium, potassium and others at a surface pressure of less than approximately 0.5 nPa (0.005 picobars). This exosphere is not stable—atoms are continuously lost and replenished from a variety of sources. Hydrogen atoms and helium atoms probably come from the solar wind, diffusing into Mercury's magnetosphere before later escaping back into space. Radioactive decay of elements within Mercury's crust is another source of helium, as well as sodium and potassium. "MESSENGER" found high proportions of calcium, helium, hydroxide, magnesium, oxygen, potassium, silicon and sodium. Water vapor is present, released by a combination of processes such as: comets striking its surface, sputtering creating water out of hydrogen from the solar wind and oxygen from rock, and sublimation from reservoirs of water ice in the permanently shadowed polar craters. The detection of high amounts of water-related ions like O, OH, and HO was a surprise. Because of the quantities of these ions that were detected in Mercury's space environment, scientists surmise that these molecules were blasted from the surface or exosphere by the solar wind.

Sodium, potassium and calcium were discovered in the atmosphere during the 1980–1990s, and are thought to result primarily from the vaporization of surface rock struck by micrometeorite impacts including presently from Comet Encke. In 2008, magnesium was discovered by "MESSENGER". Studies indicate that, at times, sodium emissions are localized at points that correspond to the planet's magnetic poles. This would indicate an interaction between the magnetosphere and the planet's surface.

On November 29, 2012, NASA confirmed that images from "MESSENGER" had detected that craters at the north pole contained water ice. "MESSENGER" principal investigator Sean Solomon is quoted in "The New York Times" estimating the volume of the ice to be large enough to "encase Washington, D.C., in a frozen block two and a half miles deep".

Despite its small size and slow 59-day-long rotation, Mercury has a significant, and apparently global, magnetic field. According to measurements taken by , it is about 1.1% the strength of Earth's. The magnetic-field strength at Mercury's equator is about . Like that of Earth, Mercury's magnetic field is dipolar. Unlike Earth's, Mercury's poles are nearly aligned with the planet's spin axis. Measurements from both the and "MESSENGER" space probes have indicated that the strength and shape of the magnetic field are stable.

It is likely that this magnetic field is generated by a dynamo effect, in a manner similar to the magnetic field of Earth. This dynamo effect would result from the circulation of the planet's iron-rich liquid core. Particularly strong tidal effects caused by the planet's high orbital eccentricity would serve to keep the core in the liquid state necessary for this dynamo effect.

Mercury's magnetic field is strong enough to deflect the solar wind around the planet, creating a magnetosphere. The planet's magnetosphere, though small enough to fit within Earth, is strong enough to trap solar wind plasma. This contributes to the space weathering of the planet's surface. Observations taken by the spacecraft detected this low energy plasma in the magnetosphere of the planet's nightside. Bursts of energetic particles in the planet's magnetotail indicate a dynamic quality to the planet's magnetosphere.

During its second flyby of the planet on October 6, 2008, "MESSENGER" discovered that Mercury's magnetic field can be extremely "leaky". The spacecraft encountered magnetic "tornadoes" – twisted bundles of magnetic fields connecting the planetary magnetic field to interplanetary space – that were up to wide or a third of the radius of the planet. These twisted magnetic flux tubes, technically known as flux transfer events, form open windows in the planet's magnetic shield through which the solar wind may enter and directly impact Mercury's surface via magnetic reconnection This also occurs in Earth's magnetic field. The "MESSENGER" observations showed the reconnection rate is ten times higher at Mercury, but its proximity to the Sun only accounts for about a third of the reconnection rate observed by "MESSENGER".

Mercury has the most eccentric orbit of all the planets; its eccentricity is 0.21 with its distance from the Sun ranging from . It takes 87.969 Earth days to complete an orbit. The diagram illustrates the effects of the eccentricity, showing Mercury's orbit overlaid with a circular orbit having the same semi-major axis. Mercury's higher velocity when it is near perihelion is clear from the greater distance it covers in each 5-day interval. In the diagram the varying distance of Mercury to the Sun is represented by the size of the planet, which is inversely proportional to Mercury's distance from the Sun. This varying distance to the Sun leads to Mercury's surface being flexed by tidal bulges raised by the Sun that are about 17 times stronger than the Moon's on Earth. Combined with a 3:2 spin–orbit resonance of the planet's rotation around its axis, it also results in complex variations of the surface temperature.
The resonance makes a single solar day on Mercury last exactly two Mercury years, or about 176 Earth days.

Mercury's orbit is inclined by 7 degrees to the plane of Earth's orbit (the ecliptic), as shown in the diagram on the right. As a result, transits of Mercury across the face of the Sun can only occur when the planet is crossing the plane of the ecliptic at the time it lies between Earth and the Sun, which is in May or November. This occurs about every seven years on average.

Mercury's axial tilt is almost zero, with the best measured value as low as 0.027 degrees. This is significantly smaller than that of Jupiter, which has the second smallest axial tilt of all planets at 3.1 degrees. This means that to an observer at Mercury's poles, the center of the Sun never rises more than 2.1 arcminutes above the horizon.

At certain points on Mercury's surface, an observer would be able to see the Sun peek up a little more than two-thirds of the way over the horizon, then reverse and set before rising again, all within the same Mercurian day. This is because approximately four Earth days before perihelion, Mercury's angular orbital velocity equals its angular rotational velocity so that the Sun's apparent motion ceases; closer to perihelion, Mercury's angular orbital velocity then exceeds the angular rotational velocity. Thus, to a hypothetical observer on Mercury, the Sun appears to move in a retrograde direction. Four Earth days after perihelion, the Sun's normal apparent motion resumes. A similar effect would have occurred if Mercury had been in synchronous rotation: the alternating gain and loss of rotation over revolution would have caused a libration of 23.65° in longitude.

For the same reason, there are two points on Mercury's equator, 180 degrees apart in longitude, at either of which, around perihelion in alternate Mercurian years (once a Mercurian day), the Sun passes overhead, then reverses its apparent motion and passes overhead again, then reverses a second time and passes overhead a third time, taking a total of about 16 Earth-days for this entire process. In the other alternate Mercurian years, the same thing happens at the other of these two points. The amplitude of the retrograde motion is small, so the overall effect is that, for two or three weeks, the Sun is almost stationary overhead, and is at its most brilliant because Mercury is at perihelion, its closest to the Sun. This prolonged exposure to the Sun at its brightest makes these two points the hottest places on Mercury. Maximum temperature occurs when the Sun is at an angle of about 25 degrees past noon due to diurnal temperature lag, at 0.4 Mercury days and 0.8 Mercury years past sunrise. Conversely, there are two other points on the equator, 90 degrees of longitude apart from the first ones, where the Sun passes overhead only when the planet is at aphelion in alternate years, when the apparent motion of the Sun in Mercury's sky is relatively rapid. These points, which are the ones on the equator where the apparent retrograde motion of the Sun happens when it is crossing the horizon as described in the preceding paragraph, receive much less solar heat than the first ones described above.

Mercury attains inferior conjunction (nearest approach to Earth) every 116 Earth days on average, but this interval can range from 105 days to 129 days due to the planet's eccentric orbit. Mercury can come as near as to Earth, and that is slowly declining: The next approach to within is in 2679, and to within in 4487, but it will not be closer to Earth than until 28,622. Its period of retrograde motion as seen from Earth can vary from 8 to 15 days on either side of inferior conjunction. This large range arises from the planet's high orbital eccentricity. On average, Mercury is the closest planet to the Earth, and it is the closest planet to each of the other planets in the Solar System.

The longitude convention for Mercury puts the zero of longitude at one of the two hottest points on the surface, as described above. However, when this area was first visited, by , this zero meridian was in darkness, so it was impossible to select a feature on the surface to define the exact position of the meridian. Therefore, a small crater further west was chosen, called Hun Kal, which provides the exact reference point for measuring longitude. The center of Hun Kal defines the 20° west meridian. A 1970 International Astronomical Union resolution suggests that longitudes be measured positively in the westerly direction on Mercury. The two hottest places on the equator are therefore at longitudes 0° W and 180° W, and the coolest points on the equator are at longitudes 90° W and 270° W. However, the "MESSENGER" project uses an east-positive convention.

For many years it was thought that Mercury was synchronously tidally locked with the Sun, rotating once for each orbit and always keeping the same face directed towards the Sun, in the same way that the same side of the Moon always faces Earth. Radar observations in 1965 proved that the planet has a 3:2 spin-orbit resonance, rotating three times for every two revolutions around the Sun. The eccentricity of Mercury's orbit makes this resonance stable—at perihelion, when the solar tide is strongest, the Sun is nearly still in Mercury's sky.

The rare 3:2 resonant tidal locking is stabilized by the variance of the tidal force along Mercury's eccentric orbit, acting on a permanent dipole component of Mercury's mass distribution. In a circular orbit there is no such variance, so the only resonance stabilized in such an orbit is at 1:1 (e.g., Earth–Moon), when the tidal force, stretching a body along the "center-body" line, exerts a torque that aligns the body's axis of least inertia (the "longest" axis, and the axis of the aforementioned dipole) to point always at the center. However, with noticeable eccentricity, like that of Mercury's orbit, the tidal force has a maximum at perihelion and therefore stabilizes resonances, like 3:2, enforcing that the planet points its axis of least inertia roughly at the Sun when passing through perihelion.

The original reason astronomers thought it was synchronously locked was that, whenever Mercury was best placed for observation, it was always nearly at the same point in its 3:2 resonance, hence showing the same face. This is because, coincidentally, Mercury's rotation period is almost exactly half of its synodic period with respect to Earth. Due to Mercury's 3:2 spin-orbit resonance, a solar day (the length between two meridian transits of the Sun) lasts about 176 Earth days. A sidereal day (the period of rotation) lasts about 58.7 Earth days.

Simulations indicate that the orbital eccentricity of Mercury varies chaotically from nearly zero (circular) to more than 0.45 over millions of years due to perturbations from the other planets. 
This was thought to explain Mercury's 3:2 spin-orbit resonance (rather than the more usual 1:1), because this state is more likely to arise during a period of high eccentricity. 
However, accurate modeling based on a realistic model of tidal response has demonstrated that Mercury was captured into the 3:2 spin-orbit state at a very early stage of its history, within 20 (more likely, 10) million years after its formation.

Numerical simulations show that a future secular orbital resonant perihelion interaction with Jupiter may cause the eccentricity of Mercury's orbit to increase to the point where there is a 1% chance that the planet may collide with Venus within the next five billion years.

In 1859, the French mathematician and astronomer Urbain Le Verrier reported that the slow precession of Mercury's orbit around the Sun could not be completely explained by Newtonian mechanics and perturbations by the known planets. He suggested, among possible explanations, that another planet (or perhaps instead a series of smaller 'corpuscules') might exist in an orbit even closer to the Sun than that of Mercury, to account for this perturbation. (Other explanations considered included a slight oblateness of the Sun.) The success of the search for Neptune based on its perturbations of the orbit of Uranus led astronomers to place faith in this possible explanation, and the hypothetical planet was named Vulcan, but no such planet was ever found.

The perihelion precession of Mercury is 5,600 arcseconds (1.5556°) per century relative to Earth, or 574.10±0.65 arcseconds per century relative to the inertial ICRF. Newtonian mechanics, taking into account all the effects from the other planets, predicts a precession of 5,557 arcseconds (1.5436°) per century. In the early 20th century, Albert Einstein's general theory of relativity provided the explanation for the observed precession, by formalizing gravitation as being mediated by the curvature of spacetime. The effect is small: just 42.98 arcseconds per century for Mercury; it therefore requires a little over twelve million orbits for a full excess turn. Similar, but much smaller, effects exist for other Solar System bodies: 8.62 arcseconds per century for Venus, 3.84 for Earth, 1.35 for Mars, and 10.05 for 1566 Icarus.

Einstein's formula for the perihelion shift per revolution is formula_1, where formula_2 is the orbital eccentricity, formula_3 the semi-major axis, and formula_4 the orbital period. Filling in the values gives a result of 0.1035 arcseconds per revolution or 0.4297 arcseconds per Earth year, i.e., 42.97 arcseconds per century. This is in close agreement with the accepted value of Mercury's perihelion advance of 42.98 arcseconds per century.

There may be scientific support, based on studies reported in March 2020, for considering that parts of the planet Mercury may have been habitable, and perhaps that life forms, albeit likely primitive microorganisms, may have existed on the planet.

Mercury's apparent magnitude is calculated to vary between −2.48 (brighter than Sirius) around superior conjunction and +7.25 (below the limit of naked-eye visibility) around inferior conjunction. The mean apparent magnitude is 0.23 while the standard deviation of 1.78 is the largest of any planet. The mean apparent magnitude at superior conjunction is −1.89 while that at inferior conjunction is +5.93. Observation of Mercury is complicated by its proximity to the Sun, as it is lost in the Sun's glare for much of the time. Mercury can be observed for only a brief period during either morning or evening twilight.

Mercury can, like several other planets and the brightest stars, be seen during a total solar eclipse.

Like the Moon and Venus, Mercury exhibits phases as seen from Earth. It is "new" at inferior conjunction and "full" at superior conjunction. The planet is rendered invisible from Earth on both of these occasions because of its being obscured by the Sun, except its new phase during a transit.

Mercury is technically brightest as seen from Earth when it is at a full phase. Although Mercury is farthest from Earth when it is full, the greater illuminated area that is visible and the opposition brightness surge more than compensates for the distance. The opposite is true for Venus, which appears brightest when it is a crescent, because it is much closer to Earth than when gibbous.
Nonetheless, the brightest (full phase) appearance of Mercury is an essentially impossible time for practical observation, because of the extreme proximity of the Sun. Mercury is best observed at the first and last quarter, although they are phases of lesser brightness. The first and last quarter phases occur at greatest elongation east and west of the Sun, respectively. At both of these times Mercury's separation from the Sun ranges anywhere from 17.9° at perihelion to 27.8° at aphelion. At greatest "western" elongation, Mercury rises at its earliest before sunrise, and at greatest "eastern" elongation, it sets at its latest after sunset.

Mercury can be easily seen from the tropics and subtropics more than from higher latitudes. Viewed from low latitudes and at the right times of year, the ecliptic intersects the horizon at a steep angle. Mercury is 10° above the horizon when the planet appears directly above the Sun (i.e. its orbit appears vertical) and is at maximum elongation from the Sun (28°) and also when the Sun is 18° below the horizon, so the sky is just completely dark. This angle is the maximum altitude at which Mercury is visible in a completely dark sky.

At middle latitudes, Mercury is more often and easily visible from the Southern Hemisphere than from the Northern. This is because Mercury's maximum western elongation occurs only during early autumn in the Southern Hemisphere, whereas its greatest eastern elongation happens only during late winter in the Southern Hemisphere. In both of these cases, the angle at which the planet's orbit intersects the horizon is maximized, allowing it to rise several hours before sunrise in the former instance and not set until several hours after sundown in the latter from southern mid-latitudes, such as Argentina and South Africa.

An alternate method for viewing Mercury involves observing the planet during daylight hours when conditions are clear, ideally when it is at its greatest elongation. This allows the planet to be found easily, even when using telescopes with apertures. Care must be taken to ensure the instrument isn't pointed directly towards the Sun because of the risk for eye damage. This method bypasses the limitation of twilight observing when the ecliptic is located at a low elevation (e.g. on autumn evenings).

Ground-based telescope observations of Mercury reveal only an illuminated partial disk with limited detail. The first of two spacecraft to visit the planet was , which mapped about 45% of its surface from 1974 to 1975. The second is the "MESSENGER" spacecraft, which after three Mercury flybys between 2008 and 2009, attained orbit around Mercury on March 17, 2011, to study and map the rest of the planet.

The Hubble Space Telescope cannot observe Mercury at all, due to safety procedures that prevent its pointing too close to the Sun.

Because the shift of 0.15 revolutions in a year makes up a seven-year cycle (0.15 × 7 ≈ 1.0), in the seventh year Mercury follows almost exactly (earlier by 7 days) the sequence of phenomena it showed seven years before.

The earliest known recorded observations of Mercury are from the Mul.Apin tablets. These observations were most likely made by an Assyrian astronomer around the 14th century BC. The cuneiform name used to designate Mercury on the Mul.Apin tablets is transcribed as Udu.Idim.Gu\u.Ud ("the jumping planet"). Babylonian records of Mercury date back to the 1st millennium BC. The Babylonians called the planet Nabu after the messenger to the gods in their mythology.

The ancients knew Mercury by different names depending on whether it was an evening star or a morning star. By about 350 BC, the ancient Greeks had realized the two stars were one. They knew the planet as Στίλβων "Stilbōn", meaning "twinkling", and Ἑρμής "Hermēs", for its fleeting motion, a name that is retained in modern Greek (Ερμής "Ermis"). The Romans named the planet after the swift-footed Roman messenger god, Mercury (Latin "Mercurius"), which they equated with the Greek Hermes, because it moves across the sky faster than any other planet. The astronomical symbol for Mercury is a stylized version of Hermes' caduceus.

The Greco-Egyptian astronomer Ptolemy wrote about the possibility of planetary transits across the face of the Sun in his work "Planetary Hypotheses". He suggested that no transits had been observed either because planets such as Mercury were too small to see, or because the transits were too infrequent.

In ancient China, Mercury was known as "the Hour Star" ("Chen-xing" ). It was associated with the direction north and the phase of water in the Five Phases system of metaphysics. Modern Chinese, Korean, Japanese and Vietnamese cultures refer to the planet literally as the "water star" (), based on the Five elements. Hindu mythology used the name Budha for Mercury, and this god was thought to preside over Wednesday. The god Odin (or Woden) of Germanic paganism was associated with the planet Mercury and Wednesday. The Maya may have represented Mercury as an owl (or possibly four owls; two for the morning aspect and two for the evening) that served as a messenger to the underworld.

In medieval Islamic astronomy, the Andalusian astronomer Abū Ishāq Ibrāhīm al-Zarqālī in the 11th century described the deferent of Mercury's geocentric orbit as being oval, like an egg or a pignon, although this insight did not influence his astronomical theory or his astronomical calculations. In the 12th century, Ibn Bajjah observed "two planets as black spots on the face of the Sun", which was later suggested as the transit of Mercury and/or Venus by the Maragha astronomer Qotb al-Din Shirazi in the 13th century. (Note that most such medieval reports of transits were later taken as observations of sunspots.)

In India, the Kerala school astronomer Nilakantha Somayaji in the 15th century developed a partially heliocentric planetary model in which Mercury orbits the Sun, which in turn orbits Earth, similar to the Tychonic system later proposed by Tycho Brahe in the late 16th century.

The first telescopic observations of Mercury were made by Galileo in the early 17th century. Although he observed phases when he looked at Venus, his telescope was not powerful enough to see the phases of Mercury. In 1631, Pierre Gassendi made the first telescopic observations of the transit of a planet across the Sun when he saw a transit of Mercury predicted by Johannes Kepler. In 1639, Giovanni Zupi used a telescope to discover that the planet had orbital phases similar to Venus and the Moon. The observation demonstrated conclusively that Mercury orbited around the Sun.

A rare event in astronomy is the passage of one planet in front of another (occultation), as seen from Earth. Mercury and Venus occult each other every few centuries, and the event of May 28, 1737 is the only one historically observed, having been seen by John Bevis at the Royal Greenwich Observatory. The next occultation of Mercury by Venus will be on December 3, 2133.

The difficulties inherent in observing Mercury mean that it has been far less studied than the other planets. In 1800, Johann Schröter made observations of surface features, claiming to have observed mountains. Friedrich Bessel used Schröter's drawings to erroneously estimate the rotation period as 24 hours and an axial tilt of 70°. In the 1880s, Giovanni Schiaparelli mapped the planet more accurately, and suggested that Mercury's rotational period was 88 days, the same as its orbital period due to tidal locking. This phenomenon is known as synchronous rotation. The effort to map the surface of Mercury was continued by Eugenios Antoniadi, who published a book in 1934 that included both maps and his own observations. Many of the planet's surface features, particularly the albedo features, take their names from Antoniadi's map.

In June 1962, Soviet scientists at the Institute of Radio-engineering and Electronics of the USSR Academy of Sciences, led by Vladimir Kotelnikov, became the first to bounce a radar signal off Mercury and receive it, starting radar observations of the planet. Three years later, radar observations by Americans Gordon H. Pettengill and Rolf B. Dyce, using the 300-meter Arecibo Observatory radio telescope in Puerto Rico, showed conclusively that the planet's rotational period was about 59 days. The theory that Mercury's rotation was synchronous had become widely held, and it was a surprise to astronomers when these radio observations were announced. If Mercury were tidally locked, its dark face would be extremely cold, but measurements of radio emission revealed that it was much hotter than expected. Astronomers were reluctant to drop the synchronous rotation theory and proposed alternative mechanisms such as powerful heat-distributing winds to explain the observations.

Italian astronomer Giuseppe Colombo noted that the rotation value was about two-thirds of Mercury's orbital period, and proposed that the planet's orbital and rotational periods were locked into a 3:2 rather than a 1:1 resonance. Data from subsequently confirmed this view. This means that Schiaparelli's and Antoniadi's maps were not "wrong". Instead, the astronomers saw the same features during every "second" orbit and recorded them, but disregarded those seen in the meantime, when Mercury's other face was toward the Sun, because the orbital geometry meant that these observations were made under poor viewing conditions.

Ground-based optical observations did not shed much further light on Mercury, but radio astronomers using interferometry at microwave wavelengths, a technique that enables removal of the solar radiation, were able to discern physical and chemical characteristics of the subsurface layers to a depth of several meters. Not until the first space probe flew past Mercury did many of its most fundamental morphological properties become known. Moreover, recent technological advances have led to improved ground-based observations. In 2000, high-resolution lucky imaging observations were conducted by the Mount Wilson Observatory 1.5 meter Hale telescope. They provided the first views that resolved surface features on the parts of Mercury that were not imaged in the mission. Most of the planet has been mapped by the Arecibo radar telescope, with resolution, including polar deposits in shadowed craters of what may be water ice.

Reaching Mercury from Earth poses significant technical challenges, because it orbits so much closer to the Sun than Earth. A Mercury-bound spacecraft launched from Earth must travel over into the Sun's gravitational potential well. Mercury has an orbital speed of , whereas Earth's orbital speed is . Therefore, the spacecraft must make a large change in velocity (delta-v) to get to Mercury and then enter orbit, as compared to the delta-v required for other planetary missions.

The potential energy liberated by moving down the Sun's potential well becomes kinetic energy; requiring another large delta-v change to do anything other than rapidly pass by Mercury. To land safely or enter a stable orbit the spacecraft would rely entirely on rocket motors. Aerobraking is ruled out because Mercury has a negligible atmosphere. A trip to Mercury requires more rocket fuel than that required to escape the Solar System completely. As a result, only two space probes have visited it so far. A proposed alternative approach would use a solar sail to attain a Mercury-synchronous orbit around the Sun.

The first spacecraft to visit Mercury was NASA's (1974–1975). The spacecraft used the gravity of Venus to adjust its orbital velocity so that it could approach Mercury, making it both the first spacecraft to use this gravitational "slingshot" effect and the first NASA mission to visit multiple planets. provided the first close-up images of Mercury's surface, which immediately showed its heavily cratered nature, and revealed many other types of geological features, such as the giant scarps that were later ascribed to the effect of the planet shrinking slightly as its iron core cools. Unfortunately, the same face of the planet was lit at each of close approaches. This made close observation of both sides of the planet impossible, and resulted in the mapping of less than 45% of the planet's surface.

The spacecraft made three close approaches to Mercury, the closest of which took it to within of the surface. At the first close approach, instruments detected a magnetic field, to the great surprise of planetary geologists—Mercury's rotation was expected to be much too slow to generate a significant dynamo effect. The second close approach was primarily used for imaging, but at the third approach, extensive magnetic data were obtained. The data revealed that the planet's magnetic field is much like Earth's, which deflects the solar wind around the planet. For many years after the encounters, the origin of Mercury's magnetic field remained the subject of several competing theories.

On March 24, 1975, just eight days after its final close approach, ran out of fuel. Because its orbit could no longer be accurately controlled, mission controllers instructed the probe to shut down. is thought to be still orbiting the Sun, passing close to Mercury every few months.

A second NASA mission to Mercury, named "MESSENGER" (MErcury Surface, Space ENvironment, GEochemistry, and Ranging), was launched on August 3, 2004. It made a fly-by of Earth in August 2005, and of Venus in October 2006 and June 2007 to place it onto the correct trajectory to reach an orbit around Mercury. A first fly-by of Mercury occurred on January 14, 2008, a second on October 6, 2008, and a third on September 29, 2009. Most of the hemisphere not imaged by was mapped during these fly-bys. The probe successfully entered an elliptical orbit around the planet on March 18, 2011. The first orbital image of Mercury was obtained on March 29, 2011. The probe finished a one-year mapping mission, and then entered a one-year extended mission into 2013. In addition to continued observations and mapping of Mercury, "MESSENGER" observed the 2012 solar maximum.

The mission was designed to clear up six key issues: Mercury's high density, its geological history, the nature of its magnetic field, the structure of its core, whether it has ice at its poles, and where its tenuous atmosphere comes from. To this end, the probe carried imaging devices that gathered much-higher-resolution images of much more of Mercury than , assorted spectrometers to determine abundances of elements in the crust, and magnetometers and devices to measure velocities of charged particles. Measurements of changes in the probe's orbital velocity were expected to be used to infer details of the planet's interior structure. "MESSENGER" final maneuver was on April 24, 2015, and it crashed into Mercury's surface on April 30, 2015. The spacecraft's impact with Mercury occurred near 3:26 PM EDT on April 30, 2015, leaving a crater estimated to be in diameter.

The European Space Agency and the Japanese Space Agency developed and launched a joint mission called "BepiColombo", which will orbit Mercury with two probes: one to map the planet and the other to study its magnetosphere. Launched on October 20, 2018, "BepiColombo" is expected to reach Mercury in 2025. It will release a magnetometer probe into an elliptical orbit, then chemical rockets will fire to deposit the mapper probe into a circular orbit. Both probes will operate for one terrestrial year. The mapper probe carries an array of spectrometers similar to those on "MESSENGER", and will study the planet at many different wavelengths including infrared, ultraviolet, X-ray and gamma ray.




</doc>
<doc id="19701" url="https://en.wikipedia.org/wiki?curid=19701" title="Monty Python and the Holy Grail">
Monty Python and the Holy Grail

Monty Python and the Holy Grail is a 1975 British comedy film reflecting the Arthurian legend, written and performed by the Monty Python comedy group (Chapman, Cleese, Gilliam, Idle, Jones and Palin), directed by Gilliam and Jones. It was conceived during the hiatus between the third and fourth series of their BBC television series "Monty Python's Flying Circus".

While the group's first film, "And Now for Something Completely Different", was a compilation of sketches from the first two television series, "Holy Grail" is a new story that parodies the legend of King Arthur's quest for the Holy Grail. Thirty years later, Idle used the film as the basis for the musical "Spamalot".

"Monty Python and the Holy Grail" grossed more than any other British film exhibited in the US in 1975. In the US, it was selected as the second-best comedy of all time in the ABC special "". In the UK, readers of "Total Film" magazine in 2000 ranked it the fifth-greatest comedy film of all time; a similar poll of Channel 4 viewers in 2006 placed it sixth.

In AD 932, King Arthur and his squire, Patsy, travel throughout Britain searching for men to join the Knights of the Round Table. Along the way, Arthur debates whether swallows could carry coconuts, recounts receiving Excalibur from the Lady of the Lake, defeats the Black Knight and observes an impromptu witch trial. He recruits Sir Bedevere the Wise, Sir Lancelot the Brave, Sir Galahad the Pure and Sir Robin the Not-Quite-So-Brave-as-Sir-Lancelot, along with their squires and Robin's minstrels. Arthur leads the knights to Camelot, but upon further consideration (thanks to a musical number) he decides not to go there because it is "a silly place". As they turn away, God (an image of W. G. Grace) appears and orders Arthur to find the Holy Grail.

Searching for the Grail, Arthur and his knights arrive at a castle occupied by French soldiers who claim to have the Grail and taunt the Englishmen, before driving them back with a barrage of barnyard animals. Bedevere concocts a plan to sneak in using a Trojan Rabbit, but no one hides inside it, and the Englishmen are forced to flee when the rabbit is flung back at them. Arthur decides the knights should go their separate ways to search for the Grail. A modern-day historian filming a documentary on the Arthurian legends is abruptly killed by an unknown knight on horseback, triggering a police investigation.

On the knights' travels, Arthur and Bedevere are given directions by an old man in Scene 24 and then attempt to satisfy the strange requests of the dreaded Knights Who Say "Ni!". Sir Robin avoids a fight with a Three-Headed Knight by running away while the heads are arguing. Sir Galahad is led by a grail-shaped beacon to Castle Anthrax, populated by 150 young women, but is unwillingly "rescued" by Lancelot. Next, Lancelot receives an arrow-shot note from Swamp Castle and believes it to be from a lady being forced to marry against her will. He storms the castle and slaughters several members of the wedding party, only to discover the note was from an effeminate prince being coerced into marriage by his ambitious father.

Arthur and his knights regroup and are joined by three new knights as well as Brother Maynard and his monk brethren. They meet Tim the Enchanter, who directs them to a cave where the location of the Grail is said to be written. The entrance to the cave is guarded by the Rabbit of Caerbannog. Fatally underestimating its lethal prowess, the knights attack the Rabbit, which easily kills Sirs Bors, Gawain and Ector. Arthur uses the "Holy Hand Grenade of Antioch", provided by Brother Maynard, to destroy the creature. Inside the cave, they find an inscription from Joseph of Arimathea, directing them to Castle Aarrgh.

A giant animated cave monster devours Brother Maynard, but Arthur and the knights escape after the animator suffers a fatal heart attack, ending the beast's existence. The protagonists approach the Bridge of Death, where the bridge-keeper challenges them to answer three questions to pass or they will be cast into the Gorge of Eternal Peril. Lancelot goes first, easily answers the elementary questions and crosses. Robin is done in by an unexpectedly difficult question, and Galahad misses the answer to an easy one; both are flung into the gorge. When the bridge keeper asks "What is the air-speed velocity of an unladen swallow?" Arthur asks for clarification ("African or European?"). The bridge keeper cannot answer, and is thrown into the gorge himself.

Upon crossing the bridge, Arthur and Bedevere cannot find Lancelot, unaware he has been arrested by police investigating the historian's death. Arthur and Bedevere reach Castle Aarrgh, only to find it occupied by the same French soldiers who taunted them previously. After being repelled by showers of manure, they summon an army of knights and prepare to assault the castle. Just as the army charges, police arrive and arrest Arthur and Bedevere for the historian's death. The film ends with an officer breaking the camera.

Fifteen months before the BBC visited the set in May 1974, the Monty Python troupe assembled the first version of the screenplay. When half of the resulting material was set in the Middle Ages, and half was set in the present day, the group opted to focus on the Middle Ages, revolving on the legend of the Holy Grail. By the fourth or fifth version of their screenplay, the story was complete, and the cast joked the fact that the Grail was never retrieved would be "a big let-down ... a great anti-climax". Graham Chapman said a challenge was incorporating scenes that did not fit the Holy Grail motif.

Neither Terry Gilliam nor Terry Jones had directed a film before, and described it as a learning experience in which they would learn to make a film by making an entire full-length film. The cast humorously described the novice directing style as employing the level of mutual disrespect always found in Monty Python's work.

The film's initial budget of approximately £200,000 was raised by convincing 10 separate investors to contribute £20,000 apiece. Three of those investors were the rock bands Pink Floyd, Led Zeppelin and Genesis, who were persuaded to help fund the film by Tony Stratton-Smith, head of Charisma Records (the record label that released Python's early comedy albums). According to Terry Gilliam, the Pythons turned to rock stars like Pink Floyd, Led Zeppelin and Elton John for finance as the studios refused to fund the film and rock stars saw it as "a good tax write-off" due to UK income tax being "as high as 90%" at the time.

"Monty Python and the Holy Grail" was mostly shot on location in Scotland, particularly around Doune Castle, Glen Coe, and the privately owned Castle Stalker. The many castles seen throughout the film were mainly either Doune Castle shot from different angles or hanging miniatures. There are several exceptions to this: the very first exterior shot of a castle at the beginning of the film is Kidwelly Castle in South Wales, and the single exterior shot of the Swamp Castle during "Tale of Sir Lancelot" is Bodiam Castle in East Sussex; all subsequent shots of the exterior and interior of those scenes were filmed at Doune Castle. Production designer Julian Doyle recounted that his crew constructed walls in the forest near Doune. Terry Jones later recalled the crew had selected more castles around Scotland for locations, but during the two weeks prior to principal photography, the Scottish Department of the Environment declined permission for use of the castles in its jurisdiction, for fear of damage.

At the start of "The Tale of Sir Robin", there is a slow camera zoom in on rocky scenery (that in the voice-over is described as "the dark forest of Ewing"). This is actually a still photograph of the gorge at Mount Buffalo National Park in Victoria, Australia. Doyle stated in 2000 during an interview with "Hotdog" magazine that it was a still image filmed with candles underneath the frame (to give a heat haze). This was a low-cost method of achieving a convincing location effect.
On the DVD audio commentary, Cleese described challenges shooting and editing Castle Anthrax in "The Tale of Sir Galahad", with what he felt the most comedic take being unused because an anachronistic coat was visible in it. Castle Anthrax was also shot in one part of Doune, where costume designer Hazel Pethig advised against nudity, dressing the girls in shifts.

In the scene where the knights were combatting the Rabbit of Caerbannog, a real white rabbit was used, switched with puppets for its killings. It was covered with red liquid to simulate blood, though the rabbit's owner did not want the animal dirty and was kept unaware. The liquid was difficult to remove from the fur. He also stated that he thought that, had they been more experienced in filmmaking, the crew would have just purchased a rabbit instead. Otherwise, the rabbit himself was unharmed. Also, the rabbit-bite effects were done by special puppetry by both Gilliam and SFX technician John Horton.

As chronicled in "The Life of Python", "The First 20 Years of Monty Python", and "The Pythons' Autobiography", Chapman suffered from acrophobia, trembling and bouts of forgetfulness during filming due to his alcoholism, prompting him to refrain from drinking while the production continued in order to remain "on an even keel". Nearly three years later, in December 1977, Chapman achieved sobriety.

Originally the knight characters were going to ride real horses, but after it became clear that the film's small budget precluded real horses (except for a lone horse appearing in a couple of scenes), the Pythons decided their characters would mime horse-riding while their porters trotted behind them banging coconut shells together. The joke was derived from the old-fashioned sound effect used by radio shows to convey the sound of hooves clattering. This was later referred to in the German release of the film, which translated the title as "Die Ritter der Kokosnuß" ("The Knights of the Coconut").

The opening credits of the film feature pseudo-Swedish subtitles, which soon turn into an appeal to visit Sweden and see the country's moose. The subtitles are soon stopped, but moose references continue throughout the actual credits until the credits are stopped again and restarted in a different visual style and with references to llamas, animals often mentioned in "Flying Circus". The subtitles were written by Michael Palin as a way to "entertain the 'captive' audience" at the beginning of the film.

In addition to several songs written by Python regular Neil Innes, several pieces of music were licensed from De Wolfe Music Library. These include:

"Monty Python and the Holy Grail" had its theatrical debut in the United Kingdom in 3 April 1975, followed by a United States release on 27 April 1975. It was re-released on 14 October 2015 in the United Kingdom.

The film had its television premiere 25 February 1977 on the "CBS Late Movie". Reportedly, the Pythons were displeased to discover a number of edits were done by the network to reduce use of profanity and the showing of blood. The troupe pulled back the rights and thereafter had it broadcast in the United States only on PBS and later other channels such as Comedy Central and IFC, where it runs uncut.

In Region 1, The Criterion Collection released a LaserDisc version of the film featuring audio commentary from directors Jones and Gilliam.

In 2001, Columbia Tristar published a two-disc, special-edition DVD. Disc one includes the Jones and Gilliam commentary, a second commentary with Idle, Palin and Cleese, the film's screenplay on a subtitle track and "Subtitles for People Who Don't Like the Film"–consisting of lines taken from William Shakespeare's "Henry IV, Part 2". Disc two includes "Monty Python and the Holy Grail in Lego", a "brickfilm" version of the "Camelot Song" as sung by Lego minifigures. It was created by Spite Your Face Productions on commission from the Lego Group and Python Pictures. The project was conceived by the original film's respective producer and co-director, John Goldstone and Terry Gilliam. Disc two also includes two scenes from the film's Japanese dub, literally translated back into English through subtitles. "The Quest for the Holy Grail Locations", hosted by Palin and Jones, shows places in Scotland used for the setting titled as "England 932 A.D." (as well as the two Pythons purchasing a copy of their own script as a guide). Also included is a who's who page, advertising galleries and sing-alongs. A "Collector's Edition" DVD release additionally included a book of the screenplay, a limited-edition film cell/senitype, and limited-edition art cards. 

A 35th-anniversary edition on Blu-ray was released in the US on 6 March 2012. Special features include "The Holy Book of Days," a second-screen experience that can be downloaded as an app on an iOS device and played with the Blu-ray to enhance its viewing, lost animation sequences with a new intro from animator Terry Gilliam, outtakes and extended scenes with Python member and the movie's co-director Terry Jones.

Contemporary reviews were mixed. Vincent Canby of "The New York Times" wrote in a favourable review that the film had "some low spots," but had gags which were "nonstop, occasionally inspired and should not be divulged, though it's not giving away too much to say that I particularly liked a sequence in which the knights, to gain access to an enemy castle, come up with the idea of building a Trojan rabbit." Charles Champlin of the "Los Angeles Times" was also positive, writing that the film, "like "Mad" comics, is not certain to please every taste. But its youthful exuberance and its rousing zaniness are hard not to like. As a matter of fact, the sense of fun is dangerously contagious." Penelope Gilliatt of "The New Yorker" called the film "often recklessly funny and sometimes a matter of comic genius."

Other reviews were less enthusiastic. "Variety" wrote that the storyline was "basically an excuse for set pieces, some amusing, others overdone." Gene Siskel of the "Chicago Tribune" gave the film two-and-a-half stars, writing that he felt "it contained about 10 very funny moments and 70 minutes of silence. Too many of the jokes took too long to set up, a trait shared by both "Blazing Saddles" and "Young Frankenstein". I guess I prefer Monty Python in chunks, in its original, television revue format." Gary Arnold of "The Washington Post" called the film "a fitfully amusing spoof of the Arthurian legends" but "rather poky" in tempo, citing the running gag of Swedish subtitles in the opening credits as an example of how the Pythons "don't know when to let go of any "shtik"". Geoff Brown of "The Monthly Film Bulletin" wrote in a mixed review that "the team's visual buffooneries and verbal rigamaroles (some good, some bad, but mostly indifferent) are piled on top of each other with no attention to judicious timing or structure, and a form which began as a jaunty assault on the well-made revue sketch and an ingenious misuse of television's fragmented style of presentation, threatens to become as unyielding and unfruitful as the conventions it originally attacked."

The film's reputation grew over time. In 2000, readers of "Total Film" magazine voted "Holy Grail" the fifth-greatest comedy film of all time. The next Python film, "Life of Brian", was ranked first. A 2006 poll of Channel 4 viewers on the 50 Greatest Comedy Films saw "Holy Grail" placed in sixth place (with "Life of Brian" again topping the list). In 2011, an ABC prime-time special, "", counted down the best films chosen by fans based on results of a poll conducted by ABC and "People". "Holy Grail" was selected as the second best comedy after "Airplane!". In 2016, "Empire" magazine ranked "Holy Grail" 18th in their list of the 100 best British films ("Life of Brian" was ranked 2nd), with their entry stating, "Elvis ordered a print of this comedy classic and watched it five times. If it's good enough for the King, it's good enough for you."

In a 2017 interview at Indiana University in Bloomington, John Cleese expressed disappointment with the film's conclusion. "'The ending annoys me the most'", he said after a screening of the film on the Indiana campus, adding that "'It ends the way it does because we couldn't think of any other way'". However, scripts for the film and notebooks that are among Michael Palin's private archive, which he donated to the British Library in 2017, do document at least one alternate ending that the troupe considered: "a battle between the knights of Camelot, the French, and the Killer Rabbit of Caerbannog". Due to the film's small production budget, that idea or a "much pricier option" was discarded by the Pythons in favour of the ending with "King Arthur getting arrested", which Palin deemed "'cheaper'" and "'funnier'".

In 2005, the film was adapted as a Tony Award-winning Broadway musical, "Spamalot". Written primarily by Idle, the show has more of an overarching plot and leaves out certain portions of the movie due to difficulties in rendering certain effects on stage. Nonetheless, many of the jokes from the film are present in the show.

In 2013, the Pythons lost a legal case to Mark Forstater, the film's producer, over royalties for the derivative work, "Spamalot". They owed a combined £800,000 in legal fees and back royalties to Forstater. To help cover the cost of these royalties and fees, the group arranged and performed in a stage show, "Monty Python Live (Mostly)", held at the O Arena in London in July 2014.





</doc>
<doc id="19702" url="https://en.wikipedia.org/wiki?curid=19702" title="Mutation">
Mutation

In biology, a mutation is an alteration in the nucleotide sequence of the genome of an organism, virus, or extrachromosomal DNA. Viral genomes contain either DNA or RNA. Mutations result from errors during DNA or viral replication, mitosis, or meiosis or other types of damage to DNA (such as pyrimidine dimers caused by exposure to ultraviolet radiation), which then may undergo error-prone repair (especially microhomology-mediated end joining), cause an error during other forms of repair, or cause an error during replication (translesion synthesis). Mutations may also result from insertion or deletion of segments of DNA due to mobile genetic elements.

Mutations may or may not produce discernible changes in the observable characteristics (phenotype) of an organism. Mutations play a part in both normal and abnormal biological processes including: evolution, cancer, and the development of the immune system, including junctional diversity. Mutation is the ultimate source of all genetic variation, providing the raw material on which evolutionary forces such as natural selection can act.

Mutation can result in many different types of change in sequences. Mutations in genes can have no effect, alter the product of a gene, or prevent the gene from functioning properly or completely. Mutations can also occur in nongenic regions. A 2007 study on genetic variations between different species of "Drosophila" suggested that, if a mutation changes a protein produced by a gene, the result is likely to be harmful, with an estimated 70% of amino acid polymorphisms that have damaging effects, and the remainder being either neutral or marginally beneficial. Due to the damaging effects that mutations can have on genes, organisms have mechanisms such as DNA repair to prevent or correct mutations by reverting the mutated sequence back to its original state.

Mutations can involve the duplication of large sections of DNA, usually through genetic recombination. These duplications are a major source of raw material for evolving new genes, with tens to hundreds of genes duplicated in animal genomes every million years. Most genes belong to larger gene families of shared ancestry, detectable by their sequence homology. Novel genes are produced by several methods, commonly through the duplication and mutation of an ancestral gene, or by recombining parts of different genes to form new combinations with new functions.

Here, protein domains act as modules, each with a particular and independent function, that can be mixed together to produce genes encoding new proteins with novel properties. For example, the human eye uses four genes to make structures that sense light: three for cone cell or color vision and one for rod cell or night vision; all four arose from a single ancestral gene. Another advantage of duplicating a gene (or even an entire genome) is that this increases engineering redundancy; this allows one gene in the pair to acquire a new function while the other copy performs the original function. Other types of mutation occasionally create new genes from previously noncoding DNA.

Changes in chromosome number may involve even larger mutations, where segments of the DNA within chromosomes break and then rearrange. For example, in the Homininae, two chromosomes fused to produce human chromosome 2; this fusion did not occur in the lineage of the other apes, and they retain these separate chromosomes. In evolution, the most important role of such chromosomal rearrangements may be to accelerate the divergence of a population into new species by making populations less likely to interbreed, thereby preserving genetic differences between these populations.

Sequences of DNA that can move about the genome, such as transposons, make up a major fraction of the genetic material of plants and animals, and may have been important in the evolution of genomes. For example, more than a million copies of the Alu sequence are present in the human genome, and these sequences have now been recruited to perform functions such as regulating gene expression. Another effect of these mobile DNA sequences is that when they move within a genome, they can mutate or delete existing genes and thereby produce genetic diversity.

Nonlethal mutations accumulate within the gene pool and increase the amount of genetic variation. The abundance of some genetic changes within the gene pool can be reduced by natural selection, while other "more favorable" mutations may accumulate and result in adaptive changes.
For example, a butterfly may produce offspring with new mutations. The majority of these mutations will have no effect; but one might change the color of one of the butterfly's offspring, making it harder (or easier) for predators to see. If this color change is advantageous, the chances of this butterfly's surviving and producing its own offspring are a little better, and over time the number of butterflies with this mutation may form a larger percentage of the population.

Neutral mutations are defined as mutations whose effects do not influence the fitness of an individual. These can increase in frequency over time due to genetic drift. It is believed that the overwhelming majority of mutations have no significant effect on an organism's fitness. Also, DNA repair mechanisms are able to mend most changes before they become permanent mutations, and many organisms have mechanisms for eliminating otherwise-permanently mutated somatic cells.

Beneficial mutations can improve reproductive success.

Four classes of mutations are (1) spontaneous mutations (molecular decay), (2) mutations due to error-prone replication bypass of naturally occurring DNA damage (also called error-prone translesion synthesis), (3) errors introduced during DNA repair, and (4) induced mutations caused by mutagens. Scientists may also deliberately introduce mutant sequences through DNA manipulation for the sake of scientific experimentation.

One 2017 study claimed that 66% of cancer-causing mutations are random, 29% are due to the environment (the studied population spanned 69 countries), and 5% are inherited.

Humans on average pass 60 new mutations to their children but fathers pass more mutations depending on their age with every year adding two new mutations to a child.

"Spontaneous mutations" occur with non-zero probability even given a healthy, uncontaminated cell. Naturally occurring oxidative DNA damage is estimated to occur 10,000 times per cell per day in humans and 100,000 times per cell per day in rats. Spontaneous mutations can be characterized by the specific change:


There is increasing evidence that the majority of spontaneously arising mutations are due to error-prone replication (translesion synthesis) past DNA damage in the template strand. In mice, the majority of mutations are caused by translesion synthesis. Likewise, in yeast, Kunz et al. found that more than 60% of the spontaneous single base pair substitutions and deletions were caused by translesion synthesis.

Although naturally occurring double-strand breaks occur at a relatively low frequency in DNA, their repair often causes mutation. Non-homologous end joining (NHEJ) is a major pathway for repairing double-strand breaks. NHEJ involves removal of a few nucleotides to allow somewhat inaccurate alignment of the two ends for rejoining followed by addition of nucleotides to fill in gaps. As a consequence, NHEJ often introduces mutations.

Induced mutations are alterations in the gene after it has come in contact with mutagens and environmental causes.

"Induced mutations" on the molecular level can be caused by:

Whereas in former times mutations were assumed to occur by chance, or induced by mutagens, molecular mechanisms of mutation have been discovered in bacteria and across the tree of life. As S. Rosenberg states, "These mechanisms reveal a picture of highly regulated mutagenesis, up-regulated temporally by stress responses and activated when cells/organisms are maladapted to their environments—when stressed—potentially accelerating adaptation." Since they are self-induced mutagenic mechanisms that increase the adaptation rate of organisms, they have some times been named as adaptive mutagenesis mechanisms, and include the SOS response in bacteria, ectopic intrachromosomal recombination and other chromosomal events such as duplications.

The sequence of a gene can be altered in a number of ways. Gene mutations have varying effects on health depending on where they occur and whether they alter the function of essential proteins.
Mutations in the structure of genes can be classified into several types.

Large-scale mutations in chromosomal structure include:


Small-scale mutations affect a gene in one or a few nucleotides. (If only a single nucleotide is affected, they are called point mutations.) Small-scale mutations include:


The effect of a mutation on protein sequence depends in part on where in the genome it occurs, especially whether it is in a coding or non-coding region. Mutations in the non-coding regulatory sequences of a gene, such as promoters, enhancers, and silencers, can alter levels of gene expression, but are less likely to alter the protein sequence. Mutations within introns and in regions with no known biological function (e.g. pseudogenes, retrotransposons) are generally neutral, having no effect on phenotype – though intron mutations could alter the protein product if they affect mRNA splicing.

Mutations in that occur in coding regions of the genome are more likely to alter the protein product, and can be categorized by their effect on amino acid sequence:



In applied genetics, it is usual to speak of mutations as either harmful or beneficial.

Attempts have been made to infer the distribution of fitness effects (DFE) using mutagenesis experiments and theoretical models applied to molecular sequence data. DFE, as used to determine the relative abundance of different types of mutations (i.e., strongly deleterious, nearly neutral or advantageous), is relevant to many evolutionary questions, such as the maintenance of genetic variation, the rate of genomic decay, the maintenance of outcrossing sexual reproduction as opposed to inbreeding and the evolution of sex and genetic recombination. DFE can also be tracked by tracking the skewness of the distribution of mutations with putatively severe effects as compared to the distribution of mutations with putatively mild or absent effect. In summary, the DFE plays an important role in predicting evolutionary dynamics. A variety of approaches have been used to study the DFE, including theoretical, experimental and analytical methods.



One of the earliest theoretical studies of the distribution of fitness effects was done by Motoo Kimura, an influential theoretical population geneticist. His neutral theory of molecular evolution proposes that most novel mutations will be highly deleterious, with a small fraction being neutral. Hiroshi Akashi more recently proposed a bimodal model for the DFE, with modes centered around highly deleterious and neutral mutations. Both theories agree that the vast majority of novel mutations are neutral or deleterious and that advantageous mutations are rare, which has been supported by experimental results. One example is a study done on the DFE of random mutations in vesicular stomatitis virus. Out of all mutations, 39.6% were lethal, 31.2% were non-lethal deleterious, and 27.1% were neutral. Another example comes from a high throughput mutagenesis experiment with yeast. In this experiment it was shown that the overall DFE is bimodal, with a cluster of neutral mutations, and a broad distribution of deleterious mutations.

Though relatively few mutations are advantageous, those that are play an important role in evolutionary changes. Like neutral mutations, weakly selected advantageous mutations can be lost due to random genetic drift, but strongly selected advantageous mutations are more likely to be fixed. Knowing the DFE of advantageous mutations may lead to increased ability to predict the evolutionary dynamics. Theoretical work on the DFE for advantageous mutations has been done by John H. Gillespie and H. Allen Orr. They proposed that the distribution for advantageous mutations should be exponential under a wide range of conditions, which, in general, has been supported by experimental studies, at least for strongly selected advantageous mutations.

In general, it is accepted that the majority of mutations are neutral or deleterious, with advantageous mutations being rare; however, the proportion of types of mutations varies between species. This indicates two important points: first, the proportion of effectively neutral mutations is likely to vary between species, resulting from dependence on effective population size; second, the average effect of deleterious mutations varies dramatically between species. In addition, the DFE also differs between coding regions and noncoding regions, with the DFE of noncoding DNA containing more weakly selected mutations.

In multicellular organisms with dedicated reproductive cells, mutations can be subdivided into germline mutations, which can be passed on to descendants through their reproductive cells, and somatic mutations (also called acquired mutations), which involve cells outside the dedicated reproductive group and which are not usually transmitted to descendants.

Diploid organisms (e.g., humans) contain two copies of each gene—a paternal and a maternal allele. Based on the occurrence of mutation on each chromosome, we may classify mutations into three types. A wild type or homozygous non-mutated organism is one in which neither allele is mutated.


A germline mutation in the reproductive cells of an individual gives rise to a "constitutional mutation" in the offspring, that is, a mutation that is present in every cell. A constitutional mutation can also occur very soon after fertilisation, or continue from a previous constitutional mutation in a parent. A germline mutation can be passed down through subsequent generations of organisms.

The distinction between germline and somatic mutations is important in animals that have a dedicated germline to produce reproductive cells. However, it is of little value in understanding the effects of mutations in plants, which lack a dedicated germline. The distinction is also blurred in those animals that reproduce asexually through mechanisms such as budding, because the cells that give rise to the daughter organisms also give rise to that organism's germline.

A new germline mutation not inherited from either parent is called a "de novo" mutation.

A change in the genetic structure that is not inherited from a parent, and also not passed to offspring, is called a somatic mutation"." Somatic mutations are not inherited by an organism's offspring because they do not affect the germline. However, they are passed down to all the progeny of a mutated cell within the same organism during mitosis. A major section of an organism therefore might carry the same mutation. These types of mutations are usually prompted by environmental causes, such as ultraviolet radiation or any exposure to certain harmful chemicals, and can cause diseases including cancer.""

With plants, some somatic mutations can be propagated without the need for seed production, for example, by grafting and stem cuttings. These type of mutation have led to new types of fruits, such as the "Delicious" apple and the "Washington" navel orange.

Human and mouse somatic cells have a mutation rate more than ten times higher than the germline mutation rate for both species; mice have a higher rate of both somatic and germline mutations per cell division than humans. The disparity in mutation rate between the germline and somatic tissues likely reflects the greater importance of genome maintenance in the germline than in the soma.


In order to categorize a mutation as such, the "normal" sequence must be obtained from the DNA of a "normal" or "healthy" organism (as opposed to a "mutant" or "sick" one), it should be identified and reported; ideally, it should be made publicly available for a straightforward nucleotide-by-nucleotide comparison, and agreed upon by the scientific community or by a group of expert geneticists and biologists, who have the responsibility of establishing the "standard" or so-called "consensus" sequence. This step requires a tremendous scientific effort. Once the consensus sequence is known, the mutations in a genome can be pinpointed, described, and classified. The committee of the Human Genome Variation Society (HGVS) has developed the standard human sequence variant nomenclature, which should be used by researchers and DNA diagnostic centers to generate unambiguous mutation descriptions. In principle, this nomenclature can also be used to describe mutations in other organisms. The nomenclature specifies the type of mutation and base or amino acid changes.


Mutation rates vary substantially across species, and the evolutionary forces that generally determine mutation are the subject of ongoing investigation.

The genomes of RNA viruses are based on RNA rather than DNA. The RNA viral genome can be double-stranded (as in DNA) or single-stranded. In some of these viruses (such as the single-stranded human immunodeficiency virus), replication occurs quickly, and there are no mechanisms to check the genome for accuracy. This error-prone process often results in mutations.

Changes in DNA caused by mutation in a coding region of DNA can cause errors in protein sequence that may result in partially or completely non-functional proteins. Each cell, in order to function correctly, depends on thousands of proteins to function in the right places at the right times. When a mutation alters a protein that plays a critical role in the body, a medical condition can result. Some mutations alter a gene's DNA base sequence but do not change the function of the protein made by the gene. One study on the comparison of genes between different species of "Drosophila" suggests that if a mutation does change a protein, the mutation will most likely be harmful, with an estimated 70 percent of amino acid polymorphisms having damaging effects, and the remainder being either neutral or weakly beneficial. However, studies have shown that only 7% of point mutations in noncoding DNA of yeast are deleterious and 12% in coding DNA are deleterious. The rest of the mutations are either neutral or slightly beneficial.

If a mutation is present in a germ cell, it can give rise to offspring that carries the mutation in all of its cells. This is the case in hereditary diseases. In particular, if there is a mutation in a DNA repair gene within a germ cell, humans carrying such germline mutations may have an increased risk of cancer. A list of 34 such germline mutations is given in the article DNA repair-deficiency disorder. An example of one is albinism, a mutation that occurs in the OCA1 or OCA2 gene. Individuals with this disorder are more prone to many types of cancers, other disorders and have impaired vision.

DNA damage can cause an error when the DNA is replicated, and this error of replication can cause a gene mutation that, in turn, could cause a genetic disorder. DNA damages are repaired by the DNA repair system of the cell. Each cell has a number of pathways through which enzymes recognize and repair damages in DNA. Because DNA can be damaged in many ways, the process of DNA repair is an important way in which the body protects itself from disease. Once DNA damage has given rise to a mutation, the mutation cannot be repaired.

On the other hand, a mutation may occur in a somatic cell of an organism. Such mutations will be present in all descendants of this cell within the same organism. The accumulation of certain mutations over generations of somatic cells is part of cause of malignant transformation, from normal cell to cancer cell.

Cells with heterozygous loss-of-function mutations (one good copy of gene and one mutated copy) may function normally with the unmutated copy until the good copy has been spontaneously somatically mutated. This kind of mutation happens often in living organisms, but it is difficult to measure the rate. Measuring this rate is important in predicting the rate at which people may develop cancer.

Point mutations may arise from spontaneous mutations that occur during DNA replication. The rate of mutation may be increased by mutagens. Mutagens can be physical, such as radiation from UV rays, X-rays or extreme heat, or chemical (molecules that misplace base pairs or disrupt the helical shape of DNA). Mutagens associated with cancers are often studied to learn about cancer and its prevention.

Prions are proteins and do not contain genetic material. However, prion replication has been shown to be subject to mutation and natural selection just like other forms of replication. The human gene PRNP codes for the major prion protein, PrP, and is subject to mutations that can give rise to disease-causing prions.

Although mutations that cause changes in protein sequences can be harmful to an organism, on occasions the effect may be positive in a given environment. In this case, the mutation may enable the mutant organism to withstand particular environmental stresses better than wild-type organisms, or reproduce more quickly. In these cases a mutation will tend to become more common in a population through natural selection. Examples include the following:

HIV resistance: a specific 32 base pair deletion in human CCR5 (CCR5-Δ32) confers HIV resistance to homozygotes and delays AIDS onset in heterozygotes. One possible explanation of the etiology of the relatively high frequency of CCR5-Δ32 in the European population is that it conferred resistance to the bubonic plague in mid-14th century Europe. People with this mutation were more likely to survive infection; thus its frequency in the population increased. This theory could explain why this mutation is not found in Southern Africa, which remained untouched by bubonic plague. A newer theory suggests that the selective pressure on the CCR5 Delta 32 mutation was caused by smallpox instead of the bubonic plague.

Malaria resistance: An example of a harmful mutation is sickle-cell disease, a blood disorder in which the body produces an abnormal type of the oxygen-carrying substance hemoglobin in the red blood cells. One-third of all indigenous inhabitants of Sub-Saharan Africa carry the allele, because, in areas where malaria is common, there is a survival value in carrying only a single sickle-cell allele (sickle cell trait). Those with only one of the two alleles of the sickle-cell disease are more resistant to malaria, since the infestation of the malaria "Plasmodium" is halted by the sickling of the cells that it infests.

Antibiotic resistance: Practically all bacteria develop antibiotic resistance when exposed to antibiotics. In fact, bacterial populations already have such mutations that get selected under antibiotic selection. Obviously, such mutations are only beneficial for the bacteria but not for those infected.

Lactase persistence. A mutation allowed humans to express the enzyme lactase after they are naturally weaned from breast milk, allowing adults to digest lactose, which is likely one of the most beneficial mutations in recent human evolution.

Mutationism is one of several alternatives to evolution by natural selection that have existed both before and after the publication of Charles Darwin's 1859 book, "On the Origin of Species". In the theory, mutation was the source of novelty, creating new forms and new species, potentially instantaneously, in a sudden jump. This was envisaged as driving evolution, which was limited by the supply of mutations.

Before Darwin, biologists commonly believed in saltationism, the possibility of large evolutionary jumps, including immediate speciation. For example, in 1822 Étienne Geoffroy Saint-Hilaire argued that species could be formed by sudden transformations, or what would later be called macromutation. Darwin opposed saltation, insisting on gradualism in evolution as in geology. In 1864, Albert von Kölliker revived Geoffroy's theory. In 1901 the geneticist Hugo de Vries gave the name "mutation" to seemingly new forms that suddenly arose in his experiments on the evening primrose "Oenothera lamarckiana", and in the first decade of the 20th century, mutationism, or as de Vries named it "mutationstheorie", became a rival to Darwinism supported for a while by geneticists including William Bateson, Thomas Hunt Morgan, and Reginald Punnett.

Understanding of mutationism is clouded by the mid-20th century portrayal of the early mutationists by supporters of the modern synthesis as opponents of Darwinian evolution and rivals of the biometrics school who argued that selection operated on continuous variation. In this portrayal, mutationism was defeated by a synthesis of genetics and natural selection that supposedly started later, around 1918, with work by the mathematician Ronald Fisher. However, the alignment of Mendelian genetics and natural selection began as early as 1902 with a paper by Udny Yule, and built up with theoretical and experimental work in Europe and America. Despite the controversy, the early mutationists had by 1918 already accepted natural selection and explained continuous variation as the result of multiple genes acting on the same characteristic, such as height.

Mutationism, along with other alternatives to Darwinism like Lamarckism and orthogenesis, was discarded by most biologists as they came to see that Mendelian genetics and natural selection could readily work together; mutation took its place as a source of the genetic variation essential for natural selection to work on. However, mutationism did not entirely vanish. In 1940, Richard Goldschmidt again argued for single-step speciation by macromutation, describing the organisms thus produced as "hopeful monsters", earning widespread ridicule. In 1987, Masatoshi Nei argued controversially that evolution was often mutation-limited. Modern biologists such as Douglas J. Futuyma conclude that essentially all claims of evolution driven by large mutations can be explained by Darwinian evolution.



</doc>
<doc id="19705" url="https://en.wikipedia.org/wiki?curid=19705" title="Microgyrus">
Microgyrus

A microgyrus is an area of the cerebral cortex that includes only four cortical layers instead of six.

Microgyria are believed by some to be part of the genetic lack of prenatal development which is a cause of, or one of the causes of, dyslexia.

Albert Galaburda of Harvard Medical School noticed that language centers in dyslexic brains showed microscopic flaws known as ectopias and microgyria (Galaburda "et al.", 2006, "Nature Neuroscience" 9(10): 1213-1217). Both affect the normal six-layer structure of the cortex. These flaws affect connectivity and functionality of the cortex in critical areas related to sound and visual processing. These and similar structural abnormalities may be the basis of the inevitable and hard to overcome difficulty in reading.



</doc>
<doc id="19708" url="https://en.wikipedia.org/wiki?curid=19708" title="Mercantilism">
Mercantilism

Mercantilism is an economic policy that is designed to maximize the exports and minimize the imports for an economy. It promotes imperialism, tariffs and subsidies on traded goods to achieve that goal. These policies aim to reduce a possible current account deficit or reach a current account surplus. Mercantilism includes measures aimed at accumulating monetary reserves through a positive balance of trade, especially of finished goods. Historically, such policies frequently led to war and also motivated colonial expansion. Mercantilist theory varies in sophistication from one writer to another and has evolved over time.

Mercantilism was dominant in modernized parts of Europe from the 16th to the 18th centuries, a period of proto-industrialization, before falling into decline, although some commentators argue that it is still practiced in the economies of industrializing countries, in the form of economic interventionism. It promotes government regulation of a nation's economy for the purpose of augmenting state power at the expense of rival national powers. High tariffs, especially on manufactured goods, were an almost universal feature of mercantilist policy.

With the efforts of supranational organizations such as the World Trade Organization to reduce tariffs globally, non-tariff barriers to trade have assumed a greater importance in neomercantilism.

Mercantilism became the dominant school of economic thought in Europe throughout the late Renaissance and the early-modern period (from the 15th to the 18th centuries). Evidence of mercantilistic practices appeared in early-modern Venice, Genoa, and Pisa regarding control of the Mediterranean trade in bullion. However, the empiricism of the Renaissance, which first began to quantify large-scale trade accurately, marked mercantilism's birth as a codified school of economic theories. The Italian economist and mercantilist Antonio Serra is considered to have written one of the first treatises on political economy with his 1613 work, "A Short Treatise on the Wealth and Poverty of Nations".

Mercantilism in its simplest form is bullionism, yet mercantilist writers emphasize the circulation of money and reject hoarding. Their emphasis on monetary metals accords with current ideas regarding the money supply, such as the stimulative effect of a growing money-supply. Fiat money and floating exchange rates have since rendered specie concerns irrelevant. In time, industrial policy supplanted the heavy emphasis on money, accompanied by a shift in focus from the capacity to carry on wars to promoting general prosperity. Mature neomercantilist theory recommends selective high tariffs for "infant" industries or the promotion of the mutual growth of countries through national industrial specialization.

England began the first large-scale and integrative approach to mercantilism during the Elizabethan Era (1558–1603). An early statement on national balance of trade appeared in "Discourse of the Common Weal of this Realm of England", 1549: "We must always take heed that we buy no more from strangers than we sell them, for so should we impoverish ourselves and enrich them." The period featured various but often disjointed efforts by the court of Queen Elizabeth (reigned 1558-1603) to develop a naval and merchant fleet capable of challenging the Spanish stranglehold on trade and of expanding the growth of bullion at home. Queen Elizabeth promoted the Trade and Navigation Acts in Parliament and issued orders to her navy for the protection and promotion of English shipping. A systematic and coherent explanation of balance of trade emerged in Thomas Mun's argument "England's Treasure by Forraign Trade or the Balance of our Forraign Trade is The Rule of Our Treasure" - written in the 1620s and published in 1664.

Elizabeth's efforts organized national resources sufficiently in the defense of England against the far larger and more powerful Spanish Empire, and in turn, paved the foundation for establishing a global empire in the 19th century. Authors noted most for establishing the English mercantilist system include Gerard de Malynes ( 1585–1641) and Thomas Mun (1571-1641), who first articulated the Elizabethan system ("England's Treasure by Forraign Trade or the Balance of Forraign Trade is the Rule of Our Treasure"), which Josiah Child ( 1630/31 – 1699) then developed further. Numerous French authors helped cement French policy around mercantilism in the 17th century. Jean-Baptiste Colbert (Intendant général, 1661–1665; Contrôleur général des finances, 1661–1683) best articulated this French mercantilism. French economic policy liberalized greatly under Napoleon (in power from 1799 to 1814/1815)

Many nations applied the theory, notably France, which was the most important state economically in Europe at the time. King Louis XIV (reigned 1643-1715) followed the guidance of Jean Baptiste Colbert, his Controller-General of Finances from 1665 to 1683. It was determined that the state should rule in the economic realm as it did in the diplomatic, and that the interests of the state as identified by the king were superior to those of merchants and of everyone else. Mercantilist economic policies aimed to build up the state, especially in an age of incessant warfare, and theorists charged the state with looking for ways to strengthen the economy and to weaken foreign adversaries.

In Europe, academic belief in mercantilism began to fade in the late-18th century after the British seized control of the Mughal Bengal, a major trading nation, and the establishment of the British India through the activities of the East India Company, in light of the arguments of Adam Smith (1723-1790) and of the classical economists.
The British Parliament's repeal of the Corn Laws under Robert Peel in 1846 symbolized the emergence of free trade as an alternative system.

Most of the European economists who wrote between 1500 and 1750 are today generally considered mercantilists; this term was initially used solely by critics, such as Mirabeau and Smith, but historians proved quick to adopt it. Originally the standard English term was "mercantile system". The word "mercantilism" came into English from German in the early-19th century.

The bulk of what is commonly called "mercantilist literature" appeared in the 1620s in Great Britain. Smith saw the English merchant Thomas Mun (1571–1641) as a major creator of the mercantile system, especially in his posthumously published "Treasure by Foreign Trade" (1664), which Smith considered the archetype or manifesto of the movement. Perhaps the last major mercantilist work was James Steuart's "Principles of Political Economy", published in 1767.

Mercantilist literature also extended beyond England. Italy and France produced noted writers of mercantilist themes, including Italy's Giovanni Botero (1544–1617) and Antonio Serra (1580–?) and, in France, Jean Bodin and Colbert. Themes also existed in writers from the German historical school from List, as well as followers of the American system and British free-trade imperialism, thus stretching the system into the 19th century. However, many British writers, including Mun and Misselden, were merchants, while many of the writers from other countries were public officials. Beyond mercantilism as a way of understanding the wealth and power of nations, Mun and Misselden are noted for their viewpoints on a wide range of economic matters.

The Austrian lawyer and scholar Philipp Wilhelm von Hornick, one of the pioneers of Cameralism, detailed a nine-point program of what he deemed effective national economy in his "Austria Over All, If She Only Will" of 1684, which comprehensively sums up the tenets of mercantilism:

Other than Von Hornick, there were no mercantilist writers presenting an overarching scheme for the ideal economy, as Adam Smith would later do for classical economics. Rather, each mercantilist writer tended to focus on a single area of the economy. Only later did non-mercantilist scholars integrate these "diverse" ideas into what they called mercantilism. Some scholars thus reject the idea of mercantilism completely, arguing that it gives "a false unity to disparate events". Smith saw the mercantile system as an enormous conspiracy by manufacturers and merchants against consumers, a view that has led some authors, especially Robert E. Ekelund and Robert D. Tollison, to call mercantilism "a rent-seeking society". To a certain extent, mercantilist doctrine itself made a general theory of economics impossible. Mercantilists viewed the economic system as a zero-sum game, in which any gain by one party required a loss by another. Thus, any system of policies that benefited one group would by definition harm the other, and there was no possibility of economics being used to maximize the commonwealth, or common good. Mercantilists' writings were also generally created to rationalize particular practices rather than as investigations into the best policies.

Mercantilist domestic policy was more fragmented than its trade policy. While Adam Smith portrayed mercantilism as supportive of strict controls over the economy, many mercantilists disagreed. The early modern era was one of letters patent and government-imposed monopolies; some mercantilists supported these, but others acknowledged the corruption and inefficiency of such systems. Many mercantilists also realized that the inevitable results of quotas and price ceilings were black markets. One notion that mercantilists widely agreed upon was the need for economic oppression of the working population; laborers and farmers were to live at the "margins of subsistence". The goal was to maximize production, with no concern for consumption. Extra money, free time, and education for the lower classes were seen to inevitably lead to vice and laziness, and would result in harm to the economy.

The mercantilists saw a large population as a form of wealth that made possible the development of bigger markets and armies. Opposite to mercantilism was the doctrine of physiocracy, which predicted that mankind would outgrow its resources. The idea of mercantilism was to protect the markets as well as maintain agriculture and those who were dependent upon it.

Mercantilist ideas were the dominant economic ideology of all of Europe in the early modern period, and most states embraced it to a certain degree. Mercantilism was centred on England and France, and it was in these states that mercantilist policies were most often enacted.

The policies have included:

Mercantilism arose in France in the early 16th century soon after the monarchy had become the dominant force in French politics. In 1539, an important decree banned the import of woolen goods from Spain and some parts of Flanders. The next year, a number of restrictions were imposed on the export of bullion.

Over the rest of the 16th century, further protectionist measures were introduced. The height of French mercantilism is closely associated with Jean-Baptiste Colbert, finance minister for 22 years in the 17th century, to the extent that French mercantilism is sometimes called Colbertism. Under Colbert, the French government became deeply involved in the economy in order to increase exports. Protectionist policies were enacted that limited imports and favored exports. Industries were organized into guilds and monopolies, and production was regulated by the state through a series of more than one thousand directives outlining how different products should be produced.

To encourage industry, foreign artisans and craftsmen were imported. Colbert also worked to decrease internal barriers to trade, reducing internal tariffs and building an extensive network of roads and canals. Colbert's policies were quite successful, and France's industrial output and the economy grew considerably during this period, as France became the dominant European power. He was less successful in turning France into a major trading power, and Britain and the Netherlands remained supreme in this field.

France imposed its mercantilist philosophy on its colonies in North America, especially New France. It sought to derive the maximum material benefit from the colony, for the homeland, with a minimum of imperial investment in the colony itself. The ideology was embodied in New France through the establishment under Royal Charter of a number of corporate trading monopolies including La Compagnie des Marchands, which operated from 1613 to 1621, and the Compagnie de Montmorency, from that date until 1627. It was in turn replaced by La Compagnie des Cent-Associés, created in 1627 by King Louis XIII, and the Communauté des habitants in 1643. These were the first corporations to operate in what is now Canada.

In England, mercantilism reached its peak during the Long Parliament government (1640–60). Mercantilist policies were also embraced throughout much of the Tudor and Stuart periods, with Robert Walpole being another major proponent. In Britain, government control over the domestic economy was far less extensive than on the Continent, limited by common law and the steadily increasing power of Parliament. Government-controlled monopolies were common, especially before the English Civil War, but were often controversial.

With respect to its colonies, British mercantilism meant that the government and the merchants became partners with the goal of increasing political power and private wealth, to the exclusion of other empires. The government protected its merchants—and kept others out—through trade barriers, regulations, and subsidies to domestic industries in order to maximize exports from and minimize imports to the realm. The government had to fight smuggling, which became a favorite American technique in the 18th century to circumvent the restrictions on trading with the French, Spanish, or Dutch. The goal of mercantilism was to run trade surpluses so that gold and silver would pour into London. The government took its share through duties and taxes, with the remainder going to merchants in Britain. The government spent much of its revenue on a superb Royal Navy, which not only protected the British colonies but threatened the colonies of the other empires, and sometimes seized them. Thus the British Navy captured New Amsterdam (New York) in 1664. The colonies were captive markets for British industry, and the goal was to enrich the mother country.

British mercantilist writers were themselves divided on whether domestic controls were necessary. British mercantilism thus mainly took the form of efforts to control trade. A wide array of regulations were put in place to encourage exports and discourage imports. Tariffs were placed on imports and bounties given for exports, and the export of some raw materials was banned completely. The Navigation Acts expelled foreign merchants from England's domestic trade. The nation aggressively sought colonies and once under British control, regulations were imposed that allowed the colony to only produce raw materials and to only trade with Britain. This led to friction with the inhabitants of these colonies, and mercantilist policies (such as forbidding trade with other empires and controls over smuggling) were a major irritant leading to the American Revolution.

Mercantilism taught that trade was a zero-sum game, with one country's gain equivalent to a loss sustained by the trading partner. Overall, however, mercantilist policies had a positive impact on Britain helping turn it into the world's dominant trader and the global hegemon. One domestic policy that had a lasting impact was the conversion of "wastelands" to agricultural use. Mercantilists believed that to maximize a nation's power, all land and resources had to be used to their highest and best use, and this era thus saw projects like the draining of The Fens.

The other nations of Europe also embraced mercantilism to varying degrees. The Netherlands, which had become the financial centre of Europe by being its most efficient trader, had little interest in seeing trade restricted and adopted few mercantilist policies. Mercantilism became prominent in Central Europe and Scandinavia after the Thirty Years' War (1618–48), with Christina of Sweden, Jacob Kettler of Courland, and Christian IV of Denmark being notable proponents.

The Habsburg Holy Roman Emperors had long been interested in mercantilist policies, but the vast and decentralized nature of their empire made implementing such notions difficult. Some constituent states of the empire did embrace Mercantilism, most notably Prussia, which under Frederick the Great had perhaps the most rigidly controlled economy in Europe.

Spain benefited from mercantilism early on as it brought a large amount of precious metals such as gold and silver into their treasury by way of the new world. In the long run, Spain's economy collapsed as it was unable to adjust to the inflation that came with the large influx of bullion. Heavy intervention from the crown put crippling laws for the protection of Spanish goods and services. Mercantilist protectionist policy in Spain caused the long-run failure of the Castilian textile industry as the efficiency severely dropped off with each passing year due to the production being held at a specific level. Spain's heavily protected industries led to famines as much of its agricultural land was required to be used for sheep instead of grain. Much of their grain was imported from the Baltic region of Europe which caused a shortage of food in the inner regions of Spain. Spain limiting the trade of their colonies is one of the causes that lead to the separation of the Dutch from the Spanish Empire. The culmination of all of these policies lead to Spain defaulting in 1557, 1575, and 1596.

During the economic collapse of the 17th century, Spain had little coherent economic policy, but French mercantilist policies were imported by Philip V with some success. Russia under Peter I (Peter the Great) attempted to pursue mercantilism, but had little success because of Russia's lack of a large merchant class or an industrial base.

Mercantilism was the economic version of warfare using economics as a tool for warfare by other means backed up by the state apparatus and was well suited to an era of military warfare. Since the level of world trade was viewed as fixed, it followed that the only way to increase a nation's trade was to take it from another. A number of wars, most notably the Anglo-Dutch Wars and the Franco-Dutch Wars, can be linked directly to mercantilist theories. Most wars had other causes but they reinforced mercantilism by clearly defining the enemy, and justified damage to the enemy's economy.

Mercantilism fueled the imperialism of this era, as many nations expended significant effort to conquer new colonies that would be sources of gold (as in Mexico) or sugar (as in the West Indies), as well as becoming exclusive markets. European power spread around the globe, often under the aegis of companies with government-guaranteed monopolies in certain defined geographical regions, such as the Dutch East India Company or the British Hudson's Bay Company (operating in present-day Canada).

With the establishment of overseas colonies by European powers early in the 17th century, mercantile theory gained a new and wider significance, in which its aim and ideal became both national and imperialistic.

Mercantilism as a weapon has continued to be used by nations through the 21st century by way of modern tariffs as it puts smaller economies in a position to conform to the larger economies goals or risk economic ruin due to an imbalance in trade. Trade wars are often dependent on such tariffs and restrictions hurting the opposing economy.

The term "mercantile system" was used by its foremost critic, Adam Smith, but Mirabeau (1715–1789) had used "mercantilism" earlier.

Mercantilism functioned as the economic counterpart of the older version of political power: divine right of kings and absolute monarchy.

Scholars debate over why mercantilism dominated economic ideology for 250 years. One group, represented by Jacob Viner, sees mercantilism as simply a straightforward, common-sense system whose logical fallacies remained opaque to people at the time, as they simply lacked the required analytical tools.

The second school, supported by scholars such as Robert B. Ekelund, portrays mercantilism not as a mistake, but rather as the best possible system for those who developed it. This school argues that rent-seeking merchants and governments developed and enforced mercantilist policies. Merchants benefited greatly from the enforced monopolies, bans on foreign competition, and poverty of the workers. Governments benefited from the high tariffs and payments from the merchants. Whereas later economic ideas were often developed by academics and philosophers, almost all mercantilist writers were merchants or government officials.

Monetarism offers a third explanation for mercantilism. European trade exported bullion to pay for goods from Asia, thus reducing the money supply and putting downward pressure on prices and economic activity. The evidence for this hypothesis is the lack of inflation in the British economy until the Revolutionary and Napoleonic Wars, when paper money came into vogue.

A fourth explanation lies in the increasing professionalisation and technification of the wars of the era, which turned the maintenance of adequate reserve funds (in the prospect of war) into a more and more expensive and eventually competitive business.

Mercantilism developed at a time of transition for the European economy. Isolated feudal estates were being replaced by centralized nation-states as the focus of power. Technological changes in shipping and the growth of urban centers led to a rapid increase in international trade. Mercantilism focused on how this trade could best aid the states. Another important change was the introduction of double-entry bookkeeping and modern accounting. This accounting made extremely clear the inflow and outflow of trade, contributing to the close scrutiny given to the balance of trade. Of course, the impact of the discovery of America cannot be ignored. New markets and new mines propelled foreign trade to previously inconceivable volumes, resulting in "the great upward movement in prices" and an increase in "the volume of merchant activity itself".

Prior to mercantilism, the most important economic work done in Europe was by the medieval scholastic theorists. The goal of these thinkers was to find an economic system compatible with Christian doctrines of piety and justice. They focused mainly on microeconomics and on local exchanges between individuals. Mercantilism was closely aligned with the other theories and ideas that began to replace the medieval worldview. This period saw the adoption of the very Machiavellian realpolitik and the primacy of the "raison d'état" in international relations. The mercantilist idea of all trade as a zero-sum game, in which each side was trying to best the other in a ruthless competition, was integrated into the works of Thomas Hobbes. This dark view of human nature also fit well with the Puritan view of the world, and some of the most stridently mercantilist legislation, such as the Navigation Ordinance of 1651, was enacted by the government of Oliver Cromwell.

Jean-Baptiste Colbert's work in 17th-century France came to exemplify classical mercantilism. In the English-speaking world, its ideas were criticized by Adam Smith with the publication of "The Wealth of Nations" in 1776 and later by David Ricardo with his explanation of comparative advantage. Mercantilism was rejected by Britain and France by the mid-19th century. The British Empire embraced free trade and used its power as the financial center of the world to promote the same. The Guyanese historian Walter Rodney describes mercantilism as the period of the worldwide development of European commerce, which began in the 15th century with the voyages of Portuguese and Spanish explorers to Africa, Asia, and the New World.

Adam Smith and David Hume were the founding fathers of anti-mercantilist thought. A number of scholars found important flaws with mercantilism long before Smith developed an ideology that could fully replace it. Critics like Hume, Dudley North and John Locke undermined much of mercantilism and it steadily lost favor during the 18th century.

In 1690, Locke argued that prices vary in proportion to the quantity of money. Locke's "Second Treatise" also points towards the heart of the anti-mercantilist critique: that the wealth of the world is not fixed, but is created by human labor (represented embryonically by Locke's labor theory of value). Mercantilists failed to understand the notions of absolute advantage and comparative advantage (although this idea was only fully fleshed out in 1817 by David Ricardo) and the benefits of trade.

For instance, imagine that Portugal was a more efficient producer of wine than England, yet in England, cloth could be produced more efficiently than it could in Portugal. Thus if Portugal specialized in wine and England in cloth, "both" states would end up "better off" if they traded. This is an example of the reciprocal benefits of trade (whether due to comparative or absolute advantage). In modern economic theory, trade is "not" a zero-sum game of cutthroat competition, because both sides can benefit from it.

Hume famously noted the impossibility of the mercantilists' goal of a constant positive balance of trade. As bullion flowed into one country, the supply would increase, and the value of bullion in that state would steadily decline relative to other goods. Conversely, in the state exporting bullion, its value would slowly rise. Eventually, it would no longer be cost-effective to export goods from the high-price country to the low-price country, and the balance of trade would reverse. Mercantilists fundamentally misunderstood this, long arguing that an increase in the money supply simply meant that everyone gets richer.

The importance placed on bullion was also a central target, even if many mercantilists had themselves begun to de-emphasize the importance of gold and silver. Adam Smith noted that at the core of the mercantile system was the "popular folly of confusing wealth with money", that bullion was just the same as any other commodity, and that there was no reason to give it special treatment. More recently, scholars have discounted the accuracy of this critique. They believe Mun and Misselden were not making this mistake in the 1620s, and point to their followers Josiah Child and Charles Davenant, who in 1699 wrote, "Gold and Silver are indeed the Measures of Trade, but that the Spring and Original of it, in all nations is the Natural or Artificial Product of the Country; that is to say, what this Land or what this Labour and Industry Produces." The critique that mercantilism was a form of rent seeking has also seen criticism, as scholars such Jacob Viner in the 1930s pointed out that merchant mercantilists such as Mun understood that they would not gain by higher prices for English wares abroad.

The first school to completely reject mercantilism was the physiocrats, who developed their theories in France. Their theories also had several important problems, and the replacement of mercantilism did not come until Adam Smith published "The Wealth of Nations" in 1776. This book outlines the basics of what is today known as classical economics. Smith spent a considerable portion of the book rebutting the arguments of the mercantilists, though often these are simplified or exaggerated versions of mercantilist thought.

Scholars are also divided over the cause of mercantilism's end. Those who believe the theory was simply an error hold that its replacement was inevitable as soon as Smith's more accurate ideas were unveiled. Those who feel that mercantilism amounted to rent-seeking hold that it ended only when major power shifts occurred. In Britain, mercantilism faded as the Parliament gained the monarch's power to grant monopolies. While the wealthy capitalists who controlled the House of Commons benefited from these monopolies, Parliament found it difficult to implement them because of the high cost of group decision making.

Mercantilist regulations were steadily removed over the course of the 18th century in Britain, and during the 19th century, the British government fully embraced free trade and Smith's laissez-faire economics. On the continent, the process was somewhat different. In France, economic control remained in the hands of the royal family, and mercantilism continued until the French Revolution. In Germany, mercantilism remained an important ideology in the 19th and early 20th centuries, when the historical school of economics was paramount.

Adam Smith rejected the mercantilist focus on production, arguing that consumption was paramount to production. He added that mercantilism was popular among merchants because it was what is now called rent seeking. John Maynard Keynes argued that encouraging production was just as important as encouraging consumption, and he favored the "new mercantilism". Keynes also noted that in the early modern period the focus on the bullion supplies was reasonable. In an era before paper money, an increase in bullion was one of the few ways to increase the money supply. Keynes said mercantilist policies generally improved both domestic and foreign investment—domestic because the policies lowered the domestic rate of interest, and investment by foreigners by tending to create a favorable balance of trade. Keynes and other economists of the 20th century also realized that the balance of payments is an important concern. Keynes also supported government intervention in the economy as necessity, as did mercantilism.

, the word "mercantilism" remains a pejorative term, often used to attack various forms of protectionism. The similarities between Keynesianism (and its successor ideas) and mercantilism have sometimes led critics to call them neo-mercantilism.

Paul Samuelson, writing within a Keynesian framework, wrote of mercantilism, "With employment less than full and Net National Product suboptimal, all the debunked mercantilist arguments turn out to be valid."

Some other systems that copy several mercantilist policies, such as Japan's economic system, are also sometimes called neo-mercantilist. In an essay appearing in the 14 May 2007 issue of "Newsweek", business columnist Robert J. Samuelson wrote that China was pursuing an essentially neo-mercantilist trade policy that threatened to undermine the post–World War II international economic structure.

Murray Rothbard, representing the Austrian School of economics, describes it this way:

In specific instances, protectionist mercantilist policies also had an important and positive impact on the state that enacted them. Adam Smith, for instance, praised the Navigation Acts, as they greatly expanded the British merchant fleet and played a central role in turning Britain into the world's naval and economic superpower from the 18th century onward. Some economists thus feel that protecting infant industries, while causing short-term harm, can be beneficial in the long term.





</doc>
<doc id="19709" url="https://en.wikipedia.org/wiki?curid=19709" title="Meat Puppets">
Meat Puppets

Meat Puppets are an American rock band formed in January 1980 in Phoenix, Arizona. The group's original lineup was Curt Kirkwood (guitar/vocals), his brother Cris Kirkwood (bass guitar/vocals), and Derrick Bostrom (drums). The Kirkwood brothers met Bostrom while attending Brophy Prep High School in Phoenix. The three then moved to Tempe, Arizona (a Phoenix suburb and home to Arizona State University), where the Kirkwood brothers purchased two adjacent homes, one of which had a shed in the back where they regularly practiced.

Meat Puppets started as a punk rock band, but like most of their labelmates on SST Records, they established their own unique style, blending punk with country and psychedelic rock, and featuring Curt's warbling vocals. Meat Puppets later gained significant exposure when the Kirkwood brothers served as guest musicians on Nirvana's MTV Unplugged performance in 1993. The band's 1994 album "Too High to Die" subsequently became their most successful release. The band broke up twice, in 1996 and 2002, but reunited again in 2006.

Meat Puppets have influenced a number of rock bands, including Nirvana, Soundgarden, Dinosaur Jr, Sebadoh, Pavement, and Jawbreaker.

In the late 1970s, drummer Derrick Bostrom played with guitarist Jack Knetzger in a band called Atomic Bomb Club, which began as a duo, but would come to include bassist Cris Kirkwood. The band played a few local shows and recorded some demos, but began to dissolve quickly thereafter. Derrick and Cris began rehearsing together with Cris' brother Curt Kirkwood by learning songs from Bostrom's collection of punk rock 45s. After briefly toying with the name "The Bastions of Immaturity", they settled on the name Meat Puppets in June, 1980 after a song by Curt of the same name which appears on their first album. Their earliest EP "In A Car" was made entirely of short hardcore punk with goofy lyrics, and attracted the attention of Joe Carducci as he was starting to work with legendary punk label SST Records. Carducci suggested they sign with the label, and Meat Puppets released their first album "Meat Puppets" in 1982, which among several new originals and a pair of heavily skewed Doc Watson and Bob Nolan covers, featured the songs "The Gold Mine" and "Melons Rising", two tunes Derrick and Cris originally had written and performed as Atomic Bomb Club previously. Years later, when the Meat Puppets reissued all of their albums in 1999, the five songs on In A Car would be combined with their debut album.
By the release of 1984's "Meat Puppets II", the bandmembers "were so sick of the hardcore thing," according to Bostrom. "We were really into pissing off the crowd." Here, the band experimented with acid rock and country and western sounds, while still retaining some punk influence on the tracks "Split Myself in Two" and "New Gods." This album contains some of the band's best known songs, such as "Lake of Fire" and "Plateau." While the album had been recorded in early 1983, the album's release was delayed for a year by SST. "Meat Puppets II" turned the band into one of the leading bands on SST Records, and along with the Violent Femmes, the Gun Club and others, helped establish the genre called "cow punk".

Meat Puppets II was followed by 1985's "Up on the Sun". The album's psychedelic sound resembled the folk-rock of The Byrds, while the songs still retained hardcore influences in the lengths of the songs and the tempos. Examples of this new style are the self titled track, "Enchanted Porkfist" and "Swimming Ground." "Up On The Sun" featured the Kirkwood brothers harmonizing their vocals for the first time. These two albums were mainstays of college and independent radio at that time.

During the rest of the 1980s, Meat Puppets remained on SST and released a series of albums while touring relentlessly. Between tours they would regularly play small shows in bars around the Phoenix area such as The Mason Jar (now The Rebel Lounge) and The Sun Club in Tempe. After the release of the hard-rock styled "Out My Way" EP in 1986, however, the band was briefly sidelined by an accident when Curt's finger was broken after being slammed in their touring van's door. The accident delayed the band's next album, the even more psychedelic "Mirage", until the next year. The final result included synthesizers and electronic drums, and as such was considered their most polished sounding album to date. The tour for Mirage lasted less than 6 months, as the band found it difficult to recreate many of this album's songs in a concert atmosphere.

Their next album, the ZZ-Top inspired "Huevos", came out less than six months afterward, in late summer of 1987. In stark contrast to its predecessor, "Huevos" was recorded in a swift, fiery fashion, with many first takes, and minimal second guessing. These recordings were completed in only a matter of days, and along with a few drawings and one of Curt's paintings taken from the wall to serve as cover art (a dish of three boiled eggs, a green pepper, and a bottle of Tabasco sauce), were all sent to SST shortly before the band returned to the road en route to their next gig. Curt revealed in an interview that one of the reasons for the album being called Huevos (meaning 'eggs' in Spanish) was because of the multitude of first-takers on the record, as similarly eggs can only be used once.

"Monsters" was released in 1989, featuring new elements to their sound with extended jams (such as "Touchdown King" and "Flight of the Fire Weasel") and heavy metal ("Attacked by Monsters"). This album was mostly motivated by the Meat Puppets' desire to attract the attention of a major label, as they were becoming frustrated with SST Records by this time.

As numerous bands from the seminal SST label and other kindred punk-oriented indies had before them, Meat Puppets grappled with the decision to switch to a major label. Two years after their final studio recording for SST, 1989's "Monsters", the trio released its major-label debut, "Forbidden Places", on the indie-friendly London Records. The band chose London Records because it was the first label that ZZ Top, one of their favorite bands, was signed to.

"Forbidden Places" combined many elements of the band's sounds over the years (cowpunk, psychedelia, riffy heavier rock) while some songs had a more laid back early alternative sound. Songs include "Sam" and "Whirlpool," and the title track. Despite being a fan favorite, Forbidden Places is now out of print, and as such it remains a highly sought-after collectible online.

In 1992 following his departure from the Red Hot Chili Peppers, guitarist John Frusciante auditioned for the band. Cris Kirkwood stated "He showed up with his guitar out of its case and barefoot. We were on a major label then, we just got signed, and those guys had blown up to where they were at and John needed to get out. John gets to our pad and we started getting ready to play and I said, 'You want to use my tuner?' He said, 'No, I'll bend it in.' It was so far out. Then we jammed but it didn't come to anything. Maybe he wasn't in the right place and we were a tight little unit. It just didn't quite happen but it could have worked."
In late 1993, Meat Puppets achieved mainstream popularity when Nirvana's Kurt Cobain, who became a fan after seeing them open for Black Flag in the ‘80s, invited Cris and Curt to join him on MTV Unplugged for acoustic performances of "Plateau", "Oh Me" and "Lake of Fire" (all originally from "Meat Puppets II"). The resulting album, "MTV Unplugged in New York," served as a swan song for Nirvana, as Cobain died less than 5 months after the concert. "Lake of Fire" became a cult favorite for its particularly wrenching vocal performance from Cobain. Subsequently, the Nirvana exposure and the strength of the single "Backwater" (their highest charting single) helped lift Meat Puppets to new commercial heights. The band's studio return was 1994's "Too High To Die", produced by Butthole Surfers guitarist Paul Leary. The album featured "Backwater", which reached #47 on the Billboard Hot 100, and a hidden-track update of "Lake of Fire." This album features a more straightforward alternative rock style, with occasional moments of pop, country and neo-psychedelic moments. "Too High To Die" earned the band a gold record (500,000 sold), outselling their previous records combined.

1995's "No Joke!" was the final album recorded by the original Meat Puppets lineup. Stylistically it is very similar to Too High to Die, although much heavier and with darker lyrics. Examples of this are the single "Scum" and "Eyeball," however the band's usual laid-back style is still heard on tracks like "Chemical Garden." Though the band's drug use included cocaine, heroin, LSD and many others, Cris' use of heroin and crack cocaine became so bad he rarely left his house except to obtain more drugs. At least two people (including his wife and one of his best friends) died of overdoses at his house in Tempe, AZ during this time. The Kirkwood brothers had always had a legendary appetite for illegal substances and during the tour to support "Too High To Die" with Stone Temple Pilots, the easy availability of drugs was too much for Cris. When it was over, he was severely addicted to cocaine and heroin. When their record label discovered Cris' addictions, support for No Joke! was subsequently dropped and it was met with poor sales figures.

Derrick recorded a solo EP under the moniker "Today's Sounds" in 1996, and later on in 1999 took charge of re-issuing the Puppets' original seven records on Rykodisc as well as putting out their first live album, "Live in Montana." Curt formed a new band in Austin, Texas called the Royal Neanderthal Orchestra, but they changed their name to Meat Puppets for legal reasons and released a promotional EP entitled "You Love Me" in 1999, "Golden Lies" in 2000 and "Live" in 2002. The line-up was Curt (voc/git), Kyle Ellison (voc/git), Andrew Duplantis (voc/bass) and Shandon Sahm (drums). Sahm's father was the legendary fiddler-singer-songwriter Doug Sahm of The Sir Douglas Quintet and Texas Tornados. The concluding track to "Classic Puppets" entitled "New Leaf" also dates from this incarnation of the band.

Around 2002, Meat Puppets dissolved after Duplantis left the band. Curt went on to release albums with the groups Eyes Adrift and Volcano. In 2005, he released his first solo album entitled "Snow".

Bassist Cris was arrested in December 2003 for attacking a security guard at the main post office in downtown Phoenix, AZ with the guard's baton. The guard shot Kirkwood in the stomach at least twice during the melee, causing serious gunshot injuries requiring major surgery. Kirkwood was subsequently denied bail, the judge citing Kirkwood's previous drug arrests and probation violations. He eventually went to prison at the Arizona state prison in Florence, Arizona for felony assault. He was released in July 2005.

Derrick Bostrom began a web site for the band about six months before the original trio stopped working together. The site went through many different permutations before it was essentially mothballed in 2003. In late 2005, Bostrom revamped it, this time as a "blog" for his recollections and as a place to share pieces of Meat Puppets history.

On March 24, 2006, Curt Kirkwood polled fans at his MySpace page with a bulletin that asked: "Question for all ! Would the original line up of Meat Puppets interest anyone ? Feedback is good – do you want a reunion!?" The response from fans was overwhelmingly positive within a couple of hours, leading to speculation of a full-blown Meat Puppets reunion in the near future. However, a post made by Derrick Bostrom on the official Meat Puppets site dismissed the notion.

In April 2006 "Billboard" reported that the Kirkwood brothers would reunite as Meat Puppets without original drummer Derrick Bostrom. Although Primus drummer Tim Alexander was announced as Bostrom's replacement, the position was later filled by Ted Marcus. The new lineup recorded a new full-length album, "Rise to Your Knees", in mid-to-late 2006. The album was released by Anodyne Records on July 17, 2007.

On January 20, 2007, Meat Puppets brothers performed two songs during an Army of Anyone concert, at La Zona Rosa in Austin, Texas. The first song was played with Curt Kirkwood and Cris Kirkwood along with Army of Anyone's Ray Luzier and Dean DeLeo. Then the second song was played with original members Curt and Cris Kirkwood and new Meat Puppets drummer Ted Marcus. This was in the middle of Army of Anyone's set, which they listed as "Meat Puppet Theatre" on the evening's set list. The band performed several new songs in March at the South by Southwest festival. On March 28, 2007, the band announced a West Coast tour through their MySpace page. This is the first tour with original bassist Cris in eleven years. The tour continued into the east coast and midwest later in 2007.

In 2008 they performed their classic second album live in its entirety at the ATP New York festival.

The band parted ways with Anodyne, signed to Megaforce and began recording new material in the winter of 2008. The resulting album, entitled "Sewn Together", was released on May 12, 2009.
In the summer of 2009 the band continued to tour across America. They appeared in Rochester, Minnesota outside in front of over 5,000 fans, after playing Summerfest in Milwaukee, Wisconsin the night prior. Meat Puppets performed at the 2009 Voodoo Music Experience in New Orleans over the Halloween weekend.
As of November 2009, Shandon Sahm was back as the drummer in Meat Puppets, replacing Ted Marcus. The band was chosen by Animal Collective to perform the album 'Up on the Sun' live in its entirety at the All Tomorrow's Parties festival that they curated in May 2011.

The band's thirteenth studio album, entitled "Lollipop", was released on April 12, 2011. The Dandies supported Meat Puppets on all European dates in 2011.

Meat Puppets have played several gigs in their hometown since 2009, such as the Marquee show in June 2011 with Dead Confederate.

As of early 2011 Elmo Kirkwood, son of Curt Kirkwood and nephew of Cris Kirkwood, was touring regularly with the band playing rhythm guitar.

Meat Puppets also contributed to Spin Magazine's exclusive album "", playing Nirvana's "Smells Like Teen Spirit".

In June 2012, a book titled "Too High to Die: Meet the Meat Puppets" by author Greg Prato was released, which featured all-new interviews with band members past and present and friends of the band (including Peter Buck, Kim Thayil, Scott Asheton, Mike Watt, and Henry Rollins, among others), and covered the band's entire career.

In October 2012, it was announced that the group had just completed recording new songs. "Rat Farm", the band's 14th album, was released in April 2013.

In March 2013, Meat Puppets opened for Dave Grohl's Sound City Players at the SXSW Festival in Austin, Texas.

In April 2014, Meat Puppets completed a tour with The Moistboyz, and in the summer of 2015, they toured with Soul Asylum. 

The Meat Puppets were picked to open for an 11 show tour as support of The Dean Ween Group in October 2016 after Curt Kirkwood and drummer Chuck Treece contribute to "The Deaner Album". Also the same year, Cris either produced and/or played with the following artists for Slope Records - The Exterminators, the Linecutters, and Sad Kid.

On August 17, 2017, original drummer Derrick Bostrom posted an update on his website derrickbostrom.net. He performed with Cris, Curt and Elmo Kirkwood at a concert honoring the Meat Puppets. It appears that, while Bostrom enjoyed himself, this was a one-off performance. 

On July 8, 2018, it was confirmed that Bostrom had replaced Sahm as the drummer for the band, and that keyboardist Ron Stabinsky had joined, as well.

The band released their 15th studio album, "Dusty Notes", on March 8, 2019.

On April 21, 2018 a fan-sponsored petition on MoveOn.org was initiated to induct the Meat Puppets into the Rock and Roll Hall of Fame.





</doc>
<doc id="19710" url="https://en.wikipedia.org/wiki?curid=19710" title="List of mathematics competitions">
List of mathematics competitions

Mathematics competitions or mathematical olympiads are competitive events where participants sit a mathematics test. These tests may require multiple choice or numeric answers, or a detailed written solution or proof.




















Generally, registering for these contests is based on the grade level of math at which the student works rather than the age or the enrolled grade of the student. Also normally only competitions where the participants write a full proof are called Mathematical Olympiads.






</doc>
<doc id="19711" url="https://en.wikipedia.org/wiki?curid=19711" title="Michael Polanyi">
Michael Polanyi

Michael Polanyi (; ; 11 March 1891 – 22 February 1976) was a Hungarian-British polymath, who made important theoretical contributions to physical chemistry, economics, and philosophy. He argued that positivism supplies a false account of knowing, which if taken seriously undermines humanity's highest achievements.

His wide-ranging research in physical science included chemical kinetics, x-ray diffraction, and adsorption of gases. He pioneered the theory of fibre diffraction analysis in 1921, and the dislocation theory of plastic deformation of ductile metals and other materials in 1934. He emigrated to Germany, in 1926 becoming a chemistry professor at the Kaiser Wilhelm Institute in Berlin, and then in 1933 to England, becoming first a chemistry professor, and then a social sciences professor at the University of Manchester. Two of his pupils, and his son John Charles Polanyi won Nobel Prizes in Chemistry. In 1944 Polanyi was elected to the Royal Society.

The contributions which Polanyi made to the social sciences include an understanding of tacit knowledge, and the concept of a polycentric spontaneous order to intellectual inquiry were developed in the context of his opposition to central planning.

Polanyi, born Mihály Pollacsek in Budapest, was the fifth child of Mihály and Cecília Pollacsek (born as Cecília Wohl), secular Jews from Ungvár (then in Hungary but now in Ukraine) and Wilno, then Russian Empire, respectively. His father's family were entrepreneurs, while his mother's father – Osher Leyzerovich Vol (1833 – after 1906) – was the senior teacher of Jewish history at the Vilna rabbinic seminary, from which he had graduated as a rabbi. The family moved to Budapest and Magyarized their surname to Polányi. His father built much of the Hungarian railway system, but lost most of his fortune in 1899 when bad weather caused a railway building project to go over budget. He died in 1905. Cecília Polányi established a salon that was well known among Budapest's intellectuals, and which continued until her death in 1939. His older brother was Karl Polanyi, the political economist and anthropologist, and his niece was Eva Zeisel, a world-renowned ceramist.

In 1909, after leaving his teacher-training secondary school (Mintagymnasium), Polanyi studied to be a physician, obtaining his medical diploma in 1914. He was an active member of the Galilei Society. With the support of Ignác Pfeifer, professor of chemistry at the József Technical University of Budapest, he obtained a scholarship to study chemistry at the Technische Hochschule in Karlsruhe, Germany. In the First World War, he served in the Austro-Hungarian army as a medical officer, and was sent to the Serbian front. While on sick-leave in 1916, he wrote a PhD thesis on adsorption. His research, which was encouraged by Albert Einstein, was supervised by Gusztáv Buchböck, and in 1919 the University of Budapest awarded him a doctorate.

In October 1918, Mihály Károlyi established the Hungarian Democratic Republic, and Polanyi became Secretary to the Minister of Health. When the Communists seized power in March 1919, he returned to medicine. When the Hungarian Soviet Republic was overthrown, Polanyi emigrated to Karlsruhe in Germany, and was invited by Fritz Haber to join the Kaiser Wilhelm Institut für Faserstoffchemie (fiber chemistry) in Berlin. In 1923 he converted to Christianity, and in a Roman Catholic ceremony married Magda Elizabeth Kemeny. In 1926 he became the professorial head of department of the Institut für Physikalische Chemie und Elektrochemie (now the Fritz Haber Institute). In 1929, Magda gave birth to their son John, who was awarded a Nobel Prize in chemistry in 1986. Their other son, George Polanyi, who predeceased him, became a well-known economist.

His experience of runaway inflation and high unemployment in Weimar Germany led Polanyi to become interested in economics. With the coming to power in 1933 of the Nazi party, he accepted a chair in physical chemistry at the University of Manchester. Two of his pupils, Eugene Wigner and Melvin Calvin went on to win a Nobel Prize. Because of his increasing interest in the social sciences, Manchester University created a new chair in Social Science (1948–58) for him.

In 1944 Polanyi was elected a member of the Royal Society, and on his retirement from the University of Manchester in 1958 he was elected a senior research fellow at Merton College, Oxford. In 1962 he was elected a foreign honorary member of the American Academy of Arts and Sciences.

Polanyi's scientific interests were extremely diverse, including work in chemical kinetics, x-ray diffraction, and the adsorption of gases at solid surfaces. He is also well known for his potential adsorption theory, which was disputed for quite some time. In 1921, he laid the mathematical foundation of fibre diffraction analysis. In 1934, Polanyi, at about the same time as G. I. Taylor and Egon Orowan, realised that the plastic deformation of ductile materials could be explained in terms of the theory of dislocations developed by Vito Volterra in 1905. The insight was critical in developing the field of solid mechanics.

In 1936, as a consequence of an invitation to give lectures for the Ministry of Heavy Industry in the USSR, Polanyi met Bukharin, who told him that in socialist societies all scientific research is directed to accord with the needs of the latest Five Year Plan. Polanyi noted what had happened to the study of genetics in the Soviet Union once the doctrines of Trofim Lysenko had gained the backing of the State. Demands in Britain, for example by the Marxist John Desmond Bernal, for centrally planned scientific research led Polanyi to defend the claim that science requires free debate. Together with John Baker, he founded the influential Society for Freedom in Science.

In a series of articles, re-published in "The Contempt of Freedom" (1940) and "The Logic of Liberty" (1951), Polanyi claimed that co-operation amongst scientists is analogous to the way agents co-ordinate themselves within a free market. Just as consumers in a free market determine the value of products, science is a spontaneous order that arises as a consequence of open debate amongst specialists. Science (contrary to the claims of Bukharin) flourishes when scientists have the liberty to pursue truth as an end in itself:

[S]cientists, freely making their own choice of problems and pursuing them in the light of their own personal judgment, are in fact co-operating as members of a closely knit organization.
Such self-co-ordination of independent initiatives leads to a joint result which is unpremeditated by any of those who bring it about.
Any attempt to organize the group ... under a single authority would eliminate their independent initiatives, and thus reduce their joint effectiveness to that of the single person directing them from the centre. It would, in effect, paralyse their co-operation.
He derived the phrase spontaneous order from Gestalt psychology, and it was adopted by the classical liberal economist Friederich Hayek, although the concept can be traced back to at least Adam Smith. Polanyi (unlike Hayek) argued that there are higher and lower forms of spontaneous order, and he asserted that defending scientific inquiry on utilitarian or sceptical grounds undermined the practice of science. He extends this into a general claim about free societies. Polanyi defends a free society not on the negative grounds that we ought to respect "private liberties", but on the positive grounds that "public liberties" facilitate our pursuit of objective ideals.

According to Polanyi, a free society that strives to be value-neutral undermines its own justification. But it is not enough for the members of a free society to believe that ideals such as truth, justice, and beauty, are objective, they also have to accept that they transcend our ability to wholly capture them. The objectivity of values must be combined with acceptance that all knowing is fallible.

In "Full Employment and Free Trade" (1948) Polanyi analyses the way money circulates around an economy, and in a monetarist analysis that, according to Paul Craig Roberts, was thirty years ahead of its time, he argues that a free market economy should not be left to be wholly self-adjusting. A central bank should attempt to moderate economic booms/busts via a strict/loose monetary policy.

In 1940, he produced a film, "Unemployment and money. The principles involved", perhaps the first film about economics.

In his book "Science, Faith and Society" (1946), Polanyi set out his opposition to a positivist account of science, noting that it ignores the role personal commitments play in the practice of science. Polanyi gave the Gifford Lectures in 1951–52 at Aberdeen, and a revised version of his lectures were later published as "Personal Knowledge" (1958). In this book Polanyi claims that all knowledge claims (including those that derive from rules) rely on personal judgements. He denies that a scientific method can yield truth mechanically. All knowing, no matter how formalised, relies upon commitments. Polanyi argued that the assumptions that underlie critical philosophy are not only false, they undermine the commitments that motivate our highest achievements. He advocates a fiduciary post-critical approach, in which we recognise that we believe more than we can prove, and know more than we can say. The literary critic Rita Felski has named Polanyi as an important precursor to the project of postcritique within literary studies.

A knower does not stand apart from the universe, but participates personally within it. Our intellectual skills are driven by passionate commitments that motivate discovery and validation. According to Polanyi, a great scientist not only identifies patterns, but also chooses significant questions likely to lead to a successful resolution. Innovators risk their reputation by committing to a hypothesis. Polanyi cites the example of Copernicus, who declared that the Earth revolves around the Sun. He claims that Copernicus arrived at the Earth's true relation to the Sun not as a consequence of following a method, but via "the greater intellectual satisfaction he derived from the celestial panorama as seen from the Sun instead of the Earth." His writings on the practice of science influenced Thomas Kuhn and Paul Feyerabend.

Polanyi rejected the claim by British Empiricists that experience can be reduced into sense data, but he also rejects the notion that "indwelling" within (sometimes incompatible) interpretative frameworks traps us within them. Our tacit awareness connects us, albeit fallibly, with reality. It supplies us with the context within which our articulations have meaning. Contrary to the views of his colleague and friend Alan Turing, whose work at the Victoria University of Manchester prepared the way for the first modern computer, he denied that minds are reducible to collections of rules. His work influenced the critique by Hubert Dreyfus of "First Generation" artificial intelligence.

It was while writing "Personal Knowledge" that he identified the "structure of tacit knowing". He viewed it as his most important discovery. He claimed that we experience the world by integrating our subsidiary awareness into a focal awareness. In his later work, for example his Terry Lectures, later published as "The Tacit Dimension" (1966), he distinguishes between the phenomenological, instrumental, semantic, and ontological aspects of tacit knowing, as discussed (but not necessarily identified as such) in his previous writing.

In "Life's irreducible structure" (1968), Polanyi argues that the information contained in the DNA molecule is not reducible to the laws of physics and chemistry. Although a DNA molecule cannot exist without physical properties, these properties are constrained by higher-level ordering principles. In "Transcendence and Self-transcendence" (1970), Polanyi criticises the mechanistic world view that modern science inherited from Galileo.

Polanyi advocates emergence i.e. the claim that there are several levels of reality and of causality. He relies on the assumption that boundary conditions supply degrees of freedom that, instead of being random, are determined by higher-level realities, whose properties are dependent on but distinct from the lower level from which they emerge. An example of a higher-level reality functioning as a downward causal force is consciousness – intentionality – generating meanings – intensionality.

Mind is a higher-level expression of the capacity of living organisms for discrimination. Our pursuit of self-set ideals such as truth and justice transforms our understanding of the world. The reductionistic attempt to reduce higher-level realities into lower-level realities generates what Polanyi calls a moral inversion, in which the higher is rejected with moral passion. Polanyi identifies it as a pathology of the modern mind and traces its origins to a false conception of knowledge; although it is relatively harmless in the formal sciences, that pathology generates nihilism in the humanities. Polanyi considered Marxism an example of moral inversion. The State, on the grounds of an appeal to the logic of history, uses its coercive powers in ways that disregard any appeals to morality.

Tacit knowledge, as distinct from explicit knowledge, is an influential term developed by Polanyi in "The Tacit Dimension" to describe the idea of know how, or the ability to do something, without necessarily being able to articulate it or even be aware of all the dimensions, for example being able to ride a bicycle or play a musical instrument.





</doc>
