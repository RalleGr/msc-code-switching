<doc id="4410" url="https://en.wikipedia.org/wiki?curid=4410" title="Brewing">
Brewing

Brewing is the production of beer by steeping a starch source (commonly cereal grains, the most popular of which is barley) in water and fermenting the resulting sweet liquid with yeast. It may be done in a brewery by a commercial brewer, at home by a homebrewer, or by a variety of traditional methods such as communally by the indigenous peoples in Brazil when making cauim. Brewing has taken place since around the 6th millennium BC, and archaeological evidence suggests that emerging civilizations, including ancient Egypt and Mesopotamia, brewed beer. Since the nineteenth century the brewing industry has been part of most western economies.

The basic ingredients of beer are water and a fermentable starch source such as malted barley. Most beer is fermented with a brewer's yeast and flavoured with hops. Less widely used starch sources include millet, sorghum and cassava. Secondary sources (adjuncts), such as maize (corn), rice, or sugar, may also be used, sometimes to reduce cost, or to add a feature, such as adding wheat to aid in retaining the foamy head of the beer. The most common starch source is ground cereal or "grist" - the proportion of the starch or cereal ingredients in a beer recipe may be called grist, grain bill, or simply mash ingredients.

Steps in the brewing process include malting, milling, mashing, lautering, boiling, fermenting, conditioning, filtering, and packaging. There are three main fermentation methods, warm, cool and spontaneous. Fermentation may take place in an open or closed fermenting vessel; a secondary fermentation may also occur in the cask or bottle. There are several additional brewing methods, such as Burtonisation, barrel-ageing, double dropping, and Yorkshire Square.

Brewing has taken place since around the 6th millennium BC, and archaeological evidence suggests emerging civilizations including ancient Egypt and Mesopotamia brewed beer. Descriptions of various beer recipes can be found in cuneiform (the oldest known writing) from ancient Mesopotamia. In Mesopotamia the brewer's craft was the only profession which derived social sanction and divine protection from female deities/goddesses, specifically: Ninkasi, who covered the production of beer, Siris, who was used in a metonymic way to refer to beer, and Siduri, who covered the enjoyment of beer. In pre-industrial times, and in developing countries, women are frequently the main brewers.

As almost any cereal containing certain sugars can undergo spontaneous fermentation due to wild yeasts in the air, it is possible that beer-like beverages were independently developed throughout the world soon after a tribe or culture had domesticated cereal. Chemical tests of ancient pottery jars reveal that beer was produced as far back as about 7,000 years ago in what is today Iran. This discovery reveals one of the earliest known uses of fermentation and is the earliest evidence of brewing to date. In Mesopotamia, the oldest evidence of beer is believed to be a 6,000-year-old Sumerian tablet depicting people drinking a beverage through reed straws from a communal bowl. A 3900-year-old Sumerian poem honouring Ninkasi, the patron goddess of brewing, contains the oldest surviving beer recipe, describing the production of beer from barley via bread. The invention of bread and beer has been argued to be responsible for humanity's ability to develop technology and build civilization. The earliest chemically confirmed barley beer to date was discovered at Godin Tepe in the central Zagros Mountains of Iran, where fragments of a jug, at least 5,000 years old was found to be coated with beerstone, a by-product of the brewing process. Beer may have been known in Neolithic Europe as far back as 5,000 years ago, and was mainly brewed on a domestic scale.

Ale produced before the Industrial Revolution continued to be made and sold on a domestic scale, although by the 7th century AD beer was also being produced and sold by European monasteries. During the Industrial Revolution, the production of beer moved from artisanal manufacture to industrial manufacture, and domestic manufacture ceased to be significant by the end of the 19th century. The development of hydrometers and thermometers changed brewing by allowing the brewer more control of the process, and greater knowledge of the results. Today, the brewing industry is a global business, consisting of several dominant multinational companies and many thousands of smaller producers ranging from brewpubs to regional breweries. More than 133 billion litres (35 billion gallons) are sold per year—producing total global revenues of $294.5 billion (£147.7 billion) in 2006.

The basic ingredients of beer are water; a starch source, such as malted barley, able to be fermented (converted into alcohol); a brewer's yeast to produce the fermentation; and a flavouring, such as hops, to offset the sweetness of the malt. A mixture of starch sources may be used, with a secondary saccharide, such as maize (corn), rice, or sugar, these often being termed adjuncts, especially when used as a lower-cost substitute for malted barley. Less widely used starch sources include millet, sorghum, and cassava root in Africa, potato in Brazil, and agave in Mexico, among others. The most common starch source is ground cereal or "grist" - the proportion of the starch or cereal ingredients in a beer recipe may be called grist, grain bill, or simply mash ingredients.

Beer is composed mostly of water. Regions have water with different mineral components; as a result, different regions were originally better suited to making certain types of beer, thus giving them a regional character. For example, Dublin has hard water well suited to making stout, such as Guinness; while Pilsen has soft water well suited to making pale lager, such as Pilsner Urquell. The waters of Burton in England contain gypsum, which benefits making pale ale to such a degree that brewers of pale ales will add gypsum to the local water in a process known as Burtonisation.


The starch source in a beer provides the fermentable material and is a key determinant of the strength and flavour of the beer. The most common starch source used in beer is malted grain. Grain is malted by soaking it in water, allowing it to begin germination, and then drying the partially germinated grain in a kiln. Malting grain produces enzymes that will allow conversion from starches in the grain into fermentable sugars during the mash process. Different roasting times and temperatures are used to produce different colours of malt from the same grain. Darker malts will produce darker beers.

Nearly all beer includes barley malt as the majority of the starch. This is because of its fibrous husk, which is important not only in the sparging stage of brewing (in which water is washed over the mashed barley grains to form the wort) but also as a rich source of amylase, a digestive enzyme that facilitates conversion of starch into sugars. Other malted and unmalted grains (including wheat, rice, oats, and rye, and, less frequently, maize (corn) and sorghum) may be used. In recent years, a few brewers have produced gluten-free beer made with sorghum with no barley malt for people who cannot digest gluten-containing grains like wheat, barley, and rye.


Hops are the female flower clusters or seed cones of the hop vine "Humulus lupulus", which are used as a flavouring and preservative agent in nearly all beer made today. Hops had been used for medicinal and food flavouring purposes since Roman times; by the 7th century in Carolingian monasteries in what is now Germany, beer was being made with hops, though it isn't until the thirteenth century that widespread cultivation of hops for use in beer is recorded. Before the thirteenth century, beer was flavoured with plants such as yarrow, wild rosemary, and bog myrtle, and other ingredients such as juniper berries, aniseed and ginger, which would be combined into a mixture known as gruit and used as hops are now used; between the thirteenth and the sixteenth century, during which hops took over as the dominant flavouring, beer flavoured with gruit was known as ale, while beer flavoured with hops was known as beer. Some beers today, such as "Fraoch" by the Scottish Heather Ales company and "Cervoise Lancelot" by the French Brasserie-Lancelot company, use plants other than hops for flavouring.

Hops contain several characteristics that brewers desire in beer: they contribute a bitterness that balances the sweetness of the malt; they provide floral, citrus, and herbal aromas and flavours; they have an antibiotic effect that favours the activity of brewer's yeast over less desirable microorganisms; and they aid in "head retention", the length of time that the foam on top of the beer (the beer head) will last. The preservative in hops comes from the lupulin glands which contain soft resins with alpha and beta acids. Though much studied, the preservative nature of the soft resins is not yet fully understood, though it has been observed that unless stored at a cool temperature, the preservative nature will decrease. Brewing is the sole major commercial use of hops.


Yeast is the microorganism that is responsible for fermentation in beer. Yeast metabolises the sugars extracted from grains, which produces alcohol and carbon dioxide, and thereby turns wort into beer. In addition to fermenting the beer, yeast influences the character and flavour.
The dominant types of yeast used to make beer are "Saccharomyces cerevisiae", known as ale yeast, and "Saccharomyces pastorianus", known as lager yeast; "Brettanomyces" ferments lambics, and "Torulaspora delbrueckii" ferments Bavarian weissbier. Before the role of yeast in fermentation was understood, fermentation involved wild or airborne yeasts, and a few styles such as lambics still use this method today. Emil Christian Hansen, a Danish biochemist employed by the Carlsberg Laboratory, developed pure yeast cultures which were introduced into the Carlsberg brewery in 1883, and pure yeast strains are now the main fermenting source used worldwide.


Some brewers add one or more clarifying agents to beer, which typically precipitate (collect as a solid) out of the beer along with protein solids and are found only in trace amounts in the finished product. This process makes the beer appear bright and clean, rather than the cloudy appearance of ethnic and older styles of beer such as wheat beers.

Examples of clarifying agents include isinglass, obtained from swim bladders of fish; Irish moss, a seaweed; kappa carrageenan, from the seaweed "kappaphycus"; polyclar (a commercial brand of clarifier); and gelatin. If a beer is marked "suitable for Vegans", it was generally clarified either with seaweed or with artificial agents, although the "Fast Cask" method invented by Marston's in 2009 may provide another method.

There are several steps in the brewing process, which may include malting, mashing, lautering, boiling, fermenting, conditioning, filtering, and packaging.

Malting is the process where barley grain is made ready for brewing. Malting is broken down into three steps in order to help to release the starches in the barley. First, during steeping, the grain is added to a vat with water and allowed to soak for approximately 40 hours. During germination, the grain is spread out on the floor of the germination room for around 5 days. The final part of malting is kilning when the malt goes through a very high temperature drying in a kiln; with gradual temperature increase over several hours. When kilning is complete, the grains are now termed malt, and they will be milled or crushed to break apart the kernels and expose the cotyledon, which contains the majority of the carbohydrates and sugars; this makes it easier to extract the sugars during mashing.

Mashing converts the starches released during the malting stage into sugars that can be fermented. The milled grain is mixed with hot water in a large vessel known as a mash tun. In this vessel, the grain and water are mixed together to create a cereal mash. During the mash, naturally occurring enzymes present in the malt convert the starches (long chain carbohydrates) in the grain into smaller molecules or simple sugars (mono-, di-, and tri-saccharides). This "conversion" is called saccharification which occurs between the temperatures . The result of the mashing process is a sugar-rich liquid or "wort", which is then strained through the bottom of the mash tun in a process known as lautering. Prior to lautering, the mash temperature may be raised to about (known as a mashout) to free up more starch and reduce mash viscosity. Additional water may be sprinkled on the grains to extract additional sugars (a process known as sparging).

The wort is moved into a large tank known as a "copper" or kettle where it is boiled with hops and sometimes other ingredients such as herbs or sugars. This stage is where many chemical reactions take place, and where important decisions about the flavour, colour, and aroma of the beer are made. The boiling process serves to terminate enzymatic processes, precipitate proteins, isomerize hop resins, and concentrate and sterilize the wort. Hops add flavour, aroma and bitterness to the beer. At the end of the boil, the hopped wort settles to clarify in a vessel called a "whirlpool", where the more solid particles in the wort are separated out.

After the whirlpool, the wort is drawn away from the compacted hop trub, and rapidly cooled via a heat exchanger to a temperature where yeast can be added. A variety of heat exchanger designs are used in breweries, with the most common a plate-style. Water or glycol run in channels in the opposite direction of the wort, causing a rapid drop in temperature. It is very important to quickly cool the wort to a level where yeast can be added safely as yeast is unable to grow in very high temperatures, and will start to die in temperatures above . After the wort goes through the heat exchanger, the cooled wort goes into a fermentation tank. A type of yeast is selected and added, or "pitched", to the fermentation tank. When the yeast is added to the wort, the fermenting process begins, where the sugars turn into alcohol, carbon dioxide and other components. When the fermentation is complete the brewer may rack the beer into a new tank, called a conditioning tank. Conditioning of the beer is the process in which the beer ages, the flavour becomes smoother, and flavours that are unwanted dissipate. After conditioning for a week to several months, the beer may be filtered and force carbonated for bottling, or fined in the cask.

Mashing is the process of combining a mix of milled grain (typically malted barley with supplementary grains such as corn, sorghum, rye or wheat), known as the "grist" or "grain bill", and water, known as "liquor", and heating this mixture in a vessel called a "mash tun". Mashing is a form of steeping, and defines the act of brewing, such as with making tea, sake, and soy sauce. Technically, wine, cider and mead are not brewed but rather vinified, as there is no steeping process involving solids. Mashing allows the enzymes in the malt to break down the starch in the grain into sugars, typically maltose to create a malty liquid called wort. There are two main methods – infusion mashing, in which the grains are heated in one vessel; and decoction mashing, in which a proportion of the grains are boiled and then returned to the mash, raising the temperature. Mashing involves pauses at certain temperatures (notably ), and takes place in a "mash tun" – an insulated brewing vessel with a false bottom. The end product of mashing is called a "mash".

Mashing usually takes 1 to 2 hours, and during this time the various temperature rests activate different enzymes depending upon the type of malt being used, its modification level, and the intention of the brewer. The activity of these enzymes convert the starches of the grains to dextrins and then to fermentable sugars such as maltose. A mash rest from activates various proteases, which break down proteins that might otherwise cause the beer to be hazy. This rest is generally used only with undermodified (i.e. undermalted) malts which are decreasingly popular in Germany and the Czech Republic, or non-malted grains such as corn and rice, which are widely used in North American beers. A mash rest at activates β-glucanase, which breaks down gummy β-glucans in the mash, making the sugars flow out more freely later in the process. In the modern mashing process, commercial fungal based β-glucanase may be added as a supplement. Finally, a mash rest temperature of is used to convert the starches in the malt to sugar, which is then usable by the yeast later in the brewing process. Doing the latter rest at the lower end of the range favours β-amylase enzymes, producing more low-order sugars like maltotriose, maltose, and glucose which are more fermentable by the yeast. This in turn creates a beer lower in body and higher in alcohol. A rest closer to the higher end of the range favours α-amylase enzymes, creating more higher-order sugars and dextrins which are less fermentable by the yeast, so a fuller-bodied beer with less alcohol is the result. Duration and pH variances also affect the sugar composition of the resulting wort.

Lautering is the separation of the wort (the liquid containing the sugar extracted during mashing) from the grains. This is done either in a mash tun outfitted with a false bottom, in a lauter tun, or in a mash filter. Most separation processes have two stages: first wort run-off, during which the extract is separated in an undiluted state from the spent grains, and sparging, in which extract which remains with the grains is rinsed off with hot water. The lauter tun is a tank with holes in the bottom small enough to hold back the large bits of grist and hulls (the ground or milled cereal). The bed of grist that settles on it is the actual filter. Some lauter tuns have provision for rotating rakes or knives to cut into the bed of grist to maintain good flow. The knives can be turned so they push the grain, a feature used to drive the spent grain out of the vessel. The mash filter is a plate-and-frame filter. The empty frames contain the mash, including the spent grains, and have a capacity of around one hectoliter. The plates contain a support structure for the filter cloth. The plates, frames, and filter cloths are arranged in a carrier frame like so: frame, cloth, plate, cloth, with plates at each end of the structure. Newer mash filters have bladders that can press the liquid out of the grains between spargings. The grain does not act like a filtration medium in a mash filter.

After mashing, the beer wort is boiled with hops (and other flavourings if used) in a large tank known as a "copper" or brew kettle – though historically the mash vessel was used and is still in some small breweries. The boiling process is where chemical reactions take place, including sterilization of the wort to remove unwanted bacteria, releasing of hop flavours, bitterness and aroma compounds through isomerization, stopping of enzymatic processes, precipitation of proteins, and concentration of the wort. Finally, the vapours produced during the boil volatilise off-flavours, including dimethyl sulfide precursors. The boil is conducted so that it is even and intense – a continuous "rolling boil". The boil on average lasts between 45 and 90 minutes, depending on its intensity, the hop addition schedule, and volume of water the brewer expects to evaporate. At the end of the boil, solid particles in the hopped wort are separated out, usually in a vessel called a "whirlpool".

Copper is the traditional material for the boiling vessel, because copper transfers heat quickly and evenly, and because the bubbles produced during boiling, and which would act as an insulator against the heat, do not cling to the surface of copper, so the wort is heated in a consistent manner. The simplest boil kettles are direct-fired, with a burner underneath. These can produce a vigorous and favourable boil, but are also apt to scorch the wort where the flame touches the kettle, causing caramelisation and making cleanup difficult. Most breweries use a steam-fired kettle, which uses steam jackets in the kettle to boil the wort. Breweries usually have a boiling unit either inside or outside of the kettle, usually a tall, thin cylinder with vertical tubes, called a calandria, through which wort is pumped.

At the end of the boil, solid particles in the hopped wort are separated out, usually in a vessel called a "whirlpool" or "settling tank". The whirlpool was devised by Henry Ranulph Hudston while working for the Molson Brewery in 1960 to utilise the so-called tea leaf paradox to force the denser solids known as "trub" (coagulated proteins, vegetable matter from hops) into a cone in the centre of the whirlpool tank. Whirlpool systems vary: smaller breweries tend to use the brew kettle, larger breweries use a separate tank, and design will differ, with tank floors either flat, sloped, conical or with a cup in the centre. The principle in all is that by swirling the wort the centripetal force will push the trub into a cone at the centre of the bottom of the tank, where it can be easily removed.

A hopback is a traditional additional chamber that acts as a sieve or filter by using whole hops to clear debris (or "trub") from the unfermented (or "green") wort, as the whirlpool does, and also to increase hop aroma in the finished beer. It is a chamber between the brewing kettle and wort chiller. Hops are added to the chamber, the hot wort from the kettle is run through it, and then immediately cooled in the wort chiller before entering the fermentation chamber. Hopbacks utilizing a sealed chamber facilitate maximum retention of volatile hop aroma compounds that would normally be driven off when the hops contact the hot wort. While a hopback has a similar filtering effect as a whirlpool, it operates differently: a whirlpool uses centrifugal forces, a hopback uses a layer of whole hops to act as a filter bed. Furthermore, while a whirlpool is useful only for the removal of pelleted hops (as flowers do not tend to separate as easily), in general hopbacks are used only for the removal of whole flower hops (as the particles left by pellets tend to make it through the hopback). The hopback has mainly been substituted in modern breweries by the whirlpool.

After the whirlpool, the wort must be brought down to fermentation temperatures before yeast is added. In modern breweries this is achieved through a plate heat exchanger. A plate heat exchanger has many ridged plates, which form two separate paths. The wort is pumped into the heat exchanger, and goes through every other gap between the plates. The cooling medium, usually water, goes through the other gaps. The ridges in the plates ensure turbulent flow. A good heat exchanger can drop wort to while warming the cooling medium from about to . The last few plates often use a cooling medium which can be cooled to below the freezing point, which allows a finer control over the wort-out temperature, and also enables cooling to around . After cooling, oxygen is often dissolved into the wort to revitalize the yeast and aid its reproduction. Some of the craft brewery, particularly those wanting to create steam beer, utilize coolship instead.

While boiling, it is useful to recover some of the energy used to boil the wort. On its way out of the brewery, the steam created during the boil is passed over a coil through which unheated water flows. By adjusting the rate of flow, the output temperature of the water can be controlled. This is also often done using a plate heat exchanger. The water is then stored for later use in the next mash, in equipment cleaning, or wherever necessary. Another common method of energy recovery takes place during the wort cooling. When cold water is used to cool the wort in a heat exchanger, the water is significantly warmed. In an efficient brewery, cold water is passed through the heat exchanger at a rate set to maximize the water's temperature upon exiting. This now-hot water is then stored in a hot water tank.

Fermentation takes place in fermentation vessels which come in various forms, from enormous cylindroconical vessels, through open stone vessels, to wooden vats. After the wort is cooled and aerated – usually with sterile air – yeast is added to it, and it begins to ferment. It is during this stage that sugars won from the malt are converted into alcohol and carbon dioxide, and the product can be called beer for the first time.

Most breweries today use cylindroconical vessels, or CCVs, which have a conical bottom and a cylindrical top. The cone's aperture is typically around 60°, an angle that will allow the yeast to flow towards the cone's apex, but is not so steep as to take up too much vertical space. CCVs can handle both fermenting and conditioning in the same tank. At the end of fermentation, the yeast and other solids which have fallen to the cone's apex can be simply flushed out of a port at the apex. Open fermentation vessels are also used, often for show in brewpubs, and in Europe in wheat beer fermentation. These vessels have no tops, which makes harvesting top-fermenting yeasts very easy. The open tops of the vessels make the risk of infection greater, but with proper cleaning procedures and careful protocol about who enters fermentation chambers, the risk can be well controlled. Fermentation tanks are typically made of stainless steel. If they are simple cylindrical tanks with beveled ends, they are arranged vertically, as opposed to conditioning tanks which are usually laid out horizontally. Only a very few breweries still use wooden vats for fermentation as wood is difficult to keep clean and infection-free and must be repitched more or less yearly.

There are three main fermentation methods, warm, cool and wild or spontaneous. Fermentation may take place in open or closed vessels. There may be a secondary fermentation which can take place in the brewery, in the cask or in the bottle.

Brewing yeasts are traditionally classed as "top-cropping" (or "top-fermenting") and "bottom-cropping" (or "bottom-fermenting"); the yeasts classed as top-fermenting are generally used in warm fermentations, where they ferment quickly, and the yeasts classed as bottom-fermenting are used in cooler fermentations where they ferment more slowly. Yeast were termed top or bottom cropping, because the yeast was collected from the top or bottom of the fermenting wort to be reused for the next brew. This terminology is somewhat inappropriate in the modern era; after the widespread application of brewing mycology it was discovered that the two separate collecting methods involved two different yeast species that favoured different temperature regimes, namely "Saccharomyces cerevisiae" in top-cropping at warmer temperatures and "Saccharomyces pastorianus" in bottom-cropping at cooler temperatures. As brewing methods changed in the 20th century, cylindro-conical fermenting vessels became the norm and the collection of yeast for both "Saccharomyces" species is done from the bottom of the fermenter. Thus the method of collection no longer implies a species association. There are a few remaining breweries who collect yeast in the top-cropping method, such as Samuel Smiths brewery in Yorkshire, Marstons in Staffordshire and several German hefeweizen producers.

For both types, yeast is fully distributed through the beer while it is fermenting, and both equally flocculate (clump together and precipitate to the bottom of the vessel) when fermentation is finished. By no means do all top-cropping yeasts demonstrate this behaviour, but it features strongly in many English yeasts that may also exhibit chain forming (the failure of budded cells to break from the mother cell), which is in the technical sense different from true flocculation. The most common top-cropping brewer's yeast, "Saccharomyces cerevisiae", is the same species as the common baking yeast. However, baking and brewing yeasts typically belong to different strains, cultivated to favour different characteristics: baking yeast strains are more aggressive, in order to carbonate dough in the shortest amount of time; brewing yeast strains act slower, but tend to tolerate higher alcohol concentrations (normally 12–15% abv is the maximum, though under special treatment some ethanol-tolerant strains can be coaxed up to around 20%). Modern quantitative genomics has revealed the complexity of "Saccharomyces" species to the extent that yeasts involved in beer and wine production commonly involve hybrids of so-called pure species. As such, the yeasts involved in what has been typically called top-cropping or top-fermenting ale may be both "Saccharomyces cerevisiae" and complex hybrids of "Saccharomyces cerevisiae" and "Saccharomyces kudriavzevii". Three notable ales, Chimay, Orval and Westmalle, are fermented with these hybrid strains, which are identical to wine yeasts from Switzerland.

In general, yeasts such as "Saccharomyces cerevisiae" are fermented at warm temperatures between , occasionally as high as , while the yeast used by Brasserie Dupont for saison ferments even higher at . They generally form a foam on the surface of the fermenting beer, which is called barm, as during the fermentation process its hydrophobic surface causes the flocs to adhere to CO and rise; because of this, they are often referred to as "top-cropping" or "top-fermenting" – though this distinction is less clear in modern brewing with the use of cylindro-conical tanks. Generally, warm-fermented beers, which are usually termed ale, are ready to drink within three weeks after the beginning of fermentation, although some brewers will condition or mature them for several months.

When a beer has been brewed using a cool fermentation of around , compared to typical warm fermentation temperatures of , then stored (or lagered) for typically several weeks (or months) at temperatures close to freezing point, it is termed a "lager". During the lagering or storage phase several flavour components developed during fermentation dissipate, resulting in a "cleaner" flavour. Though it is the slow, cool fermentation and cold conditioning (or lagering) that defines the character of lager, the main technical difference is with the yeast generally used, which is "Saccharomyces pastorianus". Technical differences include the ability of lager yeast to metabolize melibiose, and the tendency to settle at the bottom of the fermenter (though ales yeasts can also become bottom settling by selection); though these technical differences are not considered by scientists to be influential in the character or flavour of the finished beer, brewers feel otherwise - sometimes cultivating their own yeast strains which may suit their brewing equipment or for a particular purpose, such as brewing beers with a high abv.

Brewers in Bavaria had for centuries been selecting cold-fermenting yeasts by storing ("lagern") their beers in cold alpine caves. The process of natural selection meant that the wild yeasts that were most cold tolerant would be the ones that would remain actively fermenting 
in the beer that was stored in the caves. A sample of these Bavarian yeasts was sent from the Spaten brewery in Munich to the Carlsberg brewery in Copenhagen in 1845 who began brewing with it. In 1883 Emile Hansen completed a study on pure yeast culture isolation and the pure strain obtained from Spaten went into industrial production in 1884 as Carlsberg yeast No 1. Another specialized pure yeast production plant was installed at the Heineken Brewery in Rotterdam the following year and together they began the supply of pure cultured yeast to brewers across Europe. This yeast strain was originally classified as "Saccharomyces carlsbergensis", a now defunct species name which has been superseded by the currently accepted taxonomic classification "Saccharomyces pastorianus".

Lambic beers are historically brewed in Brussels and the nearby Pajottenland region of Belgium without any yeast inoculation. The wort is cooled in open vats (called "coolships"), where the yeasts and microbiota present in the brewery (such as "Brettanomyces") are allowed to settle to create a spontaneous fermentation, and are then conditioned or matured in oak barrels for typically one to three years.

After an initial or primary fermentation, beer is "conditioned", matured or aged, in one of several ways, which can take from 2 to 4 weeks, several months, or several years, depending on the brewer's intention for the beer. The beer is usually transferred into a second container, so that it is no longer exposed to the dead yeast and other debris (also known as "trub") that have settled to the bottom of the primary fermenter. This prevents the formation of unwanted flavours and harmful compounds such as acetaldehyde.

Kräusening is a conditioning method in which fermenting wort is added to the finished beer. The active yeast will restart fermentation in the finished beer, and so introduce fresh carbon dioxide; the conditioning tank will be then sealed so that the carbon dioxide is dissolved into the beer producing a lively "condition" or level of carbonation. The kräusening method may also be used to condition bottled beer.

Lagers are stored at cellar temperature or below for 1–6 months while still on the yeast. The process of storing, or conditioning, or maturing, or aging a beer at a low temperature for a long period is called "lagering", and while it is associated with lagers, the process may also be done with ales, with the same result – that of cleaning up various chemicals, acids and compounds.

During secondary fermentation, most of the remaining yeast will settle to the bottom of the second fermenter, yielding a less hazy product.

Some beers undergo an additional fermentation in the bottle giving natural carbonation. This may be a second and/or third fermentation. They are bottled with a viable yeast population in suspension. If there is no residual fermentable sugar left, sugar or wort or both may be added in a process known as priming. The resulting fermentation generates CO that is trapped in the bottle, remaining in solution and providing natural carbonation. Bottle-conditioned beers may be either filled unfiltered direct from the fermentation or conditioning tank, or filtered and then reseeded with yeast.

Cask ale (or cask-conditioned beer) is unfiltered, unpasteurised beer that is conditioned by a secondary fermentation in a metal, plastic or wooden cask. It is dispensed from the cask by being either poured from a tap by gravity, or pumped up from a cellar via a beer engine (hand pump). Sometimes a cask breather is used to keep the beer fresh by allowing carbon dioxide to replace oxygen as the beer is drawn off the cask. Until 2018, the Campaign for Real Ale (CAMRA) defined real ale as beer "served without the use of extraneous carbon dioxide", which would disallow the use of a cask breather, a policy which was reversed in April 2018 to allow beer served with the use of cask breathers to meet its definition of real ale.

Barrel-ageing (US: Barrel aging) is the process of ageing beer in wooden barrels to achieve a variety of effects in the final product. Sour beers such as lambics are fully fermented in wood, while other beers are aged in barrels which were previously used for maturing wines or spirits. In 2016 "Craft Beer and Brewing" wrote: "Barrel-aged beers are so trendy that nearly every taphouse and beer store has a section of them.

Filtering the beer stabilizes the flavour, and gives beer its polished shine and brilliance. Not all beer is filtered. When tax determination is required by local laws, it is typically done at this stage in a calibrated tank. There are several forms of filters, they may be in the form of sheets or "candles", or they may be a fine powder such as diatomaceous earth, also called kieselguhr. The powder is added to the beer and recirculated past screens to form a filtration bed.

Filters range from rough filters that remove much of the yeast and any solids (e.g., hops, grain particles) left in the beer, to filters tight enough to strain colour and body from the beer. Filtration ratings are divided into rough, fine, and sterile. Rough filtration leaves some cloudiness in the beer, but it is noticeably clearer than unfiltered beer. Fine filtration removes almost all cloudiness. Sterile filtration removes almost all microorganisms.

These filters use sheets that allow only particles smaller than a given size to pass through. The sheets are placed into a filtering frame, sanitized (with boiling water, for example) and then used to filter the beer. The sheets can be flushed if the filter becomes blocked. The sheets are usually disposable and are replaced between filtration sessions. Often the sheets contain powdered filtration media to aid in filtration.

Pre-made filters have two sides. One with loose holes, and the other with tight holes. Flow goes from the side with loose holes to the side with the tight holes, with the intent that large particles get stuck in the large holes while leaving enough room around the particles and filter medium for smaller particles to go through and get stuck in tighter holes.

Sheets are sold in nominal ratings, and typically 90% of particles larger than the nominal rating are caught by the sheet.

Filters that use a powder medium are considerably more complicated to operate, but can filter much more beer before regeneration. Common media include diatomaceous earth and perlite.

Brewing by-products are "spent grain" and the sediment (or "dregs") from the filtration process which may be dried and resold as "brewers dried yeast" for poultry feed, or made into yeast extract which is used in brands such as Vegemite and Marmite. The process of turning the yeast sediment into edible yeast extract was discovered by German scientist Justus von Liebig.

Brewer's spent grain (also called spent grain, brewer's grain or draff) is the main by-product of the brewing process; it consists of the residue of malt and grain which remains in the lauter tun after the lautering process. It consists primarily of grain husks, pericarp, and fragments of endosperm. As it mainly consists of carbohydrates and proteins, and is readily consumed by animals, spent grain is used in animal feed. Spent grains can also be used as fertilizer, whole grains in bread, as well as in the production of flour and biogas. Spent grain is also an ideal medium for growing mushrooms, such as shiitake, and already some breweries are either growing their own mushrooms or supplying spent grain to mushroom farms. Spent grains can be used in the production of red bricks, to improve the open porosity and reduce thermal conductivity of the ceramic mass.

The brewing industry is a global business, consisting of several dominant multinational companies and many thousands of smaller producers known as microbreweries or regional breweries depending on size and region. More than are sold per year—producing total global revenues of $294.5 billion (£147.7 billion) as of 2006. SABMiller became the largest brewing company in the world when it acquired Royal Grolsch, brewer of Dutch premium beer brand Grolsch. InBev was the second-largest beer-producing company in the world and Anheuser-Busch held the third spot, but after the acquisition of Anheuser-Busch by InBev, the new Anheuser-Busch InBev company is currently the largest brewer in the world.

Brewing at home is subject to regulation and prohibition in many countries. Restrictions on homebrewing were lifted in the UK in 1963, Australia followed suit in 1972, and the US in 1978, though individual states were allowed to pass their own laws limiting production.




</doc>
<doc id="4417" url="https://en.wikipedia.org/wiki?curid=4417" title="Benz">
Benz

Benz, an old Germanic clan name dating to the fifth century (related to "bear", "war banner", "gau", or a "land by a waterway") also used in German as an alternative for names such as Berthold, Bernhard, or Benedict, may refer to:







</doc>
<doc id="4419" url="https://en.wikipedia.org/wiki?curid=4419" title="Breast reconstruction">
Breast reconstruction

Breast reconstruction is the surgical process of rebuilding the shape and look of a breast, most commonly in women who have had surgery to treat breast cancer. It involves using autologous tissue, prosthetic implants, or a combination of both with the goal of reconstructing a natural-looking breast. This process often also includes the rebuilding of the nipple and areola, known as nipple-areola complex (NAC) reconstruction, as one of the final stages.

Generally, the aesthetic appearance is acceptable to the woman, but the reconstructed area is commonly completely numb afterwards, which results in loss of sexual function as well as the ability to perceive pain caused by burns and other injuries.

Breast reconstruction can be performed either immediately following the mastectomy or as a separate procedure at a later date, known as immediate reconstruction and delayed reconstruction, respectively. The decision of when breast reconstruction will take place is patient-specific and based on many different factors. Breast reconstruction is a large undertaking that usually requires multiple operations. These subsequent surgeries may be spread out over weeks or months.

Breast reconstruction is termed "immediate" when it takes place during the same procedure as the mastectomy. Within the United States, approximately 35% of women who have undergone a total mastectomy for breast cancer will choose to pursue immediate breast reconstruction. One of the inherent advantages of immediate reconstruction is the potential for a single-stage procedure. This also means that the cost of immediate reconstruction is often far less to the patient. It can also reduce hospital costs by having fewer procedures and requiring a shorter length of the stay as an inpatient. Additionally, immediate reconstruction often has a better cosmetic result because of the preservation of anatomic landmarks and skin. With regards to psychosocial outcomes, opinions on timing have shifted in favor of immediate reconstruction. Originally, delayed reconstruction was believed to provide patients with time to psychologically adjust to the mastectomy and its effects on body image. However, this opinion is no longer widely held. Compared to delayed procedures, immediate reconstruction can have a more positive psychological impact on patients and their self-esteem, most likely due to the post-operative breast more closely resembling the natural breast compared to the defect left by mastectomy alone.

Delayed breast reconstruction is considered more challenging than immediate reconstruction. Frequently not just breast volume, but also skin surface area needs to be restored. Many patients undergoing delayed breast reconstruction have been previously treated with radiation or have had a reconstruction failure with immediate breast reconstruction. In nearly all cases of delayed breast reconstruction tissue must be borrowed from another part of the body to make the new breast. Patients expected to receive radiation therapy as part of their adjuvant treatment are also commonly considered for delayed autologous reconstruction due to significantly higher complication rates with tissue expander-implant techniques in those patients. While waiting to begin breast reconstruction until several months after radiation therapy may decrease the risk of complications, this risk will always be higher in patients who have received radiation therapy. As with many other surgeries, patients with significant medical comorbidities (e.g., high blood pressure, obesity, diabetes) and smokers are higher-risk candidates. Surgeons may choose to perform delayed reconstruction to decrease this risk.

There are several techniques for breast reconstruction. These options are broadly categorized into two different groups:

This is the most common technique used worldwide. Implant-based reconstruction is an option for patients who have sufficient skin after mastectomy to cover a prosthetic implant and allow for a natural shape. For women undergoing bilateral mastectomies, implants provide the greatest opportunity for symmetrical shape and lift. Additionally, these procedures are generally much faster than flap-based reconstruction since tissue does not have to be taken from another part of the patient's body.

Typically, at the time of the mastectomy, the surgeon will insert a tissue expander underneath the pectoralis major muscle of the chest wall. This temporary silastic implant is used to hold tension on the mastectomy flaps. In doing so, the tissue expander prevents the breast tissue from contracting and allows for use of a larger implant later on compared to what would be safe at the time of the mastectomy. Following this initial procedure, the patient must return to the clinic on multiple occasions for saline to be injected into a tube inside the tissue expander. By doing this slowly over the course of several weeks, the space beneath the pectoralis major muscle is safely expanded to an appropriate size without causing too much stress on the breast tissue. A second procedure is then necessary to remove the tissue expander and replace it with the final, permanent prosthetic implant.

Although in the past, prosthetic implants were placed directly under the skin, this method has fallen out of favor because of the greater risk of complications, including visible rippling of the implant and capsular contracture. The sub-pectoral technique described above is now preferred because it provides an additional muscular layer between the skin and the implant, decreasing the risk of visible deformity. Oftentimes, however, the pectoralis major muscle is not sufficiently large enough to cover the inferior portion of the prosthetic implant. If this is the case, one option is to use an acellular dermal matrix to cover the exposed portion of the prosthetic implant, improving both functional and aesthetic outcomes. This prepectoral space has recently, however, come back into practice. Both delayed and direct-to-implant reconstruction in this plane has been shown to be favourable.

Of note, a Cochrane review published in 2016 concluded that implants for use in breast reconstructive surgery have not been adequately studied in good quality clinical trials. "These days - even after a few million women have had breasts reconstructed – surgeons cannot inform women about the risks and complications of different implant-based breast reconstructive options on the basis of results derived from Randomized Controlled Trials."

Flap-based reconstruction uses tissue from other parts of the patient's body (i.e., autologous tissue) such as the back, buttocks, thigh or abdomen. In surgery, a "flap" is any type of tissue that is lifted from a donor site and moved to a recipient site using its own blood supply. Usually, the blood supply is a named vessel. Flap-based reconstruction may be performed either by leaving the donor tissue connected to the original site (also known as a pedicle flap) to retain its blood supply (where the vessels are tunneled beneath the skin surface to the new site) or by cutting the donor tissue's vessels and surgically reconnecting them to a new blood supply at the recipient site (also known as a free flap or free tissue transfer).

One option for breast reconstruction involves using the latissimus dorsi muscle as the donor tissue. As a back muscle, the latissimus dorsi is large and flat and can be used without significant loss of function. It can be moved into the breast defect while still attached to its blood supply under the arm pit (axilla). A latissimus flap is often used to recruit soft-tissue coverage over an underlying implant; however, if the latissimus flap can provide enough volume, then occasionally it is used to reconstruct small breasts without the need for an implant. The latissimus dorsi flap has a number of advantages, but despite the advances in surgical techniques, it has remained vulnerable to skin dehiscence or necrosis at the donor site (on the back). The Mannu flap is a form of latissimus dorsi flap which avoids this complication by preserving a generous subcutaneous fat layer at the donor site and has been shown to be a safe, simple and effective way of avoiding wound dehiscence at the donor site after extended latissimus dorsi flap reconstruction.

Another possible donor site for breast reconstruction is the abdomen. The TRAM (transverse rectus abdominis myocutaneous) flap or its technically distinct variants of microvascular "perforator flaps" like the DIEP/SIEA flaps are all commonly used. In a TRAM procedure, a portion of the abdominal tissue, which includes skin, subcutaneous fat, minor muscles, and connective tissues, is taken from the patient's abdomen and transplanted to the breast site. Both TRAM and DIEP/SIEA use the abdominal tissue between the umbilicus (or "belly button") and the pubis. The DIEP flap and free-TRAM flap require advanced microsurgical technique and are less common as a result. Both can provide enough tissue to reconstruct large breasts and are a good option for patients who would prefer to maintain their pre-operative breast volume. These procedures are preferred by some breast cancer patients because removal of the donor site tissue results in an abdominoplasty (tummy tuck) and allow the breast to be reconstructed with one's own tissues instead of a prosthetic implant that uses foreign material. That said, TRAM flap procedures can potentially weaken the abdominal wall and torso strength, but they are generally well tolerated by most patients. Perforator techniques such as the DIEP (deep inferior epigastric perforator) flap and SIEA (superficial inferior epigastric artery) flap require precise dissection of small perforating vessels through the rectus muscle and, thus, do not require removal of abdominal muscle. Because of this, these flaps have the advantage of maintaining the majority of abdominal wall strength.

Other donor sites for autologous breast reconstruction include the buttocks, which provides tissue for the SGAP and IGAP (superior and inferior gluteal artery perforator, respectively) flaps. The purpose of perforator flaps (DIEP, SIEA, SGAP, IGAP) is to provide sufficient skin and fat for an aesthetic reconstruction while minimizing post-operative complications from harvesting the underlying muscles. DIEP reconstruction generally produces the best outcome for most women. See free flap breast reconstruction for more information.

Mold-assisted reconstruction is a potential adjunctive process to help in flap-based reconstruction. By using a laser and 3D printer, a patient-specific silicone mold can be used as an aid during surgery, used as a guide for orienting and shaping the flap to improve accuracy and symmetry.

To restore the appearance of the pre-operative breast, there are a few options regarding the nipple-areolar complex (NAC):

When looking at the entire process of breast reconstruction, patients typically report that NAC reconstruction is the least satisfying step. Compared to a normal nipple, the reconstructed nipple often has less projection (how far the nipple extends beyond the breast mound) and lacks sensation. In women who have undergone a single mastectomy with reconstruction, another challenge in aesthetically matching the reconstructed NAC to the native breast.

The typical outcome of breast reconstruction surgery is a breast mound with a pleasing aesthetic shape, with a texture similar to a natural breast, but which feels completely or mostly numb for the woman herself. This loss of sensation, called "somatosensory loss" or the inability to perceive touch, heat, cold, and pain, sometimes results in women burning themselves or injuring themselves without noticing, or not noticing that their clothing has shifted to expose their breasts. "I can't even feel it when my kids hug me," said one mother, who had nipple-sparing breast reconstruction after a bilateral mastectomy. The loss of sensation has long-term medical consequences, because it makes the affected women unable to feel itchy rashes, infected sores, cuts, bruises, or situations that risk sunburns or frostbite on the affected areas.

More than half of women treated for breast cancer develop upper quarter dysfunction, including limits on how well they can move, pain in the breast, shoulder or arm, lymphedema, loss of sensation, and impaired strength. The risk of dysfunction is higher among women who have breast reconstruction surgery. One in three have complications, one in five need further surgery and the procedure fails in 5%.

Some methods have specific side effects. The transverse rectus abdominis myocutaneous (TRAM) flap method results in weakness and loss of flexibility in the abdominal wall. Reconstruction with implants have a higher risk of long-term pain.

Outcomes-based research on quality of life improvements and psychosocial benefits associated with breast reconstruction served as the stimulus in the United States for the 1998 Women's Health and Cancer Rights Act, which mandated that health care payer cover breast and nipple reconstruction, contralateral procedures to achieve symmetry, and treatment for the sequelae of mastectomy. This was followed in 2001 by additional legislation imposing penalties on noncompliant insurers. Similar provisions for coverage exist in most countries worldwide through national health care programs.




</doc>
<doc id="4424" url="https://en.wikipedia.org/wiki?curid=4424" title="Bob Diamond">
Bob Diamond

Bob Diamond may refer to:




</doc>
<doc id="4425" url="https://en.wikipedia.org/wiki?curid=4425" title="Brooklyn Historic Railway Association">
Brooklyn Historic Railway Association

The Brooklyn Historic Railway Association (BHRA) is a 501(c)(3) nonprofit organization with a shop, trolley barn and offices located in Red Hook, Brooklyn, New York, on the historic Beard Street Piers (c. 1870). BHRA had a fleet of 16 trolleys (15 PCC trolleys and a leased 1897 trolley car from the Oslo Trams, in Oslo, Norway). 

The BHRA's origin began with the rediscovery of the Cobble Hill Tunnel by Bob Diamond in 1980. BHRA was formed in 1982 to restore the historic tunnel. The Atlantic Avenue Tunnel (constructed in 1844) is the world's oldest subway tunnel. BHRA successfully filed and received designation for the tunnel on the National Register of Historic Places.

The BHRA received funding and permission from the city to construct a light rail route in Red Hook. However, the project was hampered due to the New York City Department of Transportation (DOT) withdrawing its support from the project. The DOT identified several potential improvements which did not include a streetcar however, that would improve access and mobility for neighborhood residents. Construction was stopped on a 7-block extension to the line due to the removal and scrapping of rails, ties, and other items of railroad equipment by the DOT, which were stored on land that was slated for the Fairway supermarket project.

On June 30, 2003, BHRA was ordered to remove and fill in all trolley tracks on public streets by the DOT. The DOT revoked consent for the project to proceed or exist on city streets. Shortly thereafter, BHRA completely ceased operation. All the PCC trolleys except for No. 3303 were removed from Brooklyn.
The BHRA ran organized tours of the Cobble Hill Tunnel from time to time, but all tours are currently suspended.




</doc>
<doc id="4427" url="https://en.wikipedia.org/wiki?curid=4427" title="Beta-lactam">
Beta-lactam

A beta-lactam (β-lactam) ring is a four-membered lactam. A "lactam" is a cyclic amide, and "beta"-lactams are named so because the nitrogen atom is attached to the β-carbon atom relative to the carbonyl. The simplest β-lactam possible is 2-azetidinone. β-lactams are significant structural units of medicines.

The β-lactam ring is part of the core structure of several antibiotic families, the principal ones being the penicillins, cephalosporins, carbapenems, and monobactams, which are, therefore, also called β-lactam antibiotics. Nearly all of these antibiotics work by inhibiting bacterial cell wall biosynthesis. This has a lethal effect on bacteria, although any given bacteria population will typically contain a subgroup that is resistant to β-lactam antibiotics. Bacterial resistance occurs as a result of the expression of one of many genes for the production of β-lactamases, a class of enzymes that break open the β-lactam ring. More than 1,800 different β-lactamase enzymes have been documented in various species of bacteria. These enzymes vary widely in their chemical structure and catalytic efficiencies. When bacterial populations have these resistant subgroups, treatment with β-lactam can result in the resistant strain becoming more prevalent and therefore more virulent. β-lactam derived antibiotics can be considered as one of the most important antibiotic classes but prone to clinical resistance. β-lactam exhibits its antibiotic properties by imitating the naturally occurring d-Ala-d-Ala substrate for the group of enzymes known as penicillin binding proteins (PBP), which have as function to cross-link the peptidoglycan part of the cell wall of the bacteria.

The first synthetic β-lactam was prepared by Hermann Staudinger in 1907 by reaction of the Schiff base of aniline and benzaldehyde with diphenylketene in a [2+2] cycloaddition (Ph indicates a phenyl functional group):

Up to 1970, most β-lactam research was concerned with the penicillin and cephalosporin groups, but since then, a wide variety of structures have been described.

Many methods have been developed for the synthesis of β-lactams.

Breckpot synthesis: The synthesis of substituted β-lactams from the cyclization of beta amino acid esters using the Grignard reagent.

Due to ring strain, β-lactams are more readily hydrolyzed than linear amides or larger lactams. This strain is further increased by fusion to a second ring, as found in most β-lactam antibiotics. This trend is due to the amide character of the β-lactam being reduced by the aplanarity of the system. The nitrogen atom of an ideal amide is sp-hybridized due to resonance, and sp-hybridized atoms have trigonal planar bond geometry. As a pyramidal bond geometry is forced upon the nitrogen atom by the ring strain, the resonance of the amide bond is reduced, and the carbonyl becomes more ketone-like. Nobel laureate Robert Burns Woodward described a parameter "h" as a measure of the height of the trigonal pyramid defined by the nitrogen (as the apex) and its three adjacent atoms. "h" corresponds to the strength of the β-lactam bond with lower numbers (more planar; more like ideal amides) being stronger and less reactive. Monobactams have "h" values between 0.05 and 0.10 angstroms (Å). Cephems have "h" values in of 0.20–0.25 Å. Penams have values in the range 0.40–0.50 Å, while carbapenems and clavams have values of 0.50–0.60 Å, being the most reactive of the β-lactams toward hydrolysis.

A new study has suggested that β-lactams can undergo ring-opening polymerization to form amide bonds, to become nylon-3 polymers. The backbones of these polymers are identical to peptides, which offer them biofunctionality. These nylon-3 polymers can either mimic host defense peptides or act as signals to stimulate 3T3 stem cell function.

Antiproliferative agents that target tubulin with β-lactams in their structure have also been reported.




</doc>
<doc id="4429" url="https://en.wikipedia.org/wiki?curid=4429" title="Prince-Bishopric of Brandenburg">
Prince-Bishopric of Brandenburg

The Prince-Bishopric of Brandenburg () was an ecclesiastical principality of the Holy Roman Empire from the 12th century until it was secularized during the second half of the 16th century. It should not be confused with the larger Diocese of Brandenburg () established by King Otto I of Germany in 948, in the territory of the "Marca Geronis" (Saxon Eastern March) east of the Elbe river. The diocese, over which the prince-bishop exercised only spiritual authority, was a suffragan diocese of the Archdiocese of Magdeburg, its seat was Brandenburg an der Havel.

The Prince-Bishopric of Brandenburg was an imperial estate of the Holy Roman Empire for some time, probably starting about 1161/1165. However, the Brandenburg bishops never managed to gain control over a significant territory, being overshadowed by the Margraviate of Brandenburg, which was originally seated in the same city. Chapter and cathedral, surrounded by further ecclesiastical institutions, were located on the "Dominsel" (Cathedral Island), which formed a prince-episcopal cathedral immunity district ("Domfreiheit"), distinct from the city of Brandenburg. Only in 1929 the - meanwhile former - immunity district was incorporated into the city itself.

The foundation charter of the Brandenburg diocese is dated 1 October 948, though the actual founding date remained disputed among historians. The medieval chronicler Thietmar of Merseburg mentions the year 938; the bishopric may also have been established in the course of the partition of the vast "Marca Geronis" and the emergence of the Northern March after Margrave Gero's death in 965. With the foundation, King Otto (Holy Roman Emperor from 962) aimed at the Christianization of the Polabian Slavs (Wends) and the incorporation of their territory into the East Frankish realm.

Brandenburg was originally a suffragan of the Archbishopric of Mainz, but in 968 it came under the jurisdiction of the Magdeburg archbishops. The Great Slav Rising of 983 practically annihilated it, when revolting Lutici tribes conquered Brandenburg and the neighbouring Bishopric of Havelberg. Brandenburg bishops continued to be appointed, but they were merely titular, residing in Magdeburg or acting as auxiliary bishops in the western territories of the Empire. Not until the final subjugation of the Wends in the 12th century by Margrave Albert the Bear, the German eastward settlement ("Ostsiedlung") in the diocesan region revived the bishopric.

Bishop Wigers of Brandenburg (acting 1138–60), an adherent of Norbert of Xanten, was the first of a series of bishops of the Premonstratensian Order, which chose the occupants of the episcopal see until 1447; in that year a bull of Pope Nicholas V gave the right of nomination to the Brandenburg elector, with whom the bishops stood in a close feudal relation. Bishop Wigers also established a Premonstratensian convent at Leitzkau (today part of Gommern, Saxony-Anhalt). Probably at the request of the Hevellian prince Pribislav-Henry, he established another convent at the Slavic "Parduin" settlement in present-day Brandenburg an der Havel, which became the nucleus of the revived Brandenburg cathedral chapter. The incorporation into the Premonstratensian Order was confirmed by Pope Clement III in 1188.
As rulers of imperial immediacy, regnant in a, however, dispersed territory partitioned into the four bailiwicks () of Brandenburg/Havel, Ketzin, Teltow and Ziesar. The prince-bishops from the early 14th century onwards resided in their fortress in Ziesar on the road to Magdeburg. The last actual bishop was Matthias von Jagow (d. 1544), who took the side of the Protestant Reformation, married, and in every way furthered the undertakings of the Hohenzollern elector Joachim II.

There were two more nominal bishops, but on the petition of the latter of these, the electoral prince John George of Brandenburg appointed in 1560, the secularisation of the bishopric was undertaken and finally accomplished in 1571, in spite of legal proceedings to reassert the imperial immediacy of the prince-bishopric within the Empire and so to likewise preserve the diocese, which dragged on into the 17th century.



Secularized and merged into Brandenburg.



</doc>
<doc id="4430" url="https://en.wikipedia.org/wiki?curid=4430" title="BASE jumping">
BASE jumping

BASE jumping is the recreational sport of jumping from fixed objects, using a parachute to descend safely to the ground.
"BASE" is an acronym that stands for four categories of fixed objects from which one can jump: building, antenna, span, and earth (cliff).
Participants exit from a fixed object such as a cliff, and after an optional freefall delay, deploy a parachute to slow their descent and land.
A popular form of BASE jumping is wingsuit BASE jumping.

In contrast to other forms of parachuting, such as skydiving from airplanes, BASE jumps are performed from fixed objects which are generally much lower altitude, and BASE jumpers typically carry only one parachute.
BASE jumping is significantly more hazardous than other forms of parachuting, and is widely considered to be one of the most dangerous extreme sports.

Fausto Veranzio is widely believed to have been the first person to build and test a parachute, by jumping from St Mark's Campanile in Venice in 1617 when over sixty-five years old.
However, these and other sporadic incidents were one-time experiments, not the systematic pursuit of a new form of parachuting.

The acronym "B.A.S.E." (now more commonly "BASE") was coined by filmmaker Carl Boenish, his wife Jean Boenish, Phil Smith, and Phil Mayfield. Carl Boenish was the catalyst behind modern BASE jumping, and in 1978, he filmed the first BASE jumps which were made using ram-air parachutes and the freefall tracking technique (from El Capitan in Yosemite National Park). While BASE jumps had been made prior to that time, the El Capitan activity was the effective birth of what is now called BASE jumping.

After 1978, the filmed jumps from El Capitan were repeated, not as a publicity exercise or as a movie stunt, but as a true recreational activity. It was this that popularized BASE jumping more widely among parachutists. Carl Boenish continued to publish films and informational magazines on BASE jumping until his death in 1984 after a BASE jump off the Troll Wall. By this time, the concept had spread among skydivers worldwide, with hundreds of participants making fixed-object jumps.

During the early eighties, nearly all BASE jumps were made using standard skydiving equipment, including two parachutes (main and reserve), and deployment components. Later on, specialized equipment and techniques were developed specifically for the unique needs of BASE jumping.

BASE numbers are awarded to those who have made at least one jump from each of the four categories (buildings, antennas, spans and earth). When Phil Smith and Phil Mayfield jumped together from a Houston skyscraper on 18 January 1981, they became the first to attain the exclusive BASE numbers (BASE #1 and #2, respectively), having already jumped from an antenna, spans, and earthen objects. Jean and Carl Boenish qualified for BASE numbers 3 and 4 soon after. A separate "award" was soon enacted for Night BASE jumping when Mayfield completed each category at night, becoming Night BASE #1, with Smith qualifying a few weeks later.
Upon completing a jump from all of the four object categories, a jumper may choose to apply for a "BASE number", awarded sequentially. The 1000th application for a BASE number was filed in March 2005 and BASE #1000 was awarded to Matt "Harley" Moilanen of Grand Rapids, Michigan. , over 2,000 BASE numbers have been issued.

In the early days of BASE jumping, people used modified skydiving gear, such as by removing the deployment bag and slider, stowing the lines in a tail pocket, and fitting a large pilot chute. However, modified skydiving gear is then prone to kinds of malfunction that are rare in normal skydiving (such as "line-overs" and broken lines). Modern purpose-built BASE jumping equipment is considered to be much safer and more reliable.

The biggest difference in gear is that while skydivers jump with a main and a reserve parachute, BASE jumpers typically jump with only one parachute. Additionally, specialized containers focus on simplicity to avoid some of the malfunctions that can occur with more complicated skydiving equipment. BASE jumpers often use extra large pilot chutes to compensate for the lower airspeed parachute deployments. Since BASE jumpers use a single parachute system, there is little need to cut-away their parachute, many BASE harnesses do not contain a 3-ring release system. On lower jumps, the slider is removed for faster openings. A modern ultralight BASE system including parachute, container, and harness can weigh as little as .

When jumping from high mountains, BASE jumpers will often use special clothing to improve control and flight characteristics in the air. Wingsuit flying has become a popular form of BASE jumping in recent years, that allows jumpers to glide over long horizontal distances. Tracking suits inflate like wingsuits to give additional lift to jumpers, but maintain separation of arms and legs to allow for greater mobility and safety.

BASE jumps can be broadly classified into low jumps and high jumps. The primary distinguishing characteristic of low BASE jumps versus high BASE jumps is the use of a slider reefing device to control the opening speed of the parachute, and whether the jumper falls long enough to reach terminal velocity.

Low BASE jumps are those where the jumper does not reach terminal velocity. Sometimes referred to as "slider down" jumps because they are typically performed without a slider reefing device on the parachute. The lack of a slider enables the parachute to open more quickly. Other techniques for low BASE jumps includes the use of a static line, direct bag, or PCA (pilot chute assist). These devices form an attachment between the parachute and the jump platform, which stretches out the parachute and suspension lines as the jumper falls, before separating and allowing the parachute to inflate. This enables the very lowest jumps—below —to be made.

Many BASE jumpers are motivated to make jumpers from higher jumps objects involving free fall. High BASE jumps are those which are high enough for the jumper to reach terminal velocity. High BASE jumps are often called "slider up" jumps due to the use of a slider reefing device.
High BASE jumps present different hazards than low BASE jumps. With greater height and airspeed, jumpers can fly away from the cliff during freefall, allowing them to deploy their parachute far away from the cliff they jumped from and significantly reduce the chance of object striking. However, high BASE jumps also present new hazards such as complications resulting from use of a wingsuit.

Tandem BASE jumping is when a skilled pilot jumps with a passenger attached to their front. It's similar to skydiving and is offered in the US. Tandem BASE is becoming a more accessible and legal form of BASE jumping. 

Felix Baumgartner jumped from Christ the Redeemer statue in Rio de Janeiro and claimed the world record for the lowest BASE jump ever, jumping from .

"Guinness World Records" first listed a BASE jumping record with Carl Boenish's 1984 leap from Trollveggen (Troll Wall) in Norway. It was described as the highest BASE jump. The jump was made two days before Boenish's death at the same site.

On August 26, 1992, Australians Nic Feteris and Glenn Singleman made a BASE jump from an altitude of 20,600 feet (6286 meters) jump off Great Trango Towers Pakistan. It was the world's highest BASE jump off the earth at the time.

On 5 May 2013, Russian Valery Rozov jumped off Changtse (the northern peak of the Mount Everest massif) from a height of . Using a specially-developed Red Bull wingsuit, he glided down to the Rongbuk glacier more than 1,000 meters below, setting a new world record for highest altitude base jump. He had previously jumped off mountains in Asia, Antarctica and South America in 2004, 2007, 2008, 2010 and 2012. 

On 5 October 2016, Rozov broke his own record for highest altitude BASE jump when he leapt from a height of from Cho Oyu, the sixth-highest mountain in the world. He fell for around 90 seconds before opening his parachute, landing on a glacier approximately two minutes later at an altitude of around . He later died while attempting another high altitude BASE jump in Nepal in 2017.

Other records include Captain Daniel G. Schilling setting the Guinness World Record for the most BASE jumps in a twenty-four-hour period. Schilling jumped off the Perrine Bridge in Twin Falls, Idaho, a record 201 times on July 8, 2006. In 2018 at Eikesdalen, Norway a world record was set with 69 BASE jumpers jumping from the cliff Katthammaren.

BASE competitions have been held since the early 1980s, with accurate landings or free fall aerobatics used as the judging criteria. Recent years have seen a formal competition held at the high Petronas Towers in Kuala Lumpur, Malaysia, judged on landing accuracy.
In 2012 the World Wingsuit League held their first wingsuit BASE jumping competition in China.


BASE jumps are typically performed from much lower altitudes than in skydiving. Skydivers are required to deploy their main parachute above altitude.
BASE jumps are frequently made from less than . A BASE jump from a object is only about 5.6 seconds from the ground if the jumper remains in free fall. Standard skydiving parachute systems are not designed for this situation, so BASE jumpers use specially designed harnesses and parachute systems.

BASE jumps generally entail slower airspeeds than typical skydives (due to the limited altitude), a BASE jumper does not always reach terminal velocity. Skydivers use the air flow to stabilize their position. BASE jumpers, falling at lower speeds, have less aerodynamic control. The attitude of the body at the moment of jumping determines the stability of flight in the first few seconds, before sufficient airspeed has built up to enable aerodynamic stability. On low BASE jumps, parachute deployment takes place during this early phase of flight. If the parachute is deployed while the jumper is unstable, there is a high risk of entanglement or malfunction. The jumper may also not be facing the right direction. Such an off-heading opening is not as problematic in skydiving, but an off-heading opening that results in object strike has caused many serious injuries and deaths in BASE jumping.

BASE jumps are more hazardous than skydives primarily due to proximity to the object serving as the jump platform. BASE jumping frequently occurs in mountainous terrain, often having much smaller areas in which to land in comparison to a typical skydiving dropzone. BASE jumping is significantly more dangerous than similar sports such as skydiving from aircraft.

BASE jumping is generally not illegal in most places. However, in some cases such as building and antenna jumps, jumping is often done covertly without the permission of owners, which can lead to charges such as trespassing. In some jurisdictions it may be permissible to use land until specifically told not to. The Perrine Bridge in Twin Falls, Idaho, is an example of a man-made structure in the United States where BASE jumping is allowed year-round without a permit.
In U.S. National Parks, BASE jumping is generally prohibited, unless special permission is given.
Other U.S. public land, including land controlled by the Bureau of Land Management, does not ban air delivery, and there are numerous jumpable objects on BLM land.

The legal position is different at other sites and in other countries. For example, in Norway's Lysefjord (from the mountain Kjerag), BASE jumpers are made welcome. Many sites in the European Alps, near Chamonix and on the Eiger, are also open to jumpers. Some other Norwegian places, like the Troll Wall, are banned because of dangerous rescue missions in the past. In Austria, jumping from mountain cliffs is generally allowed, whereas the use of bridges (such as the Europabruecke near Innsbruck, Tirol) or dams is generally prohibited. Australia has some of the toughest stances on BASE jumping: it specifically bans BASE jumping from certain objects, such as the Sydney Harbour Bridge.

The National Park Service has banned BASE jumping in U.S. National Parks. The authority comes from 36 CFR 2.17(3), which prohibits, "Delivering or retrieving a person or object by parachute, helicopter, or other airborne means, except in emergencies involving public safety or serious property loss, or pursuant to the terms and conditions of a permit." Under that Regulation, BASE is not banned, but is allowable if a permit is issued by the Superintendent. The 2001 National Park Service Management Policies state that BASE "is not an appropriate public use activity within national park areas ..." (2001 Management Policy 8.2.2.7.) However, Policy 8.2.2.7 in the 2006 volume of National Park Service Management Policies, which superseded the 2001 edition, states "Parachuting (or BASE jumping), whether from an aircraft, structure, or natural feature, is generally prohibited by 36 CFR 2.17(a)(3). However, if determined through a park planning process to be an appropriate activity, it may be allowed pursuant to the terms and conditions of a permit."

Once a year, on the third Saturday in October ("Bridge Day"), permission to BASE jump has explicitly been granted at the New River Gorge Bridge in Fayetteville, West Virginia. The New River Gorge Bridge deck is 876 feet (267 m) above the river. This annual event attracts about 450 BASE jumpers and nearly 200,000 spectators. 1,100 jumps may occur during the six hours that it's legal provided good conditions.

During the early days of BASE jumping, the NPS issued permits that authorized jumps from El Capitan. This program ran for three months in 1980 and then collapsed amid allegations of abuse by unauthorized jumpers. The NPS has since vigorously enforced the ban, charging jumpers with "aerial delivery into a National Park". One jumper drowned in the Merced River while evading arresting park rangers, having declared "No way are they gonna get me. Let them chase me—I'll just laugh in their faces and jump in the river". Despite incidents like this one, illegal jumps continue in Yosemite at a rate estimated at a few hundred per year, often at night or dawn. El Capitan, Half Dome, and Glacier Point have been used as jump sites.

A study of BASE jumping fatalities estimated that the overall annual fatality risk in 2002 was one fatality per 60 participants. A study of 20,850 BASE jumps from the Kjerag Massif in Norway reported nine fatalities over the 11-year period from 1995 to 2005, or one in every 2,317 jumps. However, at that site, one in every 254 jumps over that period resulted in a nonfatal accident. BASE jumping is one of the most dangerous recreational activities in the world, with a fatality and injury rate 43 times higher than that of parachuting from a plane.

, the BASE Fatality List records 383 deaths for BASE jumping since April 1981.

BASE jumping is often featured in action movies. 




</doc>
<doc id="4431" url="https://en.wikipedia.org/wiki?curid=4431" title="Beauty">
Beauty

Beauty is the ascription of a property or characteristic to a person, object, animal, place or idea that provides a perceptual experience of pleasure or satisfaction. Beauty is studied as part of aesthetics, culture, social psychology and sociology.

The experience of "beauty" often involves an interpretation of some entity as being in balance and harmony with nature, which may lead to feelings of attraction and emotional well-being. Because this can be a subjective experience, it is often said that "beauty is in the eye of the beholder." Often, given the observation that empirical observations of things that are considered beautiful often align among groups in consensus, beauty has been stated to have levels of objectivity and partial subjectivity which are not fully subjective in their aesthetic judgement. Ugliness is the opposite of beauty.

The word "beauty" is often used as a countable noun to describe a beautiful woman, an excellent example of something, or a pleasing feature of something.

The classical Greek noun that best translates to the English-language words "beauty" or "beautiful" was κάλλος, "kallos", and the adjective was καλός, "kalos". However, "kalos" may and is also translated as ″good″ or ″of fine quality″ and thus has a broader meaning than mere physical or material beauty. Similarly, "kallos" was used differently from the English word beauty in that it first and foremost applied to humans and bears an erotic connotation.

The Koine Greek word for beautiful was ὡραῖος, "hōraios", an adjective etymologically coming from the word ὥρα, "hōra", meaning "hour". In Koine Greek, beauty was thus associated with "being of one's hour". Thus, a ripe fruit (of its time) was considered beautiful, whereas a young woman trying to appear older or an older woman trying to appear younger would not be considered beautiful. In Attic Greek, "hōraios" had many meanings, including "youthful" and "ripe old age".

The earliest Western theory of beauty can be found in the works of early Greek philosophers from the pre-Socratic period, such as Pythagoras. The Pythagorean school saw a strong connection between mathematics and beauty. In particular, they noted that objects proportioned according to the golden ratio seemed more attractive. Ancient Greek architecture is based on this view of symmetry and proportion.

Plato considered beauty to be the Idea (Form) above all other Ideas. Aristotle saw a relationship between the beautiful ("to kalon") and virtue, arguing that "Virtue aims at the beautiful."

Classical philosophy and sculptures of men and women produced according to the Greek philosophers' tenets of ideal human beauty were rediscovered in Renaissance Europe, leading to a re-adoption of what became known as a "classical ideal". In terms of female human beauty, a woman whose appearance conforms to these tenets is still called a "classical beauty" or said to possess a "classical beauty", whilst the foundations laid by Greek and Roman artists have also supplied the standard for male beauty and female beauty in western civilization as seen, for example, in the "Winged Victory of Samothrace". During the Gothic era, the classical aesthetical canon of beauty was rejected as sinful. Later, Renaissance and Humanist thinkers rejected this view, and considered beauty to be the product of rational order and harmonious proportions. Renaissance artists and architects (such as Giorgio Vasari in his "Lives of Artists") criticised the Gothic period as irrational and barbarian. This point of view of Gothic art lasted until Romanticism, in the 19th century.

In the Middle Ages, Catholic philosophers like Thomas Aquinas included beauty among the transcendental attributes of being. In his Summa Theologica, Aquinas described the three conditions of beauty as: integritas (wholeness), consonantia (harmony and proportion), and claritas (a radiance and clarity that makes the form of a thing apparent to the mind).

In the Gothic Architecture of the High and Late Middle Ages, light was considered the most beautiful revelation of God, which was heralded in design. Examples are the stained glass of Gothic Cathedrals including Notre-Dame de Paris and Chartes Cathedral.

The Age of Reason saw a rise in an interest in beauty as a philosophical subject. For example, Scottish philosopher Francis Hutcheson argued that beauty is "unity in variety and variety in unity". He wrote that beauty was neither purely subjective nor purely objective—it could be understood not as "any Quality suppos'd to be in the Object, which should of itself be beautiful, without relation to any Mind which perceives it: For Beauty, like other Names of sensible Ideas, properly denotes the "Perception" of some mind; ... however we generally imagine that there is something in the Object just like our Perception."

Immanuel Kant believed that there could be no "universal criterion of the beautiful" and that the experience of beauty is subjective, but that an object is judged to be beautiful when it seems to display "purposiveness"; that is, when its form is perceived to have the character of a thing designed according to some principle and fitted for a purpose. He distinguished "free beauty" from "merely dependent beauty", explaining that "the first presupposes no concept of what the object ought to be; the second does presuppose such a concept and the perfection of the object in accordance therewith." By this definition, free beauty is found in seashells and wordless music; dependent beauty in buildings and the human body.

The Romantic poets, too, became highly concerned with the nature of beauty, with John Keats arguing in "Ode on a Grecian Urn" that:

In the Romantic period, Edmund Burke postulated a difference between beauty in its classical meaning and the sublime. The concept of the sublime, as explicated by Burke and Kant, suggested viewing Gothic art and architecture, though not in accordance with the classical standard of beauty, as sublime.

The 20th century saw an increasing rejection of beauty by artists and philosophers alike, culminating in postmodernism's anti-aesthetics. This is despite beauty being a central concern of one of postmodernism's main influences, Friedrich Nietzsche, who argued that the Will to Power was the Will to Beauty.

In the aftermath of postmodernism's rejection of beauty, thinkers have returned to beauty as an important value. American analytic philosopher Guy Sircello proposed his New Theory of Beauty as an effort to reaffirm the status of beauty as an important philosophical concept. He rejected the subjectivism of Kant and sought to identify the properties inherent in an object that make it beautiful. He called qualities such as vividness, boldness, and subtlety "properties of qualitative degree" (PQDs) and stated that a PQD makes an object beautiful if it is not—and does not create the appearance of—"a property of deficiency, lack, or defect"; and if the PQD is strongly present in the object. 

Elaine Scarry argues that beauty is related to justice.

Beauty is also studied by psychologists and neuroscientists in the field of experimental aesthetics and neuroesthetics respectively. Psychological theories see beauty as a form of pleasure. Correlational findings support the view that more beautiful objects are also more pleasing. Some studies suggest that higher experienced beauty is associated with activity in the medial orbitofrontal cortex. This approach of localizing the processing of beauty in one brain region has received criticism within the field.

Chinese philosophy has traditionally not made a separate discipline of the philosophy of beauty. Confucius identified beauty with goodness, and considered a virtuous personality to be the greatest of beauties: In his philosophy, "a neighborhood with a "ren" man in it is a beautiful neighborhood." Confucius’s student Zeng Shen expressed a similar idea: "few men could see the beauty in some one whom they dislike." Mencius considered "complete truthfulness" to be beauty. Zhu Xi said: "When one has strenuously implemented goodness until it is filled to completion and has accumulated truth, then the beauty will reside within it and will not depend on externals."

The word "beauty" is often used as a countable noun to describe a beautiful woman.

The characterization of a person as “beautiful”, whether on an individual basis or by community consensus, is often based on some combination of "inner beauty", which includes psychological factors such as personality, intelligence, grace, politeness, charisma, integrity, congruence and elegance, and "outer beauty" (i.e. physical attractiveness) which includes physical attributes which are valued on an aesthetic basis.

Standards of beauty have changed over time, based on changing cultural values. Historically, paintings show a wide range of different standards for beauty. However, humans who are relatively young, with smooth skin, well-proportioned bodies, and regular features, have traditionally been considered the most beautiful throughout history.

A strong indicator of physical beauty is "averageness". When images of human faces are averaged together to form a composite image, they become progressively closer to the "ideal" image and are perceived as more attractive. This was first noticed in 1883, when Francis Galton overlaid photographic composite images of the faces of vegetarians and criminals to see if there was a typical facial appearance for each. When doing this, he noticed that the composite images were more attractive compared to any of the individual images. Researchers have replicated the result under more controlled conditions and found that the computer-generated, mathematical average of a series of faces is rated more favorably than individual faces. It is argued that it is evolutionarily advantageous that sexual creatures are attracted to mates who possess predominantly common or average features, because it suggests the absence of genetic or acquired defects. There is also evidence that a preference for beautiful faces emerges early in infancy, and is probably innate,
and that the rules by which attractiveness is established are similar across different genders and cultures.

A feature of beautiful women that has been explored by researchers is a waist–hip ratio of approximately 0.70. Physiologists have shown that women with hourglass figures are more fertile than other women due to higher levels of certain female hormones, a fact that may subconsciously condition males choosing mates. However, other commentators have suggested that this preference may not be universal. For instance, in some non-Western cultures in which women have to do work such as finding food, men tend to have preferences for higher waist-hip ratios.

Beauty standards are rooted in cultural norms crafted by societies and media over centuries. Globally, it is argued that the predominance of white women featured in movies and advertising leads to a Eurocentric concept of beauty, breeding cultures that assign inferiority to women of color. Thus, societies and cultures across the globe struggle to diminish the longstanding internalized racism. The black is beautiful cultural movement sought to dispel this notion in the 1960s.

Exposure to the thin ideal in mass media, such as fashion magazines, directly correlates with body dissatisfaction, low self-esteem, and the development of eating disorders among female viewers. Further, the widening gap between individual body sizes and societal ideals continues to breed anxiety among young girls as they grow, highlighting the dangerous nature of beauty standards in society.

The concept of beauty in men is known as 'bishōnen' in Japan. Bishōnen refers to males with distinctly feminine features, physical characteristics establishing the standard of beauty in Japan and typically exhibited in their pop culture idols. A multibillion-dollar industry of Japanese Aesthetic Salons exists for this reason. However, different nations have varying male beauty ideals; Eurocentric standards for men include tallness, leanness, and muscularity; thus, these features are idolized through American media, such as in Hollywood films and magazine covers.

The prevailing eurocentric concept of beauty has varying effects on different cultures. Primarily, adherence to this standard among African American women has bred a lack of positive reification of African beauty, and philosopher Cornel West elaborates that, "much of black self-hatred and self-contempt has to do with the refusal of many black Americans to love their own black bodies-especially their black noses, hips, lips, and hair." These insecurities can be traced back to global idealization of women with light skin, green or blue eyes, and long straight or wavy hair in magazines and media that starkly contrast with the natural features of African women.

In East Asian cultures, familial pressures and cultural norms shape beauty ideals; professor and scholar Stephanie Wong's experimental study concluded that expecting that men in Asian culture didn't like women who look “fragile” impacted the lifestyle, eating, and appearance choices made by Asian American women. In addition to the male gaze, media portrayals of Asian women as petite and the portrayal of beautiful women in American media as fair complexioned and slim-figured induce anxiety and depressive symptoms among Asian American women who don't fit either of these beauty ideals. Further, the high status associated with fairer skin can be attributed to Asian societal history; upper-class people hired workers to perform outdoor, manual labor, cultivating a visual divide over time between lighter complexioned, wealthier families and sun tanned, darker laborers. This along with the Eurocentric beauty ideals embedded in Asian culture has made skin lightening creams, rhinoplasty, and blepharoplasty (an eyelid surgery meant to give Asians a more European, "double-eyelid" appearance) commonplace among Asian women, illuminating the insecurity that results from cultural beauty standards.

Much criticism has been directed at models of beauty which depend solely upon Western ideals of beauty as seen for example in the Barbie model franchise. Criticisms of Barbie are often centered around concerns that children consider Barbie a role model of beauty and will attempt to emulate her. One of the most common criticisms of Barbie is that she promotes an unrealistic idea of body image for a young woman, leading to a risk that girls who attempt to emulate her will become anorexic.

These criticisms have led to a constructive dialogue to enhance the presence of non-exclusive models of Western ideals in body type and beauty. Complaints also point to a lack of diversity in such franchises as the Barbie model of beauty in Western culture. Mattel responded to these criticisms. Starting in 1980, it produced Hispanic dolls, and later came models from across the globe. For example, in 2007, it introduced "Cinco de Mayo Barbie" wearing a ruffled red, white, and green dress (echoing the Mexican flag). "Hispanic" magazine reports that:
Researchers have found that good-looking students get higher grades from their teachers than students with an ordinary appearance. Some studies using mock criminal trials have shown that physically attractive "defendants" are less likely to be convicted—and if convicted are likely to receive lighter sentences—than less attractive ones (although the opposite effect was observed when the alleged crime was swindling, perhaps because jurors perceived the defendant's attractiveness as facilitating the crime). Studies among teens and young adults, such as those of psychiatrist and self-help author Eva Ritvo show that skin conditions have a profound effect on social behavior and opportunity.

How much money a person earns may also be influenced by physical beauty. One study found that people low in physical attractiveness earn 5 to 10 percent less than ordinary-looking people, who in turn earn 3 to 8 percent less than those who are considered good-looking. In the market for loans, the least attractive people are less likely to get approvals, although they are less likely to default. In the marriage market, women's looks are at a premium, but men's looks do not matter much.

Conversely, being very unattractive increases the individual's propensity for criminal activity for a number of crimes ranging from burglary to theft to selling illicit drugs.

Discrimination against others based on their appearance is known as lookism.

St. Augustine said of beauty "Beauty is indeed a good gift of God; but that the good may not think it a great good, God dispenses it even to the wicked."

Philosopher and novelist Umberto Eco wrote "" (2004) and "On Ugliness" (2007). The narrator of his novel "The Name of the Rose" follows Aquinas in declaring: "three things concur in creating beauty: first of all integrity or perfection, and for this reason we consider ugly all incomplete things; then proper proportion or consonance; and finally clarity and light", before going on to say "the sight of the beautiful implies peace".




</doc>
<doc id="4436" url="https://en.wikipedia.org/wiki?curid=4436" title="Brownian motion">
Brownian motion

Brownian motion, or pedesis (from "leaping"), is the random motion of particles suspended in a medium. (a liquid or a gas) .

This pattern of motion typically consists of random fluctuations in a particle's position inside a fluid sub-domain, followed by a relocation to another sub-domain. Each relocation is followed by more fluctuations within the new closed volume. This pattern describes a fluid at thermal equilibrium, defined by a given temperature. Within such a fluid, there exists no preferential direction of flow (as in transport phenomena). More specifically, the fluid's overall linear and angular momenta remain null over time. The kinetic energies of the molecular Brownian motions, together with those of molecular rotations and vibrations, sum up to the caloric component of a fluid's internal energy (the Equipartition theorem).

This motion is named after the botanist Robert Brown, who first described the phenomenon in 1827, while looking through a microscope at pollen of the plant "Clarkia pulchella" immersed in water. In 1905, almost eighty years later, theoretical physicist Albert Einstein published a paper where he modeled the motion of the pollen as being moved by individual water molecules, making one of his first major scientific contributions. This explanation of Brownian motion served as convincing evidence that atoms and molecules exist and was further verified experimentally by Jean Perrin in 1908. Perrin was awarded the Nobel Prize in Physics in 1926 "for his work on the discontinuous structure of matter". The direction of the force of atomic bombardment is constantly changing, and at different times the particle is hit more on one side than another, leading to the seemingly random nature of the motion.

The many-body interactions that yield the Brownian pattern cannot be solved by a model accounting for every involved molecule. In consequence, only probabilistic models applied to molecular populations can be employed to describe it. Two such models of the statistical mechanics, due to Einstein and Smoluchowski are presented below. Another, pure probabilistic class of models is the class of the stochastic process models. There exist sequences of both simpler and more complicated stochastic processes which converge (in the limit) to Brownian motion (see random walk and Donsker's theorem).

The Roman philosopher Lucretius' scientific poem "On the Nature of Things" (c. 60 BC) has a remarkable description of Brownian motion of dust particles in verses 113–140 from Book II. He uses this as a proof of the existence of atoms:

Although the mingling motion of dust particles is caused largely by air currents, the glittering, tumbling motion of small dust particles is, indeed, caused chiefly by true Brownian dynamics.

While Jan Ingenhousz described the irregular motion of coal dust particles on the surface of alcohol in 1785, the discovery of this phenomenon is often credited to the botanist Robert Brown in 1827. Brown was studying pollen grains of the plant "Clarkia pulchella" suspended in water under a microscope when he observed minute particles, ejected by the pollen grains, executing a jittery motion. By repeating the experiment with particles of inorganic matter he was able to rule out that the motion was life-related, although its origin was yet to be explained.

The first person to describe the mathematics behind Brownian motion was Thorvald N. Thiele in a paper on the method of least squares published in 1880. This was followed independently by Louis Bachelier in 1900 in his PhD thesis "The theory of speculation", in which he presented a stochastic analysis of the stock and option markets. The Brownian motion model of the stock market is often cited, but Benoit Mandelbrot rejected its applicability to stock price movements in part because these are discontinuous.

Albert Einstein (in one of his 1905 papers) and Marian Smoluchowski (1906) brought the solution of the problem to the attention of physicists, and presented it as a way to indirectly confirm the existence of atoms and molecules. Their equations describing Brownian motion were subsequently verified by the experimental work of Jean Baptiste Perrin in 1908.

There are two parts to Einstein's theory: the first part consists in the formulation of a diffusion equation for Brownian particles, in which the diffusion coefficient is related to the mean squared displacement of a Brownian particle, while the second part consists in relating the diffusion coefficient to measurable physical quantities. In this way Einstein was able to determine the size of atoms, and how many atoms there are in a mole, or the molecular weight in grams, of a gas. In accordance to Avogadro's law this volume is the same for all ideal gases, which is 22.414 liters at standard temperature and pressure. The number of atoms contained in this volume is referred to as the Avogadro number, and the determination of this number is tantamount to the knowledge of the mass of an atom since the latter is obtained by dividing the mass of a mole of the gas by the Avogadro constant.

The first part of Einstein's argument was to determine how far a Brownian particle travels in a given time interval. Classical mechanics is unable to determine this distance because of the enormous number of bombardments a Brownian particle will undergo, roughly of the order of 10 collisions per second. Thus Einstein was led to consider the collective motion of Brownian particles.

He regarded the increment of particle positions in time formula_1 in a one dimensional ("x") space (with the coordinates chosen so that the origin lies at the initial position of the particle) as a random variable (formula_2) with some probability density function formula_3. Further, assuming conservation of particle number, he expanded the density (number of particles per unit volume) at time formula_4 in a Taylor series,

where the second equality in the first line is by definition of formula_6. The integral in the first term is equal to one by the definition of probability, and the second and other even terms (i.e. first and other odd moments) vanish because of space symmetry. What is left gives rise to the following relation:

Where the coefficient after the Laplacian, the second moment of probability of displacement formula_2, is interpreted as mass diffusivity "D":

Then the density of Brownian particles "ρ" at point "x" at time "t" satisfies the diffusion equation:

Assuming that "N" particles start from the origin at the initial time "t" = 0, the diffusion equation has the solution

This expression (which is a normal distribution with the mean formula_12 and variance formula_13 usually called Brownian motion formula_14) allowed Einstein to calculate the moments directly. The first moment is seen to vanish, meaning that the Brownian particle is equally likely to move to the left as it is to move to the right. The second moment is, however, non-vanishing, being given by

This equation expresses the mean squared displacement in terms of the time elapsed and the diffusivity. From this expression Einstein argued that the displacement of a Brownian particle is not proportional to the elapsed time, but rather to its square root. His argument is based on a conceptual switch from the "ensemble" of Brownian particles to the "single" Brownian particle: we can speak of the relative number of particles at a single instant just as well as of the time it takes a Brownian particle to reach a given point.

The second part of Einstein's theory relates the diffusion constant to physically measurable quantities, such as the mean squared displacement of a particle in a given time interval. This result enables the experimental determination of Avogadro's number and therefore the size of molecules. Einstein analyzed a dynamic equilibrium being established between opposing forces. The beauty of his argument is that the final result does not depend upon which forces are involved in setting up the dynamic equilibrium.

In his original treatment, Einstein considered an osmotic pressure experiment, but the same conclusion can be reached in other ways.

Consider, for instance, particles suspended in a viscous fluid in a gravitational field. Gravity tends to make the particles settle, whereas diffusion acts to homogenize them, driving them into regions of smaller concentration. Under the action of gravity, a particle acquires a downward speed of "v" = "μmg", where "m" is the mass of the particle, "g" is the acceleration due to gravity, and "μ" is the particle's mobility in the fluid. George Stokes had shown that the mobility for a spherical particle with radius "r" is formula_16, where "η" is the dynamic viscosity of the fluid. In a state of dynamic equilibrium, and under the hypothesis of isothermal fluid, the particles are distributed according to the barometric distribution

where "ρ" − "ρ" is the difference in density of particles separated by a height difference of "h", "k" is the Boltzmann constant (the ratio of the universal gas constant, "R", to the Avogadro constant, "N"), and "T" is the absolute temperature.
Dynamic equilibrium is established because the more that particles are pulled down by gravity, the greater the tendency for the particles to migrate to regions of lower concentration. The flux is given by Fick's law,

where "J" = "ρv". Introducing the formula for "ρ", we find that

In a state of dynamical equilibrium, this speed must also be equal to "v" = "μmg". Both expressions for "v" are proportional to "mg", reflecting that the derivation is independent of the type of forces considered. Similarly, one can derive an equivalent formula for identical charged particles of charge "q" in a uniform electric field of magnitude "E", where "mg" is replaced with the electrostatic force "qE". Equating these two expressions yields a formula for the diffusivity, independent of "mg" or "qE" or other such forces:

Here the first equality follows from the first part of Einstein's theory, the third equality follows from the definition of Boltzmann's constant as "k" = "R" / "N", and the fourth equality follows from Stokes's formula for the mobility. By measuring the mean squared displacement over a time interval along with the universal gas constant "R", the temperature "T", the viscosity "η", and the particle radius "r", the Avogadro constant "N" can be determined.

The type of dynamical equilibrium proposed by Einstein was not new. It had been pointed out previously by J. J. Thomson in his series of lectures at Yale University in May 1903 that the dynamic equilibrium between the velocity generated by a concentration gradient given by Fick's law and the velocity due to the variation of the partial pressure caused when ions are set in motion "gives us a method of determining Avogadro's Constant which is independent of any hypothesis as to the shape or size of molecules, or of the way in which they act upon each other".

An identical expression to Einstein's formula for the diffusion coefficient was also found by Walther Nernst in 1888 in which he expressed the diffusion coefficient as the ratio of the osmotic pressure to the ratio of the frictional force and the velocity to which it gives rise. The former was equated to the law of van 't Hoff while the latter was given by Stokes's law. He writes formula_21 for the diffusion coefficient "k′", where formula_22 is the osmotic pressure and "k" is the ratio of the frictional force to the molecular viscosity which he assumes is given by Stokes's formula for the viscosity. Introducing the ideal gas law per unit volume for the osmotic pressure, the formula becomes identical to that of Einstein's. The use of Stokes's law in Nernst's case, as well as in Einstein and Smoluchowski, is not strictly applicable since it does not apply to the case where the radius of the sphere is small in comparison with the mean free path.

At first, the predictions of Einstein's formula were seemingly refuted by a series of experiments by Svedberg in 1906 and 1907, which gave displacements of the particles as 4 to 6 times the predicted value, and by Henri in 1908 who found displacements 3 times greater than Einstein's formula predicted. But Einstein's predictions were finally confirmed in a series of experiments carried out by Chaudesaigues in 1908 and Perrin in 1909. The confirmation of Einstein's theory constituted empirical progress for the kinetic theory of heat. In essence, Einstein showed that the motion can be predicted directly from the kinetic model of thermal equilibrium. The importance of the theory lay in the fact that it confirmed the kinetic theory's account of the second law of thermodynamics as being an essentially statistical law.

Smoluchowski's theory of Brownian motion starts from the same premise as that of Einstein and derives the same probability distribution "ρ"("x", "t") for the displacement of a Brownian particle along the "x" in time "t". He therefore gets the same expression for the mean squared displacement: formula_23. However, when he relates it to a particle of mass "m" moving at a velocity formula_24 which is the result of a frictional force governed by Stokes's law, he finds
where "μ" is the viscosity coefficient, and formula_26 is the radius of the particle. Associating the kinetic energy formula_27 with the thermal energy "RT"/"N", the expression for the mean squared displacement is 64/27 times that found by Einstein. The fraction 27/64 was commented on by Arnold Sommerfeld in his necrology on Smoluchowski: "The numerical coefficient of Einstein, which differs from Smoluchowski by 27/64 can only be put in doubt."

Smoluchowski attempts to answer the question of why a Brownian particle should be displaced by bombardments of smaller particles when the probabilities for striking it in the forward and rear directions are equal.
If the probability of "m" gains and "n" − "m" losses follows a binomial distribution,

with equal "a priori" probabilities of 1/2, the mean total gain is

If "n" is large enough so that Stirling's approximation can be used in the form
then the expected total gain will be
showing that it increases as the square root of the total population.

Suppose that a Brownian particle of mass "M" is surrounded by lighter particles of mass "m" which are traveling at a speed "u". Then, reasons Smoluchowski, in any collision between a surrounding and Brownian particles, the velocity transmitted to the latter will be "mu"/"M". This ratio is of the order of 10 cm/s. But we also have to take into consideration that in a gas there will be more than 10 collisions in a second, and even greater in a liquid where we expect that there will be 10 collision in one second. Some of these collisions will tend to accelerate the Brownian particle; others will tend to decelerate it. If there is a mean excess of one kind of collision or the other to be of the order of 10 to 10 collisions in one second, then velocity of the Brownian particle may be anywhere between 10 and 1000 cm/s. Thus, even though there are equal probabilities for forward and backward collisions there will be a net tendency to keep the Brownian particle in motion, just as the ballot theorem predicts.

These orders of magnitude are not exact because they don't take into consideration the velocity of the Brownian particle, "U", which depends on the collisions that tend to accelerate and decelerate it. The larger "U" is, the greater will be the collisions that will retard it so that the velocity of a Brownian particle can never increase without limit. Could such a process occur, it would be tantamount to a perpetual motion of the second type. And since equipartition of energy applies, the kinetic energy of the Brownian particle, formula_32, will be equal, on the average, to the kinetic energy of the surrounding fluid particle, formula_33.

In 1906 Smoluchowski published a one-dimensional model to describe a particle undergoing Brownian motion. The model assumes collisions with "M" ≫ "m" where "M" is the test particle's mass and "m" the mass of one of the individual particles composing the fluid. It is assumed that the particle collisions are confined to one dimension and that it is equally probable for the test particle to be hit from the left as from the right. It is also assumed that every collision always imparts the same magnitude of Δ"V". If "N" is the number of collisions from the right and "N" the number of collisions from the left then after "N" collisions the particle's velocity will have changed by Δ"V"(2"N" − "N"). The multiplicity is then simply given by:

and the total number of possible states is given by 2. Therefore, the probability of the particle being hit from the right "N" times is:

As a result of its simplicity, Smoluchowski's 1D model can only qualitatively describe Brownian motion. For a realistic particle undergoing Brownian motion in a fluid, many of the assumptions don't apply. For example, the assumption that on average occurs an equal number of collisions from the right as from the left falls apart once the particle is in motion. Also, there would be a distribution of different possible Δ"V"s instead of always just one in a realistic situation.

The diffusion equation yields an approximation of the time evolution of the probability density function associated to the position of the particle going under a Brownian movement under the physical definition. The approximation is valid on short timescales.

The time evolution of the position of the Brownian particle itself is best described using Langevin equation, an equation which involves a random force field representing the effect of the thermal fluctuations of the solvent on the particle.

The displacement of a particle undergoing Brownian motion is obtained by solving the diffusion equation under appropriate boundary conditions and finding the rms of the solution. This shows that the displacement varies as the square root of the time (not linearly), which explains why previous experimental results concerning the velocity of Brownian particles gave nonsensical results. A linear time dependence was incorrectly assumed.

At very short time scales, however, the motion of a particle is dominated by its inertia and its displacement will be linearly dependent on time: Δ"x" = "v"Δ"t". So the instantaneous velocity of the Brownian motion can be measured as "v" = Δ"x"/Δ"t", when Δ"t" « "τ", where "τ" is the momentum relaxation time. In 2010, the instantaneous velocity of a Brownian particle (a glass microsphere trapped in air with optical tweezers) was measured successfully. The velocity data verified the Maxwell–Boltzmann velocity distribution, and the equipartition theorem for a Brownian particle.

In stellar dynamics, a massive body (star, black hole, etc.) can experience Brownian motion as it responds to gravitational forces from surrounding stars. The rms velocity "V" of the massive object, of mass "M", is related to the rms velocity formula_36 of the background stars by
where formula_38 is the mass of the background stars. The gravitational force from the massive object causes nearby stars to move faster than they otherwise would, increasing both formula_36 and "V". The Brownian velocity of Sgr A*, the supermassive black hole at the center of the Milky Way galaxy, is predicted from this formula to be less than 1 km "s".

In mathematics, Brownian motion is described by the Wiener process, a continuous-time stochastic process named in honor of Norbert Wiener. It is one of the best known Lévy processes (càdlàg stochastic processes with stationary independent increments) and occurs frequently in pure and applied mathematics, economics and physics.
The Wiener process "W" is characterized by four facts:
formula_42 denotes the normal distribution with expected value "μ" and variance "σ". The condition that it has independent increments means that if formula_43 then formula_44 and formula_45 are independent random variables.

An alternative characterisation of the Wiener process is the so-called "Lévy characterisation" that says that the Wiener process is an almost surely continuous martingale with "W" = 0 and quadratic variation formula_46.

A third characterisation is that the Wiener process has a spectral representation as a sine series whose coefficients are independent formula_47 random variables. This representation can be obtained using the Karhunen–Loève theorem.

The Wiener process can be constructed as the scaling limit of a random walk, or other discrete-time stochastic processes with stationary independent increments. This is known as Donsker's theorem. Like the random walk, the Wiener process is recurrent in one or two dimensions (meaning that it returns almost surely to any fixed neighborhood of the origin infinitely often) whereas it is not recurrent in dimensions three and higher. Unlike the random walk, it is scale invariant.

The time evolution of the position of the Brownian particle itself can be described approximately by a Langevin equation, an equation which involves a random force field representing the effect of the thermal fluctuations of the solvent on the Brownian particle. On long timescales, the mathematical Brownian motion is well described by a Langevin equation. On small timescales, inertial effects are prevalent in the Langevin equation. However the mathematical "Brownian motion" is exempt of such inertial effects. Inertial effects have to be considered in the Langevin equation, otherwise the equation becomes singular. so that simply removing the inertia term from this equation would not yield an exact description, but rather a singular behavior in which the particle doesn't move at all.

The Brownian motion can be modeled by a random walk. Random walks in porous media or fractals are anomalous.

In the general case, Brownian motion is a non-Markov random process and described by stochastic integral equations.

The French mathematician Paul Lévy proved the following theorem, which gives a necessary and sufficient condition for a continuous R-valued stochastic process "X" to actually be "n"-dimensional Brownian motion. Hence, Lévy's condition can actually be used as an alternative definition of Brownian motion.

Let "X" = ("X", ..., "X") be a continuous stochastic process on a probability space (Ω, Σ, P) taking values in R. Then the following are equivalent:


The spectral content of a stochastic process formula_48can be found from the power spectral density, formally defined as

formula_49,

where formula_50 stands for the expected value. The power spectral density of Brownian motion is found to be

formula_51.

where formula_52 is the diffusion coefficient of formula_48. For naturally occurring signals, the spectral content can be found from the power spectral density of a single realization, with finite available time, i.e.,

formula_54,

which for an individual realization of a Brownian motion trajectory, it is found to have expected value formula_55

formula_56

and variance formula_57

formula_58.

For sufficiently long realization times, the expected value of the power spectrum of a single trajectory converges to the formally defined power spectral density formula_59, but its coefficient of variation formula_60 tends to formula_61. This implies the distribution of formula_62is broad even in the infinite time limit.

The infinitesimal generator (and hence characteristic operator) of a Brownian motion on R is easily calculated to be ½Δ, where Δ denotes the Laplace operator. In image processing and computer vision, the Laplacian operator has been used for various tasks such as blob and edge detection. This observation is useful in defining Brownian motion on an "m"-dimensional Riemannian manifold ("M", "g"): a Brownian motion on "M" is defined to be a diffusion on "M" whose characteristic operator formula_63 in local coordinates "x", 1 ≤ "i" ≤ "m", is given by ½Δ, where Δ is the Laplace–Beltrami operator given in local coordinates by

where ["g"] = ["g"] in the sense of the inverse of a square matrix.

The narrow escape problem is a ubiquitous problem in biology, biophysics and cellular biology which has the following formulation: a Brownian particle (ion, molecule, or protein) is confined to a bounded domain (a compartment or a cell) by a reflecting boundary, except for a small window through which it can escape. The narrow escape problem is that of calculating the mean escape time. This time diverges as the window shrinks, thus rendering the calculation a singular perturbation problem.




</doc>
<doc id="4443" url="https://en.wikipedia.org/wiki?curid=4443" title="Barcelona">
Barcelona

Barcelona ( , , ) is a city on the coast of northeastern Spain. It is the capital and largest city of the autonomous community of Catalonia, as well as the second most populous municipality of Spain. With a population of 1.6 million within city limits, its urban area extends to numerous neighbouring municipalities within the Province of Barcelona and is home to around 4.8 million people, making it the fifth most populous urban area in the European Union after Paris, the Ruhr area, Madrid, and Milan. It is one of the largest metropolises on the Mediterranean Sea, located on the coast between the mouths of the rivers Llobregat and Besòs, and bounded to the west by the Serra de Collserola mountain range, the tallest peak of which is high.

Founded as a Roman city, in the Middle Ages Barcelona became the capital of the County of Barcelona. After merging with the Kingdom of Aragon, Barcelona continued to be an important city in the Crown of Aragon as an economic and administrative centre of this Crown and the capital of the Principality of Catalonia. Barcelona has a rich cultural heritage and is today an important cultural centre and a major tourist destination. Particularly renowned are the architectural works of Antoni Gaudí and Lluís Domènech i Montaner, which have been designated UNESCO World Heritage Sites. Since 1450, it is home to the University of Barcelona, widely considered the most prestigious university in Spain. The headquarters of the Union for the Mediterranean are located in Barcelona. The city is known for hosting the 1992 Summer Olympics as well as world-class conferences and expositions and also many international sport tournaments.

Barcelona is a major cultural, economic, and financial centre in southwestern Europe, as well as the main biotech hub in Spain. As a leading world city, Barcelona's influence in global socio-economic affairs qualifies it for global city status (Beta +). 

Barcelona is a transport hub, with the Port of Barcelona being one of Europe's principal seaports and busiest European passenger port, an international airport, Barcelona–El Prat Airport, which handles over 50 million passengers per year, an extensive motorway network, and a high-speed rail line with a link to France and the rest of Europe.

The name "Barcelona" comes from the ancient Iberian "Barkeno", attested in an ancient coin inscription found on the right side of the coin in Iberian script as , in ancient Greek sources as , "Barkinṓn"; and in Latin as "Barcino", "Barcilonum" and "Barcenona".

Some older sources suggest that the city may have been named after the Carthaginian general Hamilcar Barca, who was supposed to have founded the city in the 3rd century BC, but there is no evidence that Barcelona was ever a Carthaginian settlement, or that its name in antiquity, "Barcino", had any connection with the Barcid family of Hamilcar.
During the Middle Ages, the city was variously known as "Barchinona", "Barçalona", "Barchelonaa", and "Barchenona".

Internationally, Barcelona's name is wrongly abbreviated to 'Barça'. However, this name refers only to FC Barcelona, the football club. The common abbreviated form used by locals is "Barna".

Another common abbreviation is 'BCN', which is also the IATA airport code of the Barcelona-El Prat Airport.

The city is also referred to as the "Ciutat Comtal" in Catalan, and "Ciudad Condal" in Spanish (i.e. Comital City or City of Counts), owing to its past as the seat of the Count of Barcelona.

The origin of the earliest settlement at the site of present-day Barcelona is unclear. The ruins of an early settlement have been 
found, including different tombs and dwellings dating to earlier than 5000 BC. The founding of Barcelona is the subject of two different legends. The first attributes the founding of the city to the mythological Hercules. The second legend attributes the foundation of the city directly to the historical Carthaginian general, Hamilcar Barca, father of Hannibal, who supposedly named the city "Barcino" after his family in the 3rd century BC, but there is no historical or linguistic evidence that this is true.

In about 15 BC, the Romans redrew the town as a "castrum" (Roman military camp) centred on the ""Mons Taber"", a little hill near the contemporary city hall (Plaça de Sant Jaume). Under the Romans, it was a colony with the surname of "Faventia", or, in full, "Colonia Faventia Julia Augusta Pia Barcino" or "Colonia Julia Augusta Faventia Paterna Barcino". Pomponius Mela mentions it among the small towns of the district, probably as it was eclipsed by its neighbour "Tarraco" (modern Tarragona), but it may be gathered from later writers that it gradually grew in wealth and consequence, favoured as it was with a beautiful situation and an excellent harbour. It enjoyed immunity from imperial burdens. The city minted its own coins; some from the era of Galba survive.
Important Roman vestiges are displayed in Plaça del Rei underground, as a part of the Barcelona City History Museum (MUHBA); the typically Roman grid plan is still visible today in the layout of the historical centre, the "Barri Gòtic" (Gothic Quarter). Some remaining fragments of the Roman walls have been incorporated into the cathedral. The cathedral, known very formally by the long name of "Catedral Basílica Metropolitana de Barcelona", is also sometimes called "La Seu", which simply means cathedral (and see, among other things) in Catalan. It is said to have been founded in 343.

The city was conquered by the Visigoths in the early 5th century, becoming for a few years the capital of all Hispania. After being conquered by the Arabs in the early 8th century, it was conquered in 801 by Charlemagne's son Louis, who made Barcelona the seat of the Carolingian "Hispanic March" ("Marca Hispanica"), a buffer zone ruled by the Count of Barcelona.

The Counts of Barcelona became increasingly independent and expanded their territory to include all of Catalonia, although on 6 July 985, Barcelona was sacked by the army of Almanzor. The sack was so traumatic that most of Barcelona's population was either killed or enslaved. In 1137, Aragon and the County of Barcelona merged in dynastic union by the marriage of Ramon Berenguer IV and Petronilla of Aragon, their titles finally borne by only one person when their son Alfonso II of Aragon ascended to the throne in 1162. His territories were later to be known as the Crown of Aragon, which conquered many overseas possessions and ruled the western Mediterranean Sea with outlying territories in Naples and Sicily and as far as Athens in the 13th century.

Barcelona was the leading slave trade centre of the Crown of Aragon up until the 15th century, when it was eclipsed by Valencia. It initially fed from eastern and balkan slave stock later drawing from a Maghribian and, ultimately, Subsaharan pool of slaves.

The forging of a dynastic link between the Crowns of Aragon and Castile marked the beginning of Barcelona's decline. The Bank of Barcelona (""), probably the oldest public bank in Europe, was established by the city magistrates in 1401. It originated from necessities of the state, as did the Bank of Venice (1402) and the Bank of Genoa (1407).

The marriage of Ferdinand II of Aragon and Isabella I of Castile in 1469 united the two royal lines. Madrid became the centre of political power whilst the colonisation of the Americas reduced the financial importance (at least in relative terms) of Mediterranean trade. Barcelona was a centre of Catalan separatism, including the Catalan Revolt (1640–52) against Philip IV of Spain. The great plague of 1650–1654 halved the city's population.
In the 18th century, a fortress was built at Montjuïc that overlooked the harbour. In 1794, this fortress was used by the French astronomer Pierre François André Méchain for observations relating to a survey stretching to Dunkirk that provided the official basis of the measurement of a metre. The definitive metre bar, manufactured from platinum, was presented to the French legislative assembly on 22 June 1799. Much of Barcelona was negatively affected by the Napoleonic wars, but the start of industrialisation saw the fortunes of the province improve.

During the Spanish Civil War, the city, and Catalonia in general, were resolutely Republican. Many enterprises and public services were collectivised by the CNT and UGT unions. As the power of the Republican government and the Generalitat diminished, much of the city was under the effective control of anarchist groups. The anarchists lost control of the city to their own allies, the Communists and official government troops, after the street fighting of the Barcelona May Days. The fall of the city on 26 January 1939, caused a mass exodus of civilians who fled to the French border. The resistance of Barcelona to Franco's "coup d'état" was to have lasting effects after the defeat of the Republican government. The autonomous institutions of Catalonia were abolished, and the use of the Catalan language in public life was suppressed. Barcelona remained the second largest city in Spain, at the heart of a region which was relatively industrialised and prosperous, despite the devastation of the civil war. The result was a large-scale immigration from poorer regions of Spain (particularly Andalusia, Murcia and Galicia), which in turn led to rapid urbanisation.

In 1992, Barcelona hosted the Summer Olympics. The after-effects of this are credited with driving major changes in what had, up until then, been a largely industrial city. As part of the preparation for the games, industrial buildings along the sea-front were demolished and two miles of beach were created. New construction increased the road capacity of the city by 17%, the sewage handling capacity by 27% and the amount of new green areas and beaches by 78%. Between 1990 and 2004, the number of hotel rooms in the city doubled. Perhaps more importantly, the outside perception of the city was changed making, by 2012, Barcelona the 12th most popular city destination in the world and the 5th amongst European cities.

The death of Franco in 1975 brought on a period of democratisation throughout Spain. Pressure for change was particularly strong in Barcelona, which considered that it had been punished during nearly forty years of Francoism for its support of the Republican government. Massive, but peaceful, demonstrations on 11 September 1977 assembled over a million people in the streets of Barcelona to call for the restoration of Catalan autonomy. It was granted less than a month later.

The development of Barcelona was promoted by two events in 1986: Spanish accession to the European Community, and particularly Barcelona's designation as host city of the 1992 Summer Olympics. The process of urban regeneration has been rapid, and accompanied by a greatly increased international reputation of the city as a tourist destination. The increased cost of housing has led to a slight decline (−16.6%) in the population over the last two decades of the 20th century as many families move out into the suburbs. This decline has been reversed since 2001, as a new wave of immigration (particularly from Latin America and from Morocco) has gathered pace.

In 1987, an ETA car bombing at Hipercor killed 21 people. On 17 August 2017, a van was driven into pedestrians on La Rambla in the city, killing 14 and injuring at least 100, one of whom later died. Other attacks took place elsewhere in Catalonia. The Prime Minister of Spain, Mariano Rajoy, called the attack in Barcelona a jihadist attack. Amaq News Agency attributed indirect responsibility for the attack to the Islamic State of Iraq and the Levant (ISIL).

Barcelona is located on the northeast coast of the Iberian Peninsula, facing the Mediterranean Sea, on a plain approximately wide limited by the mountain range of Collserola, the Llobregat river to the southwest and the Besòs river to the north. This plain covers an area of , of which are occupied by the city itself. It is south of the Pyrenees and the Catalan border with France.

Tibidabo, high, offers striking views over the city and is topped by the Torre de Collserola, a telecommunications tower that is visible from most of the city. Barcelona is peppered with small hills, most of them urbanised, that gave their name to the neighbourhoods built upon them, such as Carmel (), () and Rovira (). The escarpment of Montjuïc (), situated to the southeast, overlooks the harbour and is topped by Montjuïc Castle, a fortress built in the 17–18th centuries to control the city as a replacement for the Ciutadella. Today, the fortress is a museum and Montjuïc is home to several sporting and cultural venues, as well as Barcelona's biggest park and gardens.

The city borders on the municipalities of Santa Coloma de Gramenet and Sant Adrià de Besòs to the north; the Mediterranean Sea to the east; El Prat de Llobregat and L'Hospitalet de Llobregat to the south; and Sant Feliu de Llobregat, Sant Just Desvern, Esplugues de Llobregat, Sant Cugat del Vallès, and Montcada i Reixac to the west. The municipality includes two small sparsely-inhabited exclaves to the north-west.

According to the Köppen climate classification, Barcelona has a maritime Mediterranean climate ("Csa"), with mild winters and warm to hot summers, while the rainiest seasons are autumn and spring. The rainfall pattern is characterised by a short (3 months) dry season in summer, as well as less winter rainfall than in a typical Mediterranean climate. This subtype, labelled as "Portuguese" by the French geographer George Viers after the climate classification of Emmanuel de Martonne and found in the NW Mediterranean area (e.g. Marseille), can be seen as transitional to the humid subtropical climate ("Cfa") found in inland areas such as the Po Valley (e.g. Milan), whose rainfall is greater in summer, a feature of continental climates.

Its average annual temperature is during the day and at night. The average annual temperature of the sea is about . In the coldest month, January, the temperature typically ranges from during the day, at night and the average sea temperature is . In the warmest month, August, the typical temperature ranges from during the day, about at night and the average sea temperature is . Generally, the summer or "holiday" season lasts about six months, from May to October. Two months – April and November – are transitional; sometimes the temperature exceeds , with an average temperature of during the day and at night. December, January and February are the coldest months, with average temperatures around during the day and at night. Large fluctuations in temperature are rare, particularly in the summer months. Because of the proximity to the warm sea plus the urban heat island, frosts are very rare in the city of Barcelona. Snow is also very infrequent.

Barcelona averages 78 rainy days per year (≥ 1 mm), and annual average relative humidity is 72%, ranging from 69% in July to 75% in October. Rainfall totals are highest in late summer and autumn (September–November) and lowest in early and mid-summer (June–August), with a secondary winter minimum (February–March). Sunshine duration is 2,524 hours per year, from 138 (average 4.5 hours of sunshine a day) in December to 310 (average 10 hours of sunshine a day) in July.

According to Barcelona's City Council, Barcelona's population was 1,608,746 people, on a land area of . It is the main component of an administrative area of Greater Barcelona, with a population of 3,218,071 in an area of (density 5,060 inhabitants/km). The population of the urban area was 4,840,000. It is the central nucleus of the Barcelona metropolitan area, which relies on a population of 5,474,482.

Spanish is the most spoken language in Barcelona (according to the linguistic census held by the Government of Catalonia in 2013) and it is understood almost universally. Catalan is also very commonly spoken in the city: it is understood by 95% of the population, while 72.3% can speak it, 79% can read it, and 53% can write it. Knowledge of Catalan has increased significantly in recent decades thanks to a language immersion educational system.

In 1900, Barcelona had a population of 533,000 people, which grew steadily but slowly until 1950, when it started absorbing a high number of people from other less-industrialised parts of Spain. Barcelona's population peaked in 1979 with 1,906,998 people, and fell throughout the 1980s and 1990s as more people sought a higher quality of life in outlying cities in the Barcelona Metropolitan Area. After bottoming out in 2000 with 1,496,266 people, the city's population began to rise again as younger people started to return, causing a great increase in housing prices.

"Note: This text is entirely based on the municipal statistical database provided by the city council."

Barcelona is one of the most densely populated cities in Europe. For the year 2008 the city council calculated the population to 1,621,090 living in the 102.2 km sized municipality, giving the city an average population density of 15,926 inhabitants per square kilometre with Eixample being the most populated district.

In the case of Barcelona though, the land distribution is extremely uneven. Half of the municipality or 50.2 km, all of it located on the municipal edge is made up of the ten least densely populated neighbourhoods containing less than 10% of the city's population, the uninhabited Zona Franca industrial area and Montjuïc forest park. Leaving the remaining 90% or slightly below 1.5 million inhabitants living on the remaining at an average density close to 28,500 inhabitants per square kilometre.

Of the 73 neighbourhoods in the city, 45 had a population density above 20,000 inhabitants per square kilometre with a combined population of 1,313,424 inhabitants living on 38.6 km at an average density of 33,987 inhabitants per square km. The 30 most densely populated neighbourhoods accounted for 57.5% of the city population occupying only 22.7% of the municipality, or in other words, 936,406 people living at an average density of 40,322 inhabitants per square kilometre. The city's highest density is found at and around the neighbourhood of la Sagrada Família where four of the city's most densely populated neighbourhoods are located side by side, all with a population density above 50,000 inhabitants per square kilometre.

In 1900 almost a third (28.9 percent) were children (aged younger than 14 years), In 2017 this age group constituted only 12.7; those aged between 15 and 24 years in 2017 were 9 percent; those aged between 25 and 44 years a 30.6 percent. In contrast, in 2017 the aged between 45 and 64 years formed the 56.9% of all Barcelonans; while in 1900 the aged 65 and older were just the 6.5 percent, in 2017 reached a 21.5.

In 2016 about 59% of the inhabitants of the city were born in Catalonia and 18.5% coming from the rest of the country. In addition to that, 22.5% of the population was born outside of Spain, a proportion which has more than doubled since 2001 and more than quintupled since 1996 when it was 8.6% respectively 3.9%.

The most important region of origin of migrants is Europe, with many coming from Italy (26,676) or France (13,506). Moreover, many migrants come from Latin American nations such as Bolivia, Ecuador or Colombia. Since the 1990s, and similar to other migrants, many Latin Americans have settled in northern parts of the city.

There exists a relatively large Pakistani community in Barcelona with up to twenty thousand nationals. The community consists of significantly more men than women. Many of the Pakistanis are living in Ciutat Vella. First Pakistani migrants came in the 1970s, with increasing numbers in the 1990s.

Other significant migrant groups come from Asia as from China and the Philippines. There is a Japanese community clustered in Bonanova, Les Tres Torres, Pedralbes, and other northern neighbourhoods, and a Japanese international school serves that community.

Most of the inhabitants state they are Roman Catholic (208 churches). In a 2011 survey conducted by InfoCatólica, 49.5% of Barcelona residents of all ages identified themselves as Catholic. This was the first time that more than half of respondents did not identify themselves as Catholic Christians. The numbers reflect a broader trend in Spain whereby the numbers of self-identified Catholics have declined. In 2019, a survey by Centro de Investigaciones Sociológicas showed that 53.2% of residents in Barcelona identified themselves as Catholic (9.9% practising Catholics, 43.3% non-practising Catholics).

The province has the largest Muslim community in Spain, 322,698 people in Barcelona province are of Muslim religion. A considerable number of Muslims live in Barcelona due to immigration (169 locations, mostly professed by Moroccans in Spain). In 2014, 322,698 out of 5.5 million people in the province of Barcelona identified themselves as Muslim, which makes 5.6% of total population.

The city also has the largest Jewish community in Spain, with an estimated 3,500 Jews living in the city. There are also a number of other groups, including Evangelical (71 locations, mostly professed by Roma), Jehovah's Witnesses (21 Kingdom Halls), Buddhists (13 locations), and Eastern Orthodox.

The Barcelona metropolitan area comprises over 66% of the people of Catalonia, one of the richer regions in Europe and the fourth richest region per capita in Spain, with a GDP per capita amounting to €28,400 (16% more than the EU average). The greater Barcelona metropolitan area had a GDP amounting to $177 billion (equivalent to $34,821 in per capita terms, 44% more than the EU average), making it the 4th most economically powerful city by gross GDP in the European Union, and 35th in the world in 2009. Barcelona city had a very high GDP of €80,894 per head in 2004, according to Eurostat. Furthermore, Barcelona was Europe's fourth best business city and fastest improving European city, with growth improved by 17% per year .

Barcelona was the 24th most "livable city" in the world in 2015 according to lifestyle magazine "Monocle." Similarly, according to Innovation Analysts 2thinknow, Barcelona occupies 13th place in the world on "Innovation Cities™ Global Index".

Barcelona has a long-standing mercantile tradition. Less well known is that the city industrialised early, taking off in 1833, when Catalonia's already sophisticated textile industry began to use steam power. It became the first and most important industrial city in the Mediterranean basin. Since then, manufacturing has played a large role in its history.

Borsa de Barcelona (Barcelona Stock Exchange) is the main stock exchange in the northeastern part of the Iberian Peninsula.

Barcelona was recognised as the Southern European City of the Future for 2014/15, based on its economic potential, by "FDi Magazine" in their bi-annual rankings.

Drawing upon its tradition of creative art and craftsmanship, Barcelona is known for its award-winning industrial design. It also has several congress halls, notably Fira de Barcelona – the second largest trade fair and exhibition centre in Europe, that host a quickly growing number of national and international events each year (at present above 50). The total exhibition floor space of Fira de Barcelona venues is , not counting Gran Via centre on the Plaza de Europa. However, the Eurozone crisis and deep cuts in business travel affected the council's positioning of the city as a convention centre.

An important business centre, the World Trade Center Barcelona, is located in Barcelona's Port Vell harbour.

The city is known for hosting well as world-class conferences and expositions, including the 1888 "Exposición Universal de Barcelona", the 1929 Barcelona International Exposition (Expo 1929), the 2004 Universal Forum of Cultures and the 2004 World Urban Forum.

Barcelona was the 20th-most-visited city in the world by international visitors and the fifth most visited city in Europe after London, Paris, Istanbul and Rome, with 5.5 million international visitors in 2011. By 2015, both Prague and Milan had more international visitors. With its Rambles, Barcelona is ranked the most popular city to visit in Spain.

Barcelona as internationally renowned a tourist destination, with numerous recreational areas, one of the best beaches in the world, mild and warm climate, historical monuments, including eight UNESCO World Heritage Sites, 519 hotels including 35 five star hotels, and developed tourist infrastructure.

Due to its large influx of tourists each year, Barcelona, like many other tourism capitals, has to deal with pickpockets, with wallets and passports being commonly stolen items. For this reason, most travel guides recommend that visitors take precautions to ensure their possessions' safety, especially inside the metro premises. Despite its moderate pickpocket rate, Barcelona is considered one of the safest cities in terms of health security and personal safety, mainly because of a sophisticated policing strategy that has dropped crime by 32% in just over three years and has led it to be considered the 15th safest city in the world by Business Insider.

While tourism produces economic benefits, according to one report, the city is "overrun [by] hordes of tourists". In early 2017, over 150,000 protesters warned that tourism is destabilizing the city. Slogans included "Tourists go home", "Barcelona is not for sale" and "We will not be driven out". By then, number of visitors had increased from 1.7 million in 1990 to 32 million in a city with a population of 1.62 million, increasing the cost of rental housing for residents and overcrowding the public places. While tourists spent an estimated €30 billion in 2017, they are viewed by some as a threat to Barcelona's identity.

A May 2017 article in England's The Telegraph newspaper included Barcelona among the "Eight Places That Hate Tourists the Most" and included a comment from Mayor Ada Colau, "We don't want the city to become a cheap souvenir shop [like Venice]". To moderate the problem, the city has stopped issuing licenses for new hotels and holiday apartments; it also fined AirBnb with a €30,000. The mayor has suggested an additional tourist tax and setting a limit on the number of visitors. One industry insider, Justin Francis, founder of the Responsible Travel agency, stated that steps must be taken to limit the number of visitors that are causing an "overtourism crisis" in several major European cities. "Ultimately, residents must be prioritised over tourists for housing, infrastructure and access to services because they have a long-term stake in the city's success.", he said. "Managing tourism more responsibly can help", Francis later told a journalist, "but some destinations may just have too many tourists, and Barcelona may be a case of that".

Industry generates 21% of the total gross domestic product (GDP) of the region, with the energy, chemical and metallurgy industries accounting for 47% of industrial production. The Barcelona metropolitan area had 67% of the total number of industrial establishments in Catalonia as of 1997.

Barcelona has long been an important European automobile manufacturing centre. Formerly there were automobile factories of AFA, Abadal, Actividades Industriales, Alvarez, America, Artés de Arcos, Balandrás, Baradat-Esteve, Biscúter, J. Castro, Clúa, David, Delfín, Díaz y Grilló, Ebro trucks, , Elizalde, Automóviles España, Eucort, Fenix, Fábrica Hispano, Auto Academia Garriga, Fábrica Española de Automóviles Hebe, Hispano-Suiza, Huracán Motors, Talleres Hereter, Junior SL, Kapi, La Cuadra, M.A., Automóviles Matas, Motores y Motos, Nacional Custals, National Pescara, Nacional RG, Nacional Rubi, Nacional Sitjes, Automóviles Nike, Orix, Otro Ford, Partia, Pegaso, PTV, Ricart, Ricart-España, Industrias Salvador, Siata Española, Stevenson, Romagosa y Compañía, Garaje Storm, Talleres Hereter, Trimak, Automóviles Victoria, Manufacturas Mecánicas Aleu.

Today, the headquarters and a large factory of SEAT (the largest Spanish automobile manufacturer) are in one of its suburbs. There is also a Nissan factory in the logistics and industrial area of the city. The factory of Derbi, a large manufacturer of motorcycles, scooters and mopeds, also lies near the city.

As in other modern cities, the manufacturing sector has long since been overtaken by the services sector, though it remains very important. The region's leading industries are textiles, chemical, pharmaceutical, motor, electronic, printing, logistics, publishing, in telecommunications industry and culture the notable Mobile World Congress, and information technology services.

The traditional importance of textiles is reflected in Barcelona's drive to become a major fashion centre. There have been many attempts to launch Barcelona as a fashion capital, notably "Gaudi Home".

Beginning in the summer of 2000, the city hosted the Bread & Butter urban fashion fair until 2009, when its organisers announced that it would be returning to Berlin. This was a hard blow for the city as the fair brought €100 m to the city in just three days.

Since 2009, "The Brandery", an urban fashion show, has been held in Barcelona twice a year until 2012. According to the Global Language Monitor's annual ranking of the world's top fifty fashion capitals Barcelona was named as the seventh most important fashion capital of the world right after Milano and before Berlin in 2015.

As the capital of the autonomous community of Catalonia, Barcelona is the seat of the Catalan government, known as the "Generalitat de Catalunya"; of particular note are the executive branch, the parliament, and the High Court of Justice of Catalonia. The city is also the capital of the Province of Barcelona and the Barcelonès comarca (district).

Barcelona is governed by a city council formed by 41 city councillors, elected for a four-year term by universal suffrage. As one of the two biggest cities in Spain, Barcelona is subject to a special law articulated through the "Carta Municipal" (Municipal Law). A first version of this law was passed in 1960 and amended later, but the current version was approved in March 2006.<ref name="ley 1/2006"></ref> According to this law, Barcelona's city council is organised in two levels: a political one, with elected city councillors, and one executive, which administrates the programs and executes the decisions taken on the political level. This law also gives the local government a special relationship with the central government and it also gives the mayor wider prerogatives by the means of municipal executive commissions. It expands the powers of the city council in areas like telecommunications, city traffic, road safety and public safety. It also gives a special economic regime to the city's treasury and it gives the council a veto in matters that will be decided by the central government, but that will need a favourable report from the council.

The "Comissió de Govern" (Government Commission) is the executive branch, formed by 24 councillors, led by the Mayor, with 5 lieutenant-mayors and 17 city councillors, each in charge of an area of government, and 5 non-elected councillors. The plenary, formed by the 41 city councillors, has advisory, planning, regulatory, and fiscal executive functions. The six "Commissions del Consell Municipal" (City council commissions) have executive and controlling functions in the field of their jurisdiction. They are composed by a number of councillors proportional to the number of councillors each political party has in the plenary. The city council has jurisdiction in the fields of city planning, transportation, municipal taxes, public highways security through the "Guàrdia Urbana" (the municipal police), city maintenance, gardens, parks and environment, facilities (like schools, nurseries, sports centres, libraries, and so on), culture, sports, youth and social welfare. Some of these competencies are not exclusive, but shared with the Generalitat de Catalunya or the central Spanish government. In some fields with shared responsibility (such as public health, education or social services), there is a shared Agency or Consortium between the city and the Generalitat to plan and manage services.

The executive branch is led by a Chief Municipal Executive Officer which answers to the Mayor. It is made up of departments which are legally part of the city council and by separate legal entities of two types: autonomous public departments and public enterprises.

The seat of the city council is on the Plaça de Sant Jaume, opposite the seat of Generalitat de Catalunya. Since the coming of the Spanish democracy, Barcelona had been governed by the PSC, first with an absolute majority and later in coalition with ERC and ICV. After the May 2007 election, the ERC did not renew the coalition agreement and the PSC governed in a minority coalition with ICV as the junior partner.

After 32 years, on 22 May 2011, CiU gained a plurality of seats at the municipal election, gaining 15 seats to the PSC's 11. The PP hold 8 seats, ICV 5 and ERC 2.

Since 1987, the city has been divided into 10 administrative districts ("districtes" in Catalan, "distritos" in Spanish):

The districts are based mostly on historical divisions, and several are former towns annexed by the city of Barcelona in the 18th and 19th centuries that still maintain their own distinct character. Each district has its own council led by a city councillor. The composition of each district council depends on the number of votes each political party had in that district, so a district can be led by a councillor from a different party than the executive council.

Barcelona has a well-developed higher education system of public universities. Most prominent among these is the University of Barcelona (established in 1450), a world-renowned research and teaching institution with campuses around the city. Barcelona is also home to the Polytechnic University of Catalonia, and the newer Pompeu Fabra University, and, in the private sector the EADA Business School founded in 1957, became the first Barcelona institution to run manager training programmes for the business community. IESE Business School, as well as the largest private educational institution, the Ramon Llull University, which encompasses schools and institutes such as the ESADE Business School. The Autonomous University of Barcelona, another public university, is located in Bellaterra, a town in the Metropolitan Area. Toulouse Business School and the Open University of Catalonia (a private Internet-centred open university) are also based in Barcelona.
The city has a network of public schools, from nurseries to high schools, under the responsibility of a consortium led by city council (though the curriculum is the responsibility of the Generalitat de Catalunya). There are also many private schools, some of them Roman Catholic. Most such schools receive a public subsidy on a per-student basis, are subject to inspection by the public authorities, and are required to follow the same curricular guidelines as public schools, though they charge tuition. Known as "escoles concertades", they are distinct from schools whose funding is entirely private ("escoles privades").

The language of instruction at public schools and "escoles concertades" is Catalan, as stipulated by the 2009 Catalan Education Act. Spanish may be used as a language of instruction by teachers of Spanish literature or language, and foreign languages by teachers of those languages. An experimental partial immersion programme adopted by some schools allows for the teaching of a foreign language (English, generally) across the curriculum, though this is limited to a maximum of 30% of the school day. No public school or "escola concertada" in Barcelona may offer 50% or full immersion programmes in a foreign language, nor does any public school or "escola concertada" offer International Baccalaureate programmes.

Barcelona's cultural roots go back 2000 years. Since the arrival of democracy, the Catalan language (very much repressed during the dictatorship of Franco) has been promoted, both by recovering works from the past and by stimulating the creation of new works. Barcelona is designated as a world-class city by the Globalization and World Cities Study Group and Network. It has also been part of the UNESCO Creative Cities Network as a City of Literature since 2015.

Barcelona has many venues for live music and theatre, including the world-renowned Gran Teatre del Liceu opera house, the Teatre Nacional de Catalunya, the Teatre Lliure and the Palau de la Música Catalana concert hall. Barcelona also is home to the Barcelona Symphony and Catalonia National Orchestra (Orquestra Simfònica de Barcelona i Nacional de Catalunya, usually known as OBC), the largest symphonic orchestra in Catalonia. In 1999, the OBC inaugurated its new venue in the brand-new Auditorium (L'Auditori). It performs around 75 concerts per season and its current director is Eiji Oue. It is home to the Barcelona Guitar Orchestra, directed by Sergi Vicente.
The major thoroughfare of La Rambla is home to mime artists and street performers.
Yearly, two major pop music festivals take place in the city, the Sónar Festival and the Primavera Sound Festival. The city also has a thriving alternative music scene, with groups such as The Pinker Tones receiving international attention.

"El Periódico de Catalunya", "La Vanguardia" and "Ara" are Barcelona's three major daily newspapers (the first two with Catalan and Spanish editions, "Ara" only in Catalan) while "Sport" and "El Mundo Deportivo" (both in Spanish) are the city's two major sports daily newspapers, published by the same companies. The city is also served by a number of smaller publications such as "Ara" and "El Punt Avui" (in Catalan), by nationwide newspapers with special Barcelona editions like "El Pais" (in Spanish, with an online version in Catalan) and "El Mundo" (in Spanish), and by several free newspapers like "20 minutos" and "Què" (all bilingual).

Barcelona's oldest and main online newspaper "VilaWeb" is also the oldest one in Europe (with Catalan and English editions).

Several major FM stations include Catalunya Ràdio, RAC 1, RAC 105 and Cadena SER. Barcelona also has a local TV stations, BTV, owned by city council. The headquarters of Televisió de Catalunya, Catalonia's public network, are located in Sant Joan Despí, in Barcelona's metropolitan area.

Barcelona has a long sporting tradition and hosted the highly successful 1992 Summer Olympics as well as several matches during the 1982 FIFA World Cup (at the two stadiums). It has hosted about 30 sports events of international significance.

FC Barcelona is a sports club best known worldwide for its football team, one of the largest and the second richest in the world. It has 74 national trophies (while finishing 46 times as runners-up) and 17 continental prizes (with being runners-up 11 times), including five UEFA Champions League trophies out of eight finals and three FIFA Club World Cup wins out of four finals. It is the only male football team in the world to win six trophies in a calendar year (in 2009). FC Barcelona also has professional teams in other sports like FC Barcelona Regal (basketball), FC Barcelona Handbol (handball), FC Barcelona Hoquei (roller hockey), FC Barcelona Ice Hockey (ice hockey), FC Barcelona Futsal (futsal) and FC Barcelona Rugby (rugby union), all at one point winners of the highest national and/or European competitions. The club's museum is the second most visited in Catalonia. The matches against cross-town rivals RCD Espanyol are of particular interest, but there are other Barcelonan football clubs in lower categories, like CE Europa and UE Sant Andreu. FC Barcelona's basketball team has a noted rivalry in the Liga ACB with nearby Joventut Badalona.
Barcelona has three UEFA elite stadiums: FC Barcelona's Camp Nou, the largest stadium in Europe with a capacity of 99,354; the publicly owned Estadi Olímpic Lluís Companys, with a capacity of 55,926; used for the 1992 Olympics; and Estadi Cornellà-El Prat, with a capacity of 40,500. Furthermore, the city has several smaller stadiums such as Mini Estadi (also owned by FC Barcelona) with a capacity of 15,000, Camp Municipal Narcís Sala with a capacity of 6,563 and Nou Sardenya with a capacity of 7,000. The city has a further three multifunctional venues for sports and concerts: the Palau Sant Jordi with a capacity of 12,000 to 24,000 (depending on use), the Palau Blaugrana with a capacity of 7,500, and the Palau dels Esports de Barcelona with a capacity of 3,500.

Barcelona was the host city for the 2013 World Aquatics Championships, which were held at the Palau San Jordi.
Several road running competitions are organised year-round in Barcelona: the Barcelona Marathon every March with over 10,000 participants in 2010, the Cursa de Bombers in April, the Cursa de El Corte Inglés in May (with about 60,000 participants each year), the Cursa de la Mercè, the Cursa Jean Bouin, the Milla Sagrada Família and the San Silvestre. There's also the Ultratrail Collserola which passes through the Collserola forest. The Open Seat Godó, a 50-year-old ATP World Tour 500 Series tennis tournament, is held annually in the facilities of the Real Club de Tenis Barcelona. Each Christmas, a swimming race across the port is organised. Near Barcelona, in Montmeló, the 107,000 capacity Circuit de Barcelona-Catalunya racetrack hosts the Formula One Spanish Grand Prix, the Catalan motorcycle Grand Prix, the Spanish GT Championship and races in the GP2 Series. Skateboarding and cycling are also very popular in Barcelona; in and around the city there are dozens of kilometers of bicycle paths.

Barcelona is also home to numerous social centres and illegal squats that effectively form a shadow society mainly made up of the unemployed, immigrants, dropouts, anarchists, anti-authoritarians and autonomists. Peter Gelderloos estimates that there around 200 squatted buildings and 40 social centres across the city with thousands of inhabitants, making it one of the largest squatter movements in the world. He notes that they pirate electricity, internet and water allowing them to live on less than one euro a day. He argues that these squats embrace an anarcho-communist and anti-work philosophy, often freely fixing up new houses, cleaning, patching roofs, installing windows, toilets, showers, lights and kitchens. In the wake of austerity, the squats have provided a number of social services to the surrounding residents, including bicycle repair workshops, carpentry workshops, self-defense classes, free libraries, community gardens, free meals, computer labs, language classes, theatre groups, free medical care and legal support services. The squats help elderly residents avoid eviction and organise various protests throughout Barcelona. Notable squats include Can Vies and Can Masdeu. Police have repeatedly tried to shut down the squatters movement with waves of evictions and raids, but the movement is still going strong.

Barcelona is served by Barcelona-El Prat Airport, about from the centre of Barcelona. It is the second-largest airport in Spain, and the largest on the Mediterranean coast, which handled more than 50.17 million passengers in 2018, showing an annual upward trend. It is a main hub for Vueling Airlines and Ryanair, and also a focus for Iberia and Air Europa. The airport mainly serves domestic and European destinations, although some airlines offer destinations in Latin America, Asia and the United States. The airport is connected to the city by highway, metro (Airport T1 and Airport T2 stations), commuter train (Barcelona Airport railway station) and scheduled bus service. A new terminal (T1) has been built, and entered service on 17 June 2009.

Some low-cost airlines, also use Girona-Costa Brava Airport, about to the north, Reus Airport, to the south, or Lleida-Alguaire Airport, about to the west, of the city. Sabadell Airport is a smaller airport in the nearby town of Sabadell, devoted to pilot training, aerotaxi and private flights.

The Port of Barcelona has a 2000-year-old history and a great contemporary commercial importance. It is Europe's ninth largest container port, with a trade volume of 1.72 million TEU's in 2013. The port is managed by the Port Authority of Barcelona. Its are divided into three zones: Port Vell (the old port), the commercial port and the logistics port (Barcelona Free Port). The port is undergoing an enlargement that will double its size thanks to diverting the mouth of the Llobregat river to the south.
The Barcelona harbour is the leading European cruiser port and a most important Mediterranean turnaround base. In 2013, 3,6 million of pleasure cruises passengers used services of the Port of Barcelona.

The Port Vell area also houses the Maremagnum (a commercial mall), a multiplex cinema, the IMAX Port Vell and one of Europe's largest aquariums – Aquarium Barcelona, containing 8,000 fish and 11 sharks contained in 22 basins filled with 4 million litres of sea water. The Maremagnum, being situated within the confines of the port, is the only commercial mall in the city that can open on Sundays and public holidays.

Barcelona is a major hub for RENFE, the Spanish state railway network. The city's main Inter-city rail station is Barcelona Sants railway station, whilst Estació de França terminus serves a secondary role handling suburban, regional and medium distance services. Freight services operate to local industries and to the Port of Barcelona.

RENFE's AVE high-speed rail system, which is designed for speeds of , was extended from Madrid to Barcelona in 2008 in the form of the Madrid–Barcelona high-speed rail line. A shared RENFE-SNCF high-speed rail connecting Barcelona and France (Paris, Marseilles and Toulouse, through Perpignan–Barcelona high-speed rail line) was launched in 2013. Both these lines serve Barcelona Sants terminal station.

Barcelona is served by an extensive local public transport network that includes a metro system, a bus network, a regional railway system, trams, funiculars, rack railways, a Gondola lift and aerial cable cars. These networks and lines are run by a number of different operators but they are integrated into a coordinated fare system, administered by the Autoritat del Transport Metropolità (ATM). The system is divided into fare zones (1 to 6) and various Integrated Travel Cards are available.

The Barcelona Metro network comprises twelve lines, identified by an "L" followed by the line number as well as by individual colours. The Metro largely runs underground; eight Metro lines are operated on dedicated track by the Transports Metropolitans de Barcelona (TMB), whilst four lines are operated by the Ferrocarrils de la Generalitat de Catalunya (FGC) and some of them share tracks with RENFE commuter lines.

In addition to the city Metro, several regional rail lines operated by RENFE's Rodalies de Catalunya run across the city, providing connections to outlying towns in the surrounding region.

The city's two modern tram systems, Trambaix and Trambesòs, are operated by TRAMMET. A heritage tram line, the Tramvia Blau, also operates between the metro Line 7 and the Funicular del Tibidabo.

Barcelona's metro and rail system is supplemented by several aerial cable cars, funiculars and rack railways that provide connections to mountain-top stations. FGC operates the Funicular de Tibidabo up the hill of Tibidabo and the Funicular de Vallvidrera (FGC), while TMB runs the Funicular de Montjuïc up Montjuïc. The city has two aerial cable cars: the Montjuïc Cable Car, which serves Montjuïc castle, and the Port Vell Aerial Tramway that runs via Torre Jaume I and Torre Sant Sebastià over the port.

Buses in Barcelona are a major form of public transport, with extensive local, interurban and night bus networks. Most local services are operated by the TMB, although some other services are operated by a number of private companies, albeit still within the ATM fare structure. A separate private bus line, known as Aerobús, links the airport with the city centre, with its own fare structure.

The Estació del Nord (Northern Station), a former railway station which was renovated for the 1992 Olympic Games, now serves as the terminus for long-distance and regional bus services.

Barcelona has a metered taxi fleet governed by the Institut Metropolità del Taxi (Metropolitan Taxi Institute), composed of more than 10,000 cars. Most of the licences are in the hands of self-employed drivers. With their black and yellow livery, Barcelona's taxis are easily spotted, and can be caught from one of many taxi ranks, hailed on street, called by telephone or via app.

On 22 March 2007, Barcelona's City Council started the Bicing service, a bicycle service understood as a public transport. Once the user has their user card, they can take a bicycle from any of the more than 400 stations spread around the city and use it anywhere the urban area of the city, and then leave it at another station. The service has been a success, with 50,000 subscribed users in three months.

Barcelona lies on three international routes, including European route E15 that follows the Mediterranean coast, European route E90 to Madrid and Lisbon, and European route E09 to Paris. It is also served by a comprehensive network of motorways and highways throughout the metropolitan area, including A-2, A-7/AP-7, C-16, C-17, C-31, C-32, C-33, C-60.

The city is circled by three half ring roads or bypasses, Ronda de Dalt (B-20) (on the mountain side), Ronda del Litoral (B-10) (along the coast) and Ronda del Mig (separated into two parts: Travessera de Dalt in the north and the Gran Via de Carles III), two partially covered fast highways with several exits that bypass the city.

The city's main arteries include Diagonal Avenue, which crosses it diagonally, Meridiana Avenue which leads to Glòries and connects with Diagonal Avenue and Gran Via de les Corts Catalanes, which crosses the city from east to west, passing through its centre. The famous boulevard of La Rambla, whilst no longer an important vehicular route, remains an important pedestrian route.

The "Barri Gòtic" (Catalan for "Gothic Quarter") is the centre of the old city of Barcelona. Many of the buildings date from medieval times, some from as far back as the Roman settlement of Barcelona. Catalan "modernista" architecture (related to the movement known as Art Nouveau in the rest of Europe) developed between 1885 and 1950 and left an important legacy in Barcelona. Several of these buildings are World Heritage Sites. Especially remarkable is the work of architect Antoni Gaudí, which can be seen throughout the city. His best-known work is the immense but still unfinished church of the Sagrada Família, which has been under construction since 1882 and is still financed by private donations. , completion is planned for 2026.

Barcelona was also home to Mies van der Rohe's Barcelona Pavilion. Designed in 1929 for the International Exposition for Germany, it was an iconic building that came to symbolise modern architecture as the embodiment of van der Rohe's aphorisms "less is more" and "God is in the details." The Barcelona pavilion was intended as a temporary structure and was torn down in 1930 less than a year after it was constructed. A modern re-creation by Spanish architects now stands in Barcelona, however, constructed in 1986.

Barcelona won the 1999 RIBA Royal Gold Medal for its architecture, the first (and , only) time that the winner has been a city rather than an individual architect.

Barcelona is the home of many points of interest declared World Heritage Sites by UNESCO:


Barcelona has a great number of museums, which cover different areas and eras. The National Museum of Art of Catalonia possesses a well-known collection of Romanesque art, while the Barcelona Museum of Contemporary Art focuses on post-1945 Catalan and Spanish art. The Fundació Joan Miró, Picasso Museum, and Fundació Antoni Tàpies hold important collections of these world-renowned artists, as well as the Can Framis Museum, focused on post-1960 Catalan Art owned by Fundació Vila Casas.
Several museums cover the fields of history and archaeology, like the Barcelona City History Museum (MUHBA), the Museum of the History of Catalonia, the Archeology Museum of Catalonia, the Maritime Museum of Barcelona, the Music Museum of Barcelona and the privately owned Egyptian Museum. The Erotic museum of Barcelona is among the most peculiar ones, while CosmoCaixa is a science museum that received the European Museum of the Year Award in 2006.

The Museum of Natural Sciences of Barcelona was founded in 1882 under the name of "Museo Martorell de Arqueología y Ciencias Naturales" (Spanish for "Martorell Museum of Archaeology and Natural Sciences"). In 2011 the Museum of Natural Sciences ended up with a merge of five institutions: the Museum of Natural Sciences of Barcelona (the main site, at the Forum Building), the Martorell Museum (the historical seat of the Museum, opened to the public from 1924 to 2010 as a geology museum), the "Laboratori de Natura", at the Castle of the Three Dragons (from 1920 to 2010: the Zoology Museum), the Historical Botanical Garden of Barcelona, founded 1930, and the Botanical garden of Barcelona, founded 1999. Those two gardens are a part of the Botanical Institute of Barcelona too.

The FC Barcelona Museum has been the most visited museum in the city of Barcelona, with 1,506,022 visitors in 2013.

Barcelona contains sixty municipal parks, twelve of which are historic, five of which are thematic (botanical), forty-five of which are urban, and six of which are forest. They range from vest-pocket parks to large recreation areas. The urban parks alone cover 10% of the city (). The total park surface grows about per year, with a proportion of of park area per inhabitant.

Of Barcelona's parks, Montjuïc is the largest, with 203 ha located on the mountain of the same name. It is followed by Parc de la Ciutadella (which occupies the site of the old military citadel and which houses the Parliament building, the Barcelona Zoo, and several museums); including the zoo), the Guinardó Park (), Park Güell (designed by Antoni Gaudí; ), Oreneta Castle Park (also ), Diagonal Mar Park (, inaugurated in 2002), Nou Barris Central Park (), Can Dragó Sports Park and Poblenou Park (both ), the Labyrinth Park (), named after the garden maze it contains. There are also several smaller parks, for example, the Parc de Les Aigües (). A part of the Collserola Park is also within the city limits. PortAventura World, one of the largest resort in Europe, with 5,837,509 visitors per year, is located one hour's drive from Barcelona. Also, within the city lies Tibidabo Amusement Park, a smaller amusement park in Plaza del Tibidabo, with the Muntanya Russa amusement ride.

Barcelona beach was listed as number one in a list of the top ten city beaches in the world according to "National Geographic" and "Discovery Channel". Barcelona contains seven beaches, totalling of coastline. Sant Sebastià, Barceloneta and Somorrostro beaches, both in length, are the largest, oldest and the most-frequented beaches in Barcelona.

The Olympic Harbour separates them from the other city beaches: Nova Icària, Bogatell, Mar Bella, Nova Mar Bella and Llevant. These beaches (ranging from were opened as a result of the city restructuring to host the 1992 Summer Olympics, when a great number of industrial buildings were demolished. At present, the beach sand is artificially replenished given that storms regularly remove large quantities of material. The 2004 Universal Forum of Cultures left the city a large concrete bathing zone on the eastmost part of the city's coastline. Most recently, Llevant is the first beach to allow dogs access during summer season.

Barcelona is twinned with:

Barcelona also cooperates with:






</doc>
<doc id="4444" url="https://en.wikipedia.org/wiki?curid=4444" title="Bandy">
Bandy

Bandy is a team winter sport played on ice, in which skaters use sticks to direct a ball into the opposing team's goal.

The sport is considered a form of hockey and has a common background with ice hockey and field hockey. Bandy has also been influenced by the rules of association football: both games are normally played in halves of 45 minutes, there are 11 players on each team, and the fields in both games are about the same size. Bandy is played, like ice hockey, on ice but players use bowed sticks and a small ball, as in field hockey.

A variant of bandy, rink bandy, is played to the same rules but on a field the size of an ice hockey rink, with ice hockey goal cages and with six players on each team, or five in USA Rink Bandy League. Traditional eleven-a-side bandy and rink bandy are recognized by the International Olympic Committee. More informal varieties also exist, like seven-a-side bandy with normally sized goal cages but without corner strokes. Those rules were applied at Davos Cup in 2016.

Rink bandy has in turn led to the creation of the sport rinkball. Bandy is also the predecessor of floorball, which was invented when people started playing with plastic bandy-shaped sticks and lightweight balls when running on the floors of indoor gym halls.

Based on the number of participating athletes, bandy is the world's second-most participated winter sport after ice hockey. Bandy is also ranked as the number two winter sport in terms of tickets sold per day of competitions at the sport's world championship.

However, compared with the seven Winter Olympic sports, bandy's popularity compared to that of other winter sports across the globe is considered by the International Olympic Committee to represent a "gap between popularity and participation and global audiences." This is held to constitute a roadblock to future Olympic inclusion.

The earliest origin of the sport is debated. Though many Russians see their old countrymen as the creators of the sport – reflected by the unofficial title for bandy, "Russian hockey" (русский хоккей) – Russia, England and Holland each had sports or pastimes which can be seen as forerunners of the present sport.

English bandy developed as a winter sport in the Fens of East Anglia. Large expanses of ice would form on the flooded meadows or shallow washes in cold winters, and skating has been a tradition. Members of the Bury Fen Bandy Club published rules of the game in 1882, and introduced it into other countries. The first international match took place in 1891 between Bury Fen and the then Haarlemsche Hockey & Bandy Club from the Netherlands (a club which after a couple of club fusions now is named HC Bloemendaal). The same year, the National Bandy Association was started in England.

The match later dubbed "the original bandy match", was actually held in 1875 at The Crystal Palace in London. However, at the time, the game was called "hockey on the ice", probably as it was considered an ice variant of field hockey.

The first national bandy league was started in Sweden in 1902. Bandy was played at the Nordic Games in Stockholm and Kristiania (present day Oslo) in 1901, 1903, and 1905 and between Swedish, Finnish and Russian teams at similar games in Helsinki in 1907. A European championship was held in 1913 with eight countries participating.

In modern times, Russia has held a top position in the bandy area, both as a founding nation of the International Federation in 1955 and fielding the most successful team in the World Championships (when counting the previous Soviet Union team and Russia together).

The highest altitude where bandy has been played is in the capital of the Tajik autonomous province of Gorno-Badakhshan, Khorugh.

As a precursor to ice hockey bandy has influenced its development and history – mainly in European and former Soviet countries. While modern ice hockey was created in Canada, a game more similar to bandy was played initially, after British soldiers introduced the game in the late 19th century. At the same time as modern ice hockey rules were formalized in British North America (present day Canada), bandy rules were formulated in Europe. A cross between English and Russian bandy rules eventually developed, with the football-inspired English rules dominant, together with the Russian low border along most of the two sidelines, and this is the basis of the present sport since the 1950s.

Before Canadians introduced ice hockey into Europe in the early 20th century, "hockey" was another name for bandy, and still is in parts of Russia and Kazakhstan.

With football and bandy being dominant sports in parts of Europe, it was common for sports clubs to have bandy and football sections, with athletes playing both sports at different times of the year. Some examples are English Nottingham Forest Football and Bandy Club (today known just as Nottingham Forest F.C.) and Norwegian Strømsgodset IF and Mjøndalen IF, with the latter still having an active bandy section. In Sweden, most football clubs which were active during the first half of the 20th century also played bandy. Later, as the season for each sport increased in time, it was not as easy for the players to engage in both sports, so some clubs came to concentrate on one or the other. Many old clubs still have both sports on their program.

Both bandy and ice hockey were played in Europe during the 20th century, especially in Sweden, Finland, and Norway. Ice hockey became more popular than bandy in most of Europe mostly because it had become an Olympic sport, while bandy had not. Athletes in Europe who had played bandy switched to ice hockey in the 1920s to compete in the Olympics. The smaller ice fields needed for ice hockey also made its rinks easier to maintain, especially in countries with short winters. On the other hand, ice hockey was not played in the Soviet Union until the 1950s when the USSR wanted to compete internationally. The typical European style of ice hockey, with flowing, less physical play, represents a heritage of bandy.

The sport's English name comes from the verb "to bandy", from the Middle French "bander" ("to strike back and forth"), and originally referred to a 17th-century Irish game similar to field hockey. The curved stick was also called a "bandy". The etymological connection to the similarly named Welsh hockey game of bando is not clear.

An old name for bandy is "hockey on the ice"; in the first rule books from England at the turn of the Century 1900, the sport is literally called "bandy or hockey on the ice". Since the mid-20th century the term bandy is usually preferred to prevent confusion with ice hockey.

The sport is known as bandy in many languages though there are a few notable exceptions. In Russian bandy is called "Russian hockey" (русский хоккей) or more frequently, and officially, "hockey with a ball" (xоккей с мячом) while ice hockey is called "hockey with a puck" (xоккей с шайбой) or more frequently just "hockey". If the context makes it clear that bandy is the subject, it as well can be called just "hockey". In Belarusian, Ukrainian and Bulgarian it is also called "hockey with a ball" (хакей з мячoм, хокей з м'ячем and хокей с топка respectively). In Slovak "bandy hockey" (bandyhokej) is the name. In Armenian, Kazakh, Kyrgyz, Mongol and Uzbek, bandy is known as "ball hockey" (գնդակով հոկեյ, допты хоккей, топтуу хоккей, бөмбөгтэй хоккей and koptokli xokkey respectively). In Finnish the two sports are distinguished as "ice ball" (jääpallo) and "ice puck" (jääkiekko), as well as in Hungarian (jéglabda; jégkorong), although in Hungarian it is more often called "bandy" nowadays. In Estonian bandy is also called "ice ball" (jääpall). In Mandarin Chinese it is "bandy ball" (班迪球). In Scottish Gaelic the name is "ice shinty" (camanachd-deighe). In old times shinty or shinney were also sometimes used in English for bandy.

Bandy is played on ice, using a single round ball. Two teams of 11 players each compete to get the ball into the other team's goal using sticks, thereby scoring a goal.

The game is designed to be played on a rectangle of ice the same size as a football field. Bandy also has other rules that are similar to football. Each team has 11 players, one of whom is a goalkeeper. The offside rule is also employed. A goal cannot be scored from a goal throw, but unlike football, a goal can be scored from a stroke-in or a corner stroke. All free strokes are "direct" and allow a goal to be scored without another player touching the ball.

The team that has scored more goals at the end of the game is the winner. If both teams have scored an equal number of goals, then, with some exceptions, the game is a draw.

The primary rule is that the players (other than the goalkeepers) may not intentionally touch the ball with their heads, hands or arms during play. Although players usually use their sticks to move the ball around, they may use any part of their bodies other than their heads, hands or arms and may use their skates in a limited manner. Heading the ball results in a five-minute penalty.

In typical game play, players attempt to propel the ball toward their opponents' goal through individual control of the ball, such as by dribbling, passing the ball to a teammate, and taking shots at the goal, which is guarded by the opposing goalkeeper. Opposing players may try to regain control of the ball by intercepting a pass or through tackling the opponent who controls the ball. However, physical contact between opponents is limited. Bandy is generally a free-flowing game, with play stopping only when the ball has left the field of play, or when play is stopped by the referee. After a stoppage, play can recommence with a free stroke, a penalty shot or a corner stroke. If the ball has left the field along the sidelines, the referee must decide which team touched the ball last, and award a restart stroke to the opposing team, just like football's throw-in.

The rules do not specify any player positions other than goalkeeper, but a number of player specialisations have evolved. Broadly, these include three main categories: forwards, whose main task is to score goals; defenders, who specialise in preventing their opponents from scoring; and midfielders, who take the ball from the opposition and pass it to the forwards. Players in these positions are referred to as outfield players, to discern them from the single goalkeeper. These positions are further differentiated by which side of the field the player spends most time in. For example, there are central defenders, and left and right midfielders. The ten outfield players may be arranged in these positions in any combination (for example, there may be three defenders, five midfielders, and two forwards), and the number of players in each position determines the style of the team's play; more forwards and fewer defenders would create a more aggressive and offensive-minded game, while the reverse would create a slower, more defensive style of play. While players may spend most of the game in a specific position, there are few restrictions on player movement, and players can switch positions at any time. The layout of the players on the pitch is called the team's "formation", and defining the team's formation and tactics is usually the prerogative of the team's manager(s).


There are eighteen rules in official play, designed to apply to all levels of bandy, although certain modifications for groups such as juniors, veterans or women are permitted. The rules are often framed in broad terms, which allow flexibility in their application depending on the nature of the game.

The Bandy Playing Rules can be found on the official website of the Federation of International Bandy, and are overseen by the Rules and Referee Committee.

Each team consists of a maximum of 11 players (excluding substitutes), one of whom must be the goalkeeper. A team of fewer than eight players may not start a game. Goalkeepers are the only players allowed to play the ball with their hands or arms, and they are only allowed to do so within the penalty area in front of their own goal.

Though there are a variety of positions in which the outfield (non-goalkeeper) players are strategically placed by a coach, these positions are not defined or required by the rules of the game.

The positions and formations of the players in bandy are virtually the same as the common association football positions and the same terms are used for the different positions of the players. A team usually consists of defenders, midfielders and forwards. The defenders can play in the form of centre-backs, full-backs and sometimes wing-backs, midfielders playing in the centre, attacking or defensive, and forwards in the form of centre forward, second strikers and sometimes a winger. Sometimes one player is also taking up the role of a libero.

Any number of players may be replaced by substitutes during the course of the game. Substitutions can be performed without notifying the referee and can be performed while the ball is in play. However, if the substitute enters the ice before his teammate has left it, this will result in a five-minute ban. A team can bring at the most four substitutes to the game and one of these is likely to be an extra goalkeeper.

A game is officiated by a referee, the authority and enforcer of the rules, whose decisions are final. The referee may have one or two assistant referees. A secretary outside of the field often takes care of the match protocol.


The basic equipment players are required to wear includes a pair of Bandy skates, a helmet, a mouth guard and, in the case of the goalkeeper, a face guard.

The teams must wear uniforms that make it easy to distinguish the two teams. The goal keeper wears distinct colours to single him out from his or her teammates, just as in football. The skates, sticks and any tape on the stick must be of another colour than the bandy ball, which shall be orange or cerise.

In addition to the aforementioned, various protections are used to protect knees, elbows, genitals and throat. The pants and gloves may contain padding.


The stick used in bandy is an essential part of the sport. It should be made of an approved material such as wood or a similar material and should not contain any metal or sharp parts which can hurt the surrounding players. Sticks are crooked and are available in five angles, where 1 has the smallest bend and 5 has the most. Bend 4 is the most common size in professional bandy. The bandy stick should not have similar colours to the ball, such as orange or pink; it should be no longer than , and no wider than .


A bandy field is by , a total of , or about the same size as a football pitch and considerably larger than an ice hockey rink. Along the sidelines a high border (vant, sarg, wand, wall) is placed to prevent the ball from leaving the ice. It should not be attached to the ice, to glide upon collisions, and should end away from the corners.

Centered at each shortline is a wide and high goal cage and in front of the cage is a half-circular penalty area with a radius. A penalty spot is located in front of the goal and there are two free-stroke spots at the penalty area line, each surrounded by a circle.

A centre spot with a circle of radius denotes the center of the field. A centre-line is drawn through the centre spot parallel with the shortlines.

At each of the corners, a radius quarter-circle is drawn, and a dotted line is painted parallel to the shortline and away from it without extending into the penalty area. The dotted line can be replaced with a long line starting at the edge of the penalty area and extending towards the sideline, from the shortline.

A standard adult bandy match consists of two periods of 45 minutes each, known as halves. Each half runs continuously, meaning the clock is not stopped when the ball is out of play; the referee can, however, make allowance for time lost through significant stoppages as described below. There is usually a 15-minute half-time break. The end of the match is known as full-time.

The referee is the official timekeeper for the match, and may make an allowance for time lost through substitutions, injured players requiring attention, or other stoppages. This added time is commonly referred to as "stoppage time" or "injury time", and must be reported to the match secretary and the two captains. The referee alone signals the end of the match.

If it is very cold or if it is snowing, the match can be broken into thirds of 30 minutes each. At the extremely cold 1999 World Championship some matches were played in four periods of 15 minutes each and with extra long breaks in between. In the World Championships the two halves can be 30 minutes each for the nations in the B division.

In league competitions games may end in a draw, but in some knockout competitions if a game is tied at the end of regulation time it may go into extra time, which consists of two further 15-minute periods. If the score is still tied after extra time, the game will be replayed. As an alternative, the extra two times 15-minutes may be played as "golden goal" which means the first team that scores during the extra-time wins the game. If both extra periods are played without a scored goal, a penalty shootout will settle the game. The teams shoot five penalties each and if this doesn't settle the game, the teams shoot one more penalty each until one of them misses and the other scores.


Under the rules, the two basic states of play during a game are "ball in play" and "ball out of play". From the beginning of each playing period with a stroke-off (a set strike from the centre-spot by one team) until the end of the playing period, the ball is in play at all times, except when either the ball leaves the field of play, or play is stopped by the referee. When the ball becomes out of play, play is restarted by one of six restart methods depending on how it went out of play:

If the time runs out while a team is preparing for a free-stroke or penalty, the strike should still be made but it must go into the goal by one shot to count as a goal. Similarly, a goal made via a corner stroke should be allowed, but it must be executed using only one shot in addition to the strike needed to put the ball in play.

Free-strokes can be awarded to a team if a player of the opposite team breaks any rule, for example, by hitting with the stick against the opponent's stick or skates. Free-strokes can also be awarded upon incorrect execution of corner-strikes, free-strikes, goal-throws, and so on. or the use of incorrect equipment, such as a broken stick.

Rather than stopping play, the referee may allow play to continue when its continuation will benefit the team against which an offence has been committed. This is known as "playing an advantage". The referee may "call back" play and penalise the original offence if the anticipated advantage does not ensue within a short period of time, typically taken to be four to five seconds. Even if an offence is not penalised because the referee plays an advantage, the offender may still be sanctioned (see below) for any associated misconduct at the next stoppage of play.

If a defender violently attacks an opponent within the penalty area, a penalty shot is awarded. Certain other offences, when carried out within the penalty area, result in a penalty shot provided there is a goal situation. These include a defender holding or hooking an attacker, or blocking a goal situation with a lifted skate, thrown stick or glove and so on. Also, the defenders (with the exception of the goal-keeper) are not allowed to kneel or lie on the ice. The final offences that might mandate a penalty shot are those of hitting or blocking an opponent's stick or touching the ball with the hands, arms, stick or head. If any of these actions is carried out in a non-goal situation, they shall be awarded with a free-stroke from one of the free-stroke spots at the penalty area line. A penalty shot should always be accompanied by a 5 or 10 minutes penalty (see below). If the penalty results in a goal, the penalty should be considered personal meaning that a substitute can be sent in for the penalised player. This does not apply in the event of a red card (see below).

A ten-minute penalty is indicated through the use of a blue card and can be caused by protesting or behaving incorrectly, attacking an opponent violently or stopping the ball incorrectly to get an advantage.

The third time a player receives a penalty, it will be a personal penalty, meaning he or she will miss the remainder of the match. A substitute can enter the field after five or ten minutes. A full game penalty can be received upon using abusive language or directly attacking an opponent and means that the player can neither play nor be substituted for the remainder of the game. A match penalty is indicated through the use of a red card.

The offside rule effectively limits the ability of attacking players to remain forward (i.e. closer to the opponent's goal-line) of the ball, the second-to-last defending player (which can include the goalkeeper), and the half-way line. This rule is in effect just like that of soccer.

The Federation of International Bandy (FIB) has had 33 members at most. Currently, 27 members are a part. Formed in 1955, the name was changed from "International Bandy Federation" in 2001 after the International Olympic Committee approved it as a so-called "recognized sport"; the abbreviation "IBF" was already used by another recognized sports federation. In 2004, FIB was fully accepted by IOC.

FIB is now a member of Association of IOC Recognised International Sports Federations.

The Bandy World Championship for men is arranged by the FIB and was first held in 1957. It was held every two years starting in 1961, and every year since 2003. Currently the record number of countries participating in the World Championships is twenty (2019). Since the number of countries playing bandy is not large, every country which can set up a team is welcome to take part in the World Championship. The quality of the teams varies; however, with only six nations, Sweden, the Soviet Union, Russia, Finland, Norway, and Kazakhstan, having won medals (allowing for the fact that Russia's team took over from the Soviet Union in 1993). Finland won the 2004 world championship in Västerås, Sweden, while all other championships have been won by Sweden, the Soviet Union and Russia.

In February 2004, Sweden won the first World Championship for women, hosted in Finland, without conceding a goal. In the 2014 women's World Championship Russia won, for the first time toppling the Swedes from the throne. In 2016 Sweden took the title back. In 2018 the tournament was played in a totally Asian country for the first time when Chengde in China hosted it.

The same goes for the men's tournament (the area north and west of the Ural River is located in Europe, thus Kazakhstan is a transcontinental country), when Harbin hosted the 2018 Division B tournament.

There are also Youth Bandy World Championships in different age groups for boys and young men and in one age group for girls. The oldest group is the under 23 championship, Bandy World Championship Y-23.

Bandy is recognized by the International Olympic Committee, and was played as a demonstration sport at the 1952 Winter Olympics in Oslo. However, it has yet to officially be played at the Olympics.

FIB president Boris Skrynnik lobbied for Bandy to be included in the 2014 Winter Olympics in Sochi, given Russia's prominence in the sport. Members of the Chinese Olympic Committee were present at the 2017 world championships to meet with Skrynnik about the possibility of considering the sport for the 2022 Winter Olympics in Beijing. However, in 2018 it was announced no new sports would be added for 2022.

Compared with the seven Winter Olympic sports, bandy's popularity across the globe is considered by the International Olympic Committee to have a, "gap between popularity and participation and global audiences", which is a roadblock into future Olympic inclusion.

At the 2011 Asian Winter Games, open to members of the Olympic Council of Asia, men's bandy was included for the first time. Three teams contested the inaugural competition, and Kazakhstan won the gold medal. President Nursultan Nazarbayev attended the final.

There was no bandy competition at the 2017 Asian Winter Games in Japan.

Bandy made its debut at the Winter Universiade during the 2019 Games. Originally a six team tournament for men and a four team tournament for women were planned to be held. However, later China withdrew from the men's tournament and was supposed be replaced by Belarus. Since that did not happen either, participating teams among women were Russia, Sweden, Norway and USA, while among men Russia, Sweden, Norway, Finland and Kazakhstan.

There is a chance for participation also in 2023. In fact International University Sports Federation expects it to happen.

The World Championships should not be confused with the annual World Cup in Ljusdal, Sweden, which has been played annually since the 1970s and is the biggest bandy tournament for elite level club teams. It is played indoors in Sandviken since 2009 because Ljusdal has no indoor arena. It is expected to return to Ljusdal once an indoor arena has been built. World Cup matches are played day and night, and the tournament is played in four days in late October. The teams participating are mostly, and some years exclusively, from Sweden and Russia, which has the two best leagues in the world.

Since 2007, there is also a Bandy World Cup Women for women's teams.

Rink bandy is a variety played on an ice hockey-size rink. It was in the programme of the 2012 European Company Sports Games.
Some FIB countries don't have a large ice surface and only play rink bandy at home; this includes most of the World Championships Group B participants.

The China Bandy Federation was set up in 2014 and China has since then participated in a number of world championship tournaments, with men's, women's and youth teams. China Bandy is mainly financed by private resources. The development of the sport in China is supported by the Harbin Sport University.

The first recorded games of bandy on ice took place in The Fens during the great frost of 1813–1814, although it is probable that the game had been played there in the previous century. Bury Fen Bandy Club from Bluntisham-cum-Earith, near St Ives, was the most successful team, remaining unbeaten until the winter of 1890–1891. Charles G Tebbutt of the Bury Fen bandy club was responsible for the first published rules of bandy in 1882, and also for introducing the game into the Netherlands and Sweden, as well as elsewhere in England where it became popular with cricket, rowing and hockey clubs. Tebbutt's home-made bandy stick can be seen in the Norris Museum in St Ives.

The first Ice Hockey Varsity Matches between Oxford University and Cambridge University were played to bandy rules, even if it was called hockey on ice at the time.

England won the European Bandy Championships in 1913, but that turned out to be the grand finale, and bandy is now virtually unknown in England. In March 2004, Norwegian ex-player Edgar Malman invited two big clubs to play a rink bandy exhibition game in Streatham, London. Russian
Champions and World Cup Winner Vodnik met Swedish Champions Edsbyns IF in a match that ended 10–10. In 2010 England became a Federation of International Bandy member. The federation is based in Cambridgeshire, the historical heartland.

The England Bandy Federation, now the Great Britain Bandy Federation, was set up on 2 January 2017 at a meeting held in the historic old skaters public house, the Lamb and Flag in Welney in Norfolk, England, replacing the Bandy Federation of England which was founded in 2010. President is Rev Lyn Gibb-de Swarte of Littleport and past resident of Streatham in south west London, where she was chair of the Streatham ice speed club, ice hockey club and of the association of ice clubs. Vice Presidents; Simon Seager and Les Mead. Chair is Andrew Hutchinson. Treasurer is Tammy Nichol Twallin . General Secretary, Fixtures and Minutes Secretary, Cathy Gibb-de Swarte. Participation Officer, Anders Gidrup. Recruitment UK is Oscar Gillingham Aukner. They are all busy promoting the sport for all and will be instituting rink bandy around the country. The president is the project director of the Littleport Ice Stadium Project and plans are already drawn for a 400 metres indoor speed skating oval and an inner ice pad 100 × 60 metres bandy pitch. In September 2017, the federation decided to widen its territory to all of the United Kingdom and changed its name to Great Britain Bandy Federation. Great Britain entered a national team in the 2019 World Championships Group B in January and undefeated up to the final, won the silver medal in their final match against Estonia.

Bandy was played in Estonia in the 1910s to 1930s and the country had a national championship for some years. The national team played friendlies against Finland in the 1920s and '30s. The sport was played sporadically during Soviet occupation 1944–1991. It has since then become more organised again, partly through exchange with Finnish clubs and enthusiasts. As of 2018, Estonia takes part in both the men's and the Women's Bandy World Championship.


Bandy was introduced to Finland from Russia in the 1890s. Finland has been playing bandy friendlies against Sweden and Estonia since its independence in 1917.

The first Finnish national championships were held in 1908 and was the first national Finnish championship held in any team sport. National champions have been named every year except for three years in the first half of the 20th Century when Finland was at war. The top national league is called Bandyliiga and is semi-professional. The best players often go fully professional by being recruited by clubs in Sweden or Russia.

Finland was an original member of the Federation of International Bandy and is the only country beside Russia/Soviet Union and Sweden to have won a Bandy World Championship, which it did in 2004.


Bandy was played in Germany in the early 20th century, including by Crown Prince Wilhelm, but the interest died out in favour of ice hockey. Leipziger Sportclub had the best team and was also last to give bandy up. The sport was reintroduced in the 2010s, with the German Bandy Association being founded in 2013.


Bandy has a long history in many parts of the country and it used to be one of the most popular sports in Soviet times. However, after independence it suffered a rapid decline in popularity and only remained in Oral (often called by the Russian name, Uralsk), where the country's only professional club Akzhaiyk is located. They are competing in the Russian second tier division, the Supreme League. Recently bandy has started to gain popularity again outside of Oral, most notably in Petropavl and Khromtau. Those were for example the three Kazakh cities which at the Youth-17 World Championship 2016 had players in the team. The capital Nur-Sultan has hosted national youth championships in rink bandy. as well as championships in traditional eleven-a-side bandy. The former capital Almaty has in recent years hosted both the Asian Winter Games (with bandy on the program) as well as the Bandy World Championship in which Kazakhstan finished 3rd. Plans are made to reinvigorate the bandy section of the club Dynamo Almaty, who won the Soviet Championships in 1977 and 1990 as well as the European Cup in 1978. The Asian Bandy Federation also has its headquarters in Almaty. Since a few years the state is supporting bandy. Medeu in Almaty is the only arena with artificial ice. A second arena in Almaty was built for the World Championship 2012, but it was taken down afterwards. Stadion Yunost in Oral was supposed to get artificial ice for the 2017–18 season. It got delayed but in 2018 it was officially ready for use.

The national team took a silver medal at the 2011 Asian Winter Games, which led to being chosen as the best Mongolian sport team of 2011. Mongolia was proud to win the bronze medal of the B division at the 2017 Bandy World Championship after which the then President of Mongolia, Tsakhiagiin Elbegdorj, held a reception for the team.


Bandy was introduced to the Netherlands in the 1890s by Pim Mulier and the sport became popular. However, in the 1920s, the interest turned to ice hockey, but in contrast to other countries in central and western Europe, the sport has been continuously played in the Netherlands and since the 1970s, the country has become a member of FIB and games have been more formalised again. The national team started to compete at the WCS in 1991. However, without a proper venue, only rink bandy is played within the country. The national governing body is the Bandy Bond Nederland.

Bandy was introduced to Norway in the 1910s. The Swedes contributed largely, and clubs sprang up around the capital of Oslo. Oslo, including neighbouring towns, is still today the region where bandy enjoys most popularity in Norway.

In 1912 the Norwegians played their first National Championship, which was played annually up to 1940. During WWII, illegal bandy was played in hidden places in forests, on ponds and lakes. In 1943, −44 and −45, illegal championships were held. In 1946 legal play resumed and still goes on. After WWII the number of teams rose, as well as attendance which regularly were in the thousands, but mild winters in the 1970s and 80s shrunk the league, and in 2003 only five clubs (teams) fought out the 1st division with low attendance numbers and little media coverage.

In recent years, the number of artificially frozen pitches have increased in Norway, and more sports clubs have reinvigorated their bandy sections with new men's and youth teams. Because of this, as well as an increase of Swedish players in Norway, the competitiveness of the game has risen, especially in the first division Eliteserien. The adult men's game in Norway today consists of Eliteserien with eight teams, as well as three lower divisions. Bandy in Norway has also started to spread geographically, but some clubs in apart locations in the 3rd division only have access to ice hockey rinks and therefore play rink bandy for home games. Compared to the past, attendance is still fairly low, but important Eliteserien matches can attract around 1000 spectators.


In Russia bandy is known as hockey with a ball or simply Russian hockey. A similar game became popular among the Russian nobility in the early 1700s, with the imperial court of Peter the Great playing a predecessor of modern bandy on Saint Petersburg's frozen Neva river. Russians played this game using ordinary footwear, with sticks made out of juniper wood, only later were skates introduced. It was only in the second half of the 19th century that bandy became popular among the masses throughout the Russian Empire. Traditionally the Russians used a longer skate blade than other nations, giving them the advantage of skating faster. However, they would find it more difficult to turn quickly. A bandy skate has a longer blade than a hockey skate, and the "Russian skate" is even longer.

When the Federation of International Bandy was formed in 1955, with the Soviet Union as one of its founding members, the Russians largely adopted the international rules of the game developed in England in the 19th century, with one notable exception. The other countries adopted the border.

Bandy is considered a national sport in Russia and is the only discipline to have official support of the Russian Orthodox Church.

The Russian Bandy Super League is played every year and the winner in the final becomes Russian champion. The Russian Cup has been played annually (except for just some years) since 1937.

After the victory in the 2016 World Championship, the fourth in a row, President Putin received four players of the national team, Head Coach and Vice-President of the Russian Bandy Federation Sergey Myaus, the Russian Bandy Federation as well as Federation of International Bandy President Boris Skrynnik in The Kremlin. He talked, among other things, about the need to give more support to Russian bandy. It was the first time a head of state had accepted a meeting to talk about Russian bandy. Attending the meeting were also Minister of Sport, Tourism and Youth policy Vitaly Mutko and presidential adviser Igor Levitin. The month after, Igor Levitin held a follow-up meeting.

Bandy was introduced to Sweden in 1895. The Swedish royal family, noblemen and diplomats were the first players. Swedish championships for men have been played annually since 1907. In the 1920s students played the game and it became a largely middle class sport. After Slottsbrons IF won the Swedish championship in 1934 it became popular amongst workers in the smaller industrial towns and villages. Bandy remains the main sport in many of these places.

Bandy in Sweden is famous for its "culture" – both playing bandy and being a spectator requires great fortitude and dedication. A "" is the classic accessory for spectating – it is typically made of brown leather, well worn and contains a warm drink in a thermos and/or a bottle of liquor.
Bandy is most often played at outdoor arenas during winter time, so the need for spectators to carry flasks or thermoses of 'warming' liquid like glögg is a natural effect.

A notable tradition is "annandagsbandy", bandy games played on Saint Stephen's Day, which for many Swedes is an important Christmas season tradition and always draws bigger crowds than usual. Games traditionally begin at 1:15 pm.

The final match for the Swedish Championship is played every year on the third Saturday of March. From 1991 to 2012, it was played at Studenternas Idrottsplats in Uppsala, often drawing crowds in excess of 20,000. The reason the play-off match was set in Uppsala is because of IFK Uppsala's success in the beginning of the 20th century. IFK Uppsala won 11 titles in the Swedish Championships between 1907 and 1920, which made them the most successful bandy club in the entire country. Now, however, the record is held by Västerås SK. A contributing factor was the poor quality of the ice at Söderstadion, where the finals were held from 1967 to 1989.

In 2013 and 2014 the final was played indoors in Friends Arena, the national stadium for football in Solna, Stockholm, with a retractable roof and a capacity of 50 000. The first final at Friends Arena in 2013 drew a record crowd of 38,474 when Hammarby IF Bandy, after ending up in second place in six finals during the 2000s, won their second title. Due to declining attendance since, for 2015 through 2017 Tele2 Arena in southern Stockholm was chosen as a new venue. However, the new indoor venue failed to attract much more than half of the total capacity. In May 2017 it was announced that the finals will again be held at Studenternas IP in Uppsala from 2018 to 2021.

In the late 19th and early 20th century, Switzerland had become a popular place for winter vacations and people went there from all over Europe. Winter sports like skiing, sledding and bandy was played in Geneva and other towns. Students from Oxford and Cambridge went to Switzerland to play each other – the predecessor of the recurring Ice Hockey Varsity Match was a bandy match played in St. Moritz in 1885. This popularity for Swiss venues of winter sport may have been a reason, the European Championship was held there in 1913.

Bandy has mainly been played as a recreational sport in Switzerland in the last decade, but a Swiss national team took part in the 2018 Women's Bandy World Championship.

Bandy was played in Ukraine when it was part of the Soviet Union. After independence in 1991, it took some years before organised bandy formed again, but Ukrainian champions have been named annually since 2012.


Bandy has been played in the United States since around 1980. The sport is centered in Minnesota, with very few teams based elsewhere. The United States national bandy team has participated in the Bandy World Championships since 1985 and is also regularly playing friendly matches against Canada.

United States bandy championships have been played annually since the early 1980s, but the sport is not widely covered by American sports media.






</doc>
<doc id="4445" url="https://en.wikipedia.org/wiki?curid=4445" title="Bob Frankston">
Bob Frankston

Robert M. Frankston (born June 14, 1949) is an American software engineer and businessman who co-created, with Dan Bricklin, the VisiCalc spreadsheet program. Frankston is also the co-founder of Software Arts.

Frankston was born and raised in Brooklyn, New York. He graduated from Stuyvesant High School in New York City in 1966. He earned a S.B degree in computer science and mathematics from the Massachusetts Institute of Technology, followed by a Master of Engineering degree computer science, also from MIT.

Following his work with Dan Bricklin, Frankston later worked at Lotus Software and Microsoft.

Frankston became an outspoken advocate for reducing the role of telecommunications companies in the evolution of the Internet, particularly with respect to broadband and mobile communications. He coined the term "Regulatorium" to describe what he considers collusion between telecommunication companies and their regulators that prevents change.




</doc>
<doc id="4446" url="https://en.wikipedia.org/wiki?curid=4446" title="Booker Prize">
Booker Prize

The Booker Prize for Fiction, formerly known as the Booker–McConnell Prize (1969–2001) and the Man Booker Prize (2002–2019), is a literary prize awarded each year for the best original novel written in the English language and published in the United Kingdom. The winner of the Booker Prize is generally assured international renown and success; therefore, the prize is of great significance for the book trade. From its inception, only novels written by Commonwealth, Irish, and South African (and later Zimbabwean) citizens were eligible to receive the prize; in 2014 it was widened to any English-language novel—a change that proved controversial.

A high-profile literary award in British culture, the Booker Prize is greeted with anticipation and fanfare. It is also a mark of distinction for authors to be selected for inclusion in the shortlist or even to be nominated for the "longlist".

The prize was originally established as the Booker–McConnell Prize, after the company Booker, McConnell Ltd began sponsoring the event in 1969; it became commonly known as the "Booker Prize" or simply "the Booker".

When administration of the prize was transferred to the Booker Prize Foundation in 2002, the title sponsor became the investment company Man Group, which opted to retain "Booker" as part of the official title of the prize. The foundation is an independent registered charity funded by the entire profits of Booker Prize Trading Ltd, of which it is the sole shareholder. The prize money awarded with the Booker Prize was originally £21,000, and was subsequently raised to £50,000 in 2002 under the sponsorship of the Man Group, making it one of the world's richest literary prizes.

In 1970, Bernice Rubens became the first woman to win the Booker Prize, for "The Elected Member". The rules of the Booker changed in 1971; previously, it had been awarded retrospectively to books published prior to the year in which the award was given. In 1971 the year of eligibility was changed to the same as the year of the award; in effect, this meant that books published in 1970 were not considered for the Booker in either year. The Booker Prize Foundation announced in January 2010 the creation of a special award called the "Lost Man Booker Prize", with the winner chosen from a longlist of 22 novels published in 1970.

Alice Munro's "The Beggar Maid" was shortlisted in 1980, and remains the only short story collection to be shortlisted.

John Sutherland, who was a judge for the 1999 prize, has said: 

In 1972, winning writer John Berger, known for his Marxist worldview, protested during his acceptance speech against Booker McConnell. He blamed Booker's 130 years of sugar production in the Caribbean for the region's modern poverty. Berger donated half of his £5,000 prize to the British Black Panther movement, because it had a socialist and revolutionary perspective in agreement with his own.

In 1980, Anthony Burgess, writer of "Earthly Powers", refused to attend the ceremony unless it was confirmed to him in advance whether he had won. His was one of two books considered likely to win, the other being "Rites of Passage" by William Golding. The judges decided only 30 minutes before the ceremony, giving the prize to Golding. Both novels had been seen as favourites to win leading up to the prize, and the dramatic "literary battle" between two senior writers made front-page news.

In 1981, nominee John Banville wrote a letter to "The Guardian" requesting that the prize be given to him so that he could use the money to buy every copy of the longlisted books in Ireland and donate them to libraries, "thus ensuring that the books not only are bought but also read — surely a unique occurrence."

Judging for the 1983 award produced a draw between J. M. Coetzee's "Life & Times of Michael K" and Salman Rushdie's "Shame", leaving chair of judges Fay Weldon to choose between the two. According to Stephen Moss in "The Guardian", "Her arm was bent and she chose Rushdie," only to change her mind as the result was being phoned through.

In 1992, the jury split the prize between Michael Ondaatje's "The English Patient" and Barry Unsworth's "Sacred Hunger". This prompted the foundation to draw up a rule that made it mandatory for the appointed jury to make the award to just a single author/book.

In 1993, two of the judges threatened to walk out when "Trainspotting" appeared on the longlist; Irvine Welsh's novel was pulled from the shortlist to satisfy them. The novel would later receive critical acclaim, and is now considered Welsh's masterpiece.

The choice of James Kelman's book "How Late It Was, How Late" as 1994 Booker Prize winner proved to be one of the most controversial in the award's history. Rabbi Julia Neuberger, one of the judges, declared it "a disgrace" and left the event, later deeming the book to be "crap"; WHSmith's marketing manager called the award "an embarrassment to the whole book trade"; Waterstone's in Glasgow sold a mere 13 copies of Kelman's book the following week. In 1994, "The Guardian"s literary editor Richard Gott, citing the lack of objective criteria and the exclusion of American authors, described the prize as "a significant and dangerous iceberg in the sea of British culture that serves as a symbol of its current malaise."

In 1997, the decision to award Arundhati Roy's "The God of Small Things" proved controversial. Carmen Callil, chair of the previous year's Booker judges, called it an "execrable" book and said on television that it should not even have been on the shortlist. Booker Prize chairman Martyn Goff said Roy won because nobody objected, following the rejection by the judges of Bernard MacLaverty's shortlisted book due to their dismissal of him as "a wonderful short-story writer and that "Grace Notes" was three short stories strung together."

Before 2001, each year's longlist of nominees was not publicly revealed. In 2001, A. L. Kennedy, who was a judge in 1996, called the prize "a pile of crooked nonsense" with the winner determined by "who knows who, who's sleeping with who, who's selling drugs to who, who's married to who, whose turn it is".

The Booker Prize created a permanent home for the archives from 1968 to present at Oxford Brookes University Library. The Archive, which encompasses the administrative history of the Prize from 1968 to date, collects together a diverse range of material, including correspondence, publicity material, copies of both the Longlists and the Shortlists, minutes of meetings, photographs and material relating to the awards dinner (letters of invitation, guest lists, seating plans). Embargoes of ten or twenty years apply to certain categories of material; examples include all material relating to the judging process and the Longlist prior to 2002.

Between 2005 and 2008, the Booker Prize alternated between writers from Ireland and India. "Outsider" John Banville began this trend in 2005 when his novel "The Sea" was selected as a surprise winner: Boyd Tonkin, literary editor of "The Independent", famously condemned it as "possibly the most perverse decision in the history of the award" and rival novelist Tibor Fischer poured scorn on Banville's victory. Kiran Desai of India won in 2006. Anne Enright's 2007 victory came about due to a jury badly split over Ian McEwan's novel "On Chesil Beach". The following year it was India's turn again, with Aravind Adiga narrowly defeating Enright's fellow Irishman Sebastian Barry.

Historically, the winner of the Booker Prize had been required to be a citizen of the Commonwealth of Nations, the Republic of Ireland, or Zimbabwe. It was announced on 18 September 2013 that future Booker Prize awards would consider authors from anywhere in the world, so long as their work was in English and published in the UK. This change proved controversial in literary circles. Former winner A. S. Byatt and former judge John Mullan said the prize risked diluting its identity, whereas former judge A. L. Kennedy welcomed the change. Following this expansion, the first winner not from the Commonwealth, Ireland, or Zimbabwe was American Paul Beatty in 2016. Another American, George Saunders, won the following year. In 2018, publishers sought to reverse the change, arguing that the inclusion of American writers would lead to homogenisation, reducing diversity and opportunities everywhere, including in America, to learn about "great books that haven't already been widely heralded."

Man Group announced in early 2019 that the year's prize would be the last of eighteen under their sponsorship. A new sponsor, Crankstart – a charitable foundation run by Sir Michael Moritz and his wife, Harriet Heyman – then announced it would sponsor the award for five years, with the option to renew for another five years. The award title was changed to simply "The Booker Prize".

In 2019, despite having been unequivocally warned against doing so, the foundation's jury – under the chair Peter Florence – split the prize, awarding it to two authors, in breach of a rule established in 1993. Florence justified the decision, saying: "We came down to a discussion with the director of the Booker Prize about the rules. And we were told quite firmly that the rules state that you can only have one winner...and as we have managed the jury all the way through on the principle of consensus, our consensus was that it was our decision to flout the rules and divide this year’s prize to celebrate two winners." The two were British writer Bernardine Evaristo for her novel "Girl, Woman, Other" and Canadian writer Margaret Atwood for "The Testaments". Evaristo’s win marked the first time the Booker had been awarded to a black woman while at 79, Atwood’s win made her the oldest.

The selection process for the winner of the prize commences with the formation of an advisory committee, which includes a writer, two publishers, a literary agent, a bookseller, a librarian, and a chairperson appointed by the Booker Prize Foundation. The advisory committee then selects the judging panel, the membership of which changes each year, although on rare occasions a judge may be selected a second time. Judges are selected from amongst leading literary critics, writers, academics and leading public figures.

The Booker judging process and the very concept of a "best book" being chosen by a small number of literary insiders is controversial for many. "The Guardian" introduced the "Not the Booker Prize" voted for by readers partly as a reaction to this.
Author Amit Chaudhuri wrote: "The idea that a 'book of the year' can be assessed annually by a bunch of people – judges who have to read almost a book a day – is absurd, as is the idea that this is any way of honouring a writer."

The winner is usually announced at a ceremony in London's Guildhall, usually in early October.
The scholar Luke Strongman noted that the rules for the Booker prize as laid out in 1969 with recipients limited to novelists writing in English from Great Britain or nations that had once belonged to the British Empire strongly suggested the purpose of the prize was to deepen ties between the nations that had all been a part of the empire. The very book to win the Booker, "Something to Answer For" in 1969, concerned the misadventures of an Englishman in Egypt in the 1950s at the time when British influence in Egypt was ending. Strongman wrote that most of the books that have won the Booker Prize have in some way been concerned with the legacy of the British Empire, with many of the prize winners having engaged in imperial nostalgia. However, over time many of the books that won the prize have reflected the changed balance of power from the emergence of new identities in the former colonies of the empire, and with it "culture after the empire". The attempts of successive British officials to mold "the natives" into their image did not fully succeed, but did profoundly and permanently change the cultures of the colonised, a theme which some non-white winners of the Booker prize have engaged with in various ways.

In 1993, to mark the prize's 25th anniversary, a ""Booker of Bookers" Prize" was given. Three previous judges of the award, Malcolm Bradbury, David Holloway and W. L. Webb, met and chose Salman Rushdie's "Midnight's Children", the 1981 winner, as "the best novel out of all the winners".

In 2006, the Man Booker Prize set up a "Best of Beryl" prize, for the author Beryl Bainbridge, who had been nominated five times and yet failed to win once. The prize is said to count as a Booker Prize. The nominees were "An Awfully Big Adventure", "Every Man for Himself", "The Bottle Factory Outing", "The Dressmaker" and "Master Georgie", which won.

Similarly, The Best of the Booker was awarded in 2008 to celebrate the prize's 40th anniversary. A shortlist of six winners was chosen and the decision was left to a public vote; the winner was again "Midnight's Children".

In 2018, to celebrate the 50th anniversary, the Golden Man Booker was awarded. One book from each decade was selected by a panel of judges: Naipaul's "In a Free State" (the 1971 winner), Lively's "Moon Tiger" (1987), Ondaatje's "The English Patient" (1992), Mantel's "Wolf Hall" and Saunders' "Lincoln in the Bardo". The winner, by popular vote, was "The English Patient".

Since 2014, each publisher's imprint may submit a number of titles based on their longlisting history (previously they could submit two). Non-longlisted publishers can submit one title, publishers with one or two longlisted books in the previous five years can submit two, publishers with three or four longlisted books are allowed three submissions, and publishers with five or more longlisted books can have four submissions.

In addition, previous winners of the prize are automatically considered if they enter new titles. Books may also be called in: publishers can make written representations to the judges to consider titles in addition to those already entered. In the 21st century the average number of books considered by the judges has been approximately 130.

A separate prize for which any living writer in the world may qualify, the Man Booker International Prize was inaugurated in 2005. Until 2015, it was given every two years to a living author of any nationality for a body of work published in English or generally available in English translation. In 2016, the award was significantly reconfigured, and is now given annually to a single book in English translation, with a £50,000 prize for the winning title, shared equally between author and translator.

A Russian version of the Booker Prize was created in 1992 called the Booker-Open Russia Literary Prize, also known as the Russian Booker Prize. In 2007, Man Group plc established the Man Asian Literary Prize, an annual literary award given to the best novel by an Asian writer, either written in English or translated into English, and published in the previous calendar year.

As part of "The Times"' Literature Festival in Cheltenham, a Booker event is held on the last Saturday of the festival. Four guest speakers/judges debate a shortlist of four books from a given year from before the introduction of the Booker prize, and a winner is chosen. Unlike the real Man Booker (1969 through 2014), writers from outside the Commonwealth are also considered. In 2008, the winner for 1948 was Alan Paton's "Cry, the Beloved Country", beating Norman Mailer's "The Naked and the Dead", Graham Greene's "The Heart of the Matter" and Evelyn Waugh's "The Loved One". In 2015, the winner for 1915 was Ford Madox Ford's "The Good Soldier", beating "The Thirty-Nine Steps" (John Buchan), "Of Human Bondage" (W. Somerset Maugham), "Psmith, Journalist" (P. G. Wodehouse) and "The Voyage Out" (Virginia Woolf).





</doc>
<doc id="4447" url="https://en.wikipedia.org/wiki?curid=4447" title="Book of Joel">
Book of Joel

The Book of Joel is part of the Hebrew Bible and Christian Old Testament, one of twelve prophetic books known as the Twelve Minor Prophets. (The term indicates the short length of the text in relation to longer prophetic texts known as the Major Prophets.)

After a superscription ascribing the prophecy to Joel (son of Pethuel), the book may be broken down into the following sections:

The Book of Joel's division into chapters and verses differs widely between editions of the Bible; some editions have three chapters, others four. Translations with four chapters include:
In the 1611 King James Bible, the Book of Joel is formed by three chapters: the second one has 32 verses, and it is equivalent to the union of the chapter 2 (with 26 verses) and chapter 3 (with 5 verses) of other editions of the Bible.

The differences of the division is as follows:
As there are no explicit references in the book to datable persons or events, scholars have assigned a wide range of dates to the book. The main positions are:

Evidence produced for these positions includes allusions in the book to the wider world, similarities with other prophets, and linguistic details. Some commentators, such as John Calvin, attach no great importance to the precise dating.

Joel 1 and 2 are preserved in the Dead Sea Scrolls, in fragmentary manuscripts 4Q78, 4Q82, and the Scroll Wadi Muraba’at.

The Masoretic text places Joel between Hosea and Amos (the order inherited by the Tanakh and Old Testament), while the Septuagint order is Hosea–Amos–Micah–Joel–Obadiah–Jonah. The Hebrew text of Joel seems to have suffered little from scribal transmission, but is at a few points supplemented by the Septuagint, Syriac, and Vulgate versions, or by conjectural emendation. While the book purports to describe a plague of locusts, some ancient Jewish opinion saw the locusts as allegorical interpretations of Israel's enemies. This allegorical interpretation was applied to the church by many church fathers. Calvin took a literal interpretation of chapter 1, but allegorical view of chapter 2, a position echoed by some modern interpreters. Most modern interpreters, however, see Joel speaking of a literal locust plague given a prophetic/ apocalyptic interpretation.

The traditional ascription of the whole book to the prophet Joel was challenged in the late nineteenth and early twentieth centuries by a theory of a three-stage process of composition: 1:1–2:27 were from the hand of Joel, and dealt with a contemporary issue; 2:28–3:21/3:1–4:21 were ascribed to a continuator with an apocalyptic outlook. Mentions in the first half of the book to the day of the Lord were also ascribed to this continuator. 3:4–8/4:4–8 could be seen as even later. Details of exact ascriptions differed between scholars.

This splitting of the book's composition began to be challenged in the mid-twentieth century, with scholars defending the unity of the book, the plausibility of the prophet combining a contemporary and apocalyptic outlook, and later additions by the prophet. The authenticity of 3:4–8 has presented more challenges, although a number of scholars still defend it.

There are many parallels of language between Joel and other Old Testament prophets. They may represent Joel's literary use of other prophets, or vice versa.

In the New Testament, his prophecy of the outpouring of God's Holy Spirit upon all people was notably quoted by Saint Peter in his Pentecost sermon.

Joel 3:10 / 4:10 is a rare reversed reference to swords to plowshares.

The table below represents some of the more explicit quotes and allusions between specific passages in Joel and passages from the Old and New Testaments.

Plange quasi virgo ("Lament like a virgin"), the third responsory for Holy Saturday, is loosely based on verses from the Book of Joel: the title comes from .


See also works on the Minor Prophets as a whole.



</doc>
<doc id="4449" url="https://en.wikipedia.org/wiki?curid=4449" title="Book of Hosea">
Book of Hosea

The Book of Hosea (, "Sefer Hōšēaʿ") is one of the books of the Hebrew Bible. According to the traditional order of most Hebrew Bibles, it is the first of the twelve Minor Prophets.

Set around the fall of the Northern Kingdom of Israel, the Book of Hosea denounces the worship of gods other than Yahweh (the God of Israel), metaphorically comparing Israel's abandonment of Yahweh to a woman being unfaithful to her husband. According to the book's narrative, the relationship between Hosea and his unfaithful wife Gomer is comparable to the relationship between Yahweh and his unfaithful people Israel. The eventual reconciliation of Hosea and Gomer is treated as a hopeful metaphor for the eventual reconciliation between Yahweh and Israel.

Dated to c. 760–720 BC, it is one of the oldest books of the Hebrew Bible, predating most of the Torah (Pentateuch). Hosea is the source of the phrase "reap the whirlwind", which has passed into common usage in English and other languages.

Hosea prophesied during a dark and melancholic era of Israel's history, the period of the Northern Kingdom's decline and fall in the 8th century BC. According to the book, the apostasy of the people was rampant, having turned away from God in order to serve both the calves of Jeroboam and Baal, a Canaanite god.

The Book of Hosea says that, during Hosea's lifetime, the kings of the Northern Kingdom, their aristocratic supporters, and the priests had led the people away from the Law of God, as given in the Pentateuch. It says that they forsook the worship of God; they worshiped other gods, especially Baal, the Canaanite storm god, and Asherah, a Canaanite fertility goddess. Other sins followed, says the Book, including homicide, perjury, theft, and sexual sin. Hosea declares that unless they repent of these sins, God will allow their nation to be destroyed, and the people will be taken into captivity by Assyria, the greatest nation of the time.

The prophecy of Hosea centers around God's unending love towards a sinful Israel. In this text, God's agony is expressed over His betrayal by Israel. Stephen Cook asserts that the prophetic efforts of this book can be summed up in this passage "I have been the Lord your God ever since the land of Egypt; you know no God but me, and besides me there is no savior" (). Hosea's job was to speak these words during a time when they had been essentially forgotten.

The Book of Hosea contains a number of YHWH prophecies and messages for both Judah and Northern Israel (Samaria). These are delivered by the prophet Hosea.

A brief outline of the concepts presented in the Book of Hosea exist below:
No further breakdown of ideas is clear in 4–14:9/14:10.
Following this, the prophecy is made that someday this will all be changed, that God will indeed have pity on Israel.

Chapter two describes a divorce. This divorce seems to be the end of the covenant between God and the Northern Kingdom. However, it is probable that this was again a symbolic act, in which Hosea divorced Gomer for infidelity, and used the occasion to preach the message of God's rejection of the Northern Kingdom. He ends this prophecy with the declaration that God will one day renew the covenant, and will take Israel back in love.

In Chapter three, at God's command, Hosea seeks out Gomer once more. Either she has sold herself into slavery for debt, or she is with a lover who demands money in order to give her up, because Hosea has to buy her back. He takes her home, but refrains from sexual intimacy with her for many days, to symbolize the fact that Israel will be without a king for many years, but that God will take Israel back, even at a cost to Himself.

Chapters 4–14 spell out the allegory at length. Chapters 1–3 speaks of Hosea's family, and the issues with Gomer. Chapters 4–10 contain a series of oracles, or prophetic sermons, showing exactly why God is rejecting the Northern Kingdom (what the grounds are for the divorce). Chapter 11 is God's lament over the necessity of giving up the Northern Kingdom, which is a large part of the people of Israel, whom God loves. God promises not to give them up entirely. Then, in Chapter 12, the prophet pleads for Israel's repentance. Chapter 13 foretells the destruction of the kingdom at the hands of Assyria, because there has been no repentance. In Chapter 14, the prophet urges Israel to seek forgiveness, and promises its restoration, while urging the utmost fidelity to God.

Matthew 2:13 cites Hosea's prophecy in that God would call His Son out of Egypt as foretelling the flight into Egypt and return to Israel of Joseph, Mary, and the infant Jesus.

In Luke 23:30, Jesus referenced Hosea 10:8 when he said "Then they will begin to say to the mountains 'Cover us" and to the hills, 'Fall on us.' (NRSV) The quote is also echoed in Revelation 6:16.

The capital of the Northern Kingdom fell in 722 BC. All the members of the upper classes and many of the ordinary people were taken captive and carried off to live as prisoners of war.

First, Hosea was directed by God to marry a promiscuous woman of ill-repute, and he did so. Marriage here is symbolic of the covenantal relationship between God and Israel. However, Israel has been unfaithful to God by following other gods and breaking the commandments which are the terms of the covenant, hence Israel is symbolized by a harlot who violates the obligations of marriage to her husband.

Second, Hosea and his wife, Gomer, have a son. God commands that the son be named Jezreel. This name refers to a valley in which much blood had been shed in Israel's history, especially by the kings of the Northern Kingdom. (See I Kings 21 and II Kings 9:21–35). The naming of this son was to stand as a prophecy against the reigning house of the Northern Kingdom, that they would pay for that bloodshed. Jezreel's name means God Sows.

Third, the couple have a daughter. God commands that she be named Lo-ruhamah; Unloved, or, Pity or Pitied On to show Israel that, although God will still have pity on the Southern Kingdom, God will no longer have pity on the Northern Kingdom; its destruction is imminent. In the NIV translation, the omitting of the word 'him' leads to speculation as to whether Lo-Ruhamah was the daughter of Hosea or one of Gomer's lovers. James Mays, however, says that the failure to mention Hosea's paternity is "hardly an implication" of Gomer's adultery.

Fourth, a son is born to Gomer. It is questionable whether this child was Hosea's, for God commands that his name be Lo-ammi, meaning 'not my people.' The child bore this name of shame to show that the Northern Kingdom would also be shamed, for its people would no longer be known as God's People. In other words, the Northern Kingdom had been rejected by God.

In Hosea 2, the woman in the marriage metaphor could be Hosea's wife Gomer, or could be referring to the nation of Israel, invoking the metaphor of Israel as God's bride. The woman is not portrayed in a positive light. This is reflected throughout the beginning of Hosea 2. “I will strip her naked and expose her as in the day she was born” (Hosea 2:3). “Upon her children I will have no pity, because they are children of whoredom” (Hosea 2:4). “For she said, I will go after my lovers...” (Hosea 2:5).

Biblical scholar Ehud Ben Zvi reminds readers of the socio-historical context in which Hosea was composed. In his article Observations on the marital metaphor of YHWH and Israel in its ancient Israelite context: general considerations and particular images in Hosea 1.2, Ben Zvi describes the role of the Gomer in the marriage metaphor as one of the "central attributes of the ideological image of a human marriage that was shared by the male authorship and the primary and intended male readership as building blocks for their imagining of the relationship."

Tristanne J. Connolly makes a similar observation, stating that the husband-wife motif reflects marriage as it was understood at the time. Connolly also suggests that in context the marriage metaphor was necessary in that it truly exemplified the unequal interaction between Yahweh and the people Israel. Biblical scholar Michael D. Coogan describes the importance of understanding the covenant in relation to interpreting Hosea. According to Coogan, Hosea falls under a unique genre called “covenant lawsuit” where God accuses Israel of breaking their previously made agreement. God's disappointment towards Israel is therefore expressed through the broken marriage covenant made between husband and wife.

Brad E. Kelle has referred to 'many scholars' finding references to cultic sexual practices in the worship of Baal, in Hosea 2, to be evidence of an historical situation in which Israelites were either giving up Yahweh worship for Baal, or blending the two. Hosea's references to sexual acts being metaphors for Israelite 'apostasy'.

Hosea 13:1–3 describes how the Israelites are abandoning Yahweh for the worship of Baal, and accuses them of making or using molten images for 'idol' worship. Chief among these was the image of the bull at the northern shrine of Bethel, which by the time of Hosea was being worshipped as an image of Baal.

Hosea is a prophet whom God uses to portray a message of repentance to God's people. Through Hosea's marriage to Gomer, God, also known as Yahweh, shows his great love for his people, comparing himself to a husband whose wife has committed adultery. It a metaphor of the covenant between God and Israel, and he influenced latter prophets such as Jeremiah. He is among the first writing prophets, and the last chapter of Hosea has a format similar to wisdom literature.

Like Amos, Hosea elevated the religion of Israel to the altitude of ethical monotheism, being the first to emphasize the moral side of God's nature. Israel's faithlessness, which resisted all warnings, compelled Him to punish the people because of His own holiness. Hosea considers infidelity as the chief sin, of which Israel, the adulterous wife, has been guilty against her loving husband, God. Against this he sets the unquenchable love of God, who, in spite of this infidelity, does not cast Israel away forever, but will take His people unto Himself again after the judgment.




</doc>
<doc id="4450" url="https://en.wikipedia.org/wiki?curid=4450" title="Book of Obadiah">
Book of Obadiah

The Book of Obadiah is an oracle concerning the divine judgment of Edom and the restoration of Israel. The text consists of a single chapter, divided into 21 verses, making it the shortest book in the Hebrew Bible. In Judaism and Christianity, its authorship is attributed to Obadiah, a prophet who lived in the Assyrian Period.

In Judaism, Obadiah is one of the Twelve Minor Prophets in the final section of Nevi'im, the second main division of the Tanakh. In Christianity, the Book of Obadiah is classified as a minor prophet of the Old Testament, due to its short length.

The Book of Obadiah is based on a prophetic vision concerning the fall of Edom, a mountain dwelling nation whose founding father was Esau. Obadiah describes an encounter with Yahweh, who addresses Edom's arrogance and charges them for their "violence against your brother Jacob".

Throughout most of the history of Judah, Edom was controlled absolutely from Jerusalem as a vassal state. Obadiah said that the high elevation of their dwelling place in the mountains of Seir had gone to their head, and they had puffed themselves up in pride. "'Though you soar like the eagle and make your nest among the stars, from there I will bring you down,' declares the Lord" (, NIV).

In Siege of Jerusalem (597 BC), Nebuchadnezzar II sacked Jerusalem, carted away the King of Judah, and installed a puppet ruler. The Edomites helped the Babylonians loot the city. Obadiah, writing this prophecy around 590 BCE, suggests the Edomites should have remembered that blood was thicker than water. "On the day you stood aloof while strangers carried off his wealth and foreigners entered his gates and cast lots for Jerusalem, you were like one of them... You should not march through the gates of my people in the day of their disaster, nor gloat over them in their calamity in the day of their disaster, nor seize their wealth in the day of their disaster." ( NIV)

Obadiah said in judgement Yahweh would wipe out the house of Esau forever, and not even a remnant would remain. The Edomites' land would be possessed by Egypt and they would cease to exist as a people. The Day of the Lord was at hand for all nations, and someday the children of Israel would return from their exile and possess the land of Edom.

The date of composition is disputed and is difficult to determine due to the lack of personal information about Obadiah, his family, and his historical milieu. The date of composition must therefore be determined based on the prophecy itself. Edom is to be destroyed due to its lack of defense for its brother nation, Israel, when it was under attack. There are two major historical contexts within which the Edomites could have committed such an act. These are during 853 – 841 BC when Jerusalem was invaded by Philistines and Arabs during the reign of Jehoram of Judah (recorded in 2 Kings and 2 Chronicles in the Christian Old Testament) and 607 – 586 BC when Jerusalem was attacked by Nebuchadnezzar II of Babylon, which led to the Babylonian exile of Israel (recorded in Psalm 137). The earlier period would place Obadiah as a contemporary of the prophet Elijah.

The later date would place Obadiah as a contemporary of the prophet Jeremiah. A sixth-century date for Obadiah is a "near consensus" position among scholars. contains parallels to the Book of Jeremiah . The passage in the Book of Jeremiah dates from the fourth year of the reign of Jehoiakim (604 BC), and therefore seems to refer to the destruction of Jerusalem by Nebuchadnezzar II (586 BC). It is more likely that Obadiah and the Book of Jeremiah together were drawing on a common source presently unknown to us rather than Jeremiah drawing on previous writings of Obadiah as his source. There is also much material found in which Jeremiah does not quote, and which, had he had it laid out before him, would have suited his purpose admirably.

According to Obadiah 18, after judgment, "There will be no survivors from the house of Esau" (NIV). So, according to Obadiah there will not remain even a remnant after Edom’s judgment. This is in contrast to Amos 9:12, where Amos refers to such a remnant; however, it is stated that their possession will be given to Israel.

The identity of the land of "Sepharad", mentioned only in is currently unknown. It is also unknown whether or not Sepharad is a city, district or territory. Persian inscriptions refer to two places called "Saparda", one area in Media and another in Asia Minor, arguably Sardis.

The exact expression "the Day of the Lord", from , has been used by other authors throughout the Old and New Testaments, as follows:



For other parallels, compare with .

In relating to theme of Obadiah, it is important to underscore the punishment theme this book outlines against Edom. W.J. Deane and J.R. Thomson write this conclusion, "The Book of Obadiah is occupied with one subject – the punishment of Edom for its cruel and unbrotherly conduct towards Judah..." One can link this idea of punishment to one of the major prophets Ezekiel who "...interprets the exile to Babylon and the destruction of Jerusalem as deserved punishments for the sins of those who themselves committed them." Verses in Obadiah explain to the reader the reason for the punishment theme, "Confidence in one’s power, intelligence, allies, or the topographical features of one’s territory is often mentioned as an attribute of those who foolishly confront the Lord and are consequently punished." Although destruction is vital to understanding Obadiah, it is of note to understand the destruction being a consequence of action.




</doc>
<doc id="4451" url="https://en.wikipedia.org/wiki?curid=4451" title="Book of Jonah">
Book of Jonah

The Book of Jonah is a book of the Nevi'im ("Prophets") in the Hebrew Bible. It tells of a Hebrew prophet named Jonah son of Amittai who is sent by God to prophesy the destruction of Nineveh but tries to escape the divine mission. Set in the reign of Jeroboam II (786–746 BC), it was probably written in the post-exilic period, some time between the late 5th to early 4th century BC. The story has a long interpretive history and has become well known through popular children's stories. In Judaism, it is the Haftarah portion read during the afternoon of Yom Kippur to instill reflection on God's willingness to forgive those who repent; it remains a popular story among Christians. It is also retold in the Quran.

Unlike the other Prophets, the book of Jonah is almost entirely narrative, with the exception of the poem in chapter 2. The actual prophetic word against Nineveh is given only in passing through the narrative. As with any good narrative, the story of Jonah has a setting, characters, a plot, and themes. It also relies heavily on such literary devices as irony.


Jonah is the central character in the Book of Jonah, in which God commands him to go to the city of Nineveh to prophesy against it "for their great wickedness is come up before me," but Jonah instead attempts to flee from "the presence of the Lord" by going to Jaffa and sailing to Tarshish. A huge storm arises and the sailors, realizing that it is no ordinary storm, cast lots and discover that Jonah is to blame. Jonah admits this and states that if he is thrown overboard, the storm will cease. The sailors refuse to do this and continue rowing, but all their efforts fail and they are eventually forced to throw Jonah overboard. As a result, the storm calms and the sailors then offer sacrifices to God. Jonah is miraculously saved by being swallowed by a large fish, in whose belly he spends three days and three nights. While in the great fish, Jonah prays to God in his affliction and commits to thanksgiving and to paying what he has vowed. God then commands the fish to vomit Jonah out.

God again commands Jonah to travel to Nineveh and prophesy to its inhabitants. This time he goes and enters the city, crying, "In forty days Nineveh shall be overthrown." After Jonah has walked across Nineveh, the people of Nineveh begin to believe his word and proclaim a fast. The king of Nineveh puts on sackcloth and sits in ashes, making a proclamation which decrees fasting, the wearing of sackcloth, prayer, and repentance. God sees their repentant hearts and spares the city at that time. The entire city is humbled and broken with the people (and even the animals) in sackcloth and ashes.

Displeased by this, Jonah refers to his earlier flight to Tarshish while asserting that, since God is merciful, it was inevitable that God would turn from the threatened calamities. He then leaves the city and makes himself a shelter, waiting to see whether or not the city will be destroyed. God causes a plant (in Hebrew a "kikayon") to grow over Jonah's shelter to give him some shade from the sun. Later, God causes a worm to bite the plant's root and it withers. Jonah, now being exposed to the full force of the sun, becomes faint and pleads for God to kill him.

The story of Jonah has numerous theological implications, and this has long been recognized. In early translations of the Hebrew Bible, Jewish translators tended to remove anthropomorphic imagery in order to prevent the reader from misunderstanding the ancient texts. This tendency is evidenced in both the Aramaic translations (e.g. the Targums) and the Greek translations (e.g. the Septuagint). As far as the Book of Jonah is concerned, Targum Jonah offers a good example of this:

In Jonah 1:6, the Masoretic Text (MT) reads, "...perhaps God will pay heed to us..." Targum Jonah translates this passage as: "...perhaps there will be mercy from the Lord upon us..." The captain's proposal is no longer an attempt to change the divine will; it is an attempt to appeal to divine mercy. Furthermore, in Jonah 3:9, the MT reads, "Who knows, God may turn and relent [lit. repent]?" Targum Jonah translates this as, "Whoever knows that there are sins on his conscience let him repent of them and we will be pitied before the Lord." God does not change His mind; He shows pity.

Fragments of the book were found among the Dead Sea Scrolls (DSS), most of which follows the Masoretic Text closely and with Mur XII reproducing a large portion of the text. As for the non-canonical writings, the majority of references to biblical texts were made as appeals to authority. The Book of Jonah appears to have served less purpose in the Qumran community than other texts, as the writings make no references to it.

The earliest Christian interpretations of Jonah are found in the Gospel of Matthew and the Gospel of Luke. Both Matthew and Luke record a tradition of Jesus’ interpretation of the Book of Jonah (notably, Matthew includes two very similar traditions in chapters 12 and 16). As with most Old Testament interpretations found in the New Testament, Jesus’ interpretation is primarily typological. Jonah becomes a “type” for Jesus. Jonah spent three days in the belly of the fish; Jesus will spend three days in the grave. Here, Jesus plays on the imagery of Sheol found in Jonah's prayer. While Jonah metaphorically declared, “Out of the belly of Sheol I cried,” Jesus will literally be in the belly of Sheol. Finally, Jesus compares his generation to the people of Nineveh. Jesus fulfills his role as a type of Jonah, however his generation fails to fulfill its role as a type of Nineveh. Nineveh repented, but Jesus' generation, which has seen and heard one even greater than Jonah, fails to repent. Through his typological interpretation of the Book of Jonah, Jesus has weighed his generation and found it wanting.

The debate over the credibility of the miracle of Jonah is not simply a modern one. The credibility of a human being surviving in the belly of a great fish has long been questioned. In c. 409 AD, Augustine of Hippo wrote to Deogratias concerning the challenge of some to the miracle recorded in the Book of Jonah. He writes:

Augustine responds that if one is to question one miracle, then one should question all miracles as well (section 31). Nevertheless, despite his apologetic, Augustine views the story of Jonah as a figure for Christ. For example, he writes: "As, therefore, Jonah passed from the ship to the belly of the whale, so Christ passed from the cross to the sepulchre, or into the abyss of death. And as Jonah suffered this for the sake of those who were endangered by the storm, so Christ suffered for the sake of those who are tossed on the waves of this world." Augustine credits his allegorical interpretation to the interpretation of Christ himself (Matt. 12:39,40), and he allows for other interpretations as long as they are in line with Christ's.

The "Ordinary Gloss", or "Glossa Ordinaria", was the most important Christian commentary on the Bible in the later Middle Ages. "The Gloss on Jonah relies almost exclusively on Jerome’s commentary on Jonah (c. 396), so its Latin often has a tone of urbane classicism. But the Gloss also chops up, compresses, and rearranges Jerome with a carnivalesque glee and scholastic directness that renders the Latin authentically medieval." "The Ordinary Gloss on Jonah" has been translated into English and printed in a format that emulates the first printing of the Gloss.

The relationship between Jonah and his fellow Jews is ambivalent, and complicated by the Gloss's tendency to read Jonah as an allegorical prefiguration of Jesus Christ. While some glosses in isolation seem crudely supersessionist (“The foreskin believes while the circumcision remains unfaithful”), the prevailing allegorical tendency is to attribute Jonah's recalcitrance to his abiding love for his own people and his insistence that God's promises to Israel not be overridden by a lenient policy toward the Ninevites. For the glossator, Jonah's pro-Israel motivations correspond to Christ's demurral in the Garden of Gethsemane (“My Father, if it be possible, let this chalice pass from me” [Matt. 26:39]) and the Gospel of Matthew's and Paul's insistence that “salvation is from the Jews” (Jn. 4:22). While in the Gloss the plot of Jonah prefigures how God will extend salvation to the nations, it also makes abundantly clear—as some medieval commentaries on the Gospel of John do not—that Jonah and Jesus are Jews, and that they make decisions of salvation-historical consequence "as Jews".

In Jungian analysis, the belly of the whale can be seen as a symbolic death and rebirth, which is also an important stage in comparative mythologist Joseph Campbell's "hero's journey".

NCSY director of education David Bashevkin sees Jonah as a thoughtful prophet who comes to religion out of a search for theological truth and is constantly disappointed by those who come to religion to provide mere comfort in the face of adversity inherent to the human condition. "If religion is only a blanket to provide warmth from the cold, harsh realities of life," Bashevkin imagines Jonah asking, "did concerns of theological truth and creed even matter?" The lesson taught by the episode of the tree at the end of the book is that comfort is a deep human need that religion provides, but this need not obscure the role of God.

The Hebrew text of Jonah reads "dag gadol" (Hebrew: דג גדול), which literally means "great fish". The Septuagint translates this into Greek as "ketos megas" (Greek: κῆτος μέγας), "huge fish"; in Greek mythology the term was closely associated with sea monsters. Saint Jerome later translated the Greek phrase as "piscis grandis" in his Latin Vulgate, and as "cetus" in Matthew. At some point, "cetus" became synonymous with whale (cf. cetyl alcohol, which is alcohol derived from whales). In his 1534 translation, William Tyndale translated the phrase in Jonah 2:1 as "greate fyshe", and he translated the word "ketos" (Greek) or "cetus" (Latin) in Matthew as "whale". Tyndale's translation was later followed by the translators of the King James Version of 1611 and has enjoyed general acceptance in English translations.

In line 2:1 the book refers to the fish as "dag gadol", "great fish", in the masculine. However, in 2:2, it changes the gender to "dagah", meaning female fish. The verses therefore read: "And the lord provided a great fish ("dag gadol", דָּג גּדוֹל, masculine) for Jonah, and it swallowed him, and Jonah sat in the belly of the fish (still male) for three days and nights; then, from the belly of the ("dagah", דָּגָה, female) fish, Jonah began to pray." The peculiarity of this change of gender led the later rabbis to reason that this means Jonah was comfortable in the roomy male fish, so he did not pray, but that God then transferred him to a smaller, female fish, in which the prophet was uncomfortable, so that he prayed.

The Book of Jonah closes abruptly with an epistolary warning based on the emblematic trope of a fast-growing vine present in Persian narratives, and popularized in fables such as "The Gourd and the Palm-tree" during the Renaissance, for example by Andrea Alciato.

St. Jerome differed with St. Augustine in his Latin translation of the plant known in Hebrew as קיקיון ("qīqayōn"), using "hedera" (from the Greek, meaning "ivy") over the more common Latin "cucurbita," "gourd," from which the English word "gourd" (Old French "coorde," "couhourde") is derived. The Renaissance humanist artist Albrecht Dürer memorialized Jerome's decision to use an analogical type of Christ's "I am the Vine, you are the branches" in his woodcut "Saint Jerome in His Study".




</doc>
<doc id="4452" url="https://en.wikipedia.org/wiki?curid=4452" title="Book of Micah">
Book of Micah

The Book of Micah is the sixth of the twelve minor prophets in the Hebrew Bible. Ostensibly, it records the sayings of Micah, whose name is "Mikayahu" (), meaning "Who is like Yahweh?", an 8th-century BCE prophet from the village of Moresheth in Judah (Hebrew name from the opening verse: מיכה המרשתי).

The book has three major divisions, chapters 1–2, 3–5 and 6–7, each introduced by the word "Hear," with a pattern of alternating announcements of doom and expressions of hope within each division. Micah reproaches unjust leaders, defends the rights of the poor against the rich and powerful; while looking forward to a world at peace centered on Zion under the leadership of a new Davidic monarch.

While the book is relatively short, it includes lament (1.8–16; 7.8–10), theophany (1.3–4), hymnic prayer of petition and confidence (7.14–20), and the "covenant lawsuit" (6.1–8), a distinct genre in which Yahweh (God) sues Israel for breach of contract of the Mosaic covenant.

Chapter 1:1 identifies the prophet as "Micah of Moresheth" (a town in southern Judah), and states that he lived during the reigns of Yehotam, Ahaz and Hezekiah, roughly 750–700 BCE.

This corresponds to the period when, after a long period of peace, Israel, Judah, and the other nations of the region came under increasing pressure from the aggressive and rapidly expanding Neo-Assyrian empire. Between 734 and 727 Tiglath-Pileser III of Assyria conducted almost annual campaigns in Palestine, reducing the Kingdom of Israel, the Kingdom of Judah and the Philistine cities to vassalage, receiving tribute from Ammon, Moab and Edom, and absorbing Damascus (the Kingdom of Aram) into the Empire. On Tiglath-Pileser's death Israel rebelled, resulting in an Assyrian counter-attack and the destruction of the capital, Samaria, in 721 after a three-year siege. Micah 1:2–7 draws on this event: Samaria, says the prophet, has been destroyed by God because of its crimes of idolatry, oppression of the poor, and misuse of power. The Assyrian attacks on Israel (the northern kingdom) led to an influx of refugees into Judah, which would have increased social stresses, while at the same time the authorities in Jerusalem had to invest huge amounts in tribute and defense.

When the Assyrians attacked Judah in 701 they did so via the Philistine coast and the Shephelah, the border region which included Micah's village of Moresheth, as well as Lachish, Judah's second largest city. This in turn forms the background to verses 1:8–16, in which Micah warns the towns of the coming disaster (Lachish is singled out for special mention, accused of the corrupt practices of both Samaria and Jerusalem). In verses 2:1–5 he denounces the appropriation of land and houses, which might simply be the greed of the wealthy and powerful, or possibly the result of the militarising of the area in preparation for the Assyrian attack.

Some, but not all, scholars accept that only chapters 1–3 contain material from the late 8th century prophet Micah. The latest material comes from the post-Exilic period after the Temple was rebuilt in 515 BCE, so that the early 5th century BCE seems to be the period when the book was completed. The first stage was the collection and arrangement of some spoken sayings of the historical Micah (the material in chapters 1–3), in which the prophet attacks those who build estates through oppression and depicts the Assyrian invasion of Judah as Yahweh's punishment on the kingdom's corrupt rulers, including a prophecy that the Temple will be destroyed.

The prophecy was not fulfilled in Micah's time, but a hundred years later when Judah was facing a similar crisis with the Neo-Babylonian Empire, Micah's prophecies were reworked and expanded to reflect the new situation. Still later, after Jerusalem did fall to the Neo-Babylonian Empire, the book was revised and expanded further to reflect the circumstances of the late exilic and post-exilic community.

At the broadest level, Micah can be divided into three roughly equal parts:

Within this broad three-part structure are a series of alternating oracles of judgment and promises of restoration:


Micah addresses the future of Judah/Israel after the Babylonian exile. Like Isaiah, the book has a vision of the punishment of Israel and creation of a "remnant", followed by world peace centered on Zion under the leadership of a new Davidic monarch; the people should do justice, turn to Yahweh, and await the end of their punishment. However, whereas Isaiah sees Jacob/Israel joining "the nations" under Yahweh's rule, Micah looks forward to Israel ruling over the nations. Insofar as Micah appears to draw on and rework parts of Isaiah, it seems designed at least partly to provide a counterpoint to that book.

In the New Testament, the Book of Matthew quotes from the Book of Micah in relation to Jesus being born in Bethlehem:

Jesus quotes Micah when he warns that families will be divided by the gospel:






</doc>
<doc id="4453" url="https://en.wikipedia.org/wiki?curid=4453" title="Book of Nahum">
Book of Nahum

The Book of Nahum is the seventh book of the 12 minor prophets of the Hebrew Bible. It is attributed to the prophet Nahum, and was probably written in Jerusalem in the 7th century BC.

Josephus places Nahum during the reign of Jotham, while others place him in the beginning of the reign of Ahaz, Judah's next king, or even the latter half of the reign of Hezekiah, Ahaz's son; all three accounts date the book to the 8th century BC. The book would then have been written in Jerusalem, where Nahum would have witnessed the invasion of Sennacherib and the destruction of his host (2 Kings 19:35).

The scholarly consensus is that the "book of vision" was written at the time of the fall of Nineveh at the hands of the Medes and Babylonians in 612 BC. This theory is demonstrated by the fact that the oracles must be dated after the Assyrian destruction of Thebes, Egypt in 663 BC, as this event is mentioned in .

Little is known about Nahum's personal history. His name means "comforter", and he was from the town of "Elkosh" or "Alqosh" (), which scholars have attempted to identify with several cities, including the modern `Alqush of Assyria and Capharnaum of northern Galilee. He was a very nationalistic Hebrew, and lived amongst the Elkoshites in peace. His writings were likely written in about 615 BC, before the downfall of Assyria.

The subject of Nahum's prophecy is the approaching complete and final destruction of Nineveh, the capital of the great and at that time flourishing Assyrian empire. Ashurbanipal was at the height of his glory. Nineveh was a city of vast extent, and was then the center of the civilization and commerce of the world, according to Nahum a "bloody city all full of lies and robbery" (), a reference to the Neo-Assyrian Empire's military campaigns and demand of tribute and plunder from conquered cities.

Jonah had already uttered his message of warning, and Nahum was followed by Zephaniah, who also predicted (Zephaniah ) the destruction of the city.

Nineveh was destroyed apparently by fire around 625 BC, and the Assyrian empire came to an end, an event which changed the face of Asia.
Archaeological digs have uncovered the splendor of Nineveh in its zenith under Sennacherib (705–681 BC), Esarhaddon (681–669 BC), and Ashurbanipal (669–633 BC). Massive walls were eight miles in circumference. It had a water aqueduct, palaces and a library with 20,000 clay tablets, including accounts of a creation in Enuma Elish and a flood in the Epic of Gilgamesh.

The Babylonian chronicle of the fall of Nineveh tells the story of the end of Nineveh. Nabopolassar of Babylon joined forces with Cyaxares, king of the Medes, and laid siege for three months.

Assyria lasted a few more years after the loss of its fortress, but attempts by Egyptian Pharaoh Neco II to rally the Assyrians failed due to opposition from king Josiah of Judah, and it seemed to be all over by 609 BC.

The Book of Nahum consists of two parts:

Chapters two and three describe the fall of Nineveh, which later took place in 612 BC. Nineveh is compared to Thebes, the Egyptian city that Assyria itself had destroyed in 663 BC. Nahum describes the siege and frenzied activity of Nineveh's troops as they try in vain to halt the invaders. Poetically, he becomes a participant in the battle, and with subtle irony, barks battle commands to the defenders. Nahum uses numerous similes and metaphors . Nineveh is ironically compared with a lion, in reference to the lion as an Assyrian symbol of power; Nineveh is the lion of strength that has a den full of dead prey but will become weak like the lion hiding in its den. It comes to conclusion with a taunt song and funeral dirge of the impending destruction of Nineveh and the "sleep" or death of the Assyrian people and demise of the once great Assyrian conqueror-rulers .

Nahum's prophecy carries a particular warning to the Ninevites of coming events, although he is partly in favor of the destruction. One might even say that the book of Nahum is "a celebration of the fall of Assyria." And this is not just a warning or speaking positively of the destruction of Nineveh, it is also a positive encouragement and "message of comfort for Israel, Judah, and others who had experienced the "endless cruelty" () of the Assyrians." The prophet Jonah shows us where God shows concern for the people of Nineveh, while Nahum's writing testifies to his belief in the righteousness/justice of God and how God dealt with those Assyrians in punishment according to "their cruelty" (). The Assyrians had been used as God's "rod of […] anger, and the staff in their hand [as] indignation." (Isaiah 10:5)

From its opening, Nahum shows God to be slow to anger, but that God will by no means ignore the guilty; God will bring his vengeance and wrath to pass. God is presented as a God who will punish evil, but will protect those who trust in Him. The opening passage () states: "God is jealous, and the L revengeth; the L revengeth, and is furious; the L will take vengeance on his adversaries, and he reserveth wrath for his enemies. The L is slow to anger, and great in power, and will not at all acquit the wicked". God is strong and will use means, but a mighty God doesn't need anyone else to carry out vengeance and wrath for him.

God's judgement on Nineveh is "all because of the wanton lust of a harlot, alluring, the mistress of sorceries, who enslaved nations by her prostitution and peoples by her witchcraft" ( NIV). Infidelity, according to the prophets, related to spiritual unfaithfulness. For example: "the land is guilty of the vilest adultery in departing from the L" ( NIV). John of Patmos used a similar analogy in Revelation chapter 17.

The prophecy of Nahum was referenced in the deuterocanonical Book of Tobit. In Tobit 14:4 (NRSV) a dying Tobit says to his son Tobias and Tobias' sons:

[My son] hurry off to Media, for I believe the word of God that Nahum spoke about Nineveh, that all these things will take place and overtake Assyria and Nineveh. Indeed, everything that was spoken by the prophets of Israel, whom God sent, will occur. 
However, some versions, such as the King James Version, refer to the prophet Jonah instead.

The book was introduced in Calvin's Commentary as a complete and finished poem:
Nahum, taking words from Moses himself, have shown in a general way what sort of "Being God is". The Reformation theologian Calvin argued, Nahum painted God by which His nature must be seen, and "it is from that most memorable vision, when God appeared to Moses after the breaking of the tables."

The book could be seen as an allusion to the history as described by Moses; for the minor Prophets, in promising God's assistance to his people, must often remind how God in a miraculous manner brought up the Jews from Egypt.




</doc>
<doc id="4454" url="https://en.wikipedia.org/wiki?curid=4454" title="Book of Haggai">
Book of Haggai

The Book of Haggai, also known as the Book of Aggeus, is a book of the Hebrew Bible or Tanakh, and has its place as the third-to-last of the Minor Prophets. It is a short book, consisting of only two chapters. The historical setting dates around 520 BC before the Temple has been rebuilt.

The Book of Haggai is named after its presumed author, the prophet Haggai. There is no biographical information given about the prophet in the Book of Haggai. Haggai's name is derived from the Hebrew verbal root "hgg", which means "to make a pilgrimage." W. Sibley Towner suggests that Haggai's name might come "from his single-minded effort to bring about the reconstruction of that destination of ancient Judean pilgrims, the Temple in Jerusalem."

The "Book of Haggai" was written in 520 BC, some 18 years after Cyrus had conquered Babylon and issued a decree in 538 BC, allowing the captive Jews to return to Judea. Cyrus saw the restoration of the temple as necessary for the restoration of the religious practices, and a sense of peoplehood, after a long exile.

Haggai's message is filled with an urgency for the people to proceed with the rebuilding of the second Jerusalem temple. Haggai attributes a recent drought to the people's refusal to rebuild the temple, which he sees as key to Jerusalem’s glory. The book ends with the prediction of the downfall of kingdoms, with one Zerubbabel, governor of Judah, as the Lord’s chosen leader. The language here is not as finely wrought as in some other books of the minor prophets, yet the intent seems straightforward.

The first chapter contains the first address (2–11) and its effects (12–15).

The second chapter contains:

These discourses are referred to in Ezra 5:1 and 6:14. (Compare Haggai 2:7, 8 and 22)

Haggai reports that three weeks after his first prophecy, the rebuilding of the Temple began on September 7 521 BC. "They came and began to work on the house of the LORD Almighty, their God, on the twenty-fourth day of the sixth month in the second year of Darius the King.(Haggai 1:14–15) and the Book of Ezra indicates that it was finished on February 25 516 BC "The Temple was completed on the third day of the month Adar, in the sixth year of the reign of King Darius." (Ezra 6:15)




</doc>
<doc id="4455" url="https://en.wikipedia.org/wiki?curid=4455" title="Book of Malachi">
Book of Malachi

The Book of Malachi (or Malachias; , "") is the last book of the Neviim contained in the Tanakh, canonically the last of the Twelve Minor Prophets. In the Christian ordering, the grouping of the Prophetic Books is the last section of the Old Testament, making Malachi the last book before The New Testament.

The book is commonly attributed to a prophet by the name of "Malachi," as its title has frequently been understood as a proper name, although its Hebrew meaning is simply "My Messenger " (the Septuagint reads "his messenger") and may not be the author's name at all. The name occurs in the superscription at 1:1 and in 3:1, although it is highly unlikely that the word refers to the same character in both of these references. Thus, there is substantial debate regarding the identity of the book's author. One of the Targums identifies Ezra (or Esdras) as the author of Malachi. Priest and Historian Jerome suggests that this may be because Ezra is seen as an intermediary between the prophets and the "great synagogue." There is, however, no historical evidence yet to support this claim.

Some scholars note affinities between Zechariah 9–14 and the Book of Malachi. Zechariah 9, Zechariah 12, and Malachi 1 are all introduced as The word of Elohim. Some scholars argue that this collection originally consisted of three independent and anonymous prophecies, two of which were subsequently appended to the Book of Zechariah as what they refer to as Deutero-Zechariah, with the third becoming the Book of Malachi. As a result, most scholars consider the Book of Malachi to be the work of a single author who may or may not have been identified by the title Malachi. The present division of the oracles results in a total of 12 books of minor prophets, a number parallelling the sons of Jacob who became the heads of the 12 Israelite tribes. The "Catholic Encyclopedia" asserts, "We are no doubt in presence of an abbreviation of the name Mál'akhîyah, that is Messenger of Elohim."

Little is known of the biography of the author of the Book of Malachi, although it has been suggested that he may have been Levitical. The books of Zechariah and Haggai were written during the lifetime of Ezra (see 5:1); perhaps this may explain the similarities in style.

According to the editors of the 1897 Easton's Bible Dictionary, some scholars believe the name "Malachi" is not a proper noun but rather an abbreviation of "messenger of YHWH". This reading could be based on Malachi 3:1, "Behold, I will send "my messenger"...", if "my messenger" is taken literally as the name "Malachi". Several scholars consider the book to be anonymous, regarding verse 1:1 as a later addition. However, other scholars, including the editors of the "Catholic Encyclopedia", argue that the grammatical evidence leads us to conclude that Malachi is in fact a name.

Another interpretation of the authorship comes from the Septuagint superscription, ὲν χειρὶ ἀγγήλου αὐτοῦ, which can be read as either "by the hand of his messenger" or as "by the hand of his angel". The "angel" reading found an echo among the ancient Church Fathers and ecclesiastical writers, and even gave rise to the "strangest fancies", especially among the disciples of Origen of Alexandria.

There are very few historical details in the Book of Malachi. The greatest clue as to its dating may lie in the fact that the Persian-era term for governor (pehâ) is used in 1:8. This points to a post-exilic (that is, after 538 BCE) date of composition both because of the use of the Persian period term and because Judah had a king before the exile. Since, in the same verse, the temple has been rebuilt, the book must also be later than 515 BC. Malachi was apparently known to the author of Ecclesiasticus early in the 2nd century BC. Because of the development of themes in the book of Malachi, most scholars assign it to a position after Haggai and Zechariah, close to the time when Ezra and Nehemiah came to Jerusalem in 445 BC.

The Book of Malachi was written to correct the lax religious and social behaviour of the Israelites – particularly the priests – in post-exilic Jerusalem. Although the prophets urged the people of Judah and Israel to see their exile as punishment for failing to uphold their covenant with God, it was not long after they had been restored to the land and to Temple worship that the people's commitment to their God began, once again, to wane. It was in this context that the prophet commonly referred to as Malachi delivered his prophecy.

In 1:2, Malachi has the people of Israel question God's love for them. This introduction to the book illustrates the severity of the situation which Malachi addresses. The graveness of the situation is also indicated by the dialectical style with which Malachi confronts his audience. Malachi proceeds to accuse his audience of failing to respect God as God deserves. One way in which this disrespect is made manifest is through the substandard sacrifices which Malachi claims are being offered by the priests. While God demands animals that are "without blemish" (Leviticus 1:3, NRSV), the priests, who were "to determine whether the animal was acceptable" (Mason 143), were offering blind, lame and sick animals for sacrifice because they thought nobody would notice.

In 2:1, Malachi states Yahweh Sabaoth is sending a curse on the priests who have not honored him with appropriate animal sacrifices: "Now, watch how I am going to paralyze your arm and throw dung in your face--the dung from your very solemnities--and sweep you away with it. Then you shall learn that it is I who have given you this warning of my intention to abolish my covenant with Levi, says Yahweh Sabaoth."

In 2:10, Malachi addresses the issue of divorce. On this topic, Malachi deals with divorce both as a social problem ("Why then are we faithless to one another ... ?" 2:10) and as a religious problem ("Judah ... has married the daughter of a foreign god" 2:11). In contrast to the book of Ezra, Malachi urges each to remain steadfast to the wife of his youth.

Malachi also criticizes his audience for questioning God's justice. He reminds them that God is just, exhorting them to be faithful as they await that justice. Malachi quickly goes on to point out that the people have not been faithful. In fact, the people are not giving God all that God deserves. Just as the priests have been offering unacceptable sacrifices, so the people have been neglecting to offer their full tithe to God. The result of these shortcomings is that the people come to believe that no good comes out of serving God.

Malachi assures the faithful among his audience that in the eschaton, the differences between those who served God faithfully and those who did not will become clear. The book concludes by calling upon the teachings of Moses and by promising that Elijah will return prior to the Day of Yahweh.

The book of Malachi is divided into three chapters in the Hebrew Bible and the Greek Septuagint and four chapters in the Latin Vulgate. The fourth chapter in the Vulgate consists of the remainder of the third chapter starting at verse 3:19.

The New Revised Standard Version of the Bible supplies headings for the book as follows:

The majority of scholars consider the book to be made up of six distinct oracles. According to this scheme, the book of Malachi consists of a series of disputes between Yahweh and the various groups within the Israelite community. In the course of the book's three or four chapters, Yahweh is vindicated while those who do not adhere to the law of Moses are condemned. Some scholars have suggested that the book, as a whole, is structured along the lines of a judicial trial, a suzerain treaty or a covenant—one of the major themes throughout the Hebrew Scriptures. Implicit in the prophet's condemnation of Israel's religious practices is a call to keep Yahweh's statutes.

The Book of Malachi draws upon various themes found in other books of the Bible. Malachi appeals to the rivalry between Jacob and Esau and of Yahweh's preference for Jacob contained in Book of Genesis 25–28. Malachi reminds his audience that, as descendants of Jacob (Israel), they have been and continue to be favoured by God as God's chosen people. In the second dispute, Malachi draws upon the Levitical Code (e.g. Leviticus 1:3) in condemning the priest for offering unacceptable sacrifices.

In the third dispute (concerning divorce), the author of the Book of Malachi likely intends his argument to be understood on two levels. Malachi appears to be attacking either the practice of divorcing Jewish wives in favour of foreign ones (a practice which Ezra vehemently condemns) or, alternatively, Malachi could be condemning the practice of divorcing foreign wives in favour of Jewish wives (a practice which Ezra promoted). Malachi appears adamant that nationality is not a valid reason to terminate a marriage, "For I hate divorce, says the Lord . . ." (2:16).

In many places throughout the Hebrew Scriptures – particularly the Book of Hosea – Israel is figured as Yahweh's wife or bride. Malachi's discussion of divorce may also be understood to conform to this metaphor. Malachi could very well be urging his audience not to break faith with Yahweh (the God of Israel) by adopting new gods or idols. It is quite likely that, since the people of Judah were questioning Yahweh's love and justice (1:2, 2:17), they might be tempted to adopt foreign gods. William LaSor suggests that, because the restoration to the land of Judah had not resulted in anything like the prophesied splendor of the messianic age which had been prophesied, the people were becoming quite disillusioned with their religion.

Indeed, the fourth dispute asserts that judgment is coming in the form of a messenger who "is like refiner's fire and like fullers' soap . . ." (3:2). Following this, the prophet provides another example of wrongdoing in the fifth dispute – that is, failing to offer full tithes. In this discussion, Malachi has Yahweh request the people to "Bring the full tithe . . . [and] see if I will not open the windows of heaven for you and pour down on you an overflowing blessing" (3:10). This request offers the opportunity for the people to amend their ways. It also stresses that keeping the Lord's statutes will not only allow the people to avoid God's wrath, but will also lead to God's blessing. In the sixth dispute, the people of Israel illustrate the extent of their disillusionment. Malachi has them say "'It is vain to serve God . . . Now we count the arrogant happy; evildoers not only prosper, but when they put God to the test they escape'" (3:14–15). Once again, Malachi has Yahweh assure the people that the wicked will be punished and the faithful will be rewarded.

In the light of what Malachi understands to be an imminent judgment, he exhorts his audience to "Remember the teaching of my servant Moses, that statutes and ordinances that I commanded him at Horeb for all Israel" (4:4; 3:22, MT). Before the Day of the Lord, Malachi declares that Elijah (who "ascended in a whirlwind into heaven . . . [,]" 2 Kings 2:11) will return to earth in order that people might follow in God's ways.

Primarily because of its messianic promise, the Book of Malachi is frequently referred to in the Christian New Testament. What follows is a brief comparison between the Book of Malachi and the New Testament texts which refer to it (as suggested in Hill 84–88).

Although many Christians believe that the messianic prophecies of the Book of Malachi have been fulfilled in the life, ministry, transfiguration, death and resurrection of Jesus of Nazareth, most Jews continue to await the coming of the prophet Elijah who will prepare the way for the Lord.




</doc>
<doc id="4456" url="https://en.wikipedia.org/wiki?curid=4456" title="Book of Zechariah">
Book of Zechariah

The Book of Zechariah, attributed to the Hebrew prophet Zechariah, is included in the Twelve Minor Prophets in the Hebrew Bible.

Zechariah's prophecies took place during the reign of Darius the Great, and were contemporary with Haggai in a post-exilic world after the fall of Jerusalem in 587/6 BC. Ezekiel and Jeremiah wrote before the fall of Jerusalem, while continuing to prophesy in the early exile period. Scholars believe Ezekiel, with his blending of ceremony and vision, heavily influenced the visionary works of Zechariah 1–8. Zechariah is specific about dating his writing (520–518 BC).

During the Exile many Judahites and Benjamites were taken to Babylon, where the prophets told them to make their homes, suggesting they would spend a long period of time there. Eventually freedom did come to many Israelites, when Cyrus the Great overtook the Babylonians in 539 BC. In 538 BC, the famous Edict of Cyrus was released, and the first return took place under Sheshbazzar. After the death of Cyrus in 530 BC, Darius consolidated power and took office in 522 BC. His system divided the different colonies of the empire into easily manageable districts overseen by governors. Zerubbabel comes into the story, appointed by Darius as governor over the district of Yehud Medinata.

Under the reign of Darius, Zechariah also emerged, centering on the rebuilding of the Temple. Unlike the Babylonians, the Persian Empire went to great lengths to keep “cordial relations” between vassal and lord. The rebuilding of the Temple was encouraged by the leaders of the empire in hopes that it would strengthen the authorities in local contexts. This policy was good politics on the part of the Persians, and the Jews viewed it as a blessing from God.

The name "Zechariah" means "God remembered." Not much is known about Zechariah's life other than what may be inferred from the book. It has been speculated that his grandfather Iddo was the head of a priestly family who returned with Zerubbabel, and that Zechariah may himself have been a priest as well as a prophet. This is supported by Zechariah's interest in the Temple and the priesthood, and from Iddo's preaching in the Books of Chronicles.

Most modern scholars believe the Book of Zechariah was written by at least two different people. Zechariah 1–8, sometimes referred to as First Zechariah, was written in the 6th century BC. Zechariah 9–14, often called Second Zechariah, contains within the text no datable references to specific events or individuals but most scholars give the text a date in the fifth century BC. Second Zechariah, in the opinion of some scholars, appears to make use of the books of Isaiah, Jeremiah, and Ezekiel, the Deuteronomistic History, and the themes from First Zechariah. This has led some to believe that the writer(s) or editor(s) of Second Zechariah may have been a disciple of the prophet Zechariah. There are some scholars who go even further and divide Second Zechariah into Second Zechariah (9–11) and Third Zechariah (12–14) since each begins with a heading oracle.

The return from exile is the theological premise of the prophet's visions in chapters 1–6. Chapters 7–8 address the quality of life God wants his renewed people to enjoy, containing many encouraging promises to them. Chapters 9–14 comprise two "oracles" of the future.

The book begins with a preface, which recalls the nation's history, for the purpose of presenting a solemn warning to the present generation. Then follows a series of eight visions, succeeding one another in one night, which may be regarded as a symbolical history of Israel, intended to furnish consolation to the returned exiles and stir up hope in their minds. The symbolic action, the crowning of Joshua, describes how the kingdoms of the world become the kingdom of God's Messiah.

Chapters Zechariah 7 and Zechariah 8, delivered two years later, are an answer to the question whether the days of mourning for the destruction of the city should be kept any longer, and an encouraging address to the people, assuring them of God's presence and blessing.

This section consists of two "oracles" or "burdens": 

The purpose of this book is not strictly historical but theological and pastoral. The main emphasis is that God is at work and all His good deeds, including the construction of the Second Temple, are accomplished "not by might nor by power, but by My Spirit."() Ultimately, YHWH plans to live again with His people in Jerusalem. He will save them from their enemies and cleanse them from sin. However, God requires repentance, a turning away from sin towards faith in Him ()

Zechariah's concern for purity is apparent in the temple, priesthood and all areas of life as the prophecy gradually eliminates the influence of the governor in favour of the high priest, and the sanctuary becomes ever more clearly the centre of messianic fulfillment. The prominence of prophecy is quite apparent in Zechariah, but it is also true that Zechariah (along with Haggai) allows prophecy to yield to the priesthood; this is particularly apparent in comparing Zechariah to "Third Isaiah" (chapters 55–66 of the Book of Isaiah), whose author was active sometime after the first return from exile.

Most Christian commentators read the series of predictions in chapters 7 to 14 as Messianic prophecies, either directly or indirectly. These chapters helped the writers of the Gospels understand Jesus’ suffering, death and resurrection, which they quoted as they wrote of Jesus’ final days. Much of the Book of Revelation, which narrates the denouement of history, is also colored by images in Zechariah.

Chapters 9–14 of the Book of Zechariah are an early example of apocalyptic literature. Although not as fully developed as the apocalyptic visions described in the Book of Daniel, the "oracles", as they are titled in Zechariah 9–14, contain apocalyptic elements. One theme these oracles contain is descriptions of the Day of the Lord, when "the Lord will go forth and fight against those nations as when he fights on a day of battle" (Zechariah 14:3). These chapters also contain "pessimism about the present, but optimism for the future based on the expectation of an ultimate divine victory and the subsequent transformation of the cosmos".

The final word in Zechariah proclaims that on the Day of the Lord "in that day there shall be no more the Canaanite in the house of the LORD of hosts" (, KJV), proclaiming the need for purity in the Temple, which would come when God judges at the end of time. The word כְנַעֲנִי rendered "Canaanite" is alternatively translated as "trader" (RSV) or "trafficker" (, Mechon-Mamre) as in other scripture verses.




</doc>
<doc id="4457" url="https://en.wikipedia.org/wiki?curid=4457" title="Book of Zephaniah">
Book of Zephaniah

The Book of Zephaniah (, "Tsfanya") is the ninth of the Twelve Minor Prophets, preceded by the Book of Habakkuk and followed by the Book of Haggai. Zephaniah means "Yahweh has hidden/protected," or "Yahweh hides".
Zephaniah is also a male given name.Notable people with the name are actually called Zeph which is the alternate form of it/Zephyr.

The book's superscription attributes its authorship to "Zephaniah son of Cushi son of Gedaliah son of Amariah son of Hezekiah, in the days of King Josiah son of Amon of Judah" (1:1, NRSV). All that is known of Zephaniah comes from the text.

The name "Cushi," Zephaniah's father, means "Cushite" or "Ethiopian," and the text of Zephaniah mentions the sin and restoration of Ethiopians. While some have concluded from this that Zephaniah was a black Jew, Ehud Ben Zvi maintains that, based on the context, "Cushi" must be understood as a personal name rather than an indicator of nationality. Abraham ibn Ezra interpreted the name Hezekiah in the superscription as King Hezekiah of Judah, though that is not a claim advanced in the text of Zephaniah.

As with many of the other prophets, there is no external evidence to directly associate composition of the book with a prophet by the name of Zephaniah. Some scholars, such as Kent Harold Richards and Jason DeRouchie, consider the words in Zephaniah to reflect a time early in the reign of King Josiah (640–609 BC) before his reforms of 622 BC took full effect, in which case the prophet may have been born during the reign of Manasseh (698/687–642 BC). Others agree that some portion of the book is postmonarchic, that is, dating to later than 586 BC when the Kingdom of Judah fell in the Siege of Jerusalem. Some who consider the book to have largely been written by a historical Zephaniah have suggested that he may have been a disciple of Isaiah because of the two books' similar focus on rampant corruption and injustice in Judah.

If Zephaniah was largely composed during the monarchic period, then its composition was occasioned by Judah's refusal to obey its covenant obligations toward Yahweh despite having seen Israel's exile a generation or two previously—an exile that the Judahite literary tradition attributed to Yahweh's anger against Israel's disobedience to his covenant. In this historical context, Zephaniah urges Judah to obedience to Yahweh, saying that "perhaps" he will forgive them if they do.

"The HarperCollins Study Bible" supplies headings for the book as follows:

More consistently than any other prophetic book, Zephaniah focuses on "the day of the Lord," developing this tradition from its first appearance in Amos. The day of the Lord tradition also appears in Isaiah, Ezekiel, Obadiah, Joel, and Malachi.

The book begins by describing Yahweh's judgement. The threefold repetition of "I will sweep away" in emphasizes the totality of the destruction, as the number three often signifies complete perfection in the Bible. The order of creatures in ("humans and animals ... the birds ... the fish") is the opposite of the creation order in , signifying an undoing of creation. This is also signified by the way that "from the face of the earth" forms an "inclusio" around , hearkening back to how the phrase is used in the Genesis flood narrative in , , and , where it also connotes an undoing of creation.

As is common in prophetic literature in the Bible, a "remnant" survives Yahweh's judgement in Zephaniah by humbly seeking refuge in Yahweh. The book concludes in an announcement of hope and joy, as Yahweh "bursts forth in joyful divine celebration" over his people.

Because of its hopeful tone of the gathering and restoration of exiles, has been included in Jewish liturgy.

Zephaniah served as a major inspiration for the medieval Catholic hymn "Dies Irae," whose title and opening words are from the Vulgate translation of .







</doc>
<doc id="4458" url="https://en.wikipedia.org/wiki?curid=4458" title="Book of Habakkuk">
Book of Habakkuk

The Book of Habakkuk is the eighth book of the 12 minor prophets of the Bible. It is attributed to the prophet Habakkuk, and was probably composed in the late 7th century BC.

Of the three chapters in the book, the first two are a dialog between Yahweh and the prophet. The message that "the just shall live by his faith" (2:4) plays an important role in Christian thought. It is used in the Epistle to the Romans, Epistle to the Galatians, and the Epistle to the Hebrews as the starting point of the concept of faith. A copy of these chapters is included in the Habakkuk Commentary, found among the Dead Sea Scrolls. Chapter 3 may be an independent addition, now recognized as a liturgical piece, but was possibly written by the same author as chapters 1 and 2.

The prophet Habakkuk is generally believed to have written his book in the mid-to-late 7th century BC, not long before the Babylonians' siege and capture of Jerusalem in 586 BC.

Habakkuk identifies himself as a prophet in the opening verse. Due to the liturgical nature of the book of Habakkuk, there have been some scholars who think that the author may have been a temple prophet. Temple prophets are described in 1 Chronicles 25:1 as using lyres, harps and cymbals. Some feel that this is echoed in Habakkuk 3:19b, and that Habakkuk may have been a Levite and singer in the Temple.

There is no biographical information on the prophet Habakkuk; in fact less is known about him than any other writer of the Bible. The only canonical information that exists comes from the book that is named for him. His name comes either from the Hebrew word חבק ("khavak") meaning "embrace" or else from an Akkadian word "hambakuku" for a kind of plant.

Although his name does not appear in any other part of the Jewish Bible, Rabbinic tradition holds Habakkuk to be the Shunammite woman's son, who was restored to life by Elisha in 2 Kings 4:16. The prophet Habakkuk is also mentioned in the narrative of Bel and the Dragon, part of the deuterocanonical additions to Daniel in a late section of that book. In the superscription of the Old Greek version, Habakkuk is called the son of Joshua of the tribe of Levi. In this book Habakkuk is lifted by an angel to Babylon to provide Daniel with some food while he is in the lion's den.

It is unknown when Habakkuk lived and preached, but the reference to the rise and advance of the Chaldeans in 1:6–11 places him in the middle to last quarter of the 7th century BC. One possible period might be during the reign of Jehoiakim, from 609–598 BC. The reasoning for this date is that it is during his reign that the Neo-Babylonian Empire of the Chaldeans was growing in power. The Babylonians marched against Jerusalem in 598 BC. Jehoiakim died while the Babylonians were marching towards Jerusalem and Jehoiakim's eighteen-year-old son Jehoiachin assumed the throne. Upon the Babylonians' arrival, Jehoiachin and his advisors surrendered Jerusalem after a short time. With the transition of rulers and the young age and inexperience of Jehoiachin, they were not able to stand against Chaldean forces. There is a sense of an intimate knowledge of the Babylonian brutality in 1:12–17.

The book of Habakkuk is a book of the Tanakh (the Old Testament) and stands eighth in a section known as the 12 Minor Prophets in the Masoretic and Greek texts. In the Masoretic listing, it follows Nahum and precedes Zephaniah, who are considered to be his contemporaries.

The book consists of three chapters and the book is neatly divided into three different genres:


The major theme of Habakkuk is trying to grow from a faith of perplexity and doubt to the height of absolute trust in God. Habakkuk addresses his concerns over the fact that God will use the Babylonian empire to execute judgment on Judah for their sins.

Habakkuk openly questions the wisdom of God. In the first part of the first chapter, the Prophet sees the injustice among his people and asks why God does not take action. ""1:2 Yahweh, how long will I cry, and you will not hear? I cry out to you “Violence!” and will you not save?" – World English Bible."

In the middle part of Chapter 1, God explains that he will send the Chaldeans (also known as the Babylonians) to punish his people. "1:5 “Look among the nations, watch, and wonder marvelously; for I am working a work in your days, which you will not believe though it is told you. 1:6 For, behold, I raise up the Chaldeans, that bitter and hasty nation, that march through the breadth of the earth, to possess dwelling places that are not theirs. (World English Bible)"

One of the "Eighteen Emendations to the Hebrew Scriptures" appears at 1:12. According to the professional Jewish scribes, the Sopherim, the text of 1:12 was changed from "You [God] do not die" to "We shall not die." The Sopherim considered it disrespectful to say to God, ""You" do not die."

In the final part of the first chapter, the prophet expresses shock at God's choice of instrument for judgment. "1:13 You who have purer eyes than to see evil, and who cannot look on perversity, why do you tolerate those who deal treacherously, and keep silent when the wicked swallows up the man who is more righteous than he, (World English Bible )"

In Chapter 2, he awaits God's response to his challenge. God explains that He will also judge the Chaldeans, and much more harshly. "2:8 Because you have plundered many nations, all the remnant of the peoples will plunder you, because of men’s blood, and for the violence done to the land, to the city and to all who dwell in it. 2:9 Woe to him who gets an evil gain for his house, (World English Bible )"

Finally, in Chapter 3, Habakkuk expresses his ultimate faith in God, even if he doesn't fully understand.
"3:17 For though the fig tree doesn’t flourish, nor fruit be in the vines; the labor of the olive fails, the fields yield no food; the flocks are cut off from the fold, and there is no herd in the stalls: 3:18 yet I will rejoice in Yahweh. I will be joyful in the God of my salvation! (World English Bible )"

The book of Habakkuk is accepted as canonical by adherents of the Jewish and Christian faiths. A commentary on the first two chapters of the book was found among the Dead Sea Scrolls at Qumran. Passages from Habakkuk are quoted by authors of the New Testament, and its message has inspired modern Christian hymn writers.

The Book of Habakkuk is the eighth book of the Twelve Prophets of the Hebrew Bible, and this collection appears in all copies of texts of the Septuagint, the Ancient Greek translation of the Hebrew Bible completed by 132 BC. Likewise, the book of Sirach (or Ecclesiasticus), also written in the 2nd century BC, mentions "The Twelve Prophets".

A partial copy of Habakkuk itself is included in the Habakkuk Commentary, a "pesher" found among the original seven Dead Sea Scrolls discovered in 1947. The Commentary contains a copy of the first two chapters of Habakkuk, but not of the third chapter. The writer of the "pesher" draws a comparison between the Babylonian invasion of the original text and the Roman threat of the writer's own period. What is even more significant than the commentary in the "pesher" is the quoted text of Habakkuk itself. The divergences between the Hebrew text of the scroll and the standard Masoretic Text are startlingly minimal. The biggest differences are word order, small grammatical variations, addition or omission of conjunctions, and spelling variations, but these are small enough to not to damage the meaning of the text.

Some scholars suggest that Chapter 3 may be a later independent addition to the book, in part because it is not included among the Dead Sea Scrolls. However, this chapter does appear in all copies of the Septuagint, as well as in texts from as early as the 3rd century BC. This final chapter is a poetic praise of God, and has some similarities with texts found in the Book of Daniel. However, the fact that the third chapter is written in a different style, as a liturgical piece, does not necessarily mean that Habakkuk was not also its author. Its omission from the Dead Sea Scrolls is attributed to the inability of the Qumran sect to fit Habakkuk's theology with their own narrow viewpoint.

The Talmud (Makkot 24a) mentions that various Biblical figures summarized the 613 commandments into categories that encapsulated all of the 613. At the end of this discussion, the Talmud concludes "Habakkuk came and established [the 613 mitzvoth] upon one, as it is stated: 'But the righteous person shall live by his faith' (Habakkuk 2:4)", meaning that faith encapsulates all of the other commandments.

 is well known in Christianity:
The second half of this verse is quoted by some of the earliest Christian writers. Although this passage is only three words in the original Hebrew, it is quoted three times in the New Testament. Paul the Apostle quotes this verse twice in his epistles: in Epistle to the Romans () and again in Epistle to the Galatians (). In doing so, Paul extends Habakkuk's original concept of righteous living at the present time into a future life. The same verse is quoted in Epistle to the Hebrews, where Habakkuk's vision is tied to Christ and used to comfort the church during a period of persecution. These three epistles are considered to be "the three great doctrinal books of the New Testament," and Habakkuk's statement concerning faith forms the backbone of each book.

Modern Christian hymns have been inspired by the words of the prophet Habakkuk. The Christian hymn "", written in 1900 by William J. Kirkpatrick, is based on verse 2:20. The fourth verse of William Cowper's hymn "Sometimes a Light Surprises", written in 1779, quotes Habakkuk 3:17–18.

Irish composer Charles Villiers Stanford set slightly revised portions of text from the first and second chapters of Habakkuk in his piece for SATB choir, Soprano and Tenor soloist and organ, "For Lo, I Raise Up".






</doc>
<doc id="4459" url="https://en.wikipedia.org/wiki?curid=4459" title="Backward compatibility">
Backward compatibility

Backward compatibility (sometimes known as backwards compatibility) is a property of a system, product, or technology that allows for interoperability with an older legacy system, or with input designed for such a system, especially in telecommunications and computing. Backward compatibility is sometimes also called downward compatibility.

Modifying a system in a way that does not allow backward compatibility is sometimes called "breaking" backward compatibility.

A complementary concept is forward compatibility. A design that is forward-compatible usually has a roadmap for compatibility with future standards and products.

A related term from programming jargon is hysterical reasons or hysterical raisins (homophones for "historical reasons"), as the purpose of some software features may be solely to support older hardware or software versions.

There are several incentives for a company to implement backward compatibility. Backward compatibility can be used to preserve older software that would have otherwise been lost when a manufacturer decides to stop supporting older hardware. Classic video games are a common example used when discussing the value of supporting older software. The cultural impact of video games is a large part of their continued success, and some believe ignoring backward compatibility would cause these titles to disappear. Backward compatibility also acts as an additional selling point for new hardware, as an existing player base can more affordably upgrade to subsequent generations of a console. This also helps to make up for a lack of content in the early launch of new systems, as users can pull from the previous console's large library of games while developers slowly transition to the new hardware. 

One example of this is the Sony PlayStation 2 (PS2) which was backward compatible with games for its predecessor PlayStation (PS1). While the selection of PS2 games available at launch was small, sales of the console were nonetheless strong in 2000-2001 thanks to the large library of games for the preceding PS1. This bought time for the PS2 to grow a large installed base and developers to release more quality PS2 games for the crucial 2001 holiday season.

Additionally, and despite not being included at launch, Microsoft slowly incorporated backward compatibility for select titles on the Xbox One several years into its product life cycle. Players have racked up over a billion hours with backward compatible games on Xbox, and it is anticipated that next generation consoles such as PlayStation 5 and Xbox Series X will also support this feature. A large part of the success and implementation of this feature is that the hardware within newer generation consoles is both powerful and similar enough to legacy systems that older titles can be broken down and re-configured to run on the Xbox One. The backward compatibility program not only supports the previous generation Xbox 360, but also titles from the original Xbox system. Some titles are even given slight visual improvements and additional levels at no cost to the user. This program has proven incredibly popular with Xbox players and goes against the recent trend of studio made remasters of classic titles, creating what some believe to be an important shift in console maker's strategies.

The literal costs of supporting old software is considered a large drawback to the usage of backward compatibility. The associated costs of backward compatibility are a higher bill of materials if hardware is required to support the legacy systems; increased complexity of the product that may lead to longer time to market, technological hindrances, and slowing innovation; and increased expectations from users in terms of compatibility. Because of this, several gaming consoles chose to phase out backward compatibility toward the end of the console generation in order to reduce cost and briefly re-invigorate sales before the arrival of newer hardware. 

A notable example is the Sony PlayStation 3, as the first PS3 iteration was expensive to manufacture in part due to including the Emotion Engine from the preceding PS2 in order to run PS2 games, since the PS3 architecture was completely different from the PS2. Subsequent PS3 hardware revisions have eliminated the Emotion Engine as it saved production costs while removing the ability to run PS2 titles, as Sony found out that backward compatibility was not a major selling point for the PS3 in contrast to the PS2. The PS3's chief competitor, the Microsoft Xbox 360, took a different approach to backward compatibility by using software emulation in order to run games from the first Xbox.

However, with the current decline in physical game sales and the rise of digital storefronts and downloads, some believe backward compatibility will soon be as obsolete as the phased-out consoles it supports. Many game studios are re-mastering and re-releasing their most popular titles by improving the quality of graphics and adding new content. These remasters have found success by appealing both to nostalgic players who remember enjoying the original versions when they were younger, and to newcomers who may not have had the original system it was released on. For most consumers, digital remasters are more appealing than hanging on to bulky cartridges and obsolete hardware. For the manufacturers of consoles, digital re-releases of classic titles are a large benefit. It not only removes the financial drawbacks of supporting older hardware, but also shifts all of the costs of updating software to the developers. The manufacturer gets a new addition to their system with strong name recognition, and the studio does not have to completely develop a game from the ground up.

A simple example of both backward and forward compatibility is the introduction of FM radio in stereo. FM radio was initially mono, with only one audio channel represented by one signal. With the introduction of two-channel stereo FM radio, many listeners had only mono FM receivers. Forward compatibility for mono receivers with stereo signals was achieved through sending the sum of both left and right audio channels in one signal and the difference in another signal. That allows mono FM receivers to receive and decode the sum signal while ignoring the difference signal, which is necessary only for separating the audio channels. Stereo FM receivers can receive a mono signal and decode it without the need for a second signal, and they can separate a sum signal to left and right channels if both sum and difference signals are received. Without the requirement for backward compatibility, a simpler method could have been chosen.

Full backward compatibility is particularly important in computer instruction set architectures, one of the most successful being the x86 family of microprocessors. Their full backward compatibility spans back to the 16-bit Intel 8086/8088 processors introduced in 1978. (The 8086/8088, in turn, were designed with easy machine-translatability of programs written for its predecessor in mind, although they were not instruction-set compatible with the 8-bit Intel 8080 processor as of 1974. The Zilog Z80, however, was fully backward compatible with the Intel 8080.)
Fully backward compatible processors can process the same binary executable software instructions as their predecessors, allowing the use of a newer processor without having to acquire new applications or operating systems. Similarly, the success of the Wi-Fi digital communication standard is attributed to its broad forward and backward compatibility; it became more popular than other standards that were not backward compatible.

Compiler backward compatibility may refer to the ability of a compiler of a newer version of the language to accept programs or data that worked under the previous version.

A data format is said to be backward compatible with its predecessor if every message or file that is valid under the old format is still valid, retaining its meaning under the new format.


</doc>
<doc id="4460" url="https://en.wikipedia.org/wiki?curid=4460" title="Bacterial conjugation">
Bacterial conjugation

Bacterial conjugation is the transfer of genetic material between bacterial cells by direct cell-to-cell contact or by a bridge-like connection between two cells. This takes place through a pilus.

It is a mechanism of horizontal gene transfer as are transformation and transduction although these two other mechanisms do not involve cell-to-cell contact.

Classical "E. coli" bacterial conjugation is often regarded as the bacterial equivalent of sexual reproduction or mating since it involves the exchange of genetic material. However, it is not sexual reproduction, since no exchange of gamete occurs, and indeed no generation of a new organism: instead an existing organism is transformed. During classical "E. coli" conjugation the "donor" cell provides a conjugative or mobilizable genetic element that is most often a plasmid or transposon. Most conjugative plasmids have systems ensuring that the "recipient" cell does not already contain a similar element.

The genetic information transferred is often beneficial to the recipient. Benefits may include antibiotic resistance, xenobiotic tolerance or the ability to use new metabolites.. Others elements can be detrimental and may be viewed as bacterial parasites.

Conjugation in "Escherichia coli" by spontaneous zygogenesis and in "Mycobacterium smegmatis" by distributive conjugal transfer differ from the more well studied classical "E. coli" conjugation in that these cases involve substantial blending of the parental genomes.

The process was discovered by Joshua Lederberg and Edward Tatum in 1946.

Conjugation diagram


The F-plasmid is an episome (a plasmid that can integrate itself into the bacterial chromosome by homologous recombination) with a length of about 100 kb. It carries its own origin of replication, the "oriV", and an origin of transfer, or "oriT". There can only be one copy of the F-plasmid in a given bacterium, either free or integrated, and bacteria that possess a copy are called "F-positive" or "F-plus" (denoted F). Cells that lack F plasmids are called "F-negative" or "F-minus" (F) and as such can function as recipient cells.

Among other genetic information, the F-plasmid carries a "tra" and "trb" locus, which together are about 33 kb long and consist of about 40 genes. The "tra" locus includes the "pilin" gene and regulatory genes, which together form pili on the cell surface. The locus also includes the genes for the proteins that attach themselves to the surface of F bacteria and initiate conjugation. Though there is some debate on the exact mechanism of conjugation it seems that the pili are not the structures through which DNA exchange occurs. This has been shown in experiments where the pilus are allowed to make contact, but then are denatured with SDS and yet DNA transformation still proceeds. Several proteins coded for in the "tra" or "trb" locus seem to open a channel between the bacteria and it is thought that the traD enzyme, located at the base of the pilus, initiates membrane fusion.

When conjugation is initiated by a signal the relaxase enzyme creates a nick in one of the strands of the conjugative plasmid at the "oriT". Relaxase may work alone or in a complex of over a dozen proteins known collectively as a relaxosome. In the F-plasmid system the relaxase enzyme is called TraI and the relaxosome consists of TraI, TraY, TraM and the integrated host factor IHF. The nicked strand, or "T-strand", is then unwound from the unbroken strand and transferred to the recipient cell in a 5'-terminus to 3'-terminus direction. The remaining strand is replicated either independent of conjugative action (vegetative replication beginning at the "oriV") or in concert with conjugation (conjugative replication similar to the rolling circle replication of lambda phage). Conjugative replication may require a second nick before successful transfer can occur. A recent report claims to have inhibited conjugation with chemicals that mimic an intermediate step of this second nicking event.

If the F-plasmid that is transferred has previously been integrated into the donor's genome (producing an Hfr strain ["High Frequency of Recombination"]) some of the donor's chromosomal DNA may also be transferred with the plasmid DNA. The amount of chromosomal DNA that is transferred depends on how long the two conjugating bacteria remain in contact. In common laboratory strains of "E. coli" the transfer of the entire bacterial chromosome takes about 100 minutes. The transferred DNA can then be integrated into the recipient genome via homologous recombination.

A cell culture that contains in its population cells with non-integrated F-plasmids usually also contains a few cells that have accidentally integrated their plasmids. It is these cells that are responsible for the low-frequency chromosomal gene transfers that occur in such cultures. Some strains of bacteria with an integrated F-plasmid can be isolated and grown in pure culture. Because such strains transfer chromosomal genes very efficiently they are called Hfr (high frequency of recombination). The "E. coli" genome was originally mapped by interrupted mating experiments in which various Hfr cells in the process of conjugation were sheared from recipients after less than 100 minutes (initially using a Waring blender). The genes that were transferred were then investigated.

Since integration of the F-plasmid into the "E. coli" chromosome is a rare spontaneous occurrence, and since the numerous genes promoting DNA transfer are in the plasmid genome rather than in the bacterial genome, it has been argued that conjugative bacterial gene transfer, as it occurs in the "E. coli" Hfr system, is not an evolutionary adaptation of the bacterial host, nor is it likely ancestral to eukaryotic sex.

Spontaneous zygogenesis in "E. coli"

In addition to classical bacterial conjugation described above for "E. coli", a form of conjugation referred to as spontaneous zygogenesis (Z-mating for short) is observed in certain strains of "E. coli". In Z-mating there is complete genetic mixing, and unstable diploids are formed that throw off phenotypically haploid cells, of which some show a parental phenotype and some are true recombinants.

Conjugation in "Mycobacteria smegmatis", like conjugation in "E. coli", requires stable and extended contact between a donor and a recipient strain, is DNase resistant, and the transferred DNA is incorporated into the recipient chromosome by homologous recombination. However, unlike "E. coli" Hfr conjugation, mycobacterial conjugation is chromosome rather than plasmid based. Furthermore, in contrast to "E. coli" Hfr conjugation, in "M. smegmatis" all regions of the chromosome are transferred with comparable efficiencies. The lengths of the donor segments vary widely, but have an average length of 44.2kb. Since a mean of 13 tracts are transferred, the average total of transferred DNA per genome is 575kb. This process is referred to as "Distributive conjugal transfer." Gray et al. found substantial blending of the parental genomes as a result of conjugation and regarded this blending as reminiscent of that seen in the meiotic products of sexual reproduction.

Bacteria related to the nitrogen fixing "Rhizobia" are an interesting case of inter-kingdom conjugation. For example, the tumor-inducing (Ti) plasmid of "Agrobacterium" and the root-tumor inducing (Ri) plasmid of "A. rhizogenes" contain genes that are capable of transferring to plant cells. The expression of these genes effectively transforms the plant cells into opine-producing factories. Opines are used by the bacteria as sources of nitrogen and energy. Infected cells form crown gall or root tumors. The Ti and Ri plasmids are thus endosymbionts of the bacteria, which are in turn endosymbionts (or parasites) of the infected plant.

The Ti and Ri plasmids can also be transferred between bacteria using a system (the "tra", or transfer, operon) that is different and independent of the system used for inter-kingdom transfer (the "vir", or virulence, operon). Such transfers create virulent strains from previously avirulent strains.

Conjugation is a convenient means for transferring genetic material to a variety of targets. In laboratories, successful transfers have been reported from bacteria to yeast, plants, mammalian cells, diatoms and isolated mammalian mitochondria. Conjugation has advantages over other forms of genetic transfer including minimal disruption of the target's cellular envelope and the ability to transfer relatively large amounts of genetic material (see the above discussion of "E. coli" chromosome transfer). In plant engineering, "Agrobacterium"-like conjugation complements other standard vehicles such as tobacco mosaic virus (TMV). While TMV is capable of infecting many plant families these are primarily herbaceous dicots. "Agrobacterium"-like conjugation is also primarily used for dicots, but monocot recipients are not uncommon.




</doc>
<doc id="4461" url="https://en.wikipedia.org/wiki?curid=4461" title="Galjoen">
Galjoen

The galjoen, black bream, or blackfish ("Dichistius capensis") is a species of marine fish found only along the coast of southern Africa from Angola to South Africa. 
Galjoen is the national fish of South Africa.

The galjoen is indigenous to the coasts of southern Africa from Angola to South Africa, and is generally found around reefs at shallow depths around , often near the shore.

This species can reach in total length and a weight of . The body is compressed, and the fins are well developed, with prominent spines, 10 of them, with between 18 and 23 rays. The anal fin has three spines, and usually 13 or 14 rays, the pelvic fins have 1 spine and 5 rays, and the pectoral fins are typically shorter than the head. The body, fins, and head, with the except of the front of the snout, are covered in scales. The lips are thick, with strong curved incisors at the front of the mouth, with smaller teeth behind the front incisors.

The species usually feeds on red and coraline seaweed and red bait, small mussels and barnacles found off rocky shores, and appear in particular to be a partial to the white mussels residing in the sandy beaches and inlets of the rocky outcrops along the southern coast.

In 2005, the movements of the species were extensively studied. Some 25,000 galjoen were tagged at four sites in reserves in South Africa and it was concluded that their overall movement remained localised, with some 95% of fish studied seeming to indicate a funny area.

It is important to local commercial fisheries and is also popular as a game fish.

Due to their abundance in the shores off South Africa, galjoen is common in South African cuisine. A notable dish is the fish is sprinkled with pepper and lemon, or with lemon, mayonnaise and melted garlic butter and served with fresh bread and apricot jam.

Galjoen is the national fish of South Africa. The suggestion to make it the national fish came from Margaret Smith, wife of the ichthyologist J. L. B. Smith, to find a marine equivalent to the Springbok.

The scientific name of "Coracinus capensis" is a reference to its black colour when found in rocky areas, "Coracinus" meaning "raven" or "black coloured"; in sandy areas it gives off a silver-bronze colour.


</doc>
<doc id="4462" url="https://en.wikipedia.org/wiki?curid=4462" title="Blue crane">
Blue crane

The blue crane ("Grus paradisea"), also known as the Stanley crane and the paradise crane, is the national bird of South Africa. The species is listed as Vulnerable by the IUCN.

The blue crane is a tall, ground-dwelling bird, but is fairly small by the standards of the crane family. It is tall, with a wingspan of and weighs . Among standard measurements, the wing chord measures , the exposed culmen measures and the tarsus measures . This crane is pale blue-gray in color becoming darker on the upper head, neck and nape. From the crown to the lores, the plumage is distinctly lighter, sometimes whitish. The bill is ochre to greyish, with a pink tinge. The long wingtip feathers which trail to the ground. The primaries are black to slate grey, with dark coverts and blackish on the secondaries. Unlike most cranes, it has a relatively large head and a proportionately thin neck. Juveniles are similar but slightly lighter, with tawny coloration on the head, and no long wing plumes.

Blue cranes are birds of the dry grassy uplands, usually the pastured grasses of hills, valleys, and plains with a few scattered trees. They prefer areas in the nesting season that have access to both upland and wetland areas, though they feed almost entirely in dry areas. They are altitudinal migrants, generally nesting in the lower grasslands of an elevation of around 1,300 to 2,000 m and moving down to lower altitudes for winter. Though historically found in areas of low human disturbance, the blue crane is currently thriving in the highly transformed agricultural areas of the Western Cape. This is the only portion of its range where the population is increasing, though they still face threats such as poisoning in the region.

Of the 15 species of crane, the blue crane has the most restricted distribution of all. Even species with lower population numbers now (such as Siberian or whooping cranes) are found over a considerable range in their migratory movements. The blue crane is migratory, primarily altitudinal, but details are little known.

The blue crane is partially social, less so during the breeding season. There is a strict hierarchy in groups, with the larger adult males being dominant. They overlap in range with 3 other crane species but interactions with these species and other "large wader" type birds are not known. They are relentlessly aggressive to various other animals during the nesting season, attacking non-predatory species such as cattle, tortoises, plovers and even sparrows. Humans are also attacked if they approach a nest too closely, with the aggressive male having torn clothes and drawn blood in such cases.

Blue cranes feed from the ground and appear to rarely feed near wetland areas. Most of their diet is comprised by grasses and sedges, with many types fed on based on their proximity to the nests. They are also regularly insectivorous, feeding on numerous, sizeable insects such as grasshoppers. Small animals such as crabs, snails, frogs, small lizards and snakes may supplement the diet, with such protein-rich food often being broken down and fed to the young.

The breeding period is highly seasonal, with eggs being recorded between October and March. Pair-formation amongst groups often starts in October, beginning with both potential parents running in circles with each other. The male then engages in a "dance" flings various objects in the air and then jumps. Eventually, a female from the group and the male appear to "select" each other and both engage in the dance of throwing objects and jumping. After the dance, mating commences in around two weeks.

In a great majority of known nests, two eggs are laid (rarely 1 or 3). Both males and females will incubate, with the male often incubating at night and, during the day, defending the nest territory while the female incubates. The incubation stage lasts around 30 days. The young are able to walk after two days and can swim well shortly thereafter. They are fed primarily by their mothers, who regurgitates food into the mouths. The chicks fledge in the age of 3–5 months. The young continue to be tended to until the next breeding season, at which time they are chased off by their parents.

While it remains common in parts of its historic range, and approx. 26 000 individuals remain, it began a sudden population decline from around 1980 and is now classified as vulnerable.

In the last two decades, the blue crane has largely disappeared from the Eastern Cape, Lesotho, and Swaziland. The population in the northern Free State, Limpopo, Gauteng, Mpumalanga and North West Province has declined by up to 90%. The majority of the remaining population is in eastern and southern South Africa, with a small and separate population in the Etosha Pan of northern Namibia. Occasionally, isolated breeding pairs are found in five neighbouring countries.

The primary causes of the sudden decline of the blue crane are human population growth, the conversion of grasslands into commercial tree plantations, and poisoning: deliberate (to protect crops) or accidental (baits intended for other species, and as a side-effect of crop dusting).

The South African government has stepped up legal protection for the blue crane. Other conservation measures are focusing on research, habitat management, education, and recruiting the help of private landowners.

The blue crane is one of the species to which the "Agreement on the Conservation of African-Eurasian Migratory Waterbirds" (AEWA) applies.

The blue crane is culturally significant to the Xhosa people, who call it "indwe" (flag). Traditionally, when a man distinguished himself in battle or otherwise, he was often decorated by a chief with blue crane feathers in a ceremony called "ukundzabela". Men so honoured, who would wear the feathers sticking out of their hair, were known as men of (trouble)—the implication being that if trouble arose, they would reinstate peace and order.

It is also of significance to the Zulu people, whose kings and warriors wore a single or many feathers as a headdress.

Because of the association with warriors and heroism, the Isitwalandwe Medal was created to honour those who had "made an outstanding contribution and sacrifice to the liberation struggle", that is, those who resisted the apartheid regime in South Africa (1949−1991) in various ways. Isitwalandwe means "the one who wears the plumes of the rare bird", or blue crane.

The blue crane is also the national bird of South Africa.

 


</doc>
<doc id="4463" url="https://en.wikipedia.org/wiki?curid=4463" title="Babrak Karmal">
Babrak Karmal

Babrak Karmal (Dari/, Russian: Бабра́к Карма́ль, born Sultan Hussein; 6 January 1929 – 1 or 3 December 1996) was an Afghan politician who was installed as President of Afghanistan by the Soviet Union when they intervened in 1979. Karmal was born in Kamari and educated at Kabul University. When the People's Democratic Party of Afghanistan (PDPA) was formed, Karmal became one of its leading members, having been introduced to Marxism by Mir Akbar Khyber during his imprisonment for activities deemed too radical by the government. He eventually became the leader of the Parcham faction when the PDPA split in 1967, with their ideological nemesis being the Khalq faction. Under Karmal's leadership, the Parchamite PDPA participated in Mohammad Daoud Khan's rise to power in 1973, and his subsequent regime. While relations were good at the beginning, Daoud began a major purge of leftist influence in the mid-1970s. This in turn led to the reformation of the PDPA in 1977, and Karmal played a major role in the 1978 Saur Revolution when the PDPA took power, though in later years he denounced it.

Karmal was appointed Deputy Chairman of the Revolutionary Council, synonymous with vice head of state, in the communist government. The Parchamite faction found itself under significant pressure by the Khalqists soon after taking power. In June 1978, a PDPA Central Committee meeting voted in favor of giving the Khalqist faction exclusive control over PDPA policy. This decision was followed by a failed Parchamite coup, after which Hafizullah Amin, a Khalqist, initiated a purge against the Parchamites. Karmal survived this purge but was exiled to Prague and eventually dismissed from his post. Instead of returning to Kabul, he feared for his life and lived with his family in the forests protected by the Czechoslovak secret police StB. The Afghan secret police KHAD had alledgedly sent members to Czechoslovakia to assassinate Karmal. In late 1979 he was brought to Moscow by the KGB and eventually, in December 1979, the Soviet Union intervened in Afghanistan (with the consent of Amin's government) to stabilize the country. The Soviet troops staged a coup and assassinated Amin. 

Karmal was promoted to Chairman of the Revolutionary Council and Chairman of the Council of Ministers on 27 December 1979. He remained in office until 1981, when he was succeeded by Sultan Ali Keshtmand. Throughout his term, Karmal worked to establish a support base for the PDPA by introducing several reforms. Among these were the "Fundamental Principles of the Democratic Republic of Afghanistan", introducing a general amnesty for those people imprisoned during Nur Mohammad Taraki's and Amin's rule. He also replaced the red Khalqist flag with a more traditional one. These policies failed to increase the PDPA's legitimacy in the eyes of the Afghan people and the mujahideen rebels. Karmal had always been highly critical of his Khalqist predecessors Nur Muhammad Taraki and Amin's ultra-left radicalism.

These policy failures, and the stalemate that ensued after the Soviet intervention, led the Soviet leadership to become highly critical of Karmal's leadership. Under Mikhail Gorbachev, the Soviet Union deposed Karmal in 1986 and replaced him with Mohammad Najibullah. Following his loss of power, he was again exiled, this time to Moscow. It was Anahita Ratebzad who persuaded Najibullah to allow Babrak Karmal to return to Afghanistan in 1991, where Karmal became an associate of Abdul Rashid Dostum and possibly helped remove the Najibullah government from power in 1992. He eventually left Afghanistan again for Moscow. Not long after, in 1996, Karmal died from liver cancer.

Karmal was born Sultan Hussein on 6 January 1929, was the son of Muhammad Hussein Hashem, a Major General in the Afghan Army and former governor of the province of Paktia, and was the second of five siblings. His family was one of the wealthier families in Kabul. His ethnic background was Tajik(kabuli) from his father's side and Ghilzai Pashtun from his mother's side.

Karmal was born in Kamari, a village close to Kabul. He attended Nejat High School, a German-speaking school, and graduated from it in 1948, and applied to enter the Faculty of Law and Political Science of Kabul University. Karmal's application was initially denied admission to Kabul University because of his student political activist and his openly leftist views. He was always a charismatic speaker and became involved in the student union and the Wikh-i-Zalmayan (Awakened Youth Movement), a progressive and leftist organization. He studied at the College of Law and Political Science at Kabul University from 1951 to 1953. In 1953 Karmal was arrested because of his student union activities, but was released three years later in 1956 in an amnesty by Muhammad Daoud Khan. Shortly after, in 1957, Karmal found work as an English and German translator, before quitting and leaving for military training. Karmal graduated from the College of Law and Political Science in 1960, and in 1961, he found work as an employee in the Compilation and Translation Department of the Ministry of Education. From 1961 to 1963 he worked in the Ministry of Planning. When his mother died, Karmal left with his maternal aunt to live somewhere else. His father disowned him because of his leftist views. Karmal was involved in much debauchery, which was controversial in the mostly conservative Afghan society.

Imprisoned from 1953 to 1956, Karmal befriended fellow inmate Mir Akbar Khyber, who introduced Karmal to Marxism. Karmal changed his name from Sultan Hussein to Babrak Karmal, which means "Comrade of the Workers'" in Pashtun, to disassociate himself from his bourgeois background. When he was released from prison, he continued his activities in the student union, and began to promote Marxism. Karmal spent the rest of the 1950s and the early 1960s becoming involved with Marxist organizations, of which there were at least four in Afghanistan at the time; two of the four were established by Karmal. When the 1964 Afghan Provisional Constitution, which legalised the establishment of new political entities, was introduced several prominent Marxists agreed to establish a communist political party. The People's Democratic Party of Afghanistan (PDPA, the Communist Party) was established in January 1965 in Nur Muhammad Taraki's home. Factionalism within the PDPA quickly became a problem; the party split into the Khalq led by Taraki alongside Hafizullah Amin, and the Parcham led by Karmal.

During the 1965 parliamentary election Karmal was one of four PDPA members elected to the lower house of parliament; the three others were Anahita Ratebzad, Nur Ahmed Nur and Fezanul Haq Fezan. No Khalqists were elected; however, Amin was 50 votes short of being elected. The Parchamite victory may be explained by the simple fact that Karmal could contribute financially to the PDPA electoral campaign. Karmal became a leading figure within the student movement in the 1960s, electing Mohammad Hashim Maiwandwal as Prime Minister after a student demonstration (called for by Karmal) concluded with three deaths under the former leadership.

In 1967, the PDPA unofficially split into two formal parties, one Khalqist and one Parchamist. The dissolution of the PDPA was initiated by the closing down of the Khalqist newspaper, "Khalq". Karmal criticised the "Khalq" for being too communist, and believed that its leadership should have hidden its Marxist orientation instead of promoting it. According to the official version of events, the majority of the PDPA Central Committee rejected Karmal's criticism. The vote was a close one, and it is reported that Taraki expanded the Central Committee to win the vote; this plan resulted in eight of the new members becoming politically unaligned with and one switching to the Parchamite side. Karmal and half the PDPA Central Committee left the PDPA to establish a Parchamite-led PDPA. Officially the split was caused by ideological differences, but the party may have divided between the different leadership styles and plans of Taraki versus Karmal. Taraki wanted to model the party after Leninist norms while Karmal wanted to establish a democratic front. Other differences were socioeconomic. The majority of Khalqists came from rural areas; hence they were poorer, and were of Pashtun origin. The Parchamites were urban, richer, and spoke Dari more often than not. The Khalqists accused the Parchamites of having a connection with the monarchy, and because of it, referred to the Parchamite PDPA as the "Royal Communist Party". Both Karmal and Amin retained their seats in the lower house of parliament in the 1969 parliamentary election.

Mohammed Daoud Khan, in collaboration with the Parchamite PDPA and radical military officers, overthrew the monarchy and instituted the Afghan Republic in 1973. After Daoud's seizure of power, an American embassy cable stated that the new government had established a Soviet-style Central Committee, in which Karmal and Mir Akbar Khyber were given leading positions. Most ministries were given to Parchamites; Hassan Sharq became Deputy Prime Minister, Major Faiz Mohammad became Minister of Internal Affairs and Nematullah Pazhwak became Minister of Education. The Parchamites took control over the ministries of finance, agriculture, communications and border affairs. The new government quickly suppressed the opposition, and secured their power base. At first, the National Front government between Daoud and the Parchamites seemed to work. By 1975, Daoud had strengthened his position by enhancing the executive, legislative and judicial powers of the Presidency. To the dismay of the Parchamites, all parties other than the National Revolutionary Party (NRP, established by Daoud) were made illegal.

Shortly after the ban on opposition to the NRP, Daoud began a massive purge of Parchamites in government. Mohammad lost his position as interior minister, Abdul Qadir was demoted, and Karmal was put under government surveillance. To mitigate Daoud's suddenly anti-communist directives, the Soviet Union reestablished the PDPA; Taraki was elected its General Secretary and Karmal, Second Secretary. While the Saur Revolution (literally the April Revolution) was planned for August, the assassination of Khyber led to a chain of events which ended with the communists seizing power. Karmal, when taking power in 1979, accused Amin of ordering the assassination of Khyber.

Taraki was appointed Chairman of the Presidium of the Revolutionary Council and Chairman of the Council of Ministers, retaining his post as PDPA general secretary. Taraki initially formed a government which consisted of both Khalqists and Parchamites; Karmal became Deputy Chairman of the Revolutionary Council, while Amin became Minister of Foreign Affairs and Deputy Chairman of the Council of Ministers.Mohammad Aslam Watanjar became Deputy Chairman of the Council of Ministers. The two Parchamites Abdul Qadir and Mohammad Rafi, became Minister of Defence and Minister of Public Works, respectively. The appointment of Amin, Karmal and Watanjar led to splits within the Council of Ministers: the Khalqists answered to Amin; Karmal led the civilian Parchamites; and the military officers (who were Parchamites) were answerable to Watanjar (a Khalqist). The first conflict arose when the Khalqists wanted to give PDPA Central Committee membership to military officers who had participated in the Saur Revolution; Karmal opposed such a move but was overruled. A PDPA Politburo meeting voted in favour of giving Central Committee membership to the officers.

On 27 June, three months after the Saur Revolution, Amin outmaneuvered the Parchamites at a Central Committee meeting, giving the Khalqists exclusive right over formulating and deciding policy. A purge against the Parchamites was initiated by Amin and supported by Taraki on 1 July 1979. Karmal, fearing for his safety, went into hiding in one of his Soviet friends' homes. Karmal tried to contact Alexander Puzanov, the Soviet ambassador to Afghanistan, to talk about the situation. Puzanov refused, and revealed Karmal's location to Amin. The Soviets probably saved Karmal's life by sending him to the Socialist Republic of Czechoslovakia. In exile, Karmal established a network with the remaining Parchamites in government. A coup to overthrow Amin was planned for 4 September 1979. Its leading members in Afghanistan were Qadir and the Army Chief of Staff General Shahpur Ahmedzai. The coup was planned for the Festival of Eid, in anticipation of relaxed military vigilance. The conspiracy failed when the Afghan ambassador to India told the Afghan leadership about the plan. Another purge was initiated, and Parchamite ambassadors were recalled. Few returned to Afghanistan; Karmal and Mohammad Najibullah stayed in their respective countries. The Soviets decided that Amin should be removed to make way for a Karmal-Taraki coalition government. However Amin managed to order the arrest and later the murder of Taraki.

Amin was informed of the Soviet decision to intervene in Afghanistan and was initially supportive, but was assassinated. Under the command of the Soviets, Karmal ascended to power. On 27 December 1979, Karmal's pre-recorded speech to the Afghan people was broadcast via Radio Kabul from Tashkent in the Uzbek SSR (the radio wavelength was changed to that of Kabul), saying: "Today the torture machine of Amin has been smashed, his accomplices – the primitive executioners, usurpers and murderers of tens of thousand of our fellow countrymen – fathers, mothers, sisters, brothers, sons and daughters, children and old people ..." Karmal was not in Kabul when the speech was broadcast; he was in Bagram, protected by the KGB.

That evening Yuri Andropov, the Chairman of the KGB, congratulated Karmal on his rise to the Chairmanship of the Presidium of the Revolutionary Council, some time before Karmal received an official appointment. Karmal returned to Kabul on 28 December. He travelled alongside a Soviet military column. For the next few days Karmal lived in a villa on the outskirts of Kabul under the protection of the KGB. On 1 January 1980 Leonid Brezhnev, the General Secretary of the Central Committee of the Communist Party of the Soviet Union, and Alexei Kosygin, the Soviet Chairman of the Council of Ministers, congratulated Karmal on his "election" as leader.

When he came to power, Karmal promised an end to executions, the establishment of democratic institutions and free elections, the creation of a constitution, and legalization of alternative political parties. Prisoners incarcerated under the two previous governments would be freed in a general amnesty (which occurred on 6 January). He promised the creation of a coalition government which would not espouse socialism. At the same time, he told the Afghan people that he had negotiated with the Soviet Union to give economic, military and political assistance. The mistrust most Afghans felt towards the government was a problem for Karmal. Many still remembered he had said he would protect private capital in 1978—a promise later proven to be a lie.

Karmal's three most important promises were the general amnesty of prisoners, the promulgation of the Fundamental Principles of the Democratic Republic of Afghanistan and the adoption of a new flag containing the traditional black, red and green (the flag of Taraki and Amin was red). His government granted concessions to religious leaders and the restoration of confiscated property. Some property, which was confiscated during earlier land reforms, was also partially restored. All these measures, with the exception of the general amnesty of prisoners, were introduced gradually. Of 2,700 prisoners, 2,600 were released from prison; 600 of these were Parchamites. The general amnesty was greatly publicized by the government. While the event was hailed with enthusiasm by some, many others greeted the event with disdain, since their loved ones or associates had died during earlier purges. Amin had planned to introduce a general amnesty on 1 January 1980, to coincide with the PDPA's sixteenth anniversary.

Work on the Fundamental Principles had started under Amin: it guaranteed democratic rights such as freedom of speech, the right to security and life, the right to peaceful association, the right to demonstrate and the right that "no one would be accused of crime but in accord with the provisions of law" and that the accused had the right to a fair trial. The Fundamental Principles envisaged a democratic state led by the PDPA, the only party then permitted by law. The Revolutionary Council, the organ of supreme power, would convene twice every year. The Revolutionary Council in turn elected a Presidium which would take decisions on behalf of the Revolutionary Council when it was not in session. The Presidium consisted mostly of PDPA Politburo members. The state would safeguard three kinds of property: state, cooperative and private property. The Fundamental Principles said that the state had the right to change the Afghan economy from an economy where man was exploited to an economy were man was free. Another clause stated that the state had the right to take "families, both parents and children, under its supervision." While it looked democratic at the outset, the Fundamental Principles was based on contradictions.

The Fundamental Principles led to the establishment of two important state organs: the Special Revolutionary Court, a specialized court for crimes against national security and territorial integrity, and the Institute for Legal and Scientific Research and Legislative Affairs, the supreme legislative organ of state, This body could amend and draft laws, and introduce regulations and decrees on behalf of the government. The introduction of more Soviet-style institutions led the Afghan people to distrust the communist government even more.

The Fundamental Principles constitution came into power on 22 April 1980.

With Karmal's ascension to power, Parchamites began to "settle old scores". Revolutionary Troikas were created to arrest, sentence and execute people. Amin's guard were the first victims of the terror which ensued. Those commanders who had stayed loyal to Amin were arrested, filling the prisons. The Soviets protested, and Karmal replied, "As long as you keep my hands bound and do not let me deal with the Khalq faction there will be no unity in the PDPA and the government cannot become strong ... They tortured and killed us. They still hate us! They are the enemies of the party ..." Amin's daughter, along with her baby, was imprisoned for twelve years, until Mohammad Najibullah, then leader of the PDPA, released her. When Karmal took power, leading posts in the Party and Government bureaucracy were taken over by Parchamites. The Khalq faction was removed from power, and only technocrats, opportunists and individuals which the Soviets trusted would be appointed to the higher echelons of government. Khalqists remained in control of the Ministry of Interior, but Parchamites were given control over KHAD and the secret police. The Parchamites and the Khalqists controlled an equal share of the military. Two out of Karmal's three Council of Ministers deputy chairmen were Khalqists. Khalqists controlled the Ministry of Communications and the interior ministry. Parchamites, on the other hand, controlled the Ministry of Foreign Affairs and the Ministry of Defence. In addition to the changes in government, the Parchamites held clear majority in the PDPA Central Committee. Only one Khalqi, Saleh Mohammad Zeary, was a member of the PDPA Secretariat during Karmal's rule.

Over 14 and 15 March 1982 the PDPA held a party "conference" at the Kabul Polytechnic Institute instead of a party "congress", since a party congress would have given the Khalq faction a majority and could have led to a Khalqist takeover of the PDPA. The rules of holding a party conference were different, and the Parchamites had a three-fifths majority. This infuriated several Khalqists; the threat of expulsion did not lessen their anger. The conference was not successful, but it was portrayed as such by the official media. The conference broke up after one and a half days of a 3-day long program, because of the inter-party struggle for power between the Khalqists and the Parchamites. A "program of action" was introduced, and party rules were given minor changes. As an explanation of the low party membership, the official media also made it seem hard to become a member of the party.

When Karmal took power, he began expanding the support base of the PDPA. Karmal tried to persuade certain groups, which had been referred to class enemies of the revolution during Taraki and Amin's rule, to support the PDPA. Karmal appointed several non-communists to top positions. Between March and May 1980, 78 out of the 191 people appointed to government posts were not members of the PDPA. Karmal reintroduced the old Afghan custom of having an Islamic invocation every time the government issued a proclamation. In his first live speech to the Afghan people, Karmal called for the establishment of the National Fatherland Front (NFF); the NFF's founding congress was held in June 1981. Unfortunately for Karmal, his policies did not lead to a notable increase in support for his regime, and it did not help Karmal that most Afghans saw the Soviet intervention as an invasion.

By 1981, the government gave up on political solutions to the conflict. At the fifth PDPA Central Committee plenum in June, Karmal resigned from his Council of Ministers chairmanship and was replaced by Sultan Ali Keshtmand, while Nur Ahmad Nur was given a bigger role in the Revolutionary Council. This was seen as "base broadening". The previous weight given to non-PDPA members in top positions ceased to be an important matter in the media by June 1981. This was significant, considering that up to five members of the Revolutionary Council were non-PDPA members. By the end of 1981, the previous contenders, who had been heavily presented in the media, were all gone; two were given ambassadorships, two ceased to be active in politics, and one continued as an advisor to the government. The other three changed sides, and began to work for the opposition.

The national policy of reconciliation continued: in January 1984 the land reform introduced by Taraki and Amin was drastically modified, the limits of landholdings were increased to win the support of middle class peasants, the literacy programme was continued, and concessions to women were made. In 1985 the Loya Jirga was reconvened. The 1985 Loya Jirga was followed by a tribal jirga in September. In 1986 Abdul Rahim Hatef, a non-PDPA member, was elected to the NFF chairmanship. During the 1985–86 elections it was said that 60 percent of the elected officials were non-PDPA members. By the end of Karmal's rule, several non-PDPA members had high-level government positions.

In March 1979, the military budget was 6.4 million US$, which was 8.3 percent of the government budget, but only 2.2 of gross national product. After the Soviet intervention, the defence budget increased to 208 million US$ in 1980, and 325 million US$ by 1981. In 1982 it was reported that the government spent around 22 percent of total expenditure.

When the political solution failed (see "PDPA base" section), the Afghan government and the Soviet military decided to solve the conflict militarily. The change from a political to a military solution did not come suddenly. It began in January 1981, as Karmal doubled wages for military personnel, issued several promotions, and decorated one general and thirteen colonels. The draft age was lowered, the obligatory length of arms duty was extended and the age for reservists was increased to thirty-five years of age. In June 1981, Assadullah Sarwari lost his seat in the PDPA Politburo, replaced by Mohammad Aslam Watanjar, a former tank commander and Minister of Communications, Major General Mohammad Rafi was madeMinister of Defence and Mohammad Najibullah appointed KHAD Chairman.

These measures were introduced due to the collapse of the army during the Soviet intervention. Before the invasion the army could field 100,000 troops, after the invasion only 25,000. Desertions were pandemic, and the recruitment campaigns for young people often drove them to the opposition. To better organize the military, seven military zones were established, each with its own Defence Council. The Defence Councils were established at the national, provincial and district level to empower the local PDPA. It is estimated that the Afghan government spent as much as 40 percent of government revenue on defense.

During the civil war and the ensuing Soviet–Afghan War, most of the country's infrastructure was destroyed. Normal patterns of economic activity were disrupted. The Gross national product (GNP) fell substantially during Karmal's rule because of the conflict; trade and transport was disrupted with loss of labor and capital. In 1981 the Afghan GDP stood at 154.3 billion Afghan afghanis, a drop from 159.7 billion in 1978. GNP per capita decreased from 7,370 in 1978 to 6,852 in 1981. The dominant form of economic activity was in the agricultural sector. Agriculture accounted for 63 percent of gross domestic product (GDP) in 1981; 56 percent of the labor force was working in agriculture in 1982. Industry accounted for 21 percent of GDP in 1982, and employed 10 percent of the labor force. All industrial enterprises were government-owned. The service sector, the smallest of the three, accounted for 10 percent of GDP in 1981, and employed an estimated one-third of the labour force. The balance of payments, which had grown in the pre-communist administration of Muhammad Daoud Khan, decreased, turning negative by 1982 at 70.3 million $US. The only economic activity which grew substantially during Karmal's rule was export and import.

Karmal observed in early 1983 that without Soviet intervention, "It is unknown what the destiny of the Afghan Revolution would be ... We are realists and we clearly realize that in store for us yet lie trials and deprivations, losses and difficulties." Two weeks before this statement Sultan Ali Keshtmand, the Chairman of the Council of Ministers, lamented the fact that half the schools and three-quarters of communications had been destroyed since 1979. The Soviet Union rejected several Western-made peace plans, such as the Carrington Plan, since they did not take into consideration the PDPA government. Most Western peace plans had been made in collaboration with the Afghan opposition forces. At the 26th Congress of the Communist Party of the Soviet Union (CPSU) Leonid Brezhnev, the General Secretary of the CPSU Central Committee, stated;
The stance of the Pakistani government was clear, demanding complete Soviet withdrawal from Afghanistan and the establishment of a non-PDPA government. Karmal, summarizing his discussions with Iran and Pakistan, said "Iran and Pakistan have so far not opted for concrete and constructive positions." During Karmal's rule Afghan–Pakistani relations remained hostile; the Soviet intervention in Afghanistan was the catalyst for the hostile relationship. The increasing numbers of Afghan refugees in Pakistan challenged the PDPA's legitimacy to rule.

The Soviet Union threatened in 1985 that it would support the Baloch separatist movement in Pakistan if the Pakistani government continued to aid the mujahideen in Afghanistan. Karmal, problematically for the Soviets, did not want a Soviet withdrawal, and he hampered attempts to improve relations with Pakistan since the Pakistani government had refused to recognise the PDPA government.

Mikhail Gorbachev, then General Secretary of the Central Committee of the Communist Party of the Soviet Union, said, "The main reason that there has been no national consolidation so far is that Comrade Karmal is hoping to continue sitting in Kabul with our help." Karmal's position became less secure when the Soviet leadership began blaming him for the failures in Afghanistan. Gorbachev, worried over the situation, told the Soviet Politburo "If we don't change approaches [to evacuate Afghanistan], we will be fighting there for another 20 or 30 years." It is not clear when the Soviet leadership began to campaign for Karmal's dismissal, but Andrei Gromyko discussed the possibility of Karmal's resignation with Javier Pérez de Cuéllar, the Secretary-General of the United Nations in 1982. While it was Gorbachev who would dismiss Karmal, there may have been a consensus within the Soviet leadership in 1983 that Karmal should resign. Gorbachev's own plan was to replace Karmal with Mohammad Najibullah, who had joined the PDPA at its creation. Najibullah was thought highly of by Yuri Andropov, Boris Ponomarev and Dmitriy Ustinov, and negotiations for his succession may have started in 1983. Najibullah was not the Soviet leadership's only choice for Karmal's succession; a GRU report noted that the majority of the PDPA leadership would support Assadullah Sarwari's ascension to leadership. According to the GRU, Sarwari was a better candidate as he could balance between the Pashtuns, Tajiks and Uzbeks; Najibullah was a Pashtun nationalist. Another viable candidate was Abdul Qadir, who had been a participant in the Saur Revolution.

Najibullah was appointed to the PDPA Secretariat in November 1985. During Karmal's March 1986 visit to the Soviet Union, the Soviets tried to persuade Karmal that he was too ill to govern, and that he should resign. This backfired, as a Soviet doctor attending to Karmal told him he was in good health. Karmal asked to return home to Kabul, and said that he understood and would listen to the Soviet recommendations. Before leaving, Karmal promised he would step down as PDPA General Secretary. The Soviets did not trust him and sent Vladimir Kryuchkov, the head of intelligence (FCD) in the KGB, into Afghanistan. At a meeting in Kabul, Karmal confessed his undying love for the Soviet Union, comparing his ardor to his Muslim faith. Kryuchkov, concluding that he could not persuade Karmal to resign, left the meeting. After Kryuchkov left the room, the Afghan defence minister and the state security minister visited Karmal's office, telling him that he had to resign from one of his posts. Understanding that his Soviet support had been eliminated, Karmal resigned from the office of the General Secretary at the 18th PDPA Central Committee plenum. He was succeeded in his post by Najibullah.

Karmal still had support within the party, and used his base to curb Najibullah's powers. He began spreading rumors that he would be reappointed General Secretary. Najibullah's power base was in the KHAD, the Afghan equivalent to the KGB, and not the party. Considering the fact that the Soviet Union had supported Karmal for over six years, the Soviet leadership wanted to ease him out of power gradually. Yuli Vorontsov, the Soviet ambassador to Afghanistan, told Najibullah to begin undermining Karmal's power slowly. Najibullah complained to the Soviet leadership that Karmal used most of his spare time looking for errors and "speaking against the National Reconciliation [programme]". At a meeting of the Soviet Politburo on 13 November 1986 it was decided that Najibullah should remove Karmal; this motion was supported by Gromyko, Vorontsov, Eduard Shevardnadze, Anatoly Dobrynin and Viktor Chebrikov. A PDPA meeting in November relieved Karmal of his Revolutionary Council chairmanship, and exiled him to Moscow where he was given a state-owned apartment and a dacha. Karmal was succeeded as Revolutionary Council chairman by Haji Mohammad Chamkani, who was not a member of the PDPA.

Many years after the end of his Presidency, he denounced the Saur Revolution of 1978 in which he took part, taking aim at the Khalq governments of Taraki and Amin. He told a Russian reporter:

It was the greatest crime against the people of Afghanistan. Parcham's leaders were against armed actions because the country was not ready for a revolution... I knew that people would not support us if we decided to keep power without such support.

For unknown reasons, Karmal was invited back to Kabul by Najibullah, and "for equally obscure reasons Karmal accepted", returning on 20 June 1991. (This could have been on the recommendation of Anahita Ratebzad who was very close to Karmal & also respected by Najib & generally respected by great part of Left movement in Afghanestan.) If Najibullah's plan was to strengthen his position within the Watan Party (the renamed PDPA) by appeasing the pro-Karmal Parchamites, he failed – Karmal's apartment became a center for opposition to Najibullah's government. When Najibullah was toppled in 1992, Karmal became the most powerful politician in Kabul through leadership of the Parcham. However, his negotiations with the rebels collapsed quickly, and on 16 April 1992 the rebels, led by Gulbuddin Hekmatyar, took Kabul. After the fall of Najibullah's government, Karmal was based in Hairatan. There, it is alleged, Karmal used most of his time either trying to establish a new party, or advising people to join the secular National Islamic Movement ("Junbish-i-Milli"). Abdul Rashid Dostum, the leader of Junbish-i-Milli, was a supporter of Karmal during his rule. It is unknown how much control Karmal had over Dostum, but there is little evidence that Karmal was in any commanding position. Karmal's influence over Dostum appeared indirect – some of his former associates supported Dostum. Those who spoke with Karmal during this period noted his lack of interest in politics. In June 1992 it was reported that he had died in a plane crash along with Dostum, although these reports later proved to be false.

In early December 1996, Karmal died in Moscow's Central Clinical Hospital from liver cancer. The date of his death was reported by some sources as 1 December and by others as 3 December. The Taliban summed up his rule as follows:
[he] committed all kinds of crimes during his illegitimate rule ... God inflicted on him various kinds of hardship and pain. Eventually he died of cancer in a hospital belonging to his paymasters, the Russians.



</doc>
<doc id="4468" url="https://en.wikipedia.org/wiki?curid=4468" title="Buddhist philosophy">
Buddhist philosophy

Buddhist philosophy refers to the philosophical investigations and systems of inquiry that developed among various Buddhist schools in India following the parinirvana (i.e. death) of the Buddha and later spread throughout Asia. The Buddhist path combines both philosophical reasoning and meditation. The Buddhist traditions present a multitude of Buddhist paths to liberation, and Buddhist thinkers in India and subsequently in East Asia have covered topics as varied as phenomenology, ethics, ontology, epistemology, logic and philosophy of time in their analysis of these paths.

Early Buddhism was based on empirical evidence gained by the sense organs ("ayatana") and the Buddha seems to have retained a skeptical distance from certain metaphysical questions, refusing to answer them because they were not conducive to liberation but led instead to further speculation. A recurrent theme in Buddhist philosophy has been the reification of concepts, and the subsequent return to the Buddhist Middle Way.

Particular points of Buddhist philosophy have often been the subject of disputes between different schools of Buddhism. These elaborations and disputes gave rise to various schools in early Buddhism of Abhidharma, and to the Mahayana traditions such as Prajnaparamita, Madhyamaka, Buddha-nature and Yogācāra.

Edward Conze splits the development of Indian Buddhist philosophy into three phases:


Various elements of these three phases are incorporated and/or further developed in the philosophy and world view of the various sects of Buddhism that then emerged.

Philosophy in India was aimed mainly at spiritual liberation and had soteriological goals. In his study of Mādhyamaka Buddhist philosophy in India, Peter Deller Santina writes:

For the Indian Buddhist philosophers, the teachings of the Buddha were not meant to be taken on faith alone, but to be confirmed by logical analysis ("pramana") of the world. The early Buddhist texts mention that a person becomes a follower of the Buddha's teachings after having pondered them over with wisdom and the gradual training also requires that a disciple "investigate" ("upaparikkhati") and "scrutinize" ("tuleti") the teachings. The Buddha also expected his disciples to approach him as a teacher in a critical fashion and scrutinize his actions and words, as shown in the "Vīmaṃsaka Sutta."

Scholarly opinion varies as to whether the Buddha himself was engaged in philosophical inquiry. The Buddha (c. 5th century BCE) was a north Indian sramana (wandering ascetic) from Magadha. He cultivated various yogic techniques and ascetic practices and taught throughout north India, where his teachings took hold. These teachings are preserved in the Pali Nikayas and in the Agamas as well as in other surviving fragmentary textual collections (collectively known as the Early Buddhist Texts). Dating these texts is difficult, and there is disagreement on how much of this material goes back to a single religious founder. While the focus of the Buddha's teachings are about attaining the highest good of nirvana, they also contain an analysis of the source of human suffering, the nature of personal identity, and the process of acquiring knowledge about the world.

The Buddha defined his teaching as "the middle way" (Pali: "Majjhimāpaṭipadā"). In the "Dhammacakkappavattana Sutta", this is used to refer to the fact that his teachings steer a middle course between the extremes of asceticism and bodily denial (as practiced by the Jains and other ascetic groups) and sensual hedonism or indulgence. Many sramanas of the Buddha's time placed much emphasis on a denial of the body, using practices such as fasting, to liberate the mind from the body. The Buddha, however, realized that the mind was embodied and causally dependent on the body, and therefore that a malnourished body did not allow the mind to be trained and developed. Thus, Buddhism's main concern is not with luxury or poverty, but instead with the human response to circumstances.

Certain basic teachings appear in many places throughout these early texts, so older studies by various scholars conclude that the Buddha must at least have taught some of these key teachings:

According to N. Ross Reat, all of these doctrines are shared by the Theravada Pali texts and the Mahasamghika school's "Śālistamba Sūtra". A recent study by Bhikkhu Analayo concludes that the Theravada "Majjhima Nikaya" and Sarvastivada "Madhyama Agama" contain mostly the same major doctrines. Richard Salomon, in his study of the Gandharan texts (which are the earliest manuscripts containing early discourses), has confirmed that their teachings are "consistent with non-Mahayana Buddhism, which survives today in the Theravada school of Sri Lanka and Southeast Asia, but which in ancient times was represented by eighteen separate schools."

However, some scholars such as Schmithausen, Vetter, and Bronkhorst argue that critical analysis reveals discrepancies among these various doctrines. They present alternative possibilities for what was taught in early Buddhism and question the authenticity of certain teachings and doctrines.

For example, some scholars think that karma was not central to the teaching of the historical Buddha, while other disagree with this position. Likewise, there is scholarly disagreement on whether insight was seen as liberating in early Buddhism or whether it was a later addition to the practice of the four "dhyāna." According to Vetter and Bronkhorst, "dhyāna" constituted the original "liberating practice", while discriminating insight into transiency as a separate path to liberation was a later development."" Scholars such as Bronkhorst and Carol Anderson also think that the four noble truths may not have been formulated in earliest Buddhism but as Anderson writes "emerged as a central teaching in a slightly later period that still preceded the final redactions of the various Buddhist canons."

According to some scholars, the philosophical outlook of earliest Buddhism was primarily negative, in the sense that it focused on what doctrines to "reject" more than on what doctrines to "accept". Only knowledge that is useful in achieving enlightenment is valued. According to this theory, the cycle of philosophical upheavals that in part drove the diversification of Buddhism into its many schools and sects only began once Buddhists began attempting to make explicit the implicit philosophy of the Buddha and the early texts.

The four noble truths or "truths of the noble one" are a central feature of the teachings and are put forth in the "Dhammacakkappavattana Sutta". The first truth of dukkha, often translated as "suffering", is the inherent unsatisfactoriness of life. This unpleasantness is said to be not just physical pain, but also a kind of existential unease caused by the inevitable facts of our mortality and ultimately by the impermanence of all phenomena. It also arises because of contact with unpleasant events, and due to not getting what one desires. The second truth is that this unease arises out of conditions, mainly 'craving' (tanha) and ignorance (avidya). The third truth is then the fact that if you let go of craving and remove ignorance through knowledge, dukkha ceases (nirodha). The fourth is the eightfold path which are eight practices that end suffering. They are: right view, right intention, right speech, right action, right livelihood, right effort, right mindfulness and right samadhi (mental unification, meditation). The goal taught by the Buddha, nirvana, literally means 'extinguishing' and signified "the complete extinguishing of greed, hatred, and delusion (i.e. ignorance), the forces which power "samsara". Nirvana also means that after an enlightened being's death, there is no further rebirth. In early Buddhism, the concept of dependent origination was most likely limited to processes of mental conditioning and not to all physical phenomena. The Buddha understood the world in procedural terms, not in terms of things or substances. His theory posits a flux of events arising under certain conditions which are interconnected and dependent, such that the processes in question at no time are considered to be static or independent. Craving, for example, is always dependent on, and caused by sensations. Sensations are always dependent on contact with our surroundings. Buddha's causal theory is simply descriptive: "This existing, that exists; this arising, that arises; this not existing, that does not exist; this ceasing, that ceases." This understanding of causation as "impersonal lawlike causal ordering" is important because it shows how the processes that give rise to suffering work, and also how they can be reversed.

The removal of suffering, then, requires a deep understanding of the nature of reality (prajña). While philosophical analysis of arguments and concepts is clearly necessary to develop this understanding, it is not enough to remove our unskillful mental habits and deeply ingrained prejudices, which require meditation, paired with understanding. According to the Buddha of the early texts, we need to train the mind in meditation to be able to truly see the nature of reality, which is said to have the marks of suffering, impermanence and not-self. Understanding and meditation are said to work together to 'clearly see' (vipassana) the nature of human experience and this is said to lead to liberation.

The Buddha argued that compounded entities lacked essence, correspondingly the self is without essence. This means there is no part of a person which is unchanging and essential for continuity, and it means that there is no individual "part of the person that accounts for the identity of that person over time". This is in opposition to the Upanishadic concept of an unchanging ultimate self (Atman) and any view of an eternal soul. The Buddha held that attachment to the appearance of a permanent self in this world of change is the cause of suffering, and the main obstacle to liberation.

The most widely used argument that the Buddha employed against the idea of an unchanging ego is an empiricist one, based on the observation of the five aggregates that make up a person and the fact that these are always changing. This argument can be put in this way:


This argument requires the implied premise that the five aggregates are an exhaustive account of what makes up a person, or else the self could exist outside of these aggregates. This premise is affirmed in other suttas, such as SN 22.47 which states: "whatever ascetics and brahmins regard various kinds of things as self, all regard the five grasping aggregates, or one of them."

This argument is famously expounded in the "Anattalakkhana Sutta". According to this text, the apparently fixed self is merely the result of identification with the temporary aggregates, the changing processes making up an individual human being. In this view a 'person' is only a convenient nominal designation on a certain grouping of processes and characteristics, and an 'individual' is a conceptual construction overlaid upon a stream of experiences just like a chariot is merely a conventional designation for the parts of a chariot and how they are put together. The foundation of this argument is empiricist, for it is based on the fact that all we observe is subject to change, especially everything observed when looking inwardly in meditation.

Another argument for 'non-self', the 'argument from lack of control', is based on the fact that we often seek to change certain parts of ourselves, that the 'executive function' of the mind is that which finds certain things unsatisfactory and attempts to alter them. Furthermore, it is also based on the Indian 'Anti Reflexivity Principle' which states an entity cannot operate on or control itself (a knife can cut other things but not itself, a finger can point at other things but not at itself, etc.). This means then, that the self could never desire to change itself and could not do so (another reason for this is that in most Indian traditions besides Buddhism, the true self or Atman is perfectly blissful and does not suffer). The Buddha uses this idea to attack the concept of self. This argument could be structured thus:


This argument then denies that there is one permanent "controller" in the person. Instead it views the person as a set of constantly changing processes which include volitional events seeking change and an awareness of that desire for change. According to Mark Siderits:"What the Buddhist has in mind is that on one occasion one part of the person might perform the executive function, on another occasion another part might do so. This would make it possible for every part to be subject to control without there being any part that always fills the role of controller (and so is the self). On some occasions a given part might fall on the controller side, while on other occasions it might fall on the side of the controlled. This would explain how it's possible for us to seek to change any of the skandhas while there is nothing more to us than just those skandhas."As noted by K.R. Norman and Richard Gombrich, the Buddha extended his anatta critique to the Brahmanical belief expounded in the "Brihadaranyaka Upanishad" that the Self (Atman) was indeed the whole world, or Brahman. This is shown by the "Alagaddupama Sutta", where the Buddha argues that an individual cannot experience the suffering of the entire world. He used the example of someone carrying off and burning grass and sticks from the Jeta grove and how a monk would not sense or consider themselves harmed by that action. In this example the Buddha is arguing that we do not have direct experience of the entire world, and hence the Self cannot be the whole world. In this sutta (as well as in the "Soattā Sutta") the Buddha outlines six wrong views about Self:

"There are six wrong views: An unwise, untrained person may think of the body, 'This is mine, this is me, this is my self'; he may think that of feelings; of perceptions; of volitions; or of what has been seen, heard, thought, cognized, reached, sought or considered by the mind. The sixth is to identify the world and self, to believe: 'At death I shall become permanent, eternal, unchanging, and so remain forever the same; and that is mine, that is me, that is my self.' A wise and well-trained person sees that all these positions are wrong, and so he is not worried about something that does not exist."

Furthermore, the Buddha argues that the world can be observed to be a cause of suffering (Brahman was held to be ultimately blissful) and that since we cannot control the world as we wish, the world cannot be the Self. The idea that "this cosmos is the self" is one of the views rejected by the Buddha along with the related Monistic theory that held that "everything is a Oneness" (SN 12.48 "Lokayatika Sutta"). The Buddha also held that understanding and seeing the truth of not-self led to un-attachment, and hence to the cessation of suffering, while ignorance about the true nature of personality led to further suffering.

All schools of Indian philosophy recognize various sets of valid justifications for knowledge, or "pramana" and many see the Vedas as providing access to truth. The Buddha denied the authority of the Vedas, though, like his contemporaries, he affirmed the soteriological importance of having a proper understanding of reality (right view). However, this understanding was not conceived primarily as metaphysical and cosmological knowledge, but as a knowledge into the arising and cessation of suffering in human experience. Therefore, the Buddha's epistemic project is different than that of modern philosophy; it is primarily a solution to the fundamental human spiritual/existential problem.

The Buddha's epistemology has been compared to empiricism, in the sense that it was based on experience of the world through the senses. The Buddha taught that empirical observation through the six sense fields (ayatanas) was the proper way of verifying any knowledge claims. Some suttas go further, stating that "the All", or everything that exists ("sabbam"), are these six sense spheres (SN 35.23, Sabba Sutta) and that anyone who attempts to describe another "All" will be unable to do so because "it lies beyond range". This sutta seems to indicate that for the Buddha, things in themselves or noumena, are beyond our epistemological reach ("avisaya").

Furthermore, in the Kalama Sutta the Buddha tells a group of confused villagers that the only proper reason for one's beliefs is verification in one's own personal experience (and the experience of the wise) and denies any verification which stems from personal authority, sacred tradition ("anussava") or any kind of rationalism which constructs metaphysical theories ("takka"). In the Tevijja Sutta (DN 13), the Buddha rejects the personal authority of Brahmins because none of them can prove they have had personal experience of Brahman. The Buddha also stressed that experience is the only criterion for verification of the truth in this passage from the Majjhima Nikaya (MN.I.265):

Furthermore, the Buddha's standard for personal verification was a pragmatic and salvific one, for the Buddha a belief counts as truth only if it leads to successful Buddhist practice (and hence, to the destruction of craving). In the "Discourse to Prince Abhaya" (MN.I.392–4) the Buddha states this pragmatic maxim by saying that a belief should only be accepted if it leads to wholesome consequences. This tendency of the Buddha to see what is true as what was useful or 'what works' has been called by scholars such as Mrs Rhys Davids and Vallée-Poussin a form of Pragmatism. However, K. N. Jayatilleke argues the Buddha's epistemology can also be taken to be a form of correspondence theory (as per the 'Apannaka Sutta') with elements of Coherentism and that for the Buddha, it is causally impossible for something which is false to lead to cessation of suffering and evil.

The Buddha discouraged his followers from indulging in intellectual disputation for its own sake, which is fruitless, and distracts one from the goal of awakening. Only philosophy and discussion which has pragmatic value for liberation from suffering is seen as important. According to the scriptures, during his lifetime the Buddha remained silent when asked several metaphysical questions which he regarded as the basis for "unwise reflection". These 'unanswered questions' (avyākata) regarded issues such as whether the universe is eternal or non-eternal (or whether it is finite or infinite), the unity or separation of the body and the self, the complete inexistence of a person after Nirvana and death, and others. The Buddha stated that thinking about these imponderable (Acinteyya) issues led to "a thicket of views, a wilderness of views, a contortion of views, a writhing of views, a fetter of views" (Aggi-Vacchagotta Sutta).

One explanation for this pragmatic suspension of judgment or epistemic Epoché is that such questions distract from activity that is practical to realizing enlightenment and bring about the danger of substituting the experience of liberation by conceptual understanding of the doctrine or by religious faith. According to the Buddha, the Dharma is not an ultimate end in itself or an explanation of all metaphysical reality, but a pragmatic set of teachings. The Buddha used two parables to clarify this point, the 'Parable of the raft' and the Parable of the Poisoned Arrow. The Dharma is like a raft in the sense that it is only a pragmatic tool for attaining nirvana ("for the purpose of crossing over, not for the purpose of holding onto", MN 22); once one has done this, one can discard the raft. It is also like medicine, in that the particulars of how one was injured by a poisoned arrow (i.e. metaphysics, etc.) do not matter in the act of removing and curing the arrow wound itself (removing suffering). In this sense, the Buddha was often called 'the great physician' because his goal was to cure the human condition of suffering first and foremost, not to speculate about metaphysics.

Having said this, it is still clear that resisting (even refuting) a false or slanted doctrine can be useful to extricate the interlocutor, or oneself, from error; hence, to advance in the way of liberation. Witness the Buddha's confutation of several doctrines by Nigantha Nataputta and other purported sages which sometimes had large followings (e.g., Kula Sutta, Sankha Sutta, Brahmana Sutta). This shows that a virtuous and appropriate use of dialectics can take place. By implication, reasoning and argument shouldn't be disparaged by Buddhists.

After the Buddha's death, some Buddhists such as Dharmakirti went on to use the sayings of the Buddha as sound evidence equal to perception and inference.

Another possible reason why the Buddha refused to engage in metaphysics is that he saw ultimate reality and nirvana as devoid of sensory mediation and conception and therefore language itself is "a priori" inadequate to explain it. Thus, the Buddha's silence does not indicate misology or disdain for philosophy. Rather, it indicates that he viewed the answers to these questions as not understandable by the unenlightened. Dependent arising provides a framework for analysis of reality that is not based on metaphysical assumptions regarding existence or non-existence, but instead on direct cognition of phenomena as they are presented to the mind in meditation.

The Buddha of the earliest Buddhists texts describes Dharma (in the sense of "truth") as "beyond reasoning" or "transcending logic", in the sense that reasoning is a subjectively introduced aspect of the way unenlightened humans perceive things, and the conceptual framework which underpins their cognitive process, rather than a feature of things as they really are. Going "beyond reasoning" means in this context penetrating the nature of reasoning from the inside, and removing the causes for experiencing any future stress as a result of it, rather than functioning outside the system as a whole.

The Buddha's ethics are based on the soteriological need to eliminate suffering and on the premise of the law of karma. Buddhist ethics have been termed eudaimonic (with their goal being well-being) and also compared to virtue ethics (this approach began with Damien Keown). Keown writes that Buddhist Nirvana is analogous to the Aristotelian Eudaimonia, and that Buddhist moral acts and virtues derive their value from how they lead us to or act as an aspect of the nirvanic life.

The Buddha outlined five precepts (no killing, stealing, sexual misconduct, lying, or drinking alcohol) which were to be followed by his disciples, lay and monastic. There are various reasons the Buddha gave as to why someone should be ethical.

First, the universe is structured in such a way that if someone intentionally commits a misdeed, a bad karmic fruit will be the result (and vice versa). Hence, from a pragmatic point of view, it is best to abstain from these negative actions which bring forth negative results. However, the important word here is "intentionally": for the Buddha, karma is nothing else but intention/volition, and hence unintentionally harming someone does not create bad karmic results. Unlike the Jains who believed that karma was a quasi-physical element, for the Buddha karma was a volitional mental event, what Richard Gombrich calls 'an ethicised consciousness'.

This idea leads into the second moral justification of the Buddha: intentionally performing negative actions reinforces and propagates mental defilements which keep persons bound to the cycle of rebirth and interfere with the process of liberation, and hence intentionally performing good karmic actions is participating in mental purification which leads to nirvana, the highest happiness. This perspective sees immoral acts as unskillful ("akusala") in our quest for happiness, and hence it is pragmatic to do good.

The third meta-ethical consideration takes the view of not-self and our natural desire to end our suffering to its logical conclusion. Since there is no self, there is no reason to prefer our own welfare over that of others because there is no ultimate grounding for the differentiation of "my" suffering and someone else's. Instead, an enlightened person would just work to end suffering "tout court", without thinking of the conventional concept of persons. According to this argument, anyone who is selfish does so out of ignorance of the true nature of personal identity and irrationality.

The main Indian Buddhist philosophical schools practiced a form of analysis termed "Abhidharma" which sought to systematize the teachings of the early Buddhist discourses (sutras). Abhidharma analysis broke down human experience into momentary phenomenal events or occurrences called ""dharmas"". Dharmas are impermanent and dependent on other causal factors, they arise and pass as part of a web of other interconnected dharmas, and are never found alone. The Abhidharma schools held that the teachings of the Buddha in the sutras were merely conventional, while the Abhidharma analysis was ultimate truth (paramattha sacca), the way things really are when seen by an enlightened being. The Abhidharmic project has been likened as a form of phenomenology or process philosophy. Abhidharma philosophers not only outlined what they believed to be an exhaustive listing of "dharmas", or phenomenal events, but also the causal relations between them. In the Abhidharmic analysis, the only thing which is ultimately real is the interplay of dharmas in a causal stream; everything else is merely conceptual ("paññatti") and nominal.

This view has been termed "mereological reductionism" by Mark Siderits because it holds that only impartite entities are real, not wholes. Abhidharmikas such as Vasubandhu argued that conventional things (tables, persons, etc.) "disappear under analysis" and that this analysis reveals only a causal stream of phenomenal events and their relations. The mainstream Abhidharmikas defended this view against their main Hindu rivals, the Nyaya school, who were substance theorists and posited the existence of universals. Some Abhidharmikas such as the Prajñaptivāda were also strict nominalists, and held that all things - even dharmas - were merely conceptual.

An important Abhidhamma work from the Theravāda school is the Kathāvatthu ("Points of controversy"), attributed to the Indian scholar-monk Moggaliputta-Tissa (ca.327–247 BCE). This text is important because it attempts to refute several philosophical views which had developed after the death of the Buddha, especially the theory that 'all exists' ("sarvāstivāda"), the theory of momentariness ("khāṇavāda") and the personalist view ("pudgalavada") These were the major philosophical theories which divided the Buddhist Abhidharma schools in India. After being brought to Sri Lanka in the first century BCE, the Theravada Pali language Abhidhamma tradition was heavily influenced by the works of Buddhaghosa (4-5th century AD), the most important philosopher and commentator of the Theravada school. The Theravada philosophical enterprise was mostly carried out in the genre of Atthakatha, commentaries (as well as sub-commentaries) on the Pali Abhidhamma, but also included short summaries and compendiums.

The Sarvāstivāda was one of the major Buddhist philosophical schools in India, and they were so named because of their belief that dharmas exist in all three times: past, present and future. Though the Sarvāstivāda Abhidharma system began as a mere categorization of mental events, their philosophers and exegetes such as Dharmatrata and Katyāyāniputra (the compiler of the Mahavibhasa, a central text of the school) eventually refined this system into a robust realism, which also included a type of essentialism. This realism was based on a quality of dharmas, which was called svabhava or 'intrinsic existence'. Svabhava is a sort of essence, though it is not a completely independent essence, since all dharmas were said to be causally dependent. The Sarvāstivāda system extended this realism across time, effectively positing a type of eternalism with regards to time; hence, the name of their school means "the view that everything exists".

Other Buddhist schools such as the Prajñaptivadins ('nominalists'), the Purvasailas and the Vainasikas refused to accept the concept of svabhava. The main topic of the Tattvasiddhi Śāstra by Harivarman (3-4th century AD), an influential Abhidharma text, is the emptiness (shunyata) of dharmas.

The Theravādins and other schools such as the Sautrāntikas attacked the realism of the Sarvāstivādins, especially their theory of time. A major figure in this argument was the scholar Vasubandhu, an ex-Sarvāstivādin, who critiqued the theory of all exists and argued for philosophical presentism in his comprehensive treatise, the Abhidharmakosa. This work is the major Abhidharma text used in Tibetan and East Asian Buddhism today. The Theravāda also holds that dharmas only exist in the present, and are thus also presentists. The Theravādin presentation of Abhidharma is also not as concerned with ontology as the Sarvāstivādin view, but is more of a phenomenology and hence the concept of svabhava for the Theravādins is more of a certain characteristic or dependent feature of a dharma, than any sort of essence or metaphysical grounding. According to Y Karunadasa:

In the Pali tradition it is only for the sake of definition and description that each dhamma is postulated as if it were a separate entity; but in reality it is by no means a solitary phenomenon having an existence of its own...If this Abhidhammic view of existence, as seen from its doctrine of dhammas, cannot be interpreted as a radical pluralism, neither can it be interpreted as an out-and-out monism. For what are called dhammas -- the component factors of the universe, both within us and outside us -- are not fractions of an absolute unity but a multiplicity of co-ordinate factors. They are not reducible to, nor do they emerge from, a single reality, the fundamental postulate of monistic metaphysics. If they are to be interpreted as phenomena, this should be done with the proviso that they are phenomena with no corresponding noumena, no hidden underlying ground. For they are not manifestations of some mysterious metaphysical substratum, but processes taking place due to the interplay of a multitude of conditions.

An important theory held by some Sarvāstivādins, Theravādins and Sautrāntikas was the theory of "momentariness" (Skt., kṣāṇavāda, Pali, khāṇavāda). This theory held that dhammas only last for a minute moment ("ksana") after they arise. The Sarvāstivādins saw these 'moments' in an atomistic way, as the smallest length of time possible (they also developed a material atomism). Reconciling this theory with their eternalism regarding time was a major philosophical project of the Sarvāstivāda. The Theravādins initially rejected this theory, as evidenced by the Khaṇikakathā of the Kathavatthu which attempts to refute the doctrine that "all phenomena (dhamma) are as momentary as a single mental entity." However, momentariness with regards to mental dhammas (but not physical or rūpa dhammas) was later adopted by the Sri Lankan Theravādins, and it is possible that it was first introduced by the scholar Buddhagosa.

All Abhidharma schools also developed complex theories of causation and conditionality to explain how dharmas interacted with each other. Another major philosophical project of the Abhidharma schools was the explanation of perception. Some schools such as the Sarvastivadins explained perception as a type of phenomenalist realism while others such as the Sautrantikas preferred representationalism and held that we only perceive objects indirectly. The major argument used for this view by the Sautrāntikas was the "time lag argument." According to Mark Siderits: "The basic idea behind the argument is that since there is always a tiny gap between when the sense comes in contact with the external object and when there is sensory awareness, what we are aware of can't be the external object that the senses were in contact with, since it no longer exists." This is related to the theory of extreme momentariness.

One major philosophical view which was rejected by all the schools mentioned above was the view held by the Pudgalavadin or 'personalist' schools. They seemed to have held that there was a sort of 'personhood' in some ultimately real sense which was not reducible to the five aggregates. This controversial claim was in contrast to the other Buddhists of the time who held that a personality was a mere conceptual construction (prajñapti) and only conventionally real.

From about the 1st century BCE, a new textual tradition began to arise in Indian Buddhist thought called Mahāyāna (Great Vehicle), which would slowly come to dominate Indian Buddhist philosophy. Buddhist philosophy thrived in large monastery-university complexes such as Nalanda and Vikramasila, which became centres of learning in North India. Mahāyāna philosophers continued the philosophical projects of Abhidharma while at the same time critiquing them and introducing new concepts and ideas. Since the Mahāyāna held to the pragmatic concept of truth which states that doctrines are regarded as conditionally "true" in the sense of being spiritually beneficial, the new theories and practices were seen as 'skillful means' (Upaya). The Mahayana also promoted the Bodhisattva ideal, which included an attitude of compassion for all sentient beings. The Bodhisattva is someone who chooses to remain in "samsara" (the cycle of birth and death) to benefit all other beings who are suffering.

Major Mahayana philosophical schools and traditions include the Prajnaparamita, Madhyamaka, Tathagatagarbha, the Epistemological school of Dignaga, Yogācāra, Huayan, Tiantai and the Chan/Zen schools.

The earliest Prajñāpāramitā-sutras ("perfection of insight" sutras) (circa 1st century BCE) emphasize the shunyata (emptiness) of phenomena and dharmas. The Prajñāpāramitā is said to be true knowledge of the nature of ultimate reality, which is illusory and empty of essence.

The "Diamond Sutra" states that: 
The "Heart Sutra" famously affirms the shunyata of phenomena:
"Oh, Sariputra, form does not differ from shunyata,and shunyata does not differ from form.
Form is shunyata and shunyata is form;
the same is true for feelings,
perceptions, volitions and consciousness".

The Prajñāpāramitā teachings are associated with the work of the Buddhist philosopher Nāgārjuna (c. 150 – c. 250 CE) and the Madhyamaka (Middle way) school. Nāgārjuna was one of the most influential Indian Buddhist thinkers; he gave the classical arguments for the empty nature of phenomena and attacked the Sarvāstivāda and Pudgalavada schools' essentialism in his magnum opus, "The Fundamental Verses on the Middle Way" ("Mūlamadhyamakakārikā"). In the "Mūlamadhyamakakārikā", Nagarjuna relies on reductio ad absurdum arguments to refute various theories which assume svabhava (an inherent essence or "own being"). In this work, he covers topics such as causation, motion, and the sense faculties.

Nagarjuna asserted a direct connection between, even identity of, dependent origination, non-self ("anatta"), and emptiness ("śūnyatā"). He pointed out that implicit in the early Buddhist concept of dependent origination is the lack of anatta (substantial being) underlying the participants in origination, so that they have no independent existence, a state identified as śūnyatā (i.e., emptiness of a nature or essence ("svabhāva sunyam").

Later philosophers of the Madhyamaka school built upon Nagarjuna's analysis and defended Madhyamaka against their opponents. These included Āryadeva (3rd century CE), Nāgārjuna's pupil; Candrakīrti (600–c. 650), who wrote an important commentary on the Mūlamadhyamakakārikā; and Shantideva (8th century). Buddhapālita (470–550) has been understood as the originator of the 'prāsaṅgika' approach which is based on critiquing essentialism only through "reductio ad absurdum" arguments. He was criticized by Bhāvaviveka (c. 500 – c. 578), who argued for the use of syllogisms "to set one's own doctrinal stance". These two approaches were later termed the Prāsaṅgika and the Svātantrika approaches to Madhyamaka by Tibetan philosophers and commentators.

Influenced by the work of Dignaga, Bhāvaviveka's Madhyamika philosophy makes use of Buddhist epistemology. Candrakīrti, on the other hand, critiqued Bhāvaviveka's adoption of the epistemological ("pramana") tradition on the grounds that it contained a subtle essentialism. He quotes Nagarjuna's famous statement in the "Vigrahavyavartani" which says "I have no thesis" for his rejection of positive epistemic Madhyamaka statements. Candrakīrti held that a true Madhyamika could only use "consequence" ("prasanga"), in which one points out the inconsistencies of their opponent's position without asserting an "autonomous inference" ("svatantra"), for no such inference can be ultimately true from the point of view of Madhyamaka.

In China, the Madhyamaka school (known as Sānlùn) was founded by Kumārajīva (344–413 CE), who translated the works of Nagarjuna to Chinese. Other Chinese Madhymakas include Kumārajīva 's pupil Sengzhao, Jizang (549–623), who wrote over 50 works on Madhyamaka, and Hyegwan, a Korean monk who brought Madhyamaka teachings to Japan.

The Yogācāra school ("Yoga practice") was a Buddhist philosophical tradition which arose in between the 2nd century CE and the 4th century CE and is associated with the philosophers Asanga and Vasubandhu and with various sutras such as the Sandhinirmocana Sutra and the Lankavatara Sutra. The central feature of Yogācāra thought is the concept of "Vijñapti-mātra", often translated as "impressions only" or "appearance only" and this has been interpreted as a form of Idealism or as a form of Phenomenology. Other names for the Yogacara school are 'Vijñanavada' (the doctrine of consciousness) and 'Cittamatra' (mind-only).

Yogacara thinkers like Vasubandhu argued against the existence of external objects by pointing out that we only ever have access to our own mental impressions, and hence our inference of the existence of external objects is based on faulty logic. Vasubandhu's "Vijnaptimatratasiddhi", or "The Proof that There Are Only Impressions" (20 verses), begins thus:"I. This [world] is nothing but impressions, since it manifests itself as an unreal object, 
Just like the case of those with cataracts seeing unreal hairs in the moon and the like."According to Vasubandhu then, all our experiences are like seeing hairs on the moon when we have cataracts, that is, we project our mental images into something "out there" when there are no such things. Vasubandhu then goes on to use the dream argument to argue that mental impressions do not require external objects to (1) seem to be spatio-temporally located, (2) to seem to have an inter-subjective quality, and (3) to seem to operate by causal laws. The fact that purely mental events can have causal efficacy and be intersubjective is proved by the event of a wet dream and by the mass or shared hallucinations created by the karma of certain types of beings.

After having argued that impressions-only is a theory which can explain our everyday experience, Vasubandhu then appeals to parsimony - since we do not need the concept of external objects to explain reality, then we can do away with those superfluous concepts altogether as they are most likely just mentally superimposed on our concepts of reality by the mind. Inter-subjective reality for Vasubandhu is then the causal interaction between various mental streams and their karma, and does not include any external physical objects. The soteriological importance of this theory is that, by removing the concept of an external world, it also weakens the 'internal' sense of self as observer which is supposed to be separate from the external world. To dissolve the dualism of inner and outer is also to dissolve the sense of self and other. The later Yogacara commentator Sthiramati explains this thus:"There is a grasper if there is something to be grasped, but not in the absence of what is to be grasped. Where there is no thing to be grasped, the absence of a grasper also follows, there is not just the absence of the thing to be grasped. Thus there arises the extra-mundane non-conceptual cognition that is alike without object and without cognizer."Vasubandhu also attacked the realist theories of Buddhist atomism and the Abhidharma theory of svabhava. He argued that atoms as conceived by the atomists (un-divisible entities) would not be able to come together to form larger aggregate entities, and hence that they were illogical concepts.

Later Yogacara thinkers include Dharmapala of Nalanda, Sthiramati, Chandragomin (who debated Candrakirti), and Śīlabhadra. Yogacarins such as Paramartha and Guṇabhadra brought the school to China and translated Yogacara works there, where it is known as Wéishí-zōng or Fǎxiàng-zōng. An important contribution to East Asian Yogācāra is Xuanzang's "Cheng Weishi Lun", or "Discourse on the Establishment of Consciousness Only".

Jñānagarbha (8th century) and his student Śāntarakṣita (725–788) brought together Yogacara, Madhyamaka and the Dignaga school of epistemology into a philosophical synthesis known as the "Yogācāra-Svatantrika-Mādhyamika". Śāntarakṣita was also instrumental in the introduction of Buddhism and the Sarvastivadin monastic ordination lineage to Tibet, which was conducted at Samye. Śāntarakṣita's disciples included Haribhadra and Kamalaśīla. This philosophical tradition is influential in Tibetan Buddhist thought.

The "tathāgathagarbha sutras", in a departure from mainstream Buddhist language, insist that the potential for awakening is inherent to every sentient being. They marked a shift from a largely apophatic (negative) philosophical trend within Buddhism to a decidedly more cataphatic (positive) modus.

Prior to the period of these scriptures, Mahāyāna metaphysics had been dominated by teachings on emptiness in the form of Madhyamaka philosophy. The language used by this approach is primarily negative, and the "tathāgatagarbha" genre of sutras can be seen as an attempt to state orthodox Buddhist teachings of dependent origination using positive language instead, to prevent people from being turned away from Buddhism by a false impression of nihilism.

In these sutras, the perfection of the wisdom of not-self is stated to be the true self; the ultimate goal of the path is then characterized using a range of positive language that had been used previously in Indian philosophy by essentialist philosophers, but which was now transmuted into a new Buddhist vocabulary to describe a being who has successfully completed the Buddhist path.

The word "self" ("atman") is used in a way idiosyncratic to these sutras; the "true self" is described as the perfection of the wisdom of not-self in the "Buddha-Nature Treatise", for example. Language that had previously been used by essentialist non-Buddhist philosophers was now adopted, with new definitions, by Buddhists to promote orthodox teachings.

The "tathāgatagarbha" does not, according to some scholars, represent a substantial self; rather, it is a positive language expression of emptiness and represents the potentiality to realize Buddhahood through Buddhist practices. In this interpretation, the intention of the teaching of "tathāgatagarbha" is soteriological rather than theoretical.

The "tathāgathagarbha", the Theravāda doctrine of "bhavaṅga", and the Yogācāra store consciousness were all identified at some point with the luminous mind of the Nikāyas.

In the Mahayana "Mahaparinirvana Sutra", the Buddha insists that while pondering upon Dharma is vital, one must then relinquish fixation on words and letters, as these are utterly divorced from liberation and the Buddha-nature.

Dignāga (c. 480–540) and Dharmakīrti (c. 6-7th century) were Buddhist philosophers who developed a system of epistemology (pramana) and logic in their debates with the Brahminical philosophers in order to defend Buddhist doctrine. This tradition is called "those who follow reasoning" (Tibetan: "rigs pa rjes su 'brang ba"); in modern literature it is sometimes known by the Sanskrit "'pramāṇavāda"', or "the Epistemological School." They were associated with the Yogacara and Sautrantika schools, and defended theories held by both of these schools. Dignaga's influence was profound and led to an "epistemological turn" among all Buddhist and also all Sanskrit language philosophers in India after his death. In the centuries following Dignaga's work, Sanskrit philosophers became much more focused on defending all of their propositions with fully developed theories of knowledge.

The "School of Dignāga" includes later philosophers and commentators like Santabhadra, Dharmottara (8th century), Jñanasrimitra (975–1025), Ratnakīrti (11th century) and Samkarananda. The epistemology they developed defends the view that there are only two 'instruments of knowledge' or 'valid cognitions' ("pramana"): "perception" (pratyaksa) and "inference" (anumāṇa). Perception is a non-conceptual awareness of particulars which is bound by causality, while inference is reasonable, linguistic and conceptual.

These Buddhist philosophers argued in favor of the theory of momentariness, the Yogacara "awareness only" view, the reality of particulars (svalakṣaṇa), atomism, nominalism and the self-reflexive nature of consciousness (svasaṃvedana). They attacked Hindu theories of God (Isvara), universals, the authority of the Vedas, and the existence of a permanent soul ("atman").

The tradition associated with a group of texts known as the Buddhist Tantras, known as Vajrayana, developed by the eighth century in North India. By this time Tantra was a key feature of Indian Buddhism, and Indian Tantric scholars developed philosophical defenses, hermeneutics and explanations of the Buddhist tantric systems, especially through commentaries on key tantras such as the Guhyasamāja Tantra and the Guhyagarbha Tantra.

While the view of the Vajrayana was based on Madhyamaka, Yogacara and Buddha-nature theories, it saw itself as being a faster vehicle to liberation containing many skillful methods (upaya) of tantric ritual. The need for an explication and defense of the Tantras arose out of the unusual nature of the rituals associated with them, which included the use of secret mantras, alcohol, sexual yoga, complex visualizations of mandalas filled with wrathful deities and other practices and injunctions which were discordant with or at least novel in comparison to traditional Buddhist thought. The Guhyasamāja Tantra, for example, states: "you should kill living beings, speak lying words, take things that are not given and have sex with many women". Other features of tantra included a focus on the physical body as the means to liberation and a reaffirmation of feminine elements, feminine deities and sexuality.

The defense of these practices is based on the theory of transformation which states that negative mental factors and physical actions can be cultivated and transformed in a ritual setting. The Hevajra tantra states:

Those things by which evil men are bound, others turn into means and gain thereby release from the bonds of existence. By passion the world is bound, by passion too it is released, but by heretical Buddhists this practice of reversals is not known.

Another hermeneutic of Buddhist Tantric commentaries such as the Vimalaprabha of Pundarika (a commentary on the Kalacakra Tantra) is one of interpreting taboo or unethical statements in the Tantras as metaphorical statements about tantric practice. For example, in the Vimalaprabha, "killing living beings" refers to stopping the prana at the top of the head. In the Tantric Candrakirti's "Pradipoddyotana", a commentary to the Guhyasamaja Tantra, killing living beings is glossed as "making them void" by means of a "special samadhi" which according to Bus-ton is associated with completion stage tantric practice.

Douglas Duckworth notes that Vajrayana philosophical outlook is one of embodiment, which sees the physical and cosmological body as already containing wisdom and divinity. Liberation (nirvana) and Buddhahood are not seen as something outside or an event in the future, but as imminently present and accessible right now through unique tantric practices like deity yoga, and hence Vajrayana is also called the "resultant vehicle". Duckworth names the philosophical view of Vajrayana as a form of pantheism, by which he means the belief that every existing entity is in some sense divine and that all things express some form of unity.

Major Indian Tantric Buddhist philosophers such as Buddhaguhya, Padmavajra (author of the "Guhyasiddhi"), Nagarjuna (7th-century disciple of Saraha), Indrabhuti (author of the "Jñānasiddhi"), Anangavajra, Dombiheruka, Durjayacandra, Ratnākaraśānti and Abhayakaragupta wrote tantric texts and commentaries systematizing the tradition. Others such as Vajrabodhi and Śubhakarasiṃha brought Tantra to Tang China (716 to 720), and tantric philosophy continued to be developed in Chinese and Japanese by thinkers such as Yi Xing and Kūkai. In Tibet, philosophers such as Sakya Pandita (1182-28 – 1251), Longchenpa (1308–1364) and Tsongkhapa (1357–1419) continued the tradition of Buddhist Tantric philosophy in Tibetan.

Tibetan Buddhist philosophy is mainly a continuation and refinement of the Indian traditions of Madhyamaka, Yogacara and the Dignaga-Dharmakīrti school of epistemology or "reliable cognition" (Sanskrit: "pramana", Tib. "tshad ma"). The initial efforts of Śāntarakṣita and Kamalaśīla brought their eclectic scholarly tradition to Tibet. Other influences include Buddhist Tantras and the Buddha nature texts.

The initial work of early Tibetan Buddhist philosophers was in translation of classical Indian philosophical treatises and the writing of commentaries. This initial period is from the 8th to the 10th century. Early Tibetan commentator philosophers were heavily influenced by the work of Dharmakirti and these include Ngok Lo-dza-wa (1059-1109) and Cha-ba (1182-1251). Their works are now lost. The 12th and 13th centuries saw the translation of the works of Chandrakirti, the promulgation of his views in Tibet by scholars such as Patsab Nyima Drakpa, Kanakavarman and Jayananda (12th century) and the development of the Tibetan debate between the prasangika and svatantrika views which continues to this day among Tibetan Buddhist schools. The main disagreement between these views is the use of reasoned argument. For Śāntarakṣita, Kamalaśīla and their defenders, reason is useful in establishing arguments that lead one to a correct understanding of emptiness, then, through the use of meditation, one can reach non-conceptual gnosis that does not rely on reason. For Chandrakirti, however, this is wrong, because meditation on emptiness cannot possibly involve any object. Reason's role here is to negate any essence or essentialist views, and then eventually negate itself along with any Conceptual proliferation.

There are various Tibetan Buddhist schools or monastic orders. According to Georges B.J. Dreyfus, within Tibetan thought, the Sakya school holds a mostly anti-realist philosophical position, while the Gelug school tends to defend a form of realism. The Kagyu and Nyingma schools also tend to follow Sakya anti-realism (with some differences).

The 14th century saw increasing interest in the Buddha nature texts and doctrines. This can be seen in the work of the third Kagyu Karmapa Rangjung Dorje (1284-1339), especially his treatise "Profound Inner Meaning". This treatise describes ultimate nature or suchness as Buddha nature which is the basis for nirvana and samsara, radiant in nature and empty in essence, surpassing thought.

Dolpopa ("Dol-bo-ba", 1292–1361), founder of the Jonang school, developed a view called shentong (Wylie: gzhan ) (other empty), which is closely tied to Yogacara and Buddha-nature theories. This view holds that the qualities of Buddhahood or Buddha nature are already present in the mind, and that it is empty of all conventional reality which occludes its own nature as Buddhahood or Dharmakaya. According to Dolpopa, all beings are said to have Buddha nature, which is real, unchanging, permanent, non-conditioned, eternal, blissful and compassionate. Dolpopa's shentong view taught that ultimate reality was truly a "Great Self" or "Supreme Self" referring to works such as the "Mahāyāna Mahāparinirvāṇa Sūtra", the "Aṅgulimālīya Sūtra" and the "Śrīmālādevī Siṃhanāda Sūtra." This view had an influence on philosophers of other schools, such as Nyingma and Kagyu thinkers, and was also widely criticized in some circles as being similar to the Hindu notions of Atman. The Shentong philosophy was also expounded in Tibet and Mongolia by the later Jonang scholar Tāranātha (1575–1634).

In the late 17th century, the Jonang order and its teachings came under attack by the 5th Dalai Lama, who converted the majority of their monasteries in Tibet to the Gelug order, although several survived in secret.

Je Tsongkhapa (Dzong-ka-ba) (1357–1419) founded the Gelug school of Tibetan Buddhism, which came to dominate the country through the office of the Dalai Lama and is the major defender of the Prasaṅgika Madhyamaka view. His work is influenced by the philosophy of Candrakirti and Dharmakirti. Tsongkhapa's magnum opus is "The Ocean of Reasoning", a Commentary on Nagarjuna's Mulamadhyamakakarika. Gelug philosophy is based upon study of Madhyamaka texts and Tsongkhapa's works as well as formal debate (rtsod pa).

Tsongkhapa defended Prasangika Madhyamaka as the highest view and critiqued the Svatantrika. Tsongkhapa argued that, because the Svatantrika conventionally establish things by their own characteristics, they fail to completely understand the emptiness of phenomena and hence do not achieve the same realization. Drawing on Chandrakirti, Tsongkhapa rejected the Yogacara teachings, even as a provisional stepping point to the Madhyamaka view. Tsongkhapa was also critical of the Shengtong view of Dolpopa, which he saw as dangerously absolutist and hence outside the middle way. Tsongkhapa identified two major flaws in interpretations of Madhyamika, under-negation (of svabhava or own essence), which could lead to Absolutism, and over-negation, which could lead to Nihilism. Tsongkhapa's solution to this dilemma was the promotion of the use of inferential reasoning only within the conventional realm of the two truths framework, allowing for the use of reason for ethics, conventional monastic rules and promoting a conventional epistemic realism, while holding that, from the view of ultimate truth ("paramarthika satya"), all things (including Buddha nature and Nirvana) are empty of inherent existence (svabhava), and that true enlightenment is this realization of emptiness.

Sakya scholars such as Rongtön and Gorampa disagreed with Tsongkhapa, and argued that the prasangika svatantrika distinction was merely pedagogical. Gorampa also critiqued Tsongkhapa's realism, arguing that the structures which allow an empty object to be presented as conventionally real eventually dissolve under analysis and are thus unstructured and non-conceptual (spros bral). Tsongkhapa's students Gyel-tsap, Kay-drup, and Ge-dun-drup set forth an epistemological realism against the Sakya scholars' anti-realism.

Sakya Pandita (1182–1251) was a 13th-century head of the Sakya school and ruler of Tibet. He was also one of the most important Buddhist philosophers in the Tibetan tradition, writing works on logic and epistemology and promoting Dharmakirti's "Pramanavarttika" (Commentary on Valid Cognition) as central to scholastic study. Sakya Pandita's 'Treasury of Logic on Valid Cognition' ("Tshad ma rigs pa'i gter") set forth the classic Sakya epistemic anti-realist position, arguing that concepts such as universals are not known through valid cognition and hence are not real objects of knowledge. Sakya Pandita was also critical of theories of sudden enlightenment, which were held by some teachers of the "Chinese Great Perfection" in Tibet.

Later Sakyas such as Gorampa (1429–1489) and Sakya Chokden (1428–1507) would develop and defend Sakya anti-realism, and they are seen as the major interpreters and critics of Sakya Pandita's philosophy. Sakya Chokden also critiqued Tsongkhapa's interpretation of Madhyamaka and Dolpopa's Shentong. In his "Definite ascertainment of the middle way", Chokden criticized Tsongkhapa's view as being too logo-centric and still caught up in conceptualization about the ultimate reality which is beyond language. Sakya Chokden's philosophy attempted to reconcile the views of the Yogacara and Madhyamaka, seeing them both as valid and complementary perspectives on ultimate truth. Madhyamaka is seen by Chokden as removing the fault of taking the unreal as being real, and Yogacara removes the fault of the denial of Reality. Likewise, the Shentong and Rangtong views are seen as complementary by Sakya Chokden; Rangtong negation is effective in cutting through all clinging to wrong views and conceptual rectification, while Shentong is more amenable for describing and enhancing meditative experience and realization. Therefore, for Sakya Chokden, the same realization of ultimate reality can be accessed and described in two different but compatible ways.

The Nyingma school is strongly influenced by the view of Dzogchen (Great Perfection) and the Dzogchen Tantric literature. Longchenpa (1308–1364) was a major philosopher of the Nyingma school and wrote an extensive number of works on the Tibetan practice of Dzogchen and on Buddhist Tantra. These include the "Seven Treasures", the "Trilogy of Natural Ease", and his "Trilogy of Dispelling Darkness". Longchenpa's works provide a philosophical understanding of Dzogchen, a defense of Dzogchen in light of the sutras, as well as practical instructions. For Longchenpa, the ground of reality is luminous clarity, rigpa, or Buddha nature, and this ground is also the bridge between sutra and tantra. Longchenpa's philosophy sought to establish the positive aspects of Buddha nature thought against the totally negative theology of Madhyamika without straying into the absolutism of Dolpopa. For Longchenpa, the basis for Dzogchen and Tantric practice in Vajrayana is the "Ground" ("gzhi"), the immanent Buddha nature, "the primordially luminous reality that is unconditioned and spontaneously present" which is "free from all elaborated extremes".

The 19th century saw the rise of the Rimé movement (non-sectarian, unbiased) which sought to push back against the politically dominant Gelug school's criticisms of the Sakya, Kagyu, Nyingma and Bon philosophical views, and develop a more eclectic or universal system of textual study. Jamyang Khyentse Wangpo (1820-1892) and Jamgön Kongtrül (1813-1899) were the founders of Rimé. The Rimé movement came to prominence at a point in Tibetan history when the religious climate had become partisan. The aim of the movement was "a push towards a middle ground where the various views and styles of the different traditions were appreciated for their individual contributions rather than being refuted, marginalized, or banned." Philosophically, Jamgön Kongtrül defended Shentong as being compatible with Madhyamaka while another Rimé scholar Jamgon Ju Mipham Gyatso (1846–1912) criticized Tsongkhapa from a Nyingma perspective. Mipham argued that the view of the middle way is Unity (zung 'jug), meaning that from the ultimate perspective the duality of sentient beings and Buddhas is also dissolved. Mipham also affirmed the view of "rangtong" (self emptiness). The later Nyingma scholar Botrul (1894–1959) classified the major Tibetan Madhyamaka positions as shentong (other emptiness), Nyingma rangtong (self emptiness) and Gelug bdentong (emptiness of true existence). The main difference between them is their "object of negation"; shengtong states that inauthentic experience is empty, rangtong negates any conceptual reference and bdentong negates any true existence.

The 14th Dalai Lama was also influenced by this eclectic approach. Having studied under teachers from all major Tibetan Buddhist schools, his philosophical position tends to be that the different perspectives on emptiness are complementary:

There is a tradition of making a distinction between two different perspectives on the nature of emptiness: one is when emptiness is presented within a philosophical analysis of the ultimate reality of things, in which case it ought to be understood in terms of a non-affirming negative phenomena. On the other hand, when it is discussed from the point of view of experience, it should be understood more in terms of an affirming negation – 14th Dalai Lama

The schools of Buddhism that had existed in China prior to the emergence of the Tiantai are generally believed to represent direct transplantations from India, with little modification to their basic doctrines and methods. The Tiantai school, founded by Zhiyi (538–597), was the first truly unique Chinese Buddhist philosophical school. The doctrine of Tiantai was based on the ekayana or "one vehicle" doctrine taught in the Lotus sutra and sought to bring together all Buddhist teachings and texts into a comprehensively inclusive hierarchical system, which placed the Lotus sutra at the top of this hierarchy.

Tiantai's metaphysics is an immanent holism, which sees every phenomenon, moment or event as conditioned and manifested by the whole of reality. Every instant of experience is a reflection of every other, and hence, suffering and nirvana, good and bad, Buddhahood and evildoing, are all "inherently entailed" within each other. Each moment of consciousness is simply the Absolute itself, infinitely immanent and self reflecting.

This metaphysics is entailed in the Tiantai teaching of the "three truths", which is an extension of the Mādhyamaka two truths doctrine. The three truths are: the conventional truth of appearance, the truth of emptiness (shunyata) and the third truth of 'the exclusive Center' (但中 "danzhong") or middle way, which is beyond conventional truth and emptiness. This third truth is the Absolute and expressed by the claim that nothing is "Neither-Same-Nor-Different" than anything else, but rather each 'thing' is the absolute totality of all things manifesting as a particular, everything is mutually contained within each thing. Everything is a reflection of 'The Ultimate Reality of All Appearances'(諸法實相 zhufashixiang) and each thought "contains three thousand worlds". This perspective allows the Tiantai school to state such seemingly paradoxical things as "evil is ineradicable from the highest good, Buddhahood." Moreover, in Tiantai, nirvana and samsara are ultimately the same; as Zhiyi writes, "A single, unalloyed reality is all there is – no entities whatever exist outside of it."

Though Zhiyi did write "One thought contains three thousand worlds", this does not entail idealism. According to Zhiyi, "The objects of the [true] aspects of reality are not something produced by Buddhas, gods, or men. They exist inherently on their own and have no beginning" (The Esoteric Meaning, 210). This is then a form of realism, which sees the mind as real as the world, interconnected with and inseparable from it. In Tiantai thought, ultimate reality is simply the phenomenal world of interconnected events or dharmas.

Other key figures of Tiantai thought are Zhanran (711–782) and Siming Zhili (960–1028). Zhanran developed the idea that non-sentient beings have Buddha nature, since they are also a reflection of the Absolute. In Japan, this school was known as Tendai and was first brought to the island by Saicho.

The Huayan developed the doctrine of "interpenetration" or "coalescence" (Wylie: "zung-'jug"; Sanskrit: "yuganaddha"), based on the "Avataṃsaka Sūtra" (Flower Garland sutra), a Mahāyāna scripture. Huayan holds that all phenomena (Sanskrit: "dharmas") are deeply interconnected, mutually arising and that every phenomenon contains all other phenomena. Various metaphors and images are used to illustrate this idea. The first is known as Indra's net. The net is set with jewels which have the extraordinary property that they reflect all of the other jewels, while the reflections also contain every other reflection, ad infinitum. The second image is that of the world text. This image portrays the world as consisting of an enormous text which is as large as the universe itself. The words of the text are composed of the phenomena that make up the world. However, every atom of the world contains the whole text within it. It is the work of a Buddha to let out the text so that beings can be liberated from suffering.

Fazang (Fa-tsang, 643–712), one of the most important Huayan thinkers, wrote 'Essay on the Golden Lion' and 'Treatise on the Five Teachings', which contain other metaphors for the interpenetration of reality. He also used the metaphor of a House of mirrors. Fazang introduced the distinction of "the Realm of Principle" and "the Realm of Things". This theory was further developed by Cheng-guan (738–839) into the major Huayan thesis of "the fourfold Dharmadhatu" (dharma realm): the Realm of Principle, the Realm of Things, the Realm of the Noninterference between Principle and Things, and the Realm of the Noninterference of All Things. The first two are the universal and the particular, the third is the interpenetration of universal and particular, and the fourth is the interpenetration of all particulars. The third truth was explained by the metaphor of a golden lion: the gold is the universal and the particular is the shape and features of the lion. 
While both Tiantai and Huayan hold to the interpenetration and interconnection of all things, their metaphysics have some differences. Huayan metaphysics is influenced by Yogacara thought and is closer to idealism. The Avatamsaka sutra compares the phenomenal world to a dream, an illusion, and a magician's conjuring. The sutra states nothing has true reality, location, beginning and end, or substantial nature. The Avatamsaka also states that "The triple world is illusory – it is only made by one mind", and Fazang echoes this by writing, "outside of mind there is not a single thing that can be apprehended." Furthermore, according to Huayan thought, each mind creates its own world "according to their mental patterns", and "these worlds are infinite in kind" and constantly arising and passing away. However, in Huayan, mind is not real either, but also empty. The true reality in Huayan, the noumenon, or "Principle", is likened to a mirror, while phenomena are compared to reflections in the mirror. It is also compared to the ocean, and phenomena to waves.

In Korea, this school was known as Hwaeom and is represented in the work of Wonhyo (617–686), who also wrote about the idea of essence-function, a central theme in Korean Buddhist thought. In Japan, Huayan is known as Kegon and one of its major proponents was Myōe, who also introduced Tantric practices.

The philosophy of Chinese Chan Buddhism and Japanese Zen is based on various sources; these include Chinese Madhyamaka ("Sānlùn"), Yogacara ("Wéishí"), the Laṅkāvatāra Sūtra, and the Buddha nature texts. An important issue in Chan is that of subitism or "sudden enlightenment", the idea that enlightenment happens all at once in a flash of insight. This view was promoted by Shenhui and is a central issue discussed in the Platform Sutra, a key Chan scripture composed in China.

Huayan philosophy also had an influence on Chan. The theory of the Fourfold Dharmadhatu influenced the Five Ranks of Dongshan Liangjie (806-869), the founder of the Caodong Chan lineage. Guifeng Zongmi, who was also a patriarch of Huayan Buddhism, wrote extensively on the philosophy of Chan and on the Avatamsaka sutra.

Japanese Buddhism during the 6th and 7th centuries saw an increase in the proliferation of new schools and forms of thought, a period known as the six schools of Nara ("Nanto Rokushū"). The Kamakura Period (1185–1333) also saw another flurry of intellectual activity. During this period, the influential figure of Nichiren (1222–1282) made the practice and universal message of the Lotus Sutra more readily available to the population. He is of particular importance in the history of thought and religion, as his teachings constitute a separate sect of Buddhism, one of the only major sects to have originated in Japan

Also during the Kamakura period, the founder of Soto Zen, Dogen (1200–1253), wrote many works on the philosophy of Zen, and the "Shobogenzo" is his magnum opus. In Korea, Chinul was an important exponent of Seon Buddhism at around the same time.

Tantric Buddhism arrived in China in the 7th century, during the Tang Dynasty. In China, this form of Buddhism is known as Mìzōng (密宗), or "Esoteric School", and "Zhenyan" (true word, Sanskrit: Mantrayana). Kūkai (AD774–835) is a major Japanese Buddhist philosopher and the founder of the Tantric Shingon (true word) school in Japan. He wrote on a wide variety of topics such as public policy, language, the arts, literature, music and religion. After studying in China under Huiguo, Kūkai brought together various elements into a cohesive philosophical system of Shingon.

Kūkai's philosophy is based on the Mahavairocana Tantra and the Vajrasekhara Sutra (both from the seventh century). His "Benkenmitsu nikkyôron" (Treatise on the Differences Between Esoteric and Exoteric Teachings) outlines the difference between exoteric, mainstream Mahayana Buddhism (kengyô) and esoteric Tantric Buddhism (mikkyô). Kūkai provided the theoretical framework for the esoteric Buddhist practices of Mantrayana, bridging the gap between the doctrine of the sutras and tantric practices. At the foundation of Kūkai's thought is the Trikaya doctrine, which holds there are three "bodies of the Buddha".

According to Kūkai, esoteric Buddhism has the Dharmakaya (Jpn: "hosshin", embodiment of truth) as its source, which is associated with Vairocana Buddha (Dainichi). Hosshin is embodied absolute reality and truth. Hosshin is mostly ineffable but can be experienced through esoteric practices such as mudras and mantras. While Mahayana is taught by the historical Buddha (nirmankaya), it does not have ultimate reality as its source or the practices to experience the esoteric truth. For Shingon, from an enlightened perspective, the whole phenomenal world itself is also the teaching of Vairocana. The body of the world, its sounds and movements, is the body of truth (dharma) and furthermore it is also identical with the personal body of the cosmic Buddha. For Kūkai, world, actions, persons and Buddhas are all part of the cosmic monologue of Vairocana, they are the truth being preached, to its own self manifestations. This is "hosshin seppô" (literally: "the dharmakâya's expounding of the Dharma") which can be accessed through mantra which is the cosmic language of Vairocana emanating through cosmic vibration concentrated in sound. In a broad sense, the universe itself a huge text expressing ultimate truth (Dharma) which must be "read".

Dainichi means "Great Sun" and Kūkai uses this as a metaphor for the great primordial Buddha, whose teaching and presence illuminates and pervades all, like the light of the sun. This immanent presence also means that every being already has access to enlightenment (hongaku) and Buddha nature, and that, because of this, there is the possibility of "becoming Buddha in this very embodied existence" ("sokushinjôbutsu"). This is achieved because of the non-dual relationship between the macrocosm of Hosshin and the microcosm of the Shingon practitioner.

Kūkai's exposition of what has been called Shingon's "metaphysics" is based on the three aspects of the cosmic truth or Hosshin – body, appearance and function. The body is the physical and mental elements, which are the body and mind of the cosmic Buddha and which is also empty (Shunyata). The physical universe for Shingon contains the interconnected mental and physical events. The appearance aspect is the form of the world, which appears as mandalas of interconnected realms and is depicted in mandala art such as the Womb Realm mandala. The function is the movement and change which happens in the world, which includes change in forms, sounds and thought. These forms, sounds and thoughts are expressed by the Shingon practitioner in various rituals and tantric practices which allow them to connect with and inter-resonate with Dainichi and hence reach enlightenment here and now.

In Sri Lanka, Buddhist modernists such as Anagarika Dharmapala (1864-1933) and the American convert Henry Steel Olcott sought to show that Buddhism was rational and compatible with modern Scientific ideas such as the theory of evolution. Dharmapala also argued that Buddhism included a strong social element, interpreting it as liberal, altruistic and democratic. K. N. Jayatilleke wrote the classic modern account of Buddhist epistemology ("Early Buddhist Theory of Knowledge", 1963) and his student David Kalupahana wrote on the history of Buddhist thought and psychology. Other important Sri Lankan Buddhist thinkers include Ven Ñāṇananda ("Concept and Reality"), Walpola Rahula, Hammalawa Saddhatissa ("Buddhist Ethics", 1987), Gunapala Dharmasiri ("A Buddhist critique of the Christian concept of God", 1988), P. D. Premasiri and R. G. de S. Wettimuny.

In 20th-century China, the modernist Taixu (1890-1947) advocated a reform and revival of Buddhism. He promoted an idea of a Buddhist Pure Land, not as a metaphysical place in Buddhist cosmology but as something possible to create here and now in this very world, which could be achieved through a "Buddhism for Human Life" () which was free of supernatural beliefs. Taixu also wrote on the connections between modern science and Buddhism, ultimately holding that "scientific methods can only corroborate the Buddhist doctrine, they can never advance beyond it". Like Taixu, Yin Shun (1906–2005) advocated a form of Humanistic Buddhism grounded in concern for humanitarian issues, and his students and followers have been influential in promoting Humanistic Buddhism in Taiwan. This period also saw a revival of the study of Weishi (Yogachara), by Yang Rensan (1837-1911), Ouyang Jinwu (1871-1943) and Liang Shuming (1893–1988).

One of Tibetan Buddhism's most influential modernist thinkers is Gendün Chöphel (1903–1951), who, according to Donald S. Lopez Jr., "was arguably the most important Tibetan intellectual of the twentieth century." Gendün Chöphel traveled throughout India with the Indian Buddhist Rahul Sankrityayan and wrote a wide variety of material, including works promoting the importance of modern science to his Tibetan countrymen and also Buddhist philosophical texts such as "Adornment for Nagarjuna’s Thought". Another very influential Tibetan Buddhist modernist was Chögyam Trungpa, whose Shambhala Training was meant to be more suitable to modern Western sensitivities by offering a vision of "secular enlightenment".

In Southeast Asia, thinkers such as Buddhadasa, Thích Nhất Hạnh, Sulak Sivaraksa and Aung San Suu Kyi have promoted a philosophy of socially Engaged Buddhism and have written on the socio-political application of Buddhism. Likewise, Buddhist approaches to economic ethics (Buddhist economics) have been explored in the works of E. F. Schumacher, Prayudh Payutto, Neville Karunatilake and Padmasiri de Silva. The study of the Pali Abhidhamma tradition continued to be influential in Myanmar, where it was developed by monks such as Ledi Sayadaw and Mahasi Sayadaw.

Japanese Buddhist philosophy was heavily influenced by the work of the Kyoto School which included Kitaro Nishida, Keiji Nishitani, Hajime Tanabe and Masao Abe. These thinkers brought Buddhist ideas in dialogue with Western philosophy, especially European phenomenologists and existentialists. The most important trend in Japanese Buddhist thought after the formation of the Kyoto school is Critical Buddhism, which argues against several Mahayana concepts such as Buddha nature and original enlightenment. In Nichiren Buddhism, the work of Daisaku Ikeda has also been popular.

The Japanese Zen Buddhist D.T. Suzuki (1870–1966) was instrumental in bringing Zen Buddhism to the West and his Buddhist modernist works were very influential in the United States. Suzuki's worldview was a Zen Buddhism influenced by Romanticism and Transcendentalism, which promoted a spiritual freedom as "a spontaneous, emancipatory consciousness that transcends rational intellect and social convention." This idea of Buddhism influenced the Beat writers, and a contemporary representative of Western Buddhist Romanticism is Gary Snyder. The American Theravada Buddhist monk Thanissaro Bhikkhu has critiqued 'Buddhist Romanticism' in his writings.

Western Buddhist monastics and priests such as Nanavira Thera, Bhikkhu Bodhi, Nyanaponika Thera, Robert Aitken, Taigen Dan Leighton, and Matthieu Ricard have written texts on Buddhist philosophy. A feature of Buddhist thought in the West has been a desire for dialogue and integration with modern science and psychology, and various modern Buddhists such as Alan Wallace, James H. Austin, Mark Epstein and the 14th Dalai Lama have worked and written on this issue. Another area of convergence has been Buddhism and environmentalism, which is explored in the work of Joanna Macy. Another Western Buddhist philosophical trend has been the project to secularize Buddhism, as seen in the works of Stephen Batchelor.

In the West, Comparative philosophy between Buddhist and Western thought began with the work of Charles A. Moore, who founded the journal Philosophy East and West. Contemporary Western Academics such as Mark Siderits, Jan Westerhoff, Jonardon Ganeri, Miri Albahari, Owen Flanagan, Damien Keown, Tom Tillemans, David Loy, Evan Thompson and Jay Garfield have written various works which interpret Buddhist ideas through Western philosophy.

Scholars such as Thomas McEvilley, Christopher I. Beckwith, and Adrian Kuzminski have identified cross influences between ancient Buddhism and the ancient Greek philosophy of Pyrrhonism. The Greek philosopher Pyrrho spent 18 months in India as part of Alexander the Great's court on Alexander's conquest of western India, where ancient biographers say his contact with the gymnosophists caused him to create his philosophy. Because of the high degree of similarity between Nāgārjuna's philosophy and Pyrrhonism, particularly the surviving works of Sextus Empiricus Thomas McEvilley suspects that Nāgārjuna was influenced by Greek Pyrrhonist texts imported into India.

Baruch Spinoza, though he argued for the existence of a permanent reality, asserts that all phenomenal existence is transitory. In his opinion sorrow is conquered "by finding an object of knowledge which is not transient, not ephemeral, but is immutable, permanent, everlasting." The Buddha taught that the only thing which is eternal is Nirvana. David Hume, after a relentless analysis of the mind, concluded that consciousness consists of fleeting mental states. Hume's Bundle theory is a very similar concept to the Buddhist "skandhas", though his skepticism about causation lead him to opposite conclusions in other areas. Arthur Schopenhauer's philosophy parallels Buddhism in his affirmation of asceticism and renunciation as a response to suffering and desire.

Ludwig Wittgenstein's "language-game" closely parallel the warning that intellectual speculation or papañca is an impediment to understanding, as found in the Buddhist "Parable of the Poison Arrow". Friedrich Nietzsche, although himself dismissive of Buddhism as yet another nihilism, had a similar impermanent view of the self. Heidegger's ideas on being and nothingness have been held by some to be similar to Buddhism today.

An alternative approach to the comparison of Buddhist thought with Western philosophy is to use the concept of the Middle Way in Buddhism as a critical tool for the assessment of Western philosophies. In this way Western philosophies can be classified in Buddhist terms as eternalist or nihilist. In a Buddhist view all philosophies are considered non-essential views (ditthis) and not to be clung to.




</doc>
<doc id="4471" url="https://en.wikipedia.org/wiki?curid=4471" title="Billy Bob Thornton">
Billy Bob Thornton

Billy Bob Thornton (born August 4, 1955) is an American actor, writer, director, and musician.

Thornton had his first break when he co-wrote and starred in the 1992 thriller "One False Move", and received international attention after writing, directing, and starring in the independent drama film "Sling Blade" (1996), for which he won an Academy Award for Best Adapted Screenplay and was nominated for an Academy Award for Best Actor. He appeared in several major film roles in the 1990s following "Sling Blade", including Oliver Stone's neo-noir "U Turn" (1997), political drama "Primary Colors" (1998), science fiction disaster film "Armageddon" (1998), the highest-grossing film of that year, and the crime drama "A Simple Plan" (1998), which earned him his third Oscar nomination.

In the 2000s, Thornton achieved further success in starring dramas "Monster's Ball" (2001), "The Man Who Wasn't There" (2001), and "Friday Night Lights" (2004); comedies "Bandits" (2001), "Intolerable Cruelty" (2003), and "Bad Santa" (2003); and action films "Eagle Eye" (2008) and "Faster" (2010). In 2014, Thornton starred as Lorne Malvo in the first season of the anthology series "Fargo", earning a nomination for the Outstanding Lead Actor in a Miniseries or TV Movie at the Emmy Awards and won Best Actor in a Miniseries or TV Film at the 72nd Golden Globe Awards. In 2016, he starred in an Amazon original series, "Goliath," which earned him a Golden Globe Award for Best Actor – Television Series Drama.

Thornton has been vocal about his distaste for celebrity culture, choosing to keep his life out of the public eye. However, the attention of the media has proven unavoidable in certain cases, his marriage to Angelina Jolie being a notable example. Thornton has written a variety of films, usually set in the Southern United States and mainly co-written with Tom Epperson, including "A Family Thing" (1996) and "The Gift" (2000). After "Sling Blade", he directed several other films, including "Daddy and Them" (2001), "All the Pretty Horses" (2000), and "Jayne Mansfield's Car" (2012).

Thornton has received the President's Award from the Academy of Science Fiction, Fantasy & Horror Films, a Special Achievement Award from the National Board of Review, and a star on the Hollywood Walk of Fame. He has also been nominated for an Emmy Award, four Golden Globes, and three Screen Actors Guild Awards. In addition to film work, Thornton began a career as a singer-songwriter. He has released four solo albums and is the vocalist of the blues rock band The Boxmasters.

Billy Bob Thornton was born on August 4, 1955, in Hot Springs, Arkansas, the son of Virginia Roberta ("née" Faulkner; died July 29, 2017), a self-proclaimed psychic, and William Raymond "Billy Ray" Thornton (November 1929 – August 1974), a high school history teacher and basketball coach. His brother Jimmy Don (April 1958 – October 1988) wrote a number of songs; Thornton recorded two of them ("Island Avenue" and "Emily") on his solo albums. He is of part Irish descent. He has another brother, John David Thornton.

Thornton lived in numerous places in Arkansas during his childhood, including Alpine, Malvern, and Mount Holly. He was raised Methodist in an extended family in a shack that had no electricity or plumbing. He graduated from Malvern High School in 1973. A good high school baseball player, he tried out for the Kansas City Royals, but was released after an injury. After a short period laying asphalt for the Arkansas State Transportation Department, he attended Henderson State University to pursue studies in psychology but dropped out after two semesters.

In the mid-1980s, Thornton settled in Los Angeles to pursue his career as an actor, with future writing partner Tom Epperson. He had a difficult time succeeding as an actor and worked in telemarketing, offshore wind farming, and fast food management between auditioning for acting jobs. He also played the drums and sang with South African rock band Jack Hammer. While working as a waiter for an industry event, he served film director and screenwriter Billy Wilder. He struck up a conversation with Wilder, who advised Thornton to consider a career as a screenwriter.

Thornton's first screen role was in 1988 "South of Reno", where he played a small role as a counter man in a restaurant. He also made an appearance as a pawn store clerk in the 1987 "Matlock" episode "The Photographer". Another one of his early screen roles was as a cast member on the CBS sitcom Hearts Afire and in 1989 he appeared as an angry heckler in Adam Sandler's debut film "Going Overboard". His role as the villain in 1992's "One False Move", which he also co-wrote, brought him to the attention of critics. He also had small roles in the 1990s films "Indecent Proposal", "On Deadly Ground", "Bound by Honor", and "Tombstone". He went on to write, direct, and star in the 1996 independent film "Sling Blade". The film, an expansion of the short film "Some Folks Call It a Sling Blade", introduced the story of a mentally handicapped man imprisoned for a gruesome and seemingly inexplicable murder.

"Sling Blade" garnered international acclaim. Thornton's screenplay earned him an Academy Award for Best Adapted Screenplay, a Writers Guild of America Award, and an Edgar Award, while his performance received Oscar and Screen Actors Guild nominations for Best Actor. In 1998, Thornton portrayed the James Carville-like Richard Jemmons in "Primary Colors". He adapted the book "All the Pretty Horses" into a 2000 film of the same name. The negative experience (he was forced to cut more than an hour of footage) led to his decision to never direct another film; a subsequent release, "Daddy and Them", had been filmed earlier. Also in 2000, an early script which he and Tom Epperson wrote together was made into "The Gift".

In 2000, Thornton appeared in Travis Tritt's music video for the song "Modern Day Bonnie and Clyde". His screen persona has been described by the press as that of a "tattooed, hirsute man's man". He appeared in several major film roles following the success of "Sling Blade", including 1998's "Armageddon" and "A Simple Plan". In 2001, he directed "Daddy and Them" while securing starring roles in three Hollywood films: "Monster's Ball", "Bandits", and "The Man Who Wasn't There", for which he received many awards.

Thornton played a malicious mall Santa in 2003's "Bad Santa", a black comedy that performed well at the box office and established him as a leading comic actor, and in the same year, portrayed a womanizing President of the United States in the British romantic comedy film "Love Actually". He stated that, following the success of "Bad Santa", audiences "like to watch him play that kind of guy" and that "casting directors call him up when they need an asshole". He referred to this when he said that "it's kinda that simple... you know how narrow the imagination in this business can be".

In 2004, Thornton played David Crockett in "The Alamo". Later that year, he received a star on the Hollywood Walk of Fame on October 7. He appeared in the 2006 comic film "School for Scoundrels". In the film, he plays a self-help doctor, which was written specifically for him. More recent films include 2007 drama "The Astronaut Farmer" and the comedy "Mr. Woodcock", in which he played a sadistic gym teacher. In September 2008, he starred in the action film "Eagle Eye". He has also expressed an interest in directing another film, possibly a period piece about cave explorer Floyd Collins, based on the book "Trapped! The Story of Floyd Collins".

In 2014, Thornton starred as sociopathic hitman Lorne Malvo in the FX miniseries "Fargo", inspired by the 1996 film of the same name, for which he won a Golden Globe for Best Actor in a Mini-Series.

Thornton made a guest appearance on "The Big Bang Theory" in 2014, where he played a middle-aged urologist who gets excited about every woman who touches him.

"Goliath", a television series by Amazon Studios, features Thornton as a formerly brilliant and personable lawyer, who is now washed up and alcoholic. It premiered on October 13, 2016, on Amazon Video. On February 15, 2017, Amazon announced the series had been renewed for a second season.

In 2017, Thornton starred in the music video "Stand Down" by Kario Salem (musically known as K.O.). It received the Best Music Video award from the Toronto Shorts International Film Festival and has had 13 million views on Facebook and counting.

From the time he was 10 years old, Thornton has been in bands. His first performance was on drums at a school PTA meeting where his band played "The Ballad of The Green Berets" instrumentally. Several bands followed, with Thornton's first recording experience coming at Widget Sound in Muscle Shoals, Alabama in 1974. Later in the 1970s, Thornton was the drummer of a blues rock band named "Tres Hombres". Guitarist Billy Gibbons referred to the band as "the best little cover band in Texas", and Thornton bears a tattoo with the band's name on it.

In 1985, Thornton joined Piet Botha in the South African rock band Jack Hammer, while Botha worked in Los Angeles. Thornton recorded one studio album with Jack Hammer, "Death" "of" "a" "Gypsy", which was released in 1986.

In 2001, Thornton released an album titled "Private Radio" on Lost Highway Records. Subsequent albums include "The Edge of the World" (2003), "Hobo" (2005) and "Beautiful Door" (2007). He performed the Warren Zevon song "The Wind" on the tribute album "". Thornton recorded a cover of the Johnny Cash classic "Ring of Fire" with Earl Scruggs, for the "Oxford American" magazine's Southern Music CD in 2001. The song also appeared on Scruggs' 2001 album "Earl Scruggs and Friends".

In 2007, Thornton formed The Boxmasters with J.D. Andrew.


Thornton has been married six times. The first five marriages ended in divorce, and he has four children by three women.

From 1978 to 1980, he was married to Melissa Lee Gatlin, who in her divorce petition cited “incompatibility and adultery on his part”. They had a daughter, Amanda (Brumfield), who in 2008 was sentenced to 20 years in prison for the death of her friend's 1-year-old daughter.

Thornton married actress Toni Lawrence in 1986; they separated the following year and divorced in 1988.

From 1990 to 1992, he was married to actress Cynda Williams, whom he cast in his writing debut, "One False Move" (1992).

In 1993, Thornton married "Playboy" model Pietra Dawn Cherniak, with whom he had two sons, Harry James and William. The marriage ended in 1997, with Cherniak accusing Thornton of spousal abuse, sometimes in front of his children.

Thornton was engaged to be married to actress Laura Dern, whom he dated from 1997 to 1999, but in 2000, he married actress Angelina Jolie, with whom he starred in "Pushing Tin" (1999) and who was 20 years his junior. The marriage became known for the couple's eccentric displays of affection, which reportedly included wearing vials of each other's blood around their necks; Thornton later clarified that the "vials" were actually two small lockets, each containing only a single drop of blood. Thornton and Jolie announced the adoption of a child from Cambodia in March 2002, but it was later revealed that Jolie had adopted the child as a single parent. They separated in June 2002 and divorced the following year.

In 2003, Thornton began a relationship with makeup effects crew member Connie Angland, with whom he has a daughter named Bella. They reside in Los Angeles, California. Although he once said that he likely would not marry again, saying that he believes marriage "doesn't work" for him, his representatives confirmed that he and Angland were married on October 22, 2014, in Los Angeles.

During his early years in Los Angeles, Thornton was admitted to a hospital and diagnosed with myocarditis. He has since said that he follows a vegan diet and is "extremely healthy", eating no junk food as he is allergic to wheat and dairy.

Thornton suffers from OCD. Various idiosyncratic behaviors have been well documented in interviews with Thornton; among these is a phobia of antique furniture, a disorder shared by Dwight Yoakam's character Doyle Hargraves in the Thornton-penned "Sling Blade" and by Thornton's own character in the 2001 film "Bandits". Additionally, he has stated that he has a fear of certain types of silverware, a trait assumed by his character in 2001's "Monster's Ball", in which Grotowski insists on a plastic spoon for his daily bowl of ice cream.

In a 2004 interview with "The Independent", Thornton explained, "It's just that I won't use real silver. You know, like the big, old, heavy-ass forks and knives, I can't do that. It's the same thing as the antique furniture. I just don't like old stuff. I'm creeped out by it, and I have no explanation why ... I don't have a phobia about American antiques, it's mostly French—you know, like the big, old, gold-carved chairs with the velvet cushions. The Louis XIV type. That's what creeps me out. I can spot the imitation antiques a mile off. They have a different vibe. Not as much dust.".

Thornton is a baseball fan; his favorite team is the St. Louis Cardinals, and he has said that his childhood dream was to play for them. He narrated "The 2006 World Series Film", the year-end retrospective DVD chronicling the Cardinals' championship season. He is also a professed fan of the Indianapolis Colts football team.



</doc>
<doc id="4472" url="https://en.wikipedia.org/wiki?curid=4472" title="The Big O">
The Big O

The television series was designed as a tribute to Japanese and Western shows from the 1960s and 1970s. The series is presented in the style of "film noir" and combines themes of detective fiction and mecha anime. The setpieces are reminiscent of "tokusatsu" productions of the 1950s and 1960s, particularly Toho's "kaiju" movies, and the score is an eclectic mix of styles and musical homages.

"The Big O" aired on WOWOW satellite television from October 13, 1999, and January 19, 2000. The English-language version premiered on Cartoon Network on April 2, 2001, and ended on April 18, 2001. Originally planned as a 26-episode series, low viewership in Japan reduced production to the first 13. Positive international reception resulted in a second season consisting of the remaining 13 episodes; co-produced by Cartoon Network, Sunrise, and Bandai Visual. Season two premiered on Japan's SUN-TV on January 2, 2003, and the American premiere took place seven months later. Following the closure of Bandai Entertainment by parent company Bandai (owned by Bandai Namco Holdings) in 2012, Sunrise announced at Otakon 2013 that Sentai Filmworks rescued both seasons of "The Big O".

"The Big O" is set in the fictional city-state of . The city is located on a seacoast and is surrounded by a vast desert wasteland. The partially domed city is wholly controlled by the monopolistic Paradigm Corporation, resulting in a corporate police state. Paradigm is known as because of forty years prior to the story, " destroyed the world outside the city and left the survivors without any prior memories. The city is characterized by severe class inequity; the higher-income population resides inside the more pleasant domes, with the remainder left in tenements outside. Residents of the city believe that they are the last survivors of the world and no other nations exist outside the city. Androids and giant robots known as "Megadeus" coexist with the residents of Paradigm City and do not find them unusual.

After failing to negotiate with terrorists at the cost of his client's life, Roger Smith is obligated to care for Dorothy Wayneright, a young female android. Over the course of the series, Roger Smith continues to accept negotiation work from the residents of Paradigm City, he often leads to uncovering the nature and mystery of Paradigm City and encountering megadeus or other giant enemies that require Big O. Supporting characters are Angel, a mysterious woman in search of memories; Dan Dastun, chief of the military police of Paradigm city and old friend of Roger Smith; and Norman Burg, the butler of Roger Smith and mechanic of Big O.

The main antagonist is Alex Rosewater, chairman of Paradigm City whose goal is to revive the megadeus "Big Fau" in attempts to become the god of Paradigm City. Other recurring antagonists are Jason Beck, criminal and con-artist attempting to humiliate Roger Smith; Schwarzwald, an ex-reporter obsessed with finding the truth of Paradigm City and also pilot of the megadeus "Big Duo"; Vera Ronstadt, leader of a group of foreigners known as the Union searching for memories and revenge against Paradigm City; and Allen Gabriel, a cyborg assassin working for Alex Rosewater and the Union.

The series ends with the awakening of a new megadeus, and the revelation that the world is a simulated reality. A climactic battle ensues between Big O and Big Fau, after which reality is systematically erased by the new megadeus, an incarnation of Angel, recognized as "Big Venus" by Dorothy. Roger implores Angel to "let go of the past" regardless of its existential reality, and focus only on the present and the future. In an isolated control room, the real Angel observes Roger and her past encounters with him on a series of television monitors. On the control panel lies "Metropolis", a book featured prominently since the thirteenth episode, with the cover featuring an illustration of angel wings and gives the author's name as "Angel Rosewater". Big Venus and Big O physically merge, causing the virtual reality to reset. The final scene shows Roger Smith driving down a restored Paradigm city with Dorothy and Angel observing him from the side of the road.

Development of the retro-styled series began in 1996. Keiichi Sato came up with the concept of "The Big O": a giant city-smashing robot, piloted by a man in black, in a Gotham-like environment. He later met up with Kazuyoshi Katayama, who had just finished directing "Those Who Hunt Elves", and started work on the layouts and character designs. But when things "were about to really start moving," production on Katayama's "Sentimental Journey" began, putting plans on hold. Meanwhile, Sato was heavily involved with his work on "City Hunter".

Sato admits it all started as "a gimmick for a toy" but the representatives at Bandai Hobby Division did not see the same potential. From there on, the dealings would be with Bandai Visual, but Sunrise still needed some safeguards and requested more robots be designed to increase prospective toy sales. In 1999, with the designs complete, Chiaki J. Konaka was brought on as head writer. Among other things, Konaka came up with the idea of "a town without memory" and his writing staff put together the outline for a 26-episodes series.

"The Big O" premiered on 13 October 1999 on WOWOW. When the production staff was informed the series would be shortened to 13 episodes, the writers decided to end it with a cliffhanger, hoping the next 13 episodes would be picked up. In April 2001, "The Big O" premiered on Cartoon Network's Toonami lineup.

The series garnered positive fan response internationally that resulted in a second season co-produced by Cartoon Network and Sunrise. Season two premiered on Japan's SUN-TV on January 2003, with the American premiere taking place seven months later as an Adult Swim exclusive. The second season would not be seen on Toonami until July 27, 2013, 10 years after it began airing on Adult Swim.

The second season was scripted by Chiaki Konaka with input from the American producers. Cartoon Network raised two requests for the second season: more action and reveal the mystery in the first season, although Kazuyoshi Katayama admitted that he didn't intend to reveal it, just to make an anthology of adventures setting in the universe. Along with the 13 episodes of season two, Cartoon Network had an option for 26 additional episodes to be written by Konaka, but according to Jason DeMarco, executive producer for season two, the middling ratings and DVD sales in the United States and Japan made any further episodes impossible to be produced.

Following the closure of Bandai Entertainment by parent company in 2012, Sunrise announced at Otakon 2013 that Sentai Filmworks rescued both seasons of "The Big O". On June 20, 2017, Sentai Filmworks released both seasons on Blu-ray.

"The Big O" was scored by "Geidai" alumnus Toshihiko Sahashi. His composition is richly symphonic and classical, with a number of pieces delving into electronica and jazz. Chosen because of his "frightening amount of musical knowledge about TV dramas overseas," Sahashi integrates musical homages into the soundtrack. The background music draws from "film noir", spy films and sci-fi television series like "The Twilight Zone". The battle themes are reminiscent of Akira Ifukube's compositions for the "Godzilla" series.

The first opening theme is the Queen-influenced "Big-O!". Composed, arranged and performed by Rui Nagai, the song resembles the theme to the "Flash Gordon" film. The second opening theme is "Respect," composed by Sahashi. The track is an homage to the music of Gerry Anderson's "UFO", composed by Barry Gray. In 2007, Rui Nagai composed "Big-O! Show Must Go On," a 1960s hard rock piece, for Animax's reruns of the show. The closing theme is the slow love ballad "And Forever..." written by Chie and composed by Ken Shima. The duet is performed by Robbie Danzie and Naoki Takao.

Along with Sahashi's original compositions, the soundtrack features Chopin's Prelude No. 15 and a jazz saxophone rendition of "Jingle Bells." The complete score was released in two volumes by Victor Entertainment.

"The Big O" is the brainchild of Keiichi Sato and Kazuyoshi Katayama, an homage to the shows they grew up with. The show references the works of "tokusatsu" produced by the Toei Company and Tsuburaya Productions, as well as shows such as "Super Robot Red Baron" and "Super Robot Mach Baron" and "old school" super robot anime. The series is done in the style of "film noir" and pulp fiction and combines the feel of a detective show with the giant robot genre.

"The Big O" shares much of its themes, diction, archetypes and visual iconography with "film noirs" of the 1940s like "The Big Sleep" (1946). The series incorporates the use of long dark shadows in the tradition of "chiaroscuro" and tenebrism. "Film noir" is also known for its use of odd angles, such as Roger's low shot introduction in the first episode. "Noir" cinematographers favoured this angle because it made characters almost rise from the ground, giving them dramatic girth and symbolic overtones. Other disorientating devices like dutch angles, mirror reflection and distorting shots are employed throughout the series.

The characters of "The Big O" fit the "noir" and pulp fiction archetypes. Roger Smith is a protagonist in the mold of Chandler's Philip Marlowe or Hammett's Sam Spade. He is canny and cynical, a disillusioned cop-turned-negotiator whose job has more in common with detective-style work than negotiating. Big Ear is Roger's street informant and Dan Dastun is the friend on the police force. The recurring Beck is the imaginative thug compelled by delusions of grandeur while Angel fills the role of the "femme fatale". Minor characters include crooked cops, corrupt business men and deranged scientists.

The dialogue in the series is recognized for its witty, wry sense of humor. The characters come off as charming and exchange banter not often heard in anime series, as the dialogue has the tendency to be straightforward. The plot is moved along by Roger's voice-over narration, a device used in "film noir" to place the viewer in the mind of the protagonist so it can intimately experience the character's angst and partly identify with the narrator.

The tall buildings and giant domes create a sense of claustrophobia and paranoia characteristic of the style. The rural landscape, Ailesberry Farm, contrasts Paradigm City. "Noir" protagonists often look for sanctuary in such settings but they just as likely end up becoming a killing ground. The series score is representative of its setting. While no classic "noir" possesses a jazz score, the music could be heard in nightclubs within the films. Roger's recurring theme, a lone saxophone accompaniment to the protagonist's narration, best exemplifies the "noir" stylings of the series.

Amnesia is a common plot device in "film noir". Because most of these stories focused on a character proving his innocence, authors up the ante by making him an amnesiac, unable to prove his innocence even to himself.

Before "The Big O", Sunrise was a subcontractor for Warner Bros. Animation's "", one of the series' influences. Cartoon Network, under the Toonami flag advertised the series as "One part Bond. One part Bruce Wayne. One part City Smashing Robot."

Roger Smith is a pastiche of the Bruce Wayne persona and the Batman. The character design resembles Wayne, complete with slicked-back hair and double-breasted business suit. Like Bruce, Roger prides himself in being a rich playboy to the extent that one of his household's rules is only women may be let into his mansion without his permission. Like Batman, Roger Smith carries a no-gun policy, albeit more flexible. Unlike the personal motives of the Batman, Roger enforces this rule for "it's all part of being a gentleman." Among Roger's gadgetry is the Griffon, a large, black hi-tech sedan comparable to the Batmobile, a grappling cable that shoots out his wristwatch and the giant robot that Angel calls "Roger's alter ego."

"The Big O"'s cast of supporting characters includes Norman, Roger's faithful mechanically-inclined butler who fills the role of Alfred Pennyworth; R. Dorothy Wayneright, who plays the role of the sidekick; and Dan Dastun, a good honest cop who, like Jim Gordon, is both a friend to the hero and greatly respected by his comrades.

The other major influence is Mitsuteru Yokoyama's "Giant Robo". Before working on "The Big O", Kazuyoshi Katayama and other animators worked with Yasuhiro Imagawa on "". The feature, a "retro chic" homage to Yokoyama's career, took seven years to produce and suffered low sales and high running costs. Frustrated by the experience, Katayama and his staff put all their efforts into making "good" with "The Big O".

Like Giant Robo, the megadeuses of "Big O" are metal behemoths. The designs are strange and "more macho than practical," sporting big stovepipe arms and exposed rivets. Unlike the giants of other mecha series, the megadeuses do not exhibit ninja-like speed nor grace. Instead, the robots are armed with "old school" weaponry such as missiles, piston powered punches, machine guns and laser cannons.

Katayama also cited "Super Robot Red Baron" and "Super Robot Mach Baron" among influences on the inspiration of "The Big O". Believing that because "Red Baron" had such a low budget and the big fights always happened outside of a city setting, he wanted "Big O" to be the show he felt "Red Baron" could be with a bigger budget. He also spoke of how he first came up with designs for the robots first as if they were making designs to appeal to toy companies, rather than how "Gundam" was created with a toy company wanting an anime to represent their new product. Big O's large pumping piston "Sudden Impact" arms, for example, he felt would be cool gimmicks in a toy.

"The Big O" was conceived as a media franchise. To this effect, Sunrise requested a manga be produced along with the animated series. "The Big O" manga started serialization in Kodansha's "Magazine Z" on July 1999, three months before the anime premiere. Authored by Hitoshi Ariga, the manga uses Keiichi Sato's concept designs in an all-new story. The series ended on October 2001. The issues were later collected in six volumes. The English version of the manga is published by Viz Media.

In anticipation of the broadcast of the second season, a new manga series was published. , authored by Hitoshi Ariga. "Lost Memory" takes place between volumes five and six of the original manga. The issues were serialized in "Magazine Z" from November 2002 to September 2003 and were collected in two volumes. , a novel by Yuki Taniguchi, was released 16 July 2003 by Tokuma Shoten.

"The Big O Visual: The official companion to the TV series" () was published by Futabasha in 2003. The book contains full-color artwork, character bios and concept art, mecha sketches, video/LD/DVD jacket illustrations, history on the making of The Big O, staff interviews, "Roger's Monologues" comic strip and the original script for the final episode of the series.

"Walking Together On The Yellow Brick Road" was released by Victor Entertainment on 21 September 2000. The drama CD was written by series head writer Chiaki J. Konaka and featured the series' voice cast. An English translation, written by English dub translator David Fleming, was posted on Konaka's website.

The first season of Big O is featured in "Super Robot Wars D" for the Game Boy Advance in 2003. The series, including its second season, is also featured in "Super Robot Wars Z", released in 2008. "The Big O" became a mainstay of the "Z" games, appearing in each entry of the subseries.

Bandai released a non-scale model kit of Big O in 2000. Though it was an easy snap-together kit, it required painting, as all of the parts (except the clear orange crown and canopy) were molded in dark gray. The kit included springs that enabled the slide-action Side Piles on the forearms to simulate Big O's Sudden Impact maneuver. Also included was an unpainted Roger Smith figure.

PVC figures of Big O and Big Duo (Schwarzwald's Megadeus) were sold by Bandai America. Each came with non-poseable figures of Roger, Dorothy and Angel. Mini-figure sets were sold in Japan and America during the run of the second season. The characters included Big O (standard and attack modes), Roger, Dorothy & Norman, Griffon (Roger's car), Dorothy-1 (Big O's first opponent), Schwarzwald and Big Duo.

In 2009, Bandai released a plastic/diecast figure of the Big O under their Soul of Chogokin line. The figure has the same features as the model kit, but with added detail and accessories. Its design was closely supervised by original designer Keiichi Sato.

In 2011, Max Factory released action figures of Roger and Dorothy through their Figma toyline. Like most Figmas, they are very detailed, articulated and come with accessories and interchangeable faces. In the same year, Max Factory also released a 12-inch, diecast figure of Big O under their Max Gokin line. The figure contained most of the accessories as the Soul of Chogokin figure but also included some others that could be bought separately from the SOC figure, such as the Mobydick (hip) Anchors and Roger Smith's car: the Griffon. Like the Soul of Chogokin figure, its design was also supervised by Keiichi Sato. As well, in that same year, Max Factory released soft vinyl figures of Big Duo and Big Fau, in-scale with the Max Gokin Big O. These figures are high in detail but limited in articulation, such as the arms and legs being the only things to move. To date, this is the only action figure of Big Fau.

"The Big O" premiered on 13 October 1999. The show was not a hit in its native Japan, rather it was reduced from an outlined 26 episodes to 13 episodes. Western audiences were more receptive and the series achieved the success its creators were looking for. In an interview with AnimePlay, Keiichi Sato said "This is exactly as we had planned", referring to the success overseas.

Several words appear constantly in the English-language reviews; adjectives like "hip", "sleek," "stylish",

The first season's reception was positive. Anime on DVD recommends it as an essential series. Chris Beveridge of the aforementioned site gave an A− to Vols. 1 and 2, and a B+ to Vols. 3 and 4. Mike Toole of Anime Jump gave it 4.5 (out of a possible 5) stars, while the review at the Anime Academy gave it a grade of 83, listing the series' high points as being "unique", the characters "interesting," and the action "nice." Reviewers, and fans alike, agree the season's downfall was the ending, or its lack thereof. The dangling plot threads frustrated the viewers and prompted Cartoon Network's involvement in the production of further episodes.

The look and feel of the show received a big enhancement in the second season. This time around, the animation is "near OVA quality" and the artwork "far more lush and detailed." Also enhanced are the troubles of the first season. The giant robot battles still seem out of place to some, while others praise the "over-the-top-ness" of their execution.

For some reviewers, the second season "doesn't quite match the first" addressing to "something" missing in these episodes. Andy Patrizio of IGN points out changes in Roger Smith's character, who "lost some of his cool and his very funny side in the second season." Like a repeat of season one, this season's ending is considered its downfall. Chris Beveridge of Anime on DVD wonders if this was head writer "Konaka's attempt to throw his hat into the ring for creating one of the most confusing and oblique endings of any series." Patrizio states "the creators watched "The Truman Show" and "The Matrix" a few times too many."

The series continues to have a strong cult following into the 2010s. In 2014 BuzzFeed writer Ryan Broderick ranked "The Big O" as one of the best anime series to binge-watch. Dan Casey host of The Nerdist's "Dan Cave" stated "The Big O" was the anime series he was most eager to see rebooted or remade, along with "Trigun" and "Soul Eater". In 2017, Ollie Barder of Forbes wrote,"From the classic and retro styled mecha design of Keiichi Sato to the overall film noir visual tone of the series, The Big O was a fascinating and visually very different kind of show. It also had a fantastic voice cast, with probably the most notable of these being Akiko Yajima as the voice of Roger's disapproving android Dorothy." In 2019, Crunchyroll writer Thomas Zoth ranked "The Big O" as his top 10 anime since the 1990s.



</doc>
<doc id="4473" url="https://en.wikipedia.org/wiki?curid=4473" title="BIOS">
BIOS

BIOS (pronounced: , ; an acronym for Basic Input/Output System and also known as the System BIOS, ROM BIOS or PC BIOS) is firmware used to perform hardware initialization during the booting process (power-on startup), and to provide runtime services for operating systems and programs. The BIOS firmware comes pre-installed on a personal computer's system board, and it is the first software to run when powered on. The name originates from the Basic Input/Output System used in the CP/M operating system in 1975. The BIOS originally proprietary to the IBM PC has been reverse engineered by companies looking to create compatible systems. The interface of that original system serves as a "de facto" standard.

The BIOS in modern PCs initializes and tests the system hardware components, and loads a boot loader from a mass memory device which then initializes an operating system. In the era of DOS, the BIOS provided a hardware abstraction layer for the keyboard, display, and other input/output (I/O) devices that standardized an interface to application programs and the operating system. More recent operating systems do not use the BIOS after loading, instead accessing the hardware components directly.

Most BIOS implementations are specifically designed to work with a particular computer or motherboard model, by interfacing with various devices that make up the complementary system chipset. Originally, BIOS firmware was stored in a ROM chip on the PC motherboard. In modern computer systems, the BIOS contents are stored on flash memory so it can be rewritten without removing the chip from the motherboard. This allows easy, end-user updates to the BIOS firmware so new features can be added or bugs can be fixed, but it also creates a possibility for the computer to become infected with BIOS rootkits. Furthermore, a BIOS upgrade that fails can brick the motherboard permanently, unless the system includes some form of backup for this case.

Unified Extensible Firmware Interface (UEFI) is a successor to the legacy PC BIOS, aiming to address its technical limitations.

The term BIOS (Basic Input/Output System) was created by Gary Kildall and first appeared in the CP/M operating system in 1975, describing the machine-specific part of CP/M loaded during boot time that interfaces directly with the hardware. (A CP/M machine usually has only a simple boot loader in its ROM.)

Versions of MS-DOS, PC DOS or DR-DOS contain a file called variously "IO.SYS", "IBMBIO.COM", "IBMBIO.SYS", or "DRBIOS.SYS"; this file is known as the "DOS BIOS" (also known as the "DOS I/O System") and contains the lower-level hardware-specific part of the operating system. Together with the underlying hardware-specific but operating system-independent "System BIOS", which resides in ROM, it represents the analogue to the "CP/M BIOS".

With the introduction of PS/2 machines, IBM divided the System BIOS into real- and protected-mode portions. The real-mode portion was meant to provide backward compatibility with existing operating systems such as DOS, and therefore was named "CBIOS" (for "Compatibility BIOS"), whereas the "ABIOS" (for "Advanced BIOS") provided new interfaces specifically suited for multitasking operating systems such as OS/2.

The BIOS of the original IBM PC and XT had no interactive user interface. Error codes or messages were displayed on the screen, or coded series of sounds were generated to signal errors when the power-on self-test (POST) had not proceeded to the point of successfully initializing a video display adapter. Options on the IBM PC and XT were set by switches and jumpers on the main board and on expansion cards. Starting around the mid-1990s, it became typical for the BIOS ROM to include a ""BIOS configuration utility"" (BCU) or "BIOS setup utility", accessed at system power-up by a particular key sequence. This program allowed the user to set system configuration options, of the type formerly set using DIP switches, through an interactive menu system controlled through the keyboard. In the interim period, IBM-compatible PCsincluding the IBM ATheld configuration settings in battery-backed RAM and used a bootable configuration program on disk, not in the ROM, to set the configuration options contained in this memory. The disk was supplied with the computer, and if it was lost the system settings could not be changed. The same applied in general to computers with an EISA bus, for which the configuration program was called an EISA Configuration Utility (ECU).

A modern Wintel-compatible computer provides a setup routine essentially unchanged in nature from the ROM-resident BIOS setup utilities of the late 1990s; the user can configure hardware options using the keyboard and video display. Also, when errors occur at boot time, a modern BIOS usually displays user-friendly error messages, often presented as pop-up boxes in a TUI style, and offers to enter the BIOS setup utility or to ignore the error and proceed if possible. Instead of battery-backed RAM, the modern Wintel machine may store the BIOS configuration settings in flash ROM, perhaps the same flash ROM that holds the BIOS itself.

Early Intel processors started at physical address 000FFFF0h. Systems with later processors provide logic to start running the BIOS from the system ROM. 
If the system has just been powered up or the reset button was pressed ("cold boot"), the full power-on self-test (POST) is run. If Ctrl+Alt+Delete was pressed ("warm boot"), a special flag value stored in nonvolatile BIOS memory ("CMOS") tested by the BIOS allows bypass of the lengthy POST and memory detection.

The POST identifies, and initializes system devices such as the CPU, RAM, interrupt and DMA controllers and other parts of the chipset, video display card, keyboard, hard disk drive, optical disc drive and other basic hardware.

Early IBM PCs had a routine in the POST that would download a program into RAM through the keyboard port and run it. This feature was intended for factory test or diagnostic purposes.

After the option ROM scan is completed and all detected ROM modules with valid checksums have been called, or immediately after POST in a BIOS version that does not scan for option ROMs, the BIOS calls INT 19h to start boot processing. Post-boot, programs loaded can also call INT 19h to reboot the system, but they must be careful to disable interrupts and other asynchronous hardware processes that may interfere with the BIOS rebooting process, or else the system may hang or crash while it is rebooting.

When INT 19h is called, the BIOS attempts to locate boot loader software on a "boot device", such as a hard disk, a floppy disk, CD, or DVD. It loads and executes the first boot software it finds, giving it control of the PC.

The BIOS uses the boot devices set in EEPROM, CMOS RAM or, in the earliest PCs, DIP switches. The BIOS checks each device in order to see if it is bootable by attempting to load the first sector (boot sector). If the sector cannot be read, the BIOS proceeds to the next device. If the sector is read successfully, some BIOSes will also check for the boot sector signature 0x55 0xAA in the last two bytes of the sector (which is 512 bytes long), before accepting a boot sector and considering the device bootable.

When a bootable device is found, the BIOS transfers control to the loaded sector. The BIOS does not interpret the contents of the boot sector other than to possibly check for the boot sector signature in the last two bytes. Interpretation of data structures like partition tables and BIOS Parameter Blocks is done by the boot program in the boot sector itself or by other programs loaded through the boot process.

A non-disk device such as a network adapter attempts booting by a procedure that is defined by its option ROM or the equivalent integrated into the motherboard BIOS ROM. As such, option ROMs may also influence or supplant the boot process defined by the motherboard BIOS ROM.

The user can select the boot priority implemented by the BIOS. For example, most computers have a hard disk that is bootable, but usually there is a removable-media drive that has higher boot priority, so the user can cause a removable disk to be booted.

In most modern BIOSes, the boot priority order can be configured by the user. In older BIOSes, limited boot priority options are selectable; in the earliest BIOSes, a fixed priority scheme was implemented, with floppy disk drives first, fixed disks (i.e. hard disks) second, and typically no other boot devices supported, subject to modification of these rules by installed option ROMs. The BIOS in an early PC also usually would only boot from the first floppy disk drive or the first hard disk drive, even if there were two drives installed.

With the El Torito optical media boot standard, the optical drive actually emulates a 3.5" high-density floppy disk to the BIOS for boot purposes. Reading the "first sector" of a CD-ROM or DVD-ROM is not a simply defined operation like it is on a floppy disk or a hard disk. Furthermore, the complexity of the medium makes it difficult to write a useful boot program in one sector. The bootable virtual floppy disk can contain software that provides access to the optical medium in its native format.

On the original IBM PC and XT, if no bootable disk was found, ROM BASIC was started by calling INT 18h. Since few programs used BASIC in ROM, clone PC makers left it out; then a computer that failed to boot from a disk would display "No ROM BASIC" and halt (in response to INT 18h).

Later computers would display a message like "No bootable disk found"; some would prompt for a disk to be inserted and a key to be pressed to retry the boot process. A modern BIOS may display nothing or may automatically enter the BIOS configuration utility when the boot process fails.

The environment for the boot program is very simple: the CPU is in real mode and the general-purpose and segment registers are undefined, except SS, SP, CS, and DL. CS:IP always points to physical address codice_1. What values CS and IP actually have is not well defined. Some BIOSes use a CS:IP of codice_2 while others may use codice_3. Because boot programs are always loaded at this fixed address, there is no need for a boot program to be relocatable. DL may contain the drive number, as used with INT 13h, of the boot device. SS:SP points to a valid stack that is presumably large enough to support hardware interrupts, but otherwise SS and SP are undefined. (A stack must be already set up in order for interrupts to be serviced, and interrupts must be enabled in order for the system timer-tick interrupt, which BIOS always uses at least to maintain the time-of-day count and which it initializes during POST, to be active and for the keyboard to work. The keyboard works even if the BIOS keyboard service is not called; keystrokes are received and placed in the 15-character type-ahead buffer maintained by BIOS.) The boot program must set up its own stack, because the size of the stack set up by BIOS is unknown and its location is likewise variable; although the boot program can investigate the default stack by examining SS:SP, it is easier and shorter to just unconditionally set up a new stack.

At boot time, all BIOS services are available, and the memory below address codice_4 contains the interrupt vector table. BIOS POST has initialized the system timers, interrupt controller(s), DMA controller(s), and other motherboard/chipset hardware as necessary to bring all BIOS services to ready status. DRAM refresh for all system DRAM in conventional memory and extended memory, but not necessarily expanded memory, has been set up and is running. The interrupt vectors corresponding to the BIOS interrupts have been set to point at the appropriate entry points in the BIOS, hardware interrupt vectors for devices initialized by the BIOS have been set to point to the BIOS-provided ISRs, and some other interrupts, including ones that BIOS generates for programs to hook, have been set to a default dummy ISR that immediately returns. The BIOS maintains a reserved block of system RAM at addresses codice_5 with various parameters initialized during the POST. All memory at and above address codice_6 can be used by the boot program; it may even overwrite itself.

Peripheral cards such as some hard disk drive controllers and some video display adapters have their own BIOS extension option ROMs, which provide additional functionality to BIOS. Code in these extensions runs before the BIOS boots the system from mass storage. These ROMs typically test and initialize hardware, add new BIOS services, and augment or replace existing BIOS services with their own versions of those services. For example, a SCSI controller usually has a BIOS extension ROM that adds support for hard drives connected through that controller. Some video cards have extension ROMs that replace the video services of the motherboard BIOS with their own video services. BIOS extension ROMs gain total control of the machine, so they can in fact do anything, and they may never return control to the BIOS that invoked them. An extension ROM could in principle contain an entire operating system or an application program, or it could implement an entirely different boot process such as booting from a network. Operation of an IBM-compatible computer system can be completely changed by removing or inserting an adapter card (or a ROM chip) that contains a BIOS extension ROM.

The motherboard BIOS typically contains code to access hardware components necessary for bootstrapping the system, such as the keyboard, display, and storage. In addition, plug-in adapter cards such as SCSI, RAID, network interface cards, and video boards often include their own BIOS (e.g. Video BIOS), complementing or replacing the system BIOS code for the given component. Even devices built into the motherboard can behave in this way; their option ROMs can be stored as separate code on the main BIOS flash chip, and upgraded either in tandem with, or separately from, the main BIOS.

An add-in card requires an option ROM if the card is not supported by the main BIOS and the card needs to be initialized or made accessible through BIOS services before the operating system can be loaded (usually this means it is required in the bootstrapping process). Even when it is not required, an option ROM can allow an adapter card to be used without loading driver software from a storage device after booting begins with an option ROM, no time is taken to load the driver, the driver does not take up space in RAM nor on hard disk, and the driver software on the ROM always stays with the device so the two cannot be accidentally separated. Also, if the ROM is on the card, both the peripheral hardware and the driver software provided by the ROM are installed together with no extra effort to install the software. An additional advantage of ROM on some early PC systems (notably including the IBM PCjr) was that ROM was faster than main system RAM. (On modern systems, the case is very much the reverse of this, and BIOS ROM code is usually copied ("shadowed") into RAM so it will run faster.)

There are many methods and utilities for examining the contents of various motherboard BIOS and expansion ROMs, such as Microsoft DEBUG or the Unix dd.

If an expansion ROM wishes to change the way the system boots (such as from a network device or a SCSI adapter for which the BIOS has no driver code) in a cooperative way, it can use the "BIOS Boot Specification" (BBS) API to register its ability to do so. Once the expansion ROMs have registered using the BBS APIs, the user can select among the available boot options from within the BIOS's user interface. This is why most BBS compliant PC BIOS implementations will not allow the user to enter the BIOS's user interface until the expansion ROMs have finished executing and registering themselves with the BBS API. The specification can be downloaded from the "ACPI" (Advanced Configuration and Power Interface) "Component Architecture" website. The official title is BIOS Boot Specification (Version 1.01, 11 January 1996).

Also, if an expansion ROM wishes to change the way the system boots unilaterally, it can simply hook INT 19h or other interrupts normally called from interrupt 19h, such as INT 13h, the BIOS disk service, to intercept the BIOS boot process. Then it can replace the BIOS boot process with one of its own, or it can merely modify the boot sequence by inserting its own boot actions into it, by preventing the BIOS from detecting certain devices as bootable, or both. Before the BIOS Boot Specification was promulgated, this was the only way for expansion ROMs to implement boot capability for devices not supported for booting by the native BIOS of the motherboard.

After the motherboard BIOS completes its POST, most BIOS versions search for option ROM modules, also called BIOS extension ROMs, and execute them. The motherboard BIOS scans for extension ROMs in a portion of the "upper memory area" (the part of the x86 real-mode address space at and above address 0xA0000) and runs each ROM found, in order. To discover memory-mapped ISA option ROMs, a BIOS implementation scans the real-mode address space from codice_7 to codice_8 on 2 KiB boundaries, looking for a two-byte ROM "signature": 0x55 followed by 0xAA. In a valid expansion ROM, this signature is followed by a single byte indicating the number of 512-byte blocks the expansion ROM occupies in real memory, and the next byte is the option ROM's entry point (also known as its "entry offset"). A checksum of the specified number of 512-byte blocks is calculated, and if the ROM has a valid checksum, the BIOS transfers control to the entry address, which in a normal BIOS extension ROM should be the beginning of the extension's initialization routine.

At this point, the extension ROM code takes over, typically testing and initializing the hardware it controls and registering interrupt vectors for use by post-boot applications. It may use BIOS services (including those provided by previously initialized option ROMs) to provide a user configuration interface, to display diagnostic information, or to do anything else that it requires. It is possible that an option ROM will not return to BIOS, pre-empting the BIOS's boot sequence altogether.

An option ROM should normally return to the BIOS after completing its initialization process. Once (and if) an option ROM returns, the BIOS continues searching for more option ROMs, calling each as it is found, until the entire option ROM area in the memory space has been scanned.

Option ROMs normally reside on adapter cards. However, the original PC, and perhaps also the PC XT, have a spare ROM socket on the motherboard (the "system board" in IBM's terms) into which an option ROM can be inserted, and the four ROMs that contain the BASIC interpreter can also be removed and replaced with custom ROMs which can be option ROMs. The IBM PCjr is unique among PCs in having two ROM cartridge slots on the front. Cartridges in these slots map into the same region of the upper memory area used for option ROMs, and the cartridges can contain option ROM modules that the BIOS would recognize. The cartridges can also contain other types of ROM modules, such as BASIC programs, that are handled differently. One PCjr cartridge can contain several ROM modules of different types, possibly stored together in one ROM chip.

The BIOS ROM is customized to the particular manufacturer's hardware, allowing low-level services (such as reading a keystroke or writing a sector of data to diskette) to be provided in a standardized way to programs, including operating systems. For example, an IBM PC might have either a monochrome or a color display adapter (using different display memory addresses and hardware), but a single, standard, BIOS system call may be invoked to display a character at a specified position on the screen in text mode or graphics mode.

The BIOS provides a small library of basic input/output functions to operate peripherals (such as the keyboard, rudimentary text and graphics display functions and so forth). When using MS-DOS, BIOS services could be accessed by an application program (or by MS-DOS) by executing an INT 13h interrupt instruction to access disk functions, or by executing one of a number of other documented BIOS interrupt calls to access video display, keyboard, cassette, and other device functions.

Operating systems and executive software that are designed to supersede this basic firmware functionality provide replacement software interfaces to application software. Applications can also provide these services to themselves. This began even in the 1980s under MS-DOS, when programmers observed that using the BIOS video services for graphics display was very slow. To increase the speed of screen output, many programs bypassed the BIOS and programmed the video display hardware directly. Other graphics programmers, particularly but not exclusively in the demoscene, observed that there were technical capabilities of the PC display adapters that were not supported by the IBM BIOS and could not be taken advantage of without circumventing it. Since the AT-compatible BIOS ran in Intel real mode, operating systems that ran in protected mode on 286 and later processors required hardware device drivers compatible with protected mode operation to replace BIOS services.

In modern personal computers running modern operating systems the BIOS is used only during booting and initial loading of system software. Before the operating system's first graphical screen is displayed, input and output are typically handled through BIOS. A boot menu such as the textual menu of Windows, which allows users to choose an operating system to boot, to boot into the safe mode, or to use the last known good configuration, is displayed through BIOS and receives keyboard input through BIOS.

Most modern PCs can still boot and run legacy operating systems such as MS-DOS or DR-DOS that rely heavily on BIOS for their console and disk I/O, providing that the system has a BIOS or BIOS-compatible firmware, which is not necessarily the case with UEFI-based PCs.

Intel processors have reprogrammable microcode since the P6 microarchitecture. The BIOS may contain patches to the processor microcode that fix errors in the initial processor microcode; reprogramming is not persistent, thus loading of microcode updates is performed each time the system is powered up. Without reprogrammable microcode, an expensive processor swap would be required; for example, the Pentium FDIV bug became an expensive fiasco for Intel as it required a product recall because the original Pentium processor's defective microcode could not be reprogrammed.

Some BIOSes contain a software licensing description table (SLIC), a digital signature placed inside the BIOS by the original equipment manufacturer (OEM), for example Dell. The SLIC is inserted into the ACPI data table and contains no active code.

Computer manufacturers that distribute OEM versions of Microsoft Windows and Microsoft application software can use the SLIC to authenticate licensing to the OEM Windows Installation disk and system recovery disc containing Windows software. Systems with a SLIC can be preactivated with an OEM product key, and they verify an XML formatted OEM certificate against the SLIC in the BIOS as a means of self-activating (see System Locked Preinstallation, SLP). If a user performs a fresh install of Windows, they will need to have possession of both the OEM key (either SLP or COA) and the digital certificate for their SLIC in order to bypass activation. This can be achieved if the user performs a restore using a pre-customised image provided by the OEM. Power users can copy the necessary certificate files from the OEM image, decode the SLP product key, then perform SLP activation manually. Cracks for non-genuine Windows distributions usually edit the SLIC or emulate it in order to bypass Windows activation.

Some BIOS implementations allow overclocking, an action in which the CPU is adjusted to a higher clock rate than its manufacturer rating for guaranteed capability. Overclocking may, however, seriously compromise system reliability in insufficiently cooled computers and generally shorten component lifespan. Overclocking, when incorrectly performed, may also cause components to overheat so quickly that they mechanically destroy themselves.

Some older operating systems, for example MS-DOS, rely on the BIOS to carry out most input/output tasks within the PC.

Because the BIOS still runs in 16-bit real mode, calling BIOS services directly is inefficient for protected-mode operating systems. BIOS services are not used by modern multitasking operating systems after they initially load, so the importance of the primary part of BIOS is greatly reduced from what it was initially.

Later BIOS implementations took on more complex functions, by including interfaces such as Advanced Configuration and Power Interface (ACPI). Functions of ACPI include power management, interrupt management, hot swapping, and thermal management. After operating systems load, the System Management Mode code is still running in SMRAM. Since 2010, BIOS technology is in a transitional process toward UEFI.

Historically, the BIOS in the IBM PC and XT had no built-in user interface. The BIOS versions in earlier PCs (XT-class) were not software configurable; instead, users set the options via DIP switches on the motherboard. Later computers, including all IBM-compatibles with 80286 CPUs, had a battery-backed nonvolatile BIOS memory (CMOS RAM chip) that held BIOS settings. These settings, such as video-adapter type, memory size, and hard-disk parameters, could only be configured by running a configuration program from a disk, not built into the ROM. A special "reference diskette" was inserted in an IBM AT to configure settings such as memory size.

Early BIOS versions did not have passwords or boot-device selection options. The BIOS was hard-coded to boot from the first floppy drive, or, if that failed, the first hard disk. Access control in early AT-class machines was by a physical keylock switch (which was not hard to defeat if the computer case could be opened). Anyone who could switch on the computer could boot it.

Later, 386-class computers started integrating the BIOS setup utility in the ROM itself, alongside the BIOS code; these computers usually boot into the BIOS setup utility if a certain key or key combination is pressed, otherwise the BIOS POST and boot process are executed.

A modern BIOS setup utility has a menu-based user interface (UI) accessed by pressing a certain key on the keyboard when the PC starts. Usually, the key is advertised for short time during the early startup, for example "Press F1 to enter CMOS setup". The actual key depends on specific hardware. Features present in the BIOS setup utility typically include:


A modern BIOS setup screen often features a PC Health Status or a Hardware Monitoring tab, which directly interfaces with a Hardware Monitor chip of the mainboard. This makes it possible to monitor CPU and chassis temperature, the voltage provided by the power supply unit, as well as monitor and control the speed of the fans connected to the motherboard.

Once the system is booted, hardware monitoring and computer fan control is normally done directly by the Hardware Monitor chip itself, which can be a separate chip, interfaced through I²C or SMBus, or come as a part of a Super I/O solution, interfaced through Low Pin Count (LPC). Some operating systems, like NetBSD with envsys and OpenBSD with sysctl hw.sensors, feature integrated interfacing with hardware monitors, which is normally done without any interaction with the BIOS.

However, in certain circumstances, the BIOS vendor also provides the underlying information about hardware monitoring through ACPI, in which case, the operating system may be using ACPI to perform hardware monitoring; this is done, for example, on some ASUSTeK motherboards with the AI Booster feature.

In modern PCs the BIOS is stored in rewritable memory, allowing the contents to be replaced and modified. This rewriting of the contents is sometimes termed "flashing", based on the common use of a kind of EEPROM known technically as "flash EEPROM" and colloquially as "flash memory". It can be done by a special program, usually provided by the system's manufacturer, or at POST, with a BIOS image in a hard drive or USB flash drive. A file containing such contents is sometimes termed "a BIOS image". A BIOS might be reflashed in order to upgrade to a newer version to fix bugs or provide improved performance or to support newer hardware, or a reflashing operation might be needed to fix a damaged BIOS

The original IBM PC BIOS (and cassette BASIC) was stored on mask-programmed read-only memory (ROM) chips in sockets on the motherboard. ROMs could be replaced, but not altered, by users. To allow for updates, many compatible computers used re-programmable memory devices such as EPROM and later flash memory devices. According to Robert Braver, the president of the BIOS manufacturer Micro Firmware, Flash BIOS chips became common around 1995 because the electrically erasable PROM (EEPROM) chips are cheaper and easier to program than standard ultraviolet erasable PROM (EPROM) chips. Flash chips are programmed (and re-programmed) in-circuit, while EPROM chips need to be removed from the motherboard for re-programming. BIOS versions are upgraded to take advantage of newer versions of hardware and to correct bugs in previous revisions of BIOSes.

Beginning with the IBM AT, PCs supported a hardware clock settable through BIOS. It had a century bit which allowed for manually changing the century when the year 2000 happened. Most BIOS revisions created in 1995 and nearly all BIOS revisions in 1997 supported the year 2000 by setting the century bit automatically when the clock rolled past midnight, December 31, 1999.

The first flash chips were attached to the ISA bus. Starting in 1997, the BIOS flash moved to the LPC bus, a functional replacement for ISA, following a new standard implementation known as "firmware hub" (FWH). In 2006, the first systems supporting a Serial Peripheral Interface (SPI) appeared, and the BIOS flash memory moved again.

The size of the BIOS, and the capacity of the ROM, EEPROM, or other media it may be stored on, has increased over time as new features have been added to the code; BIOS versions now exist with sizes up to 16 megabytes. For contrast, the original IBM PC BIOS was contained in an 8 KiB mask ROM. Some modern motherboards are including even bigger NAND flash memory ICs on board which are capable of storing whole compact operating systems, such as some Linux distributions. For example, some ASUS motherboards included Splashtop OS embedded into their NAND flash memory ICs. However, the idea of including an operating system along with BIOS in the ROM of a PC is not new; in the 1980s, Microsoft offered a ROM option for MS-DOS, and it was included in the ROMs of some PC clones such as the Tandy 1000 HX.

Another type of firmware chip was found on the IBM PC AT and early compatibles. In the AT, the keyboard interface was controlled by a microcontroller with its own programmable memory. On the IBM AT, that was a 40-pin socketed device, while some manufacturers used an EPROM version of this chip which resembled an EPROM. This controller was also assigned the A20 gate function to manage memory above the one-megabyte range; occasionally an upgrade of this "keyboard BIOS" was necessary to take advantage of software that could use upper memory. 

The BIOS may contain components such as the Memory Reference Code (MRC), which is responsible for handling memory timings and related hardware settings.

IBM published the entire listings of the BIOS for its original PC, PC XT, PC AT, and other contemporary PC models, in an appendix of the "IBM PC Technical Reference Manual" for each machine type. The effect of the publication of the BIOS listings is that anyone can see exactly what a definitive BIOS does and how it does it.

In May 1984 Phoenix Software Associates released its first ROM-BIOS, which enabled OEMs to build essentially fully compatible clones without having to reverse-engineer the IBM PC BIOS themselves, as Compaq had done for the Portable, helping fuel the growth in the PC-compatibles industry and sales of non-IBM versions of DOS. And the first American Megatrends (AMI) BIOS was released on 1986.

New standards grafted onto the BIOS are usually without complete public documentation or any BIOS listings. As a result, it is not as easy to learn the intimate details about the many non-IBM additions to BIOS as about the core BIOS services.

Most PC motherboard suppliers license a BIOS "core" and toolkit from a commercial third party, known as an "independent BIOS vendor", or IBV. The motherboard manufacturer then customizes this BIOS to suit its own hardware. For this reason, updated BIOSes are normally obtained directly from the motherboard manufacturer. Major BIOS vendors include American Megatrends (AMI), Insyde Software, Phoenix Technologies and Byosoft. Former vendors include Award Software and Microid Research that were acquired by Phoenix Technologies in 1998; Phoenix later phased out the Award brand name. General Software, which was also acquired by Phoenix in 2007, sold BIOS for embedded systems based on Intel processors.

The open-source community increased their effort to develop a replacement for proprietary BIOSes and their future incarnations with an open-sourced counterpart through the libreboot, coreboot and OpenBIOS/Open Firmware projects. AMD provided product specifications for some chipsets, and Google is sponsoring the project. Motherboard manufacturer Tyan offers coreboot next to the standard BIOS with their Opteron line of motherboards. MSI and Gigabyte Technology have followed suit with the MSI K9ND MS-9282 and MSI K9SD MS-9185 resp. the M57SLI-S4 models.

EEPROM chips are advantageous because they can be easily updated by the user; it is customary for hardware manufacturers to issue BIOS updates to upgrade their products, improve compatibility and remove bugs. However, this advantage had the risk that an improperly executed or aborted BIOS update could render the computer or device unusable. To avoid these situations, more recent BIOSes use a "boot block"; a portion of the BIOS which runs first and must be updated separately. This code verifies if the rest of the BIOS is intact (using hash checksums or other methods) before transferring control to it. If the boot block detects any corruption in the main BIOS, it will typically warn the user that a recovery process must be initiated by booting from removable media (floppy, CD or USB flash drive) so the user can try flashing the BIOS again. Some motherboards have a "backup" BIOS (sometimes referred to as DualBIOS boards) to recover from BIOS corruptions.

There are at least five known BIOS attack viruses, two of which were for demonstration purposes. The first one found in the wild was "Mebromi", targeting Chinese users.

The first BIOS virus was BIOS Meningitis, which instead of erasing BIOS chips it infected them. BIOS Meningitis has relatively harmless, compared to a virus like CIH

The second BIOS virus was CIH, also known as the "Chernobyl Virus", which was able to erase flash ROM BIOS content on compatible chipsets. CIH appeared in mid-1998 and became active in April 1999. Often, infected computers could no longer boot, and people had to remove the flash ROM IC from the motherboard and reprogram it. CIH targeted the then-widespread Intel i430TX motherboard chipset and took advantage of the fact that the Windows 9x operating systems, also widespread at the time, allowed direct hardware access to all programs.

Modern systems are not vulnerable to CIH because of a variety of chipsets being used which are incompatible with the Intel i430TX chipset, and also other flash ROM IC types. There is also extra protection from accidental BIOS rewrites in the form of boot blocks which are protected from accidental overwrite or dual and quad BIOS equipped systems which may, in the event of a crash, use a backup BIOS. Also, all modern operating systems such as FreeBSD, Linux, macOS, Windows NT-based Windows OS like Windows 2000, Windows XP and newer, do not allow user-mode programs to have direct hardware access.

As a result, as of 2008, CIH has become essentially harmless, at worst causing annoyance by infecting executable files and triggering antivirus software. Other BIOS viruses remain possible, however; since most Windows home users without Windows Vista/7's UAC run all applications with administrative privileges, a modern CIH-like virus could in principle still gain access to hardware without first using an exploit. The operating system OpenBSD prevents all users from having this access and the grsecurity patch for the Linux kernel also prevents this direct hardware access by default, the difference being an attacker requiring a much more difficult kernel level exploit or reboot of the machine.

The second BIOS virus was a technique presented by John Heasman, principal security consultant for UK-based Next-Generation Security Software. In 2006, at the Black Hat Security Conference, he showed how to elevate privileges and read physical memory, using malicious procedures that replaced normal ACPI functions stored in flash memory.

The third BIOS virus was a technique called "Persistent BIOS infection." It appeared in 2009 at the CanSecWest Security Conference in Vancouver, and at the SyScan Security Conference in Singapore. Researchers Anibal Sacco and Alfredo Ortega, from Core Security Technologies, demonstrated how to insert malicious code into the decompression routines in the BIOS, allowing for nearly full control of the PC at start-up, even before the operating system is booted. The proof-of-concept does not exploit a flaw in the BIOS implementation, but only involves the normal BIOS flashing procedures. Thus, it requires physical access to the machine, or for the user to be root. Despite these requirements, Ortega underlined the profound implications of his and Sacco's discovery: "We can patch a driver to drop a fully working rootkit. We even have a little code that can remove or disable antivirus."

Mebromi is a trojan which targets computers with AwardBIOS, Microsoft Windows, and antivirus software from two Chinese companies: Rising Antivirus and Jiangmin KV Antivirus. Mebromi installs a rootkit which infects the master boot record.

In a December 2013 interview with "60 Minutes", Deborah Plunkett, Information Assurance Director for the US National Security Agency claimed the NSA had uncovered and thwarted a possible BIOS attack by a foreign nation state, targeting the US financial system. The program cited anonymous sources alleging it was a Chinese plot. However follow-up articles in "The Guardian," "The Atlantic," "Wired" and "The Register" refuted the NSA's claims.

, the legacy PC BIOS is being replaced by the more complex Extensible Firmware Interface (EFI) in many new machines. EFI is a specification which replaces the runtime interface of the legacy BIOS. Initially written for the Intel Itanium architecture, EFI is now available for x86 and x86-64 platforms; the specification development is driven by The Unified EFI Forum, an industry Special Interest Group. EFI booting has been supported in only Microsoft Windows versions supporting GPT, the Linux kernel 2.6.1 and later, and macOS on Intel-based Macs. , new PC hardware predominantly ships with UEFI firmware. The architecture of the rootkit safeguard can also prevent the system from running the user's own software changes, which makes UEFI controversial as a legacy BIOS replacement in the open hardware community.

Other alternatives to the functionality of the "Legacy BIOS" in the x86 world include coreboot and libreboot.

Some servers and workstations use a platform-independent Open Firmware (IEEE-1275) based on the Forth programming language; it is included with Sun's SPARC computers, IBM's RS/6000 line, and other PowerPC systems such as the CHRP motherboards, along with the x86-based OLPC XO-1.

As of at least 2015, Apple has removed legacy BIOS support from MacBook Pro computers. As such the bios utility no longer supports the legacy switch, and prints "Legacy mode not supported on this system". In 2017, Intel announced to remove legacy BIOS support until 2020.
Since 2019, New Intel platform OEM PC no longer support the legacy switch.





</doc>
<doc id="4474" url="https://en.wikipedia.org/wiki?curid=4474" title="Bose–Einstein condensate">
Bose–Einstein condensate

A Bose–Einstein condensate (BEC) is a state of matter (also called the fifth state of matter) which is typically formed when a gas of bosons at low densities is cooled to temperatures very close to absolute zero (-273.15 °C). Under such conditions, a large fraction of bosons occupy the lowest quantum state, at which point microscopic quantum phenomena, particularly wavefunction interference, become apparent macroscopically. A BEC is formed by cooling a gas of extremely low density, about one-hundred-thousandth (1/100,000) the density of normal air, to ultra-low temperatures.

This state was first predicted, generally, in 1924–1925 by Albert Einstein following and crediting a pioneering paper by Satyendra Nath Bose on the new field now known as quantum statistics.

Satyendra Nath Bose first sent a paper to Einstein on the quantum statistics of light quanta (now called photons), in which he derived Planck's quantum radiation law without any reference to classical physics. Einstein was impressed, translated the paper himself from English to German and submitted it for Bose to the "Zeitschrift für Physik", which published it in 1924. (The Einstein manuscript, once believed to be lost, was found in a library at Leiden University in 2005.) Einstein then extended Bose's ideas to matter in two other papers. The result of their efforts is the concept of a Bose gas, governed by Bose–Einstein statistics, which describes the statistical distribution of identical particles with integer spin, now called bosons. Bosons, which is a group of particles that includes the photon as well as atoms such as helium-4 (), are allowed to share a quantum state. Einstein proposed that cooling bosonic atoms to a very low temperature would cause them to fall (or "condense") into the lowest accessible quantum state, resulting in a new form of matter.

In 1938, Fritz London proposed the BEC as a mechanism for superfluidity in and superconductivity.

On 5 June 1995, the first gaseous condensate was produced by Eric Cornell and Carl Wieman at the University of Colorado at Boulder NIST–JILA lab, in a gas of rubidium atoms cooled to 170 nanokelvins (nK). Shortly thereafter, Wolfgang Ketterle at MIT realized a BEC in a gas of sodium atoms. For their achievements Cornell, Wieman, and Ketterle received the 2001 Nobel Prize in Physics. These early studies founded the field of ultracold atoms, and hundreds of research groups around the world now routinely produce BECs of dilute atomic vapors in their labs.

Since 1995, many other atomic species have been condensed, and BECs have also been realized using molecules, quasi-particles, and photons.

This transition to BEC occurs below a critical temperature, which for a uniform three-dimensional gas consisting of non-interacting particles with no apparent internal degrees of freedom is given by:

where:
Interactions shift the value and the corrections can be calculated by mean-field theory.
This formula is derived from finding the gas degeneracy in the Bose gas using Bose–Einstein statistics.

For an ideal Bose gas we have the equation of state:

formula_2

where formula_3 is the per particle volume, formula_4 the thermal wavelength, formula_5 the fugacity and formula_6. It is noticeable that formula_7 is a monotonically growing function of formula_5 in formula_9, which are the only values for which the series converge.

Recognizing that the second term on the right-hand side contains the expression for the average occupation number of the fundamental state formula_10, the equation of state can be rewritten as

formula_11

Because the left term on the second equation must always be positive, formula_12 and because formula_13, a stronger condition is

formula_14

which defines a transition between a gas phase and a condensed phase. On the critical region it is possible to define a critical temperature and thermal wavelength:

formula_15

formula_16

recovering the value indicated on the previous section. The critical values are such that if formula_17 or formula_18 we are in the presence of a Bose-Einstein condensate.

Understanding what happens with the fraction of particles on the fundamental level is crucial. As so, write the equation of state for formula_19, obtaining

formula_20 and equivalently formula_21.

So, if formula_22 the fraction formula_23 and if formula_24 the fraction formula_25. At temperatures near to absolute 0, particles tend to condensate in the fundamental state (state with momentum formula_26).

Consider a collection of "N" non-interacting particles, which can each be in one of two quantum states, formula_27 and formula_28. If the two states are equal in energy, each different configuration is equally likely.

If we can tell which particle is which, there are formula_29 different configurations, since each particle can be in formula_27 or formula_28 independently. In almost all of the configurations, about half the particles are in formula_27 and the other half in formula_28. The balance is a statistical effect: the number of configurations is largest when the particles are divided equally.

If the particles are indistinguishable, however, there are only "N"+1 different configurations. If there are "K" particles in state formula_28, there are particles in state formula_27. Whether any particular particle is in state formula_27 or in state formula_28 cannot be determined, so each value of "K" determines a unique quantum state for the whole system.

Suppose now that the energy of state formula_28 is slightly greater than the energy of state formula_27 by an amount "E". At temperature "T", a particle will have a lesser probability to be in state formula_28 by formula_41. In the distinguishable case, the particle distribution will be biased slightly towards state formula_27. But in the indistinguishable case, since there is no statistical pressure toward equal numbers, the most-likely outcome is that most of the particles will collapse into state formula_27.

In the distinguishable case, for large "N", the fraction in state formula_27 can be computed. It is the same as flipping a coin with probability proportional to "p" = exp(−"E"/"T") to land tails.

In the indistinguishable case, each value of "K" is a single state, which has its own separate Boltzmann probability. So the probability distribution is exponential:

For large "N", the normalization constant "C" is . The expected total number of particles not in the lowest energy state, in the limit that formula_46, is equal to formula_47. It does not grow when "N" is large; it just approaches a constant. This will be a negligible fraction of the total number of particles. So a collection of enough Bose particles in thermal equilibrium will mostly be in the ground state, with only a few in any excited state, no matter how small the energy difference.

Consider now a gas of particles, which can be in different momentum states labeled formula_48. If the number of particles is less than the number of thermally accessible states, for high temperatures and low densities, the particles will all be in different states. In this limit, the gas is classical. As the density increases or the temperature decreases, the number of accessible states per particle becomes smaller, and at some point, more particles will be forced into a single state than the maximum allowed for that state by statistical weighting. From this point on, any extra particle added will go into the ground state.

To calculate the transition temperature at any density, integrate, over all momentum states, the expression for maximum number of excited particles, :

When the integral (also known as Bose-Einstein integral) is evaluated with factors of formula_51 and ℏ restored by dimensional analysis, it gives the critical temperature formula of the preceding section. Therefore, this integral defines the critical temperature and particle number corresponding to the conditions of negligible chemical potential formula_52. In Bose–Einstein statistics distribution, formula_52 is actually still nonzero for BECs; however, formula_52 is less than the ground state energy. Except when specifically talking about the ground state, formula_52 can be approximated for most energy or momentum states as formula_56.

Nikolay Bogoliubov considered perturbations on the limit of dilute gas, finding a finite pressure at zero temperature and positive chemical potential. This leads to corrections for the ground state. The Bogoliubov state has pressure ("T" = 0): formula_57.

The original interacting system can be converted to a system of non-interacting particles with a dispersion law.

In some simplest cases, the state of condensed particles can be described with a nonlinear Schrödinger equation, also known as Gross–Pitaevskii or Ginzburg–Landau equation. The validity of this approach is actually limited to the case of ultracold temperatures, which fits well for the most alkali atoms experiments.

This approach originates from the assumption that the state of the BEC can be described by the unique wavefunction of the condensate formula_58. For a system of this nature, formula_59 is interpreted as the particle density, so the total number of atoms is formula_60

Provided essentially all atoms are in the condensate (that is, have condensed to the ground state), and treating the bosons using mean field theory, the energy (E) associated with the state formula_58 is:

Minimizing this energy with respect to infinitesimal variations in formula_58, and holding the number of atoms constant, yields the Gross–Pitaevski equation (GPE) (also a non-linear Schrödinger equation):

where:

In the case of zero external potential, the dispersion law of interacting Bose–Einstein-condensed particles is given by so-called Bogoliubov spectrum (for formula_65):

The Gross-Pitaevskii equation (GPE) provides a relatively good description of the behavior of atomic BEC's. However, GPE does not take into account the temperature dependence of dynamical variables, and is therefore valid only for formula_65.
It is not applicable, for example, for the condensates of excitons, magnons and photons, where the critical temperature is comparable to room temperature.

The Gross-Pitaevskii equation is a partial differential equation in space and time variables. Usually it does not have analytic solution and
different numerical methods, such as split-step
Crank-Nicolson

and Fourier spectral

and long-range dipolar interaction

The Gross–Pitaevskii model of BEC is a physical approximation valid for certain classes of BECs. By construction, the GPE uses the following simplifications: it assumes that interactions between condensate particles are of the contact two-body type and also neglects anomalous contributions to self-energy. These assumptions are suitable mostly for the dilute three-dimensional condensates. If one relaxes any of these assumptions, the equation for the condensate wavefunction acquires the terms containing higher-order powers of the wavefunction. Moreover, for some physical systems the amount of such terms turns out to be infinite, therefore, the equation becomes essentially non-polynomial. The examples where this could happen are the Bose–Fermi composite condensates, effectively lower-dimensional condensates, and dense condensates and superfluid clusters and droplets.

However, it is clear that in a general case the behaviour of Bose–Einstein condensate can be described by coupled evolution equations for condensate density, superfluid velocity and distribution function of elementary excitations. This problem was in 1977 by Peletminskii et al. in microscopical approach. The Peletminskii equations are valid for any finite temperatures below the critical point. Years after, in 1985, Kirkpatrick and Dorfman obtained similar equations using another microscopical approach. The Peletminskii equations also reproduce Khalatnikov hydrodynamical equations for superfluid as a limiting case.

The phenomena of superfluidity of a Bose gas and superconductivity of a strongly-correlated Fermi gas (a gas of Cooper pairs) are tightly connected to Bose–Einstein condensation. Under corresponding conditions, below the temperature of phase transition, these phenomena were observed in helium-4 and different classes of superconductors. In this sense, the superconductivity is often called the superfluidity of Fermi gas. In the simplest form, the origin of superfluidity can be seen from the weakly interacting bosons model.

In 1938, Pyotr Kapitsa, John Allen and Don Misener discovered that helium-4 became a new kind of fluid, now known as a superfluid, at temperatures less than 2.17 K (the lambda point). Superfluid helium has many unusual properties, including zero viscosity (the ability to flow without dissipating energy) and the existence of quantized vortices. It was quickly believed that the superfluidity was due to partial Bose–Einstein condensation of the liquid. In fact, many properties of superfluid helium also appear in gaseous condensates created by Cornell, Wieman and Ketterle (see below). Superfluid helium-4 is a liquid rather than a gas, which means that the interactions between the atoms are relatively strong; the original theory of Bose–Einstein condensation must be heavily modified in order to describe it. Bose–Einstein condensation remains, however, fundamental to the superfluid properties of helium-4. Note that helium-3, a fermion, also enters a superfluid phase (at a much lower temperature) which can be explained by the formation of bosonic Cooper pairs of two atoms (see also fermionic condensate).

The first "pure" Bose–Einstein condensate was created by Eric Cornell, Carl Wieman, and co-workers at JILA on 5 June 1995. They cooled a dilute vapor of approximately two thousand rubidium-87 atoms to below 170 nK using a combination of laser cooling (a technique that won its inventors Steven Chu, Claude Cohen-Tannoudji, and William D. Phillips the 1997 Nobel Prize in Physics) and magnetic evaporative cooling. About four months later, an independent effort led by Wolfgang Ketterle at MIT condensed sodium-23. Ketterle's condensate had a hundred times more atoms, allowing important results such as the observation of quantum mechanical interference between two different condensates. Cornell, Wieman and Ketterle won the 2001 Nobel Prize in Physics for their achievements.

A group led by Randall Hulet at Rice University announced a condensate of lithium atoms only one month following the JILA work. Lithium has attractive interactions, causing the condensate to be unstable and collapse for all but a few atoms. Hulet's team subsequently showed the condensate could be stabilized by confinement quantum pressure for up to about 1000 atoms. Various isotopes have since been condensed.

In the image accompanying this article, the velocity-distribution data indicates the formation of a Bose–Einstein condensate out of a gas of rubidium atoms. The false colors indicate the number of atoms at each velocity, with red being the fewest and white being the most. The areas appearing white and light blue are at the lowest velocities. The peak is not infinitely narrow because of the Heisenberg uncertainty principle: spatially confined atoms have a minimum width velocity distribution. This width is given by the curvature of the magnetic potential in the given direction. More tightly confined directions have bigger widths in the ballistic velocity distribution. This anisotropy of the peak on the right is a purely quantum-mechanical effect and does not exist in the thermal distribution on the left. This graph served as the cover design for the 1999 textbook "Thermal Physics" by Ralph Baierlein.

Bose–Einstein condensation also applies to quasiparticles in solids. Magnons, Excitons, and Polaritons have integer spin which means they are bosons that can form condensates.

Magnons, electron spin waves, can be controlled by a magnetic field. Densities from the limit of a dilute gas to a strongly interacting Bose liquid are possible. Magnetic ordering is the analog of superfluidity. In 1999 condensation was demonstrated in antiferromagnetic , at temperatures as great as 14 K. The high transition temperature (relative to atomic gases) is due to the magnons' small mass (near that of an electron) and greater achievable density. In 2006, condensation in a ferromagnetic yttrium-iron-garnet thin film was seen even at room temperature, with optical pumping.

Excitons, electron-hole pairs, were predicted to condense at low temperature and high density by Boer et al., in 1961. Bilayer system experiments first demonstrated condensation in 2003, by Hall voltage disappearance. Fast optical exciton creation was used to form condensates in sub-kelvin in 2005 on.

Polariton condensation was first detected for exciton-polaritons in a quantum well microcavity kept at 5 K.

In June 2020, the Cold Atom Laboratory experiment on board the International Space Station successfully created a BEC. Although initially just a proof of function, early results showed that, in the microgravity of the ISS, about half of the atoms formed into a halo-like cloud around the main body of the BEC.

As in many other systems, vortices can exist in BECs. These can be created, for example, by "stirring" the condensate with lasers, or rotating the confining trap. The vortex created will be a quantum vortex. These phenomena are allowed for by the non-linear formula_59 term in the GPE. As the vortices must have quantized angular momentum the wavefunction may have the form formula_69 where formula_70 and formula_71 are as in the cylindrical coordinate system, and formula_72 is the angular quantum number (a.k.a. the "charge" of the vortex). Since the energy of a vortex is proportional to the square of its angular momentum, in trivial topology only formula_73 vortices can exist in the steady state; Higher-charge vortices will have a tendency to split into formula_73 vortices, if allowed by the topology of the geometry. 

An axially symmetric (for instance, harmonic) confining potential is commonly used for the study of vortices in BEC. To determine formula_75, the energy of formula_58 must be minimized, according to the constraint formula_69. This is usually done computationally, however, in a uniform medium, the following analytic form demonstrates the correct behavior, and is a good approximation:

Here, formula_79 is the density far from the vortex and formula_80, where formula_81 is the healing length of the condensate.

A singly charged vortex (formula_73) is in the ground state, with its energy formula_83 given by

where formula_85 is the farthest distance from the vortices considered.(To obtain an energy which is well defined it is necessary to include this boundary formula_86.)

For multiply charged vortices (formula_87) the energy is approximated by

which is greater than that of formula_72 singly charged vortices, indicating that these multiply charged vortices are unstable to decay. Research has, however, indicated they are metastable states, so may have relatively long lifetimes.

Closely related to the creation of vortices in BECs is the generation of so-called dark solitons in one-dimensional BECs. These topological objects feature a phase gradient across their nodal plane, which stabilizes their shape even in propagation and interaction. Although solitons carry no charge and are thus prone to decay, relatively long-lived dark solitons have been produced and studied extensively.

Experiments led by Randall Hulet at Rice University from 1995 through 2000 showed that lithium condensates with attractive interactions could stably exist up to a critical atom number. Quench cooling the gas, they observed the condensate to grow, then subsequently collapse as the attraction overwhelmed the zero-point energy of the confining potential, in a burst reminiscent of a supernova, with an explosion preceded by an implosion.

Further work on attractive condensates was performed in 2000 by the JILA team, of Cornell, Wieman and coworkers. Their instrumentation now had better control so they used naturally "attracting" atoms of rubidium-85 (having negative atom–atom scattering length). Through Feshbach resonance involving a sweep of the magnetic field causing spin flip collisions, they lowered the characteristic, discrete energies at which rubidium bonds, making their Rb-85 atoms repulsive and creating a stable condensate. The reversible flip from attraction to repulsion stems from quantum interference among wave-like condensate atoms.

When the JILA team raised the magnetic field strength further, the condensate suddenly reverted to attraction, imploded and shrank beyond detection, then exploded, expelling about two-thirds of its 10,000 atoms. About half of the atoms in the condensate seemed to have disappeared from the experiment altogether, not seen in the cold remnant or expanding gas cloud. Carl Wieman explained that under current atomic theory this characteristic of Bose–Einstein condensate could not be explained because the energy state of an atom near absolute zero should not be enough to cause an implosion; however, subsequent mean field theories have been proposed to explain it. Most likely they formed molecules of two rubidium atoms; energy gained by this bond imparts velocity sufficient to leave the trap without being detected.

The process of creation of molecular Bose condensate during the sweep of the magnetic field throughout the Feshbach resonance, as well as the reverse process, are described by the exactly solvable model that can explain many experimental observations.

Compared to more commonly encountered states of matter, Bose–Einstein condensates are extremely fragile. The slightest interaction with the external environment can be enough to warm them past the condensation threshold, eliminating their interesting properties and forming a normal gas.

Nevertheless, they have proven useful in exploring a wide range of questions in fundamental physics, and the years since the initial discoveries by the JILA and MIT groups have seen an increase in experimental and theoretical activity. Examples include experiments that have demonstrated interference between condensates due to wave–particle duality, the study of superfluidity and quantized vortices, the creation of bright matter wave solitons from Bose condensates confined to one dimension, and the slowing of light pulses to very low speeds using electromagnetically induced transparency. Vortices in Bose–Einstein condensates are also currently the subject of analogue gravity research, studying the possibility of modeling black holes and their related phenomena in such environments in the laboratory. Experimenters have also realized "optical lattices", where the interference pattern from overlapping lasers provides a periodic potential. These have been used to explore the transition between a superfluid and a Mott insulator, and may be useful in studying Bose–Einstein condensation in fewer than three dimensions, for example the Tonks–Girardeau gas. Further, the sensitivity of the pinning transition of strongly interacting bosons confined in a shallow one-dimensional optical lattice originally observed by Haller has been explored via a tweaking of the primary optical lattice by a secondary weaker one. Thus for a resulting weak bichromatic optical lattice, it has been found that the pinning transition is robust against the
introduction of the weaker secondary optical lattice. Studies of vortices in nonuniform Bose–Einstein condensates as well as excitatons of these systems by the application of moving repulsive or attractive obstacles, have also been undertaken. Within this context, the conditions for order and chaos in the dynamics of a trapped Bose–Einstein condensate have been explored by the application of moving blue and red-detuned laser beams via the time-dependent Gross-Pitaevskii equation.

Bose–Einstein condensates composed of a wide range of isotopes have been produced.

Cooling fermions to extremely low temperatures has created degenerate gases, subject to the Pauli exclusion principle. To exhibit Bose–Einstein condensation, the fermions must "pair up" to form bosonic compound particles (e.g. molecules or Cooper pairs). The first molecular condensates were created in November 2003 by the groups of Rudolf Grimm at the University of Innsbruck, Deborah S. Jin at the University of Colorado at Boulder and Wolfgang Ketterle at MIT. Jin quickly went on to create the first fermionic condensate, working with the same system but outside the molecular regime.

In 1999, Danish physicist Lene Hau led a team from Harvard University which slowed a beam of light to about 17 meters per second using a superfluid. Hau and her associates have since made a group of condensate atoms recoil from a light pulse such that they recorded the light's phase and amplitude, recovered by a second nearby condensate, in what they term "slow-light-mediated atomic matter-wave amplification" using Bose–Einstein condensates: details are discussed in "Nature".

Another current research interest is the creation of Bose–Einstein condensates in microgravity in order to use its properties for high precision atom interferometry. The first demonstration of a BEC in weightlessness was achieved in 2008 at a drop tower in Bremen, Germany by a consortium of researchers led by Ernst M. Rasel from Leibniz University Hannover. The same team demonstrated in 2017 the first creation of a Bose–Einstein condensate in space and it is also the subject of two upcoming experiments on the International Space Station.

Researchers in the new field of atomtronics use the properties of Bose–Einstein condensates when manipulating groups of identical cold atoms using lasers.

In 1970, BECs were proposed by Emmanuel David Tannenbaum for anti-stealth technology.

P. Sikivie and Q. Yang showed that cold dark matter axions form a Bose–Einstein condensate by thermalisation because of gravitational self-interactions. Axions have not yet been confirmed to exist. However the important search for them has been greatly enhanced with the completion of upgrades to the Axion Dark Matter Experiment(ADMX) at the University of Washington in early 2018.

In 2014 a potential dibaryon was detected at the Jülich Research Center at about 2380 MeV. The center claimed that the measurements confirm results from 2011, via a more replicable method. The particle existed for 10 seconds and was named d*(2380). This particle is hypothesized to consist of three up and three down quarks. It is theorized that groups of d-stars could form Bose-Einstein condensates due to prevailing low temperatures in the early universe, and that BECs made of such hexaquarks with trapped electrons could behave like dark matter.

The effect has mainly been observed on alkaline atoms which have nuclear properties particularly suitable for working with traps. As of 2012, using ultra-low temperatures of formula_90 or below, Bose–Einstein condensates had been obtained for a multitude of isotopes, mainly of alkali metal, alkaline earth metal,
and lanthanide atoms (, , , , , , , , , , , , , , and ). Research was finally successful in hydrogen with the aid of the newly developed method of 'evaporative cooling'. In contrast, the superfluid state of below is not a good example, because the interaction between the atoms is too strong. Only 8% of atoms are in the ground state near absolute zero, rather than the 100% of a true condensate.

The bosonic behavior of some of these alkaline gases appears odd at first sight, because their nuclei have half-integer total spin. It arises from a subtle interplay of electronic and nuclear spins: at ultra-low temperatures and corresponding excitation energies, the half-integer total spin of the electronic shell and half-integer total spin of the nucleus are coupled by a very weak hyperfine interaction. The total spin of the atom, arising from this coupling, is an integer lower value. The chemistry of systems at room temperature is determined by the electronic properties, which is essentially fermionic, since room temperature thermal excitations have typical energies much higher than the hyperfine values.



</doc>
<doc id="4475" url="https://en.wikipedia.org/wiki?curid=4475" title="B (programming language)">
B (programming language)

B is a programming language developed at Bell Labs circa 1969. It is the work of Ken Thompson with Dennis Ritchie.

B was derived from BCPL, and its name may be a contraction of BCPL. Thompson's coworker Dennis Ritchie speculated that the name might be based on Bon, an earlier, but unrelated, programming language that Thompson designed for use on Multics.

B was designed for recursive, non-numeric, machine-independent applications, such as system and language software. It was a typeless language, with the only data type being the underlying machine's natural memory word format, whatever that might be. Depending on the context, the word was treated either as an integer or a memory address.

As machines with ASCII processing became common, notably the DEC PDP-11 that arrived at Bell, support for character data stuffed in memory words became important. The typeless nature of the language was seen as a disadvantage, which led Thompson and Ritchie to develop an expanded version of the language supporting new internal and user-defined types, which became the C programming language.

Circa 1969, Ken Thompson and later Dennis Ritchie developed B basing it mainly on the BCPL language Thompson used in the Multics project. B was essentially the BCPL system stripped of any component Thompson felt he could do without in order to make it fit within the memory capacity of the minicomputers of the time. The BCPL to B transition also included changes made to suit Thompson's preferences (mostly along the lines of reducing the number of non-whitespace characters in a typical program). Much of the typical ALGOL-like syntax of BCPL was rather heavily changed in this process. The assignment operator codice_1 changed to codice_2 and the equality operator codice_2 was replaced by codice_4.

Thompson added "two-address assignment operators" using codice_5 syntax to add y to x (in C the operator is written codice_6). This syntax came from Douglas McIlroy's implementation of TMG, in which B's compiler was first implemented (and it came to TMG from ALGOL 68's codice_7 syntax). Thompson went further by inventing the increment and decrement operators (codice_8 and codice_9). Their prefix or postfix position determines whether the value is taken before or after alteration of the operand. This innovation was not in the earliest versions of B. According to Dennis Ritchie, people often assumed that they were created for the auto-increment and auto-decrement address modes of the DEC PDP-11, but this is historically impossible as the machine didn't exist when B was first developed.

B is typeless, or more precisely has one data type: the computer word. Most operators (e.g. codice_10, codice_11, codice_12, codice_13) treated this as an integer, but others treated it as a memory address to be dereferenced. In many other ways it looked a lot like an early version of C. There are a few library functions, including some that vaguely resemble functions from the standard I/O library in C.

Early implementations were for the DEC PDP-7 and PDP-11 minicomputers using early Unix, and Honeywell 36-bit mainframes running the operating system GCOS. The earliest PDP-7 implementations compiled to threaded code, and Ritchie wrote a compiler using TMG which produced machine code. In 1970 a PDP-11 was acquired and threaded code was used for the port; an assembler, , and the B language itself were written in B to bootstrap the computer. An early version of yacc was produced with this PDP-11 configuration. Ritchie took over maintenance during this period.

The typeless nature of B made sense on the Honeywell, PDP-7 and many older computers, but was a problem on the PDP-11 because it was difficult to elegantly access the character data type that the PDP-11 and most modern computers fully support. Starting in 1971 Ritchie made changes to the language while converting its compiler to produce machine code, most notably adding data typing for variables. During 1971 and 1972 B evolved into "New B" (NB) and then C.

B is almost extinct, having been superseded by the C language. However, it continues to see use on GCOS mainframes () 
and on certain embedded systems () for a variety of reasons: limited hardware in small systems, extensive libraries, tooling, licensing cost issues, and simply being good enough for the job. The highly influential AberMUD was originally written in B.

The following examples are from the "Users' Reference to B" by Ken Thompson:
/* The following function will print a non-negative number, n, to

printn(n, b) {

/* The following program will calculate the constant e-2 to about

main() {

v[2000];
n 2000;


</doc>
<doc id="4476" url="https://en.wikipedia.org/wiki?curid=4476" title="Beer–Lambert law">
Beer–Lambert law

The Beer–Lambert law, also known as Beer's law, the Lambert–Beer law, or the Beer–Lambert–Bouguer law relates the attenuation of light to the properties of the material through which the light is travelling. The law is commonly applied to chemical analysis measurements and used in understanding attenuation in physical optics, for photons, neutrons, or rarefied gases. In mathematical physics, this law arises as a solution of the BGK equation.

The law was discovered by Pierre Bouguer before 1729, while looking at red wine, during a brief vacation in Alentejo, Portugal. It is often attributed to Johann Heinrich Lambert, who cited Bouguer's "Essai d'optique sur la gradation de la lumière" (Claude Jombert, Paris, 1729)—and even quoted from it—in his "Photometria" in 1760. Lambert's law stated that the loss of light intensity when it propagates in a medium is directly proportional to intensity and path length. Much later, August Beer discovered another attenuation relation in 1852. Beer's law stated that the transmittance of a solution remains constant if the product of concentration and path length stays constant. The modern derivation of the Beer–Lambert law combines the two laws and correlates the absorbance, which is the negative decadic logarithm of the transmittance, to both the concentrations of the attenuating species and the thickness of the material sample.

A common and practical expression of the Beer-Lambert law relates the optical attenuation of a physical material containing a single attenuating species of uniform concentration to the optical path length through the sample and absorptivity of the species. This expression is:

Where

A more general form of the Beer–Lambert law states that, for formula_5 attenuating species in the material sample,
or equivalently that
where

In the above equations, the transmittance formula_18 of material sample is related to its optical depth formula_19 and to its absorbance "A" by the following definition
where

Attenuation cross section and molar attenuation coefficient are related by
and number density and amount concentration by

where formula_25 is the Avogadro constant.

In case of "uniform" attenuation, these relations become
or equivalently

Cases of "non-uniform" attenuation occur in atmospheric science applications and radiation shielding theory for instance.

The law tends to break down at very high concentrations, especially if the material is highly scattering. Absorbance within range of 0.2 to 0.5 is ideal to maintain the linearity in Beer-Lambart law. If the radiation is especially intense, nonlinear optical processes can also cause variances. The main reason, however, is that the concentration dependence is in general non-linear and Beer's law is valid only under certain conditions as shown by derivation below. For strong oscillators and at high concentrations the deviations are stronger. If the molecules are closer to each other interactions can set in. These interactions can be roughly divided into physical and chemical interactions. Physical interaction do not alter the polarizability of the molecules as long as the interaction is not so strong that light and molecular quantum state intermix (strong coupling), but cause the attenuation cross sections to be non-additive via electromagnetic coupling. Chemical interactions in contrast change the polarizability and thus absorption. 

The Beer–Lambert law can be expressed in terms of attenuation coefficient, but in this case is better called Lambert's law since amount concentration, from Beer's law, is hidden inside the attenuation coefficient. The (Napierian) attenuation coefficient formula_29 and the decadic attenuation coefficient formula_30 of a material sample are related to its number densities and amount concentrations as
respectively, by definition of attenuation cross section and molar attenuation coefficient. Then the Beer–Lambert law becomes
and

In case of "uniform" attenuation, these relations become
or equivalently

In many cases, the attenuation coefficient does not vary with formula_39, in which case one does not have to perform an integral and can express the law as:
where the attenuation is usually an addition of absorption coefficient formula_41 (creation of electron-hole pairs) or scattering (for example Rayleigh scattering if the scattering centers are much smaller than the incident wavelength). Also note that for some systems we can put formula_42 (1 over inelastic mean free path) in place of formula_29.

Assume that a beam of light enters a material sample. Define "z" as an axis parallel to the direction of the beam. Divide the material sample into thin slices, perpendicular to the beam of light, with thickness d"z" sufficiently small that one particle in a slice cannot obscure another particle in the same slice when viewed along the "z" direction. The radiant flux of the light that emerges from a slice is reduced, compared to that of the light that entered, by , where "μ" is the (Napierian) attenuation coefficient, which yields the following first-order linear ODE:
The attenuation is caused by the photons that did not make it to the other side of the slice because of scattering or absorption. The solution to this differential equation is obtained by multiplying the integrating factor
throughout to obtain
which simplifies due to the product rule (applied backwards) to
Integrating both sides and solving for Φ for a material of real thickness "ℓ", with the incident radiant flux upon the slice and the transmitted radiant flux gives
and finally
Since the decadic attenuation coefficient "μ" is related to the (Napierian) attenuation coefficient by , one also have

To describe the attenuation coefficient in a way independent of the number densities "n" of the "N" attenuating species of the material sample, one introduces the attenuation cross section . "σ" has the dimension of an area; it expresses the likelihood of interaction between the particles of the beam and the particles of the specie "i" in the material sample:

One can also use the molar attenuation coefficients , where N is the Avogadro constant, to describe the attenuation coefficient in a way independent of the amount concentrations of the attenuating species of the material sample:

The above assumption that the attenuation cross sections are additive is generally incorrect since electromagnetic coupling occurs if the distances between the absorbing entities is small. 

The derivation of the concentration dependence of the absorbance is based on electromagnetic theory. Accordingly, the macroscopic polarization of a medium formula_53 derives from the microscopic dipole moments formula_54 in the absence of interaction according to

where formula_56 is the dipole moment and formula_57 the number of absorbing entities per unit volume. On the other hand, macroscopic polarization is given by:

Here formula_59represents the relative dielectric function, formula_60 the vacuum permittivity and formula_61 the electric field. After equating and solving for the relative dielectric function the result is:

If we take into account that the polarizability formula_63 is defined by formula_64 and that for the number of absorbers per unit volume formula_65holds, it follows that:

According to Maxwell's wave equation the following relation between the complex dielectric function and the complex index of refraction function holds formula_67for isotropic and homogeneous media. Therefore:

The imaginary part of the complex index of refraction is the index of absorption formula_69. Employing the imaginary part of the polarizability formula_70and the approximation formula_71 it follows that:

Taking into account the relation between formula_69 and formula_74, formula_75 it eventually follows that

As a consequence, the linear relation between concentration and absorbance is generally an approximation, and holds in particular only for small polarisabilities and weak absorptions, i.e. oscillator strengths. If we do not introduce the approximation formula_71, and employ instead the following relation between the imaginary part of the relative dielectric function and index of refraction and absorption formula_78 it can be seen that the molar attenuation coefficient depends on the index of refraction (which is itself concentration dependent):

Under certain conditions the Beer–Lambert law fails to maintain a linear relationship between attenuation and concentration of analyte. These deviations are classified into three categories:

There are at least six conditions that need to be fulfilled in order for the Beer–Lambert law to be valid. These are:

If any of these conditions are not fulfilled, there will be deviations from the Beer–Lambert law.

The Beer–Lambert law is not compatible with Maxwell's equations. Being strict, the law does not describe the transmittance through a medium, but the propagation within that medium. It can be made compatible with Maxwell's equations if the transmittance of a sample with solute is ratioed against the transmittance of the pure solvent which explains why it works so well in spectrophotometry. As this is not possible for pure media, the uncritical employment of the Beer–Lambert law can easily generate errors of the order of 100% or more. In such cases it is necessary to apply the Transfer-matrix method. A detailed discussion of the incompatibility between the Beer–Lambert law and Maxwell's equations can be found in the review The Bouguer‐Beer‐Lambert law: Shining light on the obscure .

Recently it has also been demonstrated that Beer's law is a limiting law, since the absorbance is only approximately linearly depending on concentration. The reason is that the attenuation coefficient also depends on concentration and density, even in the absence of any interactions. These changes are, however, usually negligible except for high concentrations and large oscillator strength. For high concentrations and/or oscillator strengths, it is the integrated absorbance which is linearly depending on concentration, at least as long as there are no local field effects. If there are local field effects, they can be approximately taken into account by applying the Lorentz-Lorenz relation. In fact, Beer's law, i.e. the concentration dependence of absorbance, can be derived directly from the Lorentz-Lorenz relation (or, equivalently, the Clausius-Mossotti relation). Correspondingly, it can be demonstrated that there is a twin law according to which the change of the refractive index is approximately linear to the molar concentration for diluted solutions. This twin law can also be derived from the Lorentz-Lorenz relation.

The Beer–Lambert law can be applied to the analysis of a mixture by spectrophotometry, without the need for extensive pre-processing of the sample. An example is the determination of bilirubin in blood plasma samples. The spectrum of pure bilirubin is known, so the molar attenuation coefficient "ε" is known. Measurements of decadic attenuation coefficient "μ" are made at one wavelength "λ" that is nearly unique for bilirubin and at a second wavelength in order to correct for possible interferences. The amount concentration "c" is then given by

For a more complicated example, consider a mixture in solution containing two species at amount concentrations "c" and "c". The decadic attenuation coefficient at any wavelength "λ" is, given by

Therefore, measurements at two wavelengths yields two equations in two unknowns and will suffice to determine the amount concentrations "c" and "c" as long as the molar attenuation coefficient of the two components, "ε" and "ε" are known at both wavelengths. This two system equation can be solved using Cramer's rule. In practice it is better to use linear least squares to determine the two amount concentrations from measurements made at more than two wavelengths. Mixtures containing more than two components can be analyzed in the same way, using a minimum of "N" wavelengths for a mixture containing "N" components.

The law is used widely in infra-red spectroscopy and near-infrared spectroscopy for analysis of polymer degradation and oxidation (also in biological tissue) as well as to measure the concentration of various compounds in different food samples. The carbonyl group attenuation at about 6 micrometres can be detected quite easily, and degree of oxidation of the polymer calculated.

This law is also applied to describe the attenuation of solar or stellar radiation as it travels through the atmosphere. In this case, there is scattering of radiation as well as absorption. The optical depth for a slant path is , where "τ" refers to a vertical path, "m" is called the relative airmass, and for a plane-parallel atmosphere it is determined as where "θ" is the zenith angle corresponding to the given path. The Beer–Lambert law for the atmosphere is usually written
where each "τ" is the optical depth whose subscript identifies the source of the absorption or scattering it describes:

"m" is the "optical mass" or "airmass factor", a term approximately equal (for small and moderate values of "θ") to 1/cos "θ", where "θ" is the observed object's zenith angle (the angle measured from the direction perpendicular to the Earth's surface at the observation site). This equation can be used to retrieve "τ", the aerosol optical thickness, which is necessary for the correction of satellite images and also important in accounting for the role of aerosols in climate.



</doc>
<doc id="4477" url="https://en.wikipedia.org/wiki?curid=4477" title="The Beach Boys">
The Beach Boys

The Beach Boys are an American rock band formed in Hawthorne, California in 1961. The group's original lineup consisted of brothers Brian, Dennis, and Carl Wilson, their cousin Mike Love, and their friend Al Jardine. Distinguished by their vocal harmonies and early surf songs, they are one of the most influential acts of the rock era. The band drew on the music of jazz-based vocal groups, 1950s rock and roll, and black R&B to create their unique sound, and with Brian as composer, arranger, producer, and de facto leader, they often incorporated classical or jazz elements and unconventional recording techniques in innovative ways.

One of the first self-contained rock groups, the Beach Boys began as a garage band led by Brian and managed by the Wilsons' father Murry. In 1963, the group gained national prominence with a string of top-ten singles reflecting a southern California youth culture of surfing, cars, and romance, dubbed the "California sound". They were one of the few American rock bands to sustain their commercial standing during the British Invasion. From 1965, they abandoned beachgoing themes for more personal lyrics and ambitious orchestrations. In 1966, the "Pet Sounds" album and "Good Vibrations" single raised the group's prestige as rock innovators. After scrapping the unfinished album "Smile" in 1967, Brian's contributions diminished due to his mental health issues. The group's commercial momentum faltered, and despite efforts to maintain an experimental sound, they were widely dismissed by the early rock music press.

Carl took over as the band's musical leader until the late 1970s, during which they carried out a series of successful international concert tours. Personal struggles, creative disagreements, and the overshadowing success of the band's greatest hits albums precipitated their transition into an oldies act. Dennis drowned in 1983 and Brian soon became estranged from the group. Between the 1990s and 2000s, the members filed numerous lawsuits against each other. Following Carl's death from lung cancer in 1998, the group and its corporation (Brother Records Inc.) granted Love legal rights to tour as "the Beach Boys". , Brian and Jardine do not perform with Love's Beach Boys, but remain official members of the band.

The Beach Boys are one of the most critically acclaimed and commercially successful bands of all time, with over 100 million records sold worldwide. Between the 1960s and 2010s, the group had over 80 songs chart worldwide, 36 of them in the US Top 40 (the most by a US rock band), and four topping the "Billboard" Hot 100. Their influence on other artists spans musical genres and movements such as psychedelia, power pop, progressive rock, punk, alternative, and lo-fi. In 2004, "Rolling Stone" ranked the Beach Boys number 12 on its list of the "100 Greatest Artists of All Time". The core quintet of the three Wilsons, Love, and Jardine was inducted into the Rock and Roll Hall of Fame in 1988. 

At the time of his sixteenth birthday on June 20, 1958, Brian Wilson shared a bedroom with his brothers, Dennis and Carl – aged thirteen and eleven, respectively – in their family home in Hawthorne. He had watched his father, Murry Wilson, play piano, and had listened intently to the harmonies of vocal groups such as the Four Freshmen. After dissecting songs such as "Ivory Tower" and "Good News", Brian would teach family members how to sing the background harmonies. For his birthday that year, Brian received a reel-to-reel tape recorder. He learned how to overdub, using his vocals and those of Carl and their mother. Brian played piano with Carl and David Marks, an eleven-year-old longtime neighbor, playing guitars they had each received as Christmas presents.

Soon Brian and Carl were avidly listening to Johnny Otis' KFOX radio show. Inspired by the simple structure and vocals of the rhythm and blues songs he heard, Brian changed his piano-playing style and started writing songs. Family gatherings brought the Wilsons in contact with cousin Mike Love. Brian taught Love's sister Maureen and a friend harmonies. Later, Brian, Love and two friends performed at Hawthorne High School. Brian also knew Al Jardine, a high school classmate. Brian suggested to Jardine that they team up with his cousin and brother Carl. Love gave the fledgling band its name: "The Pendletones", a pun on "Pendleton", a style of woolen shirt popular at the time. Dennis was the only avid surfer in the group, and he suggested that the group write songs that celebrated the sport and the lifestyle that it had inspired in Southern California. Brian finished the song, titled "Surfin'", and with Mike Love, wrote "Surfin' Safari". Murry recalled, "They had written a song called 'Surfin' ', which I never did like and still don't like, it was so rude and crude."

Murry Wilson, who was a sometime songwriter, arranged for the Pendletones to meet his publisher Hite Morgan. He said: "Finally, [Hite] agreed to hear it, and Mrs. Morgan said 'Drop everything, we're going to record your song. I think it's good.' And she's the one responsible." On September 15, 1961, the band recorded a demo of "Surfin'" with the Morgans. A more professional recording was made on October 3, at World Pacific Studio in Hollywood. David Marks was not present at the session as he was in school that day. Murry brought the demos to Herb Newman, owner of Candix Records and Era Records, and he signed the group on December 8. When the single was released a few weeks later, the band found that they had been renamed "the Beach Boys". Candix wanted to name the group the Surfers until Russ Regan, a young promoter with Era Records, noted that there already existed a group by that name. He suggested calling them the Beach Boys. "Surfin" was a regional success for the West Coast, and reached number 75 on the national "Billboard" Hot 100 chart. It was so successful that the number of unpaid orders for the single bankrupted Candix.

By this time the de facto manager of the Beach Boys, Murry landed the group's first paying gig (for which they earned $300) on New Year's Eve, 1961, at the Ritchie Valens Memorial Dance in Long Beach. In their earliest public appearances, the band wore heavy wool jacket-like shirts that local surfers favored before switching to their trademark striped shirts and white pants. In early 1962, Morgan requested that some of the members add vocals to a couple of instrumental tracks that he had recorded with other musicians. This led to the creation of the short-lived group Kenny & the Cadets, which Brian led under the pseudonym "Kenny". The other members were Carl, Jardine, and the Wilsons' mother Audree. In February, Jardine left the Beach Boys to study dentistry and was replaced by David Marks. Murry remembered that after "Surfin", the group had a difficult time being picked up by another label; "they [all] thought [the group was] a one-shot record."

After being turned down by Dot and Liberty, the Beach Boys signed a seven-year contract with Capitol Records. This was at the urging of Capitol executive and staff producer Nick Venet who signed the group, seeing them as the "teenage gold" he had been scouting for. On June 4, 1962, the Beach Boys debuted on Capitol with their second single, "Surfin' Safari" backed with "409". The release prompted national coverage in the June 9 issue of "Billboard", which praised Love's lead vocal and said the song had potential. "Surfin' Safari" rose to number 14 and found airplay in New York and Phoenix, a surprise for the label.

The Beach Boys completed their first album, "Surfin' Safari", with production credited to Nick Venet. Carl later denied that Venet had any significant role in the group's early music, saying that Venet "would be in the booth, and he would call the take number, and that was about it. I wouldn't call him a musical heavy by any ... Brian didn't want anything to do with Venet." "Surfin' Safari", released in October 1962, was different from other rock albums of the time in that it consisted almost entirely of original songs, primarily written by Brian with Mike Love and friend Gary Usher. Another unusual feature of the Beach Boys was that, although they were marketed as "surf music", their repertoire bore little resemblance to the music of other surf bands, which was mainly instrumental and incorporated heavy use of spring reverb. For this reason, some of the Beach Boys' early local performances had young audience members throwing vegetables at the band, believing that the group were poseurs.

In January 1963, the Beach Boys recorded their first top-ten single, "Surfin' U.S.A.", which began their long run of highly successful recording efforts. It was during the sessions for this single that Brian made the production decision from that point on to use double tracking on the group's vocals, resulting in a deeper and more resonant sound. The album of the same name followed in March and reached number 2 on the "Billboard" charts. Its success propelled the group into a nationwide spotlight, and was vital to launching surf music as a national craze, albeit the Beach Boys' vocal approach to the genre, not the original instrumental style pioneered by Dick Dale. Biographer Luis Sanchez highlights the "Surfin' U.S.A." single as a turning point for the band, "creat[ing] a direct passage to California life for a wide teenage audience ... [and] a distinct Southern California sensibility that exceeded its conception as such to advance right to the front of American consciousness."

Throughout 1963, and for the next few years, Brian produced a variety of singles for outside artists. Among these were the Honeys, a surfer trio that comprised sisters Diane and Marilyn Rovell with cousin Ginger Blake. Brian was convinced that they could potentially be a successful female counterpart to the Beach Boys, and he produced a number of singles for them, although they could not replicate the Beach Boys' popularity. He also attended some of Phil Spector's sessions at Gold Star Studios. His creative and songwriting interests were revamped upon hearing the Ronettes' 1963 song "Be My Baby", which was produced by Spector. The first time he heard the song was while driving, and was so overwhelmed that he had to pull over to the side of the road and analyze the chorus. Later, he reflected: "I was unable to really think as a producer up until the time where I really got familiar with Phil Spector's work. That was when I started to design the experience to be a record rather than just a song."

The surf music craze, along with the careers of nearly all surf acts, was slowly replaced by the British Invasion. Following a successful Australasian tour in January and February 1964, the Beach Boys returned home to face their new competition, the Beatles. Both groups shared the same record label in the US, and Capitol's support for the Beach Boys immediately began waning. This caused Murry to fight for the band at the label more than before, often visiting their offices without warning to "twist executive arms". Carl said that Phil Spector "was Brian's favorite kind of rock; he liked [him] better than the early Beatles stuff. He loved the Beatles' later music when they evolved and started making intelligent, masterful music, but before that Phil was it." According to Mike Love, Carl followed the Beatles closer than anyone else in the band, while Brian was the most "rattled" by the Beatles and felt tremendous pressure to "keep pace" with them. For Brian, the Beatles ultimately "eclipsed a lot [of what] we'd worked for ... [they] eclipsed the whole music world."

Brian wrote his last surf song in April 1964. That month, during recording of the single "I Get Around", Murry was relieved of his duties as manager. He remained in close contact with the group and attempted to continue advising on their career decisions. When "I Get Around" was released in May, it would climb to number one, their first single to do so, proving that the Beach Boys could compete with contemporary British pop groups. In July, the album that the song appeared on, "All Summer Long", reached No. 4 in the US. "All Summer Long" introduced exotic textures to the Beach Boys' sound exemplified by the piccolos and xylophones of its title track. The album was a swan-song to the surf and car music the Beach Boys built their commercial standing upon. Later albums took a different stylistic and lyrical path. Before this, a live album, "Beach Boys Concert", was released in October to a four-week chart stay at number one, containing a set list of previously recorded songs and covers that they had not yet recorded.

In June 1964, Brian recorded the bulk of "The Beach Boys' Christmas Album" with a forty-one-piece studio orchestra in collaboration with Four Freshmen arranger Dick Reynolds. The album was a response to Phil Spector's "A Christmas Gift for You" (1963). Released in December, the Beach Boys' album was divided between five new, original Christmas-themed songs, and seven reinterpretations of traditional Christmas songs. It would be regarded as one of the finest holiday albums of the rock era. One single from the album, "The Man with All the Toys", was released, peaking at No. 6 on the US "Billboard" Christmas chart. On October 29, the Beach Boys performed for "The T.A.M.I. Show", a concert film intended to bring together a wide range of musicians for a one-off performance. The result was released to movie theaters one month later.

By the end of 1964, the stress of road travel, writing, and producing became too much for Brian. On December 23, while on a flight from Los Angeles to Houston, he suffered a panic attack hours after performing with the Beach Boys on the musical variety series "Shindig!". In January 1965, he announced his withdrawal from touring to concentrate entirely on songwriting and record production. For the rest of 1964 and into 1965, session musician Glen Campbell served as Brian's temporary replacement in concert. Carl took over as the band's musical director onstage.

Now a full-time studio artist, Brian wanted to move the Beach Boys beyond their surf aesthetic, believing that their image was antiquated and distracting the public from his talents as a producer and songwriter. In the period following his resignation from touring, Brian put more distance between him and his bandmates, and began expanding his social circle to include a mix of worldly-minded friends, musicians, mystics, and business advisers. He also took an increasing interest in the developing Los Angeles "hip" scene and in recreational drugs (particularly marijuana, LSD, and Desbutal). Musically, he said he began to "take the things I learned from Phil Spector and use more instruments whenever I could. I doubled up on basses and tripled up on keyboards, which made everything sound bigger and deeper."

Released in March 1965, "The Beach Boys Today!" marked the first time the group experimented with the "album-as-art" form. The tracks on side one feature an uptempo sound that contrasts side two, which consists mostly of emotional ballads. Music writer Scott Schinder referenced its "suite-like structure" as an early example of the rock album format being used to make a cohesive artistic statement. Brian also established his new lyrical approach toward the autobiographical; journalist Nick Kent wrote that the subjects of Brian's songs "were suddenly no longer simple happy souls harmonizing their sun-kissed innocence and dying devotion to each other over a honey-coated backdrop of surf and sand. Instead, they'd become highly vulnerable, slightly neurotic and riddled with telling insecurities." In the book "Yeah Yeah Yeah: The Story of Modern Pop", Bob Stanley remarked that "Brian was aiming for Johnny Mercer but coming up proto-indie." In 2012, the album was voted 271 on "Rolling Stone" magazine's list of the 500 Greatest Albums of All Time.

In April 1965, Campbell's own career success pulled him from touring with the group. Columbia Records staff producer Bruce Johnston was asked to locate a replacement for Campbell; having failed to find one, Johnston himself became a full-time member of the band on May 19, 1965, first replacing Brian on the road and later contributing in the studio, beginning with the June 4 vocal sessions for "California Girls", which first appeared in the band's next album "Summer Days (And Summer Nights!!)" and eventually charted at number three in the US while the album went to number two. The album also included a reworked arrangement of "Help Me, Rhonda" which became the band's second number one single in the spring of 1965.

To appease Capitol's demands for a Beach Boys LP for the 1965 Christmas season, Brian conceived "Beach Boys' Party!", a live-in-the-studio album consisting mostly of acoustic covers of 1950s rock and R&B songs, in addition to covers of three Beatles songs, Bob Dylan's "The Times They Are a-Changin'", and idiosyncratic rerecordings of the group's earlier songs. The album was an early precursor of the "unplugged" trend. It included a cover of the Regents' song "Barbara Ann" which unexpectedly reached number two when released several weeks later. In November, the group released another top-twenty single, "The Little Girl I Once Knew". It was considered the band's most experimental statement thus far. The single continued Brian's ambitions for daring arrangements, featuring unexpected tempo changes and numerous false endings. It was the band's second single not to reach the top ten since their 1962 breakthrough, peaking at number 20. According to Luis Sanchez, in 1965, Bob Dylan was "rewriting the rules for pop success" with his music and image, and it was at this juncture that Wilson "led The Beach Boys into a transitional phase in an effort to win the pop terrain that had been thrown up for grabs."

In January 1966, Wilson commenced recording sessions for the Beach Boys' forthcoming album "Pet Sounds", which was largely a collaboration with jingle writer Tony Asher. The album was a refinement of the themes and ideas that were introduced in "Today!". In some ways, the music was a jarring departure from their earlier style. When the other Beach Boys returned from a three-week tour of Japan and Hawaii, they were presented with a substantial portion of the new album, and various reports suggest that they fought over the new direction. Musicologist Daniel Harrison wrote, "In terms of the structure of the songs themselves, there is comparatively little advance from what Brian had already accomplished." In "The Journal on the Art of Record Production", Marshall Heiser writes that "Pet Sounds" "diverges from previous Beach Boys' efforts in several ways: its sound field has a greater sense of depth and 'warmth;' the songs employ even more inventive use of harmony and chord voicings; the prominent use of percussion is a key feature (as opposed to driving drum backbeats); whilst the orchestrations, at times, echo the quirkiness of 'exotica' bandleader Les Baxter, or the 'cool' of Burt Bacharach, more so than Spector's teen fanfares." Tony Asher recalled witnessing "tense" recording sessions in which all of Brian's bandmates complained that the music 'isn't our kind of shit!'".

For "Pet Sounds", Brian desired to make "a complete statement", similar to what he believed the Beatles had done with their newest album "Rubber Soul", released in December 1965. Brian was immediately enamored with the album, given the impression that it had no filler tracks, a feature that was mostly unheard of at a time when 45 rpm singles were considered more noteworthy than full-length LPs. He later said: "It didn't make me want to copy them but to be as good as them. I didn't want to do the same kind of music, but on the same level." Thanks to mutual connections, Brian was introduced to the Beatles' former press officer Derek Taylor, who was subsequently employed as the Beach Boys' publicist. Responding to Brian's request to reinvent the band's image, Taylor devised a promotion campaign with the tagline "Brian Wilson is a genius", a belief Taylor sincerely held. Taylor's prestige was crucial in offering a credible perspective to those on the outside, and his efforts are widely recognized as instrumental in the album's success in Britain.

Released on May 16, 1966, "Pet Sounds" was widely influential and raised the band's prestige as an innovative rock group. Early reviews for the album in the US ranged from negative to tentatively positive, and its sales numbered approximately 500,000 units, a drop-off from the run of albums that immediately preceded it. It was assumed that Capitol considered "Pet Sounds" a risk, appealing more to an older demographic than the younger, female audience upon which the Beach Boys had built their commercial standing. Within two months, the label capitulated by releasing the group's first greatest hits compilation, "Best of the Beach Boys", which was quickly certified gold by the RIAA. By contrast, "Pet Sounds" met a highly favorable critical response in Britain, where it reached number 2 and remained among the top-ten positions for six months. Responding to the hype, "Melody Maker" ran a feature in which many pop musicians were asked whether they believed that the album was truly revolutionary and progressive, or "as sickly as peanut butter". The author concluded that "the record's impact on artists and the men behind the artists has been considerable."
In its evaluation of "Pet Sounds", the book "101 Albums that Changed Popular Music" (2009) calls it "one of the most innovative recordings in rock", and states that it "elevated Brian Wilson from talented bandleader to studio genius". In 1995, a panel of numerous musicians, songwriters and producers assembled by "Mojo" voted "Pet Sounds" the greatest record ever made. Paul McCartney frequently spoke of his affinity with the album, citing "God Only Knows" as his favorite song of all time, and crediting it with furthering his interest in devising melodic bass lines. He said that "Pet Sounds" was the primary impetus for the Beatles' 1967 album "Sgt. Pepper's Lonely Hearts Club Band". According to author Carys Wyn Jones, the interplay between the two groups during the "Pet Sounds" era remains one of the most noteworthy episodes in rock history. In 2003, when "Rolling Stone" magazine created its list of the "500 Greatest Albums of All Time", the publication placed "Pet Sounds" second to honour its influence on the highest-ranked album, "Sgt. Pepper".

Throughout the summer of 1966, Brian concentrated on finishing the group's next single, "Good Vibrations". During the making of "Pet Sounds", Wilson started changing his writing process. Rather than going to the studio with a completed song, he would record a track containing a series of chord changes he liked, take an acetate disc home, and then compose the song's melody and write its lyrics. With "Good Vibrations", Wilson said, "I had a lot of unfinished ideas, fragments of music I called 'feels.' Each feel represented a mood or an emotion I'd felt, and I planned to fit them together like a mosaic." Most of the song's structure and arrangement was written as it was recorded. Instead of working on whole songs with clear large-scale syntactical structures, Brian limited himself to recording short interchangeable fragments (or "modules"). Through the method of tape splicing, each fragment could then be assembled into a linear sequence, allowing any number of larger structures and divergent moods to be produced at a later time. Coming at a time when pop singles were usually recorded in under two hours, it was one of the most complex pop productions ever undertaken, with sessions for the song stretching over several months in four major Hollywood studios. It was also the most expensive single ever recorded to that point, with production costs estimated to be in the tens of thousands;

In the midst of "Good Vibrations" sessions, Wilson invited session musician and songwriter Van Dyke Parks to collaborate as lyricist for the Beach Boys' next album project, soon titled "Smile". Parks agreed. Wilson and Parks intended "Smile" to be a continuous suite of songs linked both thematically and musically, with the main songs linked together by small vocal pieces and instrumental segments that elaborated on the major songs' musical themes. It was explicitly American in style and subject, a conscious reaction to the overwhelming British dominance of popular music at the time. Some of the music incorporated chanting, cowboy songs, explorations in Indian and Hawaiian music, jazz, classical tone poems, cartoon sound effects, "musique concrète", and yodeling. "Saturday Evening Post" writer Jules Siegel recalled that, on one October evening, Brian announced to his wife and friends that he was "writing a teenage symphony to God". Brian told "Melody Maker", "Our new album will be better than "Pet Sounds". It will be as much an improvement over "Sounds" as that was over "Summer Days"." Derek Taylor continued to write articles in the music press, sometimes anonymously, in an effort to further speculation about the album.

Recording for "Smile" lasted about a year, from mid-1966 to mid-1967, and followed the same modular production approach as "Good Vibrations". Concurrently, Wilson planned many different multimedia side projects, such as a sound effects collage, a comedy album, and a "health food" album. Capitol did not support all these ideas, which led to the Beach Boys' desire to form their own label, Brother Records. According to biographer Steven Gaines, Love was "the most receptive" to the proposal, wanting the Beach Boys to have more creative control over their work, and supported Wilson's decision to employ his newfound "best friend" David Anderle as head of the label, even though it was against band manager Nick Grillo's wishes. In a press release, Anderle said that Brother Records was to give "entirely new concepts to the recording industry, and to give the Beach Boys total creative and promotional control over their product". The group established a short-lived film production company, Home Movies, to create live-action film and television properties starring the Beach Boys. The company completed only one production, a promotional clip for "Good Vibrations".
Released on October 10, 1966, "Good Vibrations" was the Beach Boys' third US number-one single, reaching the top of the "Billboard" Hot 100 in December, and became their first number one in Britain. That month, the record was their first single certified gold by the RIAA. It came to be widely acclaimed as one of the greatest masterpieces of rock music. In December 1966, the Beach Boys were voted the top band in the world in the "NME"s annual readers' poll, ahead of the Beatles, the Walker Brothers, the Rolling Stones, and the Four Tops. "Billboard" said the result was probably influenced by the success of "Good Vibrations" when the votes were cast, together with the band's recent UK tour, whereas the Beatles had neither a recent single nor toured the UK in 1966, but added, "The sensational success of the Beach Boys ... is being taken as a portent that the popularity of the top British groups of the last three years is past its peak."

Throughout 1966, EMI flooded the UK market with previously unreleased Beach Boys albums, including "Beach Boys' Party!", "The Beach Boys Today!" and "Summer Days (and Summer Nights!!)", and "Best of the Beach Boys" was number two there for several weeks at the end of the year.<ref name="Mawer/OCC"></ref>
Over the final quarter of 1966, the Beach Boys were the highest-selling album act in the UK, where for the first time in three years American artists broke the chart dominance of British acts. In 1971, "Cue" magazine wrote that, from mid-1966 to late 1967, the Beach Boys "were among the vanguard in practically every aspect of the counter culture". Biographer David Leaf wrote that the success of "Good Vibrations" "bought Brian some time [and] shut up everybody who said that Brian's new ways wouldn't sell ... his inability to "quickly" follow up [the single was what] became a snowballing problem." Sanchez writes that as time passed, the hype for "Smile" turned into "expectation", "doubt", and finally "bemusement".

By December 1966, Wilson had completed much of the "Smile" backing tracks. When the Beach Boys returned from a month-long tour of Europe, they were confused by the new music he had recorded and the new coterie of interlopers surrounding him. Gaines wrote that Anderle now appeared to them as the leader of "a whole group of strangers [that] had infiltrated and taken over the Beach Boys". Throughout the first half of 1967, the album's release date was repeatedly postponed as Brian tinkered with the recordings, experimenting with different takes and mixes, unable or unwilling to supply a final version. Meanwhile, he suffered from delusions and paranoia, believing on one occasion that the album track "" (also known as "Mrs. O'Leary's Cow") caused a building to burn down. On January 3, 1967, Carl Wilson refused to be drafted for military service, leading to indictment and criminal prosecution, which he challenged as a conscientious objector. The FBI arrested him in April, and it took several years for courts to resolve the matter.

After months of recording and media hype, "Smile" was shelved for numerous personal, technical, and legal reasons. A February 1967 lawsuit seeking $255,000 (equivalent to $ in ) was launched against Capitol Records over neglected royalty payments. Within the lawsuit was an attempt to terminate the band's contract with Capitol before its November 1969 expiry. Since the group's future at Capitol was in limbo, an immediate release of "Smile" would have been unlikely regardless of whether it was completed. Band quarrels led Parks to leave the project in April 1967, with Anderle following suit weeks later. Brian later said: "Time can be spent in the studio to the point where you get so next to it, you don't know where you are with it—you decide to just chuck it for a while." He discussed breaking up the Beach Boys "on many occasions", according to Anderle, "but it was easier, I think, to get rid of the outsiders like myself than it was to break up the brothers. You can't break up brothers."

In the decades following "Smile"s non-release, it became the subject of intense speculation and mystique and the most legendary unreleased album in pop music history. Many of the album's advocates believe that had it been released, it would have altered the group's direction and established them at the vanguard of rock innovators. In October 1967, "Cheetah" magazine published "Goodbye Surfing, Hello God!", a memoir by Jules Siegel that chronicled his time with Brian during the "Smile" sessions. The article propelled the mythology of "Smile" and the Beach Boys and credited the album's collapse to "an obsessive cycle of creation and destruction that threatened not only his career and his fortune but also his marriage, his friendships, his relationships with the Beach Boys and, some of his closest friends worried, his mind". Carl blamed the article and "a lot of that stuff that went around before" with "really turn[ing Brian] off". Some of the original "Smile" tracks continued to trickle out in later releases, often as filler tracks to offset Brian's unwillingness to contribute. In 2011, "Uncut" magazine staff voted "Smile" the "greatest bootleg recording of all time".

In May 1967, the Beach Boys attempted to tour Europe with four extra musicians brought from the US, but were stopped by the British musicians' union. The tour went on without the extra support, and critics described their performances as "amateurish" and "floundering". Days after announcing that "Smile" was "scrapped", Derek Taylor terminated his employment with the group to focus his attention on organizing the Monterey Pop Festival, an event held in June that the Beach Boys declined to headline at the last minute. According to David Leaf, "Monterey was a gathering place for the 'far out' sounds of the 'new' rock, and the Beach Boys in concert really had no exotic sounds (excepting 'Good Vibrations') to display. The net result of all this internal and external turmoil was that the Beach Boys didn't go to Monterey, and it is thought that this non-appearance was what really turned the 'underground' tide against them." Fan magazines speculated that the group was on the verge of breaking up.

Publicly, the band said that they could not play Monterey because of Carl's military draft, but many of the people involved with the festival thought the group was simply too scared to compete with the "new music". Love later said, "Carl was to appear in federal court the Tuesday after the concert, but for all we knew, they were going to arrest him again if he performed onstage. ... None of us were afraid to perform at Monterey." Steven Gaines wrote that the decision "had a snowballing effect" that came to represent "a damning admission that [the Beach Boys] were washed up". A controversy involving whether the band was to be taken as a serious rock group developed among critics and fans. Detractors called the band the "Bleach Boys" and "the California Hypes" as media focus shifted from Los Angeles to the happenings in San Francisco. On December 14, 1967, "Rolling Stone" co-founder and editor Jann Wenner printed an influential article that denounced the Beach Boys as "just one prominent example of a group that has gotten hung up on trying to catch The Beatles. It's a pointless pursuit." The article had the effect of excluding the group among serious rock fans.

The Beach Boys were still under pressure and a contractual obligation to record and present an album to Capitol. Carl remembered: "Brian just said, 'I can't do this. We're going to make a homespun version of ["Smile"] instead. We're just going to take it easy. I'll get in the pool and sing. Or let's go in the gym and do our parts.' That was "Smiley Smile"." Sessions for the new album lasted from June to July 1967 at Brian's new makeshift home studio. Most of the album featured the Beach Boys playing their own instruments, rather than the session musicians employed in much of their previous work. It was the first album for which production was credited to the entire group instead of Brian alone. When asked if Brian was "still the producer of "Smiley Smile"", Carl answered, "Most definitely."

In July, lead single "Heroes and Villains" was issued, arriving after months of public anticipation, and reached number 12 in US. It was met with general confusion and underwhelming reviews, and in the "NME", Jimi Hendrix famously dismissed it as a "psychedelic barbershop quartet". By then, the group's lawsuit with Capitol was resolved, and it was agreed that "Smile" would not be the band's next album. In August, the group embarked on a two-date tour of Hawaii. Bruce Johnston, who was absent for most of the "Smiley Smile" recording, did not accompany the group, but Brian did. The performances were filmed and recorded with the intention of releasing a live album, "Lei'd in Hawaii", which was also left unfinished and unreleased. In an interview that month, Brian said, "I think rock n' roll—the pop scene—is happening. It's great. But I think basically the Beach Boys are squares. We're not happening."

"Smiley Smile" was released on September 18, 1967, and peaked at number 41 in the US, making it their worst-selling album to that date. It began a string of underperforming Beach Boys albums that lasted until 1974. When released in the UK in November, it performed better, reaching number 9. Critics and fans were generally underwhelmed by the album. According to Scott Schinder, the album was released to "general incomprehension. While "Smile" may have divided the Beach Boys' fans had it been released, "Smiley Smile" merely baffled them." Over the years, the album gathered a reputation as one of the best "chill-out" albums to listen to during an LSD comedown. In 1974, "NME" voted it the 64th-greatest album of all time.

The Beach Boys immediately recorded a new album, "Wild Honey", an excursion into soul music. Carl described it as "music for Brian to cool out by. He was still very spaced." The album was a self-conscious attempt by the Beach Boys to "regroup" themselves as a rock band in opposition to their more orchestral affairs of the past. Its music differs in many ways from previous Beach Boys records: it contains very little group singing compared to previous albums, and mainly features Brian singing at his piano. Again, the Beach Boys recorded mostly at his home studio. Love reflected that "Wild Honey" was "completely out of the mainstream for what was going on at that time, which was all hard rock/psychedelic music. It just didn't have anything to do with what was going on, and that was the idea."

"Wild Honey" was released on December 18, 1967, in competition with the Beatles' "Magical Mystery Tour" and the Rolling Stones' "Their Satanic Majesties Request". It had a lower chart placing than "Smiley Smile" and remained on the charts for only 15 weeks. As with "Smiley Smile", contemporary critics viewed it as inconsequential, and it alienated fans whose expectations had been raised by "Smile". That month, Mike Love told a British journalist: "Brian has been rethinking our recording program and in any case we all have a much greater say nowadays in what we turn out in the studio." "Wild Honey" remained the last Beach Boys album to feature Brian as a primary composer until 1977. Over the coming months, its non-conforming approach was echoed in albums released by Bob Dylan ("John Wesley Harding"), the Kinks ("Village Green Preservation Society"), and the Byrds ("The Notorious Byrd Brothers").

The Beach Boys were at their lowest popularity in the late 1960s, and their cultural standing was especially worsened by their public image, which remained incongruous with their peers' "heavier" music. Capitol continued to bill them as "America's Top Surfin' Group!" and expected Brian to write more beachgoing songs for the yearly summer markets. From 1968 onward, his songwriting output declined substantially, but the public narrative of "Brian as leader" continued. The group also stopped wearing their longtime striped-shirt stage uniforms in favor of matching white, polyester suits that resembled a Las Vegas show band's.

After meeting Maharishi Mahesh Yogi at a UNICEF Variety Gala in Paris, Love and other high-profile celebrities such as the Beatles and Donovan traveled to Rishikesh, India, in February–March 1968. The following Beach Boys album, "Friends", had songs influenced by the Transcendental Meditation the Maharishi taught. In support of "Friends", Love arranged for the Beach Boys to tour with the Maharishi in the U.S.. Starting on May 3, 1968, the tour lasted five shows and was canceled when the Maharishi withdrew to fulfill film contracts. Because of disappointing audience numbers and the Maharishi's withdrawal, 24 tour dates were canceled at a cost estimated at $250,000. "Friends", released on June 24, peaked at number 126 in the US. In August, Capitol issued a collection of Beach Boys backing tracks, "Stack-o-Tracks". It was the first Beach Boys LP that failed to chart in the US and UK.

In June 1968, Dennis befriended Charles Manson, an aspiring singer-songwriter, and their relationship lasted for several months. Dennis bought him time at Brian's home studio, where recording sessions were attempted while Brian stayed in his room. Dennis then proposed that Manson be signed to Brother Records. Brian reportedly disliked Manson, and a deal was never made. In July 1968, the group released a standalone single, "Do It Again", in the style of their earlier songs. Around this time, Brian admitted himself to a psychiatric hospital. His bandmates wrote and produced material in his absence. To complete their contract with Capitol, they produced one more album, "20/20", released in January 1969. It consisted mostly of outtakes and leftovers from recent albums; Brian produced virtually none of the newer recordings. In 1976, Dennis called it "the only letdown of the Beach Boys' career that embarrassed me through and through ... we had to find things that Brian worked on and try and piece it together. That's when [he had] no involvement at all."

The Beach Boys recorded one song by Manson without his involvement: "Cease to Exist", rewritten as "Never Learn Not to Love", which was on "20/20" but first released as the B-side of a single one month earlier. Manson was enthused by the idea of the group recording one of his songs, but after Manson accrued a large monetary debt to the group, Dennis deliberately omitted his credit on its release while also altering the song's arrangement and lyrics, which angered Manson. As his cult of followers took over Dennis's home, Dennis gradually distanced himself from Manson. According to Leaf, "The entire Wilson family reportedly feared for their lives." In November 1969, three months after the Tate–LaBianca murders, police apprehended Manson, and his connection with the Beach Boys received media attention. He was later convicted for several counts of murder and conspiracy to murder. In 1976, Dennis said, "I don't talk about Manson. I think he's a sick fuck. I think of Roman [Polanski] and all those wonderful people who had a beautiful family and they fucking had their tits cut off. I want to benefit from that?"

In April 1969, the band revisited its 1967 lawsuit against Capitol after it alleged an audit revealed the band was owed over $2 million for unpaid royalties and production duties. In May, Brian told the music press that the group's funds were depleted to the point that it was considering filing for bankruptcy at the end of the year, which "Disc & Music Echo" called "stunning news" and a "tremendous shock on the American pop scene". Brian hoped that the success of a forthcoming single, "Break Away", would mend the financial issues. The song, written and produced by Brian and Murry, reached number 63 in the US and number 6 in the UK, and Brian's remarks to the press ultimately thwarted long-simmering contract negotiations with Deutsche Grammophon. The group's Capitol contract expired two weeks later with one more album still due, after which the label deleted the Beach Boys' catalog from print, effectively cutting off their royalty flow. The lawsuit was later settled in their favor and they acquired the rights to their post-1965 catalog.

In August, Sea of Tunes, the Beach Boys' catalog, was sold to Irving Almo Music for $700,000 (equivalent to $ in ). According to his wife, Marilyn Wilson, Brian was devastated by the sale. Over the years, the catalog generated more than $100 million in publishing royalties, none of which Murry or the band members ever received.

The group was signed to Reprise Records in 1970. Scott Schinder described the label as "probably the hippest and most artist-friendly major label of the time." The deal was brokered by Van Dyke Parks, who was then employed as a multimedia executive at Warner Music Group. Reprise's contract stipulated Brian's proactive involvement with the band in all albums By the time the Beach Boys' tenure ended with Capitol in 1969, they had sold 65 million records worldwide, closing the decade as the most commercially successful American group in popular music.

After recording over 30 different songs and going through several album titles, their first LP for Reprise, "Sunflower", was released on August 31, 1970. "Sunflower" featured a strong group presence with significant writing contributions from all band members. Brian was active during this period, writing or co-writing seven of "Sunflower"'s 12 songs and performing at half of the band's domestic concerts in 1970. The album received critical acclaim in both the US and the UK. This was offset by the album reaching only number 151 on US record charts during a four-week stay, becoming the worst-selling Beach Boys album at that point. In his "Rolling Stone" review, Jim Miller praised the album as "without doubt the best Beach Boys album in recent memory, a stylistically coherent "tour de force"", but mused, "It makes one wonder though whether anyone still listens to their music, or could give a shit about it." In the UK, the album reached 29. Fans generally regard the LP as the Beach Boys' finest post-"Pet Sounds" album. In 2003, it placed at number 380 on "Rolling Stone"s "Greatest Albums of All Time" list.

In 1969, Brian opened a short-lived health food store, the Radiant Radish. While working there, he met journalist and radio presenter Jack Rieley. Rieley spoke with Brian for a radio interview, with the subject eventually turning to the unreleased song "Surf's Up", a track that had taken on notoriety since "Smile"'s demise three years earlier. Brian did not feel it should be released. In August 1970, Rieley offered a six-page memo ruminating on how to stimulate "increased record sales and popularity for The Beach Boys." Within the next few months, the Beach Boys hired him as their manager. One of his initiatives was to encourage the band to record songs featuring more socially conscious lyrics. He also requested the completion of "Surf's Up" and arranged a guest appearance at a Grateful Dead concert at Bill Graham's Fillmore East in April 1971 to foreground the Beach Boys' transition into the counterculture. During this time, the group ceased wearing matching uniforms on stage.

In July 1971, the Beach Boys filmed a concert for ABC-TV in Central Park. It aired as "Good Vibrations from Central Park" on August 19, 1971. The concert also featured performances by Boz Scaggs, Kate Taylor, Carly Simon, and Ike & Tina Turner.

On August 30, 1971, the band released "Surf's Up", which included the title track. The album was moderately successful, reaching the US top 30, a marked improvement over their recent releases. While the record charted, the Beach Boys added to their renewed fame by performing a near-sellout set at Carnegie Hall; their live shows during this era included reworked arrangements of many of their previous songs, with their set lists culling from "Pet Sounds" and "Smile". Music writer Domenic Priore noted, "They basically played what they could have played at the Monterey Pop Festival in the summer of 1967." Dennis injured his hand during the "Surf's Up" sessions, leaving him temporarily unable to play the drums.

 Reprise, however, felt the album needed a strong single. This resulted in "Sail On, Sailor", a collaboration between Brian, Tandyn Almer, Ray Kennedy, Rieley and Parks, featuring a soulful lead vocal by Chaplin. Reprise approved, and the resulting album, "Holland", was released in January 1973, peaking at number 37. Brian's musical children's story, "Mount Vernon and Fairway (A Fairy Tale)", was included as a bonus EP.

In August 1973, the 41-song soundtrack to "American Graffiti" was released, including the band's early songs "Surfin' Safari" and "All Summer Long". The album was a catalyst in creating a wave of nostalgia that reintroduced the Beach Boys into contemporary American consciousness. Chaplin also left in late 1973 after an argument with Steve Love, the band's business manager (and Mike's brother). In June 1974, Capitol issued "Endless Summer", the band's first major pre-"Pet Sounds" greatest hits package. It rose to the top of the "Billboard" charts, staying on the charts for two years, the longest of any Beach Boys release. Capitol followed up with a second compilation, "Spirit of America", which also sold well. With these compilations, the Beach Boys became one of the most popular acts in rock, propelling themselves from opening for Crosby, Stills, Nash and Young to headliners selling out basketball arenas in a matter of weeks. "Rolling Stone" named the Beach Boys the "Band of the Year" for 1974.

Fataar remained with the band until 1974, when he was offered a chance to join a new group led by future Eagles member Joe Walsh. Chaplin's replacement, James William Guercio, started offering the group career advice that resulted in his becoming their new manager. A new album was attempted, with sessions held at Guercio's Caribou Ranch recording studio in Colorado and at the band's Brother Studios in L.A. Only a scattering of material from these sessions saw eventual release. The impetus had shifted from recording new material to large venue touring, and under Guercio, the Beach Boys staged a successful 1975 joint concert tour with Chicago, with each group performing some of the other's songs, including their previous year's collaboration on Chicago's single "Wishing You Were Here". While their concerts continuously sold out, the stage act slowly changed from a contemporary presentation followed by oldies encores to an show made up of mostly pre-1967 music.

Brian spent the majority of two years secluded in the chauffeur's quarters of his home, abusing alcohol, taking drugs (including heroin), overeating, and exhibiting other self-destructive behavior. Although increasingly reclusive during the day, he spent many nights at singer Danny Hutton's house, fraternizing with colleagues such as Alice Cooper and Iggy Pop. In 1975, Brian attempted to join California Music, a Los Angeles collective that included Gary Usher, Curt Boettcher, and Bruce Johnston. The Beach Boys' recent "Endless Summer" compilation was selling well, and the band was touring nonstop, making them the biggest live draw in the US. Guercio was then fired by the group and replaced by Steve Love, who urged the group to encourage Brian to return to the production helm. According to Steve, "We were under contract with Warner Bros. and we couldn't have him going on a tangent. If he was going to be productive, it's gotta be for the Beach Boys." Already tired of working with the Beach Boys, Brian was then legally ousted from California Music to focus his attention on the band. In October, Marilyn persuaded him to admit himself to the care of psychotherapist Eugene Landy, who kept him from indulging in substance abuse with constant supervision.

At the end of January 1976, the Beach Boys returned to the studio with an apprehensive Brian producing once again. At the time, he felt, "It was a little scary because [the Beach Boys and I] weren't as close. We had drifted apart personality-wise. A lot of the guys had developed new personalities through meditation. ... But we went into the studio with the attitude that we had to get it done." Landy supervised group meetings, and discussions over each song for the record reportedly lasted up to eight hours. Brian decided the band should do an album of rock and roll and doo wop standards. Carl and Dennis disagreed, feeling that an album of originals was far more ideal, while Love and Jardine wanted the album out as quickly as possible. Brian's production role was undermined as group members overdubbed and remixed tracks without his knowledge, to fight against his desire for a rough, unfinished sound. He later attributed his hoarse voice on the album to a bout of laryngitis.
Released on July 5, 1976, "15 Big Ones" was generally disliked by fans and critics upon release. Its lead single, a cover of Chuck Berry's "Rock and Roll Music", peaked at number five. Carl and Dennis disparaged the album to the press. Dennis said, "It was a great mistake to put Brian in full control. He was always the absolute producer, but little did he know that in his absence, people grew up, people became as sensitive as the next guy. Why do I relinquish my rights as an artist? The whole process was a little bruising." Brian said, "the new album is nothing too deep", but remained hopeful that their next release would be on a par with "Good Vibrations". An August 1976 NBC-TV special, "The Beach Boys", was produced by "Saturday Night Live" creator Lorne Michaels, and featured appearances by "SNL" cast members John Belushi and Dan Aykroyd. In December, Brian was released from Landy's program due to disputes over Landy's fee.

From late 1976 to early 1977, Brian made sporadic public appearances and produced the band's next album, "The Beach Boys Love You", a collection of 14 songs mostly written, arranged and produced alone. He later called "Love You" one of his favorite Beach Boys releases, saying, "That's when it all happened for me. That's where my heart lies." 
The album's engineer, Earle Mankey, compared it to the surrealist film "Eraserhead", and said that while it was "lighthearted" on the surface, it was intended to be a "serious, autobiographical work". In "Pitchfork", D. Erik Kempke wrote that the album "stands in sharp contrast to the albums that preceded and followed it, because it was a product of genuine inspiration on Brian Wilson's part, with little outside interference." Al Jardine credited Carl and Dennis with having "the most to do with that album ... [they were] paying tribute to their brother."

Released on April 11, 1977, "Love You" peaked at number 53 in the US and number 28 in the UK. It was divided between fans and critics. Some saw the album as a work of "eccentric genius" whereas others dismissed it as "childish and trivial". In a review for "Circus", Lester Bangs called the Beach Boys "a diseased bunch of motherfuckers if ever there was one ... But the miracle is that the Beach Boys have made that disease sound like the literal babyflesh pink of health." The album was released weeks after the band signed a new record deal with CBS. Gaines hypothesized that the lack of promotion Reprise put into "Love You" was a byproduct of the falling out between artist and label.

After "Love You" was released, Brian assembled "Adult/Child", an unreleased effort largely consisting of songs written by Brian from 1976 and 1977 with select big band arrangements by Dick Reynolds. Although publicized as the Beach Boys' next release, "Adult/Child" caused tension within the group and was ultimately shelved. Following this period, his concert appearances with the band gradually diminished and their performances were occasionally erratic. The internal wrangling came to a head after a show at Central Park on September 1, 1977, when the band effectively split into two camps; Dennis and Carl Wilson on one side, Mike Love and Al Jardine on the other with Brian remaining neutral. Following a confrontation on an airport tarmac, Dennis declared to "Rolling Stone" on September 3 that he had left the band: "It was Al Jardine who really knifed me in the heart when he said they didn't need me. That was the clincher. And all I told him was that he couldn't play more than four chords. They kept telling me I had my solo album now ["Pacific Ocean Blue"], like I should go off in a corner and leave the Beach Boys to them. The album really bothers them. They don't like to admit it's doing so well; they never even acknowledge it in interviews."

The band broke up for two and a half weeks, until a meeting on September 17 at Brian's house. In light of a potential new Caribou Records contract the parties negotiated a settlement resulting in Love gaining control of Brian's vote in the group, allowing Love and Jardine to outvote Carl and Dennis Wilson on any matter. Dennis started to withdraw from the group to focus on his second solo album, "Bambu". The album was shelved just as alcoholism and marital problems overcame all three Wilson brothers. Carl appeared intoxicated during concerts (especially at appearances for their 1978 Australia tour) and Brian gradually slid back into addiction and an unhealthy lifestyle.

Their last album for Reprise, "M.I.U. Album" (1978), was recorded at Maharishi International University in Iowa at the suggestion of Love. Dennis and Carl made limited contributions; the album was mostly produced by Jardine and Ron Altbach, with Brian credited as "executive producer". "M.I.U." was largely a contractual obligation to finish out their association with Reprise, who likewise did not promote the result. 

In an April 1980 interview, Carl reflected that "the last two years have been the most important and difficult time of our career. We were at the ultimate crossroads. We had to decide whether what we had been involved in since we were teenagers had lost its meaning. We asked ourselves and each other the difficult questions we'd often avoided in the past." By the next year, he left the touring group because of unhappiness with the band's nostalgia format and lackluster live performances, subsequently pursuing a solo career. He stated: "I haven't quit the Beach Boys but I do not plan on touring with them until they decide that 1981 means as much to them as 1961."

Carl returned in May 1982, after approximately 14 months of being away, on the condition that the group reconsider their rehearsal and touring policies and refrain from "Las Vegas-type" engagements. Later that year, Brian overdosed on a combination of alcohol, cocaine, and other psychoactive drugs. His former therapist Eugene Landy was once more employed, and a more radical program was undertaken to try to restore Brian to health. This involved removing him from the group on November 5, 1982, at the behest of Carl, Love, and Jardine, in addition to putting him on a rigorous diet and health regimen. Coupled with long, extreme counseling sessions, this therapy was successful in bringing Brian back to physical health, slimming down from to .

From 1980 through 1982, the Beach Boys and the Grass Roots performed Independence Day concerts at the National Mall in Washington, D.C., attracting large crowds. However, in April 1983, James G. Watt, President Ronald Reagan's Secretary of the Interior, banned Independence Day concerts on the Mall by such groups. Watt said that "rock bands" that had performed on the Mall on Independence Day in 1981 and 1982 had encouraged drug use and alcoholism and had attracted "the wrong element", who would steal from attendees. During the ensuing uproar, which included over 40,000 complaints to the Department of the Interior, the Beach Boys stated that the Soviet Union, which had invited them to perform in Leningrad in 1978, "...obviously ... did not feel that the group attracted the wrong element." Vice President George H. W. Bush said of the Beach Boys, "They're my friends and I like their music". Watt later apologized to the band after learning that President Reagan and First Lady Nancy Reagan were fans. White House staff presented Watt with a plaster foot with a hole in it, showing that he had "shot himself in the foot".

In 1983, tensions between Dennis and Love escalated so high that each obtained a restraining order against the other. With the rest of the band fearing that he would end up like Brian, Dennis was given an ultimatum after his last performance in November 1983 to check into rehab for his alcohol problems or be banned from performing live with them. Dennis checked into rehab for his chance to get sober, but on December 28, 1983, he drowned at the age of 39 in Marina del Rey while diving from a friend's boat trying to recover items that he had previously thrown overboard in fits of rage.

Between 1983 and 1986, Landy charged Brian about $430,000 annually. When he requested more money, Carl was obliged to give away a quarter of Brian's publishing royalties. As Brian's recovery consolidated, he stopped working with the Beach Boys on a regular basis. Commenting on his relationship to the band in 1988, Brian said that he avoided his family at Landy's suggestion, and that "Although we stay together as a group, as people we're a far cry from friends." In the mid 1980s, Landy stated, "I influence all of [Brian]'s thinking. I'm practically a member of the band ... [We're] partners in life." Brian later responded to allegations with, "People say that Dr. Landy runs my life, but the truth is, I'm in charge." Mike Love denied Landy's accusation that he and the band were keeping Brian from participating with the group, and later wrote that Landy's "goal ... was to destroy us ... [and become] the sole custodian of Brian's career and legacy."

The Beach Boys spent the next several years touring, often playing in front of large audiences, and recording songs for film soundtracks and various artists compilations. In 1988, they unexpectedly claimed their first U.S. number one single in 22 years with "Kokomo", which topped the chart for one week. It appeared in the movie "Cocktail" They released the album "Still Cruisin'", which went platinum in the US.

 Love filed a defamation lawsuit against Brian due to how he was presented in Brian's 1992 memoir "". Its publisher HarperCollins settled the suit for $1.5 million. He said that the suit allowed his lawyer "to gain access to the transcripts of Brian's interviews with his [book] collaborator, Todd Gold. Those interviews affirmed—according to Brian—that I had been the inspiration of the group and that I had written many of the songs that [would soon be] in dispute." Other defamation lawsuits were filed by Carl, Brother Records, and the Wilsons' mother Audree.

The day after California courts issued a restraining order between Brian and Landy, Brian phoned Sire Records staff producer Andy Paley to collaborate on new material tentatively for the Beach Boys. After losing the songwriting credits lawsuit with Love, Brian told "MOJO" in February 1995: "Mike and I are just cool. There's a lot of shit Andy and I got written for him. I just had to get through that goddamn trial!" In April, it was unclear whether the project would turn into a Wilson solo album, a Beach Boys album, or a combination of the two. The project ultimately disintegrated. Instead, Brian and his bandmates recorded "Stars and Stripes Vol. 1", an album of country music stars covering Beach Boys songs, with co-production helmed by River North Records owner Joe Thomas. Afterward, the group discussed finishing the album "Smile", but Carl rejected the idea, fearing that it would cause Brian another nervous breakdown.

In early 1997, Carl was diagnosed with lung and brain cancer after years of heavy smoking. Despite his terminal condition, Carl continued to perform with the band on its 1997 summer tour (a double-bill with the band Chicago) while undergoing chemotherapy. During performances, he sat on a stool and needed oxygen after every song. Carl died on February 6, 1998, at the age of 51, two months after the death of the Wilsons' mother, Audree.

 In turn, Jardine left the band and began to tour regularly with his band "Beach Boys: Family & Friends" until he ran into legal issues for using the name without license. Meanwhile, Jardine sued Love, claiming that he had been excluded from their concerts., through its longtime attorney, Ed McPherson, sued Jardine in Federal Court. Jardine, in turn, counter-claimed against BRI for wrongful termination. BRI ultimately prevailed. Love and Johnston continued to tour as "The Beach Boys" with supporting musicians. Marks left the touring band in 1999 because of his health.

In 2000, ABC-TV premiered a two-part television miniseries, "", that dramatized the Beach Boys' story. It was produced by John Stamos, and was criticized for historical inaccuracies. Brian Wilson said that he "didn't like the second part. It wasn't really true to the way things were. I'd like to see another movie if it was done right."

In 2004, Wilson recorded and released his solo album "Brian Wilson Presents Smile", a reinterpretation of the unfinished "Smile" project. That September, Wilson issued a free CD through the "Mail On Sunday" that included Beach Boys songs he had recently rerecorded, five of which he co-authored with Love. The 10 track compilation had 2.6 million copies distributed and prompted Love to file a lawsuit in November 2005; he claimed the promotion hurt the sales of the original recordings. Love's suit was dismissed in 2007 when a judge determined that there were no triable issues.

 On October 31, Capitol released a compilation and box set dedicated to "Smile" in the form of "The Smile Sessions". The album garnered universal critical acclaim and charting in both the Billboard US and UK Top 30. It went on to win Best Historical Album at the 2013 Grammy Awards.

On December 16, 2011, it was announced that Wilson, Love, Jardine, Johnston and David Marks would reunite for a new album and 50th anniversary tour. On February 12, 2012, the Beach Boys performed at the 2012 Grammy Awards, in what was billed as a "special performance" by organizers. It marked the group's first live performance to include Wilson since 1996, Jardine since 1998, and Marks since 1999. Released on June 5, "That's Why God Made the Radio" debuted at number 3 on U.S. charts, which expanding the group's span of "Billboard" 200 top ten albums across 49 years and one week, passing the Beatles with 47 years of top ten albums. Critics generally regarded the album as an "uneven" collection, with most of the praise centered on its closing musical suite.

On June 1, 2012, Love received an e-mail from Brian's wife and manager Melinda Ledbetter stating "no more shows for Wilson". Love, who is obligated by his license of the Beach Boys name to maintain revenue flow to Brother Records, then began accepting invitations for when the reunion was over. On June 25, Ledbetter sent another e-mail asking to disregard her last message, but by then, Love says, "it was too late. We had booked other concerts, and promoters had begun selling tickets." The next day, Love announced additional touring dates that would not feature Wilson. Wilson then denied knowledge of these new dates. He later wrote: "I had wanted to send out a joint press release, between Brian and me, formally announcing the end of the reunion tour on September 28. But I couldn't get Brian's management team on board (Brian himself doesn't make those kinds of decisions)."

In late September, news outlets began reporting that Love had dismissed Wilson from the Beach Boys. On October 5, Love responded in a self-written press release to the "LA Times" stating he "did not fire Brian Wilson from the Beach Boys. I cannot fire Brian Wilson from the Beach Boys ... I do not have such authority. And even if I did, I would never fire Brian Wilson from the Beach Boys." He explained that nobody in the band "wanted to do a 50th anniversary tour that lasted 10 years" and that its limited run "was long agreed upon". Four days later, Wilson and Jardine submitted a written response to the rumors stating: "I was completely blindsided by his press release ... We hadn't even discussed as a band what we were going to do with all the offers that were coming in for more 50th shows." Love said that Wilson's statements in this press release were falsified by his agents.

Love and Johnston continued to perform under the Beach Boys brand name, while Wilson, Jardine, and Marks continued to tour as a trio, and a subsequent tour with guitarist Jeff Beck also included Blondie Chaplin at select dates. Reflecting upon the band's reunion in 2013, Love stated: "I had a wonderful experience being in the studio together. Brian has lost none of his ability to structure those melodies and chord progressions ... Touring was more for the fans. ... It was a great experience, it had a term to it, and now everyone's going on with their ways of doing things." Jardine, Marks, Johnston and Love appeared together at the 2014 Ella Awards Ceremony, where Love was honored for his work as a singer.

In 2015, "Soundstage" aired an episode featuring Wilson performing with Jardine and former Beach Boys Blondie Chaplin and Ricky Fataar at The Venetian in Las Vegas. In April 2015, when asked if he was interested in making music with Love again, Wilson replied: "I don't think so, no," later adding in July that he "doesn't talk to the Beach Boys [or] Mike Love."

In 2016, Love and Wilson published memoirs, "" and "I Am Brian Wilson", respectively. Love was asked about negative comments that Wilson made about him in the book and said: "He's not in charge of his life, like I am mine. His every move is orchestrated and a lot of things he's purported to say, there's not tape of it. But, I don't like to put undue pressure on him, either, because I know he has a lot of issues. Out of compassion, I don't respond to everything that is purportedly said by him." In an interview with "Rolling Stone" conducted in June 2016, Wilson stated that he would like to try to repair his relationship with Love and collaborate with him again. In January 2017, Love stated "If it were possible to make it just Brian and I, and have it under control and done better than what happened in 2012, then yeah, I'd be open to something."

In July 2018, Wilson, Jardine, Love, Johnston, and Marks reunited for a one-off Q&A session moderated by director Rob Reiner at the Capitol Records Tower in Los Angeles. It was the first time the band appeared together in public since their 2012 tour. That December, Love described his new holiday album, "Reason for the Season", as a "message to Brian" and said that he "would love nothing more than to get together with Brian and do some music."

In February 2020, Wilson's official social media pages encouraged fans to boycott the band's music after it was announced that Love's Beach Boys would perform at the Safari Club International Convention in Reno, Nevada. The concert proceeded despite online protests, as Love issued a statement that said his group has always supported "freedom of thought and expression as a fundamental tenet of our rights as Americans." In March, Jardine was asked about a possible reunion and responded that the band will reunite for a string of live performances in 2021, although he believed a new album was unlikely. In response to reunion rumors, Love said in May that he was open to a 60th anniversary tour, although Wilson has "some serious health issues", while Wilson's manager Jean Sievers commented that no one had spoken to Wilson about such a tour.

In "Understanding Rock: Essays in Musical Analysis", musicologist Daniel Harrison writes:

The Beach Boys began as a garage band playing 1950s style rock and roll, reassembling styles of music such as surf to include vocal jazz harmony, which created their unique sound. In addition, they introduced their signature approach to common genres such as the pop ballad by applying harmonic or formal twists not native to rock and roll. Among the distinct elements of the Beach Boys' style were the nasal quality of their singing voices, their use of a falsetto harmony over a driving, locomotive-like melody, and the sudden chiming in of the whole group on a key line. Brian Wilson handled most stages of the group's recording process from the beginning, even though he was not properly credited on most of the early recordings.

Early on, Mike Love sang lead vocals in the rock-oriented songs, while Carl contributed guitar lines on the group's ballads. Jim Miller commented: "On straight rockers they sang tight harmonies behind Love's lead ... on ballads, Brian played his falsetto off against lush, jazz-tinged voicings, often using (for rock) unorthodox harmonic structures." Harrison adds that "even the least distinguished of the Beach Boys' early uptempo rock 'n' roll songs show traces of structural complexity at some level; Brian was simply too curious and experimental to leave convention alone." Although Brian was often dubbed a perfectionist, he was an inexperienced musician, and his understanding of music was mostly self-taught. At the lyric stage, he usually worked with Love, whose assertive persona provided youthful swagger that contrasted Brian's explorations in romanticism and sensitivity. Luis Sanchez noted a pattern where Brian would spare surfing imagery when working with collaborators outside of his band's circle, in the examples "Lonely Sea" and "In My Room".

Brian's bandmates resented the notion that he was the sole creative force in the group. In a 1966 article that asked if "the Beach Boys rely too much on sound genius Brian", Carl said that although Brian was the most responsible for their music, every member of the group contributed ideas. Mike Love wrote, "As far as I was concerned, Brian "was" a genius, deserving of that recognition. But the rest of us were seen as nameless components in Brian's music machine ... It didn't feel to us as if we were just riding on Brian's coattails." Conversely, Dennis defended Brian's stature in the band, stating: "Brian Wilson "is" the Beach Boys. He is the band. We're his fucking messengers. He is all of it. Period. We're nothing. He's everything."

The band's earliest influences came primarily from the work of Chuck Berry and the Four Freshmen. Performed by the Four Freshmen, "Their Hearts Were Full of Spring" (1961) was a particular favorite of the group. By analyzing their arrangements of pop standards, Brian educated himself on jazz harmony. Bearing this in mind, Philip Lambert noted, "If Bob Flanigan helped teach Brian how to sing, then Gershwin, Kern, Porter, and the other members of this pantheon helped him learn how to craft a song." Other general influences on the group included the Hi-Los, the Penguins, the Robins, Bill Haley & His Comets, Otis Williams, the Cadets, the Everly Brothers, the Shirelles, the Regents, and the Crystals.

The eclectic mix of white and black vocal group influences – ranging from the rock and roll of Berry, the jazz harmonies of the Four Freshmen, the pop of the Four Preps, the folk of the Kingston Trio, the R&B of groups like the Coasters and the Five Satins, and the doo wop of Dion and the Belmonts – helped contribute to the Beach Boys' uniqueness in American popular music. Carl remembered: "Most of [Mike's] classmates were black. He was the only white guy on his track team. He was really immersed in doo-wop and that music and I think he influenced Brian to listen to it. The black artists were so much better in terms of rock records in those days that the white records almost sounded like put-ons." On Jimi Hendrix and "heavy" music, Brian said he felt no pressure to go in that direction: "We never got into the heavy musical level trip. We never needed to. It's already been done."

Another significant influence on Brian's work was Burt Bacharach. He said in the 1960s: "Burt Bacharach and Hal David are more like me. They're also the best pop team – per se – today. As a producer, Bacharach has a very fresh, new approach." Regarding surf rock pioneer Dick Dale, Brian said that his influence on the group was limited to Carl and his style of guitar playing. Carl credited Chuck Berry, the Ventures, and John Walker with shaping his guitar style, and that the Beach Boys had learned to play all of the Ventures' songs by ear early in their career.

In 1967, Lou Reed wrote in "Aspen" that the Beach Boys created a "hybrid sound" out of old rock and the Four Freshmen, explaining that such songs as "Let Him Run Wild", "Don't Worry Baby", "I Get Around", and "Fun, Fun, Fun" were not unlike "Peppermint Stick" by the Elchords. Similarly, John Sebastian of the Lovin' Spoonful noted, "Brian had control of this vocal palette of which we had no idea. We had never paid attention to the Four Freshmen or doo-wop combos like the Crew Cuts. Look what gold he mined out of that."

Brian identified each member individually for their vocal range, once detailing the ranges for Carl, Dennis, Jardine ("[they] progress upwards through G, A, and B"), Love ("can go from bass to the E above middle C"), and himself ("I can take the second D in the treble clef"). He declared in 1966 that his greatest interest was to expand modern vocal harmony, owing his fascination with voice to the Four Freshmen, which he considered a "groovy sectional sound." He added, "The harmonies that we are able to produce give us a uniqueness which is really the only important thing you can put into records – some quality that no one else has got. I love peaks in a song – and enhancing them on the control panel. Most of all, I love the human voice for its own sake." For a period, Brian avoided singing falsetto for the group, saying "I thought people thought I was a fairy...the band told me, 'If that's the way you sing, don't worry about it.'"

From lowest intervals to highest, the group's vocal harmony stack usually began with Love or Dennis, followed by Jardine or Carl, and finally Brian on top, according to Jardine, while Carl said that the blend was Love on bottom, Carl above, followed by Dennis or Jardine, and then Brian on top. Jardine explains, "We always sang the same vocal intervals. ... As soon as we heard the chords on the piano we'd figure it out pretty easily. If there was a vocal move [Brian] envisioned, he'd show that particular singer that move. We had somewhat photographic memory as far as the vocal parts were concerned so that [was] never a problem for us." Striving for perfection, Brian insured that his intricate vocal arrangements exercised the group's calculated blend of intonation, attack, phrasing, and expression. Sometimes, he would sing each vocal harmony part alone through multi-track tape.

On the group's blend, Carl said: "[Love] has a beautifully rich, very full-sounding bass voice. Yet his lead singing is real nasal, real punk. [Jardine]'s voice has a bright timbre to it; it really cuts. My voice has a kind of calm sound. We're big oooh-ers; we love to oooh. It's a big, full sound, that's very pleasing to us; it opens up the heart." Rock critic Erik Davis wrote, "The 'purity' of tone and genetic proximity that smoothed their voices was almost creepy, pseudo-castrato, [and] a 'barbershop' sound." Jimmy Webb said, "They used very little vibrato and sing in very straight tones. The voices all lie down beside each other very easily – there's no bumping between them because the pitch is very precise." According to Brian: "Jack Good once told us, 'You sing like eunuchs in a Sistine Chapel,' which was a pretty good quote." Writer Richard Goldstein reported that, according to a fellow journalist who asked Brian about the black roots of his music, Brian's response was: "We're white and we sing white." Goldstein added that when he asked where his approach to vocal harmonies had derived from, Wilson answered: 'Barbershop'."

Nine months after forming, the group achieved national success, and demand for their personal appearance skyrocketed. Biographer James Murphy said, "By most contemporary accounts, they were not a very good live band when they started. ... The Beach Boys learned to play as a band in front of live audiences", eventually to become "one of the best and enduring live bands". For the recording of the Beach Boys' instrumental tracks, Brian arranged many of his compositions for a conglomerate of session musicians later known as "the Wrecking Crew". Their assistance was needed because of the increasingly complicated nature of the material. Afterward, the members only performed the instrumental tracks to certain recordings. It is the belief of Richie Unterberger that, "Before session musicians took over most of the parts, the Beach Boys could play respectably gutsy surf rock as a self-contained unit."

Carl was an exception among the group in that he played alongside these musicians whenever he was available to attend sessions. In archivist Craig Slowinski's view, "One should not sell short Carl's own contributions; the youngest Wilson had developed as a musician sufficiently to play alongside the horde of high-dollar session pros that big brother was now bringing into the studio. Carl's guitar playing [was] a key ingredient."

A common misconception is that Dennis' drumming in the Beach Boys' recordings was filled in exclusively by studio musicians. His drumming is documented on a number of the group's singles, including "I Get Around", "Fun, Fun Fun", and "Don't Worry Baby".

The band members often reflected on the spiritual nature of their music (and music in general), particularly for the recording of "Pet Sounds" and "Smile". Even though the Wilsons did not grow up in a particularly religious household, Carl was described as "the most truly religious person I know" by Brian, and Carl was forthcoming about the group's spiritual beliefs stating: "We believe in God as a kind of universal consciousness. God is love. God is you. God is me. God is everything right here in this room. It's a spiritual concept which inspires a great deal of our music." Carl told "Rave" magazine in 1967 that the group's influences are of a "religious nature", but not any religion in specific, only "an idea based upon that of Universal Consciousness. ... The spiritual concept of happiness and doing good to others is extremely important to the lyric of our songs, and the religious element of some of the better church music is also contained within some of our new work."

Brian is quoted during the "Smile" era: "I'm very religious. Not in the sense of churches, going to church; but like the essence of "all" religion." During the recording of "Pet Sounds", Brian held prayer meetings, later reflecting that "God was with us the whole time we were doing this record ... I could feel that feeling in my brain." In 1966, he explained that he wanted to move into a white spiritual sound, and predicted that the rest of the music industry would follow suit. In 2011, Brian maintained the spirituality was important to his music, and that he did not follow any particular religion.

Carl said that "Smile" was chosen as an album title because of its connection to the group's spiritual beliefs. Brian referred to "Smile" as his "teenage symphony to God", composing a hymn, "Our Prayer", as the album's opening spiritual invocation. Experimentation with psychotropic substances also proved pivotal to the group's development as artists. He spoke of his LSD trips as a "religious experience", and during a session for "Our Prayer", Brian can be heard asking the other Beach Boys: "Do you guys feel any acid yet?". In 1968, the group's interest in transcendental meditation led them to record the original song, "Transcendental Meditation".

The Beach Boys are one of the most critically acclaimed, commercially successful, and influential bands of all time. They have sold over 100 million records worldwide. The group's early songs made them major pop stars in the US, the UK, Australia and other countries, having seven top 10 singles between April 1963 and November 1964. They were one of the first American groups to exhibit the definitive traits of a self-contained rock band, playing their own instruments and writing their own songs, and they were one of the few American bands formed prior to the 1964 British Invasion to continue their success. Among artists of the 1960s, they are one of few central figures in the histories of rock.

Brian Wilson's artistic control over the Beach Boys' records was unprecedented for the time. Carl Wilson elaborated: "Record companies were used to having absolute control over their artists. It was especially nervy, because Brian was a 21-year-old kid with just two albums. It was unheard of. But what could they say? Brian made good records." This made the Beach Boys one of the first rock groups to exert studio control. Music producers after the mid 1960s would draw on Brian's influence, setting a precedent that allowed bands and artists to enter a recording studio and act as producers, either autonomously, or in conjunction with other like minds.

The band routinely appears in the upper reaches of ranked lists such as "The Top 1000 Albums of All Time." Many of the group's songs and albums, including "The Beach Boys Today!", "Smiley Smile", "Sunflower", and "Surf's Up"—and especially "Pet Sounds" and "Good Vibrations"—are featured in numerous lists devoted to the greatest albums or singles of all time. The latter two frequently appear on the number one spot. On Acclaimed Music, which aggregates the rankings of decades of critics' lists, "Pet Sounds" is ranked as the greatest album of all time, while "Good Vibrations" is the third-greatest song of all time ("God Only Knows" is also ranked 21). The group itself is ranked number 11 in its 1000 most recommended artists of all time. In 2004, "Rolling Stone" ranked the band number 12 on the magazine's list of the "100 Greatest Artists of All Time".

In 1988, the core quintet of the Wilson brothers, Love, and Jardine were inducted into the Rock and Roll Hall of Fame. Ten years later, they were selected for the Vocal Group Hall of Fame. In 2004, "Pet Sounds" was preserved in the National Recording Registry by the Library of Congress for being "culturally, historically, and aesthetically significant." Their recordings of "In My Room", "Good Vibrations", "California Girls" and the entire "Pet Sounds" album have been inducted into the Grammy Hall of Fame.

In 2017, a study of AllMusic's catalog indicated the Beach Boys as the 6th most frequently cited artist influence in its database. For the 50th anniversary of "Pet Sounds", 26 artists contributed to a "Pitchfork" retrospective on its influence, which included comments from members of Talking Heads, Yo La Tengo, Chairlift, and Deftones. The editor noted that the "wide swath of artists assembled for this feature represent but a modicum of the album's vast measure of influence. Its scope transcends just about all lines of age, race, and gender. Its impact continues to broaden with each passing generation."

Professor of cultural studies James M. Curtis wrote in 1987, "We can say that the Beach Boys represent the outlook and values of white Protestant Anglo-Saxon teenagers in the early sixties. Having said that, we immediately realize that they must mean much more than this. Their stability, their staying power, and their ability to attract new fans prove as much." Cultural historian Kevin Starr explains that the group first connected with young Americans specifically for their lyrical interpretation of a mythologized landscape: "Cars and the beach, surfing, the California Girl, all this fused in the alembic of youth: Here was a way of life, an iconography, already half-released into the chords and multiple tracks of a new sound." in Robert Christgau's opinion, "the Beach Boys were a touchstone for real rock and rollers, all of whom understood that the music had its most essential roots in an innocently hedonistic materialism."

The group's "California sound" grew to national prominence through the success of their 1963 album "Surfin' U.S.A.", which helped turn the surfing subculture into a mainstream youth-targeted advertising image widely exploited by the film, television, and food industry. The group's surf music was not entirely of their own invention, being preceded by artists such as Dick Dale. However, previous surf musicians did not project a world view as the Beach Boys did. The band's earlier surf music helped raise the profile of the state of California, creating its first major regional style with national significance, and establishing a musical identity for Southern California, as opposed to Hollywood. California ultimately supplanted New York as the center of popular music thanks to the success of Brian's productions.

A 1966 article discussing new trends in rock music writes that the Beach Boys popularized a type of drum beat heard in Jan and Dean's "Surf City", which sounds like "a locomotive getting up speed", in addition to the method of "suddenly stopping in between the chorus and verse". Pete Townshend of the Who is credited with coining the term "power pop", which he defined as "what we play—what the Small Faces used to play, and the kind of pop the Beach Boys played in the days of 'Fun, Fun, Fun' which I preferred."

The California sound gradually evolved to reflect a more musically ambitious and mature world view, becoming less to do with surfing and cars and more about social consciousness and political awareness. Between 1964 and 1969, it fueled innovation and transition, inspiring artists to tackle largely unmentioned themes such as sexual freedom, black pride, drugs, oppositional politics, other countercultural motifs, and war. Soft pop (later known as "sunshine pop") derived in part from this movement. Sunshine pop producers widely imitated the orchestral style of "Pet Sounds"; however, the Beach Boys themselves were rarely representative of the genre, which was rooted in easy-listening and advertising jingles.

By the end of the 1960s, the California sound declined due to a combination of the West Coast's cultural shifts, Wilson's professional and psychological downturn, and the Manson murders, with David Howard calling it the "sunset of the original California Sunshine Sound ... [the] sweetness advocated by the California Myth had led to chilling darkness and unsightly rot". Drawing from the Beach Boys' associations with Charles Manson and former California governor Ronald Reagan, Erik Davis remarked, "The Beach Boys may be the only bridge between those deranged poles. There is a wider range of political and aesthetic sentiments in their records than in any other band in those heady times—like the state [of California], they expand and bloat and contradict themselves."

During the 1970s, advertising jingles and imagery were predominately based on the Beach Boys' early music and image. The group also inspired the development of the West Coast style later dubbed "yacht rock". According to "Jacobin"s Dan O'Sullivan, the band's aesthetic was the first to be "scavenged" by yacht rock acts like Rupert Holmes. O'Sullivan also cites the Beach Boys' recording of "Sloop John B" as the origin of yacht rock's preoccupation with the "sailors and beachgoers" aesthetic that was "lifted by everyone, from Christopher Cross to Eric Carmen, from 'Buffalo Springfield' folksters like Jim Messina to 'Philly Sound' rockers like Hall & Oates."

"Pet Sounds" came to inform the developments of genres such as pop, rock, jazz, electronic, experimental, punk, and hip hop. Similar to subsequent experimental rock LPs by Frank Zappa, the Beatles, and the Who, "Pet Sounds" featured countertextural aspects that called attention to the very recordedness of the album. Professor of American history John Robert Greene stated that the album broke new ground and took rock music away from its casual lyrics and melodic structures into what was then uncharted territory. He furthermore called it one factor which spawned the majority of trends in post-1965 rock music, the only others being "Rubber Soul", the Beatles' "Revolver", and the contemporary folk movement. The album was the first piece in popular music to incorporate the Electro-Theremin, an easier-to-play version of the theremin, as well as the first in rock music to feature a theremin-like instrument. With "Pet Sounds", they were also the first group to make an entire album that departed from the usual small-ensemble electric rock band format.

According to David Leaf in 1978, "Pet Sounds" and "Good Vibrations" "established the group as the leaders of a new type of pop music, Art Rock." Academic Bill Martin states that the band opened a path in rock music "that went from "Sgt. Pepper's" to "Close to the Edge" and beyond". He argues that the advancing technology of multitrack recording and mixing boards were more influential to experimental rock than electronic instruments such as the synthesizer, allowing the Beatles and the Beach Boys to become the first crop of non-classically trained musicians to create extended and complex compositions. In "Strange Sounds: Offbeat Instruments and Sonic Experiments in Pop", Mark Brend writes:

The making of "Good Vibrations", according to Domenic Priore, was "unlike anything previous in the realms of classical, jazz, international, soundtrack, or any other kind of recording", while biographer Peter Ames Carlin wrote that it "sounded like nothing that had ever been played on the radio before." It contained previously untried mixes of instruments, and was the first successful pop song to have cellos in a juddering rhythm. Musicologist Charlie Gillett called it "one of the first records to flaunt studio production as a quality in its own right, rather than as a means of presenting a performance". Again, Brian employed the use of Electro-Theremin for the track. Upon release, the single prompted an unexpected revival in theremins while increasing awareness of analog synthesizers, leading Moog Music to produce their own brand of ribbon-controlled instruments. In a 1968 editorial for "Jazz & Pop", Gene Sculatti predicted that the song "may yet prove to be the most significantly revolutionary piece of the current rock renaissance ... In no minor way, 'Good Vibrations' is a primary influential piece for all producing rock artists; everyone has felt its import to some degree".

Discussing "Smiley Smile", Daniel Harrison argues that the album could "almost" be considered art music in the Western classical tradition, and that the group's innovations in the musical language of rock can be compared to those that introduced atonal and other nontraditional techniques into that classical tradition. He explains, "The spirit of experimentation is just as palpable ... as it is in, say, Schoenberg's op. 11 piano pieces." However, such notions were not widely acknowledged by rock audiences nor by the classically minded at the time. Harrison concludes: "What influences could these innovations then have? The short answer is, not much. "Smiley Smile", "Wild Honey", "Friends", and "20/20" sound like few other rock albums; they are "sui generis". ... It must be remembered that the commercial failure of the Beach Boys' experiments was hardly motivation for imitation." Musicologist David Toop, who included the "Smiley Smile" track "Fall Breaks and Back to Winter" on a companion CD for his book "Ocean of Sound", placed the Beach Boys' effect on sound pioneering in league with Les Baxter, Aphex Twin, Herbie Hancock, King Tubby, and My Bloody Valentine.

"Sunflower" marked an end to the experimental songwriting and production phase initiated by "Smiley Smile". After "Surf's Up", Harrison wrote, their albums "contain a mixture of middle-of-the-road music entirely consonant with pop style during the early 1970s with a few oddities that proved that the desire to push beyond conventional boundaries was not dead," until 1974, "the year in which the Beach Boys ceased to be a rock 'n' roll act and became an oldies act."

In the 1970s, the Beach Boys served a "totemic influence" on punk rock that later gave way to indie rock. Brad Shoup of Stereogum surmised that, thanks to the Ramones' praise for the group, many punk, pop punk, or "punk-adjacent" artists showed influence from the Beach Boys, noting cover versions of the band's songs recorded by Slickee Boys, Agent Orange, Bad Religion, Shonen Knife, the Queers, Hi-Standard, the Descendents, the Donnas, M.O.D., and the Vandals. "The Beach Boys Love You" is sometimes considered the group's "punk album", and "Pet Sounds" is sometimes advanced as the first emo album.

In the 1990s, the Beach Boys experienced a resurgence of popularity with the alternative rock generation. According to Sean O'Hagan, leader of the High Llamas and former member of Stereolab, a younger generation of record-buyers "stopped listening to indie records" in favor of the Beach Boys. Bands who advocated for the Beach Boys included founding members of the Elephant 6 Collective (Neutral Milk Hotel, the Olivia Tremor Control, the Apples in Stereo, and of Montreal). United by a shared love of the group's music, they named Pet Sounds Studio in honor of the band. "Rolling Stone" writer Barry Walters wrote in 2000 that albums such as "Surf's Up" and "Love You" "are becoming sonic blueprints, akin to what early Velvet Underground LPs meant to the previous indie peer group." The High Llamas, Eric Matthews and St. Etienne are among the "alt heroes" who contributed cover versions of "unreleased, overlooked or underappreciated Wilson/Beach Boys obscurities" on the tribute album "Caroline Now!" (2000).

"Smile" became a touchstone for many bands who were labelled "chamber pop", a term used for artists influenced by the lush orchestrations of Brian Wilson, Lee Hazlewood, and Burt Bacharach. "Pitchfork" writer Mark Richardson cited "Smiley Smile" as the origin point of "the kind of lo-fi bedroom pop that would later propel Sebadoh, Animal Collective, and other characters." The "Sunflower" track "All I Wanna Do" is also cited as one of the earliest precursors to chillwave, a microgenre that developed in the 2000s.

Between 1965 and 1967, the Beach Boys developed a musical and lyrical sophistication that contrasted their work from before and after. This divide was further solidified by the difference in sound between their albums and their stage performances. When the band's studio recordings grew more complex, they were unable to effectively reproduce them in their live show. Starting in 1966, band publicist Derek Taylor was instrumental in campaigning the idea of Brian Wilson as a "genius" to members of the burgeoning rock press, painting him as a mastermind who stays at home composing while the rest of the band tour. All of these elements combined to create a split fanbase corresponding to two distinct musical markets. One group is the conservative audience who enjoys the band's early singles as a wholesome representation of American popular culture from before the political and social movements brought on in the mid 1960s. The other group also appreciates the early songs for their energy and complexity, but not as much as the band's ambitious work that was created during the formative psychedelic era.

Initially, rock music journalists valued the Beach Boys' early records over their experimental work. Real surfers were critical of the band for not being true adherents of the sport. As authenticity became a higher concern among critics, the group's legitimacy in rock music became an oft-repeated criticism, especially since their early songs appeared to celebrate a politically unconscious youth culture. Music critic Kenneth Partridge blamed the lack of "edginess" on the group's early records for why they're "rarely talked about in the same breath as the Beatles and the Rolling Stones, and when they are, it's really only because of two albums". The "particular appeal" of Wilson's genius, according to music critic Barney Hoskyns, was "the fact that the Beach Boys were the very obverse of hip – the unlikeliness of these songs growing out of disposable surf pop – and in the singular naivety and ingenuousness of his personality." Luis Sanchez argued that despite the immaturity of their early songs, "what matters is that it captured a lack of self-consciousness—a "genuineness"—that set them apart from their peers. And it was this quality that came to define Brian's oeuvre as he moved beyond and into bigger pop productions that would culminate in "Smile"."

Generally, the record-buying public came to view the music made after "Smile" as the point marking their artistic decline. After "Smiley Smile", the group was virtually blacklisted by the music press, to the extent that reviews of the group's records were either withheld from publication or published long after the release dates. Mike Love said that, unlike Brian, he was never concerned about being taken seriously by critics, and considered the negatively described "simplicity" of their early songs as "elitism at its worst: because so many people loved our music, there must be something wrong with it." In a review of "The Smile Sessions" for "NewMusicBox", Frank Oteri argued that the popular caricature of the Beach Boys' as a "light-hearted party band" ensured that they will never earn themselves "the same pride of place in American music history held by other great innovators". Peter Ames Carlin summarized the group's various phases: "Once surfin' pin-ups, they remade themselves as avant-garde pop artists, then psychedelic oracles. After that they were down-home hippies, then retro-hip icons. Eventually they devolved into none of the above: a kind of perpetual-motion nostalgia machine."

Since the 1990s, there has been an increasing tendency to recontextualize the Beach Boys outside of their typical iconography, with academic Kirk Curnutt citing such examples as the use of "Sloop John B" as Vietnam allegory in the film "Forrest Gump" (1994) and "I Just Wasn't Made for These Times" as an LSD-inspired underscore for one episode of the television drama "Mad Men" (2012).

The Wilsons' California house, where the Wilson brothers grew up and the group began, was demolished in 1986 to make way for Interstate 105, the Century Freeway. A Beach Boys Historic Landmark (California Landmark No. 1041 at 3701 West 119th Street), dedicated on May 20, 2005, marks the location. On December 30, 1980, the Beach Boys were awarded a star on the Hollywood Walk of Fame, located at 1500 Vine Street. On September 2, 1977, the group performed before an audience of 40,000 at Narragansett Park in Pawtucket, Rhode Island, which remains the largest concert audience in Rhode Island history. In 2017, the street where the concert stage formerly stood was officially renamed to "Beach Boys Way".

Current members

Former members
Timeline
Notable supporting musicians for both the Beach Boys' live performances and studio recordings included guitarist Glen Campbell, keyboardists Daryl Dragon and Toni Tennille (Captain & Tennille), and saxophonist Charles Lloyd.

Studio albums

Selected archival releases 

See also


Bibliography

Articles

Books



</doc>
<doc id="4479" url="https://en.wikipedia.org/wiki?curid=4479" title="BCE (disambiguation)">
BCE (disambiguation)

BCE or B.C.E. may stand for:


</doc>
<doc id="4480" url="https://en.wikipedia.org/wiki?curid=4480" title="BC">
BC

BC may refer to:









</doc>
<doc id="4481" url="https://en.wikipedia.org/wiki?curid=4481" title="Beatrix Potter">
Beatrix Potter

Helen Beatrix Potter (, US , 28 July 186622 December 1943) was an English writer, illustrator, natural scientist and conservationist; she was best known for her children's books featuring animals, such as those in "The Tale of Peter Rabbit".

Born into an upper-middle-class household, Potter was educated by governesses and grew up isolated from other children. She had numerous pets and spent holidays in Scotland and the Lake District, developing a love of landscape, flora and fauna, all of which she closely observed and painted.

Potter's study and watercolours of fungi led to her being widely respected in the field of mycology. In her thirties, Potter self-published the highly successful children's book "The Tale of Peter Rabbit". Following this, Potter began writing and illustrating children's books full-time.

In all, Potter wrote thirty books; the best known being her twenty-three children's tales. With the proceeds from the books and a legacy from an aunt, Potter bought Hill Top Farm in Near Sawrey in 1905; this is a village in the Lake District which, at that time, was in Lancashire. Over the following decades, she purchased additional farms to preserve the unique hill country landscape. In 1913, at the age of 47, she married William Heelis, a respected local solicitor from Hawkshead. Potter was also a prize-winning breeder of Herdwick sheep and a prosperous farmer keenly interested in land preservation. She continued to write and illustrate, and to design spin-off merchandise based on her children's books for British publisher Warne until the duties of land management and her diminishing eyesight made it difficult to continue.

Potter died of pneumonia and heart disease on 22 December 1943 at her home in Near Sawrey at the age of 77, leaving almost all her property to the National Trust. She is credited with preserving much of the land that now constitutes the Lake District National Park. Potter's books continue to sell throughout the world in many languages with her stories being retold in songs, films, ballet and animations, and her life depicted in a feature film and television film.

Potter's paternal grandfather, Edmund Potter, from Glossop in Derbyshire, owned what was then the largest calico printing works in England, and later served as a Member of Parliament.

Beatrix's father, Rupert William Potter (1832–1914), was educated at Manchester College by the Unitarian philosopher James Martineau. He then trained as a barrister in London. Rupert practised law, specialising in equity law and conveyancing. He married Helen Leech (1839–1932) on 8 August 1863 at Hyde Unitarian Chapel, Gee Cross. Helen was the daughter of Jane Ashton (1806–1884) and John Leech, a wealthy cotton merchant and shipbuilder from Stalybridge. Helen's first cousins were Harriet Lupton ("née" Ashton), the sister of Thomas Ashton, 1st Baron Ashton of Hyde. It was reported in July 2014 that Beatrix had personally given a number of her own original hand-painted illustrations to the two daughters of Arthur and Harriet Lupton, who were cousins to both Beatrix and Catherine, Duchess of Cambridge.

Beatrix's parents lived comfortably at 2 Bolton Gardens, West Brompton, where Helen Beatrix was born on 28 July 1866 and her brother Walter Bertram on 14 March 1872. Beatrix lived in the house until her marriage in 1913. The house was destroyed in the Blitz. Bousfield Primary School now stands where the house once was. A blue plaque on the school building testifies to the former site of The Potter home.

Both parents were artistically talented, and Rupert was an adept amateur photographer. Rupert had invested in the stock market, and by the early 1890s, he was extremely wealthy.

Potter's family on both sides were from the Manchester area. They were English Unitarians, associated with dissenting Protestant congregations, influential in 19th century England, that affirmed the oneness of God and that rejected the doctrine of the Trinity.

Beatrix was educated by three able governesses, the last of whom was Annie Moore ("née" Carter), just three years older than Beatrix, who tutored Beatrix in German as well as acting as lady's companion. She and Beatrix remained friends throughout their lives, and Annie's eight children were the recipients of many of Potter's delightful picture letters. It was Annie who later suggested that these letters might make good children's books.

She and her younger brother Walter Bertram (1872–1918) grew up with few friends outside their large extended family. Her parents were artistic, interested in nature, and enjoyed the countryside. As children, Beatrix and Bertram had numerous small animals as pets which they observed closely and drew endlessly. In their schoolroom, Beatrix and Bertram kept a variety of small pets, mice, rabbits, a hedgehog and some bats, along with collections of butterflies and other insects which they drew and studied. Beatrix was devoted to the care of her small animals, often taking them with her on long holidays. In most of the first fifteen years of her life, Beatrix spent summer holidays at Dalguise, an estate on the River Tay in Perthshire, Scotland. There she sketched and explored an area that nourished her imagination and her observation. Beatrix and her brother were allowed great freedom in the country, and both children became adept students of natural history. In 1882, when Dalguise was no longer available, the Potters took their first summer holiday in the Lake District, at Wray Castle near Lake Windermere. Here Beatrix met Hardwicke Rawnsley, vicar of Wray and later the founding secretary of the National Trust, whose interest in the countryside and country life inspired the same in Beatrix and who was to have a lasting impact on her life.

At about the age of 14, Beatrix began to keep a diary. It was written in a code of her own devising which was a simple letter for letter substitution. Her "Journal" was important to the development of her creativity, serving as both sketchbook and literary experiment: in tiny handwriting, she reported on society, recorded her impressions of art and artists, recounted stories and observed life around her. The "Journal", decoded and transcribed by Leslie Linder in 1958, does not provide an intimate record of her personal life, but it is an invaluable source for understanding a vibrant part of British society in the late 19th century. It describes Potter's maturing artistic and intellectual interests, her often amusing insights on the places she visited, and her unusual ability to observe nature and to describe it. Started in 1881, her journal ends in 1897 when her artistic and intellectual energies were absorbed in scientific study and in efforts to publish her drawings. Precocious but reserved and often bored, she was searching for more independent activities and wished to earn some money of her own while dutifully taking care of her parents, dealing with her especially demanding mother, and managing their various households.

Beatrix Potter's parents did not discourage higher education. As was common in the Victorian era, women of her class were privately educated and rarely went to university.

Beatrix Potter was interested in every branch of natural science save astronomy. Botany was a passion for most Victorians and nature study was a popular enthusiasm. Potter was eclectic in her tastes: collecting fossils, studying archaeological artefacts from London excavations, and interested in entomology. In all these areas, she drew and painted her specimens with increasing skill. By the 1890s, her scientific interests centred on mycology. First drawn to fungi because of their colours and evanescence in nature and her delight in painting them, her interest deepened after meeting Charles McIntosh, a revered naturalist and amateur mycologist, during a summer holiday in Dunkeld in Perthshire in 1892. He helped improve the accuracy of her illustrations, taught her taxonomy, and supplied her with live specimens to paint during the winter. Curious as to how fungi reproduced, Potter began microscopic drawings of fungus spores (the agarics) and in 1895 developed a theory of their germination. Through the connections of her uncle Sir Henry Enfield Roscoe, a chemist and vice-chancellor of the University of London, she consulted with botanists at Kew Gardens, convincing George Massee of her ability to germinate spores and her theory of hybridisation. She did not believe in the theory of symbiosis proposed by Simon Schwendener, the German mycologist, as previously thought; instead, she proposed a more independent process of reproduction.

Rebuffed by William Thiselton-Dyer, the Director at Kew, because of her sex and her amateur status, Beatrix wrote up her conclusions and submitted a paper, "On the Germination of the Spores of the Agaricineae", to the Linnean Society in 1897. It was introduced by Massee because, as a female, Potter could not attend proceedings or read her paper. She subsequently withdrew it, realising that some of her samples were contaminated, but continued her microscopic studies for several more years. Her paper has only recently been rediscovered, along with the rich, artistic illustrations and drawings that accompanied it. Her work is only now being properly evaluated. Potter later gave her other mycological and scientific drawings to the Armitt Museum and Library in Ambleside, where mycologists still refer to them to identify fungi. There is also a collection of her fungus paintings at the Perth Museum and Art Gallery in Perth, Scotland, donated by Charles McIntosh. In 1967, the mycologist W.P.K. Findlay included many of Potter's beautifully accurate fungus drawings in his "Wayside & Woodland Fungi", thereby fulfilling her desire to one day have her fungus drawings published in a book. In 1997, the Linnean Society issued a posthumous apology to Potter for the sexism displayed in its handling of her research.

Potter's artistic and literary interests were deeply influenced by fairies, fairy tales and fantasy. She was a student of the classic fairy tales of Western Europe. As well as stories from the Old Testament, John Bunyan's "The Pilgrim's Progress" and Harriet Beecher Stowe's "Uncle Tom's Cabin", she grew up with "Aesop's Fables", the fairy tales of the Brothers Grimm and Hans Christian Andersen, Charles Kingsley's "The Water Babies", the folk tales and mythology of Scotland, the German Romantics, Shakespeare, and the romances of Sir Walter Scott. As a young child, before the age of eight, Edward Lear's "Book of Nonsense", including the much loved "The Owl and the Pussycat", and Lewis Carroll's "Alice in Wonderland" had made their impression, although she later said of "Alice" that she was more interested in Tenniel's illustrations than what they were about. The "Brer Rabbit" stories of Joel Chandler Harris had been family favourites, and she later studied his "Uncle Remus" stories and illustrated them. She studied book illustration from a young age and developed her own tastes, but the work of the picture book triumvirate Walter Crane, Kate Greenaway and Randolph Caldecott, the last an illustrator whose work was later collected by her father, was a great influence. When she started to illustrate, she chose first the traditional rhymes and stories, "Cinderella", "Sleeping Beauty", "Ali Baba and the Forty Thieves", "Puss-in-boots", and "Red Riding Hood". However, most often her illustrations were fantasies featuring her own pets: mice, rabbits, kittens, and guinea pigs.

In her teenage years, Potter was a regular visitor to the art galleries of London, particularly enjoying the summer and winter exhibitions at the Royal Academy in London. Her "Journal" reveals her growing sophistication as a critic as well as the influence of her father's friend, the artist Sir John Everett Millais, who recognised Beatrix's talent of observation. Although Potter was aware of art and artistic trends, her drawing and her prose style were uniquely her own.

As a way to earn money in the 1890s, Beatrix and her brother began to print Christmas cards of their own design, as well as cards for special occasions. Mice and rabbits were the most frequent subject of her fantasy paintings. In 1890, the firm of Hildesheimer and Faulkner bought several of the drawings of her rabbit Benjamin Bunny to illustrate verses by Frederic Weatherly titled "A Happy Pair". In 1893, the same printer bought several more drawings for Weatherly's "Our Dear Relations", another book of rhymes, and the following year Potter sold a series of frog illustrations and verses for "Changing Pictures", a popular annual offered by the art publisher Ernest Nister. Potter was pleased by this success and determined to publish her own illustrated stories.

Whenever Potter went on holiday to the Lake District or Scotland, she sent letters to young friends, illustrating them with quick sketches. Many of these letters were written to the children of her former governess Annie Carter Moore, particularly to Moore's eldest son Noel who was often ill. In September 1893, Potter was on holiday at Eastwood in Dunkeld, Perthshire. She had run out of things to say to Noel, and so she told him a story about "four little rabbits whose names were Flopsy, Mopsy, Cottontail and Peter". It became one of the most famous children's letters ever written and the basis of Potter's future career as a writer-artist-storyteller.

In 1900, Potter revised her tale about the four little rabbits, and fashioned a dummy book of it – it has been suggested, in imitation of Helen Bannerman's 1899 bestseller "The Story of Little Black Sambo". Unable to find a buyer for the work, she published it for family and friends at her own expense in December 1901. It was drawn in black and white with a coloured frontispiece. Rawnsley had great faith in Potter's tale, recast it in didactic verse, and made the rounds of the London publishing houses. Frederick Warne & Co had previously rejected the tale but, eager to compete in the booming small format children's book market, reconsidered and accepted the "bunny book" (as the firm called it) following the recommendation of their prominent children's book artist L. Leslie Brooke. The firm declined Rawnsley's verse in favour of Potter's original prose, and Potter agreed to colour her pen and ink illustrations, choosing the then-new Hentschel three-colour process to reproduce her watercolours.

On 2 October 1902, "The Tale of Peter Rabbit" was published, and was an immediate success. It was followed the next year by "The Tale of Squirrel Nutkin" and "The Tailor of Gloucester", which had also first been written as picture letters to the Moore children. Working with Norman Warne as her editor, Potter published two or three little books each year: 23 books in all. The last book in this format was "Cecily Parsley's Nursery Rhymes" in 1922, a collection of favourite rhymes. Although "The Tale of Little Pig Robinson" was not published until 1930, it had been written much earlier. Potter continued creating her little books until after the First World War when her energies were increasingly directed toward her farming, sheep-breeding and land conservation.

The immense popularity of Potter's books was based on the lively quality of her illustrations, the non-didactic nature of her stories, the depiction of the rural countryside, and the imaginative qualities she lent to her animal characters.

Potter was also a canny businesswoman. As early as 1903, she made and patented a Peter Rabbit doll. It was followed by other "spin-off" merchandise over the years, including painting books, board games, wall-paper, figurines, baby blankets and china tea-sets. All were licensed by Frederick Warne & Co and earned Potter an independent income, as well as immense profits for her publisher.

In 1905, Potter and Norman Warne became unofficially engaged. Potter's parents objected to the match because Warne was "in trade" and thus not socially suitable. The engagement lasted only one month until Warne died of pernicious anaemia at age 37. That same year, Potter used some of her income and a small inheritance from an aunt to buy Hill Top Farm in Near Sawrey in the English Lake District near Windermere. Potter and Warne may have hoped that Hill Top Farm would be their holiday home, but after Warne's death, Potter went ahead with its purchase as she had always wanted to own that farm, and live in "that charming village".

The tenant farmer John Cannon and his family agreed to stay on to manage the farm for her while she made physical improvements and learned the techniques of fell farming and of raising livestock, including pigs, cows and chickens; the following year she added sheep. Realising she needed to protect her boundaries, she sought advice from W.H. Heelis & Son, a local firm of solicitors with offices in nearby Hawkshead. With William Heelis acting for her, she bought contiguous pasture, and in 1909 the Castle Farm across the road from Hill Top Farm. She visited Hill Top at every opportunity, and her books written during this period (such as "The Tale of Ginger and Pickles", about the local shop in Near Sawrey and "The Tale of Mrs. Tittlemouse", a wood mouse) reflect her increasing participation in village life and her delight in country living.

Owning and managing these working farms required routine collaboration with the widely respected William Heelis. By the summer of 1912, Heelis had proposed marriage and Beatrix had accepted; although she did not immediately tell her parents, who once again disapproved because Heelis was only a country solicitor. Potter and Heelis were married on 15 October 1913 in London at St Mary Abbots in Kensington. The couple moved immediately to Near Sawrey, residing at Castle Cottage, the renovated farmhouse on Castle Farm, which was 34 acres large. Hill Top remained a working farm but was now remodelled to allow for the tenant family and Potter's private studio and workshop. At last her own woman, Potter settled into the partnerships that shaped the rest of her life: her country solicitor husband and his large family, her farms, the Sawrey community and the predictable rounds of country life. "The Tale of Jemima Puddle-Duck" and "The Tale of Tom Kitten" are representative of Hill Top Farm and her farming life and reflect her happiness with her country life.

Rupert Potter died in 1914 and, with the outbreak of World War I, Potter, now a wealthy woman, persuaded her mother to move to the Lake District and found a property for her to rent in Sawrey. Finding life in Sawrey dull, Helen Potter soon moved to Lindeth Howe (now a 34 bedroomed hotel) a large house the Potters had previously rented for the summer in Bowness, on the other side of Lake Windermere, Potter continued to write stories for Frederick Warne & Co and fully participated in country life. She established a Nursing Trust for local villages and served on various committees and councils responsible for footpaths and other rural issues.

Soon after acquiring Hill Top Farm, Potter became keenly interested in the breeding and raising of Herdwick sheep, the indigenous fell sheep. In 1923 she bought a large sheep farm in the Troutbeck Valley called Troutbeck Park Farm, formerly a deer park, restoring its land with thousands of Herdwick sheep. This established her as one of the major Herdwick sheep farmers in the county. She was admired by her shepherds and farm managers for her willingness to experiment with the latest biological remedies for the common diseases of sheep, and for her employment of the best shepherds, sheep breeders, and farm managers.

By the late 1920s, Potter and her Hill Top farm manager Tom Storey had made a name for their prize-winning Herdwick flock, which took many prizes at the local agricultural shows, where Potter was often asked to serve as a judge. In 1942 she became President-elect of the Herdwick Sheepbreeders' Association, the first time a woman had been elected but died before taking office.

Potter had been a disciple of the land conservation and preservation ideals of her long-time friend and mentor, Canon Hardwicke Rawnsley, the first secretary and founding member of the National Trust for Places of Historic Interest or Natural Beauty. She supported the efforts of the National Trust to preserve not just the places of extraordinary beauty but also those heads of valleys and low grazing lands that would be irreparably ruined by development. Potter was also an authority on the traditional Lakeland crafts, period furniture and stonework. She restored and preserved the farms that she bought or managed, making sure that each farm house had in it a piece of antique Lakeland furniture. Potter was interested in preserving not only the Herdwick sheep but also the way of life of fell farming. In 1930 the Heelises became partners with the National Trust in buying and managing the fell farms included in the large Monk Coniston Estate. The estate was composed of many farms spread over a wide area of north-western Lancashire, including the Tarn Hows. Potter was the "de facto" estate manager for the Trust for seven years until the National Trust could afford to repurchase most of the property from her. Potter's stewardship of these farms earned her full regard, but she was not without her critics, not the least of which were her contemporaries who felt she used her wealth and the position of her husband to acquire properties in advance of their being made public. She was notable in observing the problems of afforestation, preserving the intake grazing lands, and husbanding the quarries and timber on these farms. All her farms were stocked with Herdwick sheep and frequently with Galloway cattle.
Potter continued to write stories and to draw, although mostly for her own pleasure. Her books in the late 1920s included the semi-autobiographical "The Fairy Caravan", a fanciful tale set in her beloved Troutbeck fells. It was published only in the US during Potter's lifetime, and not until 1952 in the UK. "Sister Anne", Potter's version of the story of Bluebeard, was written for her American readers, but illustrated by Katharine Sturges. A final folktale, "Wag by Wall", was published posthumously by "The Horn Book Magazine" in 1944. Potter was a generous patron of the Girl Guides, whose troupes she allowed to make their summer encampments on her land, and whose company she enjoyed as an older woman.

Potter and William Heelis enjoyed a happy marriage of thirty years, continuing their farming and preservation efforts throughout the hard days of World War II. Although they were childless, Potter played an important role in William's large family, particularly enjoying her relationship with several nieces whom she helped educate, and giving comfort and aid to her husband's brothers and sisters.

Potter died of complications from pneumonia and heart disease on 22 December 1943 at Castle Cottage, and her remains were cremated at Carleton Crematorium. She left nearly all her property to the National Trust, including over of land, sixteen farms, cottages and herds of cattle and Herdwick sheep. Hers was the largest gift at that time to the National Trust, and it enabled the preservation of the land now included in the Lake District National Park and the continuation of fell farming. The central office of the National Trust in Swindon was named "Heelis" in 2005 in her memory. William Heelis continued his stewardship of their properties and of her literary and artistic work for the twenty months he survived her. When he died in August 1945, he left the remainder to the National Trust.

Potter left almost all the original illustrations for her books to the National Trust. The copyright to her stories and merchandise was then given to her publisher Frederick Warne & Co, now a division of the Penguin Group. On 1 January 2014, the copyright expired in the UK and other countries with a 70-years-after-death limit. Hill Top Farm was opened to the public by the National Trust in 1946; her artwork was displayed there until 1985 when it was moved to William Heelis's former law offices in Hawkshead, also owned by the National Trust as the Beatrix Potter Gallery.

Potter gave her folios of mycological drawings to the Armitt Library and Museum in Ambleside before her death. "The Tale of Peter Rabbit" is owned by Frederick Warne and Company, "The Tailor of Gloucester" by the Tate Gallery and "The Tale of the Flopsy Bunnies" by the British Museum.

The largest public collection of her letters and drawings is the Leslie Linder Bequest and Leslie Linder Collection at the Victoria and Albert Museum in London. In the United States, the largest public collections are those in the Rare Book Department of the Free Library of Philadelphia, and the Cotsen Children's Library at Princeton University.

In 2015 a manuscript for an unpublished book was discovered by Jo Hanks, a publisher at Penguin Random House Children's Books, in the Victoria and Albert Museum archive. The book "The Tale of Kitty-in-Boots", with illustrations by Quentin Blake, was published 1 September 2016, to mark the 150th anniversary of Potter's birth.

In 2017, "The Art of Beatrix Potter: Sketches, Paintings, and Illustrations" by Emily Zach was published after San Francisco publisher Chronicle Books decided to mark the 150th anniversary of Beatrix Potter's birth by showing that she was "far more than a 19th-century weekend painter. She was an artist of astonishing range."

In December 2017, the asteroid 13975 Beatrixpotter, discovered by Belgian astronomer Eric Elst in 1992, was named in her memory.

There are many interpretations of Potter's literary work, the sources of her art, and her life and times. These include critical evaluations of her corpus of children's literature and Modernist interpretations of Humphrey Carpenter and Katherine Chandler. Judy Taylor, "That Naughty Rabbit: Beatrix Potter and Peter Rabbit" (rev. 2002) tells the story of the first publication and many editions.

Potter's country life and her farming have been discussed in the work of Susan Denyer and other authors in the publications of The National Trust, such as "Beatrix Potter at Home in the Lake District" (2004).

Potter's work as a scientific illustrator and her work in mycology are discussed in Linda Lear's books "Beatrix Potter: A Life in Nature" (2006) and "Beatrix Potter: The Extraordinary Life of a Victorian Genius" (2008).

In 1971, a ballet film was released, "The Tales of Beatrix Potter", directed by Reginald Mills, set to music by John Lanchbery with choreography by Frederick Ashton, and performed in character costume by members of the Royal Ballet and the Royal Opera House orchestra. The ballet of the same name has been performed by other dance companies around the world.

In 1992, Potter's famous children's book "The Tale of Benjamin Bunny" was featured in the film "Lorenzo's Oil".

Potter is also featured in Susan Wittig Albert's series of light mysteries called The Cottage Tales of Beatrix Potter. The first of the eight-book series is "Tale of Hill Top Farm" (2004), which deals with Potter's life in the Lake District and the village of Near Sawrey between 1905 and 1913.

In 1982, the BBC produced "The Tale of Beatrix Potter". This dramatization of her life was written by John Hawkesworth, directed by Bill Hayes, and starred Holly Aird and Penelope Wilton as the young and adult Beatrix, respectively. "The World of Peter Rabbit and Friends", a TV series based on her stories, which starred actress Niamh Cusack as Beatrix Potter.

In 1993, Weston Woods Studios made an almost hour non-story film called "Beatrix Potter: Artist, Storyteller, and Countrywoman" with narration by Lynn Redgrave and music by Ernest Troost.

In 2006, Chris Noonan directed "Miss Potter", a biographical film of Potter's life focusing on her early career and romance with her editor Norman Warne. The film stars Renée Zellweger, Ewan McGregor and Emily Watson.

On 9 February 2018, Columbia Pictures released "Peter Rabbit", directed by Will Gluck, based on the work by Potter.






</doc>
<doc id="4482" url="https://en.wikipedia.org/wiki?curid=4482" title="Liberal Party (UK)">
Liberal Party (UK)

The Liberal Party was one of the two major political parties in the United Kingdom with the opposing Conservative Party in the 19th and early 20th centuries. The party arose from an alliance of Whigs and free trade-supporting Peelites and the reformist Radicals in the 1850s. By the end of the 19th century, it had formed four governments under William Gladstone. Despite being divided over the issue of Irish Home Rule, the party returned to government in 1905 and then won a landslide victory in the following year's general election.

Under prime ministers Henry Campbell-Bannerman (1905–1908) and H. H. Asquith (1908–1916), the Liberal Party passed the welfare reforms that created a basic British welfare state. Although Asquith was the party's leader, its dominant figure was David Lloyd George. Asquith was overwhelmed by the wartime role of coalition prime minister and Lloyd George replaced him as prime minister in late 1916, but Asquith remained as Liberal Party leader. The pair fought for years over control of the party, badly weakening it in the process. In "The Oxford Companion to British History", historian Martin Pugh argues:
The government of Lloyd George was dominated by the Conservative Party, which finally deposed him in 1922. By the end of the 1920s, the Labour Party had replaced the Liberals as the Conservatives' main rival. The Liberal Party went into decline after 1918 and by the 1950s won no more than six seats at general elections. Apart from notable by-election victories, its fortunes did not improve significantly until it formed the SDP–Liberal Alliance with the newly formed Social Democratic Party (SDP) in 1981. At the 1983 general election, the Alliance won over a quarter of the vote, but only 23 of the 650 seats it contested. At the 1987 general election, its share of the vote fell below 23% and the Liberals and Social Democratic Party merged in 1988 to form the Liberal Democrats. A splinter group reconstituted the Liberal Party in 1989. 

Prominent intellectuals associated with the Liberal Party include the philosopher John Stuart Mill, the economist John Maynard Keynes and social planner William Beveridge.

The Liberal Party grew out of the Whigs, who had their origins in an aristocratic faction in the reign of Charles II and the early 19th century Radicals. The Whigs were in favour of reducing the power of the Crown and increasing the power of Parliament. Although their motives in this were originally to gain more power for themselves, the more idealistic Whigs gradually came to support an expansion of democracy for its own sake. The great figures of reformist Whiggery were Charles James Fox (died 1806) and his disciple and successor Earl Grey. After decades in opposition, the Whigs returned to power under Grey in 1830 and carried the First Reform Act in 1832.

The Reform Act was the climax of Whiggism, but it also brought about the Whigs' demise. The admission of the middle classes to the franchise and to the House of Commons led eventually to the development of a systematic middle class liberalism and the end of Whiggery, although for many years reforming aristocrats held senior positions in the party. In the years after Grey's retirement, the party was led first by Lord Melbourne, a fairly traditional Whig, and then by Lord John Russell, the son of a Duke but a crusading radical, and by Lord Palmerston, a renegade Irish Tory and essentially a conservative, although capable of radical gestures.

As early as 1839, Russell had adopted the name of "Liberals", but in reality his party was a loose coalition of Whigs in the House of Lords and Radicals in the Commons. The leading Radicals were John Bright and Richard Cobden, who represented the manufacturing towns which had gained representation under the Reform Act. They favoured social reform, personal liberty, reducing the powers of the Crown and the Church of England (many Liberals were Nonconformists), avoidance of war and foreign alliances (which were bad for business) and above all free trade. For a century, free trade remained the one cause which could unite all Liberals.

In 1841, the Liberals lost office to the Conservatives under Sir Robert Peel, but their period in opposition was short because the Conservatives split over the repeal of the Corn Laws, a free trade issue; and a faction known as the Peelites (but not Peel himself, who died soon after) defected to the Liberal side. This allowed ministries led by Russell, Palmerston and the Peelite Lord Aberdeen to hold office for most of the 1850s and 1860s. A leading Peelite was William Ewart Gladstone, who was a reforming Chancellor of the Exchequer in most of these governments. The formal foundation of the Liberal Party is traditionally traced to 1859 and the formation of Palmerston's second government.

However, the Whig-Radical amalgam could not become a true modern political party while it was dominated by aristocrats and it was not until the departure of the "Two Terrible Old Men", Russell and Palmerston, that Gladstone could become the first leader of the modern Liberal Party. This was brought about by Palmerston's death in 1865 and Russell's retirement in 1868. After a brief Conservative government (during which the Second Reform Act was passed by agreement between the parties), Gladstone won a huge victory at the 1868 election and formed the first Liberal government. The establishment of the party as a national membership organisation came with the foundation of the National Liberal Federation in 1877. The philosopher John Stuart Mill was also a Liberal MP from 1865 to 1868.

For the next thirty years Gladstone and Liberalism were synonymous. William Ewart Gladstone served as prime minister four times (1868–74, 1880–85, 1886, and 1892–94). His financial policies, based on the notion of balanced budgets, low taxes and "laissez-faire", were suited to a developing capitalist society, but they could not respond effectively as economic and social conditions changed. Called the "Grand Old Man" later in life, Gladstone was always a dynamic popular orator who appealed strongly to the working class and to the lower middle class. Deeply religious, Gladstone brought a new moral tone to politics, with his evangelical sensibility and his opposition to aristocracy. His moralism often angered his upper-class opponents (including Queen Victoria), and his heavy-handed control split the Liberal Party.

In foreign policy, Gladstone was in general against foreign entanglements, but he did not resist the realities of imperialism. For example, he ordered the occupation of Egypt by British forces in 1882. His goal was to create a European order based on co-operation rather than conflict and on mutual trust instead of rivalry and suspicion; the rule of law was to supplant the reign of force and self-interest. This Gladstonian concept of a harmonious Concert of Europe was opposed to and ultimately defeated by a Bismarckian system of manipulated alliances and antagonisms.

As prime minister from 1868 to 1874, Gladstone headed a Liberal Party which was a coalition of Peelites like himself, Whigs and Radicals. He was now a spokesman for "peace, economy and reform". One major achievement was the Elementary Education Act of 1870, which provided England with an adequate system of elementary schools for the first time. He also secured the abolition of the purchase of commissions in the army and of religious tests for admission to Oxford and Cambridge; the introduction of the secret ballot in elections; the legalization of trade unions; and the reorganization of the judiciary in the Judicature Act.

Regarding Ireland, the major Liberal achievements were land reform, where he ended centuries of landlord oppression, and the disestablishment of the (Anglican) Church of Ireland through the Irish Church Act 1869.

In the 1874 general election Gladstone was defeated by the Conservatives under Benjamin Disraeli during a sharp economic recession. He formally resigned as Liberal leader and was succeeded by the Marquess of Hartington, but he soon changed his mind and returned to active politics. He strongly disagreed with Disraeli's pro-Ottoman foreign policy and in 1880 he conducted the first outdoor mass-election campaign in Britain, known as the Midlothian campaign. The Liberals won a large majority in the 1880 election. Hartington ceded his place and Gladstone resumed office.

Among the consequences of the Third Reform Act (1884) was the giving of the vote to many Catholics in Ireland. In the 1885 general election the Irish Parliamentary Party held the balance of power in the House of Commons, and demanded Irish Home Rule as the price of support for a continued Gladstone ministry. Gladstone personally supported Home Rule, but a strong Liberal Unionist faction led by Joseph Chamberlain, along with the last of the Whigs, Hartington, opposed it. The Irish Home Rule bill proposed to offer all owners of Irish land a chance to sell to the state at a price equal to 20 years' purchase of the rents and allowing tenants to purchase the land. Irish nationalist reaction was mixed, Unionist opinion was hostile, and the election addresses during the 1886 election revealed English radicals to be against the bill also. Among the Liberal rank and file, several Gladstonian candidates disowned the bill, reflecting fears at the constituency level that the interests of the working people were being sacrificed to finance a costly rescue operation for the landed élite. Further, Home Rule had not been promised in the Liberals' election manifesto, and so the impression was given that Gladstone was buying Irish support in a rather desperate manner to hold on to power.

The result was a catastrophic split in the Liberal Party, and heavy defeat in the 1886 election at the hands of Lord Salisbury, who was supported by the breakaway Liberal Unionist Party. There was a final weak Gladstone ministry in 1892, but it also was dependent on Irish support and failed to get Irish Home Rule through the House of Lords.

Historically, the aristocracy was divided between Conservatives and Liberals. However, when Gladstone committed to home rule for Ireland, Britain's upper classes largely abandoned the Liberal party, giving the Conservatives a large permanent majority in the House of Lords. Following the Queen, High Society in London largely ostracized home rulers and Liberal clubs were badly split. Joseph Chamberlain took a major element of upper-class supporters out of the Party and into a third party called Liberal Unionism on the Irish issue. It collaborated with and eventually merged into the Conservative party. The Gladstonian liberals in 1891 adopted The Newcastle Programme that included home rule for Ireland, disestablishment of the Church of England in Wales, tighter controls on the sale of liquor, major extension of factory regulation and various democratic political reforms. The Programme had a strong appeal to the nonconformist middle-class Liberal element, which felt liberated by the departure of the aristocracy.

A major long-term consequence of the Third Reform Act was the rise of Lib-Lab candidates, in the absence of any committed Labour Party. The Act split all county constituencies (which were represented by multiple MPs) into single-member constituencies, roughly corresponding to population patterns. In areas with working class majorities, in particular coal-mining areas, Lib-Lab candidates were popular, and they received sponsorship and endorsement from trade unions. In the first election after the Act was passed (1885), thirteen were elected, up from two in 1874. The Third Reform Act also facilitated the demise of the Whig old guard: in two-member constituencies, it was common to pair a Whig and a radical under the Liberal banner. After the Third Reform Act, fewer Whigs were selected as candidates.

A broad range of interventionist reforms were introduced by the 1892–1895 Liberal government. Amongst other measures, standards of accommodation and of teaching in schools were improved, factory inspection was made more stringent, and ministers used their powers to increase the wages and reduce the working hours of large numbers of male workers employed by the state.

Historian Walter L. Arnstein concludes:

Gladstone finally retired in 1894. Gladstone's support for Home Rule deeply divided the party, and it lost its upper and upper-middle-class base, while keeping support among Protestant nonconformists and the Celtic fringe. Historian R. C. K. Ensor reports that after 1886, the main Liberal Party was deserted by practically the entire whig peerage and the great majority of the upper-class and upper-middle-class members. High prestige London clubs that had a Liberal base were deeply split. Ensor notes that, "London society, following the known views of the Queen, practically ostracized home rulers." 

The new Liberal leader was the ineffectual Lord Rosebery. He led the party to a heavy defeat in the 1895 general election.

The Liberal Party lacked a unified ideological base in 1906. It contained numerous contradictory and hostile factions, such as imperialists and supporters of the Boers; near-socialists and laissez-faire classical liberals; suffragettes and opponents of women's suffrage; antiwar elements and supporters of the military alliance with France. Nonconformists – Protestants outside the Anglican fold – were a powerful element, dedicated to opposing the established church in terms of education and taxation. However, the non-conformists were losing support amid society at large and played a lesser role in party affairs after 1900. The party, furthermore, also included Irish Catholics, and secularists from the labour movement. Many Conservatives (including Winston Churchill) had recently protested against high tariff moves by the Conservatives by switching to the anti-tariff Liberal camp, but it was unclear how many old Conservative traits they brought along, especially on military and naval issues.

The middle-class business, professional and intellectual communities were generally strongholds, although some old aristocratic families played important roles as well. The working-class element was moving rapidly toward the newly emerging Labour Party. One uniting element was widespread agreement on the use of politics and Parliament as a device to upgrade and improve society and to reform politics. All Liberals were outraged when Conservatives used their majority in the House of Lords to block reform legislation. In the House of Lords, the Liberals had lost most of their members, who in the 1890s "became Conservative in all but name." The government could force the unwilling king to create new Liberal peers, and that threat did prove decisive in the battle for dominance of Commons over Lords in 1911.

The late nineteenth century saw the emergence of New Liberalism within the Liberal Party, which advocated state intervention as a means of guaranteeing freedom and removing obstacles to it such as poverty and unemployment. The policies of the New Liberalism are now known as social liberalism.
The New Liberals included intellectuals like L. T. Hobhouse, and John A. Hobson. They saw individual liberty as something achievable only under favourable social and economic circumstances. In their view, the poverty, squalor, and ignorance in which many people lived made it impossible for freedom and individuality to flourish. New Liberals believed that these conditions could be ameliorated only through collective action coordinated by a strong, welfare-oriented, and interventionist state.

After the historic 1906 victory, the Liberal Party introduced multiple reforms on range of issues, including health insurance, unemployment insurance, and pensions for elderly workers, thereby laying the groundwork for the future British welfare state. Some proposals failed, such as licensing fewer pubs, or rolling back Conservative educational policies. The People's Budget of 1909, championed by David Lloyd George and fellow Liberal Winston Churchill, introduced unprecedented taxes on the wealthy in Britain and radical social welfare programmes to the country's policies. It was the first budget with the expressed intent of redistributing wealth among the public. It imposed increased taxes on luxuries, liquor, tobacco, high incomes, and land – taxation that fell heavily on the rich. The new money was to be made available for new welfare programmes as well as new battleships. In 1911 Lloyd George succeeded in putting through Parliament his National Insurance Act, making provision for sickness and invalidism, and this was followed by his Unemployment Insurance Act.

Historian Peter Weiler argues:
Contrasting Old Liberalism with New Liberalism, David Lloyd George noted in a 1908 speech the following:

The Liberals languished in opposition for a decade while the coalition of Salisbury and Chamberlain held power. The 1890s were marred by infighting between the three principal successors to Gladstone, party leader William Harcourt, former prime minister Lord Rosebery, and Gladstone's personal secretary, John Morley. This intrigue finally led Harcourt and Morley to resign their positions in 1898 as they continued to be at loggerheads with Rosebery over Irish home rule and issues relating to imperialism. Replacing Harcourt as party leader was Sir Henry Campbell-Bannerman. Harcourt's resignation briefly muted the turmoil in the party, but the beginning of the Second Boer War soon nearly broke the party apart, with Rosebery and a circle of supporters including important future Liberal figures H. H. Asquith, Edward Grey and Richard Burdon Haldane forming a clique dubbed the Liberal Imperialists that supported the government in the prosecution of the war. On the other side, more radical members of the party formed a Pro-Boer faction that denounced the conflict and called for an immediate end to hostilities. Quickly rising to prominence among the Pro-Boers was David Lloyd George, a relatively new MP and a master of rhetoric, who took advantage of having a national stage to speak out on a controversial issue to make his name in the party. Harcourt and Morley also sided with this group, though with slightly different aims. Campbell-Bannerman tried to keep these forces together at the head of a moderate Liberal rump, but in 1901 he delivered a speech on the government's "methods of barbarism" in South Africa that pulled him further to the left and nearly tore the party in two. The party was saved after Salisbury's retirement in 1902 when his successor, Arthur Balfour, pushed a series of unpopular initiatives such as the Education Act 1902 and Joseph Chamberlain called for a new system of protectionist tariffs.

Campbell-Bannerman was able to rally the party around the traditional liberal platform of free trade and land reform and led them to the greatest election victory in their history. This would prove the last time the Liberals won a majority in their own right. Although he presided over a large majority, Sir Henry Campbell-Bannerman was overshadowed by his ministers, most notably H. H. Asquith at the Exchequer, Edward Grey at the Foreign Office, Richard Burdon Haldane at the War Office and David Lloyd George at the Board of Trade. Campbell-Bannerman retired in 1908 and died soon after. He was succeeded by Asquith, who stepped up the government's radicalism. Lloyd George succeeded Asquith at the Exchequer, and was in turn succeeded at the Board of Trade by Winston Churchill, a recent defector from the Conservatives.
The 1906 general election also represented a shift to the left by the Liberal Party. According to Rosemary Rees, almost half of the Liberal MPs elected in 1906 were supportive of the 'New Liberalism' (which advocated government action to improve people's lives),) while claims were made that “five-sixths of the Liberal party are left wing.” Other historians, however, have questioned the extent to which the Liberal Party experienced a leftward shift; according to Robert C. Self however, only between 50 and 60 Liberal MPs out of the 400 in the parliamentary party after 1906 were Social Radicals, with a core of 20 to 30. Nevertheless, important junior offices were held in the cabinet by what Duncan Tanner has termed "genuine New Liberals, Centrist reformers, and Fabian collectivists," and much legislation was pushed through by the Liberals in government. This included the regulation of working hours, National Insurance and welfare.
A political battle erupted over the People's Budget and resulted in the passage of an act ending the power of the House of Lords to block legislation. The cost was high, however, as the government was required by the king to call two general elections in 1910 to validate its position and ended up frittering away most of its large majority, being left once again dependent on the Irish Nationalists.

As a result, Asquith was forced to introduce a new third Home Rule bill in 1912. Since the House of Lords no longer had the power to block the bill, the Unionist's Ulster Volunteers led by Sir Edward Carson, launched a campaign of opposition that included the threat of armed resistance in Ulster and the threat of mass resignation of their commissions by army officers in Ireland in 1914 ("see Curragh Incident"). In their resistance to Home Rule the Ulster Protestants had the full support of the Conservatives, whose leader, Bonar Law, was of Ulster-Scots descent. The country seemed to be on the brink of civil war when the First World War broke out in August 1914. Historian George Dangerfield has argued that the multiplicity of crises in 1910 to 1914, before the war broke out, so weakened the Liberal coalition that it marked the "Strange Death of Liberal England". However, most historians date the collapse to the crisis of the First World War.

The Liberal Party might have survived a short war, but the totality of the Great War called for measures that the Party had long rejected. The result was the permanent destruction of the ability of the Liberal Party to lead a government. Historian Robert Blake explains the dilemma:
Blake further notes that it was the Liberals, not the Conservatives who needed the moral outrage of Belgium to justify going to war, while the Conservatives called for intervention from the start of the crisis on the grounds of "realpolitik" and the balance of power. However, Lloyd George and Churchill were zealous supporters of the war, and gradually forced the old peace-orientated Liberals out. 

Asquith was blamed for the poor British performance in the first year. Since the Liberals ran the war without consulting the Conservatives, there were heavy partisan attacks. However, even Liberal commentators were dismayed by the lack of energy at the top. At the time, public opinion was intensely hostile, both in the media and in the street, against any young man in civilian garb and labeled as a slacker. The leading Liberal newspaper, the "Manchester Guardian" complained:
Asquith's Liberal government was brought down in , due in particular to a crisis in inadequate artillery shell production and the protest resignation of Admiral Fisher over the disastrous Gallipoli Campaign against Turkey. Reluctant to face doom in an election, Asquith formed a new coalition government on 25 May, with the majority of the new cabinet coming from his own Liberal party and the Unionist (Conservative) party, along with a token Labour representation. The new government lasted a year and a half, and was the last time Liberals controlled the government. The analysis of historian A. J. P. Taylor is that the British people were so deeply divided over numerous issues, But on all sides there was growing distrust of the Asquith government. There was no agreement whatsoever on wartime issues. The leaders of the two parties realized that embittered debates in Parliament would further undermine popular morale and so the House of Commons did not once discuss the war before May 1915. Taylor argues:
The 1915 coalition fell apart at the end of 1916, when the Conservatives withdrew their support from Asquith and gave it instead to Lloyd George, who became prime minister at the head of a new coalition largely made up of Conservatives. Asquith and his followers moved to the opposition benches in Parliament and the Liberal Party was deeply split once again.

Lloyd George remained a Liberal all his life, but he abandoned many standard Liberal principles in his crusade to win the war at all costs. He insisted on strong government controls over business as opposed to the "laissez-faire" attitudes of traditional Liberals. He insisted on conscription of young men into the Army, a position that deeply troubled his old colleagues. That brought him and a few like-minded Liberals into the new coalition on the ground long occupied by Conservatives. There was no more planning for world peace or liberal treatment of Germany, nor discomfit with aggressive and authoritarian measures of state power. More deadly to the future of the party, says historian Trevor Wilson, was its repudiation by ideological Liberals, who decided sadly that it no longer represented their principles. Finally the presence of the vigorous new Labour Party on the left gave a new home to voters disenchanted with the Liberal performance.
In the 1918 general election, Lloyd George, hailed as "the Man Who Won the War", led his coalition into a khaki election. Lloyd George and the Conservative leader Bonar Law wrote a joint letter of support to candidates to indicate they were considered the official Coalition candidates—this "coupon", as it became known, was issued against many sitting Liberal MPs, often to devastating effect, though not against Asquith himself. The coalition won a massive victory as the Asquithian Liberals and Labour were decimated. Those remaining Liberal MPs who were opposed to the Coalition Government went into opposition under the parliamentary leadership of Sir Donald MacLean who also became Leader of the Opposition. Asquith, who had appointed MacLean, remained as overall Leader of the Liberal Party even though he lost his seat in 1918. Asquith returned to parliament in 1920 and resumed leadership. Between 1919–1923, the anti-Lloyd George Liberals were called Asquithian Liberals, Wee Free Liberals or Independent Liberals.

Lloyd George was increasingly under the influence of the rejuvenated Conservative party who numerically dominated the coalition. In 1922, the Conservative backbenchers rebelled against the continuation of the coalition, citing, in particular, Lloyd George's plan for war with Turkey in the Chanak Crisis, and his corrupt sale of honours. He resigned as prime minister and was succeeded by Bonar Law.

At the 1922 and 1923 elections the Liberals won barely a third of the vote and only a quarter of the seats in the House of Commons as many radical voters abandoned the divided Liberals and went over to Labour. In 1922, Labour became the official opposition. A reunion of the two warring factions took place in 1923 when the new Conservative prime minister Stanley Baldwin committed his party to protective tariffs, causing the Liberals to reunite in support of free trade. The party gained ground in the 1923 general election but made most of its gains from Conservatives whilst losing ground to Labour—a sign of the party's direction for many years to come. The party remained the third largest in the House of Commons, but the Conservatives had lost their majority. There was much speculation and fear about the prospect of a Labour government and comparatively little about a Liberal government, even though it could have plausibly presented an experienced team of ministers compared to Labour's almost complete lack of experience as well as offering a middle ground that could obtain support from both Conservatives and Labour in crucial Commons divisions. However, instead of trying to force the opportunity to form a Liberal government, Asquith decided instead to allow Labour the chance of office in the belief that they would prove incompetent and this would set the stage for a revival of Liberal fortunes at Labour's expense, but it was a fatal error.
Labour was determined to destroy the Liberals and become the sole party of the left. Ramsay MacDonald was forced into a snap election in 1924 and although his government was defeated he achieved his objective of virtually wiping the Liberals out as many more radical voters now moved to Labour whilst moderate middle-class Liberal voters concerned about socialism moved to the Conservatives. The Liberals were reduced to a mere forty seats in Parliament, only seven of which had been won against candidates from both parties and none of these formed a coherent area of Liberal survival. The party seemed finished, and during this period some Liberals, such as Churchill, went over to the Conservatives while others went over to Labour. Several Labour ministers of later generations, such as Michael Foot and Tony Benn, were the sons of Liberal MPs.

Asquith died in 1928 and the enigmatic figure of Lloyd George returned to the leadership and began a drive to produce coherent policies on many key issues of the day. In the 1929 general election, he made a final bid to return the Liberals to the political mainstream, with an ambitious programme of state stimulation of the economy called "We Can Conquer Unemployment!", largely written for him by the Liberal economist John Maynard Keynes. The Liberal Party stood in Northern Ireland for the first and only time in the 1929 general election gaining 17% of the vote but won no seats. The Liberals gained ground, but once again it was at the Conservatives' expense whilst also losing seats to Labour. Indeed, the urban areas of the country suffering heavily from unemployment, which might have been expected to respond the most to the radical economic policies of the Liberals, instead gave the party its worst results. By contrast, most of the party's seats were won either due to the absence of a candidate from one of the other parties or in rural areas on the Celtic fringe, where local evidence suggests that economic ideas were at best peripheral to the electorate's concerns. The Liberals now found themselves with 59 members, holding the balance of power in a Parliament where Labour was the largest party but lacked an overall majority. Lloyd George offered a degree of support to the Labour government in the hope of winning concessions, including a degree of electoral reform to introduce the alternative vote, but this support was to prove bitterly divisive as the Liberals increasingly divided between those seeking to gain what Liberal goals they could achieve, those who preferred a Conservative government to a Labour one and vice versa.

The last majority Liberal Government in Britain was elected in 1906. The years preceding the First World War were marked by worker strikes and civil unrest and saw many violent confrontations between civilians and the police and armed forces. Other issues of the period included women's suffrage and the Irish Home Rule movement. After the carnage of 1914–1918, the democratic reforms of the Representation of the People Act 1918 instantly tripled the number of people entitled to vote in Britain from seven to twenty-one million. The Labour Party benefited most from this huge change in the electorate, forming its first minority government in 1924.

In 1931 MacDonald's government fell apart in response to the Great Depression, and the Liberals agreed to join his National Government, dominated by the Conservatives. Lloyd George himself was ill and did not actually join. Soon, however, the Liberals faced another divisive crisis when a National Government was proposed to fight the 1931 general election with a mandate for tariffs. From the outside, Lloyd George called for the party to abandon the government completely in defence of free trade, but only a few MPs and candidates followed. Another group under Sir John Simon then emerged, who were prepared to continue their support for the government and take the Liberal places in the Cabinet if there were resignations. The third group under Sir Herbert Samuel pressed for the parties in government to fight the election on separate platforms. In doing so the bulk of Liberals remained supporting the government, but two distinct Liberal groups had emerged within this bulk – the Liberal Nationals (officially the "National Liberals" after 1947) led by Simon, also known as "Simonites", and the "Samuelites" or "official Liberals", led by Samuel who remained as the official party. Both groups secured about 34 MPs but proceeded to diverge even further after the election, with the Liberal Nationals remaining supporters of the government throughout its life. There were to be a succession of discussions about them rejoining the Liberals, but these usually foundered on the issues of free trade and continued support for the National Government. The one significant reunification came in 1946 when the Liberal and Liberal National party organisations in London merged.

The official Liberals found themselves a tiny minority within a government committed to protectionism. Slowly they found this issue to be one they could not support. In early 1932 it was agreed to suspend the principle of collective responsibility to allow the Liberals to oppose the introduction of tariffs. Later in 1932 the Liberals resigned their ministerial posts over the introduction of the Ottawa Agreement on Imperial Preference. However, they remained sitting on the government benches supporting it in Parliament, though in the country local Liberal activists bitterly opposed the government. Finally in late 1933 the Liberals crossed the floor of the House of Commons and went into complete opposition. By this point their number of MPs was severely depleted. In the 1935 general election, just 17 Liberal MPs were elected, along with Lloyd George and three followers as independent Liberals. Immediately after the election the two groups reunited, though Lloyd George declined to play much of a formal role in his old party. Over the next ten years there would be further defections as MPs deserted to either the Liberal Nationals or Labour. Yet there were a few recruits, such as Clement Davies, who had deserted to the National Liberals in 1931 but now returned to the party during World War II and who would lead it after the war.

Samuel had lost his seat in the 1935 election and the leadership of the party fell to Sir Archibald Sinclair. With many traditional domestic Liberal policies now regarded as irrelevant, he focused the party on opposition to both the rise of Fascism in Europe and the appeasement foreign policy of the British government, arguing that intervention was needed, in contrast to the Labour calls for pacifism. Despite the party's weaknesses, Sinclair gained a high profile as he sought to recall the Midlothian Campaign and once more revitalise the Liberals as the party of a strong foreign policy.

In 1940, they joined Churchill's wartime coalition government, with Sinclair serving as Secretary of State for Air, the last British Liberal to hold Cabinet rank office for seventy years. However, it was a sign of the party's lack of importance that they were not included in the War Cabinet; some leading party members founded Radical Action, a group which called for liberal candidates to break the war-time electoral pact. At the 1945 general election, Sinclair and many of his colleagues lost their seats to both Conservatives and Labour and the party returned just 12 MPs to Westminster, but this was just the beginning of the decline. In 1950, the general election saw the Liberals return just nine MPs. Another general election was called in 1951 and the Liberals were left with just six MPs and all but one of them were aided by the fact that the Conservatives refrained from fielding candidates in those constituencies.

In 1957, this total fell to five when one of the Liberal MPs died and the subsequent by-election was lost to the Labour Party, which selected the former Liberal Deputy Leader Megan Lloyd George as its own candidate. The Liberal Party seemed close to extinction. During this low period, it was often joked that Liberal MPs could hold meetings in the back of one taxi.

Through the 1950s and into the 1960s the Liberals survived only because a handful of constituencies in rural Scotland and Wales clung to their Liberal traditions, whilst in two English towns, Bolton and Huddersfield, local Liberals and Conservatives agreed to each contest only one of the town's two seats. Jo Grimond, for example, who became Liberal leader in 1956, was MP for the remote Orkney and Shetland islands. Under his leadership a Liberal revival began, marked by the Orpington by-election of March 1962 which was won by Eric Lubbock. There, the Liberals won a seat in the London suburbs for the first time since 1935.

The Liberals became the first of the major British political parties to advocate British membership of the European Economic Community. Grimond also sought an intellectual revival of the party, seeking to position it as a non-socialist radical alternative to the Conservative government of the day. In particular he canvassed the support of the young post-war university students and recent graduates, appealing to younger voters in a way that many of his recent predecessors had not, and asserting a new strand of Liberalism for the post-war world.

The new middle-class suburban generation began to find the Liberals' policies attractive again. Under Grimond (who retired in 1967) and his successor, Jeremy Thorpe, the Liberals regained the status of a serious third force in British politics, polling up to 20% of the vote, but unable to break the duopoly of Labour and Conservative and win more than fourteen seats in the Commons. An additional problem was competition in the Liberal heartlands in Scotland and Wales from the Scottish National Party and Plaid Cymru who both grew as electoral forces from the 1960s onwards. Although Emlyn Hooson held on to the seat of Montgomeryshire, upon Clement Davies death in 1962, the party lost five Welsh seats between 1950 and 1966. In September 1966, the Welsh Liberal Party formed their own state party, moving the Liberal Party into a fully federal structure.

In local elections, Liverpool remained a Liberal stronghold, with the party taking the plurality of seats on the elections to the new Liverpool Metropolitan Borough Council in 1973. In the February 1974 general election, the Conservative government of Edward Heath won a plurality of votes cast, but the Labour Party gained a plurality of seats. The Conservatives were unable to form a government due to the Ulster Unionist MPs refusing to support the Conservatives after the Northern Ireland Sunningdale Agreement. The Liberals now held the balance of power in the Commons. Conservatives offered Thorpe the Home Office if he would join a coalition government with Heath. Thorpe was personally in favour of it, but the party insisted on a clear government commitment to introducing proportional representation and a change of prime minister. The former was unacceptable to Heath's cabinet and the latter to Heath personally, so the talks collapsed. Instead, a minority Labour government was formed under Harold Wilson but with no formal support from Thorpe. In the October 1974 general election, the Liberals slipped back slightly and the Labour government won a wafer-thin majority.

Thorpe was subsequently forced to resign after allegations that he attempted to have his homosexual lover murdered by a hitman. The party's new leader, David Steel, negotiated the Lib-Lab pact with Wilson's successor as prime minister, James Callaghan. According to this pact, the Liberals would support the government in crucial votes in exchange for some influence over policy. The agreement lasted from 1977 to 1978, but proved mostly fruitless, for two reasons: the Liberals' key demand of proportional representation was rejected by most Labour MPs, whilst the contacts between Liberal spokespersons and Labour ministers often proved detrimental, such as between finance spokesperson John Pardoe and Chancellor of the Exchequer Denis Healey, who were mutually antagonistic.

The Conservative Party under the leadership of Margaret Thatcher won the 1979 general election, placing the Labour Party back in opposition, which served to push the Liberals back into the margins.

In 1981, defectors from a moderate faction of the Labour Party, led by former Cabinet ministers Roy Jenkins, David Owen and Shirley Williams, founded the Social Democratic Party (SDP). The new party and the Liberals quickly formed the SDP–Liberal Alliance, which for a while polled as high as 50% in the opinion polls and appeared capable of winning the next general election. Indeed, Steel was so confident of an Alliance victory that he told the 1981 Liberal conference, "Go back to your constituencies, and prepare for government!".

However, the Alliance was overtaken in the polls by the Tories in the aftermath of the Falkland Islands War and at the 1983 general election the Conservatives were re-elected by a landslide, with Labour once again forming the opposition. While the SDP–Liberal Alliance came close to Labour in terms of votes (a share of more than 25%), it only had 23 MPs compared to Labour's 209. The Alliance's support was spread out across the country, and was not concentrated in enough areas to translate into seats.

In the 1987 general election, the Alliance's share of the votes fell slightly and it now had 22 MPs. In the election's aftermath Steel proposed a merger of the two parties. Most SDP members voted in favour of the merger, but SDP leader David Owen objected and continued to lead a "rump" SDP.

In March 1988, the Liberal Party and Social Democratic Party merged to create the Social and Liberal Democrats, renamed the Liberal Democrats in October 1989. Over two-thirds of Liberal members joined the merged party, along with all sitting MPs. Steel and SDP leader Robert Maclennan served briefly as interim leaders of the merged party.

A group of Liberal opponents of the merger with the Social Democrats, including Michael Meadowcroft (the former Liberal MP for Leeds West) and Paul Wiggin (who served on Peterborough City Council as a Liberal), continued with a new party organisation under the name of the 'Liberal Party'. Meadowcroft joined the Liberal Democrats in 2007, but the Liberal Party as reconstituted in 1989 continues to hold council seats and field candidates in Westminster Parliamentary elections.

During the 19th century, the Liberal Party was broadly in favour of what would today be called classical liberalism, supporting "laissez-faire" economic policies such as free trade and minimal government interference in the economy (this doctrine was usually termed Gladstonian liberalism after the Victorian era Liberal prime minister William Ewart Gladstone). The Liberal Party favoured social reform, personal liberty, reducing the powers of the Crown and the Church of England (many of them were nonconformists) and an extension of the electoral franchise. Sir William Harcourt, a prominent Liberal politician in the Victorian era, said this about liberalism in 1872: If there be any party which is more pledged than another to resist a policy of restrictive legislation, having for its object social coercion, that party is the Liberal party. (Cheers.) But liberty does not consist in making others do what you think right, (Hear, hear.) The difference between a free Government and a Government which is not free is principally this—that a Government which is not free interferes with everything it can, and a free Government interferes with nothing except what it must. A despotic Government tries to make everybody do what it wishes; a Liberal Government tries, as far as the safety of society will permit, to allow everybody to do as he wishes. It has been the tradition of the Liberal party consistently to maintain the doctrine of individual liberty. It is because they have done so that England is the place where people can do more what they please than in any other country in the world. [...] It is this practice of allowing one set of people to dictate to another set of people what they shall do, what they shall think, what they shall drink, when they shall go to bed, what they shall buy, and where they shall buy it, what wages they shall get and how they shall spend them, against which the Liberal party have always protested.

The political terms of "modern", "progressive" or "new" Liberalism began to appear in the mid to late 1880s and became increasingly common to denote the tendency in the Liberal Party to favour an increased role for the state as more important than the classical liberal stress on self-help and freedom of choice.

By the early 20th century, the Liberals stance began to shift towards "New Liberalism", what would today be called social liberalism, namely a belief in personal liberty with a support for government intervention to provide minimum levels of welfare. This shift was best exemplified by the Liberal government of H. H. Asquith and his Chancellor David Lloyd George, whose Liberal reforms in the early 1900s created a basic welfare state.

David Lloyd George adopted a programme at the 1929 general election entitled "We Can Conquer Unemployment!", although by this stage the Liberals had declined to third-party status. The Liberals as expressed in the "Liberal Yellow Book" now regarded opposition to state intervention as being a characteristic of right-wing extremists.

After nearly becoming extinct in the 1940s and the 1950s, the Liberal Party revived its fortunes somewhat under the leadership of Jo Grimond in the 1960s by positioning itself as a radical centrist, non-socialist alternative to the Conservative and Labour Party governments of the time.

Since 1660, nonconformist Protestants have played a major role in English politics. Relatively few MPs were Dissenters. However the Dissenters were a major voting bloc in many areas, such as the East Midlands. They were very well organised and highly motivated and largely won over the Whigs and Liberals to their cause. Down to the 1830s, Dissenters demanded removal of political and civil disabilities that applied to them (especially those in the Test and Corporation Acts). The Anglican establishment strongly resisted until 1828. Numerous reforms of voting rights, especially that of 1832, increased the political power of Dissenters. They demanded an end to compulsory church rates, in which local taxes went only to Anglican churches. They finally achieved the end of religious tests for university degrees in 1905. Gladstone brought the majority of Dissenters around to support for Home Rule for Ireland, putting the dissenting Protestants in league with the Irish Roman Catholics in an otherwise unlikely alliance. The Dissenters gave significant support to moralistic issues, such as temperance and sabbath enforcement. The nonconformist conscience, as it was called, was repeatedly called upon by Gladstone for support for his moralistic foreign policy. In election after election, Protestant ministers rallied their congregations to the Liberal ticket. In Scotland, the Presbyterians played a similar role to the Nonconformist Methodists, Baptists and other groups in England and Wales.

By the 1820s, the different Nonconformists, including Wesleyan Methodists, Baptists, Congregationalists and Unitarians, had formed the Committee of Dissenting Deputies and agitated for repeal of the highly restrictive Test and Corporation Acts. These Acts excluded Nonconformists from holding civil or military office or attending Oxford or Cambridge, compelling them to set up their own Dissenting Academies privately. The Tories tended to be in favour of these Acts and so the Nonconformist cause was linked closely to the Whigs, who advocated civil and religious liberty. After the Test and Corporation Acts were repealed in 1828, all the Nonconformists elected to Parliament were Liberals. Nonconformists were angered by the Education Act 1902, which integrated Church of England denominational schools into the state system and provided for their support from taxes. John Clifford formed the National Passive Resistance Committee and by 1906 over 170 Nonconformists had gone to prison for refusing to pay school taxes. They included 60 Primitive Methodists, 48 Baptists, 40 Congregationalists and 15 Wesleyan Methodists.

The political strength of Dissent faded sharply after 1920 with the secularisation of British society in the 20th century. The rise of the Labour Party reduced the Liberal Party strongholds into the nonconformist and remote "Celtic Fringe", where the party survived by an emphasis on localism and historic religious identity, thereby neutralising much of the class pressure on behalf of the Labour movement. Meanwhile, the Anglican church was a bastion of strength for the Conservative party. On the Irish issue, the Anglicans strongly supported unionism. Increasingly after 1850, the Roman Catholic element in England and Scotland was composed of recent immigrants from Ireland. They voted largely for the Irish Parliamentary Party until its collapse in 1918.











</doc>
<doc id="4484" url="https://en.wikipedia.org/wiki?curid=4484" title="Bank of England">
Bank of England

The Bank of England is the central bank of the United Kingdom and the model on which most modern central banks have been based. Established in 1694 to act as the English Government's banker, and still one of the bankers for the Government of the United Kingdom, it is the world's eighth-oldest bank. It was privately owned by stockholders from its foundation in 1694 until it was nationalised in 1946.

The Bank became an independent public organisation in 1998, wholly owned by the Treasury Solicitor on behalf of the government, but with independence in setting monetary policy.

The Bank is one of eight banks authorised to issue banknotes in the United Kingdom, has a monopoly on the issue of banknotes in England and Wales and regulates the issue of banknotes by commercial banks in Scotland and Northern Ireland.

The Bank's Monetary Policy Committee has a devolved responsibility for managing monetary policy. The Treasury has reserve powers to give orders to the committee "if they are required in the public interest and by extreme economic circumstances", but such orders must be endorsed by Parliament within 28 days. The Bank's Financial Policy Committee held its first meeting in June 2011 as a macroprudential regulator to oversee regulation of the UK's financial sector.

The Bank's headquarters have been in London's main financial district, the City of London, on Threadneedle Street, since 1734. It is sometimes known as "The Old Lady of Threadneedle Street", a name taken from a satirical cartoon by James Gillray in 1797. The road junction outside is known as Bank junction.

As a regulator and central bank, the Bank of England has not offered consumer banking services for many years, but it still does manage some public-facing services such as exchanging superseded bank notes. Until 2016, the bank provided personal banking services as a privilege for employees.

England's crushing defeat by France, the dominant naval power, in naval engagements culminating in the 1690 Battle of Beachy Head, became the catalyst for England to rebuild itself as a global power. William III's government wanted to build a naval fleet that would rival that of France; however, the ability to construct this fleet was hampered both by a lack of available public funds and the low credit of the English government in London. This lack of credit made it impossible for the English government to borrow the £1,200,000 (at 8% per annum) that it wanted for the construction of the fleet.

To induce subscription to the loan, the subscribers were to be incorporated by the name of the Governor and Company of the Bank of England. The Bank was given exclusive possession of the government's balances, and was the only limited-liability corporation allowed to issue bank notes. The lenders would give the government cash (bullion) and issue notes against the government bonds, which can be lent again. The £1.2 million was raised in 12 days; half of this was used to rebuild the navy.

As a side effect, the huge industrial effort needed, including establishing ironworks to make more nails and advances in agriculture feeding the quadrupled strength of the navy, started to transform the economy. This helped the new Kingdom of Great Britain – England and Scotland were formally united in 1707 – to become powerful. The power of the navy made Britain the dominant world power in the late 18th and early 19th centuries.

The establishment of the bank was devised by Charles Montagu, 1st Earl of Halifax, in 1694. The plan of 1691, which had been proposed by William Paterson three years before, had not then been acted upon. 58 years earlier, in 1636, Financier to the king, Philip Burlamachi, had proposed exactly the same idea in a letter addressed to Sir Francis Windebank. He proposed a loan of £1.2 million to the government; in return the subscribers would be incorporated as The Governor and Company of the Bank of England with long-term banking privileges including the issue of notes. The royal charter was granted on 27 July through the passage of the Tonnage Act 1694. Public finances were in such dire condition at the time that the terms of the loan were that it was to be serviced at a rate of 8% per annum, and there was also a service charge of £4,000 per annum for the management of the loan. The first governor was Sir John Houblon, who is depicted in the £50 note issued in 1994. The charter was renewed in 1742, 1764, and 1781.

The Bank's original home was in Walbrook, a street in the City of London, where during reconstruction in 1954 archaeologists found the remains of a Roman temple of Mithras (Mithras is – rather fittingly – said to have been worshipped as, amongst other things, the God of Contracts); the Mithraeum ruins are perhaps the most famous of all 20th-century Roman discoveries in the City of London and can be viewed by the public.

The Bank moved to its current location in Threadneedle Street in 1734, and thereafter slowly acquired neighbouring land to create the site necessary for erecting the Bank's original home at this location, under the direction of its chief architect Sir John Soane, between 1790 and 1827. (Sir Herbert Baker's rebuilding of the Bank in the first half of the 20th century, demolishing most of Soane's masterpiece, was described by architectural historian Nikolaus Pevsner as "the greatest architectural crime, in the City of London, of the twentieth century".)

When the idea and reality of the national debt came about during the 18th century, this was also managed by the Bank. During the American war of independence, business for the Bank was so good that George Washington remained a shareholder throughout the period. By the charter renewal in 1781 it was also the bankers' bank – keeping enough gold to pay its notes on demand until 26 February 1797, when war had so diminished gold reserves that – following an invasion scare caused by the Battle of Fishguard days earlier – the government prohibited the Bank from paying out in gold by the passing of the Bank Restriction Act 1797. This prohibition lasted until 1821.

The 1844 Bank Charter Act tied the issue of notes to the gold reserves and gave the Bank sole rights with regard to the issue of banknotes. Private banks that had previously had that right retained it, provided that their headquarters were outside London and that they deposited security against the notes that they issued. A few English banks continued to issue their own notes until the last of them was taken over in the 1930s. Scottish and Northern Irish private banks still have that right.

The bank acted as lender of last resort for the first time in the panic of 1866.

The last private bank in England to issue its own notes was Thomas Fox's Fox, Fowler and Company bank in Wellington, which rapidly expanded, until it merged with Lloyds Bank in 1927. They were legal tender until 1964. There are nine notes left in circulation; one is housed at Tone Dale House, Wellington.

Britain was on the gold standard until 1931, when the Bank of England unilaterally and abruptly took Britain off the gold standard.

During the governorship of Montagu Norman, from 1920 to 1944, the Bank made deliberate efforts to move away from commercial banking and become a central bank. In 1946, shortly after the end of Norman's tenure, the bank was nationalised by the Labour government.

The Bank pursued the multiple goals of Keynesian economics after 1945, especially "easy money" and low interest rates to support aggregate demand. It tried to keep a fixed exchange rate, and attempted to deal with inflation and sterling weakness by credit and exchange controls.

In 1977, the Bank set up a wholly owned subsidiary called Bank of England Nominees Limited (BOEN), a private limited company, with two of its hundred £1 shares issued. According to its Memorandum & Articles of Association, its objectives are: "To act as Nominee or agent or attorney either solely or jointly with others, for any person or persons, partnership, company, corporation, government, state, organisation, sovereign, province, authority, or public body, or any group or association of them..." Bank of England Nominees Limited was granted an exemption by Edmund Dell, Secretary of State for Trade, from the disclosure requirements under Section 27(9) of the Companies Act 1976, because "it was considered undesirable that the disclosure requirements should apply to certain categories of shareholders." The Bank of England is also protected by its royal charter status, and the Official Secrets Act. BOEN is a vehicle for governments and heads of state to invest in UK companies (subject to approval from the Secretary of State), providing they undertake "not to influence the affairs of the company". BOEN is no longer exempt from company law disclosure requirements. Although a dormant company, dormancy does not preclude a company actively operating as a nominee shareholder. BOEN has two shareholders: the Bank of England, and the Secretary of the Bank of England.

The reserve requirement for banks to hold a minimum fixed proportion of their deposits as reserves at the Bank of England was abolished in 1981: see reserve requirement for more details. The contemporary transition from Keynesian economics to Chicago economics was analysed by Nicholas Kaldor in "The Scourge of Monetarism".

On 6 May 1997, following the 1997 general election that brought a Labour government to power for the first time since 1979, it was announced by the Chancellor of the Exchequer, Gordon Brown, that the Bank would be granted operational independence over monetary policy. Under the terms of the Bank of England Act 1998 (which came into force on 1 June 1998), the Bank's Monetary Policy Committee was given sole responsibility for setting interest rates to meet the Government's Retail Prices Index (RPI) inflation target of 2.5%. The target has changed to 2% since the Consumer Price Index (CPI) replaced the Retail Prices Index as the Treasury's inflation index. If inflation overshoots or undershoots the target by more than 1%, the Governor has to write a letter to the Chancellor of the Exchequer explaining why, and how he will remedy the situation.

The success of inflation targeting in the United Kingdom has been attributed to the Bank's focus on transparency. The Bank of England has been a leader in producing innovative ways of communicating information to the public, especially through its Inflation Report, which have been emulated by many other central banks.

Independent central banks that adopt an inflation target are known as Friedmanite central banks. This change in Labour's politics was described by Skidelsky in "The Return of the Master" as a mistake and as an adoption of the Rational Expectations Hypothesis as promulgated by Walters. Inflation targets combined with central bank independence have been characterised as a "starve the beast" strategy creating a lack of money in the public sector.

The handing over of monetary policy to the Bank had been a key plank of the Liberal Democrats' economic policy since the 1992 general election. Conservative MP Nicholas Budgen had also proposed this as a private member's bill in 1996, but the bill failed as it had the support of neither the government nor the opposition.

Mark Carney assumed the post of Governor of the Bank of England on 1 July 2013. He succeeded Mervyn King, who took over on 30 June 2003. Carney, a Canadian, was to serve an initial five-year term rather than the typical eight. He became the first Governor not to be a UK citizen, but has since been granted citizenship. At Government request, his term was extended to 2019, then again to 2020. As of January 2014, the Bank also has four Deputy Governors.

BOEN was dissolved, following liquidation, in July 2017.

Two main areas are tackled by the Bank to ensure it carries out these functions efficiently:

Note: It is important to note that "monetary" and "financial" are synonyms.

Stable prices and confidence in the currency are the two main criteria for monetary stability. Stable prices are maintained by seeking to ensure that price increases meet the Government's inflation target. The Bank aims to meet this target by adjusting the base interest rate, which is decided by the Monetary Policy Committee, and through its communications strategy, such as publishing yield curves.

The Bank works together with other institutions to secure both monetary and financial stability, including:
The 1997 memorandum of understanding describes the terms under which the Bank, the Treasury and the FSA work toward the common aim of increased financial stability. In 2010, the incoming Chancellor announced his intention to merge the FSA back into the Bank. As of 2012, the current director for financial stability is Andy Haldane.

The Bank acts as the government's banker, and it maintains the government's Consolidated Fund account. It also manages the country's foreign exchange and gold reserves. The Bank also acts as the bankers' bank, especially in its capacity as a lender of last resort.

The Bank has a monopoly on the issue of banknotes in England and Wales. Scottish and Northern Irish banks retain the right to issue their own banknotes, but they must be backed one-for-one with deposits at the Bank, excepting a few million pounds representing the value of notes they had in circulation in 1845. The Bank decided to sell its banknote-printing operations to De La Rue in December 2002, under the advice of Close Brothers Corporate Finance Ltd.

Since 1998, the Monetary Policy Committee (MPC) has had the responsibility for setting the official interest rate. However, with the decision to grant the Bank operational independence, responsibility for government debt management was transferred in 1998 to the new Debt Management Office, which also took over government cash management in 2000. Computershare took over as the registrar for UK Government bonds (gilt-edged securities or "gilts") from the Bank at the end of 2004.

The Bank used to be responsible for the regulation and supervision of the banking and insurance industries. This responsibility was transferred to the Financial Services Authority in June 1998, but after the financial crises in 2008 new banking legislation transferred the responsibility for regulation and supervision of the banking and insurance industries back to the Bank.

In 2011, the interim Financial Policy Committee (FPC) was created as a mirror committee to the MPC to spearhead the Bank's new mandate on financial stability. The FPC is responsible for macro prudential regulation of all UK banks and insurance companies.

To help maintain economic stability, the Bank attempts to broaden understanding of its role, both through regular speeches and publications by senior Bank figures, a semiannual Financial Stability Report, and through a wider education strategy aimed at the general public. It currently maintains a free museum and ran the Target Two Point Zero competition for A-level students, closing in 2017.

The Bank has operated, since January 2009, an Asset Purchase Facility (APF) to buy "high-quality assets financed by the issue of Treasury bills and the DMO's cash management operations" and thereby improve liquidity in the credit markets. It has, since March 2009, also provided the mechanism by which the Bank's policy of quantitative easing (QE) is achieved, under the auspices of the MPC. Along with the managing the £200 billion of QE funds, the APF continues to operate its corporate facilities. Both are undertaken by a subsidiary company of the Bank of England, the Bank of England Asset Purchase Facility Fund Limited (BEAPFF).

The Bank has issued banknotes since 1694. Notes were originally hand-written; although they were partially printed from 1725 onwards, cashiers still had to sign each note and make them payable to someone. Notes were fully printed from 1855. Until 1928 all notes were "White Notes", printed in black and with a blank reverse. In the 18th and 19th centuries White Notes were issued in £1 and £2 denominations. During the 20th century White Notes were issued in denominations between £5 and £1000.

Until the mid-19th century, commercial banks were allowed to issue their own banknotes, and notes issued by provincial banking companies were commonly in circulation. The Bank Charter Act 1844 began the process of restricting note issue to the Bank; new banks were prohibited from issuing their own banknotes and existing note-issuing banks were not permitted to expand their issue. As provincial banking companies merged to form larger banks, they lost their right to issue notes, and the English private banknote eventually disappeared, leaving the Bank with a monopoly of note issue in England and Wales. The last private bank to issue its own banknotes in England and Wales was Fox, Fowler and Company in 1921. However, the limitations of the 1844 Act only affected banks in England and Wales, and today three commercial banks in Scotland and four in Northern Ireland continue to issue their own banknotes, regulated by the Bank.

At the start of the First World War, the Currency and Bank Notes Act 1914 was passed, which granted temporary powers to HM Treasury for issuing banknotes to the values of £1 and 10/- (ten shillings). Treasury notes had full legal tender status and were not convertible into gold through the Bank; they replaced the gold coin in circulation to prevent a run on sterling and to enable raw material purchases for armament production. These notes featured an image of King George V (Bank of England notes did not begin to display an image of the monarch until 1960). The wording on each note was:

Treasury notes were issued until 1928, when the Currency and Bank Notes Act 1928 returned note-issuing powers to the banks. The Bank of England issued notes for ten shillings and one pound for the first time on 22 November 1928.

During the Second World War the German Operation Bernhard attempted to counterfeit denominations between £5 and £50, producing 500,000 notes each month in 1943. The original plan was to parachute the money into the UK in an attempt to destabilise the British economy, but it was found more useful to use the notes to pay German agents operating throughout Europe. Although most fell into Allied hands at the end of the war, forgeries frequently appeared for years afterwards, which led banknote denominations above £5 to be removed from circulation.

In 2006, over £53 million in banknotes belonging to the Bank was stolen from a depot in Tonbridge, Kent.

Modern banknotes are printed by contract with De La Rue Currency in Loughton, Essex.

The bank is custodian to the official gold reserves of the United Kingdom and around 30 other countries. , the bank held around 400,000 bars, which is equivalent to of gold. These gold deposits were estimated in August 2018 to have a current market value of approximately £200 billion. These estimates suggest the vault could hold as much as 3% of the gold mined throughout human history.

Following is a list of the Governors of the Bank of England since the beginning of the 20th century:

The Court of Directors is a unitary board that is responsible for setting the organisation's strategy and budget and taking key decisions on resourcing and appointments. It consists of five executive members from the Bank plus up to 9 non-executive members, all of whom are appointed by the Crown. The Chancellor selects the Chairman of the Court from among the non-executive members. The Court is required to meet at least 7 times a year.

The Governor serves for a period of eight years, the Deputy Governors for five years, and the non-executive members for up to four years.

Since 2013, the Bank has had a chief operating officer (COO). , the Bank's COO has been Joanna Place.

, the Bank's chief economist is Andrew Haldane.





</doc>
<doc id="4485" url="https://en.wikipedia.org/wiki?curid=4485" title="Bakelite">
Bakelite

Bakelite ( ; sometimes spelled Baekelite) or polyoxybenzylmethylenglycolanhydride was the first plastic made from synthetic components. It is a thermosetting phenol formaldehyde resin, formed from a condensation reaction of phenol with formaldehyde. It was developed by the Belgian-American chemist Leo Baekeland in Yonkers, New York, in 1907.

Bakelite was patented on December 7, 1909. The creation of a synthetic plastic was revolutionary for its electrical nonconductivity and heat-resistant properties in electrical insulators, radio and telephone casings and such diverse products as kitchenware, jewelry, pipe stems, children's toys, and firearms. 

In recent years the "retro" appeal of old Bakelite products has made them collectible.

Bakelite was designated a National Historic Chemical Landmark on November 9, 1993, by the American Chemical Society in recognition of its significance as the world's first synthetic plastic.

Baekeland was already wealthy due to his invention of Velox photographic paper when he began to investigate the reactions of phenol and formaldehyde in his home laboratory. Chemists had begun to recognize that many natural resins and fibers were polymers. Baekeland's initial intent was to find a replacement for shellac, a material in limited supply because it was made naturally from the excretion of lac insects (specifically "Kerria lacca"). Baekeland produced a soluble phenol-formaldehyde shellac called "Novolak", but it was not a market success.

Baekeland then began experimenting on strengthening wood by impregnating it with a synthetic resin, rather than coating it. By controlling the pressure and temperature applied to phenol and formaldehyde, Baekeland produced a hard moldable material that he named "Bakelite," after himself. It was the first synthetic thermosetting plastic produced, and Baekeland speculated on "the thousand and one ... articles" it could be used to make. Baekeland considered the possibilities of using a wide variety of filling materials, including cotton, powdered bronze, and slate dust, but was most successful with wood and asbestos fibers.

Baekeland filed a substantial number of patents in the area. Bakelite, his "method of making insoluble products of phenol and formaldehyde," was filed on July 13, 1907, and granted on December 7, 1909. Baekeland also filed for patent protection in other countries, including Belgium, Canada, Denmark, Hungary, Japan, Mexico, Russia, and Spain. He announced his invention at a meeting of the American Chemical Society on February 5, 1909.

Baekeland started semi-commercial production of his new material in his home laboratory, marketing it as a material for electrical insulators. By 1910, he was producing enough material to justify expansion. He formed the General Bakelite Company as a U.S. company to manufacture and market his new industrial material. He also made overseas connections to produce materials in other countries.

Bijker gives a detailed discussion of the development of Bakelite and the Bakelite company's production of various applications of materials. As of 1911, the company's main focus was laminating varnish, whose sales volume vastly outperformed both molding material and cast resin. By 1912, molding material was gaining ground, but its sales volume for the company did not exceed that of laminating varnish until the 1930s.

As the sales figures also show, the Bakelite Company produced "transparent" cast resin (which did not include filler) for a small ongoing market during the 1910s and 1920s. Blocks or rods of cast resin, also known as "artificial amber", were machined and carved to create items such as pipe stems, cigarette holders and jewelry. However, the demand for molded plastics led the Bakelite company to concentrate on molding, rather than concentrating on cast solid resins.

The Bakelite Corporation was formed in 1922 after patent litigation favorable to Baekeland, from a merger of three companies: Baekeland's General Bakelite Company; the Condensite Company, founded by J.W. Aylesworth; and the Redmanol Chemical Products Company, founded by Lawrence V. Redman. Under director of advertising and public relations Allan Brown, who came to Bakelite from Condensite, Bakelite was aggressively marketed as "the material of a thousand uses". A filing for a trademark featuring the letter B above the mathematical symbol for infinity was made August 25, 1925, and claimed the mark was in use as of December 1, 1924. A wide variety of uses were listed in their trademark applications.

The first issue of "Plastics" magazine, October 1925, featured Bakelite on its cover, and included the article "Bakelite – What It Is" by Allan Brown. The range of colors available included "black, brown, red, yellow, green, gray, blue, and blends of two or more of these". The article emphasized that Bakelite came in various forms. "Bakelite is manufactured in several forms to suit varying requirements. In all these forms the fundamental basis is the initial Bakelite resin. This variety includes clear material, for jewelry, smokers' articles, etc.; cement, using in sealing electric light bulbs in metal bases; varnishes, for impregnating electric coils, etc.; lacquers, for protecting the surface of hardware; enamels, for giving resistive coating to industrial equipment; Laminated Bakelite, used for silent gears and insulation; and molding material, from which are formed innumerable articles of utility and beauty. The molding material is prepared ordinarily by the impregnation of cellulose substances with the initial 'uncured' resin." In a 1925 report, the United States Tariff Commission hailed the commercial manufacture of synthetic phenolic resin as "distinctly an American achievement", and noted that "the publication of figures, however, would be a virtual disclosure of the production of an individual company".

In England, Bakelite Limited, a merger of three British phenol formaldehyde resin suppliers (Damard Lacquer Company Limited of Birmingham, Mouldensite Limited of Darley Dale and Redmanol Chemical Products Company of London), was formed in 1926. A new Bakelite factory opened in Tyseley, Birmingham, around 1928. It was demolished in 1998.

A new factory opened in Bound Brook, New Jersey, in 1931. 

In 1939, the companies were acquired by Union Carbide and Carbon Corporation. In 2005, Union Carbide's phenolic resin business, including the Bakelite and Bakelit registered trademarks, were assigned to Hexion Inc. On the 1st of April, 2019 Hexion filed for Chapter 11 bankruptcy. 

In addition to the original Bakelite material, these companies eventually made a wide range of other products, many of which were marketed under the brand name "Bakelite plastics". These included other types of cast phenolic resins similar to Catalin, and urea-formaldehyde resins, which could be made in brighter colors than polyoxy­benzyl­methylene­glycol­anhydride.

Once Baekeland's heat and pressure patents expired in 1927, Bakelite Corporation faced serious competition from other companies. Because molded Bakelite incorporated fillers to give it strength, it tended to be made in concealing dark colors. In 1927, beads, bangles and earrings were produced by the Catalin company, through a different process which enabled them to introduce 15 new colors. Translucent jewelry, poker chips and other items made of phenolic resins were introduced in the 1930s or 1940s by the Catalin company under the Prystal name. The creation of marbled phenolic resins may also be attributable to the Catalin company.

Making Bakelite was a multi-stage process. It began with the heating of phenol and formaldehyde in the presence of a catalyst such as hydrochloric acid, zinc chloride, or the base ammonia. This created a liquid condensation product, referred to as "Bakelite A", which was soluble in alcohol, acetone, or additional phenol. Heated further, the product became partially soluble and could still be softened by heat. Sustained heating resulted in an "insoluble hard gum". However, the high temperatures required to create this tended to cause violent foaming of the mixture, which resulted in the cooled material being porous and breakable. Baekeland's innovative step was to put his "last condensation product" into an egg-shaped "Bakelizer". By heating it under pressure, at about , Baekeland was able to suppress the foaming that would otherwise occur. The resulting substance was extremely hard and both infusible and insoluble. 

Molded Bakelite forms in a condensation reaction of phenol and formaldehyde, with wood flour or asbestos fiber as a filler, under high pressure and heat in a time frame of a few minutes of curing. The result is a hard plastic material.

Bakelite's molding process had a number of advantages. Bakelite resin could be provided either as powder, or as preformed partially cured slugs, increasing the speed of the casting. Thermosetting resins such as Bakelite required heat and pressure during the molding cycle, but could be removed from the molding process without being cooled, again making the molding process faster. Also, because of the smooth polished surface that resulted, Bakelite objects required less finishing. Millions of parts could be duplicated quickly and relatively cheaply.

Another market for Bakelite resin was the creation of phenolic sheet materials. Phenolic sheet is a hard, dense material made by applying heat and pressure to layers of paper or glass cloth impregnated with synthetic resin. Paper, cotton fabrics, synthetic fabrics, glass fabrics and unwoven fabrics are all possible materials used in lamination. When heat and pressure are applied, polymerization transforms the layers into thermosetting industrial laminated plastic.

Bakelite phenolic sheet is produced in many commercial grades and with various additives to meet diverse mechanical, electrical and thermal requirements. Some common types include:

Bakelite has a number of important properties. It can be molded very quickly, decreasing production time. Moldings are smooth, retain their shape and are resistant to heat, scratches, and destructive solvents. It is also resistant to electricity, and prized for its low conductivity. It is not flexible.

Phenolic resin products may swell slightly under conditions of extreme humidity or perpetual dampness. When rubbed or burnt, Bakelite has a distinctive, acrid, sickly-sweet or fishy odor.

The characteristics of Bakelite made it particularly suitable as a molding compound, an adhesive or binding agent, a varnish, and a protective coating. Bakelite was particularly suitable for the emerging electrical and automobile industries because of its extraordinarily high resistance to electricity, heat, and chemical action.

The earliest commercial use of Bakelite in the electrical industry was the molding of tiny insulating bushings, made in 1908 for the Weston Electrical Instrument Corporation by Richard W. Seabury of the Boonton Rubber Company. Bakelite was soon used for non-conducting parts of telephones, radios and other electrical devices, including bases and sockets for light bulbs and electron tubes (vacuum tubes), supports for any type of electrical components, automobile distributor caps and other insulators. By 1912, it was being used to make billiard balls, since its elasticity and the sound it made were similar to ivory.

During World War I, Bakelite was used widely, particularly in electrical systems. Important projects included the Liberty airplane engine, the wireless telephone and radio phone, and the use of micarta-bakelite propellors in the NBS-1 bomber and the DH-4B aeroplane.

Bakelite's availability and ease and speed of molding helped to lower the costs and increase product availability so that telephones and radios became common household consumer goods. It was also very important to the developing automobile industry. It was soon found in myriad other consumer products ranging from pipe stems and buttons to saxophone mouthpieces, cameras, early machine guns, and appliance casings. Bakelite was also very commonly used in making molded grip panels (stocks) on handguns, submachine guns and machineguns, as well as numerous knife handles and "scales" through the first half of the 20th century.

Beginning in the 1920s, it became a popular material for jewelry. Designer Coco Chanel included Bakelite bracelets in her costume jewelry collections. Designers such as Elsa Schiaparelli used it for jewelry and also for specially designed dress buttons. Later, Diana Vreeland, editor of Vogue, was enthusiastic about Bakelite. Bakelite was also used to make presentation boxes for Breitling watches. Jewelry designers such as Jorge Caicedo Montes De Oca still use vintage Bakelite materials to make designer jewelry.

By 1930, designer Paul T. Frankl considered Bakelite a "Materia Nova", "expressive of our own age". By the 1930s, Bakelite was used for game pieces like chessmen, poker chips, dominoes and mahjong sets. Kitchenware made with Bakelite, including canisters and tableware, was promoted for its resistance to heat and to chipping. In the mid-1930s, Northland marketed a line of skis with a black "Ebonite" base, a coating of Bakelite. By 1935, it was used in solid-body electric guitars. Performers such as Jerry Byrd loved the tone of Bakelite guitars but found them difficult to keep in tune.

The British children's construction toy Bayko, launched in 1933, originally used Bakelite for many of its parts, and took its name from the material.

During World War II, Bakelite was used in a variety of wartime equipment including pilot's goggles and field telephones. It was also used for patriotic wartime jewelry. In 1943, the thermosetting phenolic resin was even considered for the manufacture of coins, due to a shortage of traditional material. Bakelite and other non-metal materials were tested for usage for the one cent coin in the US before the Mint settled on zinc-coated steel.

During World War II, Bakelite buttons were part of the British uniforms. They were sometimes modified to Survival, Evasion, Resistance and Escape purposes in case of capture. "Following the introduction of BD (Battle Dress). MI9 was forced to adapt to meet the challenge of a number of different compass solutions were devised, both covert and overt. These included Bakelite buttons used in both Army (brown colored) and RAF (black) BD uniforms."

In 1947, Dutch art forger Han van Meegeren was convicted of forgery, after chemist and curator Paul B. Coremans proved that a purported Vermeer contained Bakelite, which van Meegeren had used as a paint hardener.

Bakelite was sometimes used as a substitute for metal in the magazine, pistol grip, fore grip, hand guard, and butt stock of firearms. The AKM and some early AK-74 rifles are frequently mistakenly identified as using Bakelite, but most were made with AG-S4.

By the late 1940s, newer materials were superseding Bakelite in many areas. Phenolics are less frequently used in general consumer products today due to their cost and complexity of production and their brittle nature. They still appear in some applications where their specific properties are required, such as small precision-shaped components, molded disc brake cylinders, saucepan handles, electrical plugs, switches and parts for electrical irons, as well as in the area of inexpensive board and tabletop games produced in China, Hong Kong and India. Items such as billiard balls, dominoes and pieces for board games such as chess, checkers, and backgammon are constructed of Bakelite for its look, durability, fine polish, weight, and sound. Common dice are sometimes made of Bakelite for weight and sound, but the majority are made of a thermoplastic polymer such as acrylonitrile butadiene styrene (ABS).
Bakelite continues to be used for wire insulation, brake pads and related automotive components, and industrial electrical-related applications. Bakelite stock is still manufactured and produced in sheet, rod and tube form for industrial applications in the electronics, power generation and aerospace industries, and under a variety of commercial brand names.

Phenolic resins have been commonly used in ablative heat shields. Soviet heatshields for ICBM warheads and spacecraft reentry consisted of asbestos textolite, impregnated with Bakelite. Bakelite is also used in the mounting of metal samples in metallography.

Bakelite items, particularly jewelry and radios, have become a popular collectible. The term "Bakelite" is sometimes used in the resale market to indicate various types of early plastics, including Catalin and Faturan, which may be brightly colored, as well as items made of Bakelite material.

The United States Patent and Trademark Office granted Baekeland a patent for a "Method of making insoluble products of phenol and formaldehyde" on December 7, 1909. Producing hard, compact, insoluble and infusible condensation products of phenols and formaldehyde marked the beginning of the modern plastics industry.





</doc>
<doc id="4487" url="https://en.wikipedia.org/wiki?curid=4487" title="Bean">
Bean

A bean is the seed of one of several genera of the flowering plant family Fabaceae, which are used as vegetables for human or animal food. They can be cooked in many different ways, including boiling, frying, and baking, and are used in several traditional dishes throughout the world.

The word "bean" and its Germanic cognates (e.g. German "Bohne") have existed in common use in West Germanic languages since before the 12th century, referring to broad beans, chickpeas, and other pod-borne seeds. This was long before the New World genus "Phaseolus" was known in Europe. After Columbian-era contact between Europe and the Americas, use of the word was extended to pod-borne seeds of "Phaseolus", such as the common bean and the runner bean, and the related genus "Vigna". The term has long been applied generally to many other seeds of similar form, such as Old World soybeans, peas, other vetches, and lupins, and even to those with slighter resemblances, such as coffee beans, vanilla beans, castor beans, and cocoa beans. Thus the term "bean" in general usage can refer to a host of different species.

Seeds called "beans" are often included among the crops called "pulses" (legumes), although a narrower prescribed sense of "pulses" reserves the word for leguminous crops harvested for their dry grain. The term "bean" usually excludes legumes with tiny seeds and which are used exclusively for forage, hay, and silage purposes (such as clover and alfalfa). The United Nations Food and Agriculture Organization defines "BEANS, DRY" (item code 176) as applicable only to species of "Phaseolus". However, in the past, several species, including "Vigna angularis" (adzuki bean), "V. mungo" (black gram), "V. radiata" (green gram), and "V. aconitifolia" (moth bean), were classified as "Phaseolus" and later reclassified and general usage is not governed by that definition.

Unlike the closely related pea, beans are a summer crop that needs warm temperatures to grow. Legumes are capable of nitrogen fixation and hence need less fertiliser than most plants. Maturity is typically 55–60 days from planting to harvest. As the bean pods mature, they turn yellow and dry up, and the beans inside change from green to their mature colour. As a vine, bean plants need external support, which may take the form of special "bean cages" or poles. Native Americans customarily grew them along with corn and squash (the so-called Three Sisters), with the tall cornstalks acting as support for the beans.

In more recent times, the so-called "bush bean" has been developed which does not require support and has all its pods develop simultaneously (as opposed to pole beans which develop gradually). This makes the bush bean more practical for commercial production.
Beans are one of the longest-cultivated plants. Broad beans, also called fava beans, in their wild state the size of a small fingernail, were gathered in Afghanistan and the Himalayan foothills. In a form improved from naturally occurring types, they were grown in Thailand from the early seventh millennium BCE, predating ceramics. They were deposited with the dead in ancient Egypt. Not until the second millennium BCE did cultivated, large-seeded broad beans appear in the Aegean, Iberia and transalpine Europe. In the "Iliad" (8th century BCE) there is a passing mention of beans and chickpeas cast on the threshing floor.

Beans were an important source of protein throughout Old and New World history, and still are today.

The oldest-known domesticated beans in the Americas were found in Guitarrero Cave, an archaeological site in Peru, and dated to around the second millennium BCE. However, genetic analyses of the common bean "Phaseolus" show that it originated in Mesoamerica, and subsequently spread southward, along with maize and squash, traditional companion crops.

Most of the kinds commonly eaten fresh or dried, those of the genus "Phaseolus", come originally from the Americas, being first seen by a European when Christopher Columbus, while exploring what may have been the Bahamas, found them growing in fields. Five kinds of "Phaseolus" beans were domesticated by pre-Columbian peoples: common beans ("P. vulgaris") grown from Chile to the northern part of what is now the United States, and lima and sieva beans ("P. lunatus"), as well as the less widely distributed teparies ("P. acutifolius"), scarlet runner beans ("P. coccineus") and polyanthus beans ("P. polyanthus") One especially famous use of beans by pre-Columbian people as far north as the Atlantic seaboard is the "Three Sisters" method of companion plant cultivation:

Dry beans come from both Old World varieties of broad beans (fava beans) and New World varieties (kidney, black, cranberry, pinto, navy/haricot).

Beans are a heliotropic plant, meaning that the leaves tilt throughout the day to face the sun. At night, they go into a folded "sleep" position.

Currently, the world genebanks hold about 40,000 bean varieties, although only a fraction are mass-produced for regular consumption.
Some bean types include:



Beans are high in protein, complex carbohydrates, folate, and iron. Beans also have significant amounts of fiber and soluble fiber, with one cup of cooked beans providing between nine and 13 grams of fiber. Soluble fiber can help lower blood cholesterol.
Adults are recommended to have up to two (female), and three (male) servings. 3/4 cup of cooked beans provide one serving.

Many types of bean contain significant amounts of antinutrients that inhibit some enzyme processes in the body. Phytic acid and phytates, present in grains, nuts, seeds and beans, interfere with bone growth and interrupt vitamin D metabolism. Pioneering work on the effect of phytic acid was done by Edward Mellanby from 1939.

Many edible beans, including broad beans, navy beans, kidney beans and soybeans, contain oligosaccharides (particularly raffinose and stachyose), a type of sugar molecule also found in cabbage. An anti-oligosaccharide enzyme is necessary to properly digest these sugar molecules. As a normal human digestive tract does not contain any anti-oligosaccharide enzymes, consumed oligosaccharides are typically digested by bacteria in the large intestine. This digestion process produces gases such as methane as a byproduct, which are then released as flatulence.

Processing the beans, such as by boiling, soaking, cooking, can leach the indigestible sugars from the beans and significantly reduce, if not entirely eliminate the problem. In addition enzyme pills are available.

Some kinds of raw beans contain a harmful, tasteless toxin: the lectin phytohaemagglutinin, which must be removed by cooking. Red kidney beans are particularly toxic, but other types also pose risks of food poisoning. A recommended method is to boil the beans for at least ten minutes; undercooked beans may be more toxic than raw beans.

Cooking beans, without bringing them to a boil, in a slow cooker at a temperature well below boiling may not destroy toxins. A case of poisoning by butter beans used to make falafel was reported; the beans were used instead of traditional broad beans or chickpeas, soaked and ground without boiling, made into patties, and shallow fried.

Bean poisoning is not well known in the medical community, and many cases may be misdiagnosed or never reported; figures appear not to be available. In the case of the UK National Poisons Information Service, available only to health professionals, the dangers of beans other than red beans were not flagged .

Fermentation is used in some parts of Africa to improve the nutritional value of beans by removing toxins. Inexpensive fermentation improves the nutritional impact of flour from dry beans and improves digestibility, according to research co-authored by Emire Shimelis, from the Food Engineering Program at Addis Ababa University. Beans are a major source of dietary protein in Kenya, Malawi, Tanzania, Uganda and Zambia.

It is common to make beansprouts by letting some types of bean, often mung beans, germinate in moist and warm conditions; beansprouts may be used as ingredients in cooked dishes, or eaten raw or lightly cooked. There have been many outbreaks of disease from bacterial contamination, often by "salmonella", "listeria", and "Escherichia coli", of beansprouts not thoroughly cooked, some causing significant mortality.

The production data for legumes are published by FAO in three categories:


The following is a summary of FAO data. 

Main crops of "Pulses, Total (dry)" are "Beans, dry [176]" 26.83 million tons, "Peas, dry [187]" 14.36 million tons, "Chick peas [191]" 12.09 million tons, "Cow peas [195]" 6.99 million tons, "Lentils [201]" 6.32 million tons, "Pigeon peas [197]" 4.49 million tons, "Broad beans, horse beans [181]" 4.46 million tons. In general, the consumption of pulses per capita has been decreasing since 1961. Exceptions are lentils and cowpeas.

The world leader in production of Dry Beans (Phaseolus spp). is Myanmar (Burma), followed by India and Brazil. In Africa, the most important producer is Tanzania.
No symbol = official figure, P = official figure, F = FAO estimate, * = Unofficial/Semi-official/mirror data, C = Calculated figure A = Aggregate (may include official, semi-official or estimates)

"Source: UN Food and Agriculture Organization (FAO)"




</doc>
<doc id="4489" url="https://en.wikipedia.org/wiki?curid=4489" title="Breast">
Breast

The breast is one of two prominences located on the upper ventral region of the torso of primates. In females, it serves as the mammary gland, which produces and secretes milk to feed infants. Both females and males develop breasts from the same embryological tissues. At puberty, estrogens, in conjunction with growth hormone, cause breast development in female humans and to a much lesser extent in other primates. Breast development in other primate females generally only occurs with pregnancy.

Subcutaneous fat covers and envelops a network of ducts that converge on the nipple, and these tissues give the breast its size and shape. At the ends of the ducts are lobules, or clusters of alveoli, where milk is produced and stored in response to hormonal signals. During pregnancy, the breast responds to a complex interaction of hormones, including estrogens, progesterone, and prolactin, that mediate the completion of its development, namely lobuloalveolar maturation, in preparation of lactation and breastfeeding.

Along with their major function in providing nutrition for infants, female breasts have social and sexual characteristics. Breasts have been featured in ancient and modern sculpture, art, and photography. They can figure prominently in the perception of a woman's body and sexual attractiveness. A number of cultures associate breasts with sexuality and tend to regard bare breasts in public as immodest or indecent. Breasts, especially the nipples, are an erogenous zone.

The English word "breast" derives from the Old English word "brēost" (breast, bosom) from Proto-Germanic "breustam" (breast), from the Proto-Indo-European base bhreus– (to swell, to sprout). The "breast" spelling conforms to the Scottish and North English dialectal pronunciations. The "Merriam-Webster Dictionary" states that "Middle English brest, [comes] from Old English brēost; akin to Old High German brust..., Old Irish brú [belly], [and] Russian bryukho"; the first known usage of the term was before the 12th century.

A large number of colloquial terms for breasts are used in English, ranging from fairly polite terms to vulgar or slang. Some vulgar slang expressions may be considered to be derogatory or sexist to women.

In women, the breasts overlie the pectoralis major muscles and usually extend from the level of the second rib to the level of the sixth rib in the front of the human rib cage; thus, the breasts cover much of the chest area and the chest walls. At the front of the chest, the breast tissue can extend from the clavicle (collarbone) to the middle of the sternum (breastbone). At the sides of the chest, the breast tissue can extend into the axilla (armpit), and can reach as far to the back as the latissimus dorsi muscle, extending from the lower back to the humerus bone (the bone of the upper arm). As a mammary gland, the breast is composed of differing layers of tissue, predominantly two types: adipose tissue; and glandular tissue, which affects the lactation functions of the breasts.
Morphologically the breast is tear-shaped. The superficial tissue layer (superficial fascia) is separated from the skin by 0.5–2.5 cm of subcutaneous fat (adipose tissue). The suspensory Cooper's ligaments are fibrous-tissue prolongations that radiate from the superficial fascia to the skin envelope. The female adult breast contains 14–18 irregular lactiferous lobes that converge at the nipple. The 2.0–4.5 mm milk ducts are immediately surrounded with dense connective tissue that support the glands. Milk exits the breast through the nipple, which is surrounded by a pigmented area of skin called the areola. The size of the areola can vary widely among women. The areola contains modified sweat glands known as Montgomery's glands. These glands secrete oily fluid that lubricate and protect the nipple during breastfeeding. Volatile compounds in these secretions may also serve as an olfactory stimulus for the newborn's appetite.

The dimensions and weight of the breast vary widely among women. A small-to-medium-sized breast weighs 500 grams (1.1 pounds) or less, and a large breast can weigh approximately 750 to 1,000 grams (1.7 to 2.2 pounds) or more. The tissue composition ratios of the breast also vary among women. Some women's breasts have varying proportions of glandular tissue than of adipose or connective tissues. The fat-to-connective-tissue ratio determines the density or firmness of the breast. During a woman's life, her breasts change size, shape, and weight due to hormonal changes during puberty, the menstrual cycle, pregnancy, breastfeeding, and menopause.

The breast is an apocrine gland that produces the milk used to feed an infant. The nipple of the breast is surrounded by the areola (nipple-areola complex). The areola has many sebaceous glands, and the skin color varies from pink to dark brown. The basic units of the breast are the terminal duct lobular units (TDLUs), which produce the fatty breast milk. They give the breast its offspring-feeding functions as a mammary gland. They are distributed throughout the body of the breast. Approximately two-thirds of the lactiferous tissue is within 30 mm of the base of the nipple. The terminal lactiferous ducts drain the milk from TDLUs into 4–18 lactiferous ducts, which drain to the nipple. The milk-glands-to-fat ratio is 2:1 in a lactating woman, and 1:1 in a non-lactating woman. In addition to the milk glands, the breast is also composed of connective tissues (collagen, elastin), white fat, and the suspensory Cooper's ligaments. Sensation in the breast is provided by the peripheral nervous system innervation by means of the front (anterior) and side (lateral) cutaneous branches of the fourth-, fifth-, and sixth intercostal nerves. The T-4 nerve (Thoracic spinal nerve 4), which innervates the dermatomic area, supplies sensation to the nipple-areola complex.

Approximately 75% of the lymph from the breast travels to the axillary lymph nodes on the same side of the body, whilst 25% of the lymph travels to the parasternal nodes (beside the sternum bone). A small amount of remaining lymph travels to the other breast and to the abdominal lymph nodes. The subareolar region has a lymphatic plexus known as the "subareolar plexus of Sappey". The axillary lymph nodes include the pectoral (chest), subscapular (under the scapula), and humeral (humerus-bone area) lymph-node groups, which drain to the central axillary lymph nodes and to the apical axillary lymph nodes. The lymphatic drainage of the breasts is especially relevant to oncology because breast cancer is common to the mammary gland, and cancer cells can metastasize (break away) from a tumour and be dispersed to other parts of the body by means of the lymphatic system.

The morphologic variations in the size, shape, volume, tissue density, pectoral locale, and spacing of the breasts determine their natural shape, appearance, and position on a woman's chest. Breast size and other characteristics do not predict the fat-to-milk-gland ratio or the potential for the woman to nurse an infant. The size and the shape of the breasts are influenced by normal-life hormonal changes (thelarche, menstruation, pregnancy, menopause) and medical conditions (e.g. virginal breast hypertrophy). The shape of the breasts is naturally determined by the support of the suspensory Cooper's ligaments, the underlying muscle and bone structures of the chest, and by the skin envelope. The suspensory ligaments sustain the breast from the clavicle (collarbone) and the clavico-pectoral fascia (collarbone and chest) by traversing and encompassing the fat and milk-gland tissues. The breast is positioned, affixed to, and supported upon the chest wall, while its shape is established and maintained by the skin envelope. In most women, one breast is slightly larger than the other. More obvious and persistent asymmetry in breast size occurs in up to 25% of women.

While it has been a common belief that breastfeeding causes breasts to sag, researchers have found that a woman's breasts sag due to four key factors: cigarette smoking, number of pregnancies, gravity, and weight loss or gain.

The base of each breast is attached to the chest by the deep fascia over the pectoralis major muscles. The space between the breast and the pectoralis major muscle, called retromammary space, gives mobility to the breast. 
The chest (thoracic cavity) progressively slopes outwards from the thoracic inlet (atop the breastbone) and above to the lowest ribs that support the breasts. The inframammary fold, where the lower portion of the breast meets the chest, is an anatomic feature created by the adherence of the breast skin and the underlying connective tissues of the chest; the IMF is the lower-most extent of the anatomic breast. Normal breast tissue typically has a texture that feels nodular or granular, to an extent that varies considerably from woman to woman.
The study "The Evolution of the Human Breast" (2001) proposed that the rounded shape of a woman's breast evolved to prevent the sucking infant offspring from suffocating while feeding at the teat; that is, because of the human infant's small jaw, which did not project from the face to reach the nipple, he or she might block the nostrils against the mother's breast if it were of a flatter form (cf. common chimpanzee). Theoretically, as the human jaw receded into the face, the woman's body compensated with round breasts.

The breasts are principally composed of adipose, glandular, and connective tissues. Because these tissues have hormone receptors, their sizes and volumes fluctuate according to the hormonal changes particular to thelarche (sprouting of breasts), menstruation (egg production), pregnancy (reproduction), lactation (feeding of offspring), and menopause (end of menstruation).

The morphological structure of the human breast is identical in males and females until puberty. For pubescent girls in thelarche (the breast-development stage), the female sex hormones (principally estrogens) in conjunction with growth hormone promote the sprouting, growth, and development of the breasts. During this time, the mammary glands grow in size and volume and begin resting on the chest. These development stages of secondary sex characteristics (breasts, pubic hair, etc.) are illustrated in the five-stage Tanner Scale.

During thelarche the developing breasts are sometimes of unequal size, and usually the left breast is slightly larger. This condition of asymmetry is transitory and statistically normal in female physical and sexual development. Medical conditions can cause overdevelopment (e.g., virginal breast hypertrophy, macromastia) or underdevelopment (e.g., tuberous breast deformity, micromastia) in girls and women.

Approximately two years after the onset of puberty (a girl's first menstrual cycle), estrogen and growth hormone stimulate the development and growth of the glandular fat and suspensory tissues that compose the breast. This continues for approximately four years until the final shape of the breast (size, volume, density) is established at about the age of 21. Mammoplasia (breast enlargement) in girls begins at puberty, unlike all other primates in which breasts enlarge only during lactation.
During the menstrual cycle, the breasts are enlarged by premenstrual water retention and temporary growth.

The breasts reach full maturity only when a woman's first pregnancy occurs. Changes to the breasts are among the very first signs of pregnancy. The breasts become larger, the nipple-areola complex becomes larger and darker, the Montgomery's glands enlarge, and veins sometimes become more visible. Breast tenderness during pregnancy is common, especially during the first trimester. By mid-pregnancy, the breast is physiologically capable of lactation and some women can express colostrum, a form of breast milk.

Pregnancy causes elevated levels of the hormone prolactin, which has a key role in the production of milk. However, milk production is blocked by the hormones progesterone and estrogen until after delivery, when progesterone and estrogen levels plummet.

At menopause, breast atrophy occurs. The breasts can decrease in size when the levels of circulating estrogen decline. The adipose tissue and milk glands also begin to wither. The breasts can also become enlarged from adverse side effects of combined oral contraceptive pills. The size of the breasts can also increase and decrease in response to weight fluctuations. Physical changes to the breasts are often recorded in the stretch marks of the skin envelope; they can serve as historical indicators of the increments and the decrements of the size and volume of a woman's breasts throughout the course of her life.

The primary function of the breasts, as mammary glands, is the nourishing of an infant with breast milk. Milk is produced in milk-secreting cells in the alveoli. When the breasts are stimulated by the suckling of her baby, the mother's brain secretes oxytocin. High levels of oxytocin trigger the contraction of muscle cells surrounding the alveoli, causing milk to flow along the ducts that connect the alveoli to the nipple.

Full-term newborns have an instinct and a need to suck on a nipple, and breastfed babies nurse for both nutrition and for comfort. Breast milk provides all necessary nutrients for the first six months of life, and then remains an important source of nutrition, alongside solid foods, until at least one or two years of age.

The breast is susceptible to numerous benign and malignant conditions. The most frequent benign conditions are puerperal mastitis, fibrocystic breast changes and mastalgia.

Lactation unrelated to pregnancy is known as galactorrhea. It can be caused by certain drugs (such as antipsychotic medications), extreme physical stress, or endocrine disorders. Lactation in newborns is caused by hormones from the mother that crossed into the baby's bloodstream during pregnancy.

Breast cancer is the most common cause of cancer death among women and it is one of the leading causes of death among women. Factors that appear to be implicated in decreasing the risk of breast cancer are regular breast examinations by health care professionals, regular mammograms, self-examination of breasts, healthy diet, and exercise to decrease excess body fat, and breastfeeding.

Both females and males develop breasts from the same embryological tissues. Normally, males produce lower levels of estrogens and higher levels of androgens, namely testosterone, which suppress the effects of estrogens in developing excessive breast tissue.
In boys and men, abnormal breast development is manifested as gynecomastia, the consequence of a biochemical imbalance between the normal levels of estrogen and testosterone in the male body. Around 70% of boys temporarily develop breast tissue during adolescence.
The condition usually resolves by itself within two years. When male lactation occurs, it is considered a symptom of a disorder of the pituitary gland.

Plastic surgery can be performed to augment or reduce the size of breasts, or reconstruct the breast in cases of deformative disease, such as breast cancer. Breast augmentation and breast lift (mastopexy) procedures are done only for cosmetic reasons, whereas breast reduction is sometimes medically indicated.
In cases where a woman's breasts are severely asymmetrical, surgery can be performed to either enlarge the smaller breast, reduce the size of the larger breast, or both.

Breast augmentation surgery generally does not interfere with future ability to breastfeed. Breast reduction surgery more frequently leads to decreased sensation in the nipple-areola complex, and to low milk supply in women who choose to breastfeed. Implants can interfere with mammography (breast x-rays images).

In Christian iconography, some works of art depict women with their breasts in their hands or on a platter, signifying that they died as a martyr by having their breasts severed; one example of this is Saint Agatha of Sicily.

Femen is a feminist activist group which uses topless protests as part of their campaigns against sex tourism religious institutions, sexism, homophobia and to "defend [women's] right to abortion". Femen activists have been regularly detained by police in response to their protests.

There is a long history of female breasts being used by comedians as a subject for comedy fodder (e.g., British comic Benny Hill's burlesque/slapstick routines).

In European pre-historic societies, sculptures of female figures with pronounced or highly exaggerated breasts were common. A typical example is the so-called Venus of Willendorf, one of many Paleolithic Venus figurines with ample hips and bosom. Artifacts such as bowls, rock carvings and sacred statues with breasts have been recorded from 15,000 BC up to late antiquity all across Europe, North Africa and the Middle East.

Many female deities representing love and fertility were associated with breasts and breast milk. Figures of the Phoenician goddess Astarte were represented as pillars studded with breasts. Isis, an Egyptian goddess who represented, among many other things, ideal motherhood, was often portrayed as suckling pharaohs, thereby confirming their divine status as rulers. Even certain male deities representing regeneration and fertility were occasionally depicted with breast-like appendices, such as the river god Hapy who was considered to be responsible for the annual overflowing of the Nile.

Female breasts were also prominent in the Minoan civilization in the form of the famous Snake Goddess statuettes. In Ancient Greece there were several cults worshipping the "Kourotrophos", the suckling mother, represented by goddesses such as Gaia, Hera and Artemis. The worship of deities symbolized by the female breast in Greece became less common during the first millennium. The popular adoration of female goddesses decreased significantly during the rise of the Greek city states, a legacy which was passed on to the later Roman Empire.

During the middle of the first millennium BC, Greek culture experienced a gradual change in the perception of female breasts. Women in art were covered in clothing from the neck down, including female goddesses like Athena, the patron of Athens who represented heroic endeavor. There were exceptions: Aphrodite, the goddess of love, was more frequently portrayed fully nude, though in postures that were intended to portray shyness or modesty, a portrayal that has been compared to modern pin ups by historian Marilyn Yalom. Although nude men were depicted standing upright, most depictions of female nudity in Greek art occurred "usually with drapery near at hand and with a forward-bending, self-protecting posture". A popular legend at the time was of the Amazons, a tribe of fierce female warriors who socialized with men only for procreation and even removed one breast to become better warriors (the idea being that the right breast would interfere with the operation of a bow and arrow). The legend was a popular motif in art during Greek and Roman antiquity and served as an antithetical cautionary tale.

Many women regard their breasts as important to their sexual attractiveness, as a sign of femininity that is important to their sense of self. A woman with smaller breasts may regard her breasts as less attractive.

Because breasts are mostly fatty tissue, their shape can—within limits—be molded by clothing, such as foundation garments. Bras are commonly worn by about 90% of Western women, and are often worn for support. The social norm in most Western cultures is to cover breasts in public, though the extent of coverage varies depending on the social context. Some religions ascribe a special status to the female breast, either in formal teachings or through symbolism. Islam forbids women from exposing their breasts in public.

Many cultures, including Western cultures in North America, associate breasts with sexuality and tend to regard bare breasts as immodest or indecent. In some cultures, like the Himba in northern Namibia, bare-breasted women are normal. In some African cultures, for example, the thigh is regarded as highly sexualised and never exposed in public, but breast exposure is not taboo. In a few Western countries and regions female toplessness at a beach is acceptable, although it may not be acceptable in the town center.

Social attitudes and laws regarding breastfeeding in public vary widely. In many countries, breastfeeding in public is common, legally protected, and generally not regarded as an issue. However, even though the practice may be legal or socially accepted, some mothers may nevertheless be reluctant to expose a breast in public to breastfeed due to actual or potential objections by other people, negative comments, or harassment. It is estimated that around 63% of mothers across the world have publicly breast-fed. Bare-breasted women are legal and culturally acceptable at public beaches in Australia and much of Europe. Filmmaker Lina Esco made a film entitled "Free the Nipple", which is about "...laws against female toplessness or restrictions on images of female, but not male, nipples", which Esco states is an example of sexism in society.

In some cultures, breasts play a role in human sexual activity. In Western culture, breasts have a "...hallowed sexual status, arguably more fetishized than either sex’s genitalia". Breasts and especially the nipples are among the various human erogenous zones. They are sensitive to the touch as they have many nerve endings; and it is common to press or massage them with hands or orally before or during sexual activity. During sexual arousal, breast size increases, venous patterns across the breasts become more visible, and nipples harden. Compared to other primates, human breasts are proportionately large throughout adult females' lives. Some writers have suggested that they may have evolved as a visual signal of sexual maturity and fertility.

Many people regard bare female breasts to be aesthetically pleasing or erotic, and they can elicit heightened sexual desires in men in many cultures. In the ancient Indian work the "Kama Sutra", light scratching of the breasts with nails and biting with teeth are considered erotic. Some people show a sexual interest in female breasts distinct from that of the person, which may be regarded as a breast fetish. A number of Western fashions include clothing which accentuate the breasts, such as the use of push-up bras and decollete (plunging neckline) gowns and blouses which show cleavage. While U.S. culture prefers breasts that are youthful and upright, some cultures venerate women with drooping breasts, indicating mothering and the wisdom of experience.

Research conducted at the Victoria University of Wellington showed that breasts are often the first thing men look at, and for a longer time than other body parts. The writers of the study had initially speculated that the reason for this is due to endocrinology with larger breasts indicating higher levels of estrogen and a sign of greater fertility, but the researchers said that "Men may be looking more often at the breasts because they are simply aesthetically pleasing, regardless of the size."

Some women report achieving an orgasm from nipple stimulation, but this is rare. Research suggests that the orgasms are genital orgasms, and may also be directly linked to "the genital area of the brain". In these cases, it seems that sensation from the nipples travels to the same part of the brain as sensations from the vagina, clitoris and cervix. Nipple stimulation may trigger uterine contractions, which then produce a sensation in the genital area of the brain.

There are many mountains named after the breast because they resemble it in appearance and so are objects of religious and ancestral veneration as a fertility symbol and of well-being. In Asia, there was "Breast Mountain", which had a cave where the Buddhist monk Bodhidharma (Da Mo) spent much time in meditation. Other such breast mountains are Mount Elgon on the Uganda-Kenya border, Beinn Chìochan and the Maiden Paps in Scotland, the "Bundok ng Susong Dalaga" (Maiden's breast mountains) in Talim Island, Philippines, the twin hills known as the Paps of Anu ("Dá Chích Anann" or "the breasts of Anu"), near Killarney in Ireland, the 2,086 m high "Tetica de Bacares" or "La Tetica" in the Sierra de Los Filabres, Spain, and Khao Nom Sao in Thailand, Cerro Las Tetas in Puerto Rico and the Breasts of Aphrodite in Mykonos, among many others. In the United States, the Teton Range is named after the French word for "breast".





</doc>
<doc id="4492" url="https://en.wikipedia.org/wiki?curid=4492" title="Baghdad">
Baghdad

Baghdad (; , ) is the capital of Iraq and the third-largest city in the Arab world after Cairo and Riyadh. Located along the Tigris River, the city was founded in the 8th century, and became the capital of the Abbasid Caliphate. Within a short time of its inception, Baghdad evolved into a significant cultural, commercial, and intellectual center of the Muslim world. This, in addition to housing several key academic institutions, including the House of Wisdom, as well as hosting a multiethnic and multireligious environment, garnered the city a worldwide reputation as the "Centre of Learning".

Baghdad was the largest city in the world for much of the Abbasid era during the Islamic Golden Age, peaking at a population of more than a million. The city was largely destroyed at the hands of the Mongol Empire in 1258, resulting in a decline that would linger through many centuries due to frequent plagues and multiple successive empires. With the recognition of Iraq as an independent state (formerly the British Mandate of Mesopotamia) in 1932, Baghdad gradually regained some of its former prominence as a significant center of Arabic culture, with a population variously estimated at 6 or over 7 million.

In contemporary times, the city has often faced severe infrastructural damage, most recently due to the United States-led 2003 invasion of Iraq, and the subsequent Iraq War that lasted until December 2011. In recent years, the city has been frequently subjected to insurgent attacks, resulting in a substantial loss of cultural heritage and historical artifacts as well. , Baghdad was listed as one of the least hospitable places in the world to live, ranked by Mercer as the worst major city for quality of life in the world.

The name Baghdad is pre-Islamic, and its origin is disputed. The site where the city of Baghdad developed has been populated for millennia. By the 8th century AD, several villages had developed there, including a Persian hamlet called "Baghdad," the name which would come to be used for the Abbasid metropolis.

Arab authors, realizing the pre-Islamic origins of Baghdad's name, generally looked for its roots in Middle Persian. They suggested various meanings, the most common of which was "bestowed by God". Modern scholars generally tend to favor this etymology, which views the word as a compound of "bagh" () "god" and "dād" () "given", In Old Persian this can be traced to Sanskrit "bhag" (भग) which means "god" and "dātta" (दत्त) which means "given" or "bhagdatta" (भगदत्त) which would mean "god given, the first element can be traced to "boghu" and is related to "bog", a Slavic word for "god". A similar term in Middle Persian is the name "Mithradāt" ("Mihrdād" in New Persian), known in English by its Hellenistic form Mithridates, meaning "gift of Mithra" ("dāt" is the more archaic form of "dād", related to Latin "dat" and English "donor"). There are a number of other locations in the wider region whose names are compounds of the word "bagh", including Baghlan and Bagram in Afghanistan, Baghshan in Iran, and Baghdati in Georgia, which likely share the same etymological origins.

A few authors have suggested older origins for the name, in particular the name "Bagdadu" or "Hudadu" that existed in Old Babylonian (spelled with a sign that can represent both "bag" and "hu"), and the Babylonian Talmudic name of a place called "Baghdatha". Some scholars suggested Aramaic derivations.

When the Abbasid caliph, Al-Mansur, founded a completely new city for his capital, he chose the name Madinat al-Salaam or "City of Peace". This was the official name on coins, weights, and other official usage, although the common people continued to use the old name. By the 11th century, "Baghdad" became almost the exclusive name for the world-renowned metropolis.

After the fall of the Umayyads, the first Muslim dynasty, the victorious Abbasid rulers wanted their own capital from which they could rule. They chose a site north of the Sassanid capital of Ctesiphon (and also just north of where ancient Babylon had once stood), and on 30 July 762 the caliph Al-Mansur commissioned the construction of the city. It was built under the supervision of the Barmakids. Mansur believed that Baghdad was the perfect city to be the capital of the Islamic empire under the Abbasids. Mansur loved the site so much he is quoted saying: "This is indeed the city that I am to found, where I am to live, and where my descendants will reign afterward".

The city's growth was helped by its excellent location, based on at least two factors: it had control over strategic and trading routes along the Tigris, and it had an abundance of water in a dry climate. Water exists on both the north and south ends of the city, allowing all households to have a plentiful supply, which was very uncommon during this time. The city of Baghdad soon became so large that it had to be divided into three judicial districts: Madinat al-Mansur (the Round City), al-Sharqiyya (Karkh) and Askar al-Mahdi (on the West Bank).

Baghdad eclipsed Ctesiphon, the capital of the Sassanians, which was located some to the southeast. Today, all that remains of Ctesiphon is the shrine town of Salman Pak, just to the south of Greater Baghdad. Ctesiphon itself had replaced and absorbed Seleucia, the first capital of the Seleucid Empire, which had earlier replaced the city of Babylon.

According to the traveler Ibn Battuta, Baghdad was one of the largest cities, not including the damage it has received. The residents are mostly Hanbal. Baghdad is also home to the grave of Abu Hanifa where there is a cell and a mosque above it. The Sultan of Baghdad, Abu Said Bahadur Khan, was a Tatar king who embraced Islam.

In its early years, the city was known as a deliberate reminder of an expression in the Qur'an, when it refers to Paradise. It took four years to build (764–768). Mansur assembled engineers, surveyors, and art constructionists from around the world to come together and draw up plans for the city. Over 100,000 construction workers came to survey the plans; many were distributed salaries to start the building of the city. July was chosen as the starting time because two astrologers, Naubakht Ahvazi and Mashallah, believed that the city should be built under the sign of the lion, Leo. Leo is associated with fire and symbolises productivity, pride, and expansion.

The bricks used to make the city were on all four sides. Abu Hanifah was the counter of the bricks and he developed a canal, which brought water to the work site for both human consumption and the manufacture of the bricks. Marble was also used to make buildings throughout the city, and marble steps led down to the river's edge.

The basic framework of the city consists of two large semicircles about in diameter. The city was designed as a circle about in diameter, leading it to be known as the "Round City". The original design shows a single ring of residential and commercial structures along the inside of the city walls, but the final construction added another ring inside the first. Within the city there were many parks, gardens, villas, and promenades. In the center of the city lay the mosque, as well as headquarters for guards. The purpose or use of the remaining space in the center is unknown. The circular design of the city was a direct reflection of the traditional Persian Sasanian urban design. The Sasanian city of Gur in Fars, built 500 years before Baghdad, is nearly identical in its general circular design, radiating avenues, and the government buildings and temples at the centre of the city. This style of urban planning contrasted with Ancient Greek and Roman urban planning, in which cities are designed as squares or rectangles with streets intersecting each other at right angles.


The four surrounding walls of Baghdad were named Kufa, Basra, Khurasan, and Syria; named because their gates pointed in the directions of these destinations. The distance between these gates was a little less than . Each gate had double doors that were made of iron; the doors were so heavy it took several men to open and close them. The wall itself was about 44 m thick at the base and about 12 m thick at the top. Also, the wall was 30 m high, which included merlons, a solid part of an embattled parapet usually pierced by embrasures. This wall was surrounded by another wall with a thickness of 50 m. The second wall had towers and rounded merlons, which surrounded the towers. This outer wall was protected by a solid glacis, which is made out of bricks and quicklime. Beyond the outer wall was a water-filled moat.

The Golden Gate Palace, the residence of the caliph and his family, was in the middle of Baghdad, in the central square. In the central part of the building, there was a green dome that was 39 m high. Surrounding the palace was an esplanade, a waterside building, in which only the caliph could come riding on horseback. In addition, the palace was near other mansions and officer's residences. Near the Gate of Syria, a building served as the home for the guards. It was made of brick and marble. The palace governor lived in the latter part of the building and the commander of the guards in the front. In 813, after the death of caliph Al-Amin, the palace was no longer used as the home for the caliph and his family.
The roundness points to the fact that it was based on Arabic script. The two designers who were hired by Al-Mansur to plan the city's design were Naubakht, a Zoroastrian who also determined that the date of the foundation of the city would be astrologically auspicious, and Mashallah, a Jew from Khorasan, Iran.

Within a generation of its founding, Baghdad became a hub of learning and commerce. The city flourished into an unrivaled intellectual center of science, medicine, philosophy, and education, especially with the Abbasid Translation Movement began under the second caliph Al-Mansur and thrived under the seventh caliph Al-Ma'mun. "Baytul-Hikmah" or the "House of Wisdom" was among the most well known academies, and had the largest selection of books in the world by the middle of the 9th century. Notable scholars based in Baghdad during this time include translator Hunayn ibn Ishaq, mathematician al-Khwarizmi, and philosopher Al-Kindi. Although Arabic was used as the international language of science, the scholarship involved not only Arabs, but also Persians, Syriacs, Nestorians, Jews, Arab Christians, and people from other ethnic and religious groups native to the region. These are considered among the fundamental elements that contributed to the flourishing of scholarship in the Medieval Islamic world. Baghdad was also a significant center of Islamic religious learning, with Al-Jahiz contributing to the formation of Mu'tazili theology, as well as Al-Tabari culminating the scholarship on the Quranic exegesis. Baghdad was likely the largest city in the world from shortly after its foundation until the 930s, when it tied with Córdoba.
Several estimates suggest that the city contained over a million inhabitants at its peak. Many of the "One Thousand and One Nights" tales, widely known as the "Arabian Nights", are set in Baghdad during this period.

Among the notable features of Baghdad during this period were its exceptional libraries. Many of the Abbasid caliphs were patrons of learning and enjoyed collecting both ancient and contemporary literature. Although some of the princes of the previous Umayyad dynasty had begun to gather and translate Greek scientific literature, the Abbasids were the first to foster Greek learning on a large scale. Many of these libraries were private collections intended only for the use of the owners and their immediate friends, but the libraries of the caliphs and other officials soon took on a public or a semi-public character. Four great libraries were established in Baghdad during this period. The earliest was that of the famous Al-Ma'mun, who was caliph from 813 to 833. Another was established by Sabur ibn Ardashir in 991 or 993 for the literary men and scholars who frequented his academy. Unfortunately, this second library was plundered and burned by the Seljuks only seventy years after it was established. This was a good example of the sort of library built up out of the needs and interests of a literary society. The last two were examples of "madrasa" or theological college libraries. The Nezamiyeh was founded by the Persian Nizam al-Mulk, who was vizier of two early Seljuk sultans. It continued to operate even after the coming of the Mongols in 1258. The Mustansiriyah "madrasa", which owned an exceedingly rich library, was founded by Al-Mustansir, the second last Abbasid caliph, who died in 1242. This would prove to be the last great library built by the caliphs of Baghdad.

By the 10th century, the city's population was between 1.2 million and 2 million. Baghdad's early meteoric growth eventually slowed due to troubles within the Caliphate, including relocations of the capital to Samarra (during 808–819 and 836–892), the loss of the western and easternmost provinces, and periods of political domination by the Iranian Buwayhids (945–1055) and Seljuk Turks (1055–1135).

The Seljuks were a clan of the Oghuz Turks from Central Asia that converted to the Sunni branch of Islam. In 1040, they destroyed the Ghaznavids, taking over their land and in 1055, Tughril Beg, the leader of the Seljuks, took over Baghdad. The Seljuks expelled the Buyid dynasty of Shiites that had ruled for some time and took over power and control of Baghdad. They ruled as Sultans in the name of the Abbasid caliphs (they saw themselves as being part of the Abbasid regime). Tughril Beg saw himself as the protector of the Abbasid Caliphs.

Sieges and wars in which Baghdad was involved are listed below:


In 1058, Baghdad was captured by the Fatimids under the Turkish general Abu'l-Ḥārith Arslān al-Basasiri, an adherent of the Ismailis along with the 'Uqaylid Quraysh. Not long before the arrival of the Saljuqs in Baghdad, al-Basasiri petitioned to the Fatimid Imam-Caliph al-Mustansir to support him in conquering Baghdad on the Ismaili Imam's behalf. It has recently come to light that the famed Fatimid "da'i", al-Mu'ayyad al-Shirazi, had a direct role in supporting al-Basasiri and helped the general to succeed in taking Mawṣil, Wāsit and Kufa. Soon after, by December 1058, a Shi'i "adhān" (call to prayer) was implemented in Baghdad and a "khutbah" (sermon) was delivered in the name of the Fatimid Imam-Caliph. Despite his Shi'i inclinations, Al-Basasiri received support from Sunnis and Shi'is alike, for whom opposition to the Saljuq power was a common factor.
On 10 February 1258, Baghdad was captured by the Mongols led by Hulegu, a grandson of Chingiz Khan (Genghis Khan), during the siege of Baghdad. Many quarters were ruined by fire, siege, or looting. The Mongols massacred most of the city's inhabitants, including the caliph Al-Musta'sim, and destroyed large sections of the city. The canals and dykes forming the city's irrigation system were also destroyed. During this time, in Baghdad, Christians and Shia were tolerated, while Sunnis were treated as enemies. The sack of Baghdad put an end to the Abbasid Caliphate. It has been argued that this marked an end to the Islamic Golden Age and served a blow from which Islamic civilisation never fully recovered.
At this point, Baghdad was ruled by the Ilkhanate, a breakaway state of the Mongol Empire, ruling from Iran. In August 1393, Baghdad was occupied by the Central Asian Turkic conqueror Timur ("Tamerlane"), by marching there in only eight days from Shiraz. Sultan Ahmad Jalayir fled to Syria, where the Mamluk Sultan Barquq protected him and killed Timur's envoys. Timur left the Sarbadar prince Khwaja Mas'ud to govern Baghdad, but he was driven out when Ahmad Jalayir returned.

In 1401, Baghdad was again sacked, by Timur. When his forces took Baghdad, he spared almost no one, and ordered that each of his soldiers bring back two severed human heads. Baghdad became a provincial capital controlled by the Mongol Jalayirid (1400–1411), Turkic Kara Koyunlu (1411–1469), Turkic Ak Koyunlu (1469–1508), and the Iranian Safavid (1508–1534) dynasties.

In 1534, Baghdad was captured by the Ottoman Turks. Under the Ottomans, Baghdad continued into a period of decline, partially as a result of the enmity between its rulers and Iranian Safavids, which did not accept the Sunni control of the city. Between 1623 and 1638, it returned to Iranian rule before falling back into Ottoman hands. Baghdad has suffered severely from visitations of the plague and cholera, and sometimes two-thirds of its population has been wiped out.

For a time, Baghdad had been the largest city in the Middle East. The city saw relative revival in the latter part of the 18th century, under a Mamluk government. Direct Ottoman rule was reimposed by Ali Rıza Pasha in 1831. From 1851 to 1852 and from 1861 to 1867, Baghdad was governed, under the Ottoman Empire by Mehmed Namık Pasha. The Nuttall Encyclopedia reports the 1907 population of Baghdad as 185,000.

Baghdad and southern Iraq remained under Ottoman rule until 1917, when captured by the British during World War I. In 1920, Baghdad became the capital of the British Mandate of Mesopotamia with several architectural and planning projects commissioned to reinforce this administration. After receiving independence in 1932, the capital of the Kingdom of Iraq. The city's population grew from an estimated 145,000 in 1900 to 580,000 in 1950. During the Mandate, Baghdad's substantial Jewish community comprised a quarter of the city's population. On 1 April 1941, members of the "Golden Square" and Rashid Ali staged a coup in Baghdad. Rashid Ali installed a pro-German and pro-Italian government to replace the pro-British government of Regent Abdul Ilah. On 31 May, after the resulting Anglo-Iraqi War and after Rashid Ali and his government had fled, the Mayor of Baghdad surrendered to British and Commonwealth forces. On 14 July 1958, members of the Iraqi Army, under Abd al-Karim Qasim, staged a coup to topple the Kingdom of Iraq. King Faisal II, former Prime Minister Nuri as-Said, former Regent Prince 'Abd al-Ilah, members of the royal family, and others were brutally killed during the coup. Many of the victim's bodies were then dragged through the streets of Baghdad.
During the 1970s, Baghdad experienced a period of prosperity and growth because of a sharp increase in the price of petroleum, Iraq's main export. New infrastructure including modern sewerage, water, and highway facilities were built during this period. The masterplans of the city (1967, 1973) were delivered by the Polish planning office Miastoprojekt-Kraków, mediated by Polservice. However, the Iran–Iraq War of the 1980s was a difficult time for the city, as money was diverted by Saddam Hussein to the army and thousands of residents were killed. Iran launched a number of missile attacks against Baghdad in retaliation for Saddam Hussein's continuous bombardments of Tehran's residential districts. In 1991 and 2003, the Gulf War and the 2003 invasion of Iraq caused significant damage to Baghdad's transportation, power, and sanitary infrastructure as the US-led coalition forces launched massive aerial assaults in the city in the two wars. Also in 2003, the minor riot in the city (which took place on 21 July) caused some disturbance in the population. The historic "Assyrian Quarter" of the city, Dora, which boasted a population of 150,000 Assyrians in 2003, made up over 3% of the capital's Assyrian population then. The community has been subject to kidnappings, death threats, vandalism, and house burnings by Al-Qaeda and other insurgent groups. As of the end of 2014, only 1,500 Assyrians remained in Dora.

Points of interest include the National Museum of Iraq whose collection of artifacts was looted during the 2003 invasion, and the iconic Hands of Victory arches. Multiple Iraqi parties are in discussions as to whether the arches should remain as historical monuments or be dismantled. Thousands of ancient manuscripts in the National Library were destroyed under Saddam's command.

Mutanabbi Street is located near the old quarter of Baghdad; at Al Rasheed Street. It is the historic center of Baghdadi book-selling, a street filled with bookstores and outdoor book stalls. It was named after the 10th-century classical Iraqi poet Al-Mutanabbi. This street is well established for bookselling and has often been referred to as the heart and soul of the Baghdad literacy and intellectual community.

The zoological park used to be the largest in the Middle East. Within eight days following the 2003 invasion, however, only 35 of the 650 animals in the facility survived. This was a result of theft of some animals for human food, and starvation of caged animals that had no food. South African Lawrence Anthony and some of the zoo keepers cared for the animals and fed the carnivores with donkeys they had bought locally. Eventually, Paul Bremer, Director of the Coalition Provisional Authority in Iraq from 11 May 2003 to 28 June 2004 ordered protection of the zoo and U.S. engineers helped to reopen the facility.

Grand Festivities Square is the main square where public celebrations are held and is also the home to three important monuments commemorating Iraqi's fallen soldiers and victories in war; namely Al-Shaheed Monument, the Victory Arch and the Unknown Soldier's Monument.

Al-Shaheed Monument, also known as the Martyr's Memorial, is a monument dedicated to the Iraqi soldiers who died in the Iran–Iraq War. However, now it is generally considered by Iraqis to be for all of the martyrs of Iraq, especially those allied with Iran and Syria fighting ISIS, not just of the Iran–Iraq War. The monument was opened in 1983, and was designed by the Iraqi architect Saman Kamal and the Iraqi sculptor and artist Ismail Fatah Al Turk. During the 1970s and 1980s, Saddam Hussein's government spent a lot of money on new monuments, which included the al-Shaheed Monument.

Qushla or Qishla is a public square and the historical complex located in Rusafa neighborhood at the riverbank of Tigris. Qushla and its surroundings is where the historical features and cultural capitals of Baghdad are concentrated, from the Mutanabbi Street, Abbasid-era palace and bridges, Ottoman-era mosques to the Mustansariyah Madrasa. The square developed during the Ottoman era as a military barracks. Today, it is a place where the citizens of Baghdad find leisure such as reading poetry in gazebos. It is characterized by the iconic clock tower which was donated by George V. The entire area is submitted to the UNESCO World Heritage Site Tentative list.



Firdos Square is a public open space in Baghdad and the location of two of the best-known hotels, the Palestine Hotel and the Sheraton Ishtar, which are both also the tallest buildings in Baghdad. The square was the site of the statue of Saddam Hussein that was pulled down by U.S. coalition forces in a widely televised event during the 2003 invasion of Iraq.

Administratively, Baghdad Governorate is divided into districts which are further divided into sub-districts. Municipally, the governorate is divided into 9 municipalities, which have responsibility for local issues. Regional services, however, are coordinated and carried out by a mayor who oversees the municipalities. There is no single city council that singularly governs Baghdad at a municipal level. The governorate council is responsible for the governorate-wide policy. These official subdivisions of the city served as administrative centres for the delivery of municipal services but until 2003 had no political function. Beginning in April 2003, the U.S. controlled Coalition Provisional Authority (CPA) began the process of creating new functions for these. The process initially focused on the election of neighbourhood councils in the official neighbourhoods, elected by neighbourhood caucuses. The CPA convened a series of meetings in each neighbourhood to explain local government, to describe the caucus election process and to encourage participants to spread the word and bring friends, relatives and neighbours to subsequent meetings. Each neighbourhood process ultimately ended with a final meeting where candidates for the new neighbourhood councils identified themselves and asked their neighbours to vote for them. Once all 88 (later increased to 89) neighbourhood councils were in place, each neighbourhood council elected representatives from among their members to serve on one of the city's nine district councils. The number of neighbourhood representatives on a district council is based upon the neighbourhood's population. The next step was to have each of the nine district councils elect representatives from their membership to serve on the 37 member Baghdad City Council. This three tier system of local government connected the people of Baghdad to the central government through their representatives from the neighbourhood, through the district, and up to the city council. The same process was used to provide representative councils for the other communities in Baghdad Province outside of the city itself. There, local councils were elected from 20 neighbourhoods (Nahia) and these councils elected representatives from their members to serve on six district councils (Qada). As within the city, the district councils then elected representatives from among their members to serve on the 35 member Baghdad Regional Council. The first step in the establishment of the system of local government for Baghdad Province was the election of the Baghdad Provincial Council. As before, the representatives to the Provincial Council were elected by their peers from the lower councils in numbers proportional to the population of the districts they represent. The 41 member Provincial Council took office in February 2004 and served until national elections held in January 2005, when a new Provincial Council was elected. This system of 127 separate councils may seem overly cumbersome; however, Baghdad Province is home to approximately seven million people. At the lowest level, the neighbourhood councils, each council represents an average of 75,000 people. The nine District Advisory Councils (DAC) are as follows:


The nine districts are subdivided into 89 smaller neighborhoods which may make up sectors of any of the districts above. The following is a "selection" (rather than a complete list) of these neighborhoods:

The city is located on a vast plain bisected by the Tigris river. The Tigris splits Baghdad in half, with the eastern half being called "Risafa" and the Western half known as "Karkh". The land on which the city is built is almost entirely flat and low-lying, being of alluvial origin due to the periodic large floods which have occurred on the river.

Baghdad has a hot desert climate (Köppen "BWh"), featuring extremely hot, prolonged, dry summers and mild to cool, slightly wet, short winters. In the summer, from June through August, the average maximum temperature is as high as and accompanied by sunshine. Rainfall has been recorded on fewer than half a dozen occasions at this time of year and has never exceeded . Even at night, temperatures in summer are seldom below . Baghdad's record highest temperature of was reached on 28 July 2020. The humidity is typically under 50% in summer due to Baghdad's distance from the marshy southern Iraq and the coasts of Persian Gulf, and dust storms from the deserts to the west are a normal occurrence during the summer.

Winter temperatures are typical of hot desert climates. From December through February, Baghdad has maximum temperatures averaging , though highs above are not unheard of. Lows below freezing occur a couple of times per year on average.

Annual rainfall, almost entirely confined to the period from November through March, averages approximately , but has been as high as and as low as . On 11 January 2008, light snow fell across Baghdad for the first time in 100 years. Snowfall was again reported on 11 February 2020, with accumulations across the city.

Baghdad's population was estimated at 7.22 million in 2015. The city historically had a predominantly Sunni population, but by the early 21st century around 82% of the city's population were Iraqi Shi'ites. At the beginning of the 21st century, some 1.5 million people migrated to Baghdad, most of them Shiites and a few Sunnis. Sunni Muslims make up 23% of Iraq's population and they are still a majority in west and north Iraq. As early as 2003, about 20 percent of the population of the city was the result of mixed marriages between Shi'ites and Sunnis: they are often referred to as "Sushis". Following the sectarian violence in Iraq between the Sunni and Shia militia groups during the U.S. occupation of Iraq, the city's population became overwhelmingly Shia. Despite the government's promise to resettle Sunnis displaced by the violence, little has been done to bring this about. The Iraqi Civil War following ISIS' invasion in 2014 caused hundreds of thousands of Iraqi internally displaced people to flee to the city. The city has Sunni, Shia, Assyrian/Chaldean/Syriacs, Armenians and mixed neighborhoods. The city was also home to a large Jewish community and regularly visited by Sikh pilgrims.

Baghdad accounts for 22.2 per cent of Iraq's population and 40 per cent of the country's gross domestic product (PPP). Iraqi Airways, the national airline of Iraq, has its headquarters on the grounds of Baghdad International Airport in Baghdad. 

Most Iraqi reconstruction efforts have been devoted to the restoration and repair of badly damaged urban infrastructure. More visible efforts at reconstruction through private development, like architect and urban designer Hisham N. Ashkouri's Baghdad Renaissance Plan and the Sindbad Hotel Complex and Conference Center have also been made. A plan was proposed by a Government agency to rebuild a tourist island in 2008. In late 2009, a construction plan was proposed to rebuild the heart of Baghdad, but the plan was never realized because corruption was involved in it.

The Baghdad Eye, a tall Ferris wheel, was proposed for Baghdad in August 2008. At that time, three possible locations had been identified, but no estimates of cost or completion date were given.<ref name="msnbc.msn.com/id/26425911"></ref> In October 2008, it was reported that Al-Zawraa Park was expected to be the site, and a wheel was installed there in March 2011.

Iraq's Tourism Board is also seeking investors to develop a "romantic" island on the River Tigris in Baghdad that was once a popular honeymoon spot for newlywed Iraqis. The project would include a six-star hotel, spa, an 18-hole golf course and a country club. In addition, the go-ahead has been given to build numerous architecturally unique skyscrapers along the Tigris that would develop the city's financial centre in Kadhehemiah.

In October 2008, the Baghdad Metro resumed service. It connects the center to the southern neighborhood of Dora.
In May 2010, a new residential and commercial project nicknamed Baghdad Gate was announced. This project not only addresses the urgent need for new residential units in Baghdad but also acts as a real symbol of progress in the war torn city, as Baghdad has not seen projects of this scale for decades.

The Mustansiriya Madrasah was established in 1227 by the Abbasid Caliph al-Mustansir. The name was changed to Al-Mustansiriya University in 1963. The University of Baghdad is the largest university in Iraq and the second largest in the Arab world. Prior to the Gulf War, multiple international schools operated in Baghdad, including:


Baghdad has always played a significant role in the broader Arab cultural sphere, contributing several significant writers, musicians and visual artists. Famous Arab poets and singers such as Nizar Qabbani, Umm Kulthum, Fairuz, Salah Al-Hamdani, Ilham al-Madfai and others have performed for the city. The dialect of Arabic spoken in Baghdad today differs from that of other large urban centres in Iraq, having features more characteristic of nomadic Arabic dialects (Versteegh, "The Arabic Language"). It is possible that this was caused by the repopulating of the city with rural residents after the multiple sackings of the late Middle Ages. For poetry written about Baghdad, see Reuven Snir (ed.), "Baghdad: The City in Verse" (Harvard, 2013) Baghdad joined the UNESCO Creative Cities Network as a City of Literature in December 2015.

Some of the important cultural institutions in the city include the National Theater, which was looted during the 2003 invasion of Iraq, but efforts are underway to restore the theatre. The live theatre scene received a boost during the 1990s, when UN sanctions limited the import of foreign films. As many as 30 movie theatres were reported to have been converted to live stages, producing a wide range of comedies and dramatic productions. Institutions offering cultural education in Baghdad include The Music and Ballet School of Baghdad and the Institute of Fine Arts Baghdad. The Iraqi National Symphony Orchestra is a government funded symphony orchestra in Baghdad. The INSO plays primarily classical European music, as well as original compositions based on Iraqi and Arab instruments and music. Baghdad is also home to a number of museums which housed artifacts and relics of ancient civilization; many of these were stolen, and the museums looted, during the widespread chaos immediately after United States forces entered the city.

During the 2003 occupation of Iraq, AFN Iraq ("Freedom Radio") broadcast news and entertainment within Baghdad, among other locations. There is also a private radio station called "Dijlah" (named after the Arabic word for the Tigris River) that was created in 2004 as Iraq's first independent talk radio station. Radio Dijlah offices, in the Jamia neighborhood of Baghdad, have been attacked on several occasions.

Priceless collection of artifacts in the National Museum of Iraq was looted during the 2003 US-led invasion. Thousands of ancient manuscripts in the National Library were destroyed under Saddam's command and because of neglect by the occupying coalition forces.

Baghdad is home to some of the most successful football (soccer) teams in Iraq, the biggest being Al-Shorta (Police), Al-Quwa Al-Jawiya (Airforce club), Al-Zawra'a, and Talaba (Students). The largest stadium in Baghdad is Al-Shaab Stadium, which was opened in 1966. The city has also had a strong tradition of horse racing ever since World War I, known to Baghdadis simply as 'Races'. There are reports of pressures by the Islamists to stop this tradition due to the associated gambling.








</doc>
<doc id="4493" url="https://en.wikipedia.org/wiki?curid=4493" title="Outline of biology">
Outline of biology

Biology – The natural science that involves the study of life and living organisms, including their structure, function, growth, origin, evolution, distribution, and taxonomy.

Branch of biology – subdiscipline of biology, also referred to as a biological science (note that biology and all its branches are also life sciences).



Outline of ecology

Outline of evolution


Outline of cell biology

Outline of biochemistry

Outline of genetics








</doc>
<doc id="4495" url="https://en.wikipedia.org/wiki?curid=4495" title="British thermal unit">
British thermal unit

The British thermal unit (BTU or Btu) is a unit of heat; it is defined as the amount of heat required to raise the temperature of one pound of water by one degree Fahrenheit. It is also part of the United States customary units. Its counterpart in the metric system is the calorie, which is defined as the amount of heat required to raise the temperature of one gram of water by one degree Celsius. Heat is now known to be equivalent to energy, for which the SI unit is the joule; one BTU is about 1055 joules. While units of heat are often supplanted by energy units in scientific work, they are still used in some fields. For example, in the United States the price of natural gas is quoted in dollars per million BTUs.

A BTU was originally defined as the amount of heat required to raise the temperature of 1 avoirdupois pound of liquid water by 1 degree Fahrenheit at a constant pressure of one atmospherical unit. There are several different definitions of the BTU that are now known to differ slightly. This reflects the fact that the temperature change of a mass of water due to the addition of a specific amount of heat (calculated in energy units, usually joules) depends slightly upon the water's initial temperature. As seen in the table below, definitions of the BTU based on different water temperatures vary by up to 0.5%.

Units kBtu are used in building energy use tracking and heating system sizing. Energy Use Index (EUI) represents kBtu per square foot of conditioned floor area. "k" stands for 1,000.

The unit MBTU is used in natural gas and other industries to indicate 1,000 BTUs. However, there is an ambiguity in that the metric system (SI) uses the prefix "M" to indicate one million (1,000,000), and consequently "MMBtu" is often used to indicate one million BTUs.

Energy analysts accustomed to the metric "k" for 1,000 are more likely to use MBtu to represent one million, especially in documents where M represents one million in other energy or cost units, such as MW, MWh and $.

The unit "therm" is used to represent 100,000 (or 10) BTUs. A decatherm is 10 therms or one MBtu (million Btu). The unit "quad" is commonly used to represent one quadrillion (10) BTUs.

One Btu is approximately:

A Btu can be approximated as the heat produced by burning a single wooden kitchen match or as the amount of energy it takes to lift a weight .


When used as a unit of power for heating and cooling systems, Btu "per hour" (Btu/h) is the correct unit, though this is often abbreviated to just "Btu". "MBH"—thousands of Btus per hour—is also common.



The Btu should not be confused with the Board of Trade Unit (B.O.T.U.), an obsolete UK synonym for kilowatt hour ().

The Btu is often used to express the conversion-efficiency of heat into electrical energy in power plants. Figures are quoted in terms of the quantity of heat in Btu required to generate 1 kW⋅h of electrical energy. A typical coal-fired power plant works at , an efficiency of 32–33%.

The centigrade heat unit (CHU) is the amount of heat required to raise the temperature of one pound of water by one Celsius degree. It is equal to 1.8 BTU or 1899 joules. This unit was sometimes used in the United Kingdom as an alternative to BTU but is now obsolete.




</doc>
<doc id="4497" url="https://en.wikipedia.org/wiki?curid=4497" title="Bugatti">
Bugatti

Automobiles Ettore Bugatti was a French car manufacturer of high-performance automobiles, founded in 1909 in the then-German city of Molsheim, Alsace by the Italian-born industrial designer Ettore Bugatti. The cars were known for their design beauty and for their many race victories. Famous Bugattis include the Type 35 Grand Prix cars, the Type 41 "Royale", the Type 57 "Atlantic" and the Type 55 sports car.

The death of Ettore Bugatti in 1947 proved to be the end for the marque, and the death of his son Jean Bugatti in 1939 ensured there was not a successor to lead the factory. No more than about 8,000 cars were made. The company struggled financially, and released one last model in the 1950s, before eventually being purchased for its airplane parts business in 1963.

In 1987, an Italian entrepreneur bought the brand and revived it as a builder of limited production exclusive sports cars based in Modena. In 1998, the Volkswagen Group bought the rights to the Bugatti marque and set up a subsidiary based back in Molsheim, Alsace.

Founder Ettore Bugatti was born in Milan, Italy, and the automobile company that bears his name was founded in 1909 in Molsheim located in the Alsace region which was part of the German Empire from 1871 to 1919. The company was known both for the level of detail of its engineering in its automobiles, and for the artistic manner in which the designs were executed, given the artistic nature of Ettore's family (his father, Carlo Bugatti (1856–1940), was an important Art Nouveau furniture and jewelry designer).

During the war Ettore Bugatti was sent away, initially to Milan and later to Paris, but as soon as hostilities had been concluded he returned to his factory at Molsheim. Less than four months after the Versailles Treaty formalised the transfer of Alsace from Germany to France, Bugatti was able to obtain, at the last minute, a stand at the 15th Paris motor show in October 1919. He exhibited three light cars, all of them closely based on their pre-war equivalents, and each fitted with the same overhead camshaft 4-cylinder 1,368cc engine with four valves per cylinder. Smallest of the three was a "Type 13" with a racing body (constructed by Bugatti themselves) and using a chassis with a wheelbase. The others were a "Type 22" and a "Type 23" with wheelbases of respectively.

The company also enjoyed great success in early Grand Prix motor racing: in 1929 a privately entered Bugatti won the first ever Monaco Grand Prix. Racing success culminated with driver Jean-Pierre Wimille winning the 24 hours of Le Mans twice (in 1937 with Robert Benoist and 1939 with Pierre Veyron).

Bugatti cars were extremely successful in racing. The little Bugatti Type 10 swept the top four positions at its first race. The 1924 Bugatti Type 35 is one of the most successful racing cars. The Type 35 was developed by Bugatti with master engineer and racing driver Jean Chassagne who also drove it in the car's first ever Grand Prix in 1924 Lyon. Bugattis swept to victory in the Targa Florio for five years straight from 1925 through 1929. Louis Chiron held the most podiums in Bugatti cars, and the modern marque revival Bugatti Automobiles S.A.S. named the 1999 Bugatti 18/3 Chiron concept car in his honour. But it was the final racing success at Le Mans that is most remembered—Jean-Pierre Wimille and Pierre Veyron won the 1939 race with just one car and meagre resources.

In the 1930s, Ettore Bugatti got involved in the creation of a racer airplane, hoping to beat the Germans in the Deutsch de la Meurthe prize. This would be the Bugatti 100P, which never flew. It was designed by Belgian engineer Louis de Monge who had already applied Bugatti Brescia engines in his "Type 7.5" lifting body.

Ettore Bugatti also designed a successful motorised railcar, the "" (Autorail Bugatti).

The death of Ettore Bugatti's son, Jean Bugatti, on 11 August 1939 marked a turning point in the company's fortunes. Jean died while testing a Type 57 tank-bodied race car near the Molsheim factory.

World War II left the Molsheim factory in ruins and the company lost control of the property. During the war, Bugatti planned a new factory at Levallois, a northwestern suburb of Paris. After the war, Bugatti designed and planned to build a series of new cars, including the Type 73 road car and Type 73C single seat racing car, but in all Bugatti built only five Type 73 cars.

Development of a 375 cc supercharged car was stopped when Ettore Bugatti died on 21 August 1947. Following Ettore Bugatti's death, the business declined further and made its last appearance as a business in its own right at a Paris Motor Show in October 1952.

After a long decline, the original incarnation of Bugatti ceased operations in 1952.

Bugattis are noticeably focused on design. Engine blocks were hand scraped to ensure that the surfaces were so flat that gaskets were not required for sealing, many of the exposed surfaces of the engine compartment featured "guilloché" (engine turned) finishes on them, and safety wires had been threaded through almost every fastener in intricately laced patterns. Rather than bolt the springs to the axles as most manufacturers did, Bugatti's axles were forged such that the spring passed through a carefully sized opening in the axle, a much more elegant solution requiring fewer parts. He famously described his arch competitor Bentley's cars as "the world's fastest lorries" for focusing on durability. According to Bugatti, "weight was the enemy".

Relatives of Harold Carr found a rare 1937 Bugatti Type 57S Atalante when cataloguing the doctor's belongings after his death in 2009. Carr's Type 57S is notable because it was originally owned by British race car driver Earl Howe. Because much of the car's original equipment is intact, it can be restored without relying on replacement parts.

On 10 July 2009, a 1925 Bugatti Brescia Type 22 which had lain at the bottom of Lake Maggiore on the border of Switzerland and Italy for 75 years was recovered from the lake. The Mullin Museum in Oxnard, California bought it at auction for $351,343 at Bonham's Rétromobile sale in Paris in 2010.

The company attempted a comeback under Roland Bugatti in the mid-1950s with the mid-engined Type 251 race car. Designed with help from Gioacchino Colombo, the car failed to perform to expectations and the company's attempts at automobile production were halted.

In the 1960s, Virgil Exner designed a Bugatti as part of his "Revival Cars" project. A show version of this car was actually built by Ghia using the last Bugatti Type 101 chassis, and was shown at the 1965 Turin Motor Show. Finance was not forthcoming, and Exner then turned his attention to a revival of Stutz.

Bugatti continued manufacturing airplane parts and was sold to Hispano-Suiza, also a former auto maker turned aircraft supplier, in 1963. Snecma took over Hispano-Suiza in 1968. After acquiring Messier, Snecma merged Messier and Bugatti into Messier-Bugatti in 1977.

Italian entrepreneur Romano Artioli acquired the Bugatti brand in 1987, and established Bugatti Automobili S.p.A.. Artioli commissioned architect Giampaolo Benedini to design the factory which was built in Campogalliano, Modena, Italy. Construction of the plant began in 1988, alongside the development of the first model, and it was inaugurated two years later—in 1990. By 1989, the plans for the new Bugatti revival were presented by Paolo Stanzani and Marcello Gandini, designers of the Lamborghini Miura and Lamborghini Countach.

The first production vehicle was the Bugatti EB110 GT which featured a 3.5-litre, 5-valve per cylinder, quad-turbocharged 60° V12 engine, a six-speed gearbox, and four-wheel drive. Stanzani proposed an aluminium honeycomb chassis, which was used for all early prototypes. He and president Artioli clashed over engineering decisions so Stanzani left the project and Artioli sought Nicola Materazzi to replace him in June 1990. Materazzi, who had been the chief designer for the Ferrari 288 GTO and Ferrari F40 replaced the aluminium chassis with a carbon fibre one manufactured by Aerospatiale and also altered the torque distribution of the car from 40:60 to 27:73. He remained Director until late 1992.
Racing car designer Mauro Forghieri served as Bugatti's technical director from 1993 through 1994. On 27 August 1993, through his holding company, ACBN Holdings S.A. of Luxembourg, Romano Artioli purchased Lotus Cars from General Motors. Plans were made to list Bugatti shares on international stock exchanges.

Bugatti presented a prototype large saloon called the EB112 in 1993.

Perhaps the most famous Bugatti EB110 owner was seven-time Formula One World Champion racing driver Michael Schumacher who purchased an EB110 in 1994. Schumacher sold his EB110, which had been repaired after a severe 1994 crash, to Modena Motorsport, a Ferrari service and race preparation garage in Germany.

By the time the EB110 came to market, the North American and European economies were in recession. Poor economic conditions forced the company to fail and operations ceased in September 1995. A model specific to the US market called the "Bugatti America" was in the preparatory stages when the company ceased operations.

Bugatti's liquidators sold Lotus Cars to Proton of Malaysia. German firm Dauer Racing purchased the EB110 licence and remaining parts stock in 1997 in order to produce five more EB110 SS vehicles. These five SS versions of the EB110 were greatly refined by Dauer. The Campogalliano factory was sold to a furniture-making company, which became defunct prior to moving in, leaving the building unoccupied. After Dauer stopped producing cars in 2011, Toscana-Motors GmbH of Germany purchased the remaining parts stock from Dauer.

Ex vice-president Jean-Marc Borel and ex employees Federico Trombi, Gianni Sighinolfi and Nicola Materazzi established the B Engineering company and designed and built the Edonis using the chassis and engine from the Bugatti EB110 SS, but simplifying the turbocharging system and driveline (from 4WD to 2WD).

Volkswagen Group acquired the Bugatti brand in 1998. Bugatti Automobiles S.A.S. commissioned Giorgetto Giugiaro of ItalDesign to produce Bugatti Automobiles's first concept vehicle, the EB118, a coupé that debuted at the 1998 Paris Auto Show. The EB118 concept featured a , W-18 engine. After its Paris debut, the EB118 concept was shown again in 1999 at the Geneva Auto Show and the Tokyo Motor Show. Bugatti introduced its next concepts, the EB 218 at the 1999 Geneva Motor Show and the 18/3 Chiron at the 1999 Frankfurt Motor Show (IAA).

Bugatti Automobiles S.A.S. began assembling its first regular-production vehicle, the Bugatti Veyron 16.4 (the 1001 PS super car with an 8-litre W-16 engine with four turbochargers) in September 2005 at the Bugatti Molsheim, France assembly "studio". On 23 February 2015, Bugatti sold its last Veyron Grand Sport Vitesse, which was named La Finale.

The Bugatti Chiron is a mid-engined, two-seated sports car, designed by Achim Anscheidt, developed as the successor to the Bugatti Veyron. The Chiron was first revealed at the Geneva Motor Show on March 1, 2016.




</doc>
<doc id="4498" url="https://en.wikipedia.org/wiki?curid=4498" title="Benchmark">
Benchmark

Benchmark may refer to:





</doc>
<doc id="4499" url="https://en.wikipedia.org/wiki?curid=4499" title="Band">
Band

Band or BAND may refer to:











</doc>
<doc id="4501" url="https://en.wikipedia.org/wiki?curid=4501" title="Black Death">
Black Death

The Black Death (also known as the Pestilence, the Great Mortality, or the Plague) was the deadliest pandemic recorded in human history. The Black Death resulted in the deaths of up to <ref name="ABC/Reuters"></ref> people in Eurasia and North Africa, peaking in Europe from 1347 to 1351. Plague, the disease caused by the bacterium "Yersinia pestis", was the cause; "Y. pestis" infection most commonly results in bubonic plague, but can cause septicaemic or pneumonic plagues.

The Black Death was the beginning of the second plague pandemic. The plague created religious, social, and economic upheavals, with profound effects on the course of European history.

The Black Death most likely originated in Central Asia or East Asia, from where it travelled along the Silk Road, reaching Crimea by 1347. From there, it was most likely carried by fleas living on the black rats that travelled on Genoese merchant ships, spreading throughout the Mediterranean Basin and reaching Africa, Western Asia, and the rest of Europe via Constantinople, Sicily, and the Italian Peninsula. Current evidence indicates that once it came onshore, the Black Death was in large part spread by human fleas – which cause pneumonic plague – and the person-to-person contact via aerosols which pneumonic plague enables, thus explaining the very fast inland spread of the epidemic, which was faster than would be expected if the primary vector was rat fleas causing bubonic plague.

The Black Death was the second disaster affecting Europe during the Late Middle Ages (the first one being the Great Famine) and is estimated to have killed 30% to 60% of Europe's population. In total, the plague may have reduced the world population from an estimated 475 million to 350–375 million in the 14th century. There were further outbreaks throughout the Late Middle Ages, and with other contributing factors it took until 1500 for the European population to regain the levels of 1300. Outbreaks of the plague recurred at various locations around the world until the early 19th century.

European writers contemporary with the plague described the disease in Latin as or ; ; . In English prior to the 18th century, the event was called the "pestilence" or "great pestilence", "the plague" or the "great death". Subsequent to the pandemic "the "furste moreyn"" (first murrain) or "first pestilence" was applied, to distinguish the mid-14th century phenomenon from other infectious diseases and epidemics of plague. The 1347 pandemic plague was not referred to specifically as "black" in the 14th or 15th centuries in any European language, though the expression "black death" had occasionally been applied to fatal disease beforehand.

"Black death" was not used to describe the plague pandemic in English until the 1750s; the term is first attested in 1755, where it translated . This expression as a proper name for the pandemic had been popularised by Swedish and Danish chroniclers in the 15th and early 16th centuries, and in the 16th and 17th centuries was the transferred to other languages as a calque: , , and . Previously, most European languages had named the pandemic a variant or calque of the .

The phrase 'black death' – describing Death as black – is very old. Homer used it in the Odyssey to describe the monstrous Scylla, with her mouths "full of black Death" (). Seneca the Younger may have been the first to describe an epidemic as 'black death', () but only in reference to the acute lethality and dark prognosis of disease. The 12th–13th century French physician Gilles de Corbeil had already used "" to refer to a "pestilential fever" () in his work "On the Signs and Symptoms of Diseases" (). The phrase , was used in 1350 by Simon de Covino (or Couvin), a Belgian astronomer, in his poem "On the Judgement of the Sun at a Feast of Saturn" (), which attributes the plague to an astrological conjunction of Jupiter and Saturn. His use of the phrase is not connected unambiguously with the plague pandemic of 1347 and appears to refer to the fatal outcome of disease.

The historian Cardinal Francis Aidan Gasquet wrote about the Great Pestilence in 1893 and suggested that it had been "some form of the ordinary Eastern or bubonic plague". In 1908, Gasquet claimed that use of the name ' for the 14th-century epidemic first appeared in a 1631 book on Danish history by J. I. Pontanus: "Commonly and from its effects, they called it the black death" (').

Recent research has suggested plague first infected humans in Europe and Asia in the Late Neolithic-Early Bronze Age. Research in 2018 found evidence of "Yersinia pestis" in an ancient Swedish tomb, which may have been associated with the "Neolithic decline" around 3000 BCE, in which European populations fell significantly. This "Y. pestis" may have been different to more modern types, with bubonic plague transmissible by fleas first known from Bronze Age remains near Samara.

The symptoms of bubonic plague are first attested in a fragment of Rufus of Ephesus preserved by Oribasius; these ancient medical authorities suggest bubonic plague had appeared in the Roman Empire before the reign of Trajan, six centuries before arriving at Pelusium in the reign of Justinian I. In 2013, researchers confirmed earlier speculation that the cause of the Plague of Justinian (541–542 CE, with recurrences until 750) was "Y". "pestis". This is known as the First plague pandemic.

The 13th-century Mongol conquest of China caused a decline in farming and trading. Economic recovery had been observed at the beginning of the fourteenth century. In the 1330s, many natural disasters and epidemics led to widespread famine, starting in 1331, with the deadly plague pandemic arriving soon after. Other conditions, such as war, famine, and weather, contributed to the severity of the Black Death.

The most authoritative contemporary account is found in a report from the medical faculty in Paris to Philip VI of France. It blamed the heavens, in the form of a conjunction of three planets in 1345 that caused a "great pestilence in the air" (miasma theory).

Muslim religious scholars taught that the pandemic was a “martyrdom and mercy” from God, assuring the believer's place in paradise. For non-believers, it was a punishment. Some Muslim doctors cautioned against trying to prevent or treat a disease sent by God. Others adopted preventive measures and treatments for plague used by Europeans. These Muslim doctors also depended on the writings of the ancient Greeks.

Due to climate change in Asia, rodents began to flee the dried-out grasslands to more populated areas, spreading the disease. The plague disease, caused by the bacterium "Yersinia pestis", is enzootic (commonly present) in populations of fleas carried by ground rodents, including marmots, in various areas, including Central Asia, Kurdistan, Western Asia, North India, Uganda and the western United States.

"Y. pestis" was discovered by Alexandre Yersin, a pupil of Louis Pasteur, during an epidemic of bubonic plague in Hong Kong in 1894; Yersin also proved this bacillus was present in rodents and suggested the rat was the main vehicle of transmission. The mechanism by which "Y. pestis" is usually transmitted was established in 1898 by Paul-Louis Simond and was found to involve the bites of fleas whose midguts had become obstructed by replicating "Y. pestis" several days after feeding on an infected host. This blockage starves the fleas and drives them to aggressive feeding behaviour and attempts to clear the blockage by regurgitation, resulting in thousands of plague bacteria being flushed into the feeding site, infecting the host. The bubonic plague mechanism was also dependent on two populations of rodents: one resistant to the disease, which act as hosts, keeping the disease endemic, and a second that lack resistance. When the second population dies, the fleas move on to other hosts, including people, thus creating a human epidemic.

The importance of hygiene was recognised only in the nineteenth century with the development of the germ theory of disease; until then streets were commonly filthy, with live animals of all sorts around and human parasites abounding, facilitating the spread of transmissible disease.

The spread of disease was significantly more rampant in areas of poverty. Epidemics ravaged cities, and particularly children. Plague was easily spread by lice, unsanitary drinking water, armies, or by poor sanitation.

One early medical advance as a result of the Black Death was the establishment of the idea of quarantine in the city-state of Ragusa (modern Dubrovnik, Croatia) in 1377 after continuing outbreaks.

According to international medical geneticists led by Mark Achtman that analysed the global sequence variation of the bacterium, all three of the great waves of the pandemic had their bacterium "evolved in or near China". The analysis also found that "sylvatic cycles of disease depend on transmission by flea vectors" and "the likely origin of the plague in China has nothing to do with its people or crowded cities".

Nestorian graves dating to 1338–1339 near Issyk-Kul in Kyrgyzstan have inscriptions referring to plague, which has led many epidemiologists to think they mark the outbreak of the epidemic; from which it could easily have spread to China and India.

Epidemics, that may have included plague, killed an estimated 25 million across Asia during the fifteen years before it reached Constantinople in 1347.

The disease may have travelled along the Silk Road with Mongol armies and traders, or it could have arrived via ship. By the end of 1346, reports of plague had reached the seaports of Europe: "India was depopulated, Tartary, Mesopotamia, Syria, Armenia were covered with dead bodies".

Plague was reportedly first introduced to Europe via Genoese traders from their port city of Kaffa in the Crimea in 1347. During a protracted siege of the city, in 1345–1346 the Mongol Golden Horde army of Jani Beg, whose mainly Tatar troops were suffering from the disease, catapulted infected corpses over the city walls of Kaffa to infect the inhabitants, though it is more likely that infected rats travelled across the siege lines to spread the epidemic to the inhabitants. As the disease took hold, Genoese traders fled across the Black Sea to Constantinople, where the disease first arrived in Europe in summer 1347. The epidemic there killed the 13 year-old son of the Byzantine emperor, John VI Kantakouzenos, who wrote a description of the disease modelled on Thucydides's account of the 5th century BCE Plague of Athens, but noting the spread of the Black Death by ship between maritime cities. Nicephorus Gregoras also described in writing to Demetrios Kydones the rising death toll, the futility of medicine, and the panic of the citizens. The first outbreak in Constantinople lasted a year, but the disease recurred ten times before 1400.

Carried by twelve Genoese galleys, plague arrived by ship in Sicily in October 1347; the disease spread rapidly all over the island. Galleys from Kaffa reached Genoa and Venice in January 1348, but it was the outbreak in Pisa a few weeks later that was the entry point to northern Italy. Towards the end of January, one of the galleys expelled from Italy arrived in Marseilles.

From Italy, the disease spread northwest across Europe, striking France, Spain (which was hit due to the heat – the epidemic raged in the early weeks of July), Portugal and England by June 1348, then spread east and north through Germany, Scotland and Scandinavia from 1348 to 1350. It was introduced into Norway in 1349 when a ship landed at Askøy, then spread to Bjørgvin (modern Bergen) and Iceland. Finally, it spread to northwestern Russia in 1351. Plague was somewhat more uncommon in parts of Europe with less developed trade with their neighbours, including the majority of the Basque Country, isolated parts of Belgium and the Netherlands, and isolated Alpine villages throughout the continent.

According to some epidemiologists, periods of unfavourable weather decimated plague-infected rodent populations and forced their fleas onto alternative hosts, inducing plague outbreaks which often peaked in the hot summers of the Mediterranean, as well as during the cool autumn months of the southern Baltic states. Among many other culprits of plague contagiousness, malnutrition, even if distantly, also contributed to such an immense loss in European population, since it weakened immune systems.

The disease struck various regions in the Middle East and North Africa during the pandemic, leading to serious depopulation and permanent change in both economic and social structures. As infected rodents infected new rodents, the disease spread across the region, entering also from southern Russia.

By autumn 1347, plague had reached Alexandria in Egypt, transmitted by sea from Constantinople; according to a contemporary witness, from a single merchant ship carrying slaves. By late summer 1348 it reached Cairo, capital of the Mamluk Sultanate, cultural centre of the Islamic world, and the largest city in the Mediterranean Basin; the Bahriyya child sultan an-Nasir Hasan fled and more than a third of the 600,000 residents died. The Nile was choked with corpses despite Cairo having a medieval hospital, the late 13th century bimaristan of the Qalawun complex. The historian al-Maqrizi described the abundant work for grave-diggers and practitioners of funeral rites, and plague recurred in Cairo more than fifty times over the following century and half.

During 1347, the disease travelled eastward to Gaza by April; by July it had reached Damascus, and in October plague had broken out in Aleppo. That year, in the territory of modern Lebanon, Syria, Israel, and Palestine, the cities of Ashkelon, Acre, Jerusalem, Sidon, and Homs were all infected. In 1348–1349, the disease reached Antioch. The city's residents fled to the north, but most of them ended up dying during the journey. Within two years, plague had spread throughout the Islamic world, from Arabia across North Africa. The pandemic spread westwards from Alexandria along the African coast, while in April 1348 Tunis was infected by ship from Sicily. Tunis was then under attack by an army from Morocco; this army dispersed in 1348 and brought the contagion with them to Morocco, whose epidemic may also have been seeded from the Islamic city of Almería in al-Andalus.

Mecca became infected in 1348 by pilgrims performing the Hajj. In 1351 or 1352, the Rasulid sultan of the Yemen, al-Mujahid Ali, was released from Mamluk captivity in Egypt and carried plague with him on his return home. During 1348, records show the city of Mosul suffered a massive epidemic, and the city of Baghdad experienced a second round of the disease.

Symptoms of the disease include fever of , headaches, painful aching joints, nausea and vomiting, and a general feeling of malaise. Left untreated, of those that contract the bubonic plague, 80 percent die within eight days.

Contemporary accounts of the pandemic are varied and often imprecise. The most commonly noted symptom was the appearance of buboes (or "gavocciolos") in the groin, neck, and armpits, which oozed pus and bled when opened. Boccaccio's description:

This was followed by acute fever and vomiting of blood. Most victims died two to seven days after initial infection. Freckle-like spots and rashes, which could have been caused by flea-bites, were identified as another potential sign of plague.

Lodewijk Heyligen, whose master the Cardinal Colonna died of plague in 1348, noted a distinct form of the disease, pneumonic plague, that infected the lungs and led to respiratory problems. Symptoms include fever, cough, and blood-tinged sputum. As the disease progresses, sputum becomes free-flowing and bright red. Pneumonic plague has a mortality rate of 90 to 95 percent.

Septicaemic plague is the least common of the three forms, with a mortality rate near 100%. Symptoms are high fevers and purple skin patches (purpura due to disseminated intravascular coagulation). In cases of pneumonic and particularly septicaemic plague, the progress of the disease is so rapid that there would often be no time for the development of the enlarged lymph nodes that were noted as buboes.

There are no exact figures for the death toll; the rate varied widely by locality. In urban centres, the greater the population before the outbreak, the longer the duration of the period of abnormal mortality. It killed some people in Eurasia. The mortality rate of the Black Death in the 14th century was far greater than the worst 20th-century outbreaks of "Y. pestis" plague, which occurred in India and killed as much as 3% of the population of certain cities.
According to medieval historian Philip Daileader, it is likely that over four years, 45–50% of the European population died of plague. Norwegian historian Ole Benedictow suggests it could have been as much as 60% of the European population. In 1348, the disease spread so rapidly that before any physicians or government authorities had time to reflect upon its origins, about a third of the European population had already perished. In crowded cities, it was not uncommon for as much as 50% of the population to die. Half of Paris' population of 100,000 people died. In Italy, the population of Florence was reduced from 110,000–120,000 inhabitants in 1338 down to 50,000 in 1351. At least 60% of the population of Hamburg and Bremen perished, and a similar percentage of Londoners may have died from the disease as well, with a death toll of approximately 62,000 between 1346 and 1353. Florence's tax records suggest that 80% of the city's population died within four months in 1348. Before 1350, there were about 170,000 settlements in Germany, and this was reduced by nearly 40,000 by 1450. The disease bypassed some areas, with the most isolated areas being less vulnerable to contagion. Plague did not appear in Douai in Flanders until the turn of the 15th century, and the impact was less severe on the populations of Hainaut, Finland, northern Germany, and areas of Poland. Monks, nuns, and priests were especially hard-hit since they cared for victims of the Black Death.

The physician to the Avignon Papacy, Raimundo Chalmel de Vinario (), observed the decreasing mortality rate of successive outbreaks of plague in 1347–48, 1362, 1371, and 1382 in his 1382 treatise "On Epidemics" (). In the first outbreak, two thirds of the population contracted the illness and most patients died; in the next, half the population became ill but only some died; by the third, a tenth were affected and many survived; while by the fourth occurrence, only one in twenty people were sickened and most of them survived. By the 1380s in Europe, it predominantly affected children. The papal doctor recognised that bloodletting was ineffective (though he continued to prescribe bleeding for members of the Roman Curia, whom he disliked), and claimed that all true cases of plague were caused by astrological factors and were incurable; he himself was never able to effect a cure.

The most widely accepted estimate for the Middle East, including Iraq, Iran, and Syria, during this time, is for a death toll of about a third of the population. The Black Death killed about 40% of Egypt's population. In Cairo, with a population numbering as many as 600,000, and possibly the largest city west of China, between one third and 40% of the inhabitants died inside of eight months.

Children were hit the hardest because many diseases, such as typhus and congenital syphilis, target the immune system, leaving young children without a fighting chance. Children in city dwellings were more affected by the spread of disease than the children of the wealthy.

Italian chronicler Agnolo di Tura recorded his experience from Siena, where plague arrived in May 1348:
With such a large population decline from the pandemic, wages soared in response to a labour shortage. On the other hand, in the quarter century after the Black Death in England, it is clear many labourers, artisans, and craftsmen, those living from money-wages alone, did suffer a reduction in real incomes owing to rampant inflation. Landowners were also pushed to substitute monetary rents for labour services in an effort to keep tenants.

Some historians believe the innumerable deaths brought on by the pandemic cooled the climate by freeing up land and triggering reforestation. This may have led to the Little Ice Age.

Renewed religious fervour and fanaticism bloomed in the wake of the Black Death. Some Europeans targeted "various groups such as Jews, friars, foreigners, beggars, pilgrims", lepers, and Romani, blaming them for the crisis. Lepers, and others with skin diseases such as acne or psoriasis, were killed throughout Europe.

Because 14th-century healers and governments were at a loss to explain or stop the disease, Europeans turned to astrological forces, earthquakes, and the poisoning of wells by Jews as possible reasons for outbreaks. Many believed the epidemic was a punishment by God for their sins, and could be relieved by winning God's forgiveness.

There were many attacks against Jewish communities. In the Strasbourg massacre of February 1349, about 2,000 Jews were murdered. In August 1349, the Jewish communities in Mainz and Cologne were annihilated. By 1351, 60 major and 150 smaller Jewish communities had been destroyed. During this period many Jews relocated to Poland, where they received a warm welcome from King Casimir the Great.

One theory that has been advanced is that the devastation in Florence caused by the Black Death, which hit Europe between 1348 and 1350, resulted in a shift in the world view of people in 14th-century Italy and led to the Renaissance. Italy was particularly badly hit by the pandemic, and it has been speculated that the resulting familiarity with death caused thinkers to dwell more on their lives on Earth, rather than on spirituality and the afterlife. It has also been argued that the Black Death prompted a new wave of piety, manifested in the sponsorship of religious works of art. However, this does not fully explain why the Renaissance occurred specifically in Italy in the 14th century. The Black Death was a pandemic that affected all of Europe in the ways described, not only Italy. The Renaissance's emergence in Italy was most likely the result of the complex interaction of the above factors, in combination with an influx of Greek scholars following the fall of the Byzantine Empire.

As a result of the decimation in the populace the value of the working class increased, and commoners came to enjoy more freedom. To answer the increased need for labour, workers travelled in search of the most favourable position economically.

Cairo's population, partly owing to the numerous plague epidemics, was in the early 18th century half of what it was in 1347. The populations of some Italian cities, notably Florence, did not regain their pre-14th century size until the 19th century.

The demographic decline due to the pandemic had economic consequences: the prices of food dropped and land values declined by 30–40% in most parts of Europe between 1350 and 1400. Landholders faced a great loss, but for ordinary men and women it was a windfall. The survivors of the pandemic found not only that the prices of food were lower but also that lands were more abundant, and many of them inherited property from their dead relatives, and this probably destabilised feudalism.

Definitive confirmation of the role of "Y. pestis" arrived in 2010 with a publication in "PLOS Pathogens" by Haensch et al. They assessed the presence of DNA/RNA with polymerase chain reaction (PCR) techniques for "Y. pestis" from the tooth sockets in human skeletons from mass graves in northern, central and southern Europe that were associated archaeologically with the Black Death and subsequent resurgences. The authors concluded that this new research, together with prior analyses from the south of France and Germany, "ends the debate about the cause of the Black Death, and unambiguously demonstrates that "Y. pestis" was the causative agent of the epidemic plague that devastated Europe during the Middle Ages". In 2011, these results were further confirmed with genetic evidence derived from Black Death victims in the East Smithfield burial site in England. Schuenemann et al. concluded in 2011 "that the Black Death in medieval Europe was caused by a variant of "Y. pestis" that may no longer exist."

Later in 2011, Bos et al. reported in "Nature" the first draft genome of "Y. pestis" from plague victims from the same East Smithfield cemetery and indicated that the strain that caused the Black Death is ancestral to most modern strains of "Y. pestis".

Since this time, further genomic papers have further confirmed the phylogenetic placement of the "Y. pestis" strain responsible for the Black Death as both the ancestor of later plague epidemics including the third plague pandemic and as the descendant of the strain responsible for the Plague of Justinian. In addition, plague genomes from significantly earlier in prehistory have been recovered.

DNA taken from 25 skeletons from 14th century London have shown plague is a strain of "Y. pestis" almost identical to that which hit Madagascar in 2013.

It is recognised that an epidemiological account of plague is as important as an identification of symptoms, but researchers are hampered by the lack of reliable statistics from this period. Most work has been done on the spread of the disease in England, and even estimates of overall population at the start vary by over 100% as no census was undertaken in England between the time of publication of the Domesday Book of 1086 and the poll tax of the year 1377. Estimates of plague victims are usually extrapolated from figures for the clergy.

Mathematical modelling is used to match the spreading patterns and the means of transmission. A research in 2018 challenged the popular hypothesis that "infected rats died, their flea parasites could have jumped from the recently dead rat hosts to humans". It suggested an alternative model in which "the disease was spread from human fleas and body lice to other people". The second model claims to better fit the trends of death toll because the rat-flea-human hypothesis would have produced a delayed but very high spike in deaths, which contradict historical death data.

Walløe complains that all of these authors "take it for granted that Simond's infection model, black rat → rat flea → human, which was developed to explain the spread of plague in India, is the only way an epidemic of "Yersinia pestis" infection could spread", whilst pointing to several other possibilities. Similarly, Green has argued that greater attention is needed to the range of (especially non-commensal) animals that might be involved in the transmission of plague.

Archaeologist Barney Sloane has argued that there is insufficient evidence of the extinction of numerous rats in the archaeological record of the medieval waterfront in London and that the disease spread too quickly to support the thesis that "Y. pestis" was spread from fleas on rats; he argues that transmission must have been person to person. This theory is supported by research in 2018 which suggested transmission was more likely by body lice and human fleas during the second plague pandemic.

Although academic debate continues, no single alternative solution has achieved widespread acceptance. Many scholars arguing for "Y. pestis" as the major agent of the pandemic suggest that its extent and symptoms can be explained by a combination of bubonic plague with other diseases, including typhus, smallpox and respiratory infections. In addition to the bubonic infection, others point to additional septicaemic (a type of "blood poisoning") and pneumonic (an airborne plague that attacks the lungs before the rest of the body) forms of plague, which lengthen the duration of outbreaks throughout the seasons and help account for its high mortality rate and additional recorded symptoms. In 2014, Public Health England announced the results of an examination of 25 bodies exhumed in the Clerkenwell area of London, as well as of wills registered in London during the period, which supported the pneumonic hypothesis. Currently, while osteoarcheologists have conclusively verified the presence of "Y. pestis" bacteria in burial sites across northern Europe through examination of bones and dental pulp, no other epidemic pathogen has been discovered to bolster the alternative explanations. In the words of one researcher: "Finally, plague is plague."

The plague repeatedly returned to haunt Europe and the Mediterranean throughout the 14th to 17th centuries. According to Biraben, the plague was present somewhere in Europe in every year between 1346 and 1671. (Note that some researchers have cautions about the uncritical use of Biraben's data.) The second pandemic was particularly widespread in the following years: 1360–63; 1374; 1400; 1438–39; 1456–57; 1464–66; 1481–85; 1500–03; 1518–31; 1544–48; 1563–66; 1573–88; 1596–99; 1602–11; 1623–40; 1644–54; and 1664–67. Subsequent outbreaks, though severe, marked the retreat from most of Europe (18th century) and northern Africa (19th century). The historian George Sussman argued that the plague had not occurred in East Africa until the 1900s. However, other sources suggest that the Second pandemic did indeed reach Sub-Saharan Africa.

According to historian Geoffrey Parker, "France alone lost almost a million people to the plague in the epidemic of 1628–31." In the first half of the 17th century, a plague claimed some 1.7 million victims in Italy. More than 1.25 million deaths resulted from the extreme incidence of plague in 17th-century Spain.

The Black Death ravaged much of the Islamic world. Plague was present in at least one location in the Islamic world virtually every year between 1500 and 1850. Plague repeatedly struck the cities of North Africa. Algiers lost 30,000–50,000 inhabitants to it in 1620–21, and again in 1654–57, 1665, 1691, and 1740–42. Cairo suffered more than fifty plague epidemics within 150 years from the plague's first appearance, with the final outbreak of the second pandemic there in the 1840s. Plague remained a major event in Ottoman society until the second quarter of the 19th century. Between 1701 and 1750, thirty-seven larger and smaller epidemics were recorded in Constantinople, and an additional thirty-one between 1751 and 1800. Baghdad has suffered severely from visitations of the plague, and sometimes two-thirds of its population has been wiped out.

The third plague pandemic (1855–1859) started in China in the mid-19th century, spreading to all inhabited continents and killing 10 million people in India alone. Twelve plague outbreaks in Australia between 1900 and 1925 resulted in well over 1,000 deaths, chiefly in Sydney. This led to the establishment of a Public Health Department there which undertook some leading-edge research on plague transmission from rat fleas to humans via the bacillus "Yersinia pestis".

"Yersinia pestis" is also responsible for an epidemic that began in southern China in 1865, eventually spreading to India. The investigation of the pathogen that caused the 19th-century plague was begun by teams of scientists who visited Hong Kong in 1894, among whom was the French-Swiss bacteriologist Alexandre Yersin, after whom the pathogen was named.

The first North American plague epidemic was the San Francisco plague of 1900–1904, followed by another outbreak in 1907–1908.

Modern treatment methods include insecticides, the use of antibiotics, and a plague vaccine. It is feared that the plague bacterium could develop drug resistance and again become a major health threat. One case of a drug-resistant form of the bacterium was found in Madagascar in 1995. A further outbreak in Madagascar was reported in November 2014. In October 2017 the deadliest outbreak of the plague in modern times hit Madagascar, killing 170 people and infecting thousands.

An estimate of the case fatality rate for the modern bubonic plague, following the introduction of antibiotics, is 11%, although it may be higher in underdeveloped regions.



Informational notes
Citations
Bibliography

Further reading


</doc>
<doc id="4502" url="https://en.wikipedia.org/wiki?curid=4502" title="Biotechnology">
Biotechnology

Biotechnology is a broad area of biology, involving the use of living systems and organisms to develop or make products. Depending on the tools and applications, it often overlaps with related scientific fields. In the late 20th and early 21st centuries, biotechnology has expanded to include new and diverse sciences, such as genomics, recombinant gene techniques, applied immunology, and development of pharmaceutical therapies and diagnostic tests. The term "Biotechnology" was first used by "Karl Ereky" in 1919, meaning the production of products from raw materials with the aid of living organisms.

The wide concept of "biotech" or "biotechnology" encompasses a wide range of procedures for modifying living organisms according to human purposes, going back to domestication of animals, cultivation of the plants, and "improvements" to these through breeding programs that employ artificial selection and hybridization. Modern usage also includes genetic engineering as well as cell and tissue culture technologies. The American Chemical Society defines biotechnology as the application of biological organisms, systems, or processes by various industries to learning about the science of life and the improvement of the value of materials and organisms such as pharmaceuticals, crops, and livestock. Per the European Federation of Biotechnology, biotechnology is the integration of natural science and organisms, cells, parts thereof, and molecular analogues for products and services. Biotechnology is based on the basic biological sciences (e.g. molecular biology, biochemistry, cell biology, embryology, genetics, microbiology) and conversely provides methods to support and perform basic research in biology.

Biotechnology is the research and development in the laboratory using bioinformatics for exploration, extraction, exploitation and production from any living organisms and any source of biomass by means of biochemical engineering where high value-added products could be planned (reproduced by biosynthesis, for example), forecasted, formulated, developed, manufactured, and marketed for the purpose of sustainable operations (for the return from bottomless initial investment on R & D) and gaining durable patents rights (for exclusives rights for sales, and prior to this to receive national and international approval from the results on animal experiment and human experiment, especially on the pharmaceutical branch of biotechnology to prevent any undetected side-effects or safety concerns by using the products). The utilization of biological processes, organisms or systems to produce products that are anticipated to improve human lives is termed biotechnology.

By contrast, bioengineering is generally thought of as a related field that more heavily emphasizes higher systems approaches (not necessarily the altering or using of biological materials "directly") for interfacing with and utilizing living things. Bioengineering is the application of the principles of engineering and natural sciences to tissues, cells and molecules. This can be considered as the use of knowledge from working with and manipulating biology to achieve a result that can improve functions in plants and animals. Relatedly, biomedical engineering is an overlapping field that often draws upon and applies "biotechnology" (by various definitions), especially in certain sub-fields of biomedical or chemical engineering such as tissue engineering, biopharmaceutical engineering, and genetic engineering.

Although not normally what first comes to mind, many forms of human-derived agriculture clearly fit the broad definition of "'utilizing a biotechnological system to make products". Indeed, the cultivation of plants may be viewed as the earliest biotechnological enterprise.

Agriculture has been theorized to have become the dominant way of producing food since the Neolithic Revolution. Through early biotechnology, the earliest farmers selected and bred the best suited crops, having the highest yields, to produce enough food to support a growing population. As crops and fields became increasingly large and difficult to maintain, it was discovered that specific organisms and their by-products could effectively fertilize, restore nitrogen, and control pests. Throughout the history of agriculture, farmers have inadvertently altered the genetics of their crops through introducing them to new environments and breeding them with other plants — one of the first forms of biotechnology.

These processes also were included in early fermentation of beer. These processes were introduced in early Mesopotamia, Egypt, China and India, and still use the same basic biological methods. In brewing, malted grains (containing enzymes) convert starch from grains into sugar and then adding specific yeasts to produce beer. In this process, carbohydrates in the grains broke down into alcohols, such as ethanol. Later, other cultures produced the process of lactic acid fermentation, which produced other preserved foods, such as soy sauce. Fermentation was also used in this time period to produce leavened bread. Although the process of fermentation was not fully understood until Louis Pasteur's work in 1857, it is still the first use of biotechnology to convert a food source into another form.

Before the time of Charles Darwin's work and life, animal and plant scientists had already used selective breeding. Darwin added to that body of work with his scientific observations about the ability of science to change species. These accounts contributed to Darwin's theory of natural selection.

For thousands of years, humans have used selective breeding to improve the production of crops and livestock to use them for food. In selective breeding, organisms with desirable characteristics are mated to produce offspring with the same characteristics. For example, this technique was used with corn to produce the largest and sweetest crops.

In the early twentieth century scientists gained a greater understanding of microbiology and explored ways of manufacturing specific products. In 1917, Chaim Weizmann first used a pure microbiological culture in an industrial process, that of manufacturing corn starch using "Clostridium acetobutylicum," to produce acetone, which the United Kingdom desperately needed to manufacture explosives during World War I.

Biotechnology has also led to the development of antibiotics. In 1928, Alexander Fleming discovered the mold "Penicillium". His work led to the purification of the antibiotic compound formed by the mold by Howard Florey, Ernst Boris Chain and Norman Heatley – to form what we today know as penicillin. In 1940, penicillin became available for medicinal use to treat bacterial infections in humans.

The field of modern biotechnology is generally thought of as having been born in 1971 when Paul Berg's (Stanford) experiments in gene splicing had early success. Herbert W. Boyer (Univ. Calif. at San Francisco) and Stanley N. Cohen (Stanford) significantly advanced the new technology in 1972 by transferring genetic material into a bacterium, such that the imported material would be reproduced. The commercial viability of a biotechnology industry was significantly expanded on June 16, 1980, when the United States Supreme Court ruled that a genetically modified microorganism could be patented in the case of "Diamond v. Chakrabarty". Indian-born Ananda Chakrabarty, working for General Electric, had modified a bacterium (of the genus "Pseudomonas") capable of breaking down crude oil, which he proposed to use in treating oil spills. (Chakrabarty's work did not involve gene manipulation but rather the transfer of entire organelles between strains of the "Pseudomonas" bacterium.

The MOSFET (metal-oxide-semiconductor field-effect transistor) was invented by Mohamed M. Atalla and Dawon Kahng in 1959. Two years later, Leland C. Clark and Champ Lyons invented the first biosensor in 1962. Biosensor MOSFETs were later developed, and they have since been widely used to measure physical, chemical, biological and environmental parameters. The first BioFET was the ion-sensitive field-effect transistor (ISFET), invented by Piet Bergveld in 1970. It is a special type of MOSFET, where the metal gate is replaced by an ion-sensitive membrane, electrolyte solution and reference electrode. The ISFET is widely used in biomedical applications, such as the detection of DNA hybridization, biomarker detection from blood, antibody detection, glucose measurement, pH sensing, and genetic technology.

By the mid-1980s, other BioFETs had been developed, including the gas sensor FET (GASFET), pressure sensor FET (PRESSFET), chemical field-effect transistor (ChemFET), reference ISFET (REFET), enzyme-modified FET (ENFET) and immunologically modified FET (IMFET). By the early 2000s, BioFETs such as the DNA field-effect transistor (DNAFET), gene-modified FET (GenFET) and cell-potential BioFET (CPFET) had been developed.

A factor influencing the biotechnology sector's success is improved intellectual property rights legislation—and enforcement—worldwide, as well as strengthened demand for medical and pharmaceutical products to cope with an ageing, and ailing, U.S. population.

Rising demand for biofuels is expected to be good news for the biotechnology sector, with the Department of Energy estimating ethanol usage could reduce U.S. petroleum-derived fuel consumption by up to 30% by 2030. The biotechnology sector has allowed the U.S. farming industry to rapidly increase its supply of corn and soybeans—the main inputs into biofuels—by developing genetically modified seeds that resist pests and drought. By increasing farm productivity, biotechnology boosts biofuel production.

Biotechnology has applications in four major industrial areas, including health care (medical), crop production and agriculture, non-food (industrial) uses of crops and other products (e.g. biodegradable plastics, vegetable oil, biofuels), and environmental uses.

For example, one application of biotechnology is the directed use of microorganisms for the manufacture of organic products (examples include beer and milk products). Another example is using naturally present bacteria by the mining industry in bioleaching. Biotechnology is also used to recycle, treat waste, clean up sites contaminated by industrial activities (bioremediation), and also to produce biological weapons.

A series of derived terms have been coined to identify several branches of biotechnology, for example:

In medicine, modern biotechnology has many applications in areas such as pharmaceutical drug discoveries and production, pharmacogenomics, and genetic testing (or genetic screening).
Pharmacogenomics (a combination of pharmacology and genomics) is the technology that analyses how genetic makeup affects an individual's response to drugs. Researchers in the field investigate the influence of genetic variation on drug responses in patients by correlating gene expression or single-nucleotide polymorphisms with a drug's efficacy or toxicity. The purpose of pharmacogenomics is to develop rational means to optimize drug therapy, with respect to the patients' genotype, to ensure maximum efficacy with minimal adverse effects. Such approaches promise the advent of "personalized medicine"; in which drugs and drug combinations are optimized for each individual's unique genetic makeup.

Biotechnology has contributed to the discovery and manufacturing of traditional small molecule pharmaceutical drugs as well as drugs that are the product of biotechnology – biopharmaceutics. Modern biotechnology can be used to manufacture existing medicines relatively easily and cheaply. The first genetically engineered products were medicines designed to treat human diseases. To cite one example, in 1978 Genentech developed synthetic humanized insulin by joining its gene with a plasmid vector inserted into the bacterium "Escherichia coli". Insulin, widely used for the treatment of diabetes, was previously extracted from the pancreas of abattoir animals (cattle or pigs). The genetically engineered bacteria are able to produce large quantities of synthetic human insulin at relatively low cost. Biotechnology has also enabled emerging therapeutics like gene therapy. The application of biotechnology to basic science (for example through the Human Genome Project) has also dramatically improved our understanding of biology and as our scientific knowledge of normal and disease biology has increased, our ability to develop new medicines to treat previously untreatable diseases has increased as well.

Genetic testing allows the genetic diagnosis of vulnerabilities to inherited diseases, and can also be used to determine a child's parentage (genetic mother and father) or in general a person's ancestry. In addition to studying chromosomes to the level of individual genes, genetic testing in a broader sense includes biochemical tests for the possible presence of genetic diseases, or mutant forms of genes associated with increased risk of developing genetic disorders. Genetic testing identifies changes in chromosomes, genes, or proteins. Most of the time, testing is used to find changes that are associated with inherited disorders. The results of a genetic test can confirm or rule out a suspected genetic condition or help determine a person's chance of developing or passing on a genetic disorder. As of 2011 several hundred genetic tests were in use. Since genetic testing may open up ethical or psychological problems, genetic testing is often accompanied by genetic counseling.

Genetically modified crops ("GM crops", or "biotech crops") are plants used in agriculture, the DNA of which has been modified with genetic engineering techniques. In most cases, the main aim is to introduce a new trait that does not occur naturally in the species. Biotechnology firms can contribute to future food security by improving the nutrition and viability of urban agriculture. Furthermore, the protection of intellectual property rights encourages private sector investment in agrobiotechnology. For example, in Illinois FARM Illinois (Food and Agriculture RoadMap for Illinois) is an initiative to develop and coordinate farmers, industry, research institutions, government, and nonprofits in pursuit of food and agriculture innovation. In addition, the Illinois Biotechnology Industry Organization (iBIO) is a life sciences industry association with more than 500 life sciences companies, universities, academic institutions, service providers and others as members. The association describes its members as "dedicated to making Illinois and the surrounding Midwest one of the world’s top life sciences centers."

Examples in food crops include resistance to certain pests, diseases, stressful environmental conditions, resistance to chemical treatments (e.g. resistance to a herbicide), reduction of spoilage, or improving the nutrient profile of the crop. Examples in non-food crops include production of pharmaceutical agents, biofuels, and other industrially useful goods, as well as for bioremediation.

Farmers have widely adopted GM technology. Between 1996 and 2011, the total surface area of land cultivated with GM crops had increased by a factor of 94, from to 1,600,000 km (395 million acres). 10% of the world's crop lands were planted with GM crops in 2010. As of 2011, 11 different transgenic crops were grown commercially on 395 million acres (160 million hectares) in 29 countries such as the US, Brazil, Argentina, India, Canada, China, Paraguay, Pakistan, South Africa, Uruguay, Bolivia, Australia, Philippines, Myanmar, Burkina Faso, Mexico and Spain.

Genetically modified foods are foods produced from organisms that have had specific changes introduced into their DNA with the methods of genetic engineering. These techniques have allowed for the introduction of new crop traits as well as a far greater control over a food's genetic structure than previously afforded by methods such as selective breeding and mutation breeding. Commercial sale of genetically modified foods began in 1994, when Calgene first marketed its Flavr Savr delayed ripening tomato. To date most genetic modification of foods have primarily focused on cash crops in high demand by farmers such as soybean, corn, canola, and cotton seed oil. These have been engineered for resistance to pathogens and herbicides and better nutrient profiles. GM livestock have also been experimentally developed; in November 2013 none were available on the market, but in 2015 the FDA approved the first GM salmon for commercial production and consumption.

There is a scientific consensus that currently available food derived from GM crops poses no greater risk to human health than conventional food, but that each GM food needs to be tested on a case-by-case basis before introduction. Nonetheless, members of the public are much less likely than scientists to perceive GM foods as safe. The legal and regulatory status of GM foods varies by country, with some nations banning or restricting them, and others permitting them with widely differing degrees of regulation.

GM crops also provide a number of ecological benefits, if not used in excess. However, opponents have objected to GM crops per se on several grounds, including environmental concerns, whether food produced from GM crops is safe, whether GM crops are needed to address the world's food needs, and economic concerns raised by the fact these organisms are subject to intellectual property law.
Industrial biotechnology (known mainly in Europe as white biotechnology) is the application of biotechnology for industrial purposes, including industrial fermentation. It includes the practice of using cells such as microorganisms, or components of cells like enzymes, to generate industrially useful products in sectors such as chemicals, food and feed, detergents, paper and pulp, textiles and biofuels. In the current decades, significant progress has been done in creating genetically modified organisms (GMOs) that enhance the diversity of applications and economical viability of industrial biotechnology. By using renewable raw materials to produce a variety of chemicals and fuels, industrial biotechnology is actively advancing towards lowering greenhouse gas emissions and moving away from a petrochemical-based economy.

The environment can be affected by biotechnologies, both positively and adversely. Vallero and others have argued that the difference between beneficial biotechnology (e.g.bioremediation is to clean up an oil spill or hazard chemical leak) versus the adverse effects stemming from biotechnological enterprises (e.g. flow of genetic material from transgenic organisms into wild strains) can be seen as applications and implications, respectively. Cleaning up environmental wastes is an example of an application of environmental biotechnology; whereas loss of biodiversity or loss of containment of a harmful microbe are examples of environmental implications of biotechnology.

The regulation of genetic engineering concerns approaches taken by governments to assess and manage the risks associated with the use of genetic engineering technology, and the development and release of genetically modified organisms (GMO), including genetically modified crops and genetically modified fish. There are differences in the regulation of GMOs between countries, with some of the most marked differences occurring between the US and Europe. Regulation varies in a given country depending on the intended use of the products of the genetic engineering. For example, a crop not intended for food use is generally not reviewed by authorities responsible for food safety. The European Union differentiates between approval for cultivation within the EU and approval for import and processing. While only a few GMOs have been approved for cultivation in the EU a number of GMOs have been approved for import and processing. The cultivation of GMOs has triggered a debate about coexistence of GM and non GM crops. Depending on the coexistence regulations, incentives for cultivation of GM crops differ.

In 1988, after prompting from the United States Congress, the National Institute of General Medical Sciences (National Institutes of Health) (NIGMS) instituted a funding mechanism for biotechnology training. Universities nationwide compete for these funds to establish Biotechnology Training Programs (BTPs). Each successful application is generally funded for five years then must be competitively renewed. Graduate students in turn compete for acceptance into a BTP; if accepted, then stipend, tuition and health insurance support is provided for two or three years during the course of their Ph.D. thesis work. Nineteen institutions offer NIGMS supported BTPs. Biotechnology training is also offered at the undergraduate level and in community colleges.



</doc>
<doc id="4503" url="https://en.wikipedia.org/wiki?curid=4503" title="Battle of Poitiers">
Battle of Poitiers

The Battle of Poitiers was a major English victory in the Hundred Years' War. It was fought on 19 September 1356 in Nouaillé, near the city of Poitiers in Aquitaine, western France. Edward, the Black Prince, led an army of English, Welsh, Breton and Gascon troops, many of them veterans of the Battle of Crécy. They were attacked by a larger French force led by King John II of France, which included allied Scottish forces. The French were heavily defeated; an English counter-attack captured King John, along with his youngest son, and much of the French nobility who were present.

The effect of the defeat on France was catastrophic, leaving Dauphin Charles to rule the country. Charles faced populist revolts across the kingdom in the wake of the battle, which had destroyed the prestige of the French upper-class. The Edwardian phase of the war ended four years later in 1360, on favourable terms for England.

Poitiers was the second major English victory of the Hundred Years' War, coming a decade after the Battle of Crécy and about half a century before the Battle of Agincourt. The town and battle were often referred to as "Poictiers" in contemporaneous recordings, a name commemorated by several warships of the Royal Navy.

Following the death of Charles IV of France in 1328, Philip, Count of Valois, had been chosen as his successor and crowned King Philip VI of France, superseding his closest male relative and legal successor, Edward III of England. Edward had been reluctant to pay homage to Philip in his role as Duke of Aquitaine, resulting in Philip's confiscation of those lands in 1337, an act which provoked war between the two nations. Three years later, Edward declared himself King of France. The war had begun well for the English. They had achieved naval domination early in the conflict at the Battle of Sluys in 1340, devastated the south west of France during the Gascon campaign of 1345 and Lancaster's chevauchée the following year, inflicted a severe defeat on the French army at Crécy in 1346, and captured Calais in 1347.

In the late 1340s and early 1350s, the Black Death had devastated the population of Western Europe, even claiming Philip's wife, Queen Joan, as well as one of Edward's daughters, also named Joan; due to the disruption caused by the plague, all significant military campaigning was brought to a halt. Philip himself died in 1350, and was succeeded by his son, who was crowned King John II. In 1355, Edward III laid out plans for a second major campaign. His eldest son, Edward, the Black Prince, now an experienced soldier following the Crécy campaign, landed at Bordeaux in Aquitaine, leading his army on a march through southern France to Carcassonne. Unable to take the heavily fortified settlement, Edward withdrew back to Bordeaux. In early 1356, the Duke of Lancaster led an army through Normandy, while Edward led his army on a great chevauchée from Bordeaux on 8 August 1356.

Edward's forces met little resistance, sacking numerous settlements, until they reached the Loire river at Tours. They were unable to take the castle or burn the town due to a heavy rainstorm. This delay allowed King John to attempt to pin down and destroy Edward's army. John, who had been besieging Breteuil in Normandy, organised the bulk of his army at Chartres to the north of Tours. In order to increase the speed of his army's march, he dismissed between 15,000 and 20,000 of his lower quality infantry, just as Edward turned back to Bordeaux. The French rode hard and cut in front of the English army, crossing the bridge over the Vienne at Chauvigny. Learning of this, the Black Prince quickly moved his army south. Historians disagree over whether the outnumbered English commander was seeking battle or trying to avoid it. In any case, after preliminary manoeuvres and failed negotiations for a truce, the two armies faced-off, both ready for battle, near Poitiers on Monday, 19 September 1356.

Edward arrayed his army in a defensive posture among the hedges and orchards of the area, in front of the forest of Nouaillé. He deployed his front line of longbowmen behind a particularly prominent thick hedge, through which the road ran at right angles. The Earl of Douglas, commanding the Scottish division in the French army, advised King John that the attack should be delivered on foot, with horses being particularly vulnerable to English arrows. John heeded this advice, his army leaving its baggage behind and forming up on foot in front of the English. The English gained vantage points on the natural high ground in order for their longbowmen to have an advantage over the heavily armoured French troops.

The English army was led by Edward, the Black Prince, and composed primarily of English and Welsh troops, though there was a large contingent of Gascon and Breton soldiers with the army. Edward's army consisted of approximately 2,000 longbowmen, 3,000 men-at-arms, and a force of 1,000 Gascon infantry.

Like the earlier engagement at Crécy, the power of the English army lay in the longbow, a tall, thick self-bow made of yew. Longbows had demonstrated their effectiveness against massed infantry and cavalry in several battles, such as Falkirk in 1298, Halidon Hill in 1333, and Crécy in 1346. Poitiers was the second of three major English victories of the Hundred Years' War attributed to the longbow, though its effectiveness against armoured French knights and men-at-arms has been disputed.

Geoffrey the Baker wrote that the English archers under the Earl of Salisbury "made their arrows prevail over the [French] knights' armour", but the bowmen on the other flank, under Warwick, were initially ineffective against the mounted French men-at-arms who enjoyed the double protection of steel plate armour and large leather shields. Once Warwick's archers redeployed to a position where they could hit the unarmored sides and backs of the horses, however, they quickly routed the cavalry force opposing them. The archers were also unquestionably effective against common infantry, who did not have the wealth to afford plate armour.

The English army was an experienced force; many archers were veterans of the earlier Battle of Crécy, and two of the key commanders, Sir John Chandos, and Captal de Buch were both experienced soldiers. The English army's divisions were led by Edward, the Black Prince, the Earl of Warwick, the Earl of Salisbury, Sir John Chandos and Captal de Buch.

The French army was led by King John, and was composed largely of native French soldiers, though there was a contingent of German knights, and a large force of Scottish soldiers. The latter force was led by the Earl of Douglas and fought in the King's own division. The French army at the battle comprised approximately 8,000 men-at-arms and 3,000 common infantry, though John had made the decision to leave behind the vast majority of his infantry, numbering up to 20,000, in order to overtake and force the English to battle.

The French army was arrayed in three "battles" or divisions. The vanguard was led by the Dauphin Charles, the second by the Duke of Orléans, while the third, the largest, was led by the King himself.

Prior to the battle, the local prelate, Cardinal Hélie de Talleyrand-Périgord attempted to broker a truce between the two sides, as recorded in the writings of the English commander, Sir John Chandos. Attending the conference on the French side were King John, the Count of Tankerville, the Archbishop of Sens, and Jean de Talaru. Representing the English were the Earl of Warwick, the Earl of Suffolk, Bartholomew de Burghersh, James Audley, and Sir John Chandos. The English offered to hand over all of the war booty they had taken on their raids throughout France, as well as a seven-year truce. John, who believed his force could easily overwhelm the English, declined their proposal. John's counter suggestion that the Black Prince and his army should surrender was flatly rejected. An account of the meeting was recorded in the writings of the life of Sir John Chandos and were made in the final moments of a meeting of both sides in an effort to avoid the bloody conflict at Poitiers during The Hundred Years' War. The extraordinary narrative occurred just before that battle and reads as follows: 

At the start of the battle, the English removed their baggage train from the field, prompting a hasty French assault, believing that what they saw was the English retreating. The fighting began with a charge by a forlorn hope of 300 German knights, led by Jean de Clermont. The attack was a disaster, with many of the knights shot down or killed by English soldiery. According to Froissart, the English archers then shot their bows at the massed French infantry. The Dauphin's division reached the English line. Exhausted by a long march in heavy equipment and harassed by the hail of arrows, the division was repulsed after approximately two hours of combat.

The retreating vanguard collided with the advancing division of the Duke of Orléans, throwing the French army into chaos. Seeing the Dauphin's troops falling back, Orléans' division fell back in confusion. The third, and strongest, division led by the King advanced, and the two withdrawing divisions coalesced and resumed their advance against the English. Believing that the retreat of the first two French divisions marked the withdrawal of the French, Edward had ordered a force under Captal de Buch to pursue. Sir John Chandos urged the Prince to launch this force upon the main body of the French army under the King. Seizing upon this idea, Edward ordered all his men-at-arms and knights to mount for the charge, while de Buch's men, already mounted, were instructed to advance around the French left flank and rear.

As the French advanced, the English launched their charge. With the French stunned by the attack, the impetus carried the English and Gascon forces right into their line. Simultaneously, de Buch's mobile reserve of mounted troops fell upon the French left flank and rear. With the French army fearful of encirclement, their cohesion disintegrated as many soldiers attempted to flee the field. Low on arrows, the English and Welsh archers abandoned their bows and ran forward to join the melée. Around this time, King John and his son, Philip the Bold, found themselves surrounded. As written by Froissart, an exiled French knight fighting with the English, Sir Denis Morbeke of Artois approached the king, requesting the King's surrender. The King is said to have replied, "To whom shall I yield me? Where is my cousin the Prince of Wales? If I might see him, I would speak with him". Denis replied; "Sir, he is not here; but yield you to me and I shall bring you to him". The king handed him his right gauntlet, saying; "I yield me to you".

Following the surrender of the King and his son Philip, the French army broke up and left the field, ending the battle.

Following the battle, Edward resumed his march back to the English stronghold at Bordeaux. Jean de Venette, a Carmelite friar, vividly describes the chaos that ensued following the battle. The demise of the French nobility at the battle, only ten years from the catastrophe at Crécy, threw the kingdom into chaos. The realm was left in the hands of the Dauphin Charles, who faced popular rebellion across the kingdom in the wake of the defeat. Jean writes that the French nobles brutally repressed the rebellions, robbing, despoiling, and pillaging the peasants' goods. Mercenary companies hired by both sides added to the destruction, plundering the peasants and the churches.

Charles, to the misery of the French peasantry, began to raise additional funds to pay for the ransom of his father, and to continue the war effort. Capitalising on the discontent in France, King Edward assembled his army at Calais in 1359 and led his army on a campaign against Rheims. Unable to take Rheims or the French capital, Paris, Edward moved his army to Chartres. Later, the Dauphin Charles offered to open negotiations, and Edward agreed.

The Treaty of Brétigny was ratified on 24 October 1360, ending the Edwardian phase of the Hundred Years' War. In it, Edward agreed to renounce his claims to the French throne, in exchange for full sovereign rights over an expanded Aquitaine and Calais, essentially restoring the former Angevin Empire.

Froissart states that these men fought with the Black Prince:

Another account states that John of Ghistelles perished at the Battle of Crécy so there is some ambiguity as to this individual.

Froissart states that these men fought with King John II:

Arthur Conan Doyle's novel "Sir Nigel" features the Battle of Poitiers. The impoverished young squire Nigel Loring captures King John II of France in the melee. He fails to realise that he has accepted the surrender of the King of France, and so does not gain the King's ransom. However King John admits that Nigel was his vanquisher, so as reward Nigel is knighted by Edward, the Black Prince.

The battle appears in passing in "A Knight's Tale" when Count Adhemar is called back to the war.

Bernard Cornwell's novel "1356", the final novel in "The Grail Quest" series telling the story of Thomas of Hookton, dramatises the battle of Poitiers.

Michael Jecks's novel "Blood of the Innocents", the final novel in "The Hundred Years War" trilogy, dramatises the campaign that culminates with the battle of Poitiers.

Coldplay’s 2008 EP "Prospekt’s March" uses the "Battle of Poitiers" painting by Eugène Delacroix as its album cover.





</doc>
<doc id="4505" url="https://en.wikipedia.org/wiki?curid=4505" title="Backbone cabal">
Backbone cabal

The backbone cabal was an informal organization of large-site news server administrators of the worldwide distributed newsgroup-based discussion system Usenet. It existed from about 1983 at least into the 2000s.

The cabal was created in an effort to facilitate reliable propagation of new Usenet posts. While in the 1970s and 1980s many news servers only operated during night time to save on the cost of long distance communication, servers of the backbone cabal were available 24 hours a day. The administrators of these servers gained sufficient influence in the otherwise anarchic Usenet community to be able to push through controversial changes, for instance the Great Renaming of Usenet newsgroups during 1987.

As Usenet has few technologically or legally enforced hierarchies, just about the only ones that formed were social. People acquired power through persuasion (both publicly and privately), public debate, force of will (often via aggressive flames), garnering authority and respect by spending much time and effort contributing to the community (by being a maintainer of a FAQ, for example; see also Kibo, etc.).

Mary Ann Horton recruited membership in and designed the original physical topology of the Usenet Backbone in 1983. Gene "Spaf" Spafford then created an email list of the backbone administrators, plus a few influential posters. This list became known as the Backbone Cabal and served as a "political (i.e. decision making) backbone". Other prominent members of the cabal were Brian Reid, Bob Allisat, Chuq von Rospach and Rick Adams.

During most of its existence, the cabal (sometimes capitalized) steadfastly denied its own existence; those involved would often respond "There is no Cabal" (sometimes abbreviated as "TINC"'), whenever the existence or activities of the group were speculated on in public. It is sometimes used humorously to dispel cabal-like organizational conspiracy theories, or as an ironic statement, indicating one who knows the existence of "the cabal" will invariably deny there is one.

This belief became a model for various conspiracy theories about various Cabals with dark nefarious objectives beginning with taking over Usenet or the Internet. Spoofs include the "Eric Conspiracy" of moustachioed hackers named "Eric"; ex-members of the P.H.I.R.M.; and the Lumber Cartel putatively funding anti-spam efforts to support the paper industry.

The result of this policy was an aura of mystery, even a decade after the cabal mailing list disbanded in late 1988 following an internal fight.




</doc>
<doc id="4506" url="https://en.wikipedia.org/wiki?curid=4506" title="Bongo (antelope)">
Bongo (antelope)

The bongo ("Tragelaphus eurycerus") is a herbivorous, mostly nocturnal forest ungulate.

Bongos are characterised by a striking reddish-brown coat, black and white markings, white-yellow stripes and long slightly spiralled horns. Bongos are the only tragelaphid in which both sexes have horns. They have a complex social interaction and are found in African dense forest mosaics.

The western or lowland bongo, "T. e. eurycerus", faces an ongoing population decline, and the IUCN Antelope Specialist Group considers it to be Near Threatened on the conservation status scale.

The eastern or mountain bongo, "T. e. isaaci", of Kenya, has a coat even more vibrant than that of "T. e. eurycerus". The mountain bongo is only found in the wild in a few mountain regions of central Kenya. This bongo is classified by the IUCN Antelope Specialist Group as Critically Endangered, with fewer individuals in the wild than in captivity (where it breeds readily).

In 2000, the Association of Zoos and Aquariums in the USA (AZA) upgraded the bongo to a Species Survival Plan participant and in 2006 added the Bongo Restoration to Mount Kenya Project to its list of the Top Ten Wildlife Conservation Success Stories of the year. However, in 2013, it seems, these successes have been compromised by reports of possibly only 100 mountain bongos left in the wild due to logging and poaching.

The scientific name of the bongo is "Tragelaphus eurycerus", and it belongs to the genus "Tragelaphus" and family Bovidae. It was first described by Irish naturalist William Ogilby in 1837. The generic name "Tragelaphus" is composed of two Greek words: "trag-", meaning a goat; and "elaphos", meaning deer. The specific name "eurycerus" originated from the fusion of "eurus" (broad, widespread) and "keras" (an animal's horn). The common name "bongo" originated probably from the Kele language of Gabon. The first known use of the name "bongo" in English dates to 1861.

Bongos are further classified into two subspecies: "T. e. eurycerus", the lowland or western bongo, and the far rarer "T. e. isaaci", the mountain or eastern bongo, restricted to the mountains of Kenya only. The eastern bongo is larger and heavier than the western bongo. Two other subspecies are described from West and Central Africa, but taxonomic clarification is required. They have been observed to live up to 19 years.

Bongos are one of the largest of the forest antelopes. In addition to the deep chestnut colour of their coats, they have bright white stripes on their sides to help with camouflage.

Adults of both sexes are similar in size. Adult height is about at the shoulder and length is , including a tail of . Females weigh around , while males weigh about . Its large size puts it as the third-largest in the Bovidae tribe of Strepsicerotini, behind both the common and greater elands by about , and above the greater kudu by about .

Both sexes have heavy spiral horns; those of the male are longer and more massive. All bongos in captivity are from the isolated Aberdare Mountains of central Kenya.

The bongo sports a bright auburn or chestnut coat, with the neck, chest, and legs generally darker than the rest of the body. Coats of male bongos become darker as they age until they reach a dark mahogany-brown colour. Coats of female bongos are usually more brightly coloured than those of males. The eastern bongo is darker in color than the western and this is especially pronounced in older males which tend to be chestnut brown, especially on the forepart of their bodies.

The pigmentation in the coat rubs off quite easily; anecdotal reports suggest rain running off a bongo may be tinted red with pigment. The smooth coat is marked with 10–15 vertical white-yellow stripes, spread along the back from the base of the neck to the rump. The number of stripes on each side is rarely the same. It also has a short, bristly, brown ridge of dorsal hair from the shoulder to the rump; the white stripes run into this ridge.

A white chevron appears between the eyes, and two large white spots grace each cheek. Another white chevron occurs where the neck meets the chest. The large ears are to sharpen hearing, and the distinctive coloration may help bongos identify one another in their dark forest habitats. Bongos have no special secretion glands, so rely less on scent to find one another than do other similar antelopes. The lips of a bongo are white, topped with a black muzzle.

Bongos have two heavy and slightly spiralled horns that slope over their backs, and like many other antelope species, both male and female bongos have horns. Bongos are the only tragelaphids in which both sexes have horns. The horns of bongos are in the form of a lyre and bear a resemblance to those of the related antelope species of nyalas, sitatungas, bushbucks, kudus and elands.

Unlike deer, which have branched antlers shed annually, bongos and other antelopes have pointed horns they keep throughout their lives. Males have massive backswept horns, while females have smaller, thinner, and more parallel horns. The size of the horns range between . The horns twist once.

Like all other horns of antelopes, the core of a bongo's horn is hollow and the outer layer of the horn is made of keratin, the same material that makes up human fingernails, toenails and hair. The bongo runs gracefully and at full speed through even the thickest tangles of lianas, laying its heavy spiralled horns on its back so the brush cannot impede its flight. Bongos are hunted for their horns by humans.

Like other forest ungulates, bongos are seldom seen in large groups. Males, called bulls, tend to be solitary, while females with young live in groups of six to eight. Bongos have seldom been seen in herds of more than 20. Gestation is about 285 days (9.5 months), with one young per birth, and weaning occurs at six months. Sexual maturity is reached at 24–27 months. The preferred habitat of this species is so dense and difficult to operate in, that few Europeans or Americans observed this species until the 1960s.
As young males mature and leave their maternal groups, they most often remain solitary, although rarely they join an older male. Adult males of similar size/age tend to avoid one another. Occasionally, they meet and spar with their horns in a ritualised manner and it is rare for serious fights to take place. However, such fights are usually discouraged by visual displays, in which the males bulge their necks, roll their eyes, and hold their horns in a vertical position while slowly pacing back and forth in front of the other male. They seek out females only during mating time. When they are with a herd of females, males do not coerce them or try to restrict their movements as do some other antelopes.

Although mostly nocturnal, they are occasionally active during the day. However, like deer, bongos may exhibit crepuscular behaviour. Bongos are both timid and easily frightened; after a scare, a bongo moves away at considerable speed, even through dense undergrowth. Once they find cover, they stay alert and face away from the disturbance, but peek every now and then to check the situation. The bongo's hindquarters are less conspicuous than the forequarters, and from this position the animal can quickly flee.

When in distress, the bongo emits a bleat. It uses a limited number of vocalisations, mostly grunts and snorts; females have a weak mooing contact-call for their young. Females prefer to use traditional calving grounds restricted to certain areas, while newborn calves lie in hiding for a week or more, receiving short visits by the mother to suckle.

The calves grow rapidly and can soon accompany their mothers in the nursery herds. Their horns grow rapidly and begin to show in 3.5 months. They are weaned after six months and reach sexual maturity at about 20 months.

Bongos are found in tropical jungles with dense undergrowth up to an altitude of in Central Africa, with isolated populations in Kenya, and these West African countries: Cameroon, the Central African Republic, the Republic of the Congo, the Democratic Republic of Congo, the Ivory Coast, Equatorial Guinea, Gabon, Ghana, Guinea, Liberia, Sierra Leone, South Sudan.

Historically, bongos are found in three disjunct parts of Africa: East, Central and West. Today, all three populations' ranges have shrunk in size due to habitat loss for agriculture and uncontrolled timber cutting, as well as hunting for meat.

Bongos favour disturbed forest mosaics that provide fresh, low-level green vegetation. Such habitats may be promoted by heavy browsing by elephants, fires, flooding, tree-felling (natural or by logging), and fallowing. Mass bamboo die-off provides ideal habitat in East Africa. They can live in bamboo forests.

Like many forest ungulates, bongos are herbivorous browsers and feed on leaves, bushes, vines, bark and pith of rotting trees, grasses/herbs, roots, cereals, and fruits.

Bongos require salt in their diets, and are known to regularly visit natural salt licks. Examination of bongo feces revealed that charcoal from trees burnt by lightning is consumed. This behavior is believed to be a means of getting salts and minerals into their diets. This behavior has also been reported in the okapi. Another similarity to the okapi, though the bongo is unrelated, is that the bongo has a long prehensile tongue which it uses to grasp grasses and leaves.

Suitable habitats for bongos must have permanent water available. As a large animal, the bongo requires an ample amount of food, and is restricted to areas with abundant year-round growth of herbs and low shrubs.

Bongos are also known to eat burnt wood after a storm, as a rich source of salt and minerals.

Few estimates of population density are available. Assuming average population densities of 0.25 animals per km in regions where it is known to be common or abundant, and 0.02 per km elsewhere, and with a total area of occupancy of 327,000 km, a total population estimate of around 28,000 is suggested. Only about 60% are in protected areas, suggesting the actual numbers of the lowland subspecies may only be in the low tens of thousands. In Kenya, their numbers have declined significantly and on Mt. Kenya, they were within the last decade due to illegal hunting with dogs. Although information on their status in the wild is lacking, lowland bongos are not presently considered endangered.

Bongos are susceptible to diseases such as rinderpest, which almost exterminated the species during the 1890s. "Tragelaphus eurycerus" may suffer from goitre. Over the course of the disease, the thyroid glands greatly enlarge (up to 10 x 20 cm) and may become polycystic. Pathogenesis of goiter in the bongo may reflect a mixture of genetic predisposition coupled with environmental factors, including a period of exposure to a goitrogen. Leopards and spotted hyenas are the primary natural predators (lions are seldom encountered due to differing habitat preferences); pythons sometimes eat bongo calves. Humans prey on them for their pelts, horns, and meat, with the species being a common local source for "bush meat". Bongo populations have been greatly reduced by hunting, poaching, and animal trapping, although some bongo refuges exist.

Although bongos are quite easy for humans to catch using snares, many people native to the bongos' habitat believed that if they ate or touched bongo, they would have spasms similar to epileptic seizures. Because of this superstition, bongos were less harmed in their native ranges than expected. However, these taboos are said no longer to exist, which may account for increased hunting by humans in recent times.

An international studbook is maintained to help manage animals held in captivity. Because of its bright colour, it is very popular in zoos and private collections. In North America, over 400 individuals are thought to be held, a population that probably exceeds that of the mountain bongo in the wild.

In 2000, the Association of Zoos and Aquariums (AZA) upgraded the bongo to a Species Survival Plan participant, which works to improve the genetic diversity of managed animal populations. The target population for participating zoos and private collections in North America is 250 animals. Through the efforts of zoos in North America, a reintroduction to the population in Kenya is being developed.

At least one collaborative effort for reintroduction between North American wildlife facilities has already been carried out. In 2004, 18 eastern bongos born in North American zoos gathered at White Oak Conservation in Yulee, Florida for release in Kenya. White Oak staff members traveled with the bongos to a Mt. Kenya holding facility, where they stayed until being reintroduced.

In the last few decades, a rapid decline in the numbers of wild mountain bongo has occurred due to poaching and human pressure on their habitat, with local extinctions reported in Cherangani and Chepalungu hills, Kenya.

The Bongo Surveillance Programme, working alongside the Kenya Wildlife Service, have recorded photos of bongos at remote salt licks in the Aberdare Forests using camera traps, and, by analyzing DNA extracted from dung, have confirmed the presence of bongo in Mount Kenya, Eburru and Mau forests. The programme estimate as few as 140 animals left in the wild – spread across four isolated populations. Whilst captive breeding programmes can be viewed as having been successful in ensuring survival of this species in Europe and North America, the situation in the wild has been less promising. Evidence exists of bongo surviving in Kenya. However, these populations are believed to be small, fragmented, and vulnerable to extinction.

Animal populations with impoverished genetic diversity are inherently less able to adapt to changes in their environments (such as climate change, disease outbreaks, habitat change, etc.). The isolation of the four remaining small bongo populations, which themselves would appear to be in decline, means a substantial amount of genetic material is lost each generation. Whilst the population remains small, the impact of transfers will be greater, so the establishment of a "metapopulation management plan" occurs concurrently with conservation initiatives to enhance "in situ" population growth, and this initiative is both urgent and fundamental to the future survival of mountain bongo in the wild.

The western/lowland bongo faces an ongoing population decline as habitat destruction and hunting pressures increase with the relentless expansion of human settlement. Its long-term survival will only be assured in areas which receive active protection and management. At present, such areas comprise about 30,000 km, and several are in countries where political stability is fragile. So, a realistic possibility exists whereby its status could decline to Threatened in the near future.

As the largest and most spectacular forest antelope, the western/lowland bongo is both an important flagship species for protected areas such as national parks, and a major trophy species which has been taken in increasing numbers in Central Africa by sport hunters during the 1990s. Both of these factors are strong incentives to provide effective protection and management of populations.

Trophy hunting has the potential to provide economic justification for the preservation of larger areas of bongo habitat than national parks, especially in remote regions of Central Africa, where possibilities for commercially successful tourism are very limited.

The eastern/mountain bongo's survival in the wild is dependent on more effective protection of the surviving remnant populations in Kenya. If this does not occur, it will eventually become extinct in the wild. The existence of a healthy captive population of this subspecies offers the potential for its reintroduction.

In 2004, Dr. Jake Veasey, the head of the Department of Animal Management and Conservation at Woburn Safari Park and a member of the European Association of Zoos and Aquariums Population Management Advisory Group, with the assistance of Lindsay Banks, took over responsibility for the management and coordination of the European Endangered Species Programme for the eastern bongo. This includes some 250 animals across Europe and the Middle East.

Along with the Rothschild giraffe, the eastern bongo is arguably one of the most threatened large mammals in Africa, with recent estimates numbering less than 140 animals, below a minimum sustainable viable population. The situation is exacerbated because these animals are spread across four isolated populations. Whilst the bongo endangered species program can be viewed as having been successful in ensuring survival of this species in Europe, it has not yet become actively involved in the conservation of this species in the wild in a coordinated fashion. The plan is to engage in conservation activities in Kenya to assist in reversing the decline of the eastern bongo populations and genetic diversity in Africa, and in particular, applying population management expertise to help ensure the persistence of genetic diversity in the free ranging wild populations.

To illustrate significance of genetic diversity loss, assume the average metapopulation size is 35 animals based on 140 animals spread across four populations (140/4=35). Assuming stable populations, these populations will lose 8% of their genetic diversity every decade. By managing all four populations as one, through strategic transfers, gene loss is reduced from 8% to 2% per decade, without any increase in bongo numbers in Kenya. By managing the European and African populations as one – by strategic exports from Europe combined with "in situ" transfers, gene loss is reduced to 0.72% every 100 years, with both populations remaining stable. If populations in Kenya are allowed to grow through the implementation of effective conservation, including strategic transfers, gene loss can be effectively halted in this species and its future secured in the wild.

The initial aims of the project are: 

If effective protection were implemented immediately and bongo populations allowed to expand without transfers, then this would create a bigger population of genetically impoverished bongos. These animals would be less able to adapt to a dynamic environment. Whilst the population remains small, the impact of transfers will be greater. For this reason, the 'metapopulation management plan' must occur concurrently with conservation strategies to enhance "in situ" population growth. This initiative is both urgent and fundamental to the future survival of the mountain bongo in the wild.
In 2013, SafariCom telecommunications donated money to the Bongo Surveillance Programme to try to keep tabs on what are thought to be the last 100 eastern bongos left in the wild in the Mau Eburu Forest in central Kenya, whose numbers are still declining due to logging of their habitat and illegal poaching.

Mount Kenya Wildlife Conservancy runs a bongo rehabilitation program in collaboration with the Kenya Wildlife Service. The Conservancy aims to prevent extinction of the bongo through breeding and release back into the wild.

In 2002 the IUCN listed the western/lowland species as Near Threatened. These bongos may be endangered due to human environmental interaction, as well as hunting and illegal actions towards wildlife. CITES lists bongos as an Appendix III species, only regulating their exportation from a single country, Ghana. It is not protected by the US Endangered Species Act and is not listed by the USFWS.

The IUCN Antelope Specialist Group considers the western or lowland bongo, "T. e. eurycerus", to be Lower Risk (Near Threatened), and the eastern or mountain bongo, "T. e. isaaci", of Kenya, to be Critically Endangered. Other subspecific names have been used, but their validity has not been tested.



</doc>
<doc id="4507" url="https://en.wikipedia.org/wiki?curid=4507" title="Bunyip">
Bunyip

The bunyip is a large mythical creature from Australian Aboriginal mythology, said to lurk in swamps, billabongs, creeks, riverbeds, and waterholes.

The bunyip was part of traditional Aboriginal beliefs and stories throughout Australia, while its name varied according to tribal nomenclature. In his 2001 book, writer Robert Holden identified at least nine regional variations of the creature known as the bunyip across Aboriginal Australia. The origin of the word "bunyip" has been traced to the Wemba-Wemba or Wergaia language of the Aboriginal people of Victoria, in South-Eastern Australia. Europeans recorded various written accounts of bunyips in the early and mid-19th century, as they began to settle across the country.

The word "bunyip" is usually translated by Aboriginal Australians today as "devil" or "evil spirit". This contemporary translation may not accurately represent the role of the bunyip in pre-contact Aboriginal mythology or its possible origins before written accounts were made. Some modern sources allude to a linguistic connection between the bunyip and Bunjil, "a mythic 'Great Man' who made the mountains and rivers and man and all the animals." The word "bahnyip" first appeared in the "Sydney Gazette" in 1812. It was used by James Ives to describe "a large black animal
like a seal, with a terrible voice which creates terror among the blacks." By the 1850s, "bunyip" was also used as a "synonym for impostor, pretender, humbug and the like" in the broader Australian community. The term "bunyip aristocracy" was first coined in 1853 to describe Australians aspiring to be aristocrats. In the early 1990s, Prime Minister Paul Keating used this term to describe members of the conservative Liberal Party of Australia opposition. The word "bunyip" can still be found in a number of Australian contexts, including place names such as the Bunyip River (which flows into Westernport Bay in southern Victoria) and the town of Bunyip, Victoria.

Descriptions of bunyips vary widely. George French Angus may have collected a description of a bunyip in his account of a "water spirit" from the Moorundi people of the Murray River before 1847, stating it is "much dreaded by them ... It inhabits the Murray; but ... they have some difficulty describing it. Its most usual form ... is said to be that of an enormous starfish." The Challicum bunyip, an outline image of a bunyip carved by Aborigines into the bank of Fiery Creek, near Ararat, Victoria, was first recorded by "The Australasian" newspaper in 1851. According to the report, the bunyip had been speared after killing an Aboriginal man. Antiquarian Reynell Johns claimed that until the mid-1850s, Aboriginal people made a "habit of visiting the place annually and retracing the outlines of the figure [of the bunyip] which is about 11 paces long and 4 paces in extreme breadth." The outline image no longer exists. Robert Brough Smyth's "Aborigines of Victoria" (1878) devoted ten pages to the bunyip, but concluded "in truth little is known among the blacks respecting its form, covering or habits; they appear to have been in such dread of it as to have been unable to take note of its characteristics."

The bunyips presumably seen by witnesses, according to their descriptions, most commonly fit one of two categories: 60% of sightings resemble seals or swimming dogs, and 20% of sightings are of long-necked creatures with small heads; the remaining descriptions are ambiguous beyond categorization. The seal-dog variety is most often described as being between 4 and 6 feet long with a shaggy black or brown coat. According to reports, these bunyips have round heads resembling a bulldog, prominent ears, no tail, and whiskers like a seal or otter. The long-necked variety is allegedly between 5 and 15 feet long, and is said to have black or brown fur, large ears, small tusks, a head like a horse or emu, an elongated, maned neck about three feet long and with many folds of skin, and a horse-like tail. The bunyip has been described by natives as amphibious, nocturnal, and inhabiting lakes, rivers, and swamps. Bunyips, according to Aborigines, can swim swiftly with fins or flippers, have a loud, roaring call, and feed on crayfish, though some legends portray them as bloodthirsty predators of humans, particularly women and children. Bunyip eggs are allegedly laid in platypus nests.

Non-Aboriginal Australians have made various attempts to understand and explain the origins of the bunyip as a physical entity over the past 150 years. Writing in 1933, Charles Fenner suggested that it was likely that the "actual origin of the bunyip myth lies in the fact that from time to time seals have made their way up the Murray and Darling (Rivers)". He provided examples of seals found as far inland as Overland Corner, Loxton, and Conargo and reminded readers that "the smooth fur, prominent 'apricot' eyes, and the bellowing cry are characteristic of the seal", especially southern elephant seals and leopard seals.

Another suggestion is that the bunyip may be a cultural memory of extinct Australian marsupials such as the "Diprotodon", "Zygomaturus", "Nototherium", or "Palorchestes". This connection was first formally made by Dr George Bennett of the Australian Museum in 1871. In the early 1990s, palaeontologist Pat Vickers-Rich and geologist Neil Archbold also cautiously suggested that Aboriginal legends "perhaps had stemmed from an acquaintance with prehistoric bones or even living prehistoric animals themselves ... When confronted with the remains of some of the now extinct Australian marsupials, Aborigines would often identify them as the bunyip." They also note that "legends about the" mihirung paringmal" of western Victorian Aborigines ... may allude to the ... extinct giant birds the Dromornithidae."

In a 2017 "Australian Birdlife" article, Karl Brandt suggested Aboriginal encounters with the southern cassowary inspired the myth. According to the first written description of the bunyip from 1845, the creature, which laid pale blue eggs of immense size, possessed deadly claws, powerful hind legs, a brightly coloured chest, and an emu-like head, characteristics shared with then undiscovered Australian cassowary. As the creature's bill was described as having serrated projections, each "like the bone of the stingray", this bunyip was associated with the indigenous people of Far North Queensland, renowned for their spears tipped with stingray barbs and their proximity to the cassowary's Australian range.

Another association to the bunyip is the shy Australasian bittern ("Botaurus poiciloptilus"). During the breeding season, the male call of this marsh-dwelling bird is a "low pitched boom"; hence, it is occasionally called the "bunyip bird".

During the early settlement of Australia by Europeans, the notion became commonly held that the bunyip was an unknown animal that awaited discovery. Unfamiliar with the sights and sounds of the island continent's peculiar fauna, early Europeans believed that the bunyip described to them was one more strange Australian animal and they sometimes attributed unfamiliar animal calls or cries to it. Scholars suggest also that 19th-century bunyip lore was reinforced by imported European folklore, such as that of the Irish Púca.

A large number of bunyip sightings occurred during the 1840s and 1850s, particularly in the southeastern colonies of Victoria, New South Wales and South Australia, as European settlers extended their reach. The following is not an exhaustive list of accounts:

One of the earliest accounts relating to a large unknown freshwater animal was in 1818, when Hamilton Hume and James Meehan found some large bones at Lake Bathurst in New South Wales. They did not call the animal a bunyip, but described the remains indicating the creature as very much like a hippopotamus or manatee. The Philosophical Society of Australasia later offered to reimburse Hume for any costs incurred in recovering a specimen of the unknown animal, but for various reasons, Hume did not return to the lake. Ancient "Diprotodon" skeletons have sometimes been compared to the hippopotamus; they are a land animal, but have sometimes been found in a lake or water course.

More significant was the discovery of fossilised bones of "some quadruped much larger than the ox or buffalo" in the Wellington Caves in mid-1830 by bushman George Ranken and later by Thomas Mitchell. Sydney's Reverend John Dunmore Lang announced the find as "convincing proof of the deluge", referring to Biblical accounts of the Flood. But British anatomist Sir Richard Owen identified the fossils as the gigantic marsupials "Nototherium" and "Diprotodon". At the same time, some settlers observed that "all natives throughout these ... districts have a tradition (of) a very large animal having at one time existed in the large creeks and rivers and by many it is said that such animals now exist."

In July 1845, "The Geelong Advertiser" announced the discovery of fossils found near Geelong, under the headline "Wonderful Discovery of a new Animal". This was a continuation of a story on 'fossil remains' from the previous issue. The newspaper continued, "On the bone being shown to an intelligent black, he at once recognised it as belonging to the bunyip, which he declared he had seen. On being requested to make a drawing of it, he did so without hesitation." The account noted a story of an Aboriginal woman being killed by a bunyip and the "most direct evidence of all" – that of a man named Mumbowran "who showed several deep wounds on his breast made by the claws of the animal".

The account provided this description of the creature:
Shortly after this account appeared, it was repeated in other Australian newspapers. This appears to be the first use of the word "bunyip" in a written publication.

In January 1846, a peculiar skull was taken by a settler from the banks of Murrumbidgee River near Balranald, New South Wales. Initial reports suggested that it was the skull of something unknown to science. The squatter who found it remarked, "all the natives to whom it was shown called [it] a bunyip". By July 1847, several experts, including W. S. Macleay and Professor Owen, had identified the skull as the deformed foetal skull of a foal or calf. At the same time, the purported bunyip skull was put on display in the Australian Museum (Sydney) for two days. Visitors flocked to see it, and "The Sydney Morning Herald" reported that many people spoke out about their "bunyip sightings". Reports of this discovery used the phrase 'Kine Pratie' as well as Bunyip. Explorer William Hovell, who examined the skull, also called it a 'katen-pai'.

In March of that year "a bunyip or an immense Platibus" (Platypus) was sighted "sunning himself on the placid bosom of the Yarra, just opposite the Custom House" in Melbourne. "Immediately a crowd gathered" and three men set off by boat "to secure the stranger" which "disappeared" when they were "about a yard from him".

Another early written account is attributed to escaped convict William Buckley in his 1852 biography of thirty years living with the Wathaurong people. His 1852 account records "in ... Lake Moodewarri [now Lake Modewarre] as well as in most of the others inland ... is a ... very extraordinary amphibious animal, which the natives call Bunyip." Buckley's account suggests he saw such a creature on several occasions. He adds, "I could never see any part, except the back, which appeared to be covered with feathers of a dusky grey colour. It seemed to be about the size of a full grown calf ... I could never learn from any of the natives that they had seen either the head or tail." Buckley also claimed the creature was common in the Barwon River and cites an example he heard of an Aboriginal woman being killed by one. He emphasized the bunyip was believed to have supernatural powers.

In an article titled, 'The Bunyip', a newspaper reported on the drawings made by Edwin Stocqueler as he travelled on the Murray and Goulburn rivers: 'Amongst the latter drawings we noticed a likeness of the Bunyip, or rather a view of the neck and shoulders of the animal. Mr. Stocqueler informs us that the Bunyip is a large freshwater seal, having two small padules or fins attached to the shoulders, a long swan like neck, a head like a dog, and a curious bag hanging under the jaw, resembling the pouch of the pelican. The animal is covered with hair, like the platypus, and the colour is a glossy black. Mr. Stocqueler saw no less than six of these curious animals at different times; his boat was within thirty feet of one near M'Guire's punt on the Goulburn, and he fired at the Bunyip, but did not succeed in capturing him. The smallest appeared to be about five feet in length, and the largest exceeded fifteen feet. The head of the largest was the size of a bullock's head, and three feet out of water. After taking a sketch of the animal, Mr. Stocqueler showed it to several blacks of the Goulburn tribe, who declared that the picture was "Bunyip's brother," meaning a duplicate or likeness of the bunyip. The animals moved against the current, at the rate of about seven miles an hour, and Mr. Stockqueler states that he could have approached close to the specimens he observed, had he not been deterred by the stories of the natives concerning the power and fury of the bunyip, and by the fact that his gun had only a single barrel, and his boat was of a very frail description.'

The description varied across newspaper accounts: 'The great Bunyip question seems likely to be brought to a close, as a Mr. Stocqueler, an artist and gentleman, who has come up the Murray in a small boat, states that he saw one, and was enabled to take a drawing of this "vexed question," but could not succeed in catching him. We have seen the sketch, and it puts us in mind of an hybrid between the water mole and the great sea serpent.' 'Mr. Stocqueler, an artist, and his mother are on an expedition down the Murray, for the purpose of making some faithful sketches of the views on this fine stream, as well as of the creatures frequenting it. I have seen some of their productions, and as they pourtray localities with which I am well acquainted, can pronounce the drawings faithful representations. Mother and son go down the stream in a canoe. The lady paints flowers, &c.; the son devotes himself to choice views on the river's side. One of the drawings represents a singular creature, which the artist is unable to classify. It has the appearance in miniature of the famous sea-serpent, as that animal is described by navigators. Mr. Stocqueler was about twenty-five yards distant from it at first sight as it lay placidly on the water. On being observed, the stranger set-off, working his paddles briskly, and rapidly disappeared. Captain Cadell has tried to solve the mystery, but is not yet satisfied as to what the animal really is. Mr. Stocqueler states that there were about two feet of it above water when he first saw it, and he estimated its length at from five to six feet. The worthy Captain says, that unless the creature is the "Musk Drake" (so called from giving off a very strong odour of musk), he cannot account for the novelty.'

Stocqueler disputed the newspaper descriptions in a letter; stating that he never called the animal a bunyip, it did not have a swan like neck, and he never said anything about the size of the animal as he never saw the whole body. He went on to write that all would be revealed in his diorama as an 'almost life size portrait of the beast' would be included. The diorama took him four years to paint and was reputed to be a mile (1.6 km) long and made of 70 individual pictures. The diorama has long since disappeared and may no longer exist.


Numerous tales of the bunyip in written literature appeared in the 19th and early 20th centuries. One of the earliest known is a story in Andrew Lang's "The Brown Fairy Book" (1904).


The Australian tourism boom of the 1970s brought a renewed interest in bunyip mythology.

Bunyip stories have also been published outside Australia.


In the 21st century the bunyip has been featured in works around the world. 



</doc>
<doc id="4508" url="https://en.wikipedia.org/wiki?curid=4508" title="Brabant">
Brabant

Brabant is a traditional geographical region (or regions) in the Low Countries of Europe. It may refer to:








</doc>
<doc id="4512" url="https://en.wikipedia.org/wiki?curid=4512" title="Boone, North Carolina">
Boone, North Carolina

Boone is a town in and the county seat of Watauga County, North Carolina, United States. Located in the Blue Ridge Mountains of western North Carolina, Boone is the home of Appalachian State University. The population was 17,122 at the 2010 census.

The town is named for famous American pioneer and explorer Daniel Boone, and every summer, other than 2020, from 1952 has hosted an outdoor amphitheatre drama, "Horn in the West", portraying the British settlement of the area during the American Revolutionary War and featuring the contributions of its namesake. It is the largest community and the economic hub of the seven-county region of Western North Carolina known as the High Country.

 Boone took its name from the famous pioneer and explorer Daniel Boone, who on several occasions camped at a site generally agreed to be within the present city limits. Daniel's nephews, Jesse and Jonathan (sons of brother Israel Boone), were members of the town's first church, Three Forks Baptist, still in existence today.
Boone was served by the narrow gauge East Tennessee and Western North Carolina Railroad (nicknamed "Tweetsie") until the flood of 1940. The flood washed away much of the tracks and it was decided not to replace them.

Boone is the home of Appalachian State University, a constituent member of the University of North Carolina.
Appalachian State is the sixth-largest university in the seventeen-campus system.
Caldwell Community College & Technical Institute also operates a satellite campus in Boone.

""Horn in the West"" is a dramatization of the life and times of the early settlers of the mountain area. It features Daniel Boone as one of its characters, and has been performed in an outdoor amphitheater near the town every summer since 1952, except for when COVID-19 necessitated canceling the 2020 performances. 
The original actor in the role of "Daniel Boone" was Ned Austin. His "Hollywood Star" stands on a pedestal on King Street in downtown Boone. He was followed in the role by Glenn Causey, who portrayed the rugged frontiersman for 41 years, and whose image is still seen in many of the depictions of Boone featured in the area today.

Boone is notable for being home to the Junaluska community. Located in the hills just north of Downtown Boone, a free black community has existed in the area since before the Civil War. Although integration in the mid-20th century led to many of the businesses in the neighborhood closing in favor of their downtown counterparts, descendants of the original inhabitants still live in the neighborhood. Junaluska is also home to one of the few majority-African American Mennonite Brethren congregations.

Boone is a center for bluegrass musicians and Appalachian storytellers. Notable artists associated with Boone include the late Grammy Award-winning bluegrass guitar player Doc Watson, and the late guitarist Michael Houser, one of the founding members of and the lead guitarist for the band Widespread Panic, as well as Old Crow Medicine Show, The Blue Rags, and Eric Church, all who are Boone natives.

The Blair Farm, Daniel Boone Hotel, Jones House, John Smith Miller House, and US Post Office-Boone are listed on the National Register of Historic Places.

Boone is located at (36.211364, −81.668657) and has an elevation of 3,333 feet (1015.9 m) above sea level. An earlier survey gave the elevation as 3,332 ft and since then it has been published as having an elevation of 3,333 ft (1,016 m). Boone has the highest elevation of any town of its size (over 10,000 population) east of the Mississippi River. As such, Boone features, depending on the isotherm used, a humid continental climate (Köppen "Dfb"), a rarity for the Southeastern United States, bordering on a subtropical highland climate ("Cfb") and straddles the boundary between USDA Plant Hardiness Zones 6B and 7A; the elevation also results in enhanced precipitation, with of average annual precipitation. Compared to the lower elevations of the Carolinas, winters are long and cold, with frequent sleet and snowfall. The daily average temperature in January is , which gives Boone a winter climate more similar to coastal southern New England rather than the Southeast, where a humid subtropical climate predominates. Blizzard-like conditions are not unusual during many winters. Summers are warm, but far cooler and less humid than lower regions to the south and east, with a July daily average temperature of . Boone typically receives on average nearly of snowfall annually, far higher than the lowland areas in the rest of North Carolina.
On January 18, 1966, the temperature fell to .

As of the census of 2000, there were 13,472 people, 4,374 households, and 1,237 families residing in the town. The population density was 2,307.0 people per square mile (890.7/km). There were 4,748 housing units at an average density of 813.0 per square mile (313.9/km). The racial makeup of the town was 93.98% White, 3.42% Black or African American, 0.30% Native American, 1.19% Asian, 0.05% Pacific Islander, 0.46% from other races, and 0.60% from two or more races. 1.64% of the population were Hispanic or Latino of any race.

There were 4,374 households, out of which 9.6% had children under the age of 18 living with them, 21.0% were married couples living together, 5.6% had a female householder with no husband present, and 71.7% were non-families. 38.4% of all households were made up of individuals, and 7.3% had someone living alone who was 65 years of age or older. The average household size was 1.97 and the average family size was 2.63.

In the town, the population was spread out, with 5.8% under 18, 65.9% from 18 to 24, 12.1% from 25 to 44, 9.1% from 45 to 64, and 7.1% who were 65 or older. The median age was 21 years. For every 100 females, there are 95.6 males. For every 100 females age 18 and over, there were 94.7 males.

The median income for a household in the town was $20,541, and the median income for a family was $49,762. The per capita income was $12,256. Males had a median income of $28,060 versus $20,000 for females. About 9.2% of families and 37.0% of the population were below the poverty line, including 6.3% of those under the age of 18 and 9.1% of those 65 and older.

Boone is mainly served by three local newspapers:


A smaller newspaper, "The Appalachian", is Appalachian State University's campus newspaper; it is published once a week on Thursdays. In addition to the locally printed papers, a monthly entertainment pamphlet named "Kraut Creek Revival" has limited circulation and is funded by a Denver, NC-based newspaper.


Boone operates under a mayor–council government. The city council consists of five members. The mayor presides over the council and casts a vote on issues only in the event of a tie. , the Town Council members were: Rennie Brantz, Mayor, and Councilors: Lynne Mason (Mayor Pro-Tem), Jennifer Teague, Loretta Clawson, Charlotte Mizelle and Jeannine Underdown Collins.

Industrial, commercial, and residential development in the town of Boone is a controversial issue due to its location in the mountains of Appalachia. On October 16, 2009, the town council accepted the "Boone 2030 Land Use Plan." While the document itself is not in any way actual law, it is used by the town council, board of adjustment, and other committees to guide decision making as to what types of development are appropriate.

In 2009, the North Carolina Department of Transportation began widening 1.1 miles of U.S. 421 (King Street) to a 4-to-6-lane divided highway with a raised concrete median from U.S. 321 (Hardin Street) to east of N.C. 194 (Jefferson Road), including a new entrance and exit to the new Watauga High School, at a cost of $16.2 million. The widening has displaced 25 businesses and 63 residences east of historic downtown King Street. The project was slated to be completed by December 31, 2011, but construction continued into the spring of 2012.



Boone has one sister city, as designated by Sister Cities International:



</doc>
<doc id="4513" url="https://en.wikipedia.org/wiki?curid=4513" title="Banshee">
Banshee

A banshee ( ; Modern Irish , from , , "woman of the fairy mound" or "fairy woman") is a female spirit in Irish folklore who heralds the death of a family member, usually by wailing, shrieking, or keening. Her name is connected to the mythologically important tumuli or "mounds" that dot the Irish countryside, which are known as síde (singular "síd") in Old Irish.

Sometimes she has long streaming hair and wears a gray cloak over a green dress, and her eyes are red from continual weeping. She may be dressed in white with red hair and a ghastly complexion, according to a firsthand account by Ann, Lady Fanshawe in her "Memoirs". Lady Wilde in "Ancient Legends of Ireland" provides another:

The size of the banshee is another physical feature that differs between regional accounts. Though some accounts of her standing unnaturally tall are recorded, the majority of tales that describe her height state the banshee's stature as short, anywhere between one foot and four feet. Her exceptional shortness often goes alongside the description of her as an old woman, though it may also be intended to emphasize her state as a fairy creature.

Sometimes the banshee assumes the form of some sweet singing virgin of the family who died young, and has been given the mission by the invisible powers to become the harbinger of coming doom to her mortal kindred. Or she may be seen at night as a shrouded woman, crouched beneath the trees, lamenting with veiled face, or flying past in the moonlight, crying bitterly. The cry of this spirit is mournful beyond all other sounds on earth, and betokens certain death to some member of the family whenever it is heard in the silence of the night.

In Ireland and parts of Scotland, a traditional part of mourning is the keening woman ("bean chaointe"), who wails a lament—in , (Munster dialect), (Connaught dialect) or (Ulster dialect), "caoin" meaning "to weep, to wail". This keening woman may in some cases be a professional, and the best keeners would be in high demand.

Irish legend speaks of a lament being sung by a fairy woman, or banshee. She would sing it when a family member died or was about to die, even if the person had died far away and news of their death had not yet come. In those cases, her wailing would be the first warning the household had of the death.

The banshee also is a predictor of death. If someone is about to enter a situation where it is unlikely they will come out alive she will warn people by screaming or wailing, giving rise to a banshee also being known as a wailing woman.

It is often stated that the banshee laments only the descendants of the pure Milesian stock of Ireland, sometimes clarified as surnames prefixed with O' and Mac, and some accounts even state that each family has its own banshee. One account, however, also included the Geraldines, as they had apparently become "more Irish than the Irish themselves," countering the lore ascribing banshees exclusively to those of Milesian stock. Another exception was the Rossmore banshee which supposedly heralded the death of a member of the family of Baron Rossmore, whose ancestry was predominantly Scottish and Dutch.

When several banshees appear at once, it indicates the death of someone great or holy. The tales sometimes recounted that the woman, though called a fairy, was a ghost, often of a specific murdered woman, or a mother who died in childbirth.

Most, though not all, surnames associated with banshees have the "Ó" or "Mc/Mac" prefix - that is, surnames of Goidelic origin, indicating a family native to the Insular Celtic lands rather than those of the Norse, English, or Norman. Accounts reach as far back as 1380 to the publication of the "Cathreim Thoirdhealbhaigh" ("Triumphs of Torlough") by Sean mac Craith. Mentions of banshees can also be found in Norman literature of that time.

The Ua Briain banshee is thought to be named Aibell and the ruler of 25 other banshees who would always be at her attendance. It is possible that this particular story is the source of the idea that the wailing of numerous banshees signifies the death of a great person.

In some parts of Leinster, she is referred to as the "bean chaointe" (keening woman) whose wail can be so piercing that it shatters glass. In Scottish folklore, a similar creature is known as the bean nighe or "ban nigheachain" (little washerwoman) or "nigheag na h-àth" (little washer at the ford) and is seen washing the bloodstained clothes or armour of those who are about to die. In Welsh folklore, a similar creature is known as the cyhyraeth.

Banshees, or creatures based upon them, have appeared in many forms in popular culture.



</doc>
<doc id="4514" url="https://en.wikipedia.org/wiki?curid=4514" title="Genetically modified maize">
Genetically modified maize

Genetically modified maize (corn) is a genetically modified crop. Specific maize strains have been genetically engineered to express agriculturally-desirable traits, including resistance to pests and to herbicides. Maize strains with both traits are now in use in multiple countries. GM maize has also caused controversy with respect to possible health effects, impact on other insects and impact on other plants via gene flow. One strain, called Starlink, was approved only for animal feed in the US but was found in food, leading to a series of recalls starting in 2000.

Corn varieties resistant to glyphosate herbicides were first commercialized in 1996 by Monsanto, and are known as "Roundup Ready Corn". They tolerate the use of Roundup. Bayer CropScience developed "Liberty Link Corn" that is resistant to glufosinate. Pioneer Hi-Bred has developed and markets corn hybrids with tolerance to imidazoline herbicides under the trademark "Clearfield" – though in these hybrids, the herbicide-tolerance trait was bred using tissue culture selection and the chemical mutagen ethyl methanesulfonate, not genetic engineering. Consequently, the regulatory framework governing the approval of transgenic crops does not apply for Clearfield.

As of 2011, herbicide-resistant GM corn was grown in 14 countries. By 2012, 26 varieties of herbicide-resistant GM maize were authorised for import into the European Union., but such imports remain controversial. Cultivation of herbicide-resistant corn in the EU provides substantial farm-level benefits.

Bt corn is a variant of maize that has been genetically altered to express one or more proteins from the bacterium "Bacillus thuringiensis" including Delta endotoxins. The protein is poisonous to certain insect pests. Spores of the bacillus are widely used in organic gardening, although GM corn is not considered organic. The European corn borer causes about a billion dollars in damage to corn crops each year.

In recent years, traits have been added to ward off corn ear worms and root worms, the latter of which annually causes about a billion dollars in damages.

The Bt protein is expressed throughout the plant. When a vulnerable insect eats the Bt-containing plant, the protein is activated in its gut, which is alkaline. In the alkaline environment, the protein partially unfolds and is cut by other proteins, forming a toxin that paralyzes the insect's digestive system and forms holes in the gut wall. The insect stops eating within a few hours and eventually starves.

In 1996, the first GM maize producing a Bt Cry protein was approved, which killed the European corn borer and related species; subsequent Bt genes were introduced that killed corn rootworm larvae.

Approved Bt genes include single and stacked (event names bracketed) configurations of: Cry1A.105 (MON89034), CryIAb (MON810), CryIF (1507), Cry2Ab (MON89034), Cry3Bb1 (MON863 and MON88017), Cry34Ab1 (59122), Cry35Ab1 (59122), mCry3A (MIR604), and Vip3A (MIR162), in both corn and cotton. Corn genetically modified to produce VIP was first approved in the US in 2010.

In 2018 a study found that Bt-corn protected nearby fields of non-Bt corn and nearby vegetable crops, reducing the use of pesticides on those crops. Data from 1976-1996 (before Bt corn was widespread) was compared to data after it was adopted (1996-2016). They examined levels of the European corn borer and corn earworm. Their larvae eat a variety of crops, including peppers and green beans. Between 1992 and 2016, the amount of insecticide applied to New Jersey pepper fields decreased by 85 percent. Another factor was the introduction of more effective pesticides that were applied less often.

GM sweet corn varieties include "Attribute", the brand name for insect-resistant sweet corn developed by Syngenta and Performance Series™ insect-resistant sweet corn developed by Monsanto.

In 2013 Monsanto launched the first transgenic drought tolerance trait in a line of corn hybrids called DroughtGard. The MON 87460 trait is provided by the insertion of the cspB gene from the soil microbe "Bacillus subtilis"; it was approved by the USDA in 2011 and by China in 2013.

Research has been done on adding a single "E. coli" gene to maize to enable it to be grown with an essential amino acid (methionine).

In regular corn crops, insects promote fungal colonization by creating "wounds," or holes, in corn kernels. These wounds are favored by fungal spores for germination, which subsequently leads to mycotoxin accumulation in the crop that can be carcinogenic and toxic to humans and other animals. This can prove to be especially devastating in developing countries with drastic climate patterns such as high temperatures, which favor the development of toxic fungi. In addition, higher mycotoxin levels leads to market rejection or reduced market prices for the grain. GM corn crops encounter fewer insect attacks, and thus, have lower concentrations of mycotoxins. Fewer insect attacks also keep corn ears from being damaged, which increases overall yields.

In 2007, South African researchers announced the production of transgenic maize resistant to maize streak virus (MSV), although it has not been released as a product.

While breeding cultivars for resistance to MSV isn't done in the public, the private sector, international research centers, and national programmes have done all of the breeding.

As of 2014, there have been a few MSV-tolerant cultivars released in Africa. A private company Seedco has released 5 MSV cultivars.

US Environmental Protection Agency (EPA) regulations require farmers who plant Bt corn to plant non-Bt corn nearby (called a refuge) to provide a location to harbor vulnerable pests. Typically, 20% of corn in a grower's fields must be refuge; refuge must be at least 0.5 miles from Bt corn for lepidopteran pests, and refuge for corn rootworm must at least be adjacent to a Bt field.

The theory behind these refuges is to slow the evolution of resistance to the pesticide. EPA regulations also require seed companies to train farmers how to maintain refuges, to collect data on the refuges and to report that data to the EPA. A study of these reports found that from 2003 to 2005 farmer compliance with keeping refuges was above 90%, but that by 2008 approximately 25% of Bt corn farmers did not keep refuges properly, raising concerns that resistance would develop.

Unmodified crops received most of the economic benefits of Bt corn in the US in 1996–2007, because of the overall reduction of pest populations. This reduction came because females laid eggs on modified and unmodified strains alike.

Seed bags containing both Bt and refuge seed have been approved by the EPA in the United States. These seed mixtures were marketed as "Refuge in a Bag" (RIB) to increase farmer compliance with refuge requirements and reduce additional work needed at planting from having separate Bt and refuge seed bags on hand. The EPA approved a lower percentage of refuge seed in these seed mixtures ranging from 5 to 10%. This strategy is likely to reduce the likelihood of Bt-resistance occurring for corn rootworm, but may increase the risk of resistance for lepidopteran pests, such as European corn borer. Increased concerns for resistance with seed mixtures include partially resistant larvae on a Bt plant being able to move to a susceptible plant to survive or cross pollination of refuge pollen on to Bt plants that can lower the amount of Bt expressed in kernels for ear feeding insects.

Resistant strains of the European corn borer have developed in areas with defective or absent refuge management.

In November 2009, Monsanto scientists found the pink bollworm had become resistant to first-generation Bt cotton in parts of Gujarat, India – that generation expresses one Bt gene, "Cry1Ac". This was the first instance of Bt resistance confirmed by Monsanto anywhere in the world. Bollworm resistance to first generation Bt cotton has been identified in Australia, China, Spain, and the United States. In 2012, a Florida field trial demonstrated that army worms were resistant to pesticide-containing GM corn produced by Dupont-Dow; armyworm resistance was first discovered in Puerto Rico in 2006, prompting Dow and DuPont to voluntarily stop selling the product on the island.

Regulation of GM crops varies between countries, with some of the most-marked differences occurring between the US and Europe. Regulation varies in a given country depending on intended uses.

There is a scientific consensus that currently available food derived from GM crops poses no greater risk to human health than conventional food, but that each GM food needs to be tested on a case-by-case basis before introduction. Nonetheless, members of the public are much less likely than scientists to perceive GM foods as safe. The legal and regulatory status of GM foods varies by country, with some nations banning or restricting them, and others permitting them with widely differing degrees of regulation.

The scientific rigor of the studies regarding human health has been disputed due to alleged lack of independence and due to conflicts of interest involving governing bodies and some of those who perform and evaluate the studies.

GM crops provide a number of ecological benefits, but there are also concerns for their overuse, stalled research outside of the Bt seed industry, proper management and issues with Bt resistance arising from their misuse.

Critics have objected to GM crops on ecological, economic and health grounds. The economic issues derive from those organisms that are subject to intellectual property law, mostly patents. The first generation of GM crops lose patent protection beginning in 2015. Monsanto has claimed it will not pursue farmers who retain seeds of off-patent varieties. These controversies have led to litigation, international trade disputes, protests and to restrictive legislation in most countries.

Critics claim that Bt proteins could target predatory and other beneficial or harmless insects as well as the targeted pest. These proteins have been used as organic sprays for insect control in France since 1938 and the USA since 1958 with no ill effects on the environment reported. While "cyt" proteins are toxic towards the insect order Diptera (flies), certain "cry" proteins selectively target lepidopterans (moths and butterflies), while other "cyt" selectively target Coleoptera. As a toxic mechanism, "cry" proteins bind to specific receptors on the membranes of mid-gut (epithelial) cells, resulting in rupture of those cells. Any organism that lacks the appropriate gut receptors cannot be affected by the "cry" protein, and therefore Bt. Regulatory agencies assess the potential for the transgenic plant to impact nontarget organisms before approving commercial release.

A 1999 study found that in a lab environment, pollen from Bt maize dusted onto milkweed could harm the monarch butterfly. Several groups later studied the phenomenon in both the field and the laboratory, resulting in a risk assessment that concluded that any risk posed by the corn to butterfly populations under real-world conditions was negligible. A 2002 review of the scientific literature concluded that "the commercial large-scale cultivation of current Bt–maize hybrids did not pose a significant risk to the monarch population". A 2007 review found that "nontarget invertebrates are generally more abundant in Bt cotton and Bt maize fields than in nontransgenic fields managed with insecticides. However, in comparison with insecticide-free control fields, certain nontarget taxa are less abundant in Bt fields."

Gene flow is the transfer of genes and/or alleles from one species to another. Concerns focus on the interaction between GM and other maize varieties in Mexico, and of gene flow into refuges.

In 2009 the government of Mexico created a regulatory pathway for genetically modified maize, but because Mexico is the center of diversity for maize, gene flow could affect a large fraction of the world's maize strains. A 2001 report in "Nature" presented evidence that Bt maize was cross-breeding with unmodified maize in Mexico. The data in this paper was later described as originating from an artifact. "Nature" later stated, "the evidence available is not sufficient to justify the publication of the original paper". A 2005 large-scale study failed to find any evidence of contamination in Oaxaca. However, other authors also found evidence of cross-breeding between natural maize and transgenic maize.

A 2004 study found Bt protein in kernels of refuge corn.

In 2017, a large-scale study found "pervasive presence of transgenes and glyphosate in maize-derived food in Mexico"

The French High Council of Biotechnologies Scientific Committee reviewed the 2009 Vendômois "et al." study and concluded that it "..presents no admissible scientific element likely to ascribe any haematological, hepatic or renal toxicity to the three re-analysed GMOs." However, the French government applies the precautionary principle with respect to GMOs.

A review by Food Standards Australia New Zealand and others of the same study concluded that the results were due to chance alone.

A 2011 Canadian study looked at the presence of CryAb1 protein (BT toxin) in non-pregnant women, pregnant women and fetal blood. All groups had detectable levels of the protein, including 93% of pregnant women and 80% of fetuses at concentrations of 0.19 ± 0.30 and 0.04 ± 0.04 mean ± SD ng/ml, respectively. The paper did not discuss safety implications or find any health problems. The paper was found to be unconvincing by multiple authors and organizations. In a swine model, Cry1Ab-specific antibodies were not detected in pregnant sows or their offspring and no negative effects from feeding Bt maize to pregnant sows were observed.

In January 2013, the European Food Safety Authority released all data submitted by Monsanto in relation to the 2003 authorisation of maize genetically modified for glyphosate tolerance.

StarLink contains Cry9C, which had not previously been used in a GM crop. Starlink's creator, Plant Genetic Systems had applied to the US Environmental Protection Agency (EPA) to market Starlink for use in animal feed and in human food. However, because the Cry9C protein lasts longer in the digestive system than other Bt proteins, the EPA had concerns about its allergenicity, and PGS did not provide sufficient data to prove that Cry9C was not allergenic. As a result, PGS split its application into separate permits for use in food and use in animal feed. Starlink was approved by the EPA for use in animal feed only in May 1998.

StarLink corn was subsequently found in food destined for consumption by humans in the US, Japan, and South Korea. This corn became the subject of the widely publicized Starlink corn recall, which started when Taco Bell-branded taco shells sold in supermarkets were found to contain the corn. Sales of StarLink seed were discontinued. The registration for Starlink varieties was voluntarily withdrawn by Aventis in October 2000. Pioneer had been bought by AgrEvo which then became Aventis CropScience at the time of the incident, which was later bought by Bayer.

Fifty-one people reported adverse effects to the FDA; US Centers for Disease Control (CDC), which determined that 28 of them were possibly related to Starlink. However, the CDC studied the blood of these 28 individuals and concluded there was no evidence of hypersensitivity to the Starlink Bt protein.

A subsequent review of these tests by the Federal Insecticide, Fungicide, and Rodenticide Act Scientific Advisory Panel points out that while "the negative results decrease the probability that the Cry9C protein is the cause of allergic symptoms in the individuals examined ... in the absence of a positive control and questions regarding the sensitivity and specificity of the assay, it is not possible to assign a negative predictive value to this."

The US corn supply has been monitored for the presence of the Starlink Bt proteins since 2001.

In 2005, aid sent by the UN and the US to Central American nations also contained some StarLink corn. The nations involved, Nicaragua, Honduras, El Salvador and Guatemala refused to accept the aid.

On 19 December 2013 six Chinese citizens were indicted in Iowa on charges of plotting to steal genetically modified seeds worth tens of millions of dollars from Monsanto and DuPont. Mo Hailong, director of international business at the Beijing Dabeinong Technology Group Co., part of the Beijing-based DBN Group, was accused of stealing trade secrets after he was found digging in an Iowa cornfield.




</doc>
<doc id="4516" url="https://en.wikipedia.org/wiki?curid=4516" title="Body substance isolation">
Body substance isolation

Body substance isolation is a practice of isolating all body substances (blood, urine, feces, tears, etc.) of individuals undergoing medical treatment, particularly emergency medical treatment of those who might be infected with illnesses such as HIV, or hepatitis so as to reduce as much as possible the chances of transmitting these illnesses. BSI is similar in nature to universal precautions, but goes further in isolating workers from pathogens, including substances now known to carry HIV.

Practice of Universal precautions was introduced in 1985–88. In 1987, the practice of Universal precautions was adjusted by a set of rules known as body substance isolation. In 1996, both practices were replaced by the latest approach known as standard precautions (health care). Nowadays and in isolation, practice of body substance isolation has just historical significance.

Body substance isolation went further than universal precautions in isolating workers from pathogens, including substances now currently known to carry HIV. These pathogens fall into two broad categories, bloodborne (carried in the body fluids) and airborne. The practice of BSI was common in Pre-Hospital care and emergency medical services due to the often unknown nature of the patient and his/her disease or medical conditions. It was a part of the National Standards Curriculum for Prehospital Providers and Firefighters.

Types of body substance isolation included:

It was postulated that BSI precautions should be practiced in environment where treaters were exposed to bodily fluids, such as:

Such infection control techniques that were recommended following the AIDS outbreak in the 1980s. Every patient was treated as if infected and therefore precautions were taken to minimize risk. Other conditions which called for minimizing risks with BSI:

or any combination of the above.




</doc>
<doc id="4517" url="https://en.wikipedia.org/wiki?curid=4517" title="Boudica">
Boudica

Boudica or Boudicca (, ), also known as Boadicea (, ) or Boudicea, and in Welsh as (), was a queen of the British Celtic Iceni tribe who led an uprising against the conquering forces of the Roman Empire in AD 60 or 61. She died shortly after its failure and was said to have poisoned herself. She is considered a British folk hero.

Boudica's husband Prasutagus, with whom she had two children whose names are unknown, ruled as a nominally independent ally of Rome, and left his kingdom jointly to his daughters and to the Roman emperor in his will. However, when he died, his will was ignored, and the kingdom was annexed and his property taken. According to Tacitus, Boudica was flogged and her daughters raped. Cassius Dio explains Boudica's response by saying that previous imperial donations to influential Britons were confiscated and the Roman financier and philosopher Seneca called in the loans he had forced on the reluctant Britons.

In AD 60 or 61, when the Roman governor Gaius Suetonius Paulinus was campaigning on the island of Mona (modern Anglesey) on the northwest coast of Wales, Boudica led the Iceni, the Trinovantes, and others in revolt. They destroyed (modern Colchester), earlier the capital of the Trinovantes but at that time a , a settlement for discharged Roman soldiers and site of a temple to the former Emperor Claudius. Upon hearing of the revolt, Suetonius hurried to (modern London), the 20-year-old commercial settlement that was the rebels' next target. He lacked sufficient numbers to defend the settlement, and he evacuated and abandoned . Boudica led a very large army of Iceni, Trinovantes, and others against a detachment of , defeating them, and burning and .

An estimated 70,000–80,000 Romans and British were then killed in the three cities by those following Boudica, many by torture. Suetonius, meanwhile, regrouped his forces, possibly in the West Midlands; despite being heavily outnumbered, he decisively defeated the Britons. The crisis caused Nero to consider withdrawing all Roman forces from Britain, but Suetonius's victory over Boudica confirmed Roman control of the province. Boudica then either killed herself to avoid capture (according to Tacitus), or died of illness (according to Cassius Dio).

Interest in these events was revived in the English Renaissance and led to Boudica's fame in the Victorian era. Boudica has remained an important cultural symbol in the United Kingdom.

Boudica has been known by several versions of her name. In the 16th century, Raphael Holinshed called her Voadicia, while Edmund Spenser calls her Bunduca, a variation of which was used in the popular Jacobean play "Bonduca" of 1612. In the 18th century, William Cowper's poem "Boadicea, an ode" (1782) popularised an alternative version of the name. From the 19th century until the late 20th century, Boadicea was the most common version of the name, which is probably derived from a mistranscription when a manuscript of Tacitus was copied in the Middle Ages.

Her name was clearly spelled Boudicca in the best manuscripts of Tacitus. In the later, and probably secondary, epitome of Cassius Dio in Greek she was , , and .

Kenneth Jackson concludes, based on the later development in Welsh () and Irish (), that the name derives from the Proto-Celtic feminine adjective "*boudīkā" 'victorious', that in turn is derived from the Celtic word "*boudā" 'victory' (cf. Irish (Classical Irish ) 'victory', Scottish Gaelic 'victorious; effective', Welsh , 'victorious', 'victory'), and that the correct spelling of the name in Common Brittonic (the British Celtic language) is "Boudica", pronounced . The Gaulish version is attested in inscriptions as Boudiga in Bordeaux, Boudica in Lusitania, and Bodicca in Algeria.

The closest English equivalent to the vowel in the first syllable is the "ow" in "bow-and-arrow". John Rhys suggested that the most comparable Latin name, in meaning only, would be "Victorina".

Tacitus took a particular interest in Britain as his father-in-law Gnaeus Julius Agricola served there three times (and was the subject of his first book). Agricola was a military tribune under Suetonius Paulinus, which almost certainly gave Tacitus an eyewitness source for Boudica's revolt. Cassius Dio's account is only known from an epitome, and his sources are uncertain. He is generally agreed to have based his account on that of Tacitus, but he simplifies the sequence of events and adds details, such as the calling in of loans, that Tacitus does not mention.

It is agreed that Boudica was of royal descent. Cassius Dio describes her as tall, with tawny hair hanging down to below her waist, a harsh voice and a piercing glare. He writes that she habitually wore a large golden necklace (perhaps a torc), a colourful tunic, and a thick cloak fastened by a brooch.

Boudica's husband, Prasutagus, was the king of the Iceni, a people who inhabited roughly what is now Norfolk. During Claudius's conquest of southern Britain in AD 43, the Iceni initially allied with Rome. They were proud of their independence, and had revolted in AD 47 when the then Roman governor Publius Ostorius Scapula planned to disarm all the peoples in the area of Britain under Roman control following a number of local uprisings. Ostorius defeated them and went on to put down other uprisings around Britain. The Iceni remained independent under Prasutagus. It is unknown whether he became the king only after Ostorius's defeat of the Iceni; Tacitus does not date the start of Prasutagus's reign and first mentioned him, as a long-reigning king who had died, when he wrote about Boudica's rebellion.

Tacitus mentions longstanding reasons for the Trinovantes to hate Rome: "It was against the veterans that their hatred was most intense. For these new settlers in the colony of Camulodunum drove people out of their houses, ejected them from their farms, called them captives and slaves ..."

The immediate cause of the rebellion was gross mistreatment by the Romans. Tacitus wrote,"The Icenian king Prasutagus, celebrated for his long prosperity, had named the emperor his heir, together with his two daughters; an act of deference which he thought would place his kingdom and household beyond the risk of injury. The result was contrary – so much so that his kingdom was pillaged by centurions, his household by slaves; as though they had been prizes of war." He added that Boudica was lashed, her two daughters were raped, and that the estates of the leading Iceni men were confiscated. Cassius Dio wrote: "An excuse for the war was found in the confiscation of the sums of money that Claudius had given to the foremost Britons; for these sums, as Decianus Catus, the procurator of the island maintained, were to be paid back." He also said that another reason was "the fact that Seneca, in the hope of receiving a good rate of interest, had lent to the islanders 40,000,000 sesterces that they did not want, and had afterwards called in this loan all at once and had resorted to severe measures in exacting it."

In AD 60 or 61, while the current governor, Gaius Suetonius Paulinus, was leading a campaign against the island of (modern Anglesey) in the north of Wales, which was a refuge for British rebels and a stronghold of the druids, the Iceni conspired with their neighbours the Trinovantes, amongst others, to revolt. Boudica was chosen as their leader. Tacitus records that she addressed her army with these words, "It is not as a woman descended from noble ancestry, but as one of the people that I am avenging lost freedom, my scourged body, the outraged chastity of my daughters," and concluded, "This is a woman's resolve; as for men, they may live and be slaves." According to Tacitus, they drew inspiration from the example of Arminius, the prince of the Cherusci who had driven the Romans out of Germany in AD 9, and their own ancestors who had driven Julius Caesar from Britain. Dio says that at the outset Boudica employed a form of divination, releasing a hare from the folds of her dress and interpreting the direction in which it ran, and invoked Andraste, a British goddess of victory.
The rebels' first target was (modern Colchester), the former Trinovantian capital and, at that time, a Roman . The Roman veterans who had been settled there had mistreated the locals, and a temple to the former emperor Claudius had been erected there at local expense, making the city a focus for resentment. The Roman inhabitants sought reinforcements from the procurator, Catus Decianus, but he sent only two hundred auxiliary troops. Boudica's army fell on the poorly defended city and destroyed it, besieging the last defenders in the temple for two days before it fell. A bronze statue to the emperor Nero, which probably stood in front of the temple, was decapitated and its head taken as a trophy by Boudica's army Archaeologists have shown that the city was methodically demolished. The future governor Quintus Petillius Cerialis, then commanding the Legio IX "Hispana", attempted to relieve the city, but suffered an overwhelming defeat. The infantry with him were all killed – only the commander and some of his cavalry escaped. "The victorious enemy met Petilius Cerialis, commander of the ninth legion, as he was coming to the rescue, routed his troops, and destroyed all his infantry. Cerialis escaped with some cavalry into the camp, and was saved by its fortifications." The location of this battle is unknown, but has been claimed by some modern localities. After this defeat, Catus Decianus fled to Gaul.

When news of the rebellion reached Suetonius, he hurried along Watling Street through hostile territory to . was a relatively new settlement, founded after the conquest of AD 43, but it had grown to be a thriving commercial centre with a population of traders, and, probably, Roman officials. Suetonius considered giving battle there, but considering his lack of numbers and chastened by Petillius's defeat, decided to sacrifice the city to save the province.

Alarmed by this disaster and by the fury of the province which he had goaded into war by his rapacity, the procurator Catus crossed over into Gaul. Suetonius, however, with wonderful resolution, marched amidst a hostile population to Londinium, which, though undistinguished by the name of a colony, was much frequented by a number of merchants and trading vessels. Uncertain whether he should choose it as a seat of war, as he looked round on his scanty force of soldiers, and remembered with what a serious warning the rashness of Petillius had been punished, he resolved to save the province at the cost of a single town. Nor did the tears and weeping of the people, as they implored his aid, deter him from giving the signal of departure and receiving into his army all who would go with him. Those who were chained to the spot by the weakness of their sex, or the infirmity of age, or the attractions of the place, were cut off by the enemy.

Londinium was abandoned to the rebels, who burnt it down, torturing and killing anyone who had not evacuated with Suetonius. Archaeology shows a thick red layer of burnt debris covering coins and pottery dating before AD 60 within the bounds of Roman Londinium; while Roman-era skulls found in the Walbrook in 2013 may have been victims of the rebels. (St Albans) was next to be destroyed.

In the three settlements destroyed, between seventy and eighty thousand people are said to have been killed. Tacitus says that the Britons had no interest in taking or selling prisoners, only in slaughter by gibbet, fire, or cross. Dio's account gives more detail; that the noblest women were impaled on spikes and had their breasts cut off and sewn to their mouths, "to the accompaniment of sacrifices, banquets, and wanton behaviour" in sacred places, particularly the groves of Andraste.

While Boudica's army continued their assault in Verulamium (St. Albans), Suetonius regrouped his forces. According to Tacitus, he amassed a force including his own Legio XIV "Gemina", some "vexillationes" (detachments) of the XX "Valeria Victrix", and any available auxiliaries. The prefect of , Poenius Postumus, ignored the call, and a fourth legion, , had been routed trying to relieve , but nonetheless the governor now commanded an army of almost ten thousand men.

Suetonius took a stand at an unidentified location, probably somewhere along the Roman road now known as Watling Street, in a defile with a wood behind him – but his men were heavily outnumbered. Dio says that, even if they were lined up one deep, they would not have extended the length of Boudica's line. By now the rebel forces were said to have numbered 230,000–300,000. However, this number should be treated with scepticism – Dio's account is known only from a late epitome.

Boudica exhorted her troops from her chariot, her daughters beside her. Tacitus records her giving a short speech in which she presents herself not as an aristocrat avenging her lost wealth, but as an ordinary person, avenging her lost freedom, her battered body, and the abused chastity of her daughters. She said their cause was just, and the deities were on their side; the one legion that had dared to face them had been destroyed. She, a woman, was resolved to win or die; if the men wanted to live in slavery, that was their choice.

At first, the legionaries stood motionless, keeping to the defile as a natural protection: then, when the closer advance of the enemy had enabled them to exhaust their missiles with certitude of aim, they dashed forward in a wedge-like formation. The auxiliaries charged in the same style; and the cavalry, with lances extended, broke a way through any parties of resolute men whom they encountered. The remainder took to flight, although escape was difficult, as the cordon of wagons had blocked the outlets. The troops gave no quarter even to the women: the baggage animals themselves had been speared and added to the pile of bodies. The glory won in the course of the day was remarkable, and equal to that of our older victories: for, by some accounts, little less than eighty thousand Britons fell, at a cost of some four hundred Romans killed and a not much greater number of wounded. Boudica ended her days by poison; while Poenius Postumus, camp-prefect of the second legion, informed of the exploits of the men of the fourteenth and twentieth, and conscious that he had cheated his own corps of a share in the honours and had violated the rules of the service by ignoring the orders of his commander, ran his sword through his body.

The Roman slaughter of women and animals was unusual, as they could have been sold for profit, and point to the mutual enmity between the two sides. According to Tacitus in his "Annals", Boudica poisoned herself, though in the which was written almost twenty years before the "Annals" he mentions nothing of suicide and attributes the end of the revolt to ("indolence"); Dio says she fell sick and died and then was given a lavish burial.

Catus Decianus, who had fled to Gaul, was replaced by Gaius Julius Alpinus Classicianus. Suetonius conducted punitive operations, but criticism by Classicianus led to an investigation headed by Nero's freedman Polyclitus. Fearing Suetonius's actions would provoke further rebellion, Nero replaced the governor with the more conciliatory Publius Petronius Turpilianus. The historian Gaius Suetonius Tranquillus tells us the crisis had almost persuaded Nero to abandon Britain. No historical records tell what had happened to Boudica's two daughters.

The location of Boudica's defeat is unknown. Some historians favour a site somewhere along the Roman road now known as Watling Street. Kevin K. Carroll suggests a site close to High Cross, Leicestershire, on the junction of Watling Street and the Fosse Way, which would have allowed the Legio II Augusta, based at Exeter, to rendezvous with the rest of Suetonius's forces, had they not failed to do so. Manduessedum (Mancetter), near the modern town of Atherstone in Warwickshire, has also been suggested, and according to legend "The Rampart" near Messing, Essex and Ambresbury Banks in Epping Forest. More recently, a discovery of Roman artefacts in Kings Norton close to Metchley Camp has suggested another possibility, one individual has suggested the Cuttle Mill area of Paulerspury in Northamptonshire, where fragments of Roman pottery from the 1st century have been found.

In 2009, it was suggested that the Iceni were returning to East Anglia along the Icknield Way when they encountered the Roman army in the vicinity of Arbury Banks, Hertfordshire. In March 2010, evidence was published suggesting the site may be located at Church Stowe, Northamptonshire.

One of the earliest possible mentions of Boudica (excluding Tacitus' and Dio's accounts) was the 6th-century work "On the Ruin and Conquest of Britain" by the British monk Glidas. In it, he demonstrates his knowledge of a female leader whom he describes as a "treacherous lionness" who "butchered the governors who had been left to give fuller voice and strength to the endeavours of Roman rule." It is likely that Gildas is referring to Boudica in this statement. Polydore Vergil may have reintroduced her to British history as "Voadicea" in 1534. Raphael Holinshed also included her story in his "Chronicles" (1577) based on Tacitus and Dio.

During the reign of Elizabeth I, Boudica began to be seen as an important figure in British history. During the reign of Queen Elizabeth I, the works of Tacitus were rediscovered, and therefore interest in Boudica and her rebellion was triggered. It has been said that the Elizabethan era was a time where her popularity could flourish as Elizabeth, in 1588, was required to defend Britain from a possible invasion of Spanish Armada. Boudica had once defended Britain as well, however from the Romans. In 1610, Shakespeare's younger contemporaries Francis Beaumont and John Fletcher to write a play, "Bonduca", said to have been inspired by Holinshed's "Chronicles." William Cowper wrote a popular poem, "Boadicea, an ode", in 1782.

It was in the Victorian era that Boudica's fame took on legendary proportions as Queen Victoria came to be seen as Boudica's "namesake", their names being identical in meaning. Victoria's Poet Laureate, Alfred, Lord Tennyson, wrote a poem, "Boadicea", and several ships were named after her. "Boadicea and Her Daughters", a statue of the queen in her war chariot (anachronistically furnished with scythes after the Persian fashion) was executed by Thomas Thornycroft over the 1850s and 1860s with the encouragement of Prince Albert, who lent his horses for use as models. Thornycroft exhibited the head separately in 1864. It was cast in bronze in 1902, 17 years after Thornycroft's death, by his son Sir John, who presented it to the London County Council. They erected it on a plinth on the Victoria Embankment next to Westminster Bridge and the Houses of Parliament, inscribed with the following lines from Cowper's poem:<poem>
Regions Caesar never knew
Thy posterity shall sway.
</poem>

A statue of her now stands guard over the city she razed to the ground. The area of King's Cross, London was previously a village known as Battle Bridge which was an ancient crossing of the River Fleet. The original name of the bridge was Broad Ford Bridge. The name "Battle Bridge" led to a tradition that this was the site of a major battle between the Romans and the Iceni tribe led by Boudica. The tradition is not supported by any historical evidence and is rejected by modern historians. However, Lewis Spence's 1937 book "Boadicea – warrior queen of the Britons" went so far as to include a map showing the positions of the opposing armies. There is a belief that she was buried between platforms 9 and 10 in King's Cross station in London, England. There is no evidence for this and it is probably a post-World War II invention.

"Buddug" has yet to be conclusively identified within the canon of medieval Welsh literature and she is not apparent in the , the or Geoffrey of Monmouth's "History of the Kings of Britain".

Boudica (Buddug) was also chosen by the Welsh public as one of eleven statues of historical figures to be included in the Marble Hall at Cardiff City Hall. The statue was unveiled by David Lloyd George on 27 October 1916. Unlike the London chariot statue, it shows her as a more motherly figure without warrior trappings. The popularity of Buddug alongside other Welsh heroes such as Saint David and Owain Glyndŵr was surprising to manyof the statues, Buddug is the most ancient, the only female, and the only antecedent from outside the modern Welsh nation.




</doc>
