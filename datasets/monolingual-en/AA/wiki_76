<doc id="9469" url="https://en.wikipedia.org/wiki?curid=9469" title="Eric S. Raymond">
Eric S. Raymond

Eric Steven Raymond (born December 4, 1957), often referred to as ESR, is an American software developer, open-source software advocate, and author of the 1997 essay and 1999 book "The Cathedral and the Bazaar". He wrote a guidebook for the Roguelike game "NetHack". In the 1990s, he edited and updated the Jargon File, currently in print as "The New Hacker's Dictionary".

Raymond was born in Boston, Massachusetts, in 1957 and lived in Venezuela as a child. His family moved to Pennsylvania in 1971. He developed cerebral palsy at birth; his weakened physical condition motivated him to go into computing.

Raymond began his programming career writing proprietary software, between 1980 and 1985. In 1990, noting that the Jargon File had not been maintained since about 1983, he adopted it; he currently has a third edition in print. Paul Dourish maintains an archived original version of the Jargon File, because, he says, Raymond's updates "essentially destroyed what held it together."

In 1996 Raymond took over development of the open-source email software "popclient", renaming it to Fetchmail. Soon after this experience, in 1997, he wrote the essay "The Cathedral and the Bazaar", detailing his thoughts on open-source software development and why it should be done as openly as possible (the "bazaar" approach). The essay was based in part on his experience in developing Fetchmail. He first presented his thesis at the annual Linux Kongress on May 27, 1997. He later expanded the essay into a book, "The Cathedral and the Bazaar: Musings on Linux and Open Source by an Accidental Revolutionary", in 1999. The essay has been widely cited. The internal white paper by Frank Hecker that led to the release of the Mozilla (then Netscape) source code in 1998 cited "The Cathedral and the Bazaar" as "independent validation" of ideas proposed by Eric Hahn and Jamie Zawinski. Hahn would later describe the 1999 book as "clearly influential".

From the late 1990s onward, due in part to the popularity of his essay, Raymond became a prominent voice in the open source movement. He co-founded the Open Source Initiative in 1998, taking on the self-appointed role of ambassador of open source to the press, business and public. He remains active in OSI, and stepped down as president of the initiative in February 2005. In early March 2020, Raymond was removed from two mailing lists of the Open Source Initiative, due to a post made by him that violating the OSI's Code of Conduct by writing "...not just 'No' but 'To hell with you *and* the horse you rode in on' ". 

In 1998 Raymond received and published a Microsoft document expressing worry about the quality of rival open-source software. Raymond named this document, together with others subsequently leaked, ""The Halloween Documents"".

In 2000–2002 he created Configuration Menu Language 2 (CML2), a source code configuration system; while originally intended for the Linux operating system, it was rejected by kernel developers. Raymond attributed this rejection to "kernel list politics". Linus Torvalds on the other hand said in a 2007 mailing list post that as a matter of policy, the development team preferred more incremental changes. His 2003 book "The Art of Unix Programming" discusses user tools for programming and other tasks.

Some versions of NetHack still include his guide. He has also contributed code and content to the free software video game "The Battle for Wesnoth".

Raymond is also the main developer on NTPSec, a "secure, hardened replacement" for the Unix utility NTP.

Raymond coined an aphorism he dubbed Linus's law, inspired by Linus Torvalds: "Given enough eyeballs, all bugs are shallow". It first appeared in his book "The Cathedral and the Bazaar".

Raymond has refused to speculate on whether the "bazaar" development model could be applied to works such as books and music, saying that he does not want to "weaken the winning argument for open-sourcing software by tying it to a potential loser".

Raymond has had a number of public disputes with other figures in the free software movement. As head of the Open Source Initiative, he argued that advocates should focus on the potential for better products. The "very seductive" moral and ethical rhetoric of Richard Stallman and the Free Software Foundation fails, he said, "not because his principles are wrong, but because that kind of language ... simply does not persuade anybody".

In a 2008 essay he "defended the right of programmers to issue work under proprietary licenses because I think that if a programmer wants to write a program and sell it, it's neither my business nor anyone else's but his customer's what the terms of sale are". In the same essay he also said that the "logic of the system" puts developers into "dysfunctional roles", with bad code the result.

Raymond is a member of the Libertarian Party. He is a gun rights advocate. He has endorsed the open source firearms organization Defense Distributed, calling them "friends of freedom" and writing "I approve of any development that makes it more difficult for governments and criminals to monopolize the use of force. As 3D printers become less expensive and more ubiquitous, this could be a major step in the right direction."

In 2015 Raymond accused the Ada Initiative and other women in tech groups of attempting to entrap male open source leaders and accuse them of rape, saying "Try to avoid even being alone, ever, because there is a chance that a 'women in tech' advocacy group is going to try to collect your scalp."

Raymond has claimed that "Gays experimented with unfettered promiscuity in the 1970s and got AIDS as a consequence", and that "Police who react to a random black male behaving suspiciously who might be in the critical age range as though he is an near-imminent lethal threat, are being rational, not racist." Progressive campaign The Great Slate was successful in raising funds for candidates in part by asking for contributions from tech workers in return for not posting similar quotes by Raymond. Matasano Security employee and Great Slate fundraiser Thomas Ptacek said, "I’ve been torturing Twitter with lurid Eric S. Raymond quotes for years. Every time I do, 20 people beg me to stop." It is estimated that as of March 2018 over $30,000 has been raised in this way.

Raymond describes himself as neo-pagan.







</doc>
<doc id="9471" url="https://en.wikipedia.org/wiki?curid=9471" title="Externalization">
Externalization

In Freudian psychology, externalization (or externalisation) is an unconscious defense mechanism by which an individual "projects" their own internal characteristics onto the outside world, particularly onto other people. For example, a patient who is overly argumentative might instead perceive others as argumentative and themselves as blameless.

Like other defense mechanisms, externalization is a protection against anxiety and is, therefore, part of a healthy, normally functioning mind. However, if taken to excess, it can lead to the development of a neurosis.

Externalization can also be used in the context of a corporation. A corporation that externalizes its costs onto society and the environment is not taking full responsibility and ownership of these costs. An example might be the discharge of untreated toxic waste into a river where people wash and fish.




</doc>
<doc id="9472" url="https://en.wikipedia.org/wiki?curid=9472" title="Euro">
Euro

The euro (sign: €; code: EUR) is the official currency of 19 of the member states of the European Union. This group of states is known as the eurozone or euro area and includes about 343 million citizens . The euro, which is divided into 100 cents, is the second-largest and second-most traded currency in the foreign exchange market after the United States dollar.

The currency is also used officially by the institutions of the European Union, by four European microstates that are not EU members, the British Overseas Territory of Akrotiri and Dhekelia, as well as unilaterally by Montenegro and Kosovo. Outside Europe, a number of special territories of EU members also use the euro as their currency. Additionally, over 200 million people worldwide use currencies pegged to the euro.

The euro is the second-largest reserve currency as well as the second-most traded currency in the world after the United States dollar.
, with more than €1.3 trillion in circulation, the euro has one of the highest combined values of banknotes and coins in circulation in the world.

The name "euro" was officially adopted on 16 December 1995 in Madrid. The euro was introduced to world financial markets as an accounting currency on 1 January 1999, replacing the former European Currency Unit (ECU) at a ratio of 1:1 (US$1.1743). Physical euro coins and banknotes entered into circulation on 1 January 2002, making it the day-to-day operating currency of its original members, and by March 2002 it had completely replaced the former currencies. While the euro dropped subsequently to US$0.83 within two years (26 October 2000), it has traded above the U.S. dollar since the end of 2002, peaking at US$1.60 on 18 July 2008 and since then returning near to its original issue rate. In late 2009, the euro became immersed in the European sovereign-debt crisis, which led to the creation of the European Financial Stability Facility as well as other reforms aimed at stabilising and strengthening the currency.

The euro is managed and administered by the Frankfurt-based European Central Bank (ECB) and the Eurosystem (composed of the central banks of the eurozone countries). As an independent central bank, the ECB has sole authority to set monetary policy. The Eurosystem participates in the printing, minting and distribution of notes and coins in all member states, and the operation of the eurozone payment systems.

The 1992 Maastricht Treaty obliges most EU member states to adopt the euro upon meeting certain monetary and budgetary convergence criteria, although not all states have done so. The United Kingdom and Denmark negotiated exemptions, while Sweden (which joined the EU in 1995, after the Maastricht Treaty was signed) turned down the euro in a non-binding referendum in 2003, and has circumvented the obligation to adopt the euro by not meeting the monetary and budgetary requirements. All nations that have joined the EU since 1993 have pledged to adopt the euro in due course.

Since 1 January 2002, the national central banks (NCBs) and the ECB have issued euro banknotes on a joint basis. Eurosystem NCBs are required to accept euro banknotes put into circulation by other Eurosystem members and these banknotes are not repatriated. The ECB issues 8% of the total value of banknotes issued by the Eurosystem. In practice, the ECB's banknotes are put into circulation by the NCBs, thereby incurring matching liabilities vis-à-vis the ECB. These liabilities carry interest at the main refinancing rate of the ECB. The other 92% of euro banknotes are issued by the NCBs in proportion to their respective shares of the ECB capital key, calculated using national share of European Union (EU) population and national share of EU GDP, equally weighted.

The euro is divided into 100 cents (also referred to as "euro cents", especially when distinguishing them from other currencies, and referred to as such on the common side of all cent coins). In Community legislative acts the plural forms of "euro" and "cent" are spelled without the "s", notwithstanding normal English usage. Otherwise, normal English plurals are used, with many local variations such as "centime" in France.

All circulating coins have a "common side" showing the denomination or value, and a map in the background. Due to the linguistic plurality in the European Union, the Latin alphabet version of "euro" is used (as opposed to the less common Greek or Cyrillic) and Arabic numerals (other text is used on national sides in national languages, but other text on the common side is avoided). For the denominations except the 1-, 2- and 5-cent coins, the map only showed the 15 member states which were members when the euro was introduced. Beginning in 2007 or 2008 (depending on the country), the old map was replaced by a map of Europe also showing countries outside the EU like Norway, Ukraine, Belarus, Russia and Turkey. The 1-, 2- and 5-cent coins, however, keep their old design, showing a geographical map of Europe with the 15 member states of 2002 raised somewhat above the rest of the map. All common sides were designed by Luc Luycx. The coins also have a "national side" showing an image specifically chosen by the country that issued the coin. Euro coins from any member state may be freely used in any nation that has adopted the euro.

The coins are issued in denominations of €2, €1, 50c, 20c, 10c, 5c, 2c, and 1c. To avoid the use of the two smallest coins, some cash transactions are rounded to the nearest five cents in the Netherlands and Ireland (by voluntary agreement) and in Finland (by law). This practice is discouraged by the commission, as is the practice of certain shops of refusing to accept high-value euro notes.

Commemorative coins with €2 face value have been issued with changes to the design of the national side of the coin. These include both commonly issued coins, such as the €2 commemorative coin for the fiftieth anniversary of the signing of the Treaty of Rome, and nationally issued coins, such as the coin to commemorate the 2004 Summer Olympics issued by Greece. These coins are legal tender throughout the eurozone. Collector coins with various other denominations have been issued as well, but these are not intended for general circulation, and they are legal tender only in the member state that issued them.

The design for the euro banknotes has common designs on both sides. The design was created by the Austrian designer Robert Kalina. Notes are issued in €500, €200, €100, €50, €20, €10, €5. Each banknote has its own colour and is dedicated to an artistic period of European architecture. The front of the note features windows or gateways while the back has bridges, symbolising links between states in the union and with the future. While the designs are supposed to be devoid of any identifiable characteristics, the initial designs by Robert Kalina were of specific bridges, including the Rialto and the Pont de Neuilly, and were subsequently rendered more generic; the final designs still bear very close similarities to their specific prototypes; thus they are not truly generic. The monuments looked similar enough to different national monuments to please everyone.

The Europa series, or second series, consists of six denominations and does no longer include the €500 with issuance discontinued as of 27 April 2019. However, both the first and the second series of euro banknotes, including the €500, remain legal tender throughout the euro area.

Capital within the EU may be transferred in any amount from one state to another. All intra-Union transfers in euro are treated as domestic transactions and bear the corresponding domestic transfer costs. This includes all member states of the EU, even those outside the eurozone providing the transactions are carried out in euro. Credit/debit card charging and ATM withdrawals within the eurozone are also treated as domestic transactions; however paper-based payment orders, like cheques, have not been standardised so these are still domestic-based. The ECB has also set up a clearing system, TARGET, for large euro transactions.

A special euro currency sign (€) was designed after a public survey had narrowed the original ten proposals down to two. The European Commission then chose the design created by the Belgian Alain Billiet. Of the symbol, the Commission stated

The European Commission also specified a euro logo with exact proportions and foreground and background colour tones. Placement of the currency sign relative to the numeric amount varies from state to state, but for texts in English the symbol (or the ISO-standard "EUR") should precede the amount.

The euro was established by the provisions in the 1992 Maastricht Treaty. To participate in the currency, member states are meant to meet strict criteria, such as a budget deficit of less than 3% of their GDP, a debt ratio of less than 60% of GDP (both of which were ultimately widely flouted after introduction), low inflation, and interest rates close to the EU average. In the Maastricht Treaty, the United Kingdom and Denmark were granted exemptions per their request from moving to the stage of monetary union which resulted in the introduction of the euro. (For macroeconomic theory, see below.)

The name "euro" was officially adopted in Madrid on 16 December 1995. Belgian Esperantist Germain Pirlot, a former teacher of French and history is credited with naming the new currency by sending a letter to then President of the European Commission, Jacques Santer, suggesting the name "euro" on 4 August 1995.

Due to differences in national conventions for rounding and significant digits, all conversion between the national currencies had to be carried out using the process of triangulation via the euro. The "definitive" values of one euro in terms of the exchange rates at which the currency entered the euro are shown on the right.

The rates were determined by the Council of the European Union, based on a recommendation from the European Commission based on the market rates on 31 December 1998. They were set so that one European Currency Unit (ECU) would equal one euro. The European Currency Unit was an accounting unit used by the EU, based on the currencies of the member states; it was not a currency in its own right. They could not be set earlier, because the ECU depended on the closing exchange rate of the non-euro currencies (principally the pound sterling) that day.

The procedure used to fix the conversion rate between the Greek drachma and the euro was different since the euro by then was already two years old. While the conversion rates for the initial eleven currencies were determined only hours before the euro was introduced, the conversion rate for the Greek drachma was fixed several months beforehand.

The currency was introduced in non-physical form (traveller's cheques, electronic transfers, banking, etc.) at midnight on 1 January 1999, when the national currencies of participating countries (the eurozone) ceased to exist independently. Their exchange rates were locked at fixed rates against each other. The euro thus became the successor to the European Currency Unit (ECU). The notes and coins for the old currencies, however, continued to be used as legal tender until new euro notes and coins were introduced on 1 January 2002.

The changeover period during which the former currencies' notes and coins were exchanged for those of the euro lasted about two months, until 28 February 2002. The official date on which the national currencies ceased to be legal tender varied from member state to member state. The earliest date was in Germany, where the mark officially ceased to be legal tender on 31 December 2001, though the exchange period lasted for two months more. Even after the old currencies ceased to be legal tender, they continued to be accepted by national central banks for periods ranging from several years to indefinitely (the latter for Austria, Germany, Ireland, Estonia and Latvia in banknotes and coins, and for Belgium, Luxembourg, Slovenia and Slovakia in banknotes only). The earliest coins to become non-convertible were the Portuguese escudos, which ceased to have monetary value after 31 December 2002, although banknotes remain exchangeable until 2022.

Following the U.S. financial crisis in 2008, fears of a sovereign debt crisis developed in 2009 among investors concerning some European states, with the situation becoming particularly tense in early 2010. Greece was most acutely affected, but fellow Eurozone members Cyprus, Ireland, Italy, Portugal, and Spain were also significantly affected. All these countries utilized EU funds except Italy, which is a major donor to the EFSF. To be included in the eurozone, countries had to fulfil certain convergence criteria, but the meaningfulness of such criteria was diminished by the fact it was not enforced with the same level of strictness among countries.

According to the Economist Intelligence Unit in 2011, "[I]f the [euro area] is treated as a single entity, its [economic and fiscal] position looks no worse and in some respects, rather better than that of the US or the UK" and the budget deficit for the euro area as a whole is much lower and the euro area's government debt/GDP ratio of 86% in 2010 was about the same level as that of the United States. "Moreover", they write, "private-sector indebtedness across the euro area as a whole is markedly lower than in the highly leveraged Anglo-Saxon economies". The authors conclude that the crisis "is as much political as economic" and the result of the fact that the euro area lacks the support of "institutional paraphernalia (and mutual bonds of solidarity) of a state".

The crisis continued with S&P downgrading the credit rating of nine euro-area countries, including France, then downgrading the entire European Financial Stability Facility (EFSF) fund.

A historical parallel – to 1931 when Germany was burdened with debt, unemployment and austerity while France and the United States were relatively strong creditors – gained attention in summer 2012 even as Germany received a debt-rating warning of its own. In the enduring of this scenario the Euro serves as a mean of quantitative primitive accumulation.

The euro is the sole currency of 19 EU member states: Austria, Belgium, Cyprus, Estonia, Finland, France, Germany, Greece, Ireland, Italy, Latvia, Lithuania, Luxembourg, Malta, the Netherlands, Portugal, Slovakia, Slovenia, and Spain. These countries constitute the "eurozone", some 343 million people in total .

With all but one (Denmark) of the remaining EU members obliged to join when economic conditions permit, together with future members of the EU, the enlargement of the eurozone is set to continue. Outside the EU, the euro is also the sole currency of Montenegro and Kosovo and several European microstates (Andorra, Monaco, San Marino and the Vatican City) as well as in five overseas territories of EU members that are not themselves part of the EU (Saint Barthélemy, Saint Martin, Saint Pierre and Miquelon, the French Southern and Antarctic Lands and Akrotiri and Dhekelia). Together this direct usage of the euro outside the EU affects nearly 3 million people.

The euro has been used as a trading currency in Cuba since 1998, Syria since 2006, and Venezuela since 2018. There are also various currencies pegged to the euro (see below). In 2009, Zimbabwe abandoned its local currency and used major currencies instead, including the euro and the United States dollar.

Since its introduction, the euro has been the second most widely held international reserve currency after the U.S. dollar. The share of the euro as a reserve currency increased from 18% in 1999 to 27% in 2008. Over this period, the share held in U.S. dollar fell from 71% to 64% and that held in RMB fell from 6.4% to 3.3%. The euro inherited and built on the status of the Deutsche Mark as the second most important reserve currency. The euro remains underweight as a reserve currency in advanced economies while overweight in emerging and developing economies: according to the International Monetary Fund the total of euro held as a reserve in the world at the end of 2008 was equal to $1.1 trillion or €850 billion, with a share of 22% of all currency reserves in advanced economies, but a total of 31% of all currency reserves in emerging and developing economies.

The possibility of the euro becoming the first international reserve currency has been debated among economists. Former Federal Reserve Chairman Alan Greenspan gave his opinion in September 2007 that it was "absolutely conceivable that the euro will replace the US dollar as reserve currency, or will be traded as an equally important reserve currency". In contrast to Greenspan's 2007 assessment, the euro's increase in the share of the worldwide currency reserve basket has slowed considerably since 2007 and since the beginning of the worldwide credit crunch related recession and European sovereign-debt crisis.

Outside the eurozone, a total of 22 countries and territories that do not belong to the EU have currencies that are directly pegged to the euro including 14 countries in mainland Africa (CFA franc), two African island countries (Comorian franc and Cape Verdean escudo), three French Pacific territories (CFP franc) and three Balkan countries, Bosnia and Herzegovina (Bosnia and Herzegovina convertible mark), Bulgaria (Bulgarian lev) and North Macedonia (Macedonian denar). On 28 July 2009, São Tomé and Príncipe signed an agreement with Portugal which will eventually tie its currency to the euro. Additionally, the Moroccan dirham is tied to a basket of currencies, including the euro and the US dollar, with the euro given the highest weighting.

With the exception of Bosnia, Bulgaria, North Macedonia (which had pegged their currencies against the Deutsche Mark) and Cape Verde (formerly pegged to the Portuguese escudo), all of these non-EU countries had a currency peg to the French Franc before pegging their currencies to the euro. Pegging a country's currency to a major currency is regarded as a safety measure, especially for currencies of areas with weak economies, as the euro is seen as a stable currency, prevents runaway inflation and encourages foreign investment due to its stability.

Within the EU several currencies are pegged to the euro, mostly as a precondition to joining the eurozone. The Danish krone, Croatian kuna and Bulgarian lev are pegged due to their participation in the ERM II.

In total, , 182 million people in Africa use a currency pegged to the euro, 27 million people outside the eurozone in Europe, and another 545,000 people on Pacific islands.

Since 2005, stamps issued by the Sovereign Military Order of Malta have been denominated in euros, although the Order's official currency remains the Maltese scudo. The Maltese scudo itself is pegged to the euro and is only recognised as legal tender within the Order.

In economics, an optimum currency area, or region (OCA or OCR), is a geographical region in which it would maximise economic efficiency to have the entire region share a single currency. There are two models, both proposed by Robert Mundell: the stationary expectations model and the international risk sharing model. Mundell himself advocates the international risk sharing model and thus concludes in favour of the euro. However, even before the creation of the single currency, there were concerns over diverging economies. Before the late-2000s recession it was considered unlikely that a state would leave the euro or the whole zone would collapse. However the Greek government-debt crisis led to former British Foreign Secretary Jack Straw claiming the eurozone could not last in its current form. Part of the problem seems to be the rules that were created when the euro was set up. John Lanchester, writing for "The New Yorker", explains it: 

The most obvious benefit of adopting a single currency is to remove the cost of exchanging currency, theoretically allowing businesses and individuals to consummate previously unprofitable trades. For consumers, banks in the eurozone must charge the same for intra-member cross-border transactions as purely domestic transactions for electronic payments (e.g., credit cards, debit cards and cash machine withdrawals).

Financial markets on the continent are expected to be far more liquid and flexible than they were in the past. The reduction in cross-border transaction costs will allow larger banking firms to provide a wider array of banking services that can compete across and beyond the eurozone. However, although transaction costs were reduced, some studies have shown that risk aversion has increased during the last 40 years in the Eurozone.

Another effect of the common European currency is that differences in prices—in particular in price levels—should decrease because of the law of one price. Differences in prices can trigger arbitrage, i.e., speculative trade in a commodity across borders purely to exploit the price differential. Therefore, prices on commonly traded goods are likely to converge, causing inflation in some regions and deflation in others during the transition. Some evidence of this has been observed in specific eurozone markets.

Before the introduction of the euro, some countries had successfully contained inflation, which was then seen as a major economic problem, by establishing largely independent central banks. One such bank was the Bundesbank in Germany; the European Central Bank was modelled on the Bundesbank.

The euro has come under criticism due to its regulation, lack of flexibility and rigidity towards sharing member States on issues such as nominal interest rates.
Many national and corporate bonds denominated in euro are significantly more liquid and have lower interest rates than was historically the case when denominated in national currencies. While increased liquidity may lower the nominal interest rate on the bond, denominating the bond in a currency with low levels of inflation arguably plays a much larger role. A credible commitment to low levels of inflation and a stable debt reduces the risk that the value of the debt will be eroded by higher levels of inflation or default in the future, allowing debt to be issued at a lower nominal interest rate.

Unfortunately, there is also a cost in structurally keeping inflation lower than in the United States, UK, and China. The result is that seen from those countries, the euro has become expensive, making European products increasingly expensive for its largest importers. Hence export from the eurozone becomes more difficult.

In general, those in Europe who own large amounts of euros are served by high stability and low inflation.

A monetary union means states in that union lose the main mechanism of recovery of their international competitiveness by weakening (depreciating) their currency. When wages become too high compared to productivity in exports sector then these exports become more expensive and they are crowded out from the market within a country and abroad. This drive fall of employment and output in the exports sector and fall of trade and current account balances. Fall of output and employment in tradable goods sector may be offset by the growth of non-exports sectors, especially in construction and services. Increased purchases abroad and negative current account balance can be financed without a problem as long as credit is cheap. The need to finance trade deficit weakens currency making exports automatically more attractive in a country and abroad. A state in a monetary union cannot use weakening of currency to recover its international competitiveness. To achieve this a state has to reduce prices, including wages (deflation). This could result in high unemployment and lower incomes as it was during European sovereign-debt crisis.

A 2009 consensus from the studies of the introduction of the euro concluded that it has increased trade within the eurozone by 5% to 10%, although one study suggested an increase of only 3% while another estimated 9 to 14%. However, a meta-analysis of all available studies suggests that the prevalence of positive estimates is caused by publication bias and that the underlying effect may be negligible. Although a more recent meta-analysis shows that publication bias decreases over time and that there are positive trade effects from the introduction of the euro, as long as results from before 2010 are taken into account. This may be because of the inclusion of the Financial crisis of 2007–2008 and ongoing integration within the EU. Furthermore, older studies accounting for time trend reflecting general cohesion policies in Europe that started before, and continue after implementing the common currency find no effect on trade. These results suggest that other policies aimed at European integration might be the source of observed increase in trade.

Physical investment seems to have increased by 5% in the eurozone due to the introduction. Regarding foreign direct investment, a study found that the intra-eurozone FDI stocks have increased by about 20% during the first four years of the EMU. Concerning the effect on corporate investment, there is evidence that the introduction of the euro has resulted in an increase in investment rates and that it has made it easier for firms to access financing in Europe. The euro has most specifically stimulated investment in companies that come from countries that previously had weak currencies. A study found that the introduction of the euro accounts for 22% of the investment rate after 1998 in countries that previously had a weak currency.

The introduction of the euro has led to extensive discussion about its possible effect on inflation. In the short term, there was a widespread impression in the population of the eurozone that the introduction of the euro had led to an increase in prices, but this impression was not confirmed by general indices of inflation and other studies. A study of this paradox found that this was due to an asymmetric effect of the introduction of the euro on prices: while it had no effect on most goods, it had an effect on cheap goods which have seen their price round up after the introduction of the euro. The study found that consumers based their beliefs on inflation of those cheap goods which are frequently purchased. It has also been suggested that the jump in small prices may be because prior to the introduction, retailers made fewer upward adjustments and waited for the introduction of the euro to do so.

One of the advantages of the adoption of a common currency is the reduction of the risk associated with changes in currency exchange rates. It has been found that the introduction of the euro created "significant reductions in market risk exposures for nonfinancial firms both in and outside Europe". These reductions in market risk "were concentrated in firms domiciled in the eurozone and in non-euro firms with a high fraction of foreign sales or assets in Europe".

The introduction of the euro seems to have had a strong effect on European financial integration. According to a study on this question, it has "significantly reshaped the European financial system, especially with respect to the securities markets [...] However, the real and policy barriers to integration in the retail and corporate banking sectors remain significant, even if the wholesale end of banking has been largely integrated." Specifically, the euro has significantly decreased the cost of trade in bonds, equity, and banking assets within the eurozone. On a global level, there is evidence that the introduction of the euro has led to an integration in terms of investment in bond portfolios, with eurozone countries lending and borrowing more between each other than with other countries.

As of January 2014, and since the introduction of the euro, interest rates of most member countries (particularly those with a weak currency) have decreased. Some of these countries had the most serious sovereign financing problems.

The effect of declining interest rates, combined with excess liquidity continually provided by the ECB, made it easier for banks within the countries in which interest rates fell the most, and their linked sovereigns, to borrow significant amounts (above the 3% of GDP budget deficit imposed on the eurozone initially) and significantly inflate their public and private debt levels. Following the financial crisis of 2007–2008, governments in these countries found it necessary to bail out or nationalise their privately held banks to prevent systemic failure of the banking system when underlying hard or financial asset values were found to be grossly inflated and sometimes so near worthless there was no liquid market for them. This further increased the already high levels of public debt to a level the markets began to consider unsustainable, via increasing government bond interest rates, producing the ongoing European sovereign-debt crisis.

The evidence on the convergence of prices in the eurozone with the introduction of the euro is mixed. Several studies failed to find any evidence of convergence following the introduction of the euro after a phase of convergence in the early 1990s. Other studies have found evidence of price convergence, in particular for cars. A possible reason for the divergence between the different studies is that the processes of convergence may not have been linear, slowing down substantially between 2000 and 2003, and resurfacing after 2003 as suggested by a recent study (2009).

A study suggests that the introduction of the euro has had a positive effect on the amount of tourist travel within the EMU, with an increase of 6.5%.

The ECB targets interest rates rather than exchange rates and in general does not intervene on the foreign exchange rate markets. This is because of the implications of the Mundell–Fleming model, which implies a central bank cannot (without capital controls) maintain interest rate and exchange rate targets simultaneously, because increasing the money supply results in a depreciation of the currency. In the years following the Single European Act, the EU has liberalised its capital markets and, as the ECB has inflation targeting as its monetary policy, the exchange-rate regime of the euro is floating.

The euro is the second-most widely held reserve currency after the U.S. dollar. After its introduction on 4 January 1999 its exchange rate against the other major currencies fell reaching its lowest exchange rates in 2000 (3 May vs Pound sterling, 25 October vs the U.S. dollar, 26 October vs Japanese yen). Afterwards it regained and its exchange rate reached its historical highest point in 2008 (15 July vs U.S. dollar, 23 July vs Japanese yen, 29 December vs Pound sterling). With the advent of the global financial crisis the euro initially fell, to regain later. Despite pressure due to the European sovereign-debt crisis the euro remained stable. In November 2011 the euro's exchange rate index – measured against currencies of the bloc's major trading partners – was trading almost two percent higher on the year, approximately at the same level as it was before the crisis kicked off in 2007.


Besides the economic motivations to the introduction of the euro, its creation was also partly justified as a way to foster a closer sense of joint identity between European citizens. Statements about this goal where for instance made by Wim Duisenberg, European Central Bank Governor, in 1998, Laurent Fabius, French Finance Minister, in 2000, Romano Prodi, President of the European Commission, in 2002. However, 15 years after the introduction of the euro, a study found no evidence that it has had a positive influence on a shared sense of European identity (and no evidence that it had a negative effect either).

The formal titles of the currency are "euro" for the major unit and "cent" for the minor (one-hundredth) unit and for official use in most eurozone languages; according to the ECB, all languages should use the same spelling for the nominative singular. This may contradict normal rules for word formation in some languages, e.g., those in which there is no "eu" diphthong. Bulgaria has negotiated an exception; "euro" in the Bulgarian Cyrillic alphabet is spelled as eвро ("evro") and not eуро ("euro") in all official documents. In the Greek script the term ευρώ (evró) is used; the Greek "cent" coins are denominated in λεπτό/ά (leptó/á). Official practice for English-language EU legislation is to use the words euro and cent as both singular and plural, although the European Commission's Directorate-General for Translation states that the plural forms "euros" and "cents" should be used in English.





</doc>
<doc id="9474" url="https://en.wikipedia.org/wiki?curid=9474" title="European Central Bank">
European Central Bank

The European Central Bank (ECB) is the central bank of the Eurozone, a monetary union of 19 EU member states which employ the euro. Established by the Treaty of Amsterdam, the ECB is one of the world's most important central banks and serves as one of seven institutions of the European Union, being enshrined in the Treaty on European Union (TEU). The bank's capital stock is owned by all 27 central banks of each EU member state. The current President of the ECB is Christine Lagarde. Headquartered in Frankfurt, Germany, the bank formerly occupied the Eurotower prior to the construction of its new seat.

The primary objective of the ECB, mandated in Article 2 of the Statute of the ECB, is to maintain price stability within the Eurozone. Its basic tasks, set out in Article 3 of the Statute, are to set and implement the monetary policy for the Eurozone, to conduct foreign exchange operations, to take care of the foreign reserves of the European System of Central Banks and operation of the financial market infrastructure under the TARGET2 payments system and the technical platform (currently being developed) for settlement of securities in Europe (TARGET2 Securities). The ECB has, under Article 16 of its Statute, the exclusive right to authorise the issuance of euro banknotes. Member states can issue euro coins, but the amount must be authorised by the ECB beforehand.

The ECB is governed by European law directly, but its set-up resembles that of a corporation in the sense that the ECB has shareholders and stock capital. Its capital is €11 billion held by the national central banks of the member states as shareholders. The initial capital allocation key was determined in 1998 on the basis of the states' population and GDP, but the capital key has been adjusted. Shares in the ECB are not transferable and cannot be used as collateral.

The European Central Bank is the "de facto" successor of the European Monetary Institute (EMI). The EMI was established at the start of the second stage of the EU's Economic and Monetary Union (EMU) to handle the transitional issues of states adopting the euro and prepare for the creation of the ECB and European System of Central Banks (ESCB). The EMI itself took over from the earlier European Monetary Co-operation Fund (EMCF).

The ECB formally replaced the EMI on 1 June 1998 by virtue of the Treaty on European Union (TEU, Treaty of Maastricht), however it did not exercise its full powers until the introduction of the euro on 1 January 1999, signalling the third stage of EMU. The bank was the final institution needed for EMU, as outlined by the EMU reports of Pierre Werner and President Jacques Delors. It was established on 1 June 1998.

The first President of the Bank was Wim Duisenberg, the former president of the Dutch central bank and the European Monetary Institute. While Duisenberg had been the head of the EMI (taking over from Alexandre Lamfalussy of Belgium) just before the ECB came into existence, the French government wanted Jean-Claude Trichet, former head of the French central bank, to be the ECB's first president. The French argued that since the ECB was to be located in Germany, its president should be French. This was opposed by the German, Dutch and Belgian governments who saw Duisenberg as a guarantor of a strong euro. Tensions were abated by a gentleman's agreement in which Duisenberg would stand down before the end of his mandate, to be replaced by Trichet.

Trichet replaced Duisenberg as President in November 2003.
There had also been tension over the ECB's Executive Board, with the former member state United Kingdom demanding a seat even though it had not joined the Single Currency. Under pressure from France, three seats were assigned to the largest members, France, Germany, and Italy; Spain also demanded and obtained a seat. Despite such a system of appointment the board asserted its independence early on in resisting calls for interest rates and future candidates to it.

When the ECB was created, it covered a Eurozone of eleven members. Since then, Greece joined in January 2001, Slovenia in January 2007, Cyprus and Malta in January 2008, Slovakia in January 2009, Estonia in January 2011, Latvia in January 2014 and Lithuania in January 2015, enlarging the bank's scope and the membership of its Governing Council.

On 1 December 2009, the Treaty of Lisbon entered into force, ECB according to the article 13 of TEU, gained official status of an EU institution.

In September 2011, when German appointee to the Governing Council and Executive board, Jürgen Stark, resigned in protest of the ECB's "Securities Market Programme" which involved the purchase of sovereign bonds by the ECB, a move that was up until then considered as prohibited by the EU Treaty. The "Financial Times Deutschland" referred to this episode as "the end of the ECB as we know it", referring to its hitherto perceived "hawkish" stance on inflation and its historical Deutsche Bundesbank influence.

On 1 November 2011, Mario Draghi replaced Jean-Claude Trichet as President of the ECB.

In April 2011, the ECB raised interest rates for the first time since 2008 from 1% to 1.25%, with a further increase to 1.50% in July 2011. However, in 2012–2013 the ECB sharply lowered interest rates to encourage economic growth, reaching the historically low 0.25% in November 2013. Soon after the rates were cut to 0.15%, then on 4 September 2014 the central bank reduced the rates by two thirds from 0.15% to 0.05%. Recently, the interest rates were further reduced reaching 0.00%, the lowest rates on record.

In November 2014, the bank moved into its new premises.

On 1 November 2019, Christine Lagarde, former Managing Director of the International Monetary Fund, replaced Mario Draghi as President.

From late 2009 a handful of mainly southern eurozone member states started being unable to repay their national Euro-denominated government debt or to finance the bail-out of troubled financial sectors under their national supervision without the assistance of third parties. This so-called "European debt crisis" began after Greece's new elected government stopped masking its true indebtedness and budget deficit and the imminent danger of a Greek sovereign default.

Foreseeing a possible sovereign default in the eurozone, the general public, international and European institutions, and the financial community reassessed the economic situation and creditworthiness of some Eurozone member states, in particular Southern countries. Consequently, sovereign bonds yields of several Eurozone countries started to rise sharply. This provoked a self-fulfilling panic on financial markets: the more Greek bonds yields rose, the more likely a default became possible, the more bond yields increased in turn.

This panic was also aggravated because of the inability of the ECB to react and intervene on sovereign bonds markets for two reasons. First, because the ECB's legal framework normally forbids the purchase of sovereign bonds (Article 123. TFEU), This prevented the ECB from implementing quantitative easing like the Federal Reserve and the Bank of England did as soon as 2008, which played an important role in stabilizing markets. Secondly, a decision by the ECB made in 2005 introduced a minimum credit rating (BBB-) for all Eurozone sovereign bonds to be eligible as collateral to the ECB's open market operations. This meant that if a private rating agencies were to downgrade a sovereign bond below that threshold, many banks would suddenly become illiquid because they would lose access to ECB refinancing operations. According to former member of the governing council of the ECB Athanasios Orphanides, this change in the ECB's collateral framework "planted the seed" of the euro crisis.

Faced with those regulatory constraints, the ECB led by Jean-Claude Trichet in 2010 was reluctant to intervene to calm down financial markets. Up until May 6, 2010, Trichet formally denied at several press conferences the possibility of the ECB to embark into sovereign bonds purchases, even though Greece, Portugal, Spain and Italy faced waves of credit rating downgrades and increasing interest rate spreads.

On 10 May 2010, the ECB announced the launch of a "Securities Market Programme" (SMP) which involved the discretionary purchase of sovereign bonds in secondary markets. Extraordinarily, the decision was taken by the Governing Council during a teleconference call only three days after the ECB's usual meeting of May 6 (when Trichet still denied the possibility of purchasing sovereign bonds). The ECB justified this decision by the necessity to "address severe tensions in financial markets." The decision also coincided with the EU leaders decision of May 10 to establish the European Financial Stabilisation mechanism, which would serve as a crisis fighting fund to safeguard the euro area from future sovereign debt crisis.

The ECB's bond buying focused primarily on Spanish and Italian debt. They were intended to dampen international speculation against those countries, and thus avoid a contagion of the Greek crisis towards other Eurozone countries. The assumption is that speculative activity will decrease over time and the value of the assets increase.

Although SMP did involve an injection of new money into financial markets, all ECB injections were "sterilized" through weekly liquidity absorption. So the operation was neutral for the overall money supply.

When the ECB buys bonds from other creditors such as European banks, the ECB does not disclose the transaction prices. Creditors profit of bargains with bonds sold at prices that exceed market's quotes.

As of 18 June 2012, the ECB in total had spent €212.1bn (equal to 2.2% of the Eurozone GDP) for bond purchases covering outright debt, as part of the Securities Markets Programme. Controversially, the ECB made substantial profits out of SMP, which were largely redistributed to Eurozone countries. In 2013, the Eurogroup decided to refund those profits to Greece, however the payments were suspended over 2014 until 2017 over the conflict between Yanis Varoufakis and ministers of the Eurogroup. In 2018, profits refunds were reinstalled by the Eurogroup. However, several NGOs complained that a substantial part of the ECB profits would never be refunded to Greece.

In November 2010, it became clear that Ireland would not be able to afford to bail out its failing banks, and Anglo Irish Bank in particular which needed around 30 billion euros, a sum the government obviously could not borrow from financial markets when its bond yields were soaring to comparable levels with the Greek bonds. Instead, the government issued a 31bn EUR "promissory note" (an IOU) to Anglo – which it had nationalized. In turn, the bank supplied the promissory note as collateral to the Central Bank of Ireland, so it could access emergency liquidity assistance (ELA). This way, Anglo was able to repay its bondholders. The operation became very controversial, as it basically shifted Anglo's private debts onto the government's balance sheet.

It became clear later that the ECB played a key role in making sure the Irish government did not let Anglo default on its debts, in order to avoid a financial instability risks. On 15 October and 6 November 2010, the ECB President Jean-Claude Trichet sent two secret letters to the Irish finance Minister which essentially informed the Irish government of the possible suspension of ELA's credit lines, unless the government requested a financial assistance programme to the Eurogroup under condition of further reforms and fiscal consolidation. Over 2012 and 2013, the ECB repeatedly insisted that the promissory note should be repaid in full, and refused the Government's proposal to swap the notes with a long-term (and less costly) bond until February 2013. In addition, the ECB insisted that no debt restructuring (or bail-in) should be applied to the nationalized banks' bondholders, a measure which could have saved Ireland 8 billion euros.

In short, fearing a new financial panic, the ECB took extraordinary measures to avoid at all cost debt restructuring in Ireland, which resulted in higher public debt in Ireland.

Soon after Mario Draghi took over the presidency of the ECB, the bank announced on 8 December 2011 a new round of 1% interest loans with a term of three years (36 months) – the Long-term Refinancing operations (LTRO).

Thanks to this operation, 523 Banks tapped as much as €489.2 bn (US$640 bn). The loans were not offered to European states, but government securities issued by European states would be acceptable collateral as would mortgage-backed securities and other commercial paper that have a sufficient rating by credit agencies. Observers were surprised by the volume of the loans made when it was implemented. By far biggest amount of was tapped by banks in Greece, Ireland, Italy and Spain. This way the ECB tried to make sure that banks have enough cash to pay off of their own maturing debts in the first three months of 2012, and at the same time keep operating and loaning to businesses so that a credit crunch does not choke off economic growth. It also hoped that banks would use some of the money to buy government bonds, effectively easing the debt crisis.

On 29 February 2012, the ECB held a second 36-month auction, LTRO2, providing eurozone banks with further €529.5 billion in low-interest loans. This second long term refinancing operation auction saw 800 banks take part. Net new borrowing under the February auction was around €313 billion – out of a total of €256bn existing ECB lending €215bn was rolled into LTRO2.

In July 2012, in the midst of renewed fears about sovereigns in the eurozone, Draghi stated in a panel discussion in London that the ECB "...is ready to do "whatever it takes" to preserve the Euro. And believe me, it will be enough." This statement led to a steady decline in bond yields for eurozone countries, in particular Spain, Italy and France. In light of slow political progress on solving the eurozone crisis, Draghi's statement has been seen as a key turning point in the fortunes of the eurozone.

Following up on Draghi's speech, the Governing Council of the European Central Bank (ECB) announced on 2 August 2012, that it "may undertake outright open market operations of a size adequate to reach its objective" in order to "safeguarding an appropriate monetary policy transmission and the singleness of the monetary policy". The technical framework of these operations was formulated on 6 September 2012 when the ECB announced the launch of the Outright Monetary Transactions programme (OMT). On the same date, the bank's Securities Markets Programme (SMP) was terminated.

While the duration of the previous SMP was temporary, OMT has no ex-ante time or size limit. However, the activation of the purchases remains conditioned to the adherence by the benefitting country to an adjustment programme to the ESM. To date, OMT was never actually implemented by the ECB. However it is considered that its announcement (together with the "whatever it takes" speech) significantly contributed in stabilizing financial markets and ended the sovereign debt crisis.

Although the sovereign debt crisis was almost solved by 2014, the ECB started to face a repeated decline in the Eurozone inflation rate, indicating that the economy was going towards a deflation. Responding to this threat, the ECB announced on 4 September 2014 the launch of two bond buying purchases programmes: the Covered Bond Purchasing Programme (CBPP3) and Asset-Backed Securities Programme (ABSPP).

On 22 January 2015, the ECB announced an extension of those programmes within a full-fledge "quantitative easing" programme which also included sovereign bonds, to the tune of 60 billion euros per month up until at least September 2016. The programme was started on 9 March 2015. The program was repeatedly extended to reach about €2,500 billions and is currently expected to last until at least end of 2018.

On 8 June 2016, the ECB added euro-denominated corporate bonds to its asset purchases portfolio with the launch of the corporate sector purchase programme (CSPP). Under this programme, it conducted net purchase of corporate bonds until January 2019 to reach about €177 billion. While the programme was halted for 11 months in January 2019, the ECB restarted net purchases in November 2019.

The primary objective of the European Central Bank, set out in Article 127(1) of the Treaty on the Functioning of the European Union, is to maintain price stability within the Eurozone. The Governing Council in October 1998 defined price stability as inflation of under 2%, “a year-on-year increase in the Harmonised Index of Consumer Prices (HICP) for the euro area of below 2%” and added that price stability ”was to be maintained over the medium term”. (Harmonised Index of Consumer Prices) Unlike for example the United States Federal Reserve System, the ECB has only one primary objective—but this objective has never been defined in statutory law, and the HICP target can be termed "ad hoc".

The Governing Council confirmed this definition in May 2003 following a thorough evaluation of the ECB's monetary policy strategy. On that occasion, the Governing Council clarified that “in the pursuit of price stability, it aims to maintain inflation rates below, but close to, 2% over the medium term”. All lending to credit institutions must be collateralised as required by Article 18 of the Statute of the ESCB. The Governing Council clarification has little force in law.

Without prejudice to the objective of price stability, the Treaty also states that "the ESCB shall support the general economic policies in the Union with a view to contributing to the achievement of the objectives of the Union".

To carry out its main mission, the ECB's tasks include:


The principal monetary policy tool of the European central bank is collateralised borrowing or repo agreements. These tools are also used by the United States Federal Reserve Bank, but the Fed does more direct purchasing of financial assets than its European counterpart. The collateral used by the ECB is typically high quality public and private sector debt.

The criteria for determining "high quality" for public debt have been preconditions for membership in the European Union: total debt must not be too large in relation to gross domestic product, for example, and deficits in any given year must not become too large. Though these criteria are fairly simple, a number of accounting techniques may hide the underlying reality of fiscal solvency—or the lack of same.

In central banking, the privileged status of the central bank is that it can make as much money as it deems needed. In the United States Federal Reserve Bank, the Federal Reserve buys assets: typically, bonds issued by the Federal government. There is no limit on the bonds that it can buy and one of the tools at its disposal in a financial crisis is to take such extraordinary measures as the purchase of large amounts of assets such as commercial paper. The purpose of such operations is to ensure that adequate liquidity is available for functioning of the financial system.

Think-tanks such as the World Pensions Council have also argued that European legislators have pushed somewhat dogmatically for the adoption of the Basel II recommendations, adopted in 2005, transposed in European Union law through the Capital Requirements Directive (CRD), effective since 2008. In essence, they forced European banks, and, more importantly, the European Central Bank itself (e.g. when gauging the solvency of financial institutions) to rely more than ever on standardised assessments of credit risk marketed by two non-European private agencies: Moody's and S&P.

In the United States, the Federal Reserve System purchases Treasury securities in order to inject liquidity into the economy. The Eurosystem, on the other hand, uses a different method. There are about 1,500 eligible banks which may bid for short-term repo contracts of two weeks to three months duration.

The banks in effect borrow cash and must pay it back; the short durations allow interest rates to be adjusted continually. When the repo notes come due the participating banks bid again. An increase in the quantity of notes offered at auction allows an increase in liquidity in the economy. A decrease has the contrary effect. The contracts are carried on the asset side of the European Central Bank's balance sheet and the resulting deposits in member banks are carried as a liability. In layman terms, the liability of the central bank is money, and an increase in deposits in member banks, carried as a liability by the central bank, means that more money has been put into the economy.

To qualify for participation in the auctions, banks must be able to offer proof of appropriate collateral in the form of loans to other entities. These can be the public debt of member states, but a fairly wide range of private banking securities are also accepted. The fairly stringent membership requirements for the European Union, especially with regard to sovereign debt as a percentage of each member state's gross domestic product, are designed to ensure that assets offered to the bank as collateral are, at least in theory, all equally good, and all equally protected from the risk of inflation.

The ECB has four decision-making bodies, that take all the decisions with the objective of fulfilling the ECB's mandate:

The Executive Board is responsible for the implementation of monetary policy (defined by the Governing Council) and the day-to-day running of the bank. It can issue decisions to national central banks and may also exercise powers delegated to it by the Governing Council. Executive Board members are assigned a portfolio of responsibilities by the President of the ECB. The Executive Board normally meets every Tuesday.

It is composed of the President of the Bank (currently Christine Lagarde), the Vice-President (currently Luis de Guindos) and four other members. They are all appointed for non-renewable terms of eight years. Member of the Executive Board of the ECB are appointed "from among persons of recognised standing and professional experience in monetary or banking matters by common accord of the governments of the Member States at the level of Heads of State or Government, on a recommendation from the Council, after it has consulted the European Parliament and the Governing Council of the ECB".

José Manuel González-Páramo, a Spanish member of the Executive Board since June 2004, was due to leave the board in early June 2012, but no replacement had been named as of late May. The Spanish had nominated Barcelona-born Antonio Sáinz de Vicuña – an ECB veteran who heads its legal department – as González-Páramo's replacement as early as January 2012, but alternatives from Luxembourg, Finland, and Slovenia were put forward and no decision made by May. After a long political battle and delays due to the European Parliament's protest over the lack of gender balance at the ECB, Luxembourg's Yves Mersch was appointed as González-Páramo's replacement.

The Governing Council is the main decision-making body of the Eurosystem. It comprises the members of the Executive Board (six in total) and the governors of the National Central Banks of the euro area countries (19 as of 2015).

Since January 2015, the ECB has published on its website a summary of the Governing Council deliberations ("accounts"). These publications came as a partial response to recurring criticism against the ECB's opacity. However, in contrast to other central banks, the ECB still does not disclose individual voting records of the governors seating in its Council.

The General Council is a body dealing with transitional issues of euro adoption, for example, fixing the exchange rates of currencies being replaced by the euro (continuing the tasks of the former EMI). It will continue to exist until all EU member states adopt the euro, at which point it will be dissolved. It is composed of the President and vice-president together with the governors of all of the EU's national central banks.

The Supervisory Board meets twice a month to discuss, plan and carry out the ECB's supervisory tasks. It proposes draft decisions to the Governing Council under the non-objection procedure. It is composed of Chair (appointed for a non-renewable term of five years), Vice-Chair (chosen from among the members of the ECB's Executive Board) four ECB representatives and representatives of national supervisors. If the national supervisory authority designated by a Member State is not a national central bank (NCB), the representative of the competent authority can be accompanied by a representative from their NCB. In such cases, the representatives are together considered as one member for the purposes of the voting procedure.

It also includes the Steering Committee, which supports the activities of the Supervisory Board and prepares the Board's meetings. It is composed by the Chair of the Supervisory Board, Vice-Chair of the Supervisory Board, one ECB representative and five representatives of national supervisors. The five representatives of national supervisors are appointed by the Supervisory Board for one year based on a rotation system that ensures a fair representation of countries.

The ECB is governed by European law directly, but its set-up resembles that of a corporation in the sense that the ECB has shareholders and stock capital. Its initial capital was supposed to be €5 billion and the initial capital allocation key was determined in 1998 on the basis of the member states' populations and GDP, but the key is adjustable. The euro area NCBs were required to pay their respective subscriptions to the ECB's capital in full. The NCBs of the non-participating countries have had to pay 7% of their respective subscriptions to the ECB's capital as a contribution to the operational costs of the ECB. As a result, the ECB was endowed with an initial capital of just under €4 billion. The capital is held by the national central banks of the member states as shareholders. Shares in the ECB are not transferable and cannot be used as collateral. The NCBs are the sole subscribers to and holders of the capital of the ECB.

Today, ECB capital is about €11 billion, which is held by the national central banks of the member states as shareholders. The NCBs’ shares in this capital are calculated using a capital key which reflects the respective member's share in the total population and gross domestic product of the EU. The ECB adjusts the shares every five years and whenever a new country joins the EU. The adjustment is made on the basis of data provided by the European Commission.

All national central banks (NCBs) that own a share of the ECB capital stock as of 1 February 2020 are listed below. Non-Euro area NCBs are required to pay up only a very small percentage of their subscribed capital, which accounts for the different magnitudes of Euro area and Non-Euro area total paid-up capital.

In addition to capital subscriptions, the NCBs of the member states participating in the euro area provided the ECB with foreign reserve assets equivalent to around €40 billion. The contributions of each NCB is in proportion to its share in the ECB's subscribed capital, while in return each NCB is credited by the ECB with a claim in euro equivalent to its contribution. 15% of the contributions was made in gold, and the remaining 85% in US dollars and UK pound Sterlings.

The internal working language of the ECB is generally English, and press conferences are usually held in English. External communications are handled flexibly: English is preferred (though not exclusively) for communication within the ESCB (i.e. with other central banks) and with financial markets; communication with other national bodies and with EU citizens is normally in their respective language, but the ECB website is predominantly English; official documents such as the Annual Report are in the official languages of the EU.

The European Central Bank (and by extension, the Eurosystem) is often considered as the "most independent central bank in the world". In general terms, this means that the Eurosystem tasks and policies can be discussed, designed, decided and implemented in full autonomy, without pressure or need for instructions from any external body. The main justification for the ECB's independence is that such an institutional setup assists the maintenance of price stability.

In practice, the ECB's independence is pinned by four key principles:


The debate on the independence of the ECB has its origins in the preparatory stages of the construction of the EMU. The German government agreed to go ahead if certain crucial guarantees were respected, such as a European Central Bank independent of national governments and shielded from political pressure along the lines of the German central bank. The French government, for its part, feared that this independence would mean that politicians would no longer have any room for manoeuvre in the process. A compromise was then reached by establishing a regular dialogue between the ECB and the Council of Finance Ministers of the euro area, the Eurogroupe.

The founding model of the ECB, as advocated by the German government, is explained in an article published in 1983 by two economists, Robert Barro and David Gordon. According to them, the best way to combat the inflationary bias is for central banks to be credible. This credibility would be all the more important if the central banks were independent, so that decisions are not "contaminated" by politics. For economists, central banks should then have only one objective: to maintain a low inflation rate. For this system to work, it would then be necessary to assume that a monetary policy conducted in this way would have no effect on the real economy.

Since that publication, it has often been accepted that an independent institution to manage monetary policy can help to limit chronic inflation. This model was generalised in various variations at the national levels before being adopted at the European level.

The original European project, as intended by the founding fathers, did not attract the passions and favours of the European peoples. And rightly so. This project is thought out openly on purely technical subjects that are of little or no interest to public opinion. As a result, the founding fathers hoped that economic and ethical rationality could be exercised in all its fullness without political, ideological or historical obstacles. It is this rational messianism that the radical left has always fought against. Moreover, this project is presented as heir to the Enlightenment and Reason, the reign of human rights, a modernist and voluntarist project stemming from the tradition of the 18th century. In this order of things, the independence of the ECB allowing a rational management of monetary questionnaires outside the political game is a blessing for the supporters of this doctrine. It is difficult, if not impossible, for them to conceive of a democratisation of the ECB by attaching to it a share of political control in its operation without distorting the "European Project", this bible, this unique political reason which has guided professionals in Europe for generations.

In this same idea, we can find in the "European Project" the Kantian tradition with a model of successful subordination of political power to the law, leading to what Habermas calls "the civilising force of democratic legalification". This Habermatian theory leads us once again to isolate supranational institutions from political games. Indeed, for Habermas, European politics, like national politics for that matter, finds it impossible to define ONE uniform people, but at best a pluriform people in constant opposition, each component against the other. For this author, popular sovereignty is illusory, as is the concept of "government by the people". He prefers the search for a broad consensus legitimized by the majority of democratically elected representatives of the people. This explains his attachment to deflecting the influence of popular emotions from technical institutions, such as the ECB.

Demystify the independence of central bankers :According to Christopher Adolph (2009), the alleged neutrality of central bankers is only a legal façade and not an indisputable fact . To achieve this, the author analyses the professional careers of central bankers and mirrors them with their respective monetary decision-making. To explain the results of his analysis, he utilizes he uses the ""principal-agent"" theory. To explain that in order to create a new entity, one needs a delegator or "principal" (in this case the heads of state or government of the euro area) and a delegate or "agent" (in this case the ECB). In his illustration, he describes the financial community as a ""shadow principale""  which influences the choice of central bankers thus indicating that the central banks indeed act as interfaces between the financial world and the States. It is therefore not surprising, still according to the author, to regain their influence and preferences in the appointment of central bankers, presumed conservative, neutral and impartial according to the model of the Independent Central Bank (ICB), which eliminates this famous ""temporal inconsistency"". Central bankers had a professional life before joining the central bank and their careers will most likely continue after their tenure. They are ultimately human beings. Therefore, for the author, central bankers have interests of their own, based on their past careers and their expectations after joining the ECB, and try to send messages to their future potential employers.

The crisis: an opportunity to impose its will and extend its powers :

- "Its participation in the troika" : Thanks to its three factors which explains its independence, the ECB took advantage of this crisis to implement, through its participation in the troika, the famous structural reforms in the Member States aimed at making, more flexible the various markets, particularly the labour market, which are still considered too rigid under the ordoliberal concept.

- "Macro-prudential supervision" : At the same time, taking advantage of the reform of the financial supervision system, the Frankfurt Bank has acquired new responsibilities, such as macro-prudential supervision, in other words, supervision of the provision of financial services.

-"Take liberties with its mandate to save the Euro" : Paradoxically, the crisis undermined the ECB's ordoliberal discourse "because some of its instruments, which it had to implement, deviated significantly from its principles. It then interpreted the paradigm with enough flexibly to adapt its original reputation to these new economic conditions. It was forced to do so as a last resort to save its one and only raison d'être: the euro. This Independent was thus obliged to be pragmatic by departing from the spirit of its statutes, which is unacceptable to the hardest supporters of ordoliberalism, which will lead to the resignation of the two German leaders present within the ECB: the governor of the Bundesbank, Jens WEIDMANN and the member of the Executive Board of the ECB, Jürgen STARK.

- "Regulation of the financial system" :  The delegation of this new function to the ECB was carried out with great simplicity and with the consent of European leaders, because neither the Commission nor the Member States really wanted to obtain the monitoring of financial abuses throughout the area. In other words, in the event of a new financial crisis, the ECB would be the perfect scapegoat.

- "Capturing exchange rate policy" : The event that will most mark the definitive politicization of the ECB is, of course, the operation launched in January 2015: the quantitative easing (QE) operation. Indeed, the Euro is an overvalued currency on the world markets against the dollar and the Euro zone is at risk of deflation. In addition, Member States find themselves heavily indebted, partly due to the rescue of their national banks. The ECB, as the guardian of the stability of the euro zone, is deciding to gradually buy back more than EUR 1 100 billion Member States' public debt. In this way, money is injected back into the economy, the euro depreciates significantly, prices rise, the risk of deflation is removed, and Member States reduce their debts. However, the ECB has just given itself the right to direct the exchange rate policy of the euro zone without this being granted by the Treaties or with the approval of European leaders, and without public opinion or the public arena being aware of this.

In conclusion, for those in favour of a framework for ECB independence, there is a clear concentration of powers. In the light of these facts, it is clear that the ECB is no longer the simple guardian of monetary stability in the euro area, but has become, over the course of the crisis, a ""multi-competent economic player, at ease in this role that no one, especially not the agnostic governments of the euro Member States, seems to have the idea of challenging"". This new political super-actor, having captured many spheres of competence and a very strong influence in the economic field in the broad sense (economy, finance, budget...).  This new political super-actor can no longer act alone and refuse a counter-power, consubstantial to our liberal democracies. Indeed, the status of independence which the ECB enjoys by essence should not exempt it from a real responsibility regarding the democratic process.

In the aftermath of the euro area crisis, several proposals for a countervailing power were put forward, to deal with criticisms of a democratic deficit. For the German economist German Issing (2001) the ECB as a democratic responsibility and should be more transparent. According to him, this transparence could bring several advantages as the improvement of the efficiency and of the credibility by giving to the public adequate information. Others think that the ECB should have a closer relationship with the European Parliament which could play a major role in the evaluation of the democratic responsibility of the ECB. The development of new institutions or the creation of a minister is another solution proposed:

A minister for the Eurozone ?

The idea of a eurozone finance minister is regularly raised and supported by certain political figures, including Emmanuel Macron, as well as German Chancellor Angela Merkel, former President of the ECB Jean-Claude Trichet and former European Commissioner Pierre Moscovici. For the latter, this position would bring ""more democratic legitimacy"" and ""more efficiency"" to European politics. In his view, it is a question of merging the powers of Commissioner for the Economy and Finance with those of the President of the Eurogroup.

The main task of this minister would be to "represent a strong political authority protecting the economic and budgetary interests of the euro area as a whole, and not the interests of individual Member States". According to the Jacques Delors Institute, its competences could be as follows:


For Jean-Claude Trichet, this minister could also rely on the Eurogroup working group for the preparation and follow-up of meetings in euro zone format, and on the Economic and Financial Committee for meetings concerning all Member States. He would also have under his authority a General Secretariat of the Treasury of the euro area, whose tasks would be determined by the objectives of the budgetary union currently being set up 

This proposal was nevertheless rejected in 2017 by the Eurogroup, its President, Jeroen Dijsselbloem, spoke of the importance of this institution in relation to the European Commission.

Towards democratic institutions ?

The absence of democratic institutions such as a Parliament or a real government is a regular criticism of the ECB in its management of the euro area, and many proposals have been made in this respect, particularly after the economic crisis, which would have shown the need to improve the governance of the euro area. For Moïse Sidiropoulos, a professor in economy: “The crisis in the euro zone came as no surprise, because the euro remains an unfinished currency, a stateless currency with a fragile political legitimacy”.

French economist Thomas Piketty wrote on his blog in 2017 that it was essential to equip the euro zone with democratic institutions. An economic government could for example enable it to have a common budget, common taxes and borrowing and investment capacities. Such a government would then make the euro area more democratic and transparent by avoiding the opacity of a council such as the Eurogroup.

Nevertheless, according to him ""there is no point in talking about a government of the euro zone if we do not say to which democratic body this government will be accountable"", a real parliament of the euro zone to which a finance minister would be accountable seems to be the real priority for the economist, who also denounces the lack of action in this area.

The creation of a sub-committee within the current European Parliament was also mentioned, on the model of the Eurogroup, which is currently an under-formation of the ECOFIN Committee. This would require a simple amendment to the rules of procedure and would avoid a competitive situation between two separate parliamentary assemblies. The former President of the European Commission had, moreover, stated on this subject that he had "no sympathy for the idea of a specific Eurozone Parliament".

In addition to its independence, the ECB is subject to limited transparency obligations in contrast to EU Institutions standards and other major central banks. Indeed, as pointed out by Transparency International, "The Treaties establish transparency and openness as principles of the EU and its institutions. They do, however, grant the ECB a partial exemption from these principles. According to Art. 15(3) TFEU, the ECB is bound by the EU’s transparency principles “only when exercising [its] administrative tasks” (the exemption – which leaves the term “administrative tasks” undefined – equally applies to the Court of Justice of the European Union and to the European Investment Bank)."

In practice, there are several concrete examples where the ECB is less transparent than other institutions:

In return to its high degree of independence and discretion, the ECB is accountable to the European Parliament (and to a lesser extent to the European Court of Auditors, the European Ombudsman and the Court of Justice of the EU (CJEU)). Although no interinstitutional agreement exists between the European Parliament and the ECB to regulate the ECB's accountability framework, it has been inspired by a resolution of the European Parliament adopted in 1998 which was then informally agreed with the ECB and incorporated into the Parliament's rule of procedure.

The accountability framework involves five main mechanisms:


In 2013, an interinstitutional agreement was reached between the ECB and the European Parliament in the context of the establishment of the ECB's Banking Supervision. This agreement sets broader powers to the European Parliament then the established practice on the monetary policy side of the ECB's activities. For example, under the agreement, the Parliament can veto the appointment of the Chair and Vice-Chair of the ECB's supervisory board, and may approve removals if requested by the ECB.

The bank is based in Ostend (East End), Frankfurt am Main. The city is the largest financial centre in the Eurozone and the bank's location in it is fixed by the Amsterdam Treaty. The bank moved to a new purpose-built headquarters in 2014, designed by a Vienna-based architectural office, Coop Himmelbau. The building is approximately tall and is to be accompanied by other secondary buildings on a landscaped site on the site of the former wholesale market in the eastern part of Frankfurt am Main. The main construction on a 120,000 m² total site area began in October 2008, and it was expected that the building would become an architectural symbol for Europe. While it was designed to accommodate double the number of staff who operated in the former Eurotower, that building has been retained by the ECB, owing to more space being required since it took responsibility for banking supervision.




</doc>
<doc id="9476" url="https://en.wikipedia.org/wiki?curid=9476" title="Electron">
Electron

The electron is a subatomic particle, symbol or , whose electric charge is negative one elementary charge. Electrons belong to the first generation of the lepton particle family, and are generally thought to be elementary particles because they have no known components or substructure. The electron has a mass that is approximately 1/1836 that of the proton. Quantum mechanical properties of the electron include an intrinsic angular momentum (spin) of a half-integer value, expressed in units of the reduced Planck constant, "ħ". Being fermions, no two electrons can occupy the same quantum state, in accordance with the Pauli exclusion principle. Like all elementary particles, electrons exhibit properties of both particles and waves: they can collide with other particles and can be diffracted like light. The wave properties of electrons are easier to observe with experiments than those of other particles like neutrons and protons because electrons have a lower mass and hence a longer de Broglie wavelength for a given energy.

Electrons play an essential role in numerous physical phenomena, such as electricity, magnetism, chemistry and thermal conductivity, and they also participate in gravitational, electromagnetic and weak interactions. Since an electron has charge, it has a surrounding electric field, and if that electron is moving relative to an observer, said observer will observe it to generate a magnetic field. Electromagnetic fields produced from other sources will affect the motion of an electron according to the Lorentz force law. Electrons radiate or absorb energy in the form of photons when they are accelerated. Laboratory instruments are capable of trapping individual electrons as well as electron plasma by the use of electromagnetic fields. Special telescopes can detect electron plasma in outer space. Electrons are involved in many applications such as electronics, welding, cathode ray tubes, electron microscopes, radiation therapy, lasers, gaseous ionization detectors and particle accelerators.

Interactions involving electrons with other subatomic particles are of interest in fields such as chemistry and nuclear physics. The Coulomb force interaction between the positive protons within atomic nuclei and the negative electrons without, allows the composition of the two known as atoms. Ionization or differences in the proportions of negative electrons versus positive nuclei changes the binding energy of an atomic system. The exchange or sharing of the electrons between two or more atoms is the main cause of chemical bonding. In 1838, British natural philosopher Richard Laming first hypothesized the concept of an indivisible quantity of electric charge to explain the chemical properties of atoms. Irish physicist George Johnstone Stoney named this charge 'electron' in 1891, and J. J. Thomson and his team of British physicists identified it as a particle in 1897. Electrons can also participate in nuclear reactions, such as nucleosynthesis in stars, where they are known as beta particles. Electrons can be created through beta decay of radioactive isotopes and in high-energy collisions, for instance when cosmic rays enter the atmosphere. The antiparticle of the electron is called the positron; it is identical to the electron except that it carries electrical and other charges of the opposite sign. When an electron collides with a positron, both particles can be annihilated, producing gamma ray photons.

The ancient Greeks noticed that amber attracted small objects when rubbed with fur. Along with lightning, this phenomenon is one of humanity's earliest recorded experiences with electricity. In his 1600 treatise , the English scientist William Gilbert coined the New Latin term , to refer to those substances with property similar to that of amber which attract small objects after being rubbed. Both "electric" and "electricity" are derived from the Latin ' (also the root of the alloy of the same name), which came from the Greek word for amber, (').

In the early 1700s, French chemist Charles François du Fay found that if a charged gold-leaf is repulsed by glass rubbed with silk, then the same charged gold-leaf is attracted by amber rubbed with wool. From this and other results of similar types of experiments, du Fay concluded that electricity consists of two electrical fluids, "vitreous" fluid from glass rubbed with silk and "resinous" fluid from amber rubbed with wool. These two fluids can neutralize each other when combined. American scientist Ebenezer Kinnersley later also independently reached the same conclusion. A decade later Benjamin Franklin proposed that electricity was not from different types of electrical fluid, but a single electrical fluid showing an excess (+) or deficit (−). He gave them the modern charge nomenclature of positive and negative respectively. Franklin thought of the charge carrier as being positive, but he did not correctly identify which situation was a surplus of the charge carrier, and which situation was a deficit.

Between 1838 and 1851, British natural philosopher Richard Laming developed the idea that an atom is composed of a core of matter surrounded by subatomic particles that had unit electric charges. Beginning in 1846, German physicist William Weber theorized that electricity was composed of positively and negatively charged fluids, and their interaction was governed by the inverse square law. After studying the phenomenon of electrolysis in 1874, Irish physicist George Johnstone Stoney suggested that there existed a "single definite quantity of electricity", the charge of a monovalent ion. He was able to estimate the value of this elementary charge "e" by means of Faraday's laws of electrolysis. However, Stoney believed these charges were permanently attached to atoms and could not be removed. In 1881, German physicist Hermann von Helmholtz argued that both positive and negative charges were divided into elementary parts, each of which "behaves like atoms of electricity".

Stoney initially coined the term "electrolion" in 1881. Ten years later, he switched to "electron" to describe these elementary charges, writing in 1894: "... an estimate was made of the actual amount of this most remarkable fundamental unit of electricity, for which I have since ventured to suggest the name "electron"". A 1906 proposal to change to "electrion" failed because Hendrik Lorentz preferred to keep "electron". The word "electron" is a combination of the words "electric" and "ion". The suffix -"on" which is now used to designate other subatomic particles, such as a proton or neutron, is in turn derived from electron.

The discovery of electrons by Joseph Thomson was closely tied with the experimental and theoretical research of cathode rays for decades by many physicists. While studying electrical conductivity in rarefied gases in 1859, the German physicist Julius Plücker observed that the phosphorescent light, which was caused by radiation emitted from the cathode, appeared at the tube wall near the cathode, and the region of the phosphorescent light could be moved by application of a magnetic field. In 1869, Plucker's student Johann Wilhelm Hittorf found that a solid body placed in between the cathode and the phosphorescence would cast a shadow upon the phosphorescent region of the tube. Hittorf inferred that there are straight rays emitted from the cathode and that the phosphorescence was caused by the rays striking the tube walls. In 1876, the German physicist Eugen Goldstein showed that the rays were emitted perpendicular to the cathode surface, which distinguished between the rays that were emitted from the cathode and the incandescent light. Goldstein dubbed the rays cathode rays. 

During the 1870s, the English chemist and physicist Sir William Crookes developed the first cathode ray tube to have a high vacuum inside. He then showed in 1874 that the cathode rays can turn a small paddle wheel when placed in their path. Therefore, he concluded that the rays carried momentum. Furthermore, by applying a magnetic field, he was able to deflect the rays, thereby demonstrating that the beam behaved as though it were negatively charged. In 1879, he proposed that these properties could be explained by regarding cathode rays as composed of negatively charged gaseous molecules in fourth state of matter in which the mean free path of the particles is so long that collisions may be ignored.

The German-born British physicist Arthur Schuster expanded upon Crookes' experiments by placing metal plates parallel to the cathode rays and applying an electric potential between the plates. The field deflected the rays toward the positively charged plate, providing further evidence that the rays carried negative charge. By measuring the amount of deflection for a given level of current, in 1890 Schuster was able to estimate the charge-to-mass ratio of the ray components. However, this produced a value that was more than a thousand times greater than what was expected, so little credence was given to his calculations at the time.

In 1892 Hendrik Lorentz suggested that the mass of these particles (electrons) could be a consequence of their electric charge.
While studying naturally fluorescing minerals in 1896, the French physicist Henri Becquerel discovered that they emitted radiation without any exposure to an external energy source. These radioactive materials became the subject of much interest by scientists, including the New Zealand physicist Ernest Rutherford who discovered they emitted particles. He designated these particles alpha and beta, on the basis of their ability to penetrate matter. In 1900, Becquerel showed that the beta rays emitted by radium could be deflected by an electric field, and that their mass-to-charge ratio was the same as for cathode rays. This evidence strengthened the view that electrons existed as components of atoms.

In 1897, the British physicist J. J. Thomson, with his colleagues John S. Townsend and H. A. Wilson, performed experiments indicating that cathode rays really were unique particles, rather than waves, atoms or molecules as was believed earlier. Thomson made good estimates of both the charge "e" and the mass "m", finding that cathode ray particles, which he called "corpuscles", had perhaps one thousandth of the mass of the least massive ion known: hydrogen. He showed that their charge-to-mass ratio, "e"/"m", was independent of cathode material. He further showed that the negatively charged particles produced by radioactive materials, by heated materials and by illuminated materials were universal. The name electron was adopted for these particles by the scientific community, mainly due to the advocation by G. F. Fitzgerald, J. Larmor, and H. A. Lorenz.

The electron's charge was more carefully measured by the American physicists Robert Millikan and Harvey Fletcher in their oil-drop experiment of 1909, the results of which were published in 1911. This experiment used an electric field to prevent a charged droplet of oil from falling as a result of gravity. This device could measure the electric charge from as few as 1–150 ions with an error margin of less than 0.3%. Comparable experiments had been done earlier by Thomson's team, using clouds of charged water droplets generated by electrolysis, and in 1911 by Abram Ioffe, who independently obtained the same result as Millikan using charged microparticles of metals, then published his results in 1913. However, oil drops were more stable than water drops because of their slower evaporation rate, and thus more suited to precise experimentation over longer periods of time.

Around the beginning of the twentieth century, it was found that under certain conditions a fast-moving charged particle caused a condensation of supersaturated water vapor along its path. In 1911, Charles Wilson used this principle to devise his cloud chamber so he could photograph the tracks of charged particles, such as fast-moving electrons.

By 1914, experiments by physicists Ernest Rutherford, Henry Moseley, James Franck and Gustav Hertz had largely established the structure of an atom as a dense nucleus of positive charge surrounded by lower-mass electrons. In 1913, Danish physicist Niels Bohr postulated that electrons resided in quantized energy states, with their energies determined by the angular momentum of the electron's orbit about the nucleus. The electrons could move between those states, or orbits, by the emission or absorption of photons of specific frequencies. By means of these quantized orbits, he accurately explained the spectral lines of the hydrogen atom. However, Bohr's model failed to account for the relative intensities of the spectral lines and it was unsuccessful in explaining the spectra of more complex atoms.

Chemical bonds between atoms were explained by Gilbert Newton Lewis, who in 1916 proposed that a covalent bond between two atoms is maintained by a pair of electrons shared between them. Later, in 1927, Walter Heitler and Fritz London gave the full explanation of the electron-pair formation and chemical bonding in terms of quantum mechanics. In 1919, the American chemist Irving Langmuir elaborated on the Lewis' static model of the atom and suggested that all electrons were distributed in successive "concentric (nearly) spherical shells, all of equal thickness". In turn, he divided the shells into a number of cells each of which contained one pair of electrons. With this model Langmuir was able to qualitatively explain the chemical properties of all elements in the periodic table, which were known to largely repeat themselves according to the periodic law.

In 1924, Austrian physicist Wolfgang Pauli observed that the shell-like structure of the atom could be explained by a set of four parameters that defined every quantum energy state, as long as each state was occupied by no more than a single electron. This prohibition against more than one electron occupying the same quantum energy state became known as the Pauli exclusion principle. The physical mechanism to explain the fourth parameter, which had two distinct possible values, was provided by the Dutch physicists Samuel Goudsmit and George Uhlenbeck. In 1925, they suggested that an electron, in addition to the angular momentum of its orbit, possesses an intrinsic angular momentum and magnetic dipole moment. This is analogous to the rotation of the Earth on its axis as it orbits the Sun. The intrinsic angular momentum became known as spin, and explained the previously mysterious splitting of spectral lines observed with a high-resolution spectrograph; this phenomenon is known as fine structure splitting.

In his 1924 dissertation "" (Research on Quantum Theory), French physicist Louis de Broglie hypothesized that all matter can be represented as a de Broglie wave in the manner of light. That is, under the appropriate conditions, electrons and other matter would show properties of either particles or waves. The corpuscular properties of a particle are demonstrated when it is shown to have a localized position in space along its trajectory at any given moment. The wave-like nature of light is displayed, for example, when a beam of light is passed through parallel slits thereby creating interference patterns. In 1927, George Paget Thomson discovered the interference effect was produced when a beam of electrons was passed through thin metal foils and by American physicists Clinton Davisson and Lester Germer by the reflection of electrons from a crystal of nickel.
De Broglie's prediction of a wave nature for electrons led Erwin Schrödinger to postulate a wave equation for electrons moving under the influence of the nucleus in the atom. In 1926, this equation, the Schrödinger equation, successfully described how electron waves propagated. Rather than yielding a solution that determined the location of an electron over time, this wave equation also could be used to predict the probability of finding an electron near a position, especially a position near where the electron was bound in space, for which the electron wave equations did not change in time. This approach led to a second formulation of quantum mechanics (the first by Heisenberg in 1925), and solutions of Schrödinger's equation, like Heisenberg's, provided derivations of the energy states of an electron in a hydrogen atom that were equivalent to those that had been derived first by Bohr in 1913, and that were known to reproduce the hydrogen spectrum. Once spin and the interaction between multiple electrons were describable, quantum mechanics made it possible to predict the configuration of electrons in atoms with atomic numbers greater than hydrogen.

In 1928, building on Wolfgang Pauli's work, Paul Dirac produced a model of the electron – the Dirac equation, consistent with relativity theory, by applying relativistic and symmetry considerations to the hamiltonian formulation of the quantum mechanics of the electro-magnetic field. In order to resolve some problems within his relativistic equation, Dirac developed in 1930 a model of the vacuum as an infinite sea of particles with negative energy, later dubbed the Dirac sea. This led him to predict the existence of a positron, the antimatter counterpart of the electron. This particle was discovered in 1932 by Carl Anderson, who proposed calling standard electrons "negatons" and using "electron" as a generic term to describe both the positively and negatively charged variants.

In 1947, Willis Lamb, working in collaboration with graduate student Robert Retherford, found that certain quantum states of the hydrogen atom, which should have the same energy, were shifted in relation to each other; the difference came to be called the Lamb shift. About the same time, Polykarp Kusch, working with Henry M. Foley, discovered the magnetic moment of the electron is slightly larger than predicted by Dirac's theory. This small difference was later called anomalous magnetic dipole moment of the electron. This difference was later explained by the theory of quantum electrodynamics, developed by Sin-Itiro Tomonaga, Julian Schwinger and
Richard Feynman in the late 1940s.

With the development of the particle accelerator during the first half of the twentieth century, physicists began to delve deeper into the properties of subatomic particles. The first successful attempt to accelerate electrons using electromagnetic induction was made in 1942 by Donald Kerst. His initial betatron reached energies of 2.3 MeV, while subsequent betatrons achieved 300 MeV. In 1947, synchrotron radiation was discovered with a 70 MeV electron synchrotron at General Electric. This radiation was caused by the acceleration of electrons through a magnetic field as they moved near the speed of light.

With a beam energy of 1.5 GeV, the first high-energy
particle collider was ADONE, which began operations in 1968. This device accelerated electrons and positrons in opposite directions, effectively doubling the energy of their collision when compared to striking a static target with an electron. The Large Electron–Positron Collider (LEP) at CERN, which was operational from 1989 to 2000, achieved collision energies of 209 GeV and made important measurements for the Standard Model of particle physics.

Individual electrons can now be easily confined in ultra small (, ) CMOS transistors operated at cryogenic temperature over a range of −269 °C (4 K) to about −258 °C (15 K). The electron wavefunction spreads in a semiconductor lattice and negligibly interacts with the valence band electrons, so it can be treated in the single particle formalism, by replacing its mass with the effective mass tensor.

In the Standard Model of particle physics, electrons belong to the group of subatomic particles called leptons, which are believed to be fundamental or elementary particles. Electrons have the lowest mass of any charged lepton (or electrically charged particle of any type) and belong to the first-generation of fundamental particles. The second and third generation contain charged leptons, the muon and the tau, which are identical to the electron in charge, spin and interactions, but are more massive. Leptons differ from the other basic constituent of matter, the quarks, by their lack of strong interaction. All members of the lepton group are fermions, because they all have half-odd integer spin; the electron has spin .

The invariant mass of an electron is approximately  kilograms, or  atomic mass units. On the basis of Einstein's principle of mass–energy equivalence, this mass corresponds to a rest energy of 0.511 MeV. The ratio between the mass of a proton and that of an electron is about 1836. Astronomical measurements show that the proton-to-electron mass ratio has held the same value, as is predicted by the Standard Model, for at least half the age of the universe.

Electrons have an electric charge of coulombs, which is used as a standard unit of charge for subatomic particles, and is also called the elementary charge. Within the limits of experimental accuracy, the electron charge is identical to the charge of a proton, but with the opposite sign. As the symbol "e" is used for the elementary charge, the electron is commonly symbolized by , where the minus sign indicates the negative charge. The positron is symbolized by because it has the same properties as the electron but with a positive rather than negative charge.

The electron has an intrinsic angular momentum or spin of . This property is usually stated by referring to the electron as a spin- particle. For such particles the spin magnitude is , while the result of the measurement of a projection of the spin on any axis can only be ±. In addition to spin, the electron has an intrinsic magnetic moment along its spin axis. It is approximately equal to one Bohr magneton,=\frac{e\hbar}{2m_{\mathrm{e}}}.</math>}} which is a physical constant equal to . The orientation of the spin with respect to the momentum of the electron defines the property of elementary particles known as helicity.

The electron has no known substructure.

The issue of the radius of the electron is a challenging problem of modern theoretical physics. The admission of the hypothesis of a finite radius of the electron is incompatible to the premises of the theory of relativity. On the other hand, a point-like electron (zero radius) generates serious mathematical difficulties due to the self-energy of the electron tending to infinity. Observation of a single electron in a Penning trap suggests the upper limit of the particle's radius to be 10 meters.
The upper bound of the electron radius of 10 meters can be derived using the uncertainty relation in energy. There "is" also a physical constant called the "classical electron radius", with the much larger value of , greater than the radius of the proton. However, the terminology comes from a simplistic calculation that ignores the effects of quantum mechanics; in reality, the so-called classical electron radius has little to do with the true fundamental structure of the electron.

There are elementary particles that spontaneously decay into less massive particles. An example is the muon, with a mean lifetime of  seconds, which decays into an electron, a muon neutrino and an electron antineutrino. The electron, on the other hand, is thought to be stable on theoretical grounds: the electron is the least massive particle with non-zero electric charge, so its decay would violate charge conservation. The experimental lower bound for the electron's mean lifetime is years, at a 90% confidence level.

As with all particles, electrons can act as waves. This is called the wave–particle duality and can be demonstrated using the double-slit experiment.

The wave-like nature of the electron allows it to pass through two parallel slits simultaneously, rather than just one slit as would be the case for a classical particle. In quantum mechanics, the wave-like property of one particle can be described mathematically as a complex-valued function, the wave function, commonly denoted by the Greek letter psi ("ψ"). When the absolute value of this function is squared, it gives the probability that a particle will be observed near a location—a probability density.
Electrons are identical particles because they cannot be distinguished from each other by their intrinsic physical properties. In quantum mechanics, this means that a pair of interacting electrons must be able to swap positions without an observable change to the state of the system. The wave function of fermions, including electrons, is antisymmetric, meaning that it changes sign when two electrons are swapped; that is, , where the variables "r" and "r" correspond to the first and second electrons, respectively. Since the absolute value is not changed by a sign swap, this corresponds to equal probabilities. Bosons, such as the photon, have symmetric wave functions instead.

In the case of antisymmetry, solutions of the wave equation for interacting electrons result in a zero probability that each pair will occupy the same location or state. This is responsible for the Pauli exclusion principle, which precludes any two electrons from occupying the same quantum state. This principle explains many of the properties of electrons. For example, it causes groups of bound electrons to occupy different orbitals in an atom, rather than all overlapping each other in the same orbit.

In a simplified picture, which often tends to give the wrong idea but may serve to illustrate some aspects, every photon spends some time as a combination of a virtual electron plus its antiparticle, the virtual positron, which rapidly annihilate each other shortly thereafter. The combination of the energy variation needed to create these particles, and the time during which they exist, fall under the threshold of detectability expressed by the Heisenberg uncertainty relation, Δ"E" · Δ"t" ≥ "ħ". In effect, the energy needed to create these virtual particles, Δ"E", can be "borrowed" from the vacuum for a period of time, Δ"t", so that their product is no more than the reduced Planck constant, . Thus, for a virtual electron, Δ"t" is at most .
While an electron–positron virtual pair is in existence, the Coulomb force from the ambient electric field surrounding an electron causes a created positron to be attracted to the original electron, while a created electron experiences a repulsion. This causes what is called vacuum polarization. In effect, the vacuum behaves like a medium having a dielectric permittivity more than unity. Thus the effective charge of an electron is actually smaller than its true value, and the charge decreases with increasing distance from the electron. This polarization was confirmed experimentally in 1997 using the Japanese TRISTAN particle accelerator. Virtual particles cause a comparable shielding effect for the mass of the electron.

The interaction with virtual particles also explains the small (about 0.1%) deviation of the intrinsic magnetic moment of the electron from the Bohr magneton (the anomalous magnetic moment). The extraordinarily precise agreement of this predicted difference with the experimentally determined value is viewed as one of the great achievements of quantum electrodynamics.

The apparent paradox in classical physics of a point particle electron having intrinsic angular momentum and magnetic moment can be explained by the formation of virtual photons in the electric field generated by the electron. These photons cause the electron to shift about in a jittery fashion (known as zitterbewegung), which results in a net circular motion with precession. This motion produces both the spin and the magnetic moment of the electron. In atoms, this creation of virtual photons explains the Lamb shift observed in spectral lines. The Compton Wavelength shows that near elementary particles such as the electron, the uncertainty of the energy allows for the creation of virtual particles near the electron. This wavelength explains the "static" of virtual particles around elementary particles at a close distance.

An electron generates an electric field that exerts an attractive force on a particle with a positive charge, such as the proton, and a repulsive force on a particle with a negative charge. The strength of this force in nonrelativistic approximation is determined by Coulomb's inverse square law. When an electron is in motion, it generates a magnetic field. The Ampère-Maxwell law relates the magnetic field to the mass motion of electrons (the current) with respect to an observer. This property of induction supplies the magnetic field that drives an electric motor. The electromagnetic field of an arbitrary moving charged particle is expressed by the Liénard–Wiechert potentials, which are valid even when the particle's speed is close to that of light (relativistic).
When an electron is moving through a magnetic field, it is subject to the Lorentz force that acts perpendicularly to the plane defined by the magnetic field and the electron velocity. This centripetal force causes the electron to follow a helical trajectory through the field at a radius called the gyroradius. The acceleration from this curving motion induces the electron to radiate energy in the form of synchrotron radiation. The energy emission in turn causes a recoil of the electron, known as the Abraham–Lorentz–Dirac Force, which creates a friction that slows the electron. This force is caused by a back-reaction of the electron's own field upon itself.
Photons mediate electromagnetic interactions between particles in quantum electrodynamics. An isolated electron at a constant velocity cannot emit or absorb a real photon; doing so would violate conservation of energy and momentum. Instead, virtual photons can transfer momentum between two charged particles. This exchange of virtual photons, for example, generates the Coulomb force. Energy emission can occur when a moving electron is deflected by a charged particle, such as a proton. The acceleration of the electron results in the emission of Bremsstrahlung radiation.

An inelastic collision between a photon (light) and a solitary (free) electron is called Compton scattering. This collision results in a transfer of momentum and energy between the particles, which modifies the wavelength of the photon by an amount called the Compton shift.c} (1 - \cos \theta),</math>
where "c" is the speed of light in a vacuum and "m" is the electron mass. See Zombeck (2007). }} The maximum magnitude of this wavelength shift is "h"/"m""c", which is known as the Compton wavelength. For an electron, it has a value of . When the wavelength of the light is long (for instance, the wavelength of the visible light is 0.4–0.7 μm) the wavelength shift becomes negligible. Such interaction between the light and free electrons is called Thomson scattering or linear Thomson scattering.

The relative strength of the electromagnetic interaction between two charged particles, such as an electron and a proton, is given by the fine-structure constant. This value is a dimensionless quantity formed by the ratio of two energies: the electrostatic energy of attraction (or repulsion) at a separation of one Compton wavelength, and the rest energy of the charge. It is given by "α" ≈ , which is approximately equal to .

When electrons and positrons collide, they annihilate each other, giving rise to two or more gamma ray photons. If the electron and positron have negligible momentum, a positronium atom can form before annihilation results in two or three gamma ray photons totalling 1.022 MeV. On the other hand, a high-energy photon can transform into an electron and a positron by a process called pair production, but only in the presence of a nearby charged particle, such as a nucleus.

In the theory of electroweak interaction, the left-handed component of electron's wavefunction forms a weak isospin doublet with the electron neutrino. This means that during weak interactions, electron neutrinos behave like electrons. Either member of this doublet can undergo a charged current interaction by emitting or absorbing a and be converted into the other member. Charge is conserved during this reaction because the W boson also carries a charge, canceling out any net change during the transmutation. Charged current interactions are responsible for the phenomenon of beta decay in a radioactive atom. Both the electron and electron neutrino can undergo a neutral current interaction via a exchange, and this is responsible for neutrino-electron elastic scattering.
An electron can be "bound" to the nucleus of an atom by the attractive Coulomb force. A system of one or more electrons bound to a nucleus is called an atom. If the number of electrons is different from the nucleus' electrical charge, such an atom is called an ion. The wave-like behavior of a bound electron is described by a function called an atomic orbital. Each orbital has its own set of quantum numbers such as energy, angular momentum and projection of angular momentum, and only a discrete set of these orbitals exist around the nucleus. According to the Pauli exclusion principle each orbital can be occupied by up to two electrons, which must differ in their spin quantum number.

Electrons can transfer between different orbitals by the emission or absorption of photons with an energy that matches the difference in potential. Other methods of orbital transfer include collisions with particles, such as electrons, and the Auger effect. To escape the atom, the energy of the electron must be increased above its binding energy to the atom. This occurs, for example, with the photoelectric effect, where an incident photon exceeding the atom's ionization energy is absorbed by the electron.

The orbital angular momentum of electrons is quantized. Because the electron is charged, it produces an orbital magnetic moment that is proportional to the angular momentum. The net magnetic moment of an atom is equal to the vector sum of orbital and spin magnetic moments of all electrons and the nucleus. The magnetic moment of the nucleus is negligible compared with that of the electrons. The magnetic moments of the electrons that occupy the same orbital (so called, paired electrons) cancel each other out.

The chemical bond between atoms occurs as a result of electromagnetic interactions, as described by the laws of quantum mechanics. The strongest bonds are formed by the sharing or transfer of electrons between atoms, allowing the formation of molecules. Within a molecule, electrons move under the influence of several nuclei, and occupy molecular orbitals; much as they can occupy atomic orbitals in isolated atoms. A fundamental factor in these molecular structures is the existence of electron pairs. These are electrons with opposed spins, allowing them to occupy the same molecular orbital without violating the Pauli exclusion principle (much like in atoms). Different molecular orbitals have different spatial distribution of the electron density. For instance, in bonded pairs (i.e. in the pairs that actually bind atoms together) electrons can be found with the maximal probability in a relatively small volume between the nuclei. By contrast, in non-bonded pairs electrons are distributed in a large volume around nuclei.

If a body has more or fewer electrons than are required to balance the positive charge of the nuclei, then that object has a net electric charge. When there is an excess of electrons, the object is said to be negatively charged. When there are fewer electrons than the number of protons in nuclei, the object is said to be positively charged. When the number of electrons and the number of protons are equal, their charges cancel each other and the object is said to be electrically neutral. A macroscopic body can develop an electric charge through rubbing, by the triboelectric effect.

Independent electrons moving in vacuum are termed "free" electrons. Electrons in metals also behave as if they were free. In reality the particles that are commonly termed electrons in metals and other solids are quasi-electrons—quasiparticles, which have the same electrical charge, spin, and magnetic moment as real electrons but might have a different mass. When free electrons—both in vacuum and metals—move, they produce a net flow of charge called an electric current, which generates a magnetic field. Likewise a current can be created by a changing magnetic field. These interactions are described mathematically by Maxwell's equations.

At a given temperature, each material has an electrical conductivity that determines the value of electric current when an electric potential is applied. Examples of good conductors include metals such as copper and gold, whereas glass and Teflon are poor conductors. In any dielectric material, the electrons remain bound to their respective atoms and the material behaves as an insulator. Most semiconductors have a variable level of conductivity that lies between the extremes of conduction and insulation. On the other hand, metals have an electronic band structure containing partially filled electronic bands. The presence of such bands allows electrons in metals to behave as if they were free or delocalized electrons. These electrons are not associated with specific atoms, so when an electric field is applied, they are free to move like a gas (called Fermi gas) through the material much like free electrons.

Because of collisions between electrons and atoms, the drift velocity of electrons in a conductor is on the order of millimeters per second. However, the speed at which a change of current at one point in the material causes changes in currents in other parts of the material, the velocity of propagation, is typically about 75% of light speed. This occurs because electrical signals propagate as a wave, with the velocity dependent on the dielectric constant of the material.

Metals make relatively good conductors of heat, primarily because the delocalized electrons are free to transport thermal energy between atoms. However, unlike electrical conductivity, the thermal conductivity of a metal is nearly independent of temperature. This is expressed mathematically by the Wiedemann–Franz law, which states that the ratio of thermal conductivity to the electrical conductivity is proportional to the temperature. The thermal disorder in the metallic lattice increases the electrical resistivity of the material, producing a temperature dependence for electric current.

When cooled below a point called the critical temperature, materials can undergo a phase transition in which they lose all resistivity to electric current, in a process known as superconductivity. In BCS theory, pairs of electrons called Cooper pairs have their motion coupled to nearby matter via lattice vibrations called phonons, thereby avoiding the collisions with atoms that normally create electrical resistance. (Cooper pairs have a radius of roughly 100 nm, so they can overlap each other.) However, the mechanism by which higher temperature superconductors operate remains uncertain.

Electrons inside conducting solids, which are quasi-particles themselves, when tightly confined at temperatures close to absolute zero, behave as though they had split into three other quasiparticles: spinons, orbitons and holons. The former carries spin and magnetic moment, the next carries its orbital location while the latter electrical charge.

According to Einstein's theory of special relativity, as an electron's speed approaches the speed of light, from an observer's point of view its relativistic mass increases, thereby making it more and more difficult to accelerate it from within the observer's frame of reference. The speed of an electron can approach, but never reach, the speed of light in a vacuum, "c". However, when relativistic electrons—that is, electrons moving at a speed close to "c"—are injected into a dielectric medium such as water, where the local speed of light is significantly less than "c", the electrons temporarily travel faster than light in the medium. As they interact with the medium, they generate a faint light called Cherenkov radiation.
The effects of special relativity are based on a quantity known as the Lorentz factor, defined as formula_1 where "v" is the speed of the particle. The kinetic energy "K" of an electron moving with velocity "v" is:
where "m" is the mass of electron. For example, the Stanford linear accelerator can accelerate an electron to roughly 51 GeV.
Since an electron behaves as a wave, at a given velocity it has a characteristic de Broglie wavelength. This is given by "λ" = "h"/"p" where "h" is the Planck constant and "p" is the momentum. For the 51 GeV electron above, the wavelength is about , small enough to explore structures well below the size of an atomic nucleus.

The Big Bang theory is the most widely accepted scientific theory to explain the early stages in the evolution of the Universe. For the first millisecond of the Big Bang, the temperatures were over 10 billion kelvins and photons had mean energies over a million electronvolts. These photons were sufficiently energetic that they could react with each other to form pairs of electrons and positrons. Likewise, positron-electron pairs annihilated each other and emitted energetic photons:
An equilibrium between electrons, positrons and photons was maintained during this phase of the evolution of the Universe. After 15 seconds had passed, however, the temperature of the universe dropped below the threshold where electron-positron formation could occur. Most of the surviving electrons and positrons annihilated each other, releasing gamma radiation that briefly reheated the universe.

For reasons that remain uncertain, during the annihilation process there was an excess in the number of particles over antiparticles. Hence, about one electron for every billion electron-positron pairs survived. This excess matched the excess of protons over antiprotons, in a condition known as baryon asymmetry, resulting in a net charge of zero for the universe. The surviving protons and neutrons began to participate in reactions with each other—in the process known as nucleosynthesis, forming isotopes of hydrogen and helium, with trace amounts of lithium. This process peaked after about five minutes. Any leftover neutrons underwent negative beta decay with a half-life of about a thousand seconds, releasing a proton and electron in the process,
For about the next –, the excess electrons remained too energetic to bind with atomic nuclei. What followed is a period known as recombination, when neutral atoms were formed and the expanding universe became transparent to radiation.
Roughly one million years after the big bang, the first generation of stars began to form. Within a star, stellar nucleosynthesis results in the production of positrons from the fusion of atomic nuclei. These antimatter particles immediately annihilate with electrons, releasing gamma rays. The net result is a steady reduction in the number of electrons, and a matching increase in the number of neutrons. However, the process of stellar evolution can result in the synthesis of radioactive isotopes. Selected isotopes can subsequently undergo negative beta decay, emitting an electron and antineutrino from the nucleus. An example is the cobalt-60 (Co) isotope, which decays to form nickel-60 ().

At the end of its lifetime, a star with more than about 20 solar masses can undergo gravitational collapse to form a black hole. According to classical physics, these massive stellar objects exert a gravitational attraction that is strong enough to prevent anything, even electromagnetic radiation, from escaping past the Schwarzschild radius. However, quantum mechanical effects are believed to potentially allow the emission of Hawking radiation at this distance. Electrons (and positrons) are thought to be created at the event horizon of these stellar remnants.

When a pair of virtual particles (such as an electron and positron) is created in the vicinity of the event horizon, random spatial positioning might result in one of them to appear on the exterior; this process is called quantum tunnelling. The gravitational potential of the black hole can then supply the energy that transforms this virtual particle into a real particle, allowing it to radiate away into space. In exchange, the other member of the pair is given negative energy, which results in a net loss of mass-energy by the black hole. The rate of Hawking radiation increases with decreasing mass, eventually causing the black hole to evaporate away until, finally, it explodes.
Cosmic rays are particles traveling through space with high energies. Energy events as high as have been recorded. When these particles collide with nucleons in the Earth's atmosphere, a shower of particles is generated, including pions. More than half of the cosmic radiation observed from the Earth's surface consists of muons. The particle called a muon is a lepton produced in the upper atmosphere by the decay of a pion.
A muon, in turn, can decay to form an electron or positron.

Remote observation of electrons requires detection of their radiated energy. For example, in high-energy environments such as the corona of a star, free electrons form a plasma that radiates energy due to Bremsstrahlung radiation. Electron gas can undergo plasma oscillation, which is waves caused by synchronized variations in electron density, and these produce energy emissions that can be detected by using radio telescopes.

The frequency of a photon is proportional to its energy. As a bound electron transitions between different energy levels of an atom, it absorbs or emits photons at characteristic frequencies. For instance, when atoms are irradiated by a source with a broad spectrum, distinct dark lines appear in the spectrum of transmitted radiation in places where the corresponding frequency is absorbed by the atom's electrons. Each element or molecule displays a characteristic set of spectral lines, such as the hydrogen spectral series. When detected, spectroscopic measurements of the strength and width of these lines allow the composition and physical properties of a substance to be determined.

In laboratory conditions, the interactions of individual electrons can be observed by means of particle detectors, which allow measurement of specific properties such as energy, spin and charge. The development of the Paul trap and Penning trap allows charged particles to be contained within a small region for long durations. This enables precise measurements of the particle properties. For example, in one instance a Penning trap was used to contain a single electron for a period of 10 months. The magnetic moment of the electron was measured to a precision of eleven digits, which, in 1980, was a greater accuracy than for any other physical constant.

The first video images of an electron's energy distribution were captured by a team at Lund University in Sweden, February 2008. The scientists used extremely short flashes of light, called attosecond pulses, which allowed an electron's motion to be observed for the first time.

The distribution of the electrons in solid materials can be visualized by angle-resolved photoemission spectroscopy (ARPES). This technique employs the photoelectric effect to measure the reciprocal space—a mathematical representation of periodic structures that is used to infer the original structure. ARPES can be used to determine the direction, speed and scattering of electrons within the material.

Electron beams are used in welding. They allow energy densities up to across a narrow focus diameter of and usually require no filler material. This welding technique must be performed in a vacuum to prevent the electrons from interacting with the gas before reaching their target, and it can be used to join conductive materials that would otherwise be considered unsuitable for welding.

Electron-beam lithography (EBL) is a method of etching semiconductors at resolutions smaller than a micrometer. This technique is limited by high costs, slow performance, the need to operate the beam in the vacuum and the tendency of the electrons to scatter in solids. The last problem limits the resolution to about 10 nm. For this reason, EBL is primarily used for the production of small numbers of specialized integrated circuits.

Electron beam processing is used to irradiate materials in order to change their physical properties or sterilize medical and food products. Electron beams fluidise or quasi-melt glasses without significant increase of temperature on intensive irradiation: e.g. intensive electron radiation causes a many orders of magnitude decrease of viscosity and stepwise decrease of its activation energy.

Linear particle accelerators generate electron beams for treatment of superficial tumors in radiation therapy. Electron therapy can treat such skin lesions as basal-cell carcinomas because an electron beam only penetrates to a limited depth before being absorbed, typically up to 5 cm for electron energies in the range 5–20 MeV. An electron beam can be used to supplement the treatment of areas that have been irradiated by X-rays.

Particle accelerators use electric fields to propel electrons and their antiparticles to high energies. These particles emit synchrotron radiation as they pass through magnetic fields. The dependency of the intensity of this radiation upon spin polarizes the electron beam—a process known as the Sokolov–Ternov effect. Polarized electron beams can be useful for various experiments. Synchrotron radiation can also cool the electron beams to reduce the momentum spread of the particles. Electron and positron beams are collided upon the particles' accelerating to the required energies; particle detectors observe the resulting energy emissions, which particle physics studies .

Low-energy electron diffraction (LEED) is a method of bombarding a crystalline material with a collimated beam of electrons and then observing the resulting diffraction patterns to determine the structure of the material. The required energy of the electrons is typically in the range 20–200 eV. The reflection high-energy electron diffraction (RHEED) technique uses the reflection of a beam of electrons fired at various low angles to characterize the surface of crystalline materials. The beam energy is typically in the range 8–20 keV and the angle of incidence is 1–4°.

The electron microscope directs a focused beam of electrons at a specimen. Some electrons change their properties, such as movement direction, angle, and relative phase and energy as the beam interacts with the material. Microscopists can record these changes in the electron beam to produce atomically resolved images of the material. In blue light, conventional optical microscopes have a diffraction-limited resolution of about 200 nm. By comparison, electron microscopes are limited by the de Broglie wavelength of the electron. This wavelength, for example, is equal to 0.0037 nm for electrons accelerated across a 100,000-volt potential. The Transmission Electron Aberration-Corrected Microscope is capable of sub-0.05 nm resolution, which is more than enough to resolve individual atoms. This capability makes the electron microscope a useful laboratory instrument for high resolution imaging. However, electron microscopes are expensive instruments that are costly to maintain.

Two main types of electron microscopes exist: transmission and scanning. Transmission electron microscopes function like overhead projectors, with a beam of electrons passing through a slice of material then being projected by lenses on a photographic slide or a charge-coupled device. Scanning electron microscopes rasteri a finely focused electron beam, as in a TV set, across the studied sample to produce the image. Magnifications range from 100× to 1,000,000× or higher for both microscope types. The scanning tunneling microscope uses quantum tunneling of electrons from a sharp metal tip into the studied material and can produce atomically resolved images of its surface.

In the free-electron laser (FEL), a relativistic electron beam passes through a pair of undulators that contain arrays of dipole magnets whose fields point in alternating directions. The electrons emit synchrotron radiation that coherently interacts with the same electrons to strongly amplify the radiation field at the resonance frequency. FEL can emit a coherent high-brilliance electromagnetic radiation with a wide range of frequencies, from microwaves to soft X-rays. These devices are used in manufacturing, communication, and in medical applications, such as soft tissue surgery.

Electrons are important in cathode ray tubes, which have been extensively used as display devices in laboratory instruments, computer monitors and television sets. In a photomultiplier tube, every photon striking the photocathode initiates an avalanche of electrons that produces a detectable current pulse. Vacuum tubes use the flow of electrons to manipulate electrical signals, and they played a critical role in the development of electronics technology. However, they have been largely supplanted by solid-state devices such as the transistor.


</doc>
<doc id="9477" url="https://en.wikipedia.org/wiki?curid=9477" title="Europium">
Europium

Europium is a chemical element with the symbol Eu and atomic number 63. Europium is the most reactive lanthanide by far, having to be stored under an inert fluid to protect it from atmospheric oxygen or moisture. Europium is also the softest lanthanide, as it can be dented with a fingernail and easily cut with a knife. When oxidation is removed a shiny-white metal is visible. Europium was isolated in 1901 and is named after the continent of Europe. Being a typical member of the lanthanide series, europium usually assumes the oxidation state +3, but the oxidation state +2 is also common. All europium compounds with oxidation state +2 are slightly reducing. Europium has no significant biological role and is relatively non-toxic compared to other heavy metals. Most applications of europium exploit the phosphorescence of europium compounds. Europium is one of the rarest of the rare earth elements on Earth.

Europium is a ductile metal with a hardness similar to that of lead. It crystallizes in a body-centered cubic lattice. Some properties of europium are strongly influenced by its half-filled electron shell. Europium has the second lowest melting point and the lowest density of all lanthanides.

Europium becomes a superconductor when it is cooled below 1.8 K and compressed to above 80 GPa. This occurs because europium is divalent in the metallic state, and is converted into the trivalent state by the applied pressure. In the divalent state, the strong local magnetic moment (J = /) suppresses the superconductivity, which is induced by eliminating this local moment (J = 0 in Eu).

Europium is the most reactive rare-earth element. It rapidly oxidizes in air, so that bulk oxidation of a centimeter-sized sample occurs within several days. Its reactivity with water is comparable to that of calcium, and the reaction is

Because of the high reactivity, samples of solid europium rarely have the shiny appearance of the fresh metal, even when coated with a protective layer of mineral oil. Europium ignites in air at 150 to 180 °C to form europium(III) oxide:

Europium dissolves readily in dilute sulfuric acid to form pale pink solutions of the hydrated Eu(III), which exist as a nonahydrate:

Although usually trivalent, europium readily forms divalent compounds. This behavior is unusual for most lanthanides, which almost exclusively form compounds with an oxidation state of +3. The +2 state has an electron configuration 4"f" because the half-filled "f"-shell provides more stability. In terms of size and coordination number, europium(II) and barium(II) are similar. The sulfates of both barium and europium(II) are also highly insoluble in water. Divalent europium is a mild reducing agent, oxidizing in air to form Eu(III) compounds. In anaerobic, and particularly geothermal conditions, the divalent form is sufficiently stable that it tends to be incorporated into minerals of calcium and the other alkaline earths. This ion-exchange process is the basis of the "negative europium anomaly", the low europium content in many lanthanide minerals such as monazite, relative to the chondritic abundance. Bastnäsite tends to show less of a negative europium anomaly than does monazite, and hence is the major source of europium today. The development of easy methods to separate divalent europium from the other (trivalent) lanthanides made europium accessible even when present in low concentration, as it usually is.

Naturally occurring europium is composed of 2 isotopes, Eu and Eu, which occur in almost equal proportions; Eu is slightly more abundant (52.2% natural abundance). While Eu is stable, Eu was found to be unstable to alpha decay with a half-life of in 2007, giving about 1 alpha decay per two minutes in every kilogram of natural europium. This value is in reasonable agreement with theoretical predictions. Besides the natural radioisotope Eu, 35 artificial radioisotopes have been characterized, the most stable being Eu with a half-life of 36.9 years, Eu with a half-life of 13.516 years, and Eu with a half-life of 8.593 years. All the remaining radioactive isotopes have half-lives shorter than 4.7612 years, and the majority of these have half-lives shorter than 12.2 seconds. This element also has 8 meta states, with the most stable being Eu ("t"=12.8 hours), Eu ("t"=9.3116 hours) and Eu ("t"=96 minutes).

The primary decay mode for isotopes lighter than Eu is electron capture, and the primary mode for heavier isotopes is beta minus decay. The primary decay products before Eu are isotopes of samarium (Sm) and the primary products after are isotopes of gadolinium (Gd).

Europium is produced by nuclear fission, but the fission product yields of europium isotopes are low near the top of the mass range for fission products.

As with other lanthanides, many isotopes of europium, especially those that have odd mass numbers or are neutron-poor like Eu, have high cross sections for neutron capture, often high enough to be neutron poisons.

Eu is the beta decay product of samarium-151, but since this has a long decay half-life and short mean time to neutron absorption, most Sm instead ends up as Sm.

Eu (half-life 13.516 years) and Eu (half-life 8.593 years) cannot be beta decay products because Sm and Sm are non-radioactive, but Eu is the only long-lived "shielded" nuclide, other than Cs, to have a fission yield of more than 2.5 parts per million fissions. A larger amount of Eu is produced by neutron activation of a significant portion of the non-radioactive Eu; however, much of this is further converted to Eu.

Eu (half-life 4.7612 years) has a fission yield of 330 parts per million (ppm) for uranium-235 and thermal neutrons; most of it is transmuted to non-radioactive and nonabsorptive gadolinium-156 by the end of fuel burnup.

Overall, europium is overshadowed by caesium-137 and strontium-90 as a radiation hazard, and by samarium and others as a neutron poison.

Europium is not found in nature as a free element. Many minerals contain europium, with the most important sources being bastnäsite, monazite, xenotime and loparite-(Ce). No europium-dominant minerals are known yet, despite a single find of a tiny possible Eu–O or Eu–O–C system phase in the Moon's regolith.

Depletion or enrichment of europium in minerals relative to other rare-earth elements is known as the europium anomaly. Europium is commonly included in trace element studies in geochemistry and petrology to understand the processes that form igneous rocks (rocks that cooled from magma or lava). The nature of the europium anomaly found helps reconstruct the relationships within a suite of igneous rocks. The average crustal abundance of europium is 2–2.2 ppm.

Divalent europium (Eu) in small amounts is the activator of the bright blue fluorescence of some samples of the mineral fluorite (CaF). The reduction from Eu to Eu is induced by irradiation with energetic particles. The most outstanding examples of this originated around Weardale and adjacent parts of northern England; it was the fluorite found here that fluorescence was named after in 1852, although it was not until much later that europium was determined to be the cause.

In astrophysics, the signature of europium in stellar spectra can be used to classify stars and inform theories of how or where a particular star was born. For instance, astronomers in 2019 identified higher-than-expected levels of europium within the star J1124+4535, hypothesizing that this star originated in a dwarf galaxy that collided with the Milky Way billions of years ago.

Europium is associated with the other rare-earth elements and is, therefore, mined together with them. Separation of the rare-earth elements occurs during later processing. Rare-earth elements are found in the minerals bastnäsite, loparite-(Ce), xenotime, and monazite in mineable quantities. Bastnäsite is a group of related fluorocarbonates, Ln(CO)(F,OH). Monazite is a group of related of orthophosphate minerals (Ln denotes a mixture of all the lanthanides except promethium), loparite-(Ce) is an oxide, and xenotime is an orthophosphate (Y,Yb,Er...)PO. Monazite also contains thorium and yttrium, which complicates handling because thorium and its decay products are radioactive. For the extraction from the ore and the isolation of individual lanthanides, several methods have been developed. The choice of method is based on the concentration and composition of the ore and on the distribution of the individual lanthanides in the resulting concentrate. Roasting the ore, followed by acidic and basic leaching, is used mostly to produce a concentrate of lanthanides. If cerium is the dominant lanthanide, then it is converted from cerium(III) to cerium(IV) and then precipitated. Further separation by solvent extractions or ion exchange chromatography yields a fraction which is enriched in europium. This fraction is reduced with zinc, zinc/amalgam, electrolysis or other methods converting the europium(III) to europium(II). Europium(II) reacts in a way similar to that of alkaline earth metals and therefore it can be precipitated as a carbonate or co-precipitated with barium sulfate. Europium metal is available through the electrolysis of a mixture of molten EuCl and NaCl (or CaCl) in a graphite cell, which serves as cathode, using graphite as anode. The other product is chlorine gas.

A few large deposits produce or produced a significant amount of the world production. The Bayan Obo iron ore deposit contains significant amounts of bastnäsite and monazite and is, with an estimated 36 million tonnes of rare-earth element oxides, the largest known deposit. The mining operations at the Bayan Obo deposit made China the largest supplier of rare-earth elements in the 1990s. Only 0.2% of the rare-earth element content is europium. The second large source for rare-earth elements between 1965 and its closure in the late 1990s was the Mountain Pass rare earth mine. The bastnäsite mined there is especially rich in the light rare-earth elements (La-Gd, Sc, and Y) and contains only 0.1% of europium. Another large source for rare-earth elements is the loparite found on the Kola peninsula. It contains besides niobium, tantalum and titanium up to 30% rare-earth elements and is the largest source for these elements in Russia.

Europium compounds tend to exist trivalent oxidation state under most conditions. Commonly these compounds feature Eu(III) bound by 6–9 oxygenic ligands, typically water. These compounds, the chlorides, sulfates, nitrates, are soluble in water or polar organic solvent. Lipophilic europium complexes often feature acetylacetonate-like ligands, e.g., Eufod.

Europium metal reacts with all the halogens:
This route gives white europium(III) fluoride (EuF), yellow europium(III) chloride (EuCl), gray europium(III) bromide (EuBr), and colorless europium(III) iodide (EuI). Europium also forms the corresponding dihalides: yellow-green europium(II) fluoride (EuF), colorless europium(II) chloride (EuCl), colorless europium(II) bromide (EuBr), and green europium(II) iodide (EuI).

Europium forms stable compounds with all of the chalcogens, but the heavier chalcogens (S, Se, and Te) stabilize the lower oxidation state. Three oxides are known: europium(II) oxide (EuO), europium(III) oxide (EuO), and the mixed-valence oxide EuO, consisting of both Eu(II) and Eu(III).
Otherwise, the main chalcogenides are europium(II) sulfide (EuS), europium(II) selenide (EuSe) and europium(II) telluride (EuTe): all three of these are black solids. EuS is prepared by sulfiding the oxide at temperatures sufficiently high to decompose the EuO:
The main nitride is europium(III) nitride (EuN).

Although europium is present in most of the minerals containing the other rare elements, due to the difficulties in separating the elements it was not until the late 1800s that the element was isolated. William Crookes observed the phosphorescent spectra of the rare elements including those eventually assigned to europium.

Europium was first found in 1892 by Paul Émile Lecoq de Boisbaudran, who obtained basic fractions from samarium-gadolinium concentrates which had spectral lines not accounted for by samarium or gadolinium. However, the discovery of europium is generally credited to French chemist Eugène-Anatole Demarçay, who suspected samples of the recently discovered element samarium were contaminated with an unknown element in 1896 and who was able to isolate it in 1901; he then named it "europium".

When the europium-doped yttrium orthovanadate red phosphor was discovered in the early 1960s, and understood to be about to cause a revolution in the color television industry, there was a scramble for the limited supply of europium on hand among the monazite processors, as the typical europium content in monazite is about 0.05%. However, the Molycorp bastnäsite deposit at the Mountain Pass rare earth mine, California, whose lanthanides had an unusually high europium content of 0.1%, was about to come on-line and provide sufficient europium to sustain the industry. Prior to europium, the color-TV red phosphor was very weak, and the other phosphor colors had to be muted, to maintain color balance. With the brilliant red europium phosphor, it was no longer necessary to mute the other colors, and a much brighter color TV picture was the result. Europium has continued to be in use in the TV industry ever since as well as in computer monitors. Californian bastnäsite now faces stiff competition from Bayan Obo, China, with an even "richer" europium content of 0.2%.

Frank Spedding, celebrated for his development of the ion-exchange technology that revolutionized the rare-earth industry in the mid-1950s, once related the story of how he was lecturing on the rare earths in the 1930s, when an elderly gentleman approached him with an offer of a gift of several pounds of europium oxide. This was an unheard-of quantity at the time, and Spedding did not take the man seriously. However, a package duly arrived in the mail, containing several pounds of genuine europium oxide. The elderly gentleman had turned out to be Herbert Newby McCoy, who had developed a famous method of europium purification involving redox chemistry.

Relative to most other elements, commercial applications for europium are few and rather specialized. Almost invariably, its phosphorescence is exploited, either in the +2 or +3 oxidation state.

It is a dopant in some types of glass in lasers and other optoelectronic devices. Europium oxide (EuO) is widely used as a red phosphor in television sets and fluorescent lamps, and as an activator for yttrium-based phosphors. Color TV screens contain between 0.5 and 1 g of europium oxide. Whereas trivalent europium gives red phosphors, the luminescence of divalent europium depends strongly on the composition of the host structure. UV to deep red luminescence can be achieved. The two classes of europium-based phosphor (red and blue), combined with the yellow/green terbium phosphors give "white" light, the color temperature of which can be varied by altering the proportion or specific composition of the individual phosphors. This phosphor system is typically encountered in helical fluorescent light bulbs. Combining the same three classes is one way to make trichromatic systems in TV and computer screens, but as an additive, it can be particularly effective in improving the intensity of red phosphor. Europium is also used in the manufacture of fluorescent glass, increasing the general efficiency of fluorescent lamps. One of the more common persistent after-glow phosphors besides copper-doped zinc sulfide is europium-doped strontium aluminate. Europium fluorescence is used to interrogate biomolecular interactions in drug-discovery screens. It is also used in the anti-counterfeiting phosphors in euro banknotes.
An application that has almost fallen out of use with the introduction of affordable superconducting magnets is the use of europium complexes, such as Eu(fod), as shift reagents in NMR spectroscopy. Chiral shift reagents, such as Eu(hfc), are still used to determine enantiomeric purity.

A recent (2015) application of europium is in quantum memory chips which can reliably store information for days at a time; these could allow sensitive quantum data to be stored to a hard disk-like device and shipped around.

There are no clear indications that europium is particularly toxic compared to other heavy metals. Europium chloride, nitrate and oxide have been tested for toxicity: europium chloride shows an acute intraperitoneal LD toxicity of 550 mg/kg and the acute oral LD toxicity is 5000 mg/kg. Europium nitrate shows a slightly higher intraperitoneal LD toxicity of 320 mg/kg, while the oral toxicity is above 5000 mg/kg. The metal dust presents a fire and explosion hazard.



</doc>
<doc id="9478" url="https://en.wikipedia.org/wiki?curid=9478" title="Erbium">
Erbium

Erbium is a chemical element with the symbol Er and atomic number 68. A silvery-white solid metal when artificially isolated, natural erbium is always found in chemical combination with other elements. It is a lanthanide, a rare earth element, originally found in the gadolinite mine in Ytterby in Sweden, from which it got its name.

Erbium's principal uses involve its pink-colored Er ions, which have optical fluorescent properties particularly useful in certain laser applications. Erbium-doped glasses or crystals can be used as optical amplification media, where Er ions are optically pumped at around 980 or and then radiate light at in stimulated emission. This process results in an unusually mechanically simple laser optical amplifier for signals transmitted by fiber optics. The wavelength is especially important for optical communications because standard single mode optical fibers have minimal loss at this particular wavelength.

In addition to optical fiber amplifier-lasers, a large variety of medical applications (i.e. dermatology, dentistry) rely on the erbium ion's emission (see ) when lit at another wavelength, which is highly absorbed in water in tissues, making its effect very superficial. Such shallow tissue deposition of laser energy is helpful in laser surgery, and for the efficient production of steam which produces enamel ablation by common types of dental laser.

A trivalent element, pure erbium metal is malleable (or easily shaped), soft yet stable in air, and does not oxidize as quickly as some other rare-earth metals. Its salts are rose-colored, and the element has characteristic sharp absorption spectra bands in visible light, ultraviolet, and near infrared. Otherwise it looks much like the other rare earths. Its sesquioxide is called erbia. Erbium's properties are to a degree dictated by the kind and amount of impurities present. Erbium does not play any known biological role, but is thought to be able to stimulate metabolism.

Erbium is ferromagnetic below 19 K, antiferromagnetic between 19 and 80 K and paramagnetic above 80 K.

Erbium can form propeller-shaped atomic clusters ErN, where the distance between the erbium atoms is 0.35 nm. Those clusters can be isolated by encapsulating them into fullerene molecules, as confirmed by transmission electron microscopy.

Erbium metal tarnishes slowly in air and burns readily to form erbium(III) oxide:

Erbium is quite electropositive and reacts slowly with cold water and quite quickly with hot water to form erbium hydroxide:

Erbium metal reacts with all the halogens:

Erbium dissolves readily in dilute sulfuric acid to form solutions containing hydrated Er(III) ions, which exist as rose red [Er(OH)] hydration complexes:

Naturally occurring erbium is composed of 6 stable isotopes, , , , , , and , with being the most abundant (33.503% natural abundance). 29 radioisotopes have been characterized, with the most stable being with a half-life of , with a half-life of , with a half-life of , with a half-life of , and with a half-life of . All of the remaining radioactive isotopes have half-lives that are less than , and the majority of these have half-lives that are less than 4 minutes. This element also has 13 meta states, with the most stable being with a half-life of .

The isotopes of erbium range in atomic weight from () to (). The primary decay mode before the most abundant stable isotope, , is electron capture, and the primary mode after is beta decay. The primary decay products before are element 67 (holmium) isotopes, and the primary products after are element 69 (thulium) isotopes.

Erbium (for Ytterby, a village in Sweden) was discovered by Carl Gustaf Mosander in 1843. Mosander was working with a sample of what was thought to be the single metal oxide yttria, derived from the mineral gadolinite. He discovered that the sample contained at least two metal oxides in addition to pure yttria, which he named "erbia" and "terbia" after the village of Ytterby where the gadolinite had been found. Mosander was not certain of the purity of the oxides and later tests confirmed his uncertainty. Not only did the "yttria" contain yttrium, erbium, and terbium; in the ensuing years, chemists, geologists and spectroscopists discovered five additional elements: ytterbium, scandium, thulium, holmium, and gadolinium.

Erbia and terbia, however, were confused at this time. A spectroscopist mistakenly switched the names of the two elements during spectroscopy. After 1860, terbia was renamed erbia and after 1877 what had been known as erbia was renamed terbia. Fairly pure ErO was independently isolated in 1905 by Georges Urbain and Charles James. Reasonably pure erbium metal was not produced until 1934 when Wilhelm Klemm and Heinrich Bommer reduced the anhydrous chloride with potassium vapor. It was only in the 1990s that the price for Chinese-derived erbium oxide became low enough for erbium to be considered for use as a colorant in art glass.

The concentration of erbium in the Earth crust is about 2.8 mg/kg and in the sea water 0.9 ng/L. This concentration is enough to make erbium about 45th in elemental abundance in the Earth's crust.

Like other rare earths, this element is never found as a free element in nature but is found bound in monazite sand ores. It has historically been very difficult and expensive to separate rare earths from each other in their ores but ion-exchange chromatography methods developed in the late 20th century have greatly brought down the cost of production of all rare-earth metals and their chemical compounds.

The principal commercial sources of erbium are from the minerals xenotime and euxenite, and most recently, the ion adsorption clays of southern China; in consequence, China has now become the principal global supplier of this element. In the high-yttrium versions of these ore concentrates, yttrium is about two-thirds of the total by weight, and erbia is about 4–5%. When the concentrate is dissolved in acid, the erbia liberates enough erbium ion to impart a distinct and characteristic pink color to the solution. This color behavior is similar to what Mosander and the other early workers in the lanthanides would have seen in their extracts from the gadolinite minerals of Ytterby.

Crushed minerals are attacked by hydrochloric or sulfuric acid that transforms insoluble rare-earth oxides into soluble chlorides or sulfates. The acidic filtrates are partially neutralized with caustic soda (sodium hydroxide) to pH 3–4. Thorium precipitates out of solution as hydroxide and is removed. After that the solution is treated with ammonium oxalate to convert rare earths into their insoluble oxalates. The oxalates are converted to oxides by annealing. The oxides are dissolved in nitric acid that excludes one of the main components, cerium, whose oxide is insoluble in HNO. The solution is treated with magnesium nitrate to produce a crystallized mixture of double salts of rare-earth metals. The salts are separated by ion exchange. In this process, rare-earth ions are sorbed onto suitable ion-exchange resin by exchange with hydrogen, ammonium or cupric ions present in the resin. The rare earth ions are then selectively washed out by suitable complexing agent. Erbium metal is obtained from its oxide or salts by heating with calcium at under argon atmosphere.

Erbium's everyday uses are varied. It is commonly used as a photographic filter, and because of its resilience it is useful as a metallurgical additive.

A large variety of medical applications (i.e. dermatology, dentistry) utilize erbium ion's emission (see ), which is highly absorbed in water (absorption coefficient about ). Such shallow tissue deposition of laser energy is necessary for laser surgery, and the efficient production of steam for laser enamel ablation in dentistry.

Erbium-doped optical silica-glass fibers are the active element in erbium-doped fiber amplifiers (EDFAs), which are widely used in optical communications. The same fibers can be used to create fiber lasers. In order to work efficiently, erbium-doped fiber is usually co-doped with glass modifiers/homogenizers, often aluminum or phosphorus. These dopants help prevent clustering of Er ions and transfer the energy more efficiently between excitation light (also known as optical pump) and the signal. Co-doping of optical fiber with Er and Yb is used in high-power Er/Yb fiber lasers. Erbium can also be used in erbium-doped waveguide amplifiers.

When added to vanadium as an alloy, erbium lowers hardness and improves workability. An erbium-nickel alloy ErNi has an unusually high specific heat capacity at liquid-helium temperatures and is used in cryocoolers; a mixture of 65% ErCo and 35% ErYbNi by volume improves the specific heat capacity even more.

Erbium oxide has a pink color, and is sometimes used as a colorant for glass, cubic zirconia and porcelain. The glass is then often used in sunglasses and cheap jewelry.

Erbium is used in nuclear technology in neutron-absorbing control rods.

Erbium does not have a biological role, but erbium salts can stimulate metabolism. Humans consume 1 milligram of erbium a year on average. The highest concentration of erbium in humans is in the bones, but there is also erbium in the human kidneys and liver.

Erbium is slightly toxic if ingested, but erbium compounds are not toxic. Metallic erbium in dust form presents a fire and explosion hazard.




</doc>
<doc id="9479" url="https://en.wikipedia.org/wiki?curid=9479" title="Einsteinium">
Einsteinium

Einsteinium is a synthetic element with the symbol Es and atomic number 99. As a member of the actinide series, it is the seventh transuranic element.

Einsteinium was discovered as a component of the debris of the first hydrogen bomb explosion in 1952, and named after Albert Einstein. Its most common isotope einsteinium-253 (half-life 20.47 days) is produced artificially from decay of californium-253 in a few dedicated high-power nuclear reactors with a total yield on the order of one milligram per year. The reactor synthesis is followed by a complex process of separating einsteinium-253 from other actinides and products of their decay. Other isotopes are synthesized in various laboratories, but in much smaller amounts, by bombarding heavy actinide elements with light ions. Owing to the small amounts of produced einsteinium and the short half-life of its most easily produced isotope, there are currently almost no practical applications for it outside basic scientific research. In particular, einsteinium was used to synthesize, for the first time, 17 atoms of the new element mendelevium in 1955.

Einsteinium is a soft, silvery, paramagnetic metal. Its chemistry is typical of the late actinides, with a preponderance of the +3 oxidation state; the +2 oxidation state is also accessible, especially in solids. The high radioactivity of einsteinium-253 produces a visible glow and rapidly damages its crystalline metal lattice, with released heat of about 1000 watts per gram. Difficulty in studying its properties is due to einsteinium-253's decay to berkelium-249 and then californium-249 at a rate of about 3% per day. The isotope of einsteinium with the longest half-life, einsteinium-252 (half-life 471.7 days) would be more suitable for investigation of physical properties, but it has proven far more difficult to produce and is available only in minute quantities, and not in bulk. Einsteinium is the element with the highest atomic number which has been observed in macroscopic quantities in its pure form, and this was the common short-lived isotope einsteinium-253.

Like all synthetic transuranic elements, isotopes of einsteinium are very radioactive and are considered highly dangerous to health on ingestion.

Einsteinium was first identified in December 1952 by Albert Ghiorso and co-workers at the University of California, Berkeley in collaboration with the Argonne and Los Alamos National Laboratories, in the fallout from the "Ivy Mike" nuclear test. The test was carried out on November 1, 1952, at Enewetak Atoll in the Pacific Ocean and was the first successful test of a hydrogen bomb. Initial examination of the debris from the explosion had shown the production of a new isotope of plutonium, , which could only have formed by the absorption of six neutrons by a uranium-238 nucleus followed by two beta decays.
At the time, the multiple neutron absorption was thought to be an extremely rare process, but the identification of indicated that still more neutrons could have been captured by the uranium nuclei, thereby producing new elements heavier than californium.
Ghiorso and co-workers analyzed filter papers which had been flown through the explosion cloud on airplanes (the same sampling technique that had been used to discover ). Larger amounts of radioactive material were later isolated from coral debris of the atoll, which were delivered to the U.S. The separation of suspected new elements was carried out in the presence of a citric acid/ammonium buffer solution in a weakly acidic medium (pH ≈ 3.5), using ion exchange at elevated temperatures; fewer than 200 atoms of einsteinium were recovered in the end. Nevertheless, element 99 (einsteinium), namely its Es isotope, could be detected via its characteristic high-energy alpha decay at 6.6 MeV. It was produced by the capture of 15 neutrons by uranium-238 nuclei followed by seven beta-decays, and had a half-life of 20.5 days. Such multiple neutron absorption was made possible by the high neutron flux density during the detonation, so that newly generated heavy isotopes had plenty of available neutrons to absorb before they could disintegrate into lighter elements. Neutron capture initially raised the mass number without changing the atomic number of the nuclide, and the concomitant beta-decays resulted in a gradual increase in the atomic number:
^{238}_{92}U ->[\ce{+15n}][6 \beta^-] ^{253}_{98}Cf ->[\beta^-] ^{253}_{99}Es
</chem>

Some U atoms, however, could absorb two additional neutrons (for a total of 17), resulting in Es, as well as in the Fm isotope of another new element, fermium. The discovery of the new elements and the associated new data on multiple neutron capture were initially kept secret on the orders of the U.S. military until 1955 due to Cold War tensions and competition with Soviet Union in nuclear technologies. However, the rapid capture of so many neutrons would provide needed direct experimental confirmation of the so-called r-process multiple neutron absorption needed to explain the cosmic nucleosynthesis (production) of certain heavy chemical elements (heavier than nickel) in supernova explosions, before beta decay. Such a process is needed to explain the existence of many stable elements in the universe.

Meanwhile, isotopes of element 99 (as well as of new element 100, fermium) were produced in the Berkeley and Argonne laboratories, in a nuclear reaction between nitrogen-14 and uranium-238, and later by intense neutron irradiation of plutonium or californium:

These results were published in several articles in 1954 with the disclaimer that these were not the first studies that had been carried out on the elements. The Berkeley team also reported some results on the chemical properties of einsteinium and fermium. The "Ivy Mike" results were declassified and published in 1955.
In their discovery of the elements 99 and 100, the American teams had competed with a group at the Nobel Institute for Physics, Stockholm, Sweden. In late 1953 – early 1954, the Swedish group succeeded in the synthesis of light isotopes of element 100, in particular Fm, by bombarding uranium with oxygen nuclei. These results were also published in 1954. Nevertheless, the priority of the Berkeley team was generally recognized, as its publications preceded the Swedish article, and they were based on the previously undisclosed results of the 1952 thermonuclear explosion; thus the Berkeley team was given the privilege to name the new elements. As the effort which had led to the design of "Ivy Mike" was codenamed Project PANDA, element 99 had been jokingly nicknamed "Pandamonium" but the official names suggested by the Berkeley group derived from two prominent scientists, Albert Einstein and Enrico Fermi: "We suggest for the name for the element with the atomic number 99, einsteinium (symbol E) after Albert Einstein and for the name for the element with atomic number 100, fermium (symbol Fm), after Enrico Fermi." Both Einstein and Fermi died between the time the names were originally proposed and when they were announced. The discovery of these new elements was announced by Albert Ghiorso at the first Geneva Atomic Conference held on 8–20 August 1955. The symbol for einsteinium was first given as "E" and later changed to "Es" by IUPAC.

Einsteinium is a synthetic, silvery-white, radioactive metal. In the periodic table, it is located to the right of the actinide californium, to the left of the actinide fermium and below the lanthanide holmium with which it shares many similarities in physical and chemical properties. Its density of 8.84 g/cm is lower than that of californium (15.1 g/cm) and is nearly the same as that of holmium (8.79 g/cm), despite atomic einsteinium being much heavier than holmium. The melting point of einsteinium (860 °C) is also relatively low – below californium (900 °C), fermium (1527 °C) and holmium (1461 °C). Einsteinium is a soft metal, with the bulk modulus of only 15 GPa, which value is one of the lowest among non-alkali metals.

Contrary to the lighter actinides californium, berkelium, curium and americium which crystallize in a double hexagonal structure at ambient conditions, einsteinium is believed to have a face-centered cubic ("fcc") symmetry with the space group "Fm""m" and the lattice constant "a" = 575 pm. However, there is a report of room-temperature hexagonal einsteinium metal with "a" = 398 pm and "c" = 650 pm, which converted to the "fcc" phase upon heating to 300 °C.

The self-damage induced by the radioactivity of einsteinium is so strong that it rapidly destroys the crystal lattice, and the energy release during this process, 1000 watts per gram of Es, induces a visible glow. These processes may contribute to the relatively low density and melting point of einsteinium. Further, owing to the small size of the available samples, the melting point of einsteinium was often deduced by observing the sample being heated inside an electron microscope. Thus the surface effects in small samples could reduce the melting point value.

The metal is divalent and has a noticeably high volatility. In order to reduce the self-radiation damage, most measurements of solid einsteinium and its compounds are performed right after thermal annealing. Also, some compounds are studied under the atmosphere of the reductant gas, for example HO+HCl for EsOCl so that the sample is partly regrown during its decomposition.

Apart from the self-destruction of solid einsteinium and its compounds, other intrinsic difficulties in studying this element include scarcity – the most common Es isotope is available only once or twice a year in sub-milligram amounts – and self-contamination due to rapid conversion of einsteinium to berkelium and then to californium at a rate of about 3.3% per day:
^{253}_{99}Es ->[\alpha][20 \ce{d}] ^{249}_{97}Bk ->[\beta^-][314 \ce{d}] ^{249}_{98}Cf
</chem>

Thus, most einsteinium samples are contaminated, and their intrinsic properties are often deduced by extrapolating back experimental data accumulated over time. Other experimental techniques to circumvent the contamination problem include selective optical excitation of einsteinium ions by a tunable laser, such as in studying its luminescence properties.

Magnetic properties have been studied for einsteinium metal, its oxide and fluoride. All three materials showed Curie–Weiss paramagnetic behavior from liquid helium to room temperature. The effective magnetic moments were deduced as for EsO and for the EsF, which are the highest values among actinides, and the corresponding Curie temperatures are 53 and 37 K.

Like all actinides, einsteinium is rather reactive. Its trivalent oxidation state is most stable in solids and aqueous solution where it induces a pale pink color. The existence of divalent einsteinium is firmly established, especially in the solid phase; such +2 state is not observed in many other actinides, including protactinium, uranium, neptunium, plutonium, curium and berkelium. Einsteinium(II) compounds can be obtained, for example, by reducing einsteinium(III) with samarium(II) chloride. The oxidation state +4 was postulated from vapor studies and is yet uncertain.

Nineteen isotopes and three nuclear isomers are known for einsteinium, with mass numbers ranging from 240 to 257. All are radioactive and the most stable nuclide, Es, has a half-life of 471.7 days. The next most stable isotopes are Es (half-life 275.7 days), Es (39.8 days), and Es (20.47 days). All of the remaining isotopes have half-lives shorter than 40 hours, and most of them decay within less than 30 minutes. Of the three nuclear isomers, the most stable is Es with a half-life of 39.3 hours.

Einsteinium has a high rate of nuclear fission that results in a low critical mass for a sustained nuclear chain reaction. This mass is 9.89 kilograms for a bare sphere of Es isotope, and can be lowered to 2.9 by adding a 30-centimeter-thick steel neutron reflector, or even to 2.26 kilograms with a 20-cm-thick reflector made of water. However, even this small critical mass greatly exceeds the total amount of einsteinium isolated thus far, especially of the rare Es isotope.

Because of the short half-life of all isotopes of einsteinium, any primordial einsteinium—that is, einsteinium that could possibly have been present on the Earth during its formation—has long since decayed. Synthesis of einsteinium from naturally-occurring actinides uranium and thorium in the Earth's crust requires multiple neutron capture, which is an extremely unlikely event. Therefore, all terrestrial einsteinium is produced in scientific laboratories, high-power nuclear reactors, or in nuclear weapons tests, and is present only within a few years from the time of the synthesis.

The transuranic elements from americium to fermium, including einsteinium, occurred naturally in the natural nuclear fission reactor at Oklo, but no longer do so.

Einsteinium was observed in Przybylski's Star in 2008.

Einsteinium is produced in minute quantities by bombarding lighter actinides with neutrons in dedicated high-flux nuclear reactors. The world's major irradiation sources are the 85-megawatt High Flux Isotope Reactor (HFIR) at the Oak Ridge National Laboratory in Tennessee, U.S., and the SM-2 loop reactor at the Research Institute of Atomic Reactors (NIIAR) in Dimitrovgrad, Russia, which are both dedicated to the production of transcurium ("Z" > 96) elements. These facilities have similar power and flux levels, and are expected to have comparable production capacities for transcurium elements, although the quantities produced at NIIAR are not widely reported. In a "typical processing campaign" at Oak Ridge, tens of grams of curium are irradiated to produce decigram quantities of californium, milligram quantities of berkelium (Bk) and einsteinium and picogram quantities of fermium.

The first microscopic sample of Es sample weighing about 10 nanograms was prepared in 1961 at HFIR. A special magnetic balance was designed to estimate its weight. Larger batches were produced later starting from several kilograms of plutonium with the einsteinium yields (mostly Es) of 0.48 milligrams in 1967–1970, 3.2 milligrams in 1971–1973, followed by steady production of about 3 milligrams per year between 1974 and 1978. These quantities however refer to the integral amount in the target right after irradiation. Subsequent separation procedures reduced the amount of isotopically pure einsteinium roughly tenfold.

Heavy neutron irradiation of plutonium results in four major isotopes of einsteinium: Es (α-emitter with half-life of 20.47 days and with a spontaneous fission half-life of 7×10 years); Es (β-emitter with half-life of 39.3 hours), Es (α-emitter with half-life of about 276 days) and Es (β-emitter with half-life of 39.8 days). An alternative route involves bombardment of uranium-238 with high-intensity nitrogen or oxygen ion beams.

Einsteinium-247 (half-life 4.55 minutes) was produced by irradiating americium-241 with carbon or uranium-238 with nitrogen ions. The latter reaction was first realized in 1967 in Dubna, Russia, and the involved scientists were awarded the Lenin Komsomol Prize.

The isotope Es was produced by irradiating Cf with deuterium ions. It mainly decays by emission of electrons to Cf with a half-life of minutes, but also releases α-particles of 6.87 MeV energy, with the ratio of electrons to α-particles of about 400.

The heavier isotopes Es, Es, Es and Es were obtained by bombarding Bk with α-particles. One to four neutrons are liberated in this process making possible the formation of four different isotopes in one reaction.

Einsteinium-253 was produced by irradiating a 0.1–0.2 milligram Cf target with a thermal neutron flux of (2–5)×10 neutrons·cm·s for 500–900 hours:

The analysis of the debris at the 10-megaton "Ivy Mike" nuclear test was a part of long-term project. One of the goals of which was studying the efficiency of production of transuranium elements in high-power nuclear explosions. The motivation for these experiments was that synthesis of such elements from uranium requires multiple neutron capture. The probability of such events increases with the neutron flux, and nuclear explosions are the most powerful man-made neutron sources, providing densities of the order 10 neutrons/cm within a microsecond, or about 10 neutrons/(cm·s). In comparison, the flux of the HFIR reactor is 5 neutrons/(cm·s). A dedicated laboratory was set up right at Enewetak Atoll for preliminary analysis of debris, as some isotopes could have decayed by the time the debris samples reached the mainland U.S. The laboratory was receiving samples for analysis as soon as possible, from airplanes equipped with paper filters which flew over the atoll after the tests. Whereas it was hoped to discover new chemical elements heavier than fermium, none of these were found even after a series of megaton explosions conducted between 1954 and 1956 at the atoll.

The atmospheric results were supplemented by the underground test data accumulated in the 1960s at the Nevada Test Site, as it was hoped that powerful explosions conducted in confined space might result in improved yields and heavier isotopes. Apart from traditional uranium charges, combinations of uranium with americium and thorium have been tried, as well as a mixed plutonium-neptunium charge, but they were less successful in terms of yield and was attributed to stronger losses of heavy isotopes due to enhanced fission rates in heavy-element charges. Product isolation was problematic as the explosions were spreading debris through melting and vaporizing the surrounding rocks at depths of 300–600 meters. Drilling to such depths to extract the products was both slow and inefficient in terms of collected volumes.

Among the nine underground tests that were carried between 1962 and 1969, the last one was the most powerful and had the highest yield of transuranium elements. Milligrams of einsteinium that would normally take a year of irradiation in a high-power reactor, were produced within a microsecond. However, the major practical problem of the entire proposal was collecting the radioactive debris dispersed by the powerful blast. Aircraft filters adsorbed only about 4 of the total amount, and collection of tons of corals at Enewetak Atoll increased this fraction by only two orders of magnitude. Extraction of about 500 kilograms of underground rocks 60 days after the Hutch explosion recovered only about 1 of the total charge. The amount of transuranium elements in this 500-kg batch was only 30 times higher than in a 0.4 kg rock picked up 7 days after the test which demonstrated the highly non-linear dependence of the transuranium elements yield on the amount of retrieved radioactive rock. Shafts were drilled at the site before the test in order to accelerate sample collection after explosion, so that explosion would expel radioactive material from the epicenter through the shafts and to collecting volumes near the surface. This method was tried in two tests and instantly provided hundreds kilograms of material, but with actinide concentration 3 times lower than in samples obtained after drilling. Whereas such method could have been efficient in scientific studies of short-lived isotopes, it could not improve the overall collection efficiency of the produced actinides.

Although no new elements (apart from einsteinium and fermium) could be detected in the nuclear test debris, and the total yields of transuranium elements were disappointingly low, these tests did provide significantly higher amounts of rare heavy isotopes than previously available in laboratories.

Separation procedure of einsteinium depends on the synthesis method. In the case of light-ion bombardment inside a cyclotron, the heavy ion target is attached to a thin foil, and the generated einsteinium is simply washed off the foil after the irradiation. However, the produced amounts in such experiments are relatively low. The yields are much higher for reactor irradiation, but there, the product is a mixture of various actinide isotopes, as well as lanthanides produced in the nuclear fission decays. In this case, isolation of einsteinium is a tedious procedure which involves several repeating steps of cation exchange, at elevated temperature and pressure, and chromatography. Separation from berkelium is important, because the most common einsteinium isotope produced in nuclear reactors, Es, decays with a half-life of only 20 days to Bk, which is fast on the timescale of most experiments. Such separation relies on the fact that berkelium easily oxidizes to the solid +4 state and precipitates, whereas other actinides, including einsteinium, remain in their +3 state in solutions.

Separation of trivalent actinides from lanthanide fission products can be done by a cation-exchange resin column using a 90% water/10% ethanol solution saturated with hydrochloric acid (HCl) as eluant. It is usually followed by anion-exchange chromatography using 6 molar HCl as eluant. A cation-exchange resin column (Dowex-50 exchange column) treated with ammonium salts is then used to separate fractions containing elements 99, 100 and 101. These elements can be then identified simply based on their elution position/time, using α-hydroxyisobutyrate solution (α-HIB), for example, as eluant.

Separation of the 3+ actinides can also be achieved by solvent extraction chromatography, using bis-(2-ethylhexyl) phosphoric acid (abbreviated as HDEHP) as the stationary organic phase, and nitric acid as the mobile aqueous phase. The actinide elution sequence is reversed from that of the cation-exchange resin column. The einsteinium separated by this method has the advantage to be free of organic complexing agent, as compared to the separation using a resin column.

Einsteinium is highly reactive and therefore strong reducing agents are required to obtain the pure metal from its compounds. This can be achieved by reduction of einsteinium(III) fluoride with metallic lithium:

However, owing to its low melting point and high rate of self-radiation damage, einsteinium has high vapor pressure, which is higher than that of lithium fluoride. This makes this reduction reaction rather inefficient. It was tried in the early preparation attempts and quickly abandoned in favor of reduction of einsteinium(III) oxide with lanthanum metal:

Einsteinium(III) oxide (EsO) was obtained by burning einsteinium(III) nitrate. It forms colorless cubic crystals, which were first characterized from microgram samples sized about 30 nanometers. Two other phases, monoclinic and hexagonal, are known for this oxide. The formation of a certain EsO phase depends on the preparation technique and sample history, and there is no clear phase diagram. Interconversions between the three phases can occur spontaneously, as a result of self-irradiation or self-heating. The hexagonal phase is isotypic with lanthanum(III) oxide where the Es ion is surrounded by a 6-coordinated group of O ions.

Einsteinium halides are known for the oxidation states +2 and +3. The most stable state is +3 for all halides from fluoride to iodide.

Einsteinium(III) fluoride (EsF) can be precipitated from einsteinium(III) chloride solutions upon reaction with fluoride ions. An alternative preparation procedure is to exposure einsteinium(III) oxide to chlorine trifluoride (ClF) or F gas at a pressure of 1–2 atmospheres and a temperature between 300 and 400 °C. The EsF crystal structure is hexagonal, as in californium(III) fluoride (CfF) where the Es ions are 8-fold coordinated by fluorine ions in a bicapped trigonal prism arrangement.

Einsteinium(III) chloride (EsCl) can be prepared by annealing einsteinium(III) oxide in the atmosphere of dry hydrogen chloride vapors at about 500 °C for some 20 minutes. It crystallizes upon cooling at about 425 °C into an orange solid with a hexagonal structure of UCl type, where einsteinium atoms are 9-fold coordinated by chlorine atoms in a tricapped trigonal prism geometry. Einsteinium(III) bromide (EsBr) is a pale-yellow solid with a monoclinic structure of AlCl type, where the einsteinium atoms are octahedrally coordinated by bromine (coordination number 6).

The divalent compounds of einsteinium are obtained by reducing the trivalent halides with hydrogen:

Einsteinium(II) chloride (EsCl), einsteinium(II) bromide (EsBr), and einsteinium(II) iodide (EsI) have been produced and characterized by optical absorption, with no structural information available yet.

Known oxyhalides of einsteinium include EsOCl, EsOBr and EsOI. They are synthesized by treating a trihalide with a vapor mixture of water and the corresponding hydrogen halide: for example, EsCl + HO/HCl to obtain EsOCl.

The high radioactivity of einsteinium has a potential use in radiation therapy, and organometallic complexes have been synthesized in order to deliver einsteinium atoms to an appropriate organ in the body. Experiments have been performed on injecting einsteinium citrate (as well as fermium compounds) to dogs. Einsteinium(III) was also incorporated into beta-diketone chelate complexes, since analogous complexes with lanthanides previously showed strongest UV-excited luminescence among metallorganic compounds. When preparing einsteinium complexes, the Es ions were 1000 times diluted with Gd ions. This allowed reducing the radiation damage so that the compounds did not disintegrate during the period of 20 minutes required for the measurements. The resulting luminescence from Es was much too weak to be detected. This was explained by the unfavorable relative energies of the individual constituents of the compound that hindered efficient energy transfer from the chelate matrix to Es ions. Similar conclusion was drawn for other actinides americium, berkelium and fermium.

Luminescence of Es ions was however observed in inorganic hydrochloric acid solutions as well as in organic solution with di(2-ethylhexyl)orthophosphoric acid. It shows a broad peak at about 1064 nanometers (half-width about 100 nm) which can be resonantly excited by green light (ca. 495 nm wavelength). The luminescence has a lifetime of several microseconds and the quantum yield below 0.1%. The relatively high, compared to lanthanides, non-radiative decay rates in Es were associated with the stronger interaction of f-electrons with the inner Es electrons.

There is almost no use for any isotope of einsteinium outside basic scientific research aiming at production of higher transuranic elements and transactinides.

In 1955, mendelevium was synthesized by irradiating a target consisting of about 10 atoms of Es in the 60-inch cyclotron at Berkeley Laboratory. The resulting Es(α,n)Md reaction yielded 17 atoms of the new element with the atomic number of 101.

The rare isotope einsteinium-254 is favored for production of ultraheavy elements because of its large mass, relatively long half-life of 270 days, and availability in significant amounts of several micrograms. Hence einsteinium-254 was used as a target in the attempted synthesis of ununennium (element 119) in 1985 by bombarding it with calcium-48 ions at the superHILAC linear accelerator at Berkeley, California. No atoms were identified, setting an upper limit for the cross section of this reaction at 300 nanobarns.
Einsteinium-254 was used as the calibration marker in the chemical analysis spectrometer ("alpha-scattering surface analyzer") of the Surveyor 5 lunar probe. The large mass of this isotope reduced the spectral overlap between signals from the marker and the studied lighter elements of the lunar surface.

Most of the available einsteinium toxicity data originates from research on animals. Upon ingestion by rats, only about 0.01% einsteinium ends in the blood stream. From there, about 65% goes to the bones, where it remains for about 50 years, 25% to the lungs (biological half-life about 20 years, although this is rendered irrelevant by the short half-lives of einsteinium isotopes), 0.035% to the testicles or 0.01% to the ovaries – where einsteinium stays indefinitely. About 10% of the ingested amount is excreted. The distribution of einsteinium over the bone surfaces is uniform and is similar to that of plutonium.




</doc>
<doc id="9480" url="https://en.wikipedia.org/wiki?curid=9480" title="Edmund Stoiber">
Edmund Stoiber

Edmund Rüdiger Stoiber (born 28 September 1941) is a German politician who served as the 16th Minister President of the state of Bavaria between 1993 and 2007 and chairman of the Christian Social Union (CSU) between 1999 and 2007. In 2002 he ran for the office of Chancellor of Germany in the federal election, but in one of the narrowest elections in German history lost against Gerhard Schröder. On 18 January 2007, he announced his decision to step down from the posts of minister-president and party chairman by 30 September, after having been under fire in his own party for weeks.

Edmund Stoiber was born in Oberaudorf in the district of Rosenheim, Bavaria. Prior to entering politics in 1974 and serving in the Bavarian parliament, he was a lawyer and worked at the University of Regensburg.

Stoiber attended the Ignaz-Günther-Gymnasium in Rosenheim, where he received his Abitur (high school diploma) in 1961, although he had to repeat one year for failing in Latin. His military service was with the 1st Gebirgsdivision (mountain infantry division) in Mittenwald and Bad Reichenhall and was cut-short due to a knee injury. Then Stoiber studied at Ludwig Maximilians University of Munich political science and then, from fall 1962, law. In 1967, he passed the state law exam and then worked at the University of Regensburg in criminal law and Eastern European law. He was awarded a doctorate of jurisprudence, and then in 1971 passed the second state examination with distinction.

In 1971, Stoiber joined the Bavarian State Ministry of Development and Environment.

In 1978 Stoiber was elected secretary general of the CSU, a post he held until 1982/83. In this capacity, he served as campaign manager of Franz-Josef Strauss, the first Bavarian leader to run for the chancellorship, in the 1980 national elections. From 1982 to 1986 he served as deputy to the Bavarian secretary of the state and then, in the position of State Minister, led the State Chancellery from 1982 to 1988. From 1988 to 1993 he served as State Minister of the Interior.

In May 1993, the Landtag of Bavaria, the state's parliament, elected Stoiber as Minister-President succeeding Max Streibl. He came to power amid a political crisis involving a sex scandal, surrounding a contender for the state premiership. Upon taking office, he nominated Strauss' daughter Monika Hohlmeier as State Minister for Education and Cultural Affairs.

In his capacity as Minister-President, Stoiber served as President of the Bundesrat in 1995/96. In 1998, he also succeeded Theo Waigel as chairman of the CSU.

During Stoiber's 14 years leading Bavaria, the state solidified its position as one of Germany's richest. Already by 1998, under his leadership, the state had privatized more than $3 billion worth of state-owned businesses and used that money to invest in new infrastructure and provide venture capital for new companies. He was widely regarded a central figure in building one of Europe's most powerful regional economies, attracting thousands of hi-tech, engineering and media companies and reducing unemployment to half the national average.

In 2002, Stoiber politically outmaneuvered CDU chairwoman, Angela Merkel, and was declared the CDU/CSU's candidate for the office of chancellor by practically the entire leadership of the CSU's sister party CDU, challenging Gerhard Schröder. At that time, Merkel had generally been seen as a transitional chair and was strongly opposed by the CDU's male leaders, often called the party's "crown princes".

In the run up to the 2002 national elections, the CSU/CDU held a huge lead in the opinion polls and Stoiber famously remarked that "...this election is like a football match where it's the second half and my team is ahead by 2–0." However, on election day things had changed. The SPD had mounted a huge comeback, and the CDU/CSU was narrowly defeated (though both the SPD and CDU/CSU had 38.5% of the vote, the SPD was ahead by a small 6,000 vote margin, winning 251 seats to the CDU/CSU's 248). The election was one of modern Germany's closest votes.

Gerhard Schröder was re-elected as chancellor by the parliament in a coalition with the Greens, who had increased their vote share marginally. Many commentators faulted Stoiber's reaction to the floods in eastern Germany, in the run-up to the election, as a contributory factor in his party's poor electoral result and defeat. In addition, Schröder distinguished himself from his opponent by taking an active stance against the upcoming United States-led Iraq War. His extensive campaigning on this stance was widely seen as swinging the election to the SPD in the weeks running up to the election.

Stoiber subsequently led the CSU to an absolute majority in the 2003 Bavarian state elections, for the third time in a row, winning this time 60.7% of the votes and a two-thirds majority in the Landtag. This was the widest margin ever achieved by a German party in any state.

Between 2003 and 2004, Stoiber served as co-chair (alongside Franz Müntefering) of the First Commission on the modernization of the federal state ("Föderalismuskommission I"), which had been established to reform the division of powers between federal and state authorities in Germany. In February 2004, he became a candidate of Jacques Chirac and Gerhard Schröder for the presidency of the European Commission but he decided not to run for this office.

Stoiber had ambitions to run again for the chancellorship, but Merkel secured the nomination, and in November 2005 she won the general election. He was slated to join Merkel's first grand coalition cabinet as Economics minister. However, on 1 November 2005, he announced his decision to stay in Bavaria, due to personnel changes on the SPD side of the coalition (Franz Müntefering resigned as SPD chairman) and an unsatisfactory apportionment of competences between himself and designated Science minister Annette Schavan. Stoiber also resigned his seat in the 16th Bundestag, being a member from 18 October to 8 November.

Subsequently, criticism grew in the CSU, where other politicians had to scale back their ambitions after Stoiber's decision to stay in Bavaria. On 18 January 2007, he announced his decision to stand down from the posts of minister-president and party chairman by 30 September. Günther Beckstein, then Bavarian state minister of the interior, succeeded him as minister-president and Erwin Huber as party chairman, defeating Horst Seehofer at a convention at 18 September 2007 with 58,1% of the votes. Both Beckstein and Huber resigned after the 2008 state elections, in which the CSU vote dropped to 43,4% and the party had to form a coalition with another party for the first time since 1966.

Stoiber was first appointed in 2007 as a special adviser to then-European Commission President José Manuel Barroso to chair the "High level group on administrative burdens," made up of national experts, NGOs, business and industry organizations. Quickly nicknamed the "Stoiber Group," it produced a report in July 2014 with several proposals on streamlining the regulatory process. Stoiber was re-appointed in December 2014 by Jean-Claude Juncker to the same role, from which he resigned after one year in late 2015.

Since his retirement from German politics in 2007, Stoiber has worked as a lawyer and held paid and unpaid positions, including:


Stoiber was a CSU delegate to the Federal Convention for the purpose of electing the President of Germany in 2017.

In his capacity as Minister-President, Stoiber made 58 foreign trips, including to China (1995, 2003), Israel (2001), Egypt (2001), India (2004, 2007) and South Korea (2007).

In 2002, Stoiber publicly expressed support for the United States in their policy toward Iraq. During his election campaign, he made clear his opposition to war, and his support for the introduction of weapons inspectors to Iraq without preconditions as a way of avoiding war, and he criticized Schröder for harming the German-American alliance by not calling President George W. Bush and discussing the issue privately. He also attacked German Foreign Minister Joschka Fischer for his criticism of the U.S. position.

Stoiber is known for backing Vladimir Putin and there have been comparisons to Gerhard Schröder. One author called Stoiber a "Moscow's Trojan Horse". Putin is known to have given Stoiber "extreme forms of flattery" and privileges such as a private dinner at Putin's residence outside Moscow.

Stoiber has been said to be skeptical of Germany's decision to adopt the euro. In 1997, he joined the ministers-president of two other German states, Kurt Biedenkopf and Gerhard Schröder, in making the case for a five-year delay in Europe's currency union. When the European Commission recommended that Greece be allowed to join the eurozone in 1998, he demanded that the country be barred from adopting the common currency for several years instead. He is a staunch opponent of Turkey's integration into the European Union, claiming that its non-Christian culture would dilute the Union.
At the same time, Stoiber has repeatedly insisted he is a "good European" who is keen, for instance, on forging an EU-wide foreign policy, replete with a single European army. Earlier, in 1993, he had told German newspapers: "I want a simple confederation. That means the nation-states maintain their dominant role, at least as far as internal matters are concerned."

While the conservative wing of the German political spectrum, primarily formed of the CDU and CSU, enjoys considerable support, this support tends to be less extended to Stoiber. He enjoys considerably more support in his home state of Bavaria than in the rest of Germany, where CDU chairwoman Angela Merkel is more popular. This has its reasons: Merkel supports a kind of fiscal conservatism, but a more liberal social policy. Stoiber, on the other hand, favors a more conservative approach to both fiscal and social matters, and while this ensures him the religious vote, strongest in Bavaria, it has weakened his support at the national level.

In 2005, Stoiber successfully lobbied Novartis, the Swiss pharmaceuticals group, to move the headquarters of its Sandoz subsidiary to Munich, making it one of Europe's highest-profile corporate relocations that year as well as a significant boost to Stoiber's attempts to build up Bavaria as a pharmaceuticals and biotechnology center.

During his time as Minister-President of Bavaria, Stoiber pushed for the construction of a roughly 40-kilometer high-speed magnetic-levitation link from Munich's main station to its airport, to be built by Transrapid International, a consortium including ThyssenKrupp and Munich-based Siemens. After he left office, the German federal government abandoned the plans in 2008 because of spiraling costs of as much as €3.4 billion.

Stoiber, as a minister in the state of Bavaria, was widely known for advocating a reduction in the number of asylum seekers Germany accepts, something that prompted critics to label him xenophobic, anti-Turkish and anti-Islam. In the late 1990s he criticized the incoming Chancellor Schröder for saying that he would work hard in the interest of Germans "and" people living in Germany. Stoiber's remarks drew heavy criticism in the press.

When Germany's Federal Constitutional Court decided in 1995 that a Bavarian law requiring a crucifix to be hung in each of the state's 40,000 classrooms was unconstitutional, Stoiber said he would not order the removal of crucifixes "for the time being," and asserted that he was under no obligation to remove them in schools where parents unanimously opposed such action.

During his 2002 election campaign, Stoiber indicated he would not ban same-sex marriages – sanctioned by the Schröder government – a policy he had vehemently objected to when it was introduced.

Stoiber has been a staunch advocate of changes in German law that would give more power to owners of private TV channels. In 1995, he publicly called for the abolition of Germany's public television service ARD and a streamlining of its regional services, adding that he and Minister-President Kurt Biedenkopf of Saxony would break the contract ARD has with regional governments if reforms were not undertaken. However, when European Commissioner for Competition Karel van Miert unveiled ideas for reforming the rules governing the financing of public service broadcasters in 1998, Stoiber led the way in rejecting moves to reform established practice.

During the run-up to the German general election in 2005, which was held ahead of schedule, Stoiber created controversy through a campaign speech held in the beginning of August 2005 in the federal state of Baden-Württemberg. He said, "I do not accept that the East [of Germany] will again decide who will be Germany's chancellor. It cannot be allowed that the frustrated determine Germany's fate." People in the new federal states of Germany (the former German Democratic Republic) were offended by Stoiber's remarks. While the CSU attempted to portray them as "misinterpreted", Stoiber created further controversy when he claimed that "if it was like Bavaria everywhere, there wouldn't be any problems. Unfortunately, not everyone in Germany is as intelligent as in Bavaria." The tone of the comments was exacerbated by a perception by some within Germany of the state of Bavaria as "arrogant".

Many, including members of the CDU, attribute Stoiber's comments and behavior as a contributing factor to the CDU's losses in the 2005 general election. He was accused by many in the CDU/CSU of offering "half-hearted" support to Angela Merkel, with some even accusing him of being reluctant to support a female candidate from the East. (This also contrasted unfavorably with Merkel's robust support for his candidacy in the 2002 election.) He has insinuated that votes were lost because of the choice of a female candidate. He came under heavy fire for these comments from press and politicians alike, especially since he himself lost almost 10% of the Bavarian vote – a dubious feat in itself as Bavarians tend to consistently vote conservatively. Nonetheless, a poll has suggested over 9% may have voted differently if the conservative candidate was a man from the West, although this does not clearly show if such a candidate would have gained or lost votes for the conservatives.

When the Croatian National Bank turned down BayernLB's original bid to take over the local arm of Hypo Alpe-Adria-Bank International, this drew strong criticism from Stoiber, who said the decision was "unacceptable" and a "severe strain" for Bavaria's relations with Croatia. Croatia was seeking to join the European Union at the time. The central bank's board later reviewed and accepted BayernLB's offer of 1.6 billion euros. The investment in Hypo Group Alpe Adria was part of a series of ill-fated investments, which later forced BayernLB to take a 10 billion-euro bailout in the financial crisis.

In September 2015, Emily O'Reilly, the European Ombudsman, received a complaint from two NGOs, Corporate Europe Observatory and Friends of the Earth, according to which Stoiber's appointment as special adviser on the Commission's better regulation agenda broke internal rules on appointments.

Stoiber is Roman Catholic. He is married to Karin Stoiber. They have three children: Constanze (1971), Veronica (1977), Dominic (1980) and five grandchildren: Johannes (1999), Benedikt (2001), Theresa Marie (2005), Ferdinand (2009) and another grandson (2011).

Stoiber is a keen football fan and he serves as Co-Chairman on the Advisory Board of FC Bayern Munich. Before the 2002 election FC Bayern General Manager Uli Hoeneß expressed his support for Stoiber and the CSU. Football legend, former FC Bayern President and DFB Vice-President, Franz Beckenbauer, on the other hand, showed his support for Stoiber by letting him join the German national football team on their flight home from Japan after the 2002 FIFA World Cup.

In his youth, he played for local football side BCF Wolfratshausen.





 


</doc>
<doc id="9481" url="https://en.wikipedia.org/wiki?curid=9481" title="Erfurt">
Erfurt

Erfurt ( , ; ) is the capital and largest city in the state of Thuringia, central Germany.

Erfurt lies in the southern part of the Thuringian Basin, within the wide valley of the Gera river. It is located south-west of Leipzig, south-west of Berlin, north of Munich and north-east of Frankfurt. Together with a string of neighbouring cities Gotha, Weimar, Jena and others, Erfurt forms the central metropolitan corridor of Thuringia called "Thüringer Städtekette" (German "Thuringian city chain") with over 500,000 inhabitants.
Erfurt's old town is one of the best preserved medieval city centres in Germany. Tourist attractions include the Krämerbrücke (Merchants' bridge), the Old Synagogue, the ensemble of Erfurt Cathedral and "Severikirche" (St Severus's Church) and Petersberg Citadel, one of the largest and best preserved town fortresses in Europe. The city's economy is based on agriculture, horticulture and microelectronics. Its central location has led to it becoming a logistics hub for Germany and central Europe. Erfurt hosts the second-largest trade fair in eastern Germany (after Leipzig) as well as the public television children's channel KiKa.

The city is situated on the Via Regia, a medieval trade and pilgrims' road network. Modern day Erfurt is also a hub for ICE high speed trains and other German and European transport networks. Erfurt was first mentioned in 742, as Saint Boniface founded the diocese. Although the town did not belong to any of the Thuringian states politically, it quickly became the economic centre of the region and it was a member of the Hanseatic League. It was part of the Electorate of Mainz during the Holy Roman Empire, and later became part of the Kingdom of Prussia in 1802. From 1949 until 1990 Erfurt was part of the German Democratic Republic (East Germany).

The University of Erfurt was founded in 1379, making it the first university to be established within the geographic area which constitutes modern-day Germany. It closed in 1816 and was re-established in 1994, with the main modern campus on what was a teachers' training college. Martin Luther (1483–1546) was its most famous student, studying there from 1501 before entering St Augustine's Monastery in 1505. Other noted Erfurters include the medieval philosopher and mystic Meister Eckhart (c. 1260–1328), the Baroque composer Johann Pachelbel (1653–1706) and the sociologist Max Weber (1864–1920).

Erfurt is an old Germanic settlement. The earliest evidence of human settlement dates from the prehistoric era; archaeological finds from the north of Erfurt revealed human traces from the paleolithic period, ca. 100,000 BCE. The Melchendorf dig in the southern city part showed a settlement from the neolithic period. The Thuringii inhabited the Erfurt area ca. 480 and gave their name to Thuringia ca. 500.

The town is first mentioned in 742 under the name of "Erphesfurt": in that year, Saint Boniface wrote to Pope Zachary to inform him that he had established three dioceses in central Germany, one of them "in a place called Erphesfurt, which for a long time has been inhabited by pagan natives." All three dioceses (the other two were Würzburg and Büraburg) were confirmed by Zachary the next year, though in 755 Erfurt was brought into the diocese of Mainz. That the place was populous already is borne out by archeological evidence, which includes 23 graves and six horse burials from the sixth and seventh centuries.

Throughout the Middle Ages, Erfurt was an important trading town because of its location, near a ford across the Gera river. Together with the other five Thuringian woad towns of Gotha, Tennstedt, Arnstadt and Langensalza it was the centre of the German woad trade, which made those cities very wealthy. Erfurt was the junction of important trade routes: the Via Regia was one of the most used east–west roads between France and Russia (via Frankfurt, Erfurt, Leipzig and Wrocław) and another route in the north–south direction was the connection between the Baltic Sea ports (e. g. Lübeck) and the potent upper Italian city-states like Venice and Milan.

During the 10th and 11th centuries both the Emperor and the Electorate of Mainz held some privileges in Erfurt. The German kings had an important monastery on Petersberg hill and the Archbishops of Mainz collected taxes from the people. Around 1100, some people became free citizens by paying the annual "" (liberation tax), which marks a first step in becoming an independent city. During the 12th century, as a sign of more and more independence, the citizens built a city wall around Erfurt (in the area of today's ). After 1200, independence was fulfilled and a city council was founded in 1217; the town hall was built in 1275. In the following decades, the council bought a city-owned territory around Erfurt which consisted at its height of nearly 100 villages and castles and even another small town (Sömmerda). Erfurt became an important regional power between the Landgraviate of Thuringia around, the Electorate of Mainz to the west and the Electorate of Saxony to the east. Between 1306 and 1481, Erfurt was allied with the two other major Thuringian cities (Mühlhausen and Nordhausen) in the Thuringian City Alliance and the three cities joined the Hanseatic League together in 1430. A peak in economic development was reached in the 15th century, when the city had a population of 20,000 making it one of the largest in Germany. Between 1432 and 1446, a second and higher city wall was established. In 1483, a first city fortress was built on Cyriaksburg hill in the southwestern part of the town.

The Jewish community of Erfurt was founded in the 11th century and became, together with Mainz, Worms and Speyer, one of the most influential in Germany. Their Old Synagogue is still extant and a museum today, as is the mikveh at Gera river near . In 1349, during the wave of Black Death Jewish persecutions across Europe, the Jews of Erfurt were rounded up, with more than 100 killed and the rest driven from the city. Before the persecution, a wealthy Jewish merchant buried his property in the basement of his house. In 1998, this treasure was found during construction works. The Erfurt Treasure with various gold and silver objects is shown in the exhibition in the synagogue today. Only a few years after 1349, the Jews moved back to Erfurt and founded a second community, which was disbanded by the city council in 1458.

In 1379, the University of Erfurt was founded. Together with the University of Cologne it was one of the first city-owned universities in Germany, while they were usually owned by the "". Some buildings of this old university are extant or restored in the "Latin Quarter" in the northern city centre (like , student dorms "" and others, the hospital and the church of the university). The university quickly became a hotspot of German cultural life in Renaissance humanism with scholars like Ulrich von Hutten, Helius Eobanus Hessus and Justus Jonas.

In the year 1184, Erfurt was the location of a notable accident called the "Erfurter Latrinensturz" ('Latrine fall'). King Henry VI held council in a building of the Erfurt Cathedral to negotiate peace between two of his vassals, Archbishop Konrad I of Mainz and Landgrave Ludwig III of Thuringia. The amassed weight of all the gathered men proved too heavy for the floor to bear, which collapsed. According to contemporary accounts, dozens of people fell to their death into the latrine pit below. Ludwig III, Konrad I and Henry VI survived the affair.

In 1501 Martin Luther (1483 - 1546) moved to Erfurt and began his studies at the university. After 1505, he lived at St. Augustine's Monastery as a friar. In 1507 he was ordained as a priest in Erfurt Cathedral. He moved permanently to Wittenberg in 1511. Erfurt was an early adopter of the Protestant Reformation, in 1521.

In 1530, the city became one of the first in Europe to be officially bi-confessional with the Hammelburg Treaty. It kept that status through all the following centuries. The later 16th and the 17th century brought a slow economic decline of Erfurt. Trade shrank, the population was falling and the university lost its influence. The city's independence was endangered. In 1664, the city and surrounding area were brought under the dominion of the Electorate of Mainz and the city lost its independence. The Electorate built a huge fortress on Petersberg hill between 1665 and 1726 to control the city and instituted a governor to rule Erfurt.

During the late 18th century, Erfurt saw another cultural peak. Governor Karl Theodor Anton Maria von Dalberg had close relations with Johann Wolfgang von Goethe, Friedrich Schiller, Johann Gottfried Herder, Christoph Martin Wieland and Wilhelm von Humboldt, who often visited him at his court in Erfurt.

Erfurt became part of the Kingdom of Prussia in 1802, to compensate for territories Prussia lost to France on the Left Bank of the Rhine. In the Capitulation of Erfurt the city, its 12,000 Prussian and Saxon defenders under William VI, Prince of Orange-Nassau, 65 artillery pieces, and the Petersberg Citadel and Cyriaksburg Citadel were handed over to the French on 16 October 1806;<ref name="Petre 1907/1993"></ref> At the time of the capitulation, Joachim Murat, Marshal of France, had about 16,000 troops near Erfurt. With the attachment of the Saxe-Weimar territory of Blankenhain, the city became part of the First French Empire in 1806 as the Principality of Erfurt, directly subordinate to Napoleon as an "imperial state domain" (), separate from the Confederation of the Rhine, which the surrounding Thuringian states had joined. Erfurt was administered by a civilian and military Senate (") under a French governor, based in the , previously the seat of city's governor under the Electorate. Napoleon first visited the principality on 23 July 1807, inspecting the citadels and fortifications. In 1808, the Congress of Erfurt was held with Napoleon and Alexander I of Russia visiting the city.

During their administration, the French introduced street lighting and a tax on foreign horses to pay for maintaining the road surface. The suffered under the French occupation, with its inventory being auctioned off to other local churches – including the organ, bells and even the tower of the chapel (") – and the former monastery's library being donated to the University of Erfurt (and then to the Boineburg Library when the university closed in 1816). Similarly the Cyriaksburg Citadel was damaged by the French, with the city-side walls being partially dismantled in the hunt for imagined treasures from the convent, workers being paid from the sale of the building materials.

In 1811, to commemorate the birth of the Prince Imperial, a ceremonial column (') of wood and plaster was erected on the common. Similarly, the ' – a Greek-style temple topped by a winged victory with shield, sword and lance and containing a bust of Napoleon sculpted by Friedrich Döll – was erected in the ' woods, including a grotto with fountain and flower beds, using a large pond (') from the , inaugurated with ceremony on 14 August 1811 after extravagant celebrations for Napoleon's birthday, which were repeated in 1812 with a concert in the conducted by Louis Spohr.

With the Sixth Coalition forming after French defeat in Russia, on 24 February 1813 Napoleon ordered the Petersburg Citadel to prepare for siege, visiting the city on 25 April to inspect the fortifications, in particular both Citadels. On 10 July 1813, Napoleon put , baron of the Empire, in charge of the defences of Erfurt. However, when the French decreed that 1000 men would be conscripted into the , the recruits were joined by other citizens in rioting on 19 July that led to 20 arrests, of whom 2 were sentenced to death by French court-martial; as a result, the French ordered the closure of all inns and alehouses.

Within a week of the Sixth Coalition's decisive victory at Leipzig (16–19 October 1813), however, Erfurt was besieged by Prussian, Austrian and Russian troops under the command of Prussian Lt Gen von Kleist. After a first capitulation signed by d'Alton on 20 December 1813 the French troops withdrew to the two fortresses of Petersberg and Cyriaksburg, allowing for the Coalition forces to march into Erfurt on 6 January 1814 to jubilant greetings; the ' ceremonial column was burned and destroyed as a symbol of the citizens' oppression under the French; similarly the ' was burned on 1 November 1813 and completely destroyed by Erfurters and their besiegers in 1814. After a call for volunteers 3 days later, 300 Erfurters joined the Coalition armies in France. Finally, in May 1814, the French capitulated fully, with 1,700 French troops vacating the Petersberg and Cyriaksburg fortresses. During the two and a half months of siege, the mortality rate rose in the city greatly; 1,564 Erfurt citizens died in 1813, around a thousand more than the previous year.

After the Congress of Vienna, Erfurt was restored to Prussia on 21 June 1815, becoming the capital of one of the three districts ("") of the new Province of Saxony, but some southern and eastern parts of Erfurter lands joined Blankenhain in being transferred to the Grand Duchy of Saxe-Weimar-Eisenach the following September. Although enclosed by Thuringian territory in the west, south and east, the city remained part of the Prussian Province of Saxony until 1944.

After the 1848 Revolution, many Germans desired to have a united national state. An attempt in this direction was the failed Erfurt Union of German states in 1850.

The Industrial Revolution reached Erfurt in the 1840s, when the Thuringian Railway connecting Berlin and Frankfurt was built. During the following years, many factories in different sectors were founded. One of the biggest was the "Royal Gun Factory of Prussia" in 1862. After the Unification of Germany in 1871, Erfurt moved from the southern border of Prussia to the centre of Germany, so the fortifications of the city were no longer needed. The demolition of the city fortifications in 1873 led to a construction boom in Erfurt, because it was now possible to build in the area formerly occupied by the city walls and beyond. Many public and private buildings emerged and the infrastructure (such as a tramway, hospitals, and schools) improved rapidly. The number of inhabitants grew from 40,000 around 1870 to 130,000 in 1914 and the city expanded in all directions.

The "Erfurt Program" was adopted by the Social Democratic Party of Germany during its congress at Erfurt in 1891.

Between the wars, the city kept growing. Housing shortages were fought with building programmes and social infrastructure was broadened according to the welfare policy in the Weimar Republic. The Great Depression between 1929 and 1932 led to a disaster for Erfurt, nearly one out of three became unemployed. Conflicts between far-left and far-right-oriented milieus increased and many inhabitants supported the new Nazi government and Adolf Hitler. Others, especially some communist workers, put up resistance against the new administration. In 1938, the new synagogue was destroyed during the . Jews lost their property and emigrated or were deported to Nazi concentration camps (together with many communists). In 1914, the company "Topf and Sons" began the manufacture of crematoria later becoming the market leader in this industry. Under the Nazis, "JA Topf & Sons" supplied specially developed crematoria, ovens and associated plants to the Auschwitz-Birkenau, Buchenwald and Mauthausen-Gusen concentration camps. On 27 January 2011 a memorial and museum dedicated to the Holocaust victims was opened at the former company premises in Erfurt.

Bombed as a target of the Oil Campaign of World War II, Erfurt suffered only limited damage and was captured on 12 April 1945, by the US 80th Infantry Division. On 3 July, American troops left the city, which then became part of the Soviet Zone of Occupation and eventually of the German Democratic Republic (East Germany). In 1948, Erfurt became the capital of Thuringia, replacing Weimar. In 1952, the in the GDR were dissolved in favour of centralization under the new socialist government. Erfurt then became the capital of a new "" (district). In 1953, the of education was founded, followed by the of medicine in 1954, the first academic institutions in Erfurt since the closing of the university in 1816.

On 19 March 1970, the East and West German heads of government Willi Stoph and Willy Brandt met in Erfurt, the first such meeting since the division of Germany. During the 1970s and 1980s, as the economic situation in GDR worsened, many old buildings in city centre decayed, while the government fought against the housing shortage by building large settlements in the periphery. The Peaceful Revolution of 1989/1990 led to German reunification.
With the re-formation of the state of Thuringia in 1990, the city became the state capital. After reunification, a deep economic crisis occurred in Eastern Germany. Many factories closed and many people lost their jobs and moved to the former West Germany. At the same time, many buildings were redeveloped and the infrastructure improved massively. In 1994, the new university was opened, as was the Fachhochschule in 1991. Between 2005 and 2008, the economic situation improved as the unemployment rate decreased and new enterprises developed. In addition, the population began to increase once again.

A school shooting occurred on 26 April 2002 at the Gutenberg-Gymnasium.

Erfurt is situated in the south of the Thuringian basin, a fertile agricultural area between the Harz mountains to the north and the Thuringian forest to the southwest. Whereas the northern parts of the city area are flat, the southern ones consist of hilly landscape up to 430 m of elevation. In this part lies the municipal forest of "" with beeches and oaks as main tree species. To the east and to the west are some non-forested hills so that the Gera river valley within the town forms a basin. North of the city are some gravel pits in operation, while others are abandoned, flooded and used as leisure areas.

Erfurt has a humid continental climate (Dfb) or an oceanic climate ("Cfb") according to the Köppen climate classification system. Summers are warm and sometimes humid with average high temperatures of and lows of . Winters are relatively cold with average high temperatures of and lows of . The city's topography creates a microclimate caused by the location inside a basin with sometimes inversion in winter (quite cold nights under ) and inadequate air circulation in summer. Annual precipitation is only with moderate rainfall throughout the year. Light snowfall mainly occurs from December through February, but snow cover does not usually remain for long.

Erfurt abuts the districts of Sömmerda (municipalities Witterda, Elxleben, Walschleben, Riethnordhausen, Nöda, Alperstedt, Großrudestedt, Udestedt, Kleinmölsen and Großmölsen) in the north, Weimarer Land (municipalities Niederzimmern, Nohra, Mönchenholzhausen and Klettbach) in the east, Ilm-Kreis (municipalities Kirchheim, Rockhausen and Amt Wachsenburg) in the south and Gotha (municipalities Nesse-Apfelstädt, Nottleben, Zimmernsupra and Bienstädt) in the west.

The city itself is divided into 53 districts. The centre is formed by the district ' (old town) and the districts ' in the northwest, ' in the northeast, ' in the east, ' in the southeast, ' in the southwest and ' in the west. More former industrial districts are ' (incorporated in 1911), ' and ' in the north. Another group of districts is marked by Plattenbau settlements, constructed during the DDR period: ', ', ', ' and ' in the northern as well as ', ' and ' in the southern city parts.

Finally, there are many villages with an average population of approximately 1,000 which were incorporated during the 20th century; however, they have mostly stayed rural to date:
Around the year 1500, the city had 18,000 inhabitants and was one of the largest cities in the Holy Roman Empire. The population then more or less stagnated until the 19th century. The population of Erfurt was 21,000 in 1820, and increased to 32,000 in 1847, the year of rail connection as industrialization began. In the following decades Erfurt grew up to 130,000 at the beginning of World War I and 190,000 inhabitants in 1950. A maximum was reached in 1988 with 220,000 persons. The bad economic situation in eastern Germany after the reunification resulted in a decline in population, which fell to 200,000 in 2002 before rising again to 206,000 in 2011. The average growth of population between 2009 and 2012 was approximately 0.68% p. a, whereas the population in bordering rural regions is shrinking with accelerating tendency. Suburbanization played only a small role in Erfurt. It occurred after reunification for a short time in the 1990s, but most of the suburban areas were situated within the administrative city borders.

The birth deficit was 200 in 2012, this is −1.0 per 1,000 inhabitants (Thuringian average: -4.5; national average: -2.4). The net migration rate was +8.3 per 1,000 inhabitants in 2012 (Thuringian average: -0.8; national average: +4.6). The most important regions of origin of Erfurt migrants are rural areas of Thuringia, Saxony-Anhalt and Saxony as well as foreign countries like Poland, Russia, Syria, Afghanistan and Hungary.

Like other eastern German cities, foreigners account only for a small share of Erfurt's population: circa 3.0% are non-Germans by citizenship and overall 5.9% are migrants (according to the 2011 EU census).

Due to the official atheism of the former GDR, most of the population is non-religious. 14.8% are members of the Evangelical Church in Central Germany and 6.8% are Catholics (according to the 2011 EU census). The Jewish Community consists of 500 members. Most of them migrated to Erfurt from Russia and Ukraine in the 1990s.

Martin Luther (1483–1546) studied law and philosophy at the University of Erfurt from 1501. He lived in St. Augustine's Monastery in Erfurt, as a friar from 1505 to 1511.

The theologian, philosopher and mystic Meister Eckhart (c. 1260–1328) entered the Dominican monastery in Erfurt when he was aged about 18 (around 1275). Eckhart was the Dominican Prior at Erfurt from 1294 until 1298, and Vicar of Thuringia from 1298 to 1302. After a year in Paris, he returned to Erfurt in 1303 and administered his duties as Provincial of Saxony from there until 1311.

Max Weber (1864–1920) was born in Erfurt. He was a sociologist, philosopher, jurist, and political economist whose ideas have profoundly influenced modern social theory and social research.

The textile designer Margaretha Reichardt (1907–1984) was born and died in Erfurt. She studied at the Bauhaus from 1926 to 1930, and while there worked with Marcel Breuer on his innovative chair designs. Her former home and weaving workshop in Erfurt, the "Margaretha Reichardt Haus", is now a museum, managed by the Angermuseum Erfurt.

Johann Pachelbel (1653–1706) served as organist at the Prediger church in Erfurt from June 1678 until August 1690. Pachelbel composed approximately seventy pieces for organ while in Erfurt.

After 1906 the composer Richard Wetz (1875–1935) lived in Erfurt and became the leading person in the town's musical life. His major works were written here, including three symphonies, a Requiem and a Christmas Oratorio.

Alexander Müller (1808–1863) pianist, conductor and composer, was born in Erfurt. He later moved to Zürich, where he served as leader of the General Music Society's subscription concerts series.

The city is the birthplace of one of Johann Sebastian Bach's cousins, Johann Bernhard Bach, as well as Johann Sebastian Bach's father Johann Ambrosius Bach. Bach's parents were married in 1668 in a small church, the " (Merchant's Church), that still exists on the main square, Anger.

Famous modern musicians from Erfurt are Clueso, the Boogie Pimps and Yvonne Catterfeld.

Erfurt has a great variety of museums:


Since 2003, the modern opera house is home to Theater Erfurt and its Philharmonic Orchestra. The "grand stage" section has 800 seats and the "studio stage" can hold 200 spectators. In September 2005, the opera "Waiting for the Barbarians" by Philip Glass premiered in the opera house. The Erfurt Theater has been a source of controversy recently. In 2005, a performance of Engelbert Humperdinck's opera " stirred up the local press since the performance contained suggestions of pedophilia and incest. The opera was advertised in the program with the addition "for adults only".

On 12 April 2008, a version of Verdi's opera " directed by Johann Kresnik opened at the Erfurt Theater. The production stirred deep controversy by featuring nude performers in Mickey Mouse masks dancing on the ruins of the World Trade Center and a female singer with a painted on Hitler toothbrush moustache performing a straight arm Nazi salute, along with sinister portrayals of American soldiers, Uncle Sam, and Elvis Presley impersonators. The director described the production as a populist critique of modern American society, aimed at showing up the disparities between rich and poor. The controversy prompted one local politician to call for locals to boycott the performances, but this was largely ignored and the première was sold out.

The Messe Erfurt serves as home court for the Oettinger Rockets, a professional basketball team in Germany's first division, the Basketball Bundesliga.

Notable types of sport in Erfurt are athletics, ice skating, cycling (with the oldest velodrome in use in the world, opened in 1885), swimming, handball, volleyball, tennis and football. The city's football club is member of and based in with a capacity of 20,000. The " was the second indoor speed skating arena in Germany.

Erfurt's cityscape features a medieval core of narrow, curved alleys in the centre surrounded by a belt of " architecture, created between 1873 and 1914. In 1873, the city's fortifications were demolished and it became possible to build houses in the area in front of the former city walls. 
In the following years, Erfurt saw a construction boom. In the northern area (districts Andreasvorstadt, Johannesvorstadt and Ilversgehofen) tenements for the factory workers were built whilst the eastern area (Krämpfervorstadt and Daberstedt) featured apartments for white-collar workers and clerks and the southwestern part (Löbervorstadt and Brühlervorstadt) with its beautiful valley landscape saw the construction of villas and mansions of rich factory owners and notables.

During the interwar period, some settlements in Bauhaus style were realized, often as housing cooperatives.

After World War II and over the whole GDR period, housing shortages remained a problem even though the government started a big apartment construction programme. Between 1970 and 1990 large settlements with high-rise blocks on the northern (for 50,000 inhabitants) and southeastern (for 40,000 inhabitants) periphery were constructed. After reunification the renovation of old houses in city centre and the " areas was a big issue. The federal government granted substantial subsidies, so that many houses could be restored.

Compared to many other German cities, little of Erfurt was destroyed in World War II. This is one reason why the centre today offers a mixture of medieval, Baroque and Neoclassical architecture as well as buildings from the last 150 years.

Public green spaces are located along Gera river and in several parks like the ', the ' and the ". The largest green area is the , a horticultural exhibition park and botanic garden established in 1961.

The city centre has about 25 churches and monasteries, most of them in Gothic style, some also in Romanesque style or a mixture of Romanesque and Gothic elements, and a few in later styles. The various steeples characterize the medieval centre and led to one of Erfurt's nicknames as the "Thuringian Rome".




The oldest parts of Erfurt's "Alte Synagoge" (Old Synagogue) date to the 11th century. It was used until 1349 when the Jewish community was destroyed in a pogrom known as the Erfurt Massacre. The building had many other uses since then. It was conserved in the 1990s and in 2009 it became a museum of Jewish history.
A rare Mikveh, a ritual bath, dating from c.1250, was discovered by archeologists in 2007. It has been accessible to visitors on guided tours since September 2011.
In 2015 the Old Synagogue and Mikveh were nominated as a World Heritage Site. It has been tentatively listed but a final decision has not yet been made.

As religious freedom was granted in the 19th century, some Jews returned to Erfurt. They built their synagogue on the banks of the Gera river and used it from 1840 until 1884. The neoclassical building is known as the "Kleine Synagoge" (Small Synagogue). Today it is used an events centre. It is also open to visitors.

A larger synagogue, the "Große Synagoge" (Great Synagogue), was opened in 1884 because the community had become larger and wealthier. This moorish style building was destroyed during nationwide Nazi riots, known as on 9–10 November 1938.

In 1947 the land which the Great Synagogue had occupied was returned to the Jewish community and they built their current place of worship, the "Neue Synagoge" (New Synagogue) which opened in 1952. It was the only synagogue building erected under communist rule in East Germany.

Besides the religious buildings there is a lot of historic secular architecture in Erfurt, mostly concentrated in the city centre, but some 19th- and 20th-century buildings are located on the outskirts. 

From 1066 until 1873 the old town of Erfurt was encircled by a fortified wall. About 1168 this was extended to run around the western side of Petersberg hill, enclosing it within the city boundaries.

After German Unification in 1871, Erfurt became part of the newly created German Empire. The threat to the city from its Saxon neighbours and from Bavaria was no longer present, so it was decided to dismantle the city walls. Only a few remnants remain today. A piece of inner wall can be found in a small park at the corner Juri-Gagarin-Ring and Johannesstraße and another piece at the flood ditch ("Flutgraben") near Franckestraße. There is also a small restored part of the wall in the Brühler Garten, behind the Catholic orphanage. Only one of the wall's fortified towers was left standing, on Boyneburgufer, but this was destroyed in an air raid in 1944.

The Petersberg Citadel is one of the largest and best preserved city fortresses in Europe, covering an area of 36 hectares in the north-west of the city centre. It was built from 1665 on Petersberg hill and was in military use until 1963. Since 1990, it has been significantly restored and is now open to the public as an historic site.

The is a smaller citadel south-west of the city centre, dating from 1480. Today, it houses the German horticulture museum.

Between 1873 and 1914, a belt of ' architecture emerged around the city centre. The mansion district in the south-west around , and hosts some interesting ' and "Art Nouveau" buildings.

The "Mühlenviertel" ("mill quarter"), is an area of beautiful Art Nouveau apartment buildings, cobblestone streets and street trees just to the north of the old city, in the vicinity of Nord Park, bordered by the Gera river on its east side. The Schmale Gera stream runs through the area. In the Middle Ages numerous small enterprises using the power of water mills occupied the area, hence the name "Mühlenviertel", with street names such as Waidmühlenweg (woad, or indigo, mill way), Storchmühlenweg (stork mill way) and Papiermühlenweg (paper mill way).

The "Bauhaus" style is represented by some housing cooperative projects in the east around and and in the north around . Lutherkirke Church in (1927), is an Art Deco building.

The former malt factory "Wolff" at in the east of Erfurt is a large industrial complex built between 1880 and 1939, and in use until 2000. A new use has not been found yet, but the area is sometimes used as a location in movie productions because of its atmosphere.

Some examples of Nazi architecture are the buildings of the (Thuringian parliament) and (an event hall) in the south at . While the building (1930s) represents more the neo-Roman/fascist style, (1940s) is marked by some neo-Germanic "" style elements.

The Stalinist early-GDR style is manifested in the main building of the university at (1953) and the later more international modern GDR style is represented by the horticultural exhibition centre "" at , the housing complexes like Rieth or and the redevelopment of and area along in the city centre.

The current international glass and steel architecture is dominant among most larger new buildings like the Federal Labour Court of Germany (1999), the new opera house (2003), the new main station (2007), the university library, the Erfurt Messe (convention centre) and the ice rink.

During recent years, the economic situation of the city improved: the unemployment rate declined from 21% in 2005 to 9% in 2013. Nevertheless, some 14,000 households with 24,500 persons (12% of population) are dependent upon state social benefits (Hartz IV).

Farming has a great tradition in Erfurt: the cultivation of woad made the city rich during the Middle Ages. Today, horticulture and the production of flower seeds is still an important business in Erfurt. There is also growing of fruits (like apples, strawberries and sweet cherries), vegetables (e.g. cauliflowers, potatoes, cabbage and sugar beets) and grain on more than 60% of the municipal territory.

Industrialization in Erfurt started around 1850. Until World War I, many factories were founded in different sectors like engine building, shoes, guns, malt and later electro-technics, so that there was no industrial monoculture in the city. After 1945, the companies were nationalized by the GDR government, which led to the decline of some of them. After reunification, nearly all factories were closed, either because they failed to successfully adopt to a free market economy or because the German government sold them to west German businessmen who closed them to avoid competition to their own enterprises. However, in the early 1990s the federal government started to subsidize the foundation of new companies. It still took a long time before the economic situation stabilized around 2006. Since this time, unemployment has decreased and overall, new jobs were created. Today, there are many small and medium-sized companies in Erfurt with electro-technics, semiconductors and photovoltaics in focus. Engine production, food production, the Braugold brewery, and Born Feinkost, a producer of Thuringian mustard, remain important industries.

Erfurt is an "" (which means "supra-centre" according to Central place theory) in German regional planning. Such centres are always hubs of service businesses and public services like hospitals, universities, research, trade fairs, retail etc. Additionally, Erfurt is the capital of the federal state of Thuringia, so that there are many institutions of administration like all the Thuringian state ministries and some nationwide authorities. Typical for Erfurt are the logistic business with many distribution centres of big companies, the Erfurt Trade Fair and the media sector with KiKa and MDR as public broadcast stations. A growing industry is tourism, due to the various historical sights of Erfurt. There are 4,800 hotel beds and (in 2012) 450,000 overnight visitors spent a total of 700,000 nights in hotels. Nevertheless, most tourists are one-day visitors from Germany. The Christmas Market in December attracts some 2,000,000 visitors each year.

The ICE railway network puts Erfurt 1½ hours from Berlin, 2½ hours from Frankfurt, 2 hours from Dresden, and 45 minutes from Leipzig. In 2017, the ICE line to Munich opened, making the trip to Erfurt only 2½ hours.

There are regional trains from Erfurt to Weimar, Jena, Gotha, Eisenach, Bad Langensalza, Magdeburg, Nordhausen, Göttingen, Mühlhausen, Würzburg, Meiningen, Ilmenau, Arnstadt, and Gera.

In freight transport there is an intermodal terminal in the district of Vieselbach "()" with connections to rail and the autobahn.

The two Autobahnen crossing each other nearby at "Erfurter Kreuz" are the Bundesautobahn 4 (Frankfurt–Dresden) and the Bundesautobahn 71 (Schweinfurt–Sangerhausen). Together with the east tangent both motorways form a circle road around the city and lead the interregional traffic around the centre. Whereas the A 4 was built in the 1930s, the A 71 came into being after the reunification in the 1990s and 2000s. In addition to both motorways there are two Bundesstraßen: the Bundesstraße 7 connects Erfurt parallel to A 4 with Gotha in the west and Weimar in the east. The Bundesstraße 4 is a connection between Erfurt and Nordhausen in the north. Its southern part to Coburg was annulled when A 71 was finished (in this section, the A 71 now effectively serves as B 4). Within the circle road, B 7 and B 4 are also annulled, so that the city government has to pay for maintenance instead of the German federal government. The access to the city is restricted as " since 2012 for some vehicles. Large parts of the inner city are a pedestrian area which can not be reached by car (except for residents).

The Erfurt public transport system is marked by the area-wide (light rail) network, established as a tram system in 1883, upgraded to a light rail (") system in 1997, and continually expanded and upgraded through the 2000s. Today, there are six "Stadtbahn" lines running every ten minutes on every light rail route.

Additionally, Erfurt operates a bus system, which connects the sparsely populated outer districts of the region to the city center. Both systems are organized by "SWE EVAG", a transit company owned by the city administration. Trolleybuses were in service in Erfurt from 1948 until 1975, but are no longer in service.

Erfurt-Weimar Airport lies west of the city centre. It is linked to the central train station via Stadtbahn (tram). It was significantly extended in the 1990s, with flights mostly to Mediterranean holiday destinations and to London during the peak Christmas market tourist season. Connections to longer haul flights are easily accessible via Frankfurt Airport, which can be reached in 2 hours via a direct train from Frankfurt Airport to Erfurt, and from Leipzig/Halle Airport, which can be reached within half an hour.

Biking is becoming increasingly popular since construction of high quality cycle tracks began in the 1990s. There are cycle lanes for general commuting within Erfurt city.

Long-distance trails, such as the "Gera track" and the "" (Thuringian cities trail), connect points of tourist interest. The former runs along the Gera river valley from the Thuringian forest to the river Unstrut; the latter follows the medieval Via Regia from Eisenach to Altenburg via Gotha, Erfurt, Weimar, and Jena.

The Rennsteig Cycle Way was opened in 2000. This designated high-grade hiking and bike trail runs along the ridge of the Thuringian Central Uplands. The bike trail, about long, occasionally departs from the course of the historic Rennsteig hiking trail, which dates back to the 1300s, to avoid steep inclines. It is therefore about longer than the hiking trail.

The Rennsteig is connected to the E3 European long distance path, which goes from the Atlantic coast of Spain to the Black Sea coast of Bulgaria, and the E6 European long distance path, running from Arctic Finland to Turkey.

After reunification, the educational system was reorganized. The University of Erfurt, founded in 1379 and closed in 1816, was refounded in 1994 with a focus on social sciences, modern languages, humanities and teacher training. Today there are approximately 6,000 students working within four faculties, the Max Weber Center for Advanced Cultural and Social Studies, and three academic research institutes. The University has an international reputation and participates in international student exchange programmes.

The "Fachhochschule Erfurt", is a university of applied sciences, founded in 1991, which offers a combination of academic training and practical experience in subjects such as social work and social pedagogy, business studies, and engineering. There are nearly 5,000 students in six faculties, of which the faculty of landscaping and horticulture has a national reputation.

The International University of Applied Sciences Bad Honnef – Bonn (IUBH), is a privately run university with a focus on business and economics. It merged with the former Adam-Ries-Fachhochschule in 2013.

The world renowned Bauhaus design school was founded in 1919 in the city of Weimar, approximately from Erfurt, 12 minutes by train. The buildings are now part of a World Heritage Site and are today used by the Bauhaus-Universität Weimar, which teaches design, arts, media and technology related subjects.

Furthermore, there are eight ', six state-owned, one Catholic and one Protestant. One of the state-owned schools is a ', an elite boarding school for young talents in athletics, swimming, ice skating or football. Another state-owned school, "", offers a focus in sciences as an elite boarding school in addition to the common curriculum.

The German national public television children's channel "KiKa" is based in Erfurt.

MDR, Mitteldeutscher Rundfunk, a radio and television company, has a broadcast centre and studios in Erfurt.

The Thüringer Allgemeine is a statewide newspaper that is headquartered in the city.

The first freely elected mayor after German reunification was Manfred Ruge of the Christian Democratic Union, who served from 1990 to 2006. Since 2006, Andreas Bausewein of the Social Democratic Party (SPD) has been mayor. The most recent mayoral election was held on 15 April 2018, with a runoff held on 29 April, and the results were as follows:
! rowspan=2 colspan=2| Candidate
! rowspan=2| Party
! colspan=2| First round
! colspan=2| Second round
! Votes
! Votes
! colspan=3| Valid votes
! 83,701
! 99.3
! 60,550
! 98.0
! colspan=3| Invalid votes
! 562
! 0.7
! 1,240
! 2.0
! colspan=3| Total
! 84,263
! 100.0
! 61,790
! 100.0
! colspan=3| Electorate/voter turnout
! 172,908
! 48.7
! 172,562
! 35.8

The most recent city council election was held on 26 May 2019, and the results were as follows:
! colspan=2| Party
! Lead candidate
! Votes
! +/-
! Seats
! colspan=3| Valid votes
! 97,492
! 96.8
! 
! colspan=3| Invalid votes
! 3,232
! 3.2
! 
! colspan=3| Total
! 100,724
! 100.0
! 50
! ±0
! colspan=3| Electorate/voter turnout
! 172,389
! 58.4
! 11.1
! 

Erfurt is twinned with:




</doc>
<doc id="9482" url="https://en.wikipedia.org/wiki?curid=9482" title="Enya">
Enya

Eithne Pádraigín Ní Bhraonáin (anglicised as Enya Patricia Brennan, ; born 17 May 1961), known professionally as Enya, is an Irish singer, songwriter, record producer and musician. Born into a musical family and raised in the Irish-speaking area of Gweedore in County Donegal, Enya began her music career when she joined her family's Celtic folk band Clannad in 1980 on keyboards and backing vocals. She left in 1982 with their manager and producer Nicky Ryan to pursue a solo career, with Ryan's wife Roma Ryan as her lyricist. Enya developed her sound over the following four years with multitracked vocals and keyboards with elements of new age, Celtic, classical, church, and folk music. She has sung in ten languages.

Enya's first projects as a solo artist included soundtrack work for "The Frog Prince" (1984) and the 1987 BBC documentary series "The Celts", which was released as her debut album, "Enya" (1987). She signed with Warner Music UK, which granted her artistic freedom and minimal interference from the label. The commercial and critical success of "Watermark" (1988) propelled her to worldwide fame, helped by the international top-10 hit single "Orinoco Flow". This was followed by the multi-million-selling albums "Shepherd Moons" (1991), "The Memory of Trees" (1995) and "A Day Without Rain" (2000). Sales of the latter and its lead single, "Only Time", surged in the United States following its use in the media coverage of the September 11 attacks. Following "Amarantine" (2005) and "And Winter Came..." (2008), Enya took an extended break from music; she returned in 2012 and released "Dark Sky Island" (2015).

She is Ireland's best-selling solo artist and second-best-selling artist behind U2, with a discography that has sold 26.5 million certified albums in the United States and an estimated 75 million records worldwide, making her one of the best-selling music artists of all time. "A Day Without Rain" (2000) remains the best-selling new-age album, with an estimated 16 million copies sold worldwide. Enya has won awards including seven World Music Awards, four Grammy Awards for Best New Age Album, and an Ivor Novello Award. She was nominated for an Academy Award and a Golden Globe Award for "May It Be", written for "" (2001).

Eithne Pádraigín Ní Bhraonáin was born on 17 May 1961 in Dore, within the area of the parish Gaoth Dobhair, in the northwestern county of Donegal, Ireland. It is a Gaeltacht region where Irish is the primary language. Her name is anglicised as Enya Patricia Brennan, where Enya is the phonetic spelling of how Eithne is pronounced in her native Ulster dialect of Irish; "Ní Bhraonáin" translates to "daughter of Brennan". The fifth of nine children, Enya was born into a Roman Catholic family of musicians. Her father, Leo Brennan, was the leader of the Slieve Foy Band, an Irish showband, and ran Leo's Tavern in Meenaleck; her mother, Máire Brennan ("née" Duggan), who had distant Spanish roots and whose ancestors settled on Tory Island, was an amateur musician who played in Leo's band and taught music at Gweedore Community School. Enya's maternal grandfather Aodh was the headmaster of the primary school in Dore, and her grandmother was a teacher there. Aodh was also the founder of the Gweedore Theatre company.

Enya described her upbringing as "very quiet and happy." At age three, she took part in her first singing competition at the annual Feis Ceoil music festival. She took part in pantomimes at Gweedore Theatre and sang with her siblings in her mother's choir at St Mary's church in Derrybeg. She learned English at primary school and began piano lessons at age four. "I had to do school work and then travel to a neighbouring town for piano lessons, and then more school work. I ... remember my brothers and sisters playing outside ... and I would be inside playing the piano. This one big book of scales, practising them over and over." When Enya turned eleven, her grandfather paid for her education at a strict convent boarding school in Milford run by nuns of the Loreto order, where she developed a taste for classical music, art, Latin and watercolour painting. "It was devastating to be torn away from such a large family, but it was good for my music." Enya left the school at 17 and studied classical music in college for one year with the aim of becoming "a piano teacher sort of person. I never thought of myself composing or being on stage."

In the 1970s several members of Enya's family formed Clannad, a Celtic band with Nicky Ryan as their manager, sound engineer and producer and his future wife Roma Ryan assisting with the tour management and administrative duties. In 1980, after her year at college, Enya decided not to study music at university and instead accepted Ryan's invitation to join the group with the aim of expanding their sound by incorporating keyboards and another backup vocalist. She toured across Europe and played an uncredited role on their sixth album, "Crann Úll" (1980), with a line-up of siblings Máire, Pól and Ciarán Brennan and twin uncles, Noel and Pádraig Duggan. Enya became an official and credited member by the time of their next album "Fuaim" (1981), which features a front cover photograph of her with the band. Nicky maintains it was never his intention to make Enya a permanent member, and realised she was "fiercely independent ... intent on playing her own music. She was just not sure of how to go about it". This sparked discussions between the two on the idea of using Enya's voice to form a "choir of one", a concept based on the "wall of sound" technique by Phil Spector that interested them both.

In 1982, during a Clannad tour of Switzerland, Nicky called for a band meeting as several issues had arisen and felt they needed to be addressed. He added, "It was short and only required a vote, I was a minority of one and lost. Roma and I were out. This left the question of what happened with Enya. I decided to stand back and say nothing". Enya chose to leave to pursue a solo career with the Ryans, which initially caused some friction between the three and her family but she preferred being independent and disliked being confined in the group as "somebody in the background". Nicky then suggested to Enya that either she return to Gweedore "with no particular definite future", or live with him and Roma in their home, then located in the northern Dublin suburb of Artane, "and see what happens, musically". After their bank denied them a loan, Enya sold her saxophone and gave piano lessons and the Ryans used what they could afford from their savings to build a recording facility named Aigle Studio, named after the French word for "eagle", in a shed in their back garden, and rented it out to other artists to cover its costs. They formed a musical partnership in the process with Nicky as Enya's producer and arranger and Roma her lyricist, and became directors of their music company, Aigle Music. In the following two years, Enya developed her playing and composing by recording herself recite classical pieces on the piano and listening back to them. The process was repeated until she started to improvise sections and develop her own piano arrangements. Her first composition was "An Taibhse Uaighneach", Irish for "The Lonely Ghost". During this time, Enya played the synthesiser on "Ceol Aduaidh" (1983) by Mairéad Ní Mhaonaigh and Frankie Kennedy and performed with the duo and Mhaonaigh's brother Gearóid in their short lived group, Ragairne.

Enya's first solo endeavour arrived in 1983 when she recorded two piano instrumentals, "An Ghaoth Ón Ghrian", Irish for "The Solar Wind", and "Miss Clare Remembers", at Windmill Lane Studios in Dublin which were released on "Touch Travel" (1984), a limited release audio cassette of music from various artists on the Touch label. She is credited as "Eithne Ní Bhraonáin" on its liner notes. After several months of preparation, Enya's debut solo performance took place on 23 September 1983 at the National Stadium in Dublin that was televised for RTÉ's music show "Festival Folk". Nial Morris, a musician who worked with her during this time, recalled she "was so nervous she could barely get on stage, and she cowered behind the piano until the gig was over."

At the suggestion of Roma, who thought Enya's music would suit accompanying visual images, a demo tape of her compositions with Morris on additional keyboards was made and sent to various film producers. Among them was David Puttnam, who liked the tape and chose Enya to compose the soundtrack to the romantic comedy film "The Frog Prince" (1984), of which he served as executive producer. Enya wrote nine tracks for the film but found her songs were rearranged and orchestrated against her wishes by Richard Myhill except two tracks she sang on, "The Frog Prince" and "Dreams", with the latter's lyrics penned by Charlie McGettigan. Film editor Jim Clark later claimed the rearrangements were necessary as Enya found it difficult to compose to picture. Released in 1985 by Island Visual Arts, the album is the first commercial release that credits her as "Enya". The change from Eithne to Enya originated from Nicky Ryan, who thought her name would be too difficult for people outside Ireland to pronounce correctly, and suggested the phonetic spelling of her name. Enya looked back on the project as a good career move, but a disappointing one as "we weren't part of it at the end". She then sang on three tracks on "Ordinary Man" (1985) by Christy Moore.

In 1985, producer Tony McAuley commissioned Enya to write a song for the six-part BBC2 television documentary series "The Celts". She already had written a Celtic-influenced song named "The March of the Celts" and submitted it to the project. Each episode was to feature a different composer at first, but director David Richardson liked the track so much, he selected her to compose the entire soundtrack. Enya recorded 72 minutes of music in 1986 at Aigle Studio and the BBC studios in Wood Lane, London without recording to picture, though she was required to portray certain themes and ideas that the producers wanted. Unlike "The Frog Prince", she worked with little interference which granted her freedom to establish her sound that she adopted throughout her career, using multi-tracked vocals, keyboards, and percussion with elements of Celtic, classical, church and folk music.

In March 1987, two months before the series aired on television, a 40-minute selection of the soundtrack was released as Enya's first solo album, titled "Enya", by BBC Records in the United Kingdom and by Atlantic Records in the United States. The latter promoted it with a new-age imprint on the packaging which Nicky later thought was "a cowardly thing for them to do". The album gained enough public attention to reach number 8 on the Irish Albums Chart and number 69 on the UK Albums Chart. "I Want Tomorrow" was released as Enya's first single. "Boadicea" was sampled by The Fugees on their 1996 song "Ready or Not"; the group neither sought permission nor gave her credit, causing Enya to threaten legal action. The group subsequently gave her credit and paid a fee worth around $3 million. Later in 1987, she appeared on Sinéad O'Connor's debut album "The Lion and the Cobra", reciting Psalm 91 in Irish on the song "Never Get Old".

Several weeks after the release of her debut album, Enya secured a recording contract with Warner Music UK after Rob Dickins, the label's chairman and a fan of Clannad, took a liking to "Enya" and found himself playing it "every night before I went to bed". He then met Enya and the Ryans at a chance meeting at the Irish Recorded Music Association award ceremony in Dublin, and learned Enya was thinking about signing with a rival label. Dickins seized the opportunity and signed her to Warner Music with a deal worth £75,000, granting her wish to write and record with artistic freedom, minimal interference from the label, and without set deadlines to finish albums. Dickins said: "Sometimes you sign an act to make money, and sometimes you sign an act to make music. This was clearly the latter ... I just wanted to be involved with this music." Enya then left Atlantic and signed with the Warner-led Geffen Records to handle her American distribution.
With the green-light to produce a new studio album, Enya recorded "Watermark" from June 1987 to April 1988. It was initially recorded in analogue at Aigle Studio before Dickins requested to have it re-recorded digitally at Orinoco Studios in Bermondsey, London. "Watermark" was released in September 1988 and became an unexpected hit, reaching number 5 in the United Kingdom and number 25 on the "Billboard" 200 in the United States following its release there in January 1989. Its lead single, "Orinoco Flow", was the last song written for the album. It was not intended to be a single at first, but Enya and the Ryans chose it after Dickins asked for a single from them several times as a joke, knowing Enya's music was not made for the Top 40 chart. Dickins and engineer Ross Cullum are referenced in the songs' lyrics. "Orinoco Flow" became an international top 10 hit and was number one in the United Kingdom for three weeks, the first from Warner to reach the top spot in six years. The new-found success propelled Enya to international fame and she received endorsement deals and offers to use her music in television commercials. She spent one year travelling worldwide to promote the album which increased her exposure through interviews, appearances, and live performances. By 1996, "Watermark" had sold in excess of 1.2 million copies in the United Kingdom and 4 million in the United States.

After promoting "Watermark", Enya purchased new recording equipment and started work on her next album, "Shepherd Moons". She found the success of "Watermark" caused a considerable amount of pressure when it came to writing new songs, adding: "I kept thinking "Would this have gone on "Watermark"? Is it as good?" Eventually I had to forget about this and start on a blank canvas and just really go with what felt right." Enya wrote songs based on several ideas, including entries from her diary, the Blitz in London, and her grandparents. "Shepherd Moons" was released in November 1991, her first album released under Warner-led Reprise Records in the United States. It became a greater commercial success than "Watermark", reaching number one at home for one week and number 17 in the United States. "Caribbean Blue", its lead single, charted at number thirteen in the United Kingdom. By 1997, the album had reached multi-platinum certification for selling in excess of 1.2 million copies in the United Kingdom and 5 million in the United States.

In 1991, Warner Music released a collection of five Enya music videos as "Moonshadows" for home video. In 1993, Enya won her first Grammy Award for Best New Age Album for "Shepherd Moons". Soon after, Enya and Nicky entered discussions with Industrial Light & Magic, founded by George Lucas, regarding an elaborate stage lighting system for a proposed concert tour, but nothing came out of the meetings. In November 1992, Warner had obtained the rights to "Enya" and re-released the album as "The Celts" with new artwork. It surpassed its initial sale performance, reaching number 10 in the United Kingdom and reached platinum certification in the United States in 1996 for one million copies shipped.

After travelling worldwide to promote "Shepherd Moons", Enya started to write and record her fourth album, "The Memory of Trees". The album was released in November 1995. It peaked at number five in the United Kingdom and number nine in the United States, where it sold over 3 million copies. Its lead single, "Anywhere Is", reached number seven in the United Kingdom. The second, "On My Way Home", reached number twenty-six in the same country. In late 1994, Enya put out an extended play of Christmas music titled "The Christmas EP". Enya was offered to compose the score for "Titanic", but declined. A recording of her singing "Oíche Chiúin", an Irish-language version of "Silent Night", appeared on the charity album "A Very Special Christmas 3", released in benefit of the Special Olympics in October 1997.

In early 1997, Enya began to select tracks for her first compilation album, "trying to select the obvious ones, the hits, and others." She chose to work on the collection following the promotional tour for "The Memory of Trees" as she felt it was the right time in her career, and that her contract with WEA required her to release a "best of" album. The set, named "Paint the Sky with Stars: The Best of Enya", features two new tracks, "Paint the Sky with Stars" and "Only If...". Released in November 1997, the album was a worldwide commercial success, reaching No. 4 in the UK and No. 30 in the US, where it went on to sell over 4 million copies. "Only If..." was released as a single in 1997. Enya described the album as "like a musical diary ... each melody has a little story and I live through that whole story from the beginning ... your mind goes back to that day and what you were thinking."

Enya started work on her fifth studio album, titled "A Day Without Rain", in mid-1998. In a departure from her previous albums she incorporated the use of a string section into her compositions, something that was not a conscious decision at first, but Enya and Nicky Ryan agreed it complemented the songs that were being written. The album was released in November 2000, and reached number 6 in the United Kingdom and an initial peak of number 17 in the United States. In the aftermath of the 11 September attacks, sales of the album and its lead single, "Only Time", surged after the song was widely used during radio and television coverage of the events, leading to its description as "a post-September 11 anthem". The exposure caused "A Day Without Rain" to outperform its original chart performance to peak at number 2 on the "Billboard" 200, and the release of a maxi single containing the original and a pop remix of "Only Time" in November 2001. Enya donated its proceeds in aid of the International Association of Firefighters. The song topped the "Billboard" Hot Adult Contemporary Tracks chart and went to number 10 on the Hot 100 singles, Enya's highest charting US single to date. A second single, "Wild Child", was released in December 2001. "A Day Without Rain" remains Enya's biggest seller, with 7 million copies sold in the US and the most sold new-age album of all time with an estimated 13 million copies sold worldwide.

In 2001, Enya agreed to write and perform on two tracks for the of "" (2001) at the request of director Peter Jackson. Its composer Howard Shore "imagined her voice" as he wrote the film's score, making an uncommon exception to include another artist in one of his soundtracks. After flying to New Zealand to observe the filming and to watch a rough cut of the film, Enya returned to Ireland and composed "Aníron (Theme for Aragorn and Arwen)" with lyrics by Roma in J. R. R. Tolkien's fictional Elvish language Sindarin, and "May It Be", sung in English and another Tolkien language, Quenya. Shore then based his orchestrations around Enya's recorded vocals and themes to create "a seamless sound". In 2002, Enya released "May It Be" as a single which earned her an Academy Award nomination for Best Original Song. She performed the song live at the 74th Academy Awards ceremony with an orchestra in March 2002, and later cited the moment as a career highlight.

Enya undertook additional studio projects in 2001 and 2002. The first was work on the soundtrack to the Japanese romantic film "Calmi Cuori Appassionati" (2001) which was subsequently released as "Themes from Calmi Cuori Appassionati" (2001). The album is formed of tracks spanning her career from "Enya" to "A Day Without Rain" with two B-sides. The album went to number 2 in Japan, and became Enya's second to sell one million copies in the country. November 2002 saw the release of "Only Time – The Collection", a box set of 51 tracks recorded through her career which received a limited release of 200,000 copies.

In September 2003, Enya returned to Aigle Studio to start work on her sixth studio album, "Amarantine". Roma said the title means "everlasting". The album marks the first instance of Enya singing in Loxian, a fictional language created by Roma that came about when Enya was working on "Water Shows the Hidden Heart". After numerous attempts to sing the song in English, Irish and Latin, Roma suggested a new language based on some of the sounds Enya would sing along to when developing her songs. It was a success, and Enya sang "Less Than a Pearl" and "The River Sings" in the same way. Roma worked on the language further, creating a "culture and history" behind it surrounding the Loxian people who are of another planet, questioning the existence of life on another. "Sumiregusa (Wild Violet)" is sung in Japanese. "Amarantine" was a global success, reaching number 6 on the "Billboard" 200 and number 8 in the UK. It has sold over 1 million certified copies in the US, a considerable drop in sales in comparison to her previous albums. Enya dedicated the album to BBC producer Tony McAuley, who had commissioned Enya to write the soundtrack to "The Celts", following his death in 2003. The lead single, "Amarantine", was released in December 2005. A Christmas Special Edition was released in 2006, followed by a Deluxe Edition.

In 2006, Enya released "", a Christmas-themed EP released exclusively in the US following an exclusive partnership with the NBC network and the Target department store chain. It includes two new songs, "Christmas Secrets" and "The Magic of the Night".

In June 2007, Enya received an honorary doctorate from the National University of Ireland, Galway. A month later, she received one from the University of Ulster.

Enya continued to write music with a winter and Christmas theme for her seventh studio album, "And Winter Came...". Initially she intended to make an album of seasonal songs and hymns set for a release in late 2007, but decided to produce a winter-themed album instead. The track "My! My! Time Flies!", a tribute to the late Irish guitarist Jimmy Faulkner, incorporates a guitar solo performed by Pat Farrell, the first use of a guitar on an Enya album since "I Want Tomorrow" from "Enya". Upon its release in November 2008, "And Winter Came..." reached No. 6 in the UK and No. 8 in the US and sold almost 3.5 million copies worldwide by 2011.

After promoting "And Winter Came...", Enya took an extended break from writing and recording music. She spent her time resting, visiting family in Australia, and renovating her new home in the south of France. In March 2009, her first four studio albums were reissued in Japan in the Super High Material CD format with bonus tracks. Her second compilation album and DVD, "The Very Best of Enya", was released in November 2009 and features songs from 1987 to 2008, including a previously unreleased version of "Aníron". In 2013, "Only Time" was used in the "Epic Split" advertisement by Volvo Trucks starring Jean-Claude Van Damme who does the splits while suspended between two lorries. The video went viral, leading to numerous parodies of the commercial uploaded to YouTube also using "Only Time". The attention resulted in the song peaking at No. 43 on the "Billboard" Hot 100 singles chart.

In 2012, Enya returned to the studio to record her eighth album, "Dark Sky Island". Its name refers to the island of Sark, where it became the first island to be designated a dark-sky preserve, and a series of poems on islands by Roma Ryan. The new album was promoted with the premiere in October 2015 of its lead single, "Echoes in Rain", on Ken Bruce's radio show and with the release in the same month of the single as a digital download. Upon its release on 20 November 2015, "Dark Sky Island" went to No. 4 in the UK, Enya's highest charting studio album there since "Shepherd Moons" went to No. 1, and to No. 8 in the US. A Deluxe Edition features three additional songs. Enya completed a promotional tour of the UK and Europe, the US and Japan. During her visit to Japan, Enya performed "Orinoco Flow" and "Echoes in Rain" at the Universal Studios Japan Christmas show in Osaka. In December 2016, Enya appeared on the Raidió Teilifís Éireann Christmas special "Christmas Carols from Cork", marking her first Irish television appearance in over seven years. She sang "Adeste Fideles" and "Oiche Chiúin" as well as her own carol composition "The Spirit of Christmas Past".

Enya's vocal range is mezzo-soprano. She has cited her musical foundations as "the classics", church music, and "Irish reels and jigs" with a particular interest in Sergei Rachmaninoff, a favourite composer of hers. She has an autographed picture of him in her home. Since 1982, she has recorded her music with Nicky Ryan as producer and arranger and his wife Roma Ryan as lyricist. While in Clannad, Enya chose to work with Nicky as the two shared an interest in vocal harmonies, and Ryan, influenced by The Beach Boys and the "Wall of Sound" technique that Phil Spector pioneered, wanted to explore the idea of "the multivocals" for which her music became known. According to Enya, "Angeles" from "Shepherd Moons" has roughly 500 vocals recorded individually and layered. Enya performs all vocals and the majority of instruments in her songs apart from musicians to play percussion, guitar, uilleann pipes, cornet, and double bass. Her early works including "Watermark" feature numerous keyboards, including the Yamaha KX88 Master, Yamaha DX7, Oberheim Matrix, Akai S900, Roland D-50, and Roland Juno-60, the latter a particular favourite of hers.

Numerous critics and reviewers classify Enya's albums as new age music and she has won four Grammy Awards in the category. However, Enya does not classify her music as part of the genre. When asked what genre she would classify her music, her reply was "Enya". Nicky Ryan commented on the new age comments: "Initially it was fine, but it's really not new age. Enya plays a whole lot of instruments, not just keyboards. Her melodies are strong and she sings a lot. So I can't see a comparison." The music video to "Caribbean Blue" and the art work to "The Memory of Trees" feature adapted works from artist Maxfield Parrish.
Enya has sung in ten languages in her career, including English, Irish, Latin, Welsh, Spanish, French and Japanese. She has recorded music influenced by works from fantasy author J. R. R. Tolkien, including the instrumental "Lothlórien" from "Shepherd Moons". She sang "May It Be" in English and Tolkien's fictional language Quenya, and "Aníron", sang in Tolkien's other language Sindarin, for "". Her albums "Amarantine" and "Dark Sky Island" include songs sung in Loxian, a fictional language created by Roma, that has no official syntax. Its vocabulary was formed by Enya singing the song's notes to which Roma wrote their phonetic spelling.

Enya adopted a composing and songwriting method that has deviated little throughout her career. At the start of the recording process for an album she enters the studio, forgetting about her previous success, fame, and songs of hers that became hits. "If I did that", she said, "I'd have to call it a day". She then develops ideas on the piano, keeping note of any arrangement that can be worked on further. During her time writing, Enya works a five-day week, takes weekends off, and does not work on her music at home. With Irish as her first language, Enya initially records her songs in Irish as she can express "feeling much more directly" than English. After a period of time, Enya presents her ideas to Nicky to discuss what pieces work best, while Roma works in parallel to devise a lyric to the songs. Enya considered "Fallen Embers" from "A Day Without Rain" a perfect time when the lyrics reflect as to how she felt while writing the song. In 2008, she newly discovered her tendency to write "two or three songs" during the winter months, work on the arrangements and lyrics the following spring and summer, and then work on the next couple of songs when autumn arrives.

Enya says that Warner Music and she "did not see eye to eye" initially as the label imagined her performing on stage "with a piano ... maybe two or three synthesiser players and that's it". Enya also explained that the time put into her studio albums causes her to "run overtime", leaving little time to plan for other such projects. She also expressed the difficulty in recreating her studio-oriented sound for the stage. In 1996, Ryan said Enya had received an offer worth almost £500,000 to perform a concert in Japan. In 2016, Enya spoke about the prospect of a live concert when she revealed talks with the Ryans during her three-year break after "And Winter Came..." (2008) to perform a show at the Metropolitan Opera House in New York City that would be simulcast to cinemas worldwide. Before such an event could happen, Nicky suggested that she enter a studio and record "all the hits" live with an orchestra and choir to see how they would sound.

Enya has sung with live and lip synching vocals on various talk and music shows, events, and ceremonies throughout her career, usually during her worldwide press tours for each album. In December 1995, she performed "Anywhere Is" at a Christmas concert at Vatican City with Pope John Paul II in attendance, who met and thanked her for performing. In April 1996, Enya performed the same song during her surprise appearance at the fiftieth birthday celebration for Carl XVI Gustaf, the King of Sweden and a fan of Enya's. In 1997, Enya participated in a live Christmas Eve broadcast in London and flew to County Donegal afterwards to join her family for their annual midnight Mass choral performance, in which she partakes each year. In March 2002, she performed "May It Be" with an orchestra at the year's Academy Awards ceremony. Enya and her sisters performed as part of the local choir Cor Mhuire in July 2005 at St. Mary's church in Gweedore during the annual Earagail Arts Festival.

In 1997, Enya bought Manderley Castle, a Victorian Grade A listed castle home in Killiney, County Dublin for £2.5 million at auction. Formerly known as Victoria and Ayesha Castle, she renamed the castle after the house from the book "Rebecca" by Daphne du Maurier. In 2009, during her three-year break from music, Enya purchased a home in southern France.

Since the 1980s, Enya has attracted the attention of several stalkers. In 1996, an Italian man who was seen in Dublin wearing a photograph of Enya around his neck stabbed himself outside her parents' pub after being ejected from the premises. In May 2005, Enya applied to spend roughly £250,000 on security improvements, covering gaps in the castle's outer wall and installing bollards and iron railings. Despite these improvements, in October 2005, two people broke into her home; one attacked and tied up one of her housekeepers and left with several of Enya's items after she had raised the alarm in her safe room.

Enya is known for keeping a private lifestyle, saying: "The music is what sells. Not me, or what I stand for ... that's the way I've always wanted it". She is not married and is a surrogate aunt to the Ryans' two daughters. In 1991, she said: "I'm afraid of marriage because I'm afraid someone might want me because of who I am instead of because they loved me ... I wouldn't go rushing into anything unexpected, but I do think a great deal about this". A relationship she had with one man ended in 1997, around the time when she considered taking time out of music to have a family, but found she was putting pressure on herself over the matter and "gone the route I wanted to go". She declares herself as "more spiritual than religious ... I derive from religion what I enjoy."

In 2006, Enya ranked third in a list of the wealthiest Irish entertainers with an estimated fortune of £75 million, and No. 95 in the "Sunday Times" Rich List of the richest 250 Irish people. The 2016 edition, which listed its top 50 "Music Millionaires of Britain and Ireland", she emerged as the richest female singer with a fortune of £91 million for a place at No. 28.

In 2017 a newly discovered species of fish, "Leporinus enyae", found in the Orinoco River drainage area, was named after Enya.

Billboard Music Awards

Grammy Awards

Japan Gold Disc Awards

World Music Awards

!scope="row" rowspan="3"|1989
!scope="row"|1990
!scope="row"|1992
!scope="row" rowspan="3"|1993
!scope="row" rowspan=1|1998
!scope="row"|2001
!scope="row"|2001
!scope="row"|2002
!scope="row"|2002
!scope="row" rowspan="3"|2002
!scope="row" rowspan="1"|2003
!scope="row" rowspan="3"|2004
!scope="row"|2005
!scope="row"|2016






</doc>
<doc id="9483" url="https://en.wikipedia.org/wiki?curid=9483" title="East Berlin">
East Berlin

East Berlin was the capital city of the German Democratic Republic from 1949 to 1990. Formally, it was the Soviet sector of Berlin, established in 1945. The American, British, and French sectors were known as West Berlin. From 13 August 1961 until 9 November 1989, East Berlin was separated from West Berlin by the Berlin Wall. The Western Allied powers did not recognise East Berlin as the GDR's capital, nor the GDR's authority to govern East Berlin. On 3 October 1990, the day Germany was officially reunified, East and West Berlin formally reunited as the city of Berlin.

With the London Protocol of 1944 signed on September 12, 1944, the United States, the United Kingdom and the Soviet Union decided to divide Germany into three occupation zones and to establish a special area of Berlin, which was occupied by the three Allied Forces together. In May 1945, the Soviet Union installed a city government for the whole city that was called "Magistrate of Greater Berlin", which existed until 1947. After the war, the Allied Forces initially administered the city together within the Allied Kommandatura, which served as the governing body of the city. However, in 1948 the Soviet representative left the Kommandatura and the common administration broke apart during the following months. In the Soviet sector, a separate city government was established, which continued to call itself "Magistrate of Greater Berlin".

When the German Democratic Republic was established in 1949, it immediately claimed East Berlin as its capital—a claim that was recognised by all communist countries. Nevertheless, its representatives to the People's Chamber were not directly elected and did not have full voting rights until 1981.

In June 1948, all railways and roads leading to West Berlin were blocked, and East Berliners were not allowed to emigrate. Nevertheless, more than 1,000 East Germans were escaping to West Berlin each day by 1960, caused by the strains on the East German economy from war reparations owed to the Soviet Union, massive destruction of industry, and lack of assistance from the Marshall Plan. In August 1961, the East German Government tried to stop the population exodus by enclosing West Berlin within the Berlin Wall. It was very dangerous for fleeing residents to cross because armed soldiers were trained to shoot illegal migrants.

East Germany was a socialist republic, but there was not complete economic equality. Privileges such as prestigious apartments and good schooling were given to members of the ruling party and their family. Eventually, Christian churches were allowed to operate without restraint after years of harassment by authorities. In the 1970s, wages of East Berliners rose and working hours fell.

The Soviet Union and the Communist bloc recognised East Berlin as the GDU's capital. However, Western Allies (the US, UK, and France) never formally acknowledged the authority of the East German government to govern East Berlin. Official Allied protocol recognised only the authority of the Soviet Union in East Berlin in accordance with the occupation status of Berlin as a whole. The United States Command Berlin, for example, published detailed instructions for U.S. military and civilian personnel wishing to visit East Berlin. In fact, the three Western commandants regularly protested against the presence of the East German National People's Army (NVA) in East Berlin, particularly on the occasion of military parades. Nevertheless, the three Western Allies eventually established embassies in East Berlin in the 1970s, although they never recognised it as the capital of East Germany. Treaties instead used terms such as "seat of government."

On 3 October 1990, East and West Germany and East and West Berlin were reunited, thus formally ending the existence of East Berlin. City-wide elections in December 1990 resulted in the first “all Berlin” mayor being elected to take office in January 1991, with the separate offices of mayors in East and West Berlin expiring at the time, and Eberhard Diepgen (a former mayor of West Berlin) became the first elected mayor of a reunited Berlin.

Since reunification, the German government has spent vast amounts of money on reintegrating the two halves of the city and bringing services and infrastructure in the former East Berlin up to the standard established in West Berlin.

After reunification, the East German economy suffered significantly. Under the adopted policy of privatisation of state-owned firms under the auspices of the Treuhandanstalt, many East German factories were shut down—which also led to mass unemployment—due to gaps in productivity with and investment compared to West German companies, as well as an inability to comply with West German pollution and safety standards in a way that was deemed cost effective. Because of this, a massive amount of West German economic aid was poured into East Germany to revitalize it. This stimulus was part-funded through a 7.5% tax on income for individuals and companies (in addition to normal income tax or company tax) known as the "Solidaritätszuschlaggesetz" (SolZG) or "solidarity surcharge", which though only in effect for 1991-1992 (later reintroduced in 1995 at 7.5 and then dropped down to 5.5% in 1998 and continues to be levied to this day) led to a great deal of resentment toward the East Germans.

Despite the large sums of economic aid poured into East Berlin, there still remain obvious differences between the former East and West Berlins. East Berlin has a distinct visual style; this is partly due to the greater survival of prewar façades and streetscapes, with some even still showing signs of wartime damage. The unique look of Stalinist architecture that was used in East Berlin (along with the rest of the former GDR) also contrasts markedly with the urban development styles employed in the former West Berlin. Additionally, the former East Berlin (along with the rest of the former GDR) retains a small number of its GDR-era street and place names commemorating German socialist heroes, such as Karl-Marx-Allee, Rosa-Luxemburg-Platz, and Karl-Liebknecht-Straße. Many such names, however, were deemed inappropriate (for various reasons) and, through decommunization, changed after a long process of review (so, for instance, Leninallee reverted to Landsberger Allee in 1991, and Dimitroffstraße reverted to Danziger Straße in 1995).

Another symbolic icon of the former East Berlin (and of East Germany as a whole) is the "Ampelmännchen" (tr. "little traffic light men"), a stylized version of a fedora-wearing man crossing the street, which is found on traffic lights at many pedestrian crosswalks throughout the former East. Following a civic debate about whether the Ampelmännchen should be abolished or disseminated more widely (due to concerns of consistency), several crosswalks in some parts of the former West Berlin also employ the Ampelmännchen.

Twenty-five years after the two cities were reunified, the people of East and West Berlin still had noticeable differences between them, which became more apparent among the older generations. The two groups also had sometimes-derogatory slang terms to refer to each other. A former East Berliner (or East German) was known as an ""Ossi"" (from the German word for east, "Ost"), and a former West Berliner (or West German) was known as a ""Wessi"" (from the German word for west, "West"). Both sides also engaged in stereotyping the other. A stereotypical "Ossi" had little ambition or poor work ethic and was chronically bitter, while a stereotypical "Wessi" was arrogant, selfish, impatient and pushy.

At the time of German reunification, East Berlin comprised the boroughs of




</doc>
<doc id="9486" url="https://en.wikipedia.org/wiki?curid=9486" title="List of international environmental agreements">
List of international environmental agreements

This is a list of international environmental agreements.

Most of the following agreements are legally binding for countries that have formally ratified them. Some, such as the Kyoto Protocol, differentiate between types of countries and each nation's respective responsibilities under the agreement. Several hundred international environmental agreements exist but most link only a limited number of countries. These bilateral or sometimes trilateral agreements are only binding for the countries that have ratified them but are nevertheless essential in the international environmental regime. Including the major conventions listed below, more than 3,000 international environmental instruments have been identified by the IEA Database Project .


<section begin=air treaties />
<section end=air treaties/>


<section begin=waste treaties />
<section end=waste treaties />







</doc>
<doc id="9487" url="https://en.wikipedia.org/wiki?curid=9487" title="Epsilon">
Epsilon

Epsilon (, ; uppercase ', lowercase ' or lunate ; ) is the fifth letter of the Greek alphabet, corresponding phonetically to a . In the system of Greek numerals it also has the value five. It was derived from the Phoenician letter He . Letters that arose from epsilon include the Roman E, Ë and Ɛ, and Cyrillic Е, È, Ё, Є and Э.

The name of the letter was originally (), but the name was changed to ("e psilon" "simple e") in the Middle Ages to distinguish the letter from the digraph , a former diphthong that had come to be pronounced the same as epsilon.

The uppercase form of epsilon looks identical to Latin E but has its own code point in Unicode: . The lowercase version has two typographical variants, both inherited from medieval Greek handwriting. One, the most common in modern typography and inherited from medieval minuscule, looks like a reversed number "3" and is encoded . The other, also known as lunate or uncial epsilon and inherited from earlier uncial writing, looks like a semicircle crossed by a horizontal bar: it is encoded . While in normal typography these are just alternative font variants, they may have different meanings as mathematical symbols: computer systems therefore offer distinct encodings for them. In TeX, codice_1 ( formula_1 ) denotes the lunate form, while codice_2 ( formula_2 ) denotes the reversed-3 form.

There is also a 'Latin epsilon', or "open e", which looks similar to the Greek lowercase epsilon. It is encoded in Unicode as and and is used as an IPA phonetic symbol. The lunate or uncial epsilon provided inspiration for the euro sign (€).

The lunate epsilon () is not to be confused with the set membership symbol (); nor should the Latin uppercase epsilon () be confused with the Greek uppercase sigma (). The symbol formula_3, first used in set theory and logic by Giuseppe Peano and now used in mathematics in general for set membership ("belongs to") did, however, "evolve" from the letter epsilon, since the symbol was originally used as an abbreviation for the Latin word "est". In addition, mathematicians often read the symbol as "e element of", as in "1 is an element of the natural numbers" for formula_4, for example. As late as 1960, ε itself was used for set membership, while its negation "does not belong to" (now ) was denoted by (epsilon prime). Only gradually did a fully separate, stylized symbol take the place of epsilon in this role. In a related context, Peano also introduced the use of a backwards epsilon, , for the phrase "such that", although the abbreviation "s.t." is occasionally used in place of ϶ in informal cardinals.

The letter Ε was taken over from the Phoenician letter He () when Greeks first adopted alphabetic writing. In archaic Greek writing, its shape is often still identical to that of the Phoenician letter. Like other Greek letters, it could face either leftward or rightward (), depending on the current writing direction, but, just as in Phoenician, the horizontal bars always faced in the direction of writing. Archaic writing often preserves the Phoenician form with a vertical stem extending slightly below the lowest horizontal bar. In the classical era, through the influence of more cursive writing styles, the shape was simplified to the current E glyph.

While the original pronunciation of the Phoenician letter "He" was , the earliest Greek sound value of Ε was determined by the vowel occurring in the Phoenician letter name, which made it a natural choice for being reinterpreted from a consonant symbol to a vowel symbol denoting an sound. Besides its classical Greek sound value, the short phoneme, it could initially also be used for other -like sounds. For instance, in early Attic before c. 500 BC, it was used also both for the long, open , and for the long close . In the former role, it was later replaced in the classic Greek alphabet by Eta (Η), which was taken over from eastern Ionic alphabets, while in the latter role it was replaced by the digraph spelling ΕΙ.

Some dialects used yet other ways of distinguishing between various e-like sounds.

In Corinth, the normal function of Ε to denote and was taken by a glyph resembling a pointed B (), while Ε was used only for long close . The letter Beta, in turn, took the deviant shape .

In Sicyon, a variant glyph resembling an X () was used in the same function as Corinthian .

In Thespiai (Boeotia), a special letter form consisting of a vertical stem with a single rightward-pointing horizontal bar () was used for what was probably a raised variant of in pre-vocalic environments. This tack glyph was used elsewhere also as a form of "Heta", i.e. for the sound .
After the establishment of the canonical classical Ionian (Eucleidean) Greek alphabet, new glyph variants for Ε were introduced through handwriting. In the uncial script (used for literary papyrus manuscripts in late antiquity and then in early medieval vellum codices), the "lunate" shape () became predominant. In cursive handwriting, a large number of shorthand glyphs came to be used, where the cross-bar and the curved stroke were linked in various ways. Some of them resembled a modern lowercase Latin "e", some a "6" with a connecting stroke to the next letter starting from the middle, and some a combination of two small "c"-like curves. Several of these shapes were later taken over into minuscule book hand. Of the various minuscule letter shapes, the inverted-3 form became the basis for lower-case Epsilon in Greek typography during the modern era.

Despite its pronunciation as , in the International Phonetic Alphabet, the Latin epsilon represents open-mid front unrounded vowel, as in the English word "pet" .

The uppercase Epsilon is not commonly used outside of the Greek language because of its similarity to the Latin letter E. However, it is commonly used in structural mechanics with Young's Modulus equations for calculating tensile, compressive and areal strain.

The Greek lowercase epsilon , the lunate epsilon symbol , or the Latin lowercase epsilon (see above) is used in a variety of places:






These characters are used only as mathematical symbols. Stylized Greek text should be encoded using the normal Greek letters, with markup and formatting to indicate text style.



</doc>
<doc id="9488" url="https://en.wikipedia.org/wiki?curid=9488" title="Eta">
Eta

Eta (uppercase ', lowercase '; "ē̂ta" or "ita" ) is the seventh letter of the Greek alphabet. Originally denoting a consonant /h/, its sound value in the classical Attic dialect of Ancient Greek was a long vowel , raised to in hellenistic Greek, a process known as iotacism.

In the ancient Attic number system (Harodianic or acrophonic numbers), the number 100 was represented by "", because it was the initial of "ΗΕΚΑΤΟΝ", the ancient spelling of "ἑκατόν" = "one hundred". In the latter system of (Classical) Greek numerals it has a value of 8. 

Eta was derived from the Phoenician letter heth . Letters that arose from eta include the Latin H and the Cyrillic letter И.

The letter shape 'H' was originally used in most Greek dialects to represent the sound /h/, a voiceless glottal fricative. In this function, it was borrowed in the 8th century BC by the Etruscan and other Old Italic alphabets, which were based on the Euboean form of the Greek alphabet. This also gave rise to the Latin alphabet with its letter H.

Other regional variants of the Greek alphabet (epichoric alphabets), in dialects that still preserved the sound /h/, employed various glyph shapes for consonantal "heta" side by side with the new vocalic "eta" for some time. 
In the southern Italian colonies of Heracleia and Tarentum, the letter shape was reduced to a "half-heta" lacking the right vertical stem (Ͱ). From this sign later developed the sign for rough breathing or "spiritus asper", which brought back the marking of the /h/ sound into the standardized post-classical (polytonic) orthography.
Dionysius Thrax in the second century BC records that the letter name was still pronounced "heta" (ἥτα), correctly explaining this irregularity by stating "in the old days the letter Η served to stand for the rough breathing, as it still does with the Romans."

In the East Ionic dialect, however, the sound /h/ disappeared by the sixth century BC, and the letter was re-used initially to represent a development of a long vowel , which later merged in East Ionic with instead. In 403 BC, Athens took over the Ionian spelling system and with it the vocalic use of H (even though it still also had the /h/ sound itself at that time). This later became the standard orthography in all of Greece.

During the time of post-classical Koiné Greek, the sound represented by eta was raised and merged with several other formerly distinct vowels, a phenomenon called "iotacism" or "itacism", after the new pronunciation of the letter name as "ita" instead of "eta".

Itacism is continued into Modern Greek, where the letter name is pronounced and represents the sound /i/ (a close front unrounded vowel). It shares this function with several other letters (ι, υ) and digraphs (ει, οι), which are all pronounced alike. This phenomenon at large is called iotacism.

Eta was also borrowed with the sound value of [i] into the Cyrillic script, where it gave rise to the Cyrillic letter И.

In Modern Greek, due to iotacism, the letter (pronounced ) represents a close front unrounded vowel, . In Classical Greek, it represented a long open-mid front unrounded vowel, .

The uppercase letter Η is used as a symbol in textual criticism for the Alexandrian text-type (from Hesychius, its once-supposed editor).

In chemistry, the letter H as symbol of enthalpy sometimes is said to be a Greek eta, but since enthalpy comes from ἐνθάλπος, which begins in a smooth breathing and epsilon, it is more likely a Latin H for 'heat'.

In information theory the uppercase Greek letter H is used to represent the concept of entropy of a discrete random variable.

The lowercase letter η is used as a symbol in:



These characters are used only as mathematical symbols. Stylized Greek text should be encoded using the normal Greek letters, with markup and formatting to indicate text style.


</doc>
<doc id="9491" url="https://en.wikipedia.org/wiki?curid=9491" title="Eskimo">
Eskimo

Eskimo ( ) or Eskimos are the Indigenous circumpolar peoples who have traditionally inhabited the northern circumpolar region from eastern Siberia (Russia) to Alaska (United States), Northern Canada, Nunavik and Greenland. 

The two main peoples known as "Eskimo" are the Inuit (including the Alaskan Iñupiat peoples, the Greenlandic Inuit, and the mass-grouping Inuit peoples of Canada) and the Yupik (of eastern Siberia and Alaska). A third northern group, the Aleut, is closely related to both. They share a relatively recent common ancestor and a language group (Eskimo-Aleut). 

The non-Inuit sub-branch of the Eskimo branch of the Eskimo-Aleut language family consists of four distinct Yupik languages, two used in the Russian Far East and St. Lawrence Island, and two used in western Alaska, southwestern Alaska, and the western part of Southcentral Alaska. The extinct language of the Sirenik people is sometimes argued to be related to these.

According to recent genomic research, the Chukchi people, from eastern Siberia, are the closest living relatives of the Siberian Yupik and the Indigenous peoples of the Americas generally.

There are more than 183,000 people of Eskimo descent alive today, of which 135,000 or more live in or near the traditional circumpolar regions. The NGO known as the Inuit Circumpolar Council claims to represent 180,000 people.

The governments in Canada and the United States have made moves to cease using the term "Eskimo" in official documents, but it has not been entirely eliminated, as the word is in some places written into tribal, and therefore national, legal terminology. Canada officially uses the term "Inuit" to describe the native people living in the country's northernmost sector. The United States government legally uses "Alaska Native" for the Yupik, Inuit, and Aleut, but also for non-Eskimo indigenous Alaskans including the Tlingit, the Haida, the Eyak, the Tsimshian, in addition to at least nine separate northern Athabaskan/Dene peoples. The designation "Alaska Native" applies to official tribal members only, in contrast to individual Eskimo/Aleut persons claiming descent from the world's "most widespread aboriginal group".

Etymologically speaking, there exists a scientific consensus that the word Eskimo comes from the Innu-aimun (Montagnais) word "ayas̆kimew" meaning "a person who laces a snowshoe" and is related to "husky" (a breed of dog), and it does not have a pejorative meaning in origin. 

In Canada and Greenland, the term "Eskimo" is predominantly seen as offensive or "non-preferred", and has been widely replaced by the term "Inuit" or terms specific to a particular group or community. This has resulted in a trend whereby some Canadians and Americans believe that they should not use the word "Eskimo", and use the classifier and typical Canadian word "Inuit" instead, even for Yupik (non-Inuit) people.

Some people still believe that Eskimo translates to "eater of raw meat", which may be seen, or used, in a pejorative way. 

Section 25 of the Canadian Charter of Rights and Freedoms and section 35 of the Canadian Constitution Act of 1982, recognized the Inuit as a distinctive group of Aboriginal peoples in Canada.

The Inuit Circumpolar Council voted to replace the word "Eskimo" with "Inuit" in 1977, but even at that time such a designation was not accepted by all, for both fairly obvious reasons, and less obvious reasons.

Under U.S. and Alaskan law (as well as the linguistic and cultural traditions of Alaska), "Alaska Native" refers to all indigenous peoples of Alaska. This includes not only the Iñupiat (Alaskan Inuit) and the Yupik, but also groups such as the Aleut, who share a recent ancestor, as well as the largely unrelated indigenous peoples of the Pacific Northwest Coast and the Alaskan Athabaskans. As a result, the term Eskimo is still in use in Alaska. Alternative terms, such as "Inuit-Yupik", have been proposed, but none has gained widespread acceptance. Recent (early 21st century) population estimates registered more than 135,000 individuals of Eskimo descent, with approximately 85,000 living in North America, 50,000 in Greenland, and the rest residing in Siberia. 

Several earlier indigenous peoples existed in the northern circumpolar regions of eastern Siberia, Alaska, and Canada (although probably not in Greenland). The earliest positively identified Paleo-Eskimo cultures (Early Paleo-Eskimo) date to 5,000 years ago. They appear to have developed in Alaska from people related to the Arctic small tool tradition in eastern Asia, whose ancestors had probably migrated to Alaska at least 3,000 to 5,000 years earlier. Similar artifacts have been found in Siberia that date to perhaps 18,000 years ago.

The Yupik languages and cultures in Alaska evolved in place, beginning with the original pre-Dorset indigenous culture developed in Alaska. Approximately 4000 years ago, the Unangan culture of the Aleut became distinct. It is not generally considered an Eskimo culture.

Approximately 1,500–2,000 years ago, apparently in northwestern Alaska, two other distinct variations appeared. Inuit language became distinct and, over a period of several centuries, its speakers migrated across northern Alaska, through Canada and into Greenland. The distinct culture of the Thule people developed in northwestern Alaska and very quickly spread over the entire area occupied by Eskimo people, though it was not necessarily adopted by all of them.

The most commonly accepted etymological origin of the word "Eskimo" is derived by Ives Goddard at the Smithsonian Institution, from the Montagnais (see Algonquian languages) word meaning "snowshoe-netter" or "to net snowshoes". The word "assime·w" means "she laces a snowshoe" in Montagnais. Montagnais speakers refer to the neighbouring Mi'kmaq people using words that sound like "eskimo"

In 1978, José Mailhot, a Quebec anthropologist who speaks Montagnais, published a paper suggesting that Eskimo meant "people who speak a different language". French traders who encountered the Montagnais in the eastern areas, adopted their word for the more western peoples and spelled it as "Esquimau" in a transliteration.

Some people consider "Eskimo" offensive because it is popularly perceived to mean "eaters of raw meat" in Algonquian languages common to people along the Atlantic coast. An unnamed Cree speaker suggested the original word that became corrupted to Eskimo might have been "askamiciw" (which means "he eats it raw"); the Inuit are referred to in some Cree texts as "askipiw" (which means "eats something raw"). This etymology has been discredited. 

In some contexts, as applied to Inuit people, the continued use of ""Eskimo"" may reinforce a perception that the "Inuit" are unimportant and remote. The use of "Eskimo" in such contexts is often viewed as offensive.

One of the first printed uses of the French word 'Esquimaux' comes from Samuel Hearne's "A Journey from Prince of Wales's Fort in Hudson's Bay to the Northern Ocean in the Years 1769, 1770, 1771, 1772" first published in 1795.

In Canada and Greenland, the term "Eskimo" has largely been supplanted by the term "Inuit". While "Inuit" can be accurately applied to all of the Eskimo peoples in Canada and Greenland, that is not true in Alaska and Siberia. In Alaska the term "Eskimo" is commonly used, because it includes both Yupik and Iñupiat. "Inuit" is not accepted as a collective term and it is not used specifically for Iñupiat (although they are related to the Canadian Inuit peoples). One of the oldest known Eskimo archaeological sites, dating back to 3,800 years ago, is located in Saglek Bay, Labrador. On Umnak Island in the Aleutians, another site was found and is estimated to be an age of approximately 3,000 years old .

In 1977, the Inuit Circumpolar Conference (ICC) meeting in Utqiagvik, Alaska, officially adopted Inuit as a designation for all circumpolar native peoples, regardless of their local view on an appropriate term. As a result, the Canadian government usage has replaced the (locally) defunct term Eskimo with "Inuit" ("Inuk" in singular). The preferred term in Canada's Central Arctic is "Inuinnaq", and in the eastern Canadian Arctic "Inuit". The language is often called "Inuktitut", though other local designations are also used. Despite the ICC's 1977 decision to adopt the term "Inuit", this was never accepted by the Yupik peoples, who likened it to calling all Native American Indians "Navajo" simply because the Navajo felt that that's what all tribes should be called.

The Inuit of Greenland refer to themselves as "Greenlanders" and speak the Greenlandic language.

Because of the linguistic, ethnic, and cultural differences between Yupik and Inuit peoples, it seems unlikely that any umbrella term will be acceptable. There has been some movement to use "Inuit", and the Inuit Circumpolar Council, representing a circumpolar population of 150,000 Inuit and Yupik people of Greenland, Northern Canada, Alaska, and Siberia, in its charter defines "Inuit" for use within that ICC document as including "the Inupiat, Yupik (Alaska), Inuit, Inuvialuit (Canada), Kalaallit (Greenland) and Yupik (Russia)".

In 2010, the ICC passed a resolution in which they implored scientists to use "Inuit" and "Paleo-Inuit" instead of "Eskimo" or "Paleo-Eskimo". American linguist Lenore Grenoble has explicitly deferred to this resolution and used "Inuit–Yupik" instead of "Eskimo" with regards to the language branch. In a 2015 commentary in the journal "Arctic", Canadian archaeologist Max Friesen argued fellow Arctic archaeologists should follow the ICC and use "Paleo-Inuit" instead of "Paleo-Eskimo". In 2016, Lisa Hodgetts and "Arctic" editor Patricia Wells wrote: "In the Canadian context, continued use of any term that incorporates 'Eskimo' is potentially harmful to the relationships between archaeologists and the Inuit and Inuvialuit communities who are our hosts and increasingly our research partners"; they suggested using more specific terms when possible (e.g., Dorset and Groswater) and agreed with Frieson in using "the Inuit tradition" to replace "Neo-Eskimo", although they noted replacement for "Palaeoeskimo" was still an open question and discuss "Paleo-Inuit", "Arctic Small Tool Tradition", and "pre-Inuit", as well as Inuktitut loanwords like ""Tuniit"" and ""Sivullirmiut"" as possibilities. One 2020 paper in "Journal of Anthropological Archaeology", written by Katelyn Braymer-Hayes and colleagues, notes that there is a "clear need" to replace the terms "Neo-Eskimo" and "Paleo-Eskimo", citing the ICC resolution, but finding a consensus within the Alaskan context particularly is difficult, since Alaska Natives do not use the word Inuit to describe themselves, nor is the term legally applicable only to Inupiat and Yupik peoples in Alaska, and as such, terms used in Canada like "Paleo Inuit" and "Ancestral Inuit" would not be acceptable.

But, in Alaska, the Inuit people refer to themselves as "Iñupiat," plural, and "Iñupiaq", singular (their North Alaskan Inupiatun language is also called "Iñupiaq"). They do not commonly use the term Inuit. In Alaska, "Eskimo" has been commonly used but is decreasing in prevalence.

Alaskans also use the term Alaska Native, which is inclusive of all Eskimo, Aleut and other Native Americans of Alaska. It does not apply to Inuit or Yupik people originating outside the state. The term "Alaska Native" has important legal usage in Alaska and the rest of the United States as a result of the Alaska Native Claims Settlement Act of 1971.

The term "Eskimo" is also used in linguistic or ethnographic works to denote the larger branch of Eskimo–Aleut languages, the smaller branch being Aleut.

The Eskimo–Aleut family of languages includes two cognate branches: the Aleut (Unangan) branch and the Eskimo branch.

The number of cases varies, with Aleut languages having a greatly reduced case system compared to those of the Eskimo subfamily. Eskimo–Aleut languages possess voiceless plosives at the bilabial, coronal, velar and uvular positions in all languages except Aleut, which has lost the bilabial stops but retained the nasal. In the Eskimo subfamily a voiceless alveolar lateral fricative is also present.

The Eskimo sub-family consists of the Inuit language and Yupik language sub-groups. The Sirenikski language, which is virtually extinct, is sometimes regarded as a third branch of the Eskimo language family. Other sources regard it as a group belonging to the Yupik branch.

Inuit languages comprise a dialect continuum, or dialect chain, that stretches from Unalakleet and Norton Sound in Alaska, across northern Alaska and Canada, and east to Greenland. Changes from western (Iñupiaq) to eastern dialects are marked by the dropping of vestigial Yupik-related features, increasing consonant assimilation (e.g., "kumlu", meaning "thumb", changes to "kuvlu", changes to "kublu", changes to "kulluk", changes to "kulluq"), and increased consonant lengthening, and lexical change. Thus, speakers of two adjacent Inuit dialects would usually be able to understand one another, but speakers from dialects distant from each other on the dialect continuum would have difficulty understanding one another. Seward Peninsula dialects in western Alaska, where much of the Iñupiat culture has been in place for perhaps less than 500 years, are greatly affected by phonological influence from the Yupik languages. Eastern Greenlandic, at the opposite end of the Inuit range, has had significant word replacement due to a unique form of ritual name avoidance.

The four Yupik languages, by contrast, including Alutiiq (Sugpiaq), Central Alaskan Yup'ik, Naukan (Naukanski), and Siberian Yupik, are distinct languages with phonological, morphological, and lexical differences. They demonstrate limited mutual intelligibility. Additionally, both Alutiiq and Central Yup'ik have considerable dialect diversity. The northernmost Yupik languages – Siberian Yupik and Naukan Yupik – are linguistically only slightly closer to Inuit than is Alutiiq, which is the southernmost of the Yupik languages. Although the grammatical structures of Yupik and Inuit languages are similar, they have pronounced differences phonologically. Differences of vocabulary between Inuit and any one of the Yupik languages are greater than between any two Yupik languages. Even the dialectal differences within Alutiiq and Central Alaskan Yup'ik sometimes are relatively great for locations that are relatively close geographically.
Despite the relatively small population of Naukan speakers, documentation of the language dates back to 1732. While Naukan is only spoken in Siberia, the language acts as an intermediate between two Alaskan languages: Siberian Yupik Eskimo and Central Yup'ik Eskimo.

The Sirenikski language is sometimes regarded as a third branch of the Eskimo language family, but other sources regard it as a group belonging to the Yupik branch.
An overview of the Eskimo–Aleut languages family is given below:

The Inuit inhabit the Arctic and northern Bering Sea coasts of Alaska in the United States, and Arctic coasts of the Northwest Territories, Nunavut, Quebec, and Labrador in Canada, and Greenland (associated with Denmark). Until fairly recent times, there has been a remarkable homogeneity in the culture throughout this area, which traditionally relied on fish, marine mammals, and land animals for food, heat, light, clothing, and tools. Their food sources primarily relied on seals, whales, whale blubber, walrus, and fish, all of which they hunted using harpoons on the ice. Clothing consisted of robes made of wolfskin and reindeer skin to acclimate to the low temperatures. They maintain a unique Inuit culture.

Greenlandic Inuit make up 90% of Greenland's population. They belong to three major groups:

Canadian Inuit live primarily in Nunavut (a territory of Canada), Nunavik (the northern part of Quebec) and in Nunatsiavut (the Inuit settlement region in Labrador).

The Inuvialuit live in the western Canadian Arctic region. Their homeland – the Inuvialuit Settlement Region – covers the Arctic Ocean coastline area from the Alaskan border east to Amundsen Gulf and includes the western Canadian Arctic Islands. The land was demarked in 1984 by the Inuvialuit Final Agreement.

The Iñupiat are the Inuit of Alaska's Northwest Arctic and North Slope boroughs and the Bering Straits region, including the Seward Peninsula. Utqiagvik, the northernmost city in the United States, is above the Arctic Circle and in the Iñupiat region. Their language is known as Iñupiaq.
The Yupik are indigenous or aboriginal peoples who live along the coast of western Alaska, especially on the Yukon-Kuskokwim delta and along the Kuskokwim River (Central Alaskan Yup'ik); in southern Alaska (the Alutiiq); and along the eastern coast of Chukotka in the Russian Far East and St. Lawrence Island in western Alaska (the Siberian Yupik). The Yupik economy has traditionally been strongly dominated by the harvest of marine mammals, especially seals, walrus, and whales.

The Alutiiq, also called "Pacific Yupik" or "Sugpiaq", are a southern, coastal branch of Yupik. They are not to be confused with the Aleut, who live further to the southwest, including along the Aleutian Islands. They traditionally lived a coastal lifestyle, subsisting primarily on ocean resources such as salmon, halibut, and whales, as well as rich land resources such as berries and land mammals. Alutiiq people today live in coastal fishing communities, where they work in all aspects of the modern economy. They also maintain the cultural value of a subsistence lifestyle.

The Alutiiq language is relatively close to that spoken by the Yupik in the Bethel, Alaska area. But, it is considered a distinct language with two major dialects: the Koniag dialect, spoken on the Alaska Peninsula and on Kodiak Island, and the Chugach dialect, spoken on the southern Kenai Peninsula and in Prince William Sound. Residents of Nanwalek, located on southern part of the Kenai Peninsula near Seldovia, speak what they call Sugpiaq. They are able to understand those who speak Yupik in Bethel. With a population of approximately 3,000, and the number of speakers in the hundreds, Alutiiq communities are working to revitalize their language.

"Yup'ik", with an apostrophe, denotes the speakers of the Central Alaskan Yup'ik language, who live in western Alaska and southwestern Alaska from southern Norton Sound to the north side of Bristol Bay, on the Yukon–Kuskokwim Delta, and on Nelson Island. The use of the apostrophe in the name "Yup'ik" is a written convention to denote the long pronunciation of the "p" sound; but it is spoken the same in other Yupik languages. Of all the Alaska Native languages, Central Alaskan Yup'ik has the most speakers, with about 10,000 of a total Yup'ik population of 21,000 still speaking the language. The five dialects of Central Alaskan Yup'ik include General Central Yup'ik, and the Egegik, Norton Sound, Hooper Bay-Chevak, and Nunivak dialects. In the latter two dialects, both the language and the people are called "Cup'ik".

Siberian Yupik reside along the Bering Sea coast of the Chukchi Peninsula in Siberia in the Russian Far East and in the villages of Gambell and Savoonga on St. Lawrence Island in Alaska. The Central Siberian Yupik spoken on the Chukchi Peninsula and on St. Lawrence Island is nearly identical. About 1,050 of a total Alaska population of 1,100 Siberian Yupik people in Alaska speak the language. It is the first language of the home for most St. Lawrence Island children. In Siberia, about 300 of a total of 900 Siberian Yupik people still learn and study the language, though it is no longer learned as a first language by children.

About 70 of 400 Naukan people still speak Naukanski. The Naukan originate on the Chukot Peninsula in Chukotka Autonomous Okrug in Siberia. Despite the relatively small population of Naukan speakers, documentation of the language dates back to 1732. While Naukan is only spoken in Siberia, the language acts as an intermediate between two Alaskan languages: Siberian Yupik Eskimo and Central Yup'ik Eskimo.

Some speakers of Siberian Yupik languages used to speak an Eskimo variant in the past, before they underwent a language shift. These former speakers of Sirenik Eskimo language inhabited the settlements of Sireniki, Imtuk, and some small villages stretching to the west from Sireniki along south-eastern coasts of Chukchi Peninsula. They lived in neighborhoods with Siberian Yupik and Chukchi peoples.

As early as in 1895, Imtuk was a settlement with a mixed population of Sirenik Eskimos and Ungazigmit (the latter belonging to Siberian Yupik). Sirenik Eskimo culture has been influenced by that of Chukchi, and the language shows Chukchi language influences. Folktale motifs also show the influence of Chuckchi culture.

The above peculiarities of this (already extinct) Eskimo language amounted to mutual unintelligibility even with its nearest language relatives: in the past, Sirenik Eskimos had to use the unrelated Chukchi language as a lingua franca for communicating with Siberian Yupik.

Many words are formed from entirely different roots from in Siberian Yupik, but even the grammar has several peculiarities distinct not only among Eskimo languages, but even compared to Aleut. For example, dual number is not known in Sirenik Eskimo, while most Eskimo–Aleut languages have dual, including its neighboring Siberian Yupikax relatives.

Little is known about the origin of this diversity. The peculiarities of this language may be the result of a supposed long isolation from other Eskimo groups, and being in contact only with speakers of unrelated languages for many centuries. The influence of the Chukchi language is clear.

Because of all these factors, the classification of Sireniki Eskimo language is not settled yet: Sireniki language is sometimes regarded as a third branch of Eskimo (at least, its possibility is mentioned). Sometimes it is regarded rather as a group belonging to the Yupik branch.





</doc>
<doc id="9496" url="https://en.wikipedia.org/wiki?curid=9496" title="Epiphenomenalism">
Epiphenomenalism

Epiphenomenalism is a position on the mind–body problem which holds that physical and biochemical events within the human body (sense organs, neural impulses, and muscle contractions, for example) are causal with respect to mental events (thought, consciousness, and cognition). According to this view, subjective mental events are completely dependent for their existence on corresponding physical and biochemical events within the human body yet themselves have no causal efficacy on physical events. The appearance that subjective mental states (such as intentions) influence physical events is merely an illusion. For instance, fear seems to make the heart beat faster, but according to epiphenomenalism the biochemical secretions of the brain and nervous system (such as adrenaline)—not the experience of fear—is what raises the heartbeat. Because mental events are a kind of overflow that cannot cause anything physical, yet have non-physical properties, epiphenomenalism is viewed as a form of property dualism.

During the seventeenth century, René Descartes argued that animals are subject to mechanical laws of nature. He defended the idea of automatic behavior, or the performance of actions without conscious thought. Descartes questioned how the immaterial mind and the material body can interact causally. His interactionist model (1649) held that the body relates to the mind through the pineal gland. La Mettrie, Leibniz, and Spinoza all in their own way began this way of thinking. The idea that even if the animal were conscious nothing would be added to the production of behavior, even in animals of the human type, was first voiced by La Mettrie (1745), and then by Cabanis (1802), and was further explicated by Hodgson (1870) and Huxley (1874).

Thomas Henry Huxley agreed with Descartes that behavior is determined solely by physical mechanisms, but he also believed that humans enjoy an intelligent life. In 1874, Huxley argued, in the Presidential Address to the British Association for the Advancement of Science, that animals are conscious automata. Huxley proposed that psychical changes are collateral products of physical changes. He termed the stream of consciousness an "epiphenomenon;" like the bell of a clock that has no role in keeping the time, consciousness has no role in determining behavior.

Huxley defended automatism by testing reflex actions, originally supported by Descartes. Huxley hypothesized that frogs that undergo lobotomy would swim when thrown into water, despite being unable to initiate actions. He argued that the ability to swim was solely dependent on the molecular change in the brain, concluding that consciousness is not necessary for reflex actions. According to epiphenomenalism, animals experience pain only as a result of neurophysiology.

In 1870, Huxley conducted a case study on a French soldier who had sustained a shot in the Franco-Prussian War that fractured his left parietal bone. Every few weeks the soldier would enter a trance-like state, smoking, dressing himself, and aiming his cane like a rifle all while being insensitive to pins, electric shocks, odorous substances, vinegar, noise, and certain light conditions. Huxley used this study to show that consciousness was not necessary to execute these purposeful actions, justifying the assumption that humans are insensible machines. Huxley's mechanistic attitude towards the body convinced him that the brain alone causes behavior.

In the early 1900s scientific behaviorists such as Ivan Pavlov, John B. Watson, and B. F. Skinner began the attempt to uncover laws describing the relationship between stimuli and responses, without reference to inner mental phenomena. Instead of adopting a form of eliminativism or mental fictionalism, positions that deny that inner mental phenomena exist, a behaviorist was able to adopt epiphenomenalism in order to allow for the existence of mind. George Santayana (1905) believed that all motion has merely physical causes. Because consciousness is accessory to life and not essential to it, natural selection is responsible for ingraining tendencies to avoid certain contingencies without any conscious achievement involved. By the 1960s, scientific behaviourism met substantial difficulties and eventually gave way to the cognitive revolution. Participants in that revolution, such as Jerry Fodor, reject epiphenomenalism and insist upon the efficacy of the mind. Fodor even speaks of "epiphobia"—fear that one is becoming an epiphenomenalist.

However, since the cognitive revolution, there have been several who have argued for a version of epiphenomenalism. In 1970, Keith Campbell proposed his "new epiphenomenalism", which states that the body produces a spiritual mind that does not act on the body. How the brain causes a spiritual mind, according to Campbell, is destined to remain beyond our understanding forever (see New Mysterianism). In 2001, David Chalmers and Frank Jackson argued that claims about conscious states should be deduced a priori from claims about physical states alone. They offered that epiphenomenalism bridges, but does not close, the explanatory gap between the physical and the phenomenal realms. These more recent versions maintain that only the subjective, qualitative aspects of mental states are epiphenomenal. Imagine both Pierre and a robot eating a cupcake. Unlike the robot, Pierre is conscious of eating the cupcake while the behavior is under way. This subjective experience is often called a "quale" (plural qualia), and it describes the private "raw feel" or the subjective "what-it-is-like" that is the inner accompaniment of many mental states. Thus, while Pierre and the robot are both doing the same thing, only Pierre has the inner conscious experience.

Frank Jackson (1982), for example, once espoused the following view:

According to epiphenomenalism, mental states like Pierre's pleasurable experience—or, at any rate, their distinctive qualia—are epiphenomena; they are side-effects or by-products of physical processes in the body. If Pierre takes a second bite, it is not caused by his pleasure from the first; If Pierre says, "That was good, so I will take another bite", his speech act is not caused by the preceding pleasure. The conscious experiences that accompany brain processes are causally impotent. The mind might simply be a byproduct of other properties such as brain size or pathway activation synchronicity, which are adaptive.

Some thinkers draw distinctions between different varieties of epiphenomenalism. In "Consciousness Explained", Daniel Dennett distinguishes between a purely metaphysical sense of epiphenomenalism, in which the epiphenomenon has no causal impact at all, and Huxley's "steam whistle" epiphenomenalism, in which effects exist but are not functionally relevant.

A large body of neurophysiological data seems to support epiphenomenalism . Some of the oldest such data is the Bereitschaftspotential or "readiness potential" in which electrical activity related to voluntary actions can be recorded up to two seconds before the subject is aware of making a decision to perform the action. More recently Benjamin Libet et al. (1979) have shown that it can take 0.5 seconds before a stimulus becomes part of conscious experience even though subjects can respond to the stimulus in reaction time tests within 200 milliseconds. The conclusions of this experiment have begun to receive some backlash and criticism, mainly by neuroscientists such as Peter Tse, who claim to show that the readiness potential has nothing to do with consciousness at all. Recent research on the Event Related Potential also shows that conscious experience does not occur until the late phase of the potential (P3 or later) that occurs 300 milliseconds or more after the event. In Bregman's auditory continuity illusion, where a pure tone is followed by broadband noise and the noise is followed by the same pure tone it seems as if the tone occurs throughout the period of noise. This also suggests a delay for processing data before conscious experience occurs. Popular science author Tor Nørretranders has called the delay the "user illusion", implying that we only have the illusion of conscious control, most actions being controlled automatically by non-conscious parts of the brain with the conscious mind relegated to the role of spectator.

The scientific data seem to support the idea that conscious experience is created by non-conscious processes in the brain (i.e., there is subliminal processing that becomes conscious experience). These results have been interpreted to suggest that people are capable of action before conscious experience of the decision to act occurs. Some argue that this supports epiphenomenalism, since it shows that the feeling of making a decision to act is actually an epiphenomenon; the action happens before the decision, so the decision did not cause the action to occur.
The most powerful argument against epiphenomenalism is that it is self-contradictory: if we have knowledge about epiphenomenalism, then our brains know about the existence of the mind, but if epiphenomenalism were correct, then our brains should not have any knowledge about the mind, because the mind does not affect anything physical.

However, some philosophers do not accept this as a rigorous refutation. For example, Victor Argonov states that epiphenomenalism is a questionable, but experimentally falsifiable theory. He argues that the personal mind is not the only source of knowledge about the existence of mind in the world. A creature (even a zombie) could have knowledge about mind and the mind-body problem by virtue of some innate knowledge. The information about mind (and its problematic properties such as qualia) could have been, in principle, implicitly "written" in the material world since its creation. Epiphenomenalists can say that God created immaterial mind and a detailed "program" of material human behavior that makes it possible to speak about the mind–body problem. That version of epiphenomenalism seems highly exotic, but it cannot be excluded from consideration by pure theory. However, Argonov suggests that experiments could refute epiphenomenalism. In particular, epiphenomenalism could be refuted if neural correlates of consciousness can be found in the human brain, and it is proven that human speech about consciousness is caused by them.

Some philosophers, such as Dennett, reject both epiphenomenalism and the existence of qualia with the same charge that Gilbert Ryle leveled against a Cartesian "ghost in the machine", that they too are category mistakes. A quale or conscious experience would not belong to the category of objects of reference on this account, but rather to the category of ways of doing things.

Functionalists assert that mental states are well described by their overall role, their activity in relation to the organism as a whole. "This doctrine is rooted in Aristotle's conception of the soul, and has antecedents in Hobbes's conception of the mind as a 'calculating machine', but it has become fully articulated (and popularly endorsed) only in the last third of the 20th century." In so far as it mediates stimulus and response, a mental function is analogous to a program that processes input/output in automata theory. In principle, multiple realisability would guarantee platform dependencies can be avoided, whether in terms of hardware and operating system or, "ex hypothesi", biology and philosophy. Because a high-level language is a practical requirement for developing the most complex programs, functionalism implies that a non-reductive physicalism would offer a similar advantage over a strictly eliminative materialism.

Eliminative materialists believe "folk psychology" is so unscientific that, ultimately, it will be better to eliminate primitive concepts such as "mind," "desire" and "belief," in favor of a future neuro-scientific account. A more moderate position such as J. L. Mackie's "error theory" suggests that false beliefs should be stripped away from a mental concept without eliminating the concept itself, the legitimate core meaning being left intact.

Benjamin Libet's results are quoted in favor of epiphenomenalism, but he believes subjects still have a "conscious veto", since the readiness potential does not invariably lead to an action. In "Freedom Evolves", Daniel Dennett argues that a no-free-will conclusion is based on dubious assumptions about the location of consciousness, as well as questioning the accuracy and interpretation of Libet's results. Similar criticism of Libet-style research has been made by neuroscientist Adina Roskies and cognitive theorists Tim Bayne and Alfred Mele.

Others have argued that data such as the Bereitschaftspotential undermine epiphenomenalism for the same reason, that such experiments rely on a subject reporting the point in time at which a conscious experience and a conscious decision occurs, thus relying on the subject to be able to consciously perform an action. That ability would seem to be at odds with early epiphenomenalism, which according to Huxley is the broad claim that consciousness is "completely without any power… as the steam-whistle which accompanies the work of a locomotive engine is without influence upon its machinery".

Adrian G. Guggisberg and Annaïs Mottaz have also challenged those findings.

A study by Aaron Schurger and colleagues published in PNAS challenged assumptions about the causal nature of the readiness potential itself (and the "pre-movement buildup" of neural activity in general), thus denying the conclusions drawn from studies such as Libet's and Fried's.

In favor of interactionism, Celia Green (2003) argues that epiphenomenalism does not even provide a satisfactory solution to the problem of interaction posed by substance dualism. Although it does not entail substance dualism, according to Green, epiphenomenalism implies a one-way form of interactionism that is just as hard to conceive of as the two-way form embodied in substance dualism. Green suggests the assumption that it is less of a problem may arise from the unexamined belief that physical events have some sort of primacy over mental ones.

A number of scientists and philosophers, including William James, Karl Popper, John C. Eccles and Donald Symons, dismiss epiphenomenalism from an evolutionary perspective. They point out that the view that mind is an epiphenomenon of brain activity is not consistent with evolutionary theory, because if mind were functionless, it would have disappeared long ago, as it would not have been favoured by evolution.




</doc>
<doc id="9498" url="https://en.wikipedia.org/wiki?curid=9498" title="Esperantujo">
Esperantujo

Esperantujo () or Esperantio () is the community of speakers of the Esperanto language and their culture, as well as the places and institutions where the language is used. The term is used "as if it were a country."

Although it does not occupy its own area of Earth's surface, it can be said to constitute the 120 countries which have their own national Esperanto association.

The word is formed analogously to country names. In Esperanto, the names of countries were traditionally formed from the ethnic name of their inhabitants plus the suffix "-ujo", for example, "France" was "Francujo", from "franco" (a Frenchman).

The term analogous to "Francujo" would be "Esperantistujo" (Esperantist-nation). However, that would convey the idea of the physical body of people, whereas using the name of the language as the basis of the word gives it the more abstract connotation of a cultural sphere.

Currently, names of nation states are often formed with the suffix "-io" traditionally reserved for deriving country names from geographic features, so now "Francio", and recently the form "Esperantio" has been used "i.a." in the Pasporta Servo and the Esperanto Citizens' Community.

In 1908, Dr. Wilhelm Molly attempted to create an Esperanto state in the Prussian-Belgian condominium of Neutral Moresnet, known as "Amikejo" (place of friendship). What became of it is unclear, and Neutral Moresnet was annexed to Belgium in the Treaty of Versailles, 1919.

During the 1960s came a new effort of creating an Esperanto state, which this time was called Republic of Rose Island. The state island stood in the Adriatic Sea near Italy.

After World War II, during Esperanto events a common currency was used, but the management has stopped at the end of the 20th century.

In Europe on 2 June 2001 a number of organizations (they prefer to call themselves establishments) founded the "Esperanta Civito", which "aims to be a subject of international law" and "aims to consolidate the relations between the Esperantists who feel themselves belonging to the diaspora language group which does not belong to any country". "Esperanto Civito" always uses the name Esperantujo (introduced by Hector Hodler in 1908), which itself is defined according to their interpretation of "raumism", and the meaning, therefore, may differ from the traditional Esperanto understanding of the word "Esperantujo".

Esperantujo means any physical place as Esperanto meetings or virtual networks where they meet Esperanto speakers. Sometimes it is said that it is everywhere, where Esperanto speakers are yet connected.

There is a German city, Herzberg am Harz, which since 12 July 2006 is called "the Esperanto city". There are bilingual signs and pointers, in both German and Esperanto.

Judging by the members of the World Esperanto Association, the countries with the most Esperanto speakers are (in descending order): Brazil, Germany, Japan, France, the United States, China, Italy.

A language learning partner application called Amikumu has been launched in 2017, allowing Esperanto speakers to find one each other.

There is no governmental system in Esperantujo because it is not a true state. However, there is a social hierarchy of associations:


Also there are thematic associations worldwide, which are concerned with spirituality, hobbies, science or that brings together Esperantists which share common interests.

There is also a number of global organizations, such as Sennacieca Asocio Tutmonda (SAT), or the World Esperanto Youth Organization (TEJO), which has 46 national sections.

Universal Esperanto Association is not a governmental system; however, the association represents Esperanto worldwide. In addition to the United Nations and UNESCO, the UEA has consultative relationships with UNICEF and the Council of Europe and general cooperative relations with the Organization of American States. UEA officially collaborates with the International Organization for Standardization (ISO) by means of an active connection to the ISO Committee on terminology (ISO/TC 37). The association is active for information on the European Union and other interstate and international organizations and conferences. UEA is a member of European Language Council, a joint forum of universities and linguistic associations to promote the knowledge of languages and cultures within and outside the European Union. Moreover, on 10 May 2011, the UEA and the International Information Center for Terminology (Infoterm) signed an Agreement on Cooperation, its objectives are inter exchange information, support each other and help out for projects, meetings, publications in the field of terminology and by which the UEA become Associate Member of Infoterm.

In 2003 there was a European political movement called Europe–Democracy–Esperanto created. Within it is found a European federation that brings together local associations whose statutes depends on the countries. The working language of the movement is Esperanto. The goal is "to provide the European Union with the necessary tools to set up member rights democracy". The international language is a tool to enable cross-border political and social dialogue and actively contribute to peace and understanding between peoples. The original idea in the first ballot was mainly to spread the existence and the use of Esperanto to the general public. However, in France voices have grown steadily: 25067 (2004) 28944 (2009) and 33115 (2014). In this country there are a number of movements which support the issue: France Équité, Europe-Liberté, and Politicat.

The Esperanto-flag is called "Verda Flago" (Green Flag). It consists of:

The anthem is called ""La Espero"" since 1891: it is a poem written by L. L. Zamenhof. The song is usually sung at the triumphal march composed by Félicien Menu de Ménil in 1909.

The Jubilee symbol represents the language internally, while the flag represents Esperanto movement. It contains the Latin letter E (Esperanto) and the Cyrillic letter Э (Эсперанто) symbolizing the unification of West and East.

In addition, Ludwik Lejzer Zamenhof, the initiator of the language, is often used as a symbol. Sometimes he is even called "Uncle Zam", referring to the cartoon incarnation of American Uncle Sam.

In addition to textbooks, including the "Fundamento de Esperanto" by Zamenhof, the Assimil-methods and the video-methods such as Muzzy in Gondoland of the BBC and "Pasporto al la tuta mondo", there are many courses for learning online. Moreover, some universities teach Esperanto, and the Higher Foreign Language training (University Eötvös Loránd) delivers certificates in accordance with the Common European Framework of Reference for Languages (CEFR). More than 1600 people have such a certificate around the world: in 2014 around 470 at the level of B1, 510 at the level of B2 and 700 for C1. The International League of Esperanto Teachers (ILEI) is also working to publish learning materials for teachers.

The University of Esperanto offers video lectures in Esperanto, for specialties like Confronting War, Informational Technologies and Astronomy. Courses are also held during the World Esperanto Congress in the framework of the Internacia Kongresa Universitato (IKU). After that, UEA uploads the related documents on its website.

Science is an appropriate department for works in Esperanto. For example, the Conference on the Application of Esperanto in Science and Technology (KAEST) occurs in November every year since 1998 in the Czech Republic and Slovakia. Personal initiatives are also common: Doctor of mathematics Ulrich Matthias created a document about the foundations of Linear Algebra and the American group of Maine (USA) wrote a guidebook to learn the programming language Python.

In general, Esperanto is used as a lingua franca in some websites aiming teaching of other languages, such as German, Slovak, Swahili, Wolof or Toki Pona.

Since 1889 when "La Esperantisto" appeared, and soon other magazines in Esperanto throughout many countries in the world. Some of them are information media of Esperanto associations ("Esperanto", "Sennaciulo" and "Kontakto"). Online Esperanto magazines like "Libera Folio", launched in 2003, offer independent view of the Esperanto movement, aiming to soberly and critically shed light on current development. Most of the magazines deal with current events; one of such magazines is "Monato", which is read in more than 60 countries. Its articles are written by correspondents from 40 countries, which know the local situation very well. Other most popular Esperanto newspapers are "La Ondo de Esperanto", "Beletra Almanako", "Literatura Foiro", and "Heroldo de Esperanto". Often national associations magazines are also published in order to inform about the movement in the country, such as "Le Monde de l'espéranto" of Espéranto-France. There are also scientific journals, such as "Scienca Revuo" of Internacia Scienca Asocio Esperantista (ISAE).

"Muzaiko" is a radio that has broadcast an all-day international program of songs, interviews and current events in Esperanto since 2011. The latest two can be downloaded as podcasts. Besides Muzaiko, these other stations offer an hour of Esperanto-language broadcasting of various topics: "Radio Libertaire", "Polskie Radio", "Vatican Radio", "Varsovia Vento, "Radio Verda and "Kern.punkto".

Spread of the Internet has enabled more efficient communication among Esperanto speakers and slightly replaced slower media such as mail. Many massively used websites such as Facebook or Google offer Esperanto interface. On 15 December 2009, on the occasion of the jubilee of 150th birthday of L. L. Zamenhof, Google additionally made visible the Esperanto flag as a part of their Google Doodles. Media as Twitter, Telegram, Reddit or Ipernity also contain a significant number of people in this community. In addition, content-providers such as WordPress and YouTube also enable bloggers write in Esperanto. Esperanto versions of programs such as the office suite LibreOffice and Mozilla Firefox browser, or the educational program about programming Scratch are also available. Additionally, online games like Minecraft offer complete Esperanto interface.

Monero, an anonymous cryptocurrency, was named after the Esperanto word for ""coin"" and its official wallet is available in Esperanto. The same applies to Monerujo (""Monero container""), the only open-source wallet for Android. 

Although Esperanto is not a country, there is an Esperanto football team, which has existed since 2014 and participates in matches during World Esperanto Congresses. The team is part of the N.F.-Board and not of FIFA, and have played against the teams of Armenian-originating Argentine Community in 2014 and the team from Western Sahara in 2015.

Initially, Esperanto speakers learned the language as it was described by L. L. Zamenhof. In 1905 the "Fundamento de Esperanto" put together the first Esperanto textbook, an exercise book and a universal dictionary.

The "Declaration about the essence of Esperantism" (1905) defines an "Esperantist" to be anyone who speaks and uses Esperanto. "Esperantism" was defined to be a movement to promote the widespread use of Esperanto as a supplement to mother tongues in international and inter-ethnic contexts. As the word "esperantist" is linked with this "esperantism" (the Esperanto movement) and as -ists and -isms are linked with ideologies, today many people who speak Esperanto prefer to be called "Esperanto speaker".

The monthly magazine "La Ondo de Esperanto" every year since 1998 proclaims an 'Esperantist of the year', who remarkably contributed to the spreading of the language during the year.

Publishing and selling books, the so-called book services, is the main market and is often the first expenditure of many Esperanto associations. Some companies are already well known: for example Vinilkosmo, which publishes and makes popular Esperanto music since 1990. Then there are initiatives such as the job-seeking website "Eklaboru", created by Chuck Smith, for job offers and candidates within Esperanto associations or Esperanto meetings.

In 1907, René de Saussure proposed the spesmilo ⟨₷⟩ as an international currency.
It had some use before the First World War.
The currency Stelo was created in 1942 and has been used at meetings of the "Universala Ligo" and in Esperanto environments. Over the years it slowly became unusable and at the official closing of the Universala Ligo in the 1990s, the remaining star-coins were handed over to the UEA. You can buy them at the UEA's book service as souvenirs.

The current "steloj" are made of plastic, they are used in a number of meetings, especially among young people. The currency is maintained by Stelaro, which calculates the rates, keeps the stock, and opened branches in various e-meetings. Currently, there are "stelo"-coins of 1 ★, 3 ★ and 10 ★. Quotes of Stars at 31 December 2014 were [25] 1 EUR = 4.189 ★.

There exist Zamenhof-Esperanto objects (ZEOs), scattered in numerous countries around the world, which are the things named in honor of L. L. Zamenhof or Esperanto: monuments, street names, places and so on. There also exists a UEA-committee for ZEOs.

In addition, in several countries there are also sites dedicated to Esperanto: meetup places, workshops, seminars, festivals, Esperanto houses. These places provide attractions for Esperantists. Here are two: the Castle of Grésilion in France and the Department of Planned Languages and Esperanto Museum in Vienna (Austria).

Esperanto literary heritage is the richest and the most diverse of any constructed language. There are over 25,000 Esperanto books (originals and translations) as well as over a hundred regularly distributed Esperanto magazines.

There are also a number of movies which have been published in Esperanto. Moreover, Esperanto itself was used in numerous movies.

Many public holidays recognized by Esperanto speakers are celebrated international and already accepted in other countries and organizations such as UN or UNESCO. Here are the celebrations internationally proposed by the UEA since 2010:

Every year numerous meetings of Esperanto speakers in different topics around the world take place. They mobilize Esperanto-speakers which share the same will about a specific topic. The main example is the Universal Congress of Esperanto (UK), which annually organizes the UEA every summer for a week. Other events:

Next to these globally comprising meetings there are also local events such as New Year's Gathering (NR) or Esperanto Youth Week (JES), which occur during the last days of December and first days of January. These meetings seem to have been successful during the last 20 years.

Due to the fact that there are a lot of Esperanto meetings around the globe, there are two websites which aim to list and share them. Eventoj.hu describes them with a list and dates, and contains an archive until 1996, while Esperant.io offers world map with the locations of future meetings.



</doc>
<doc id="9499" url="https://en.wikipedia.org/wiki?curid=9499" title="Ethernet">
Ethernet

Ethernet () is a family of computer networking technologies commonly used in local area networks (LAN), metropolitan area networks (MAN) and wide area networks (WAN). It was commercially introduced in 1980 and first standardized in 1983 as IEEE 802.3. Ethernet has since been refined to support higher bit rates, a greater number of nodes, and longer link distances, but retains much backward compatibility. Over time, Ethernet has largely replaced competing wired LAN technologies such as Token Ring, FDDI and ARCNET.

The original 10BASE5 Ethernet uses coaxial cable as a shared medium, while the newer Ethernet variants use twisted pair and fiber optic links in conjunction with switches. Over the course of its history, Ethernet data transfer rates have been increased from the original 2.94 megabits per second (Mbit/s) to the latest 400 gigabits per second (Gbit/s). The comprise several wiring and signaling variants of the OSI physical layer in use with Ethernet.

Systems communicating over Ethernet divide a stream of data into shorter pieces called frames. Each frame contains source and destination addresses, and error-checking data so that damaged frames can be detected and discarded; most often, higher-layer protocols trigger retransmission of lost frames. As per the OSI model, Ethernet provides services up to and including the data link layer. The 48-bit MAC address was adopted by other IEEE 802 networking standards, including IEEE 802.11 Wi-Fi, as well as by FDDI, and EtherType values are also used in Subnetwork Access Protocol (SNAP) headers.

Ethernet is widely used in homes and industry, and interworks well with Wi-Fi. The Internet Protocol is commonly carried over Ethernet and so it is considered one of the key technologies that make up the Internet.

Ethernet was developed at Xerox PARC between 1973 and 1974. It was inspired by ALOHAnet, which Robert Metcalfe had studied as part of his PhD dissertation. The idea was first documented in a memo that Metcalfe wrote on May 22, 1973, where he named it after the luminiferous aether once postulated to exist as an "omnipresent, completely-passive medium for the propagation of electromagnetic waves." In 1975, Xerox filed a patent application listing Metcalfe, David Boggs, Chuck Thacker, and Butler Lampson as inventors. In 1976, after the system was deployed at PARC, Metcalfe and Boggs published a seminal paper. Yogen Dalal, Ron Crane, Bob Garner, and Roy Ogus facilitated the upgrade from the original 2.94 Mbit/s protocol to the 10 Mbit/s protocol, which was released to the market in 1980.

Metcalfe left Xerox in June 1979 to form 3Com. He convinced Digital Equipment Corporation (DEC), Intel, and Xerox to work together to promote Ethernet as a standard. As part of that process Xerox agreed to relinquish their 'Ethernet' trademark. The first standard was published on September 30, 1980 as "The Ethernet, A Local Area Network. Data Link Layer and Physical Layer Specifications". This so-called DIX standard (Digital Intel Xerox) specified 10 Mbit/s Ethernet, with 48-bit destination and source addresses and a global 16-bit Ethertype-type field. Version 2 was published in November, 1982 and defines what has become known as Ethernet II. Formal standardization efforts proceeded at the same time and resulted in the publication of IEEE 802.3 on June 23, 1983.

Ethernet initially competed with Token Ring and other proprietary protocols. Ethernet was able to adapt to market needs and with 10BASE2, shift to inexpensive thin coaxial cable and from 1990, to the now-ubiquitous twisted pair with 10BASE-T. By the end of the 1980s, Ethernet was clearly the dominant network technology. In the process, 3Com became a major company. 3Com shipped its first 10 Mbit/s Ethernet 3C100 NIC in March 1981, and that year started selling adapters for PDP-11s and VAXes, as well as Multibus-based Intel and Sun Microsystems computers. This was followed quickly by DEC's Unibus to Ethernet adapter, which DEC sold and used internally to build its own corporate network, which reached over 10,000 nodes by 1986, making it one of the largest computer networks in the world at that time. An Ethernet adapter card for the IBM PC was released in 1982, and, by 1985, 3Com had sold 100,000. In the 1980s, IBM's own PC Network product competed with Ethernet for the PC, and through the 1980s, LAN hardware, in general, was not common on PCs. However, in the mid to late 1980s, PC networking did become popular in offices and schools for printer and fileserver sharing, and among the many diverse competing LAN technologies of that decade, Ethernet was one of the most popular. Parallel port based Ethernet adapters were produced for a time, with drivers for DOS and Windows. By the early 1990s, Ethernet became so prevalent that Ethernet ports began to appear on some PCs and most workstations. This process was greatly sped up with the introduction of 10BASE-T and its relatively small modular connector, at which point Ethernet ports appeared even on low-end motherboards.

Since then, Ethernet technology has evolved to meet new bandwidth and market requirements. In addition to computers, Ethernet is now used to interconnect appliances and other personal devices. As Industrial Ethernet it is used in industrial applications and is quickly replacing legacy data transmission systems in the world's telecommunications networks. By 2010, the market for Ethernet equipment amounted to over $16 billion per year.

In February 1980, the Institute of Electrical and Electronics Engineers (IEEE) started project 802 to standardize local area networks (LAN). The "DIX-group" with Gary Robinson (DEC), Phil Arst (Intel), and Bob Printis (Xerox) submitted the so-called "Blue Book" CSMA/CD specification as a candidate for the LAN specification. In addition to CSMA/CD, Token Ring (supported by IBM) and Token Bus (selected and henceforward supported by General Motors) were also considered as candidates for a LAN standard. Competing proposals and broad interest in the initiative led to strong disagreement over which technology to standardize. In December 1980, the group was split into three subgroups, and standardization proceeded separately for each proposal.

Delays in the standards process put at risk the market introduction of the Xerox Star workstation and 3Com's Ethernet LAN products. With such business implications in mind, David Liddle (General Manager, Xerox Office Systems) and Metcalfe (3Com) strongly supported a proposal of Fritz Röscheisen (Siemens Private Networks) for an alliance in the emerging office communication market, including Siemens' support for the international standardization of Ethernet (April 10, 1981). Ingrid Fromm, Siemens' representative to IEEE 802, quickly achieved broader support for Ethernet beyond IEEE by the establishment of a competing Task Group "Local Networks" within the European standards body ECMA TC24. On March 1982, ECMA TC24 with its corporate members reached an agreement on a standard for CSMA/CD based on the IEEE 802 draft. Because the DIX proposal was most technically complete and because of the speedy action taken by ECMA which decisively contributed to the conciliation of opinions within IEEE, the IEEE 802.3 CSMA/CD standard was approved in December 1982. IEEE published the 802.3 standard as a draft in 1983 and as a standard in 1985.

Approval of Ethernet on the international level was achieved by a similar, cross-partisan action with Fromm as the liaison officer working to integrate with International Electrotechnical Commission (IEC) Technical Committee 83 and International Organization for Standardization (ISO) Technical Committee 97 Sub Committee 6. The ISO 8802-3 standard was published in 1989.

Ethernet has evolved to include higher bandwidth, improved medium access control methods, and different physical media. The coaxial cable was replaced with point-to-point links connected by Ethernet repeaters or switches.

Ethernet stations communicate by sending each other data packets: blocks of data individually sent and delivered. As with other IEEE 802 LANs, adapters come programmed with globally unique 48-bit MAC address so that each Ethernet station has a unique address. The MAC addresses are used to specify both the destination and the source of each data packet. Ethernet establishes link-level connections, which can be defined using both the destination and source addresses. On reception of a transmission, the receiver uses the destination address to determine whether the transmission is relevant to the station or should be ignored. A network interface normally does not accept packets addressed to other Ethernet stations.

An EtherType field in each frame is used by the operating system on the receiving station to select the appropriate protocol module (e.g., an Internet Protocol version such as IPv4). Ethernet frames are said to be "self-identifying", because of the EtherType field. Self-identifying frames make it possible to intermix multiple protocols on the same physical network and allow a single computer to use multiple protocols together. Despite the evolution of Ethernet technology, all generations of Ethernet (excluding early experimental versions) use the same frame formats. Mixed-speed networks can be built using Ethernet switches and repeaters supporting the desired Ethernet variants.

Due to the ubiquity of Ethernet, the ever-decreasing cost of the hardware needed to support it, and the reduced panel space needed by twisted pair Ethernet, most manufacturers now build Ethernet interfaces directly into PC motherboards, eliminating the need for installation of a separate network card.

Ethernet was originally based on the idea of computers communicating over a shared coaxial cable acting as a broadcast transmission medium. The method used was similar to those used in radio systems, with the common cable providing the communication channel likened to the "Luminiferous aether" in 19th century physics, and it was from this reference that the name "Ethernet" was derived.

Original Ethernet's shared coaxial cable (the shared medium) traversed a building or campus to every attached machine. A scheme known as carrier sense multiple access with collision detection (CSMA/CD) governed the way the computers shared the channel. This scheme was simpler than competing Token Ring or Token Bus technologies. Computers are connected to an Attachment Unit Interface (AUI) transceiver, which is in turn connected to the cable (with thin Ethernet the transceiver is usually integrated into the network adapter). While a simple passive wire is highly reliable for small networks, it is not reliable for large extended networks, where damage to the wire in a single place, or a single bad connector, can make the whole Ethernet segment unusable.

Through the first half of the 1980s, Ethernet's 10BASE5 implementation used a coaxial cable in diameter, later called "thick Ethernet" or "thicknet". Its successor, 10BASE2, called "thin Ethernet" or "thinnet", used the RG-58 coaxial cable. The emphasis was on making installation of the cable easier and less costly.

Since all communication happens on the same wire, any information sent by one computer is received by all, even if that information is intended for just one destination. The network interface card interrupts the CPU only when applicable packets are received: the card ignores information not addressed to it. Use of a single cable also means that the data bandwidth is shared, such that, for example, available data bandwidth to each device is halved when two stations are simultaneously active.

A collision happens when two stations attempt to transmit at the same time. They corrupt transmitted data and require stations to re-transmit. The lost data and re-transmission reduces throughput. In the worst case, where multiple active hosts connected with maximum allowed cable length attempt to transmit many short frames, excessive collisions can reduce throughput dramatically. However, a Xerox report in 1980 studied performance of an existing Ethernet installation under both normal and artificially generated heavy load. The report claimed that 98% throughput on the LAN was observed. This is in contrast with token passing LANs (Token Ring, Token Bus), all of which suffer throughput degradation as each new node comes into the LAN, due to token waits. This report was controversial, as modeling showed that collision-based networks theoretically became unstable under loads as low as 37% of nominal capacity. Many early researchers failed to understand these results. Performance on real networks is significantly better.

In a modern Ethernet, the stations do not all share one channel through a shared cable or a simple repeater hub; instead, each station communicates with a switch, which in turn forwards that traffic to the destination station. In this topology, collisions are only possible if station and switch attempt to communicate with each other at the same time, and collisions are limited to this link. Furthermore, the 10BASE-T standard introduced a full duplex mode of operation which became common with Fast Ethernet and the de facto standard with Gigabit Ethernet. In full duplex, switch and station can send and receive simultaneously, and therefore modern Ethernets are completely collision-free.

For signal degradation and timing reasons, coaxial Ethernet segments have a restricted size. Somewhat larger networks can be built by using an Ethernet repeater. Early repeaters had only two ports, allowing, at most, a doubling of network size. Once repeaters with more than two ports became available, it was possible to wire the network in a star topology. Early experiments with star topologies (called "Fibernet") using optical fiber were published by 1978.

Shared cable Ethernet is always hard to install in offices because its bus topology is in conflict with the star topology cable plans designed into buildings for telephony. Modifying Ethernet to conform to twisted pair telephone wiring already installed in commercial buildings provided another opportunity to lower costs, expand the installed base, and leverage building design, and, thus, twisted-pair Ethernet was the next logical development in the mid-1980s.

Ethernet on unshielded twisted-pair cables (UTP) began with StarLAN at 1 Mbit/s in the mid-1980s. In 1987 SynOptics introduced the first twisted-pair Ethernet at 10 Mbit/s in a star-wired cabling topology with a central hub, later called LattisNet. These evolved into 10BASE-T, which was designed for point-to-point links only, and all termination was built into the device. This changed repeaters from a specialist device used at the center of large networks to a device that every twisted pair-based network with more than two machines had to use. The tree structure that resulted from this made Ethernet networks easier to maintain by preventing most faults with one peer or its associated cable from affecting other devices on the network.

Despite the physical star topology and the presence of separate transmit and receive channels in the twisted pair and fiber media, repeater-based Ethernet networks still use half-duplex and CSMA/CD, with only minimal activity by the repeater, primarily generation of the jam signal in dealing with packet collisions. Every packet is sent to every other port on the repeater, so bandwidth and security problems are not addressed. The total throughput of the repeater is limited to that of a single link, and all links must operate at the same speed.

While repeaters can isolate some aspects of Ethernet segments, such as cable breakages, they still forward all traffic to all Ethernet devices. The entire network is one collision domain, and all hosts have to be able to detect collisions anywhere on the network. This limits the number of repeaters between the farthest nodes and creates practical limits on how many machines can communicate on an Ethernet network. Segments joined by repeaters have to all operate at the same speed, making phased-in upgrades impossible.

To alleviate these problems, bridging was created to communicate at the data link layer while isolating the physical layer. With bridging, only well-formed Ethernet packets are forwarded from one Ethernet segment to another; collisions and packet errors are isolated. At initial startup, Ethernet bridges work somewhat like Ethernet repeaters, passing all traffic between segments. By observing the source addresses of incoming frames, the bridge then builds an address table associating addresses to segments. Once an address is learned, the bridge forwards network traffic destined for that address only to the associated segment, improving overall performance. Broadcast traffic is still forwarded to all network segments. Bridges also overcome the limits on total segments between two hosts and allow the mixing of speeds, both of which are critical to the incremental deployment of faster Ethernet variants.

In 1989, Motorola Codex introduced their 6310 EtherSpan, and Kalpana introduced their EtherSwitch; these were examples of the first commercial Ethernet switches. Early switches such as this used cut-through switching where only the header of the incoming packet is examined before it is either dropped or forwarded to another segment. This reduces the forwarding latency. One drawback of this method is that it does not readily allow a mixture of different link speeds. Another is that packets that have been corrupted are still propagated through the network. The eventual remedy for this was a return to the original store and forward approach of bridging, where the packet is read into a buffer on the switch in its entirety, its frame check sequence verified and only then the packet is forwarded. In modern network equipment, this process is typically done using application-specific integrated circuits allowing packets to be forwarded at wire speed.

When a twisted pair or fiber link segment is used and neither end is connected to a repeater, full-duplex Ethernet becomes possible over that segment. In full-duplex mode, both devices can transmit and receive to and from each other at the same time, and there is no collision domain. This doubles the aggregate bandwidth of the link and is sometimes advertised as double the link speed (for example, 200 Mbit/s for Fast Ethernet). The elimination of the collision domain for these connections also means that all the link's bandwidth can be used by the two devices on that segment and that segment length is not limited by the constraints of collision detection.

Since packets are typically delivered only to the port they are intended for, traffic on a switched Ethernet is less public than on shared-medium Ethernet. Despite this, switched Ethernet should still be regarded as an insecure network technology, because it is easy to subvert switched Ethernet systems by means such as ARP spoofing and MAC flooding.

The bandwidth advantages, the improved isolation of devices from each other, the ability to easily mix different speeds of devices and the elimination of the chaining limits inherent in non-switched Ethernet have made switched Ethernet the dominant network technology.

Simple switched Ethernet networks, while a great improvement over repeater-based Ethernet, suffer from single points of failure, attacks that trick switches or hosts into sending data to a machine even if it is not intended for it, scalability and security issues with regard to switching loops, broadcast radiation and multicast traffic.

Advanced networking features in switches use shortest path bridging (SPB) or the spanning-tree protocol (STP) to maintain a loop-free, meshed network, allowing physical loops for redundancy (STP) or load-balancing (SPB). Advanced networking features also ensure port security, provide protection features such as MAC lockdown and broadcast radiation filtering, use virtual LANs to keep different classes of users separate while using the same physical infrastructure, employ multilayer switching to route between different classes, and use link aggregation to add bandwidth to overloaded links and to provide some redundancy.

Shortest path bridging includes the use of the link-state routing protocol IS-IS to allow larger networks with shortest path routes between devices. In 2012, it was stated by David Allan and Nigel Bragg, in "802.1aq Shortest Path Bridging Design and Evolution: The Architect's Perspective" that shortest path bridging is one of the most significant enhancements in Ethernet's history.

Ethernet has replaced InfiniBand as the most popular system interconnect of TOP500 supercomputers.

The Ethernet physical layer evolved over a considerable time span and encompasses coaxial, twisted pair and fiber-optic physical media interfaces, with speeds from to , with 400 Gbit/s expected by 2018. The first introduction of twisted-pair CSMA/CD was StarLAN, standardized as 802.3 1BASE5. While 1BASE5 had little market penetration, it defined the physical apparatus (wire, plug/jack, pin-out, and wiring plan) that would be carried over to 10BASE-T.

The most common forms used are 10BASE-T, 100BASE-TX, and 1000BASE-T. All three use twisted pair cables and 8P8C modular connectors. They run at , , and , respectively.

Fiber optic variants of Ethernet (that use SFP) are also very common in larger networks, offering high performance, better electrical isolation and longer distance (tens of kilometers with some versions). In general, network protocol stack software will work similarly on all varieties.

In IEEE 802.3, a datagram is called a "packet" or "frame". "Packet" is used to describe the overall transmission unit and includes the preamble, start frame delimiter (SFD) and carrier extension (if present). The "frame" begins after the start frame delimiter with a frame header featuring source and destination MAC addresses and the EtherType field giving either the protocol type for the payload protocol or the length of the payload. The middle section of the frame consists of payload data including any headers for other protocols (for example, Internet Protocol) carried in the frame. The frame ends with a 32-bit cyclic redundancy check, which is used to detect corruption of data in transit. Notably, Ethernet packets have no time-to-live field, leading to possible problems in the presence of a switching loop.

Autonegotiation is the procedure by which two connected devices choose common transmission parameters, e.g. speed and duplex mode. Autonegotiation was initially an optional feature, first introduced with 100BASE-TX, while it is also backward compatible with 10BASE-T. Autonegotiation is mandatory for 1000BASE-T and faster.

A switching loop or bridge loop occurs in computer networks when there is more than one Layer 2 (OSI model) path between two endpoints (e.g. multiple connections between two network switches or two ports on the same switch connected to each other). The loop creates broadcast storms as broadcasts and multicasts are forwarded by switches out every port, the switch or switches will repeatedly rebroadcast the broadcast messages flooding the network. Since the Layer 2 header does not support a "time to live" (TTL) value, if a frame is sent into a looped topology, it can loop forever.

A physical topology that contains switching or bridge loops is attractive for redundancy reasons, yet a switched network must not have loops. The solution is to allow physical loops, but create a loop-free logical topology using the shortest path bridging (SPB) protocol or the older spanning tree protocols (STP) on the network switches.

A node that is sending longer than the maximum transmission window for an Ethernet packet is considered to be "jabbering". Depending on the physical topology, jabber detection and remedy differ somewhat.





</doc>
<doc id="9502" url="https://en.wikipedia.org/wiki?curid=9502" title="List of explorations">
List of explorations

Some of the most important explorations of State Societies, in chronological order:



</doc>
<doc id="9505" url="https://en.wikipedia.org/wiki?curid=9505" title="Elias Canetti">
Elias Canetti

Elias Canetti (; ; 25 July 1905 – 14 August 1994) was a German-language author, born in Ruse, Bulgaria to a merchant family. They moved to Manchester, England, but his father died in 1912, and his mother took her three sons back to the continent. They settled in Vienna.

Canetti moved to England in 1938 after the Anschluss to escape Nazi persecution. He became a British citizen in 1952. He is known as a modernist novelist, playwright, memoirist, and non-fiction writer. He won the Nobel Prize in Literature in 1981, "for writings marked by a broad outlook, a wealth of ideas and artistic power". He is noted for his non-fiction book "Crowds and Power", among other works.

Born in 1905 to businessman Jacques Canetti and Mathilde "née" Arditti in Ruse, a city on the Danube in Bulgaria, Canetti was the eldest of three sons. His ancestors were Sephardi Jews. His paternal ancestors settled in Ruse from Ottoman Adrianople. The original family name was "Cañete", named after Cañete, Cuenca, a village in Spain.

In Ruse, Canetti's father and grandfather were successful merchants who operated out of a commercial building, which they had built in 1898. Canetti's mother descended from the Arditti family, one of the oldest Sephardi families in Bulgaria, who were among the founders of the Ruse Jewish colony in the late 18th century. The Ardittis can be traced to the 14th century, when they were court physicians and astronomers to the Aragonese royal court of Alfonso IV and Pedro IV. Before settling in Ruse, they had migrated into Italy and lived in Livorno in the 17th century.
Canetti spent his childhood years, from 1905 to 1911, in Ruse until the family moved to Manchester, England, where Canetti's father joined a business established by his wife's brothers. In 1912, his father died suddenly, and his mother moved with their children first to Lausanne, then Vienna in the same year. They lived in Vienna from the time Canetti was aged seven onwards. His mother insisted that he speak German, and taught it to him. By this time Canetti already spoke Ladino (his native language), Bulgarian, English, and some French; the latter two he studied in the one year they were in Britain. Subsequently, the family moved first (from 1916 to 1921) to Zürich and then (until 1924) to Frankfurt, where Canetti graduated from high school.

Canetti went back to Vienna in 1924 in order to study chemistry. However, his primary interests during his years in Vienna became philosophy and literature. Introduced into the literary circles of First-Republic-Vienna, he started writing. Politically leaning towards the left, he was present at the July Revolt of 1927 – he came near to the action accidentally, was most impressed by the burning of books (recalled frequently in his writings), and left the place quickly with his bicycle. He gained a degree in chemistry from the University of Vienna in 1929, but never worked as a chemist.

He published two works in Vienna before escaping to Great Britain. He reflected the experiences of Nazi Germany and political chaos in his works, especially exploring mob action and group thinking in his novel "Die Blendung" ("Auto-da-Fé", 1935) and non-fiction "Crowds and Power" (1960). He wrote several volumes of memoirs, contemplating the influence of his multi-lingual background and childhood.
In 1934 in Vienna he married Veza (Venetiana) Taubner-Calderon (1897–1963), who acted as his muse and devoted literary assistant. Canetti remained open to relationships with other women. He had a short affair with Anna Mahler. In 1938, after the "Anschluss" with Germany, the Canettis moved to London. He became closely involved with the painter Marie-Louise von Motesiczky, who was to remain a close companion for many years. His name has also been linked with the author Iris Murdoch (see John Bayley's "Iris, A Memoir of Iris Murdoch", which has several references to an author, referred to as "the Dichter", who was a Nobel Laureate and whose works included "Die Blendung" [English title "Auto-da-Fé"]).

After Veza died in 1963, Canetti married Hera Buschor (1933–1988), with whom he had a daughter, Johanna, in 1972. Canetti's brother Jacques Canetti settled in Paris, where he championed a revival of French chanson. Despite being a German-language writer, Canetti settled in Britain until the 1970s, receiving British citizenship in 1952. For his last 20 years, Canetti lived mostly in Zürich.

A writer in German, Canetti won the Nobel Prize in Literature in 1981, "for writings marked by a broad outlook, a wealth of ideas and artistic power". He is known chiefly for his celebrated trilogy of autobiographical memoirs of his childhood and of pre-Anschluss Vienna: "Die Gerettete Zunge" (The Tongue Set Free); "Die Fackel im Ohr" (The Torch in My Ear), and "Das Augenspiel" (The Play of the Eyes); for his modernist novel "Auto-da-Fé" ("Die Blendung"); and for "Crowds and Power", a psychological study of crowd behaviour as it manifests itself in human activities ranging from mob violence to religious congregations.

In the 1970s, Canetti began to travel more frequently to Zurich, where he settled and lived for his last 20 years. He died in Zürich in 1994.







</doc>
<doc id="9506" url="https://en.wikipedia.org/wiki?curid=9506" title="Edward Jenner">
Edward Jenner

Edward Jenner, FRS FRCPE (17 May 1749 – 26 January 1823) was an English physician and scientist who pioneered the concept of vaccines including creating the smallpox vaccine, the world's first vaccine. The terms "vaccine" and "vaccination" are derived from "Variolae vaccinae" (smallpox of the cow), the term devised by Jenner to denote cowpox. He used it in 1798 in the long title of his "Inquiry into the Variolae vaccinae known as the Cow Pox", in which he described the protective effect of cowpox against smallpox.

Jenner is often called "the father of immunology", and his work is said to have "saved more lives than the work of any other human". In Jenner's time, smallpox killed around 10% of the population, with the number as high as 20% in towns and cities where infection spread more easily. In 1821, he was appointed physician extraordinary to King George IV, and was also made mayor of Berkeley and justice of the peace. A member of the Royal Society, in the field of zoology he was the first person to describe the brood parasitism of the cuckoo. In 2002, Jenner was named in the BBC’s list of the 100 Greatest Britons.

Edward Jenner was born on 17 May 1749 (6 May Old Style) in Berkeley, Gloucestershire, as the eighth of nine children. His father, the Reverend Stephen Jenner, was the vicar of Berkeley, so Jenner received a strong basic education.

He went to school in Wotton-under-Edge at Katherine Lady Berkeley's School and in Cirencester. During this time, he was inoculated (by variolation) for smallpox, which had a lifelong effect upon his general health. At the age of 14, he was apprenticed for seven years to Daniel Ludlow, a surgeon of Chipping Sodbury, South Gloucestershire, where he gained most of the experience needed to become a surgeon himself.
In 1770, aged 21, Jenner became apprenticed in surgery and anatomy under surgeon John Hunter and others at St George's Hospital, London. William Osler records that Hunter gave Jenner William Harvey's advice, well known in medical circles (and characteristic of the Age of Enlightenment), "Don't think; try." Hunter remained in correspondence with Jenner over natural history and proposed him for the Royal Society. Returning to his native countryside by 1773, Jenner became a successful family doctor and surgeon, practising on dedicated premises at Berkeley.

Jenner and others formed the Fleece Medical Society or Gloucestershire Medical Society, so called because it met in the parlour of the Fleece Inn, Rodborough, Gloucestershire. Members dined together and read papers on medical subjects. Jenner contributed papers on angina pectoris, ophthalmia, and cardiac valvular disease and commented on cowpox. He also belonged to a similar society which met in Alveston, near Bristol.

He became a master mason on 30 December 1802, in Lodge of Faith and Friendship #449. From 1812–1813, he served as worshipful master of Royal Berkeley Lodge of Faith and Friendship.

Edward Jenner was elected fellow of the Royal Society in 1788, following his publication of a careful study of the previously misunderstood life of the nested cuckoo, a study that combined observation, experiment, and dissection.

Edward Jenner described how the newly hatched cuckoo pushed its host's eggs and fledgling chicks out of the nest (contrary to existing belief that the adult cuckoo did it). Having observed this behaviour, Jenner demonstrated an anatomical adaptation for it—the baby cuckoo has a depression in its back, not present after 12 days of life, that enables it to cup eggs and other chicks. The adult does not remain long enough in the area to perform this task. Jenner's findings were published in "Philosophical Transactions of the Royal Society" in 1788.

"The singularity of its shape is well adapted to these purposes; for, different from other newly hatched birds, its back from the scapula downwards is very broad, with a considerable depression in the middle. This depression seems formed by nature for the design of giving a more secure lodgement to the egg of the Hedge-sparrow, or its young one, when the young Cuckoo is employed in removing either of them from the nest. When it is about twelve days old, this cavity is quite filled up, and then the back assumes the shape of nestling birds in general." Jenner's nephew assisted in the study. He was born on 30 June 1737.

Jenner's understanding of the cuckoo's behaviour was not entirely believed until the artist Jemima Blackburn, a keen observer of birdlife, saw a blind nestling pushing out a host's egg. Her description and illustration of this were enough to convince Charles Darwin to revise a later edition of "On the Origin of Species".

Jenner's interest in Zoology played a large role in his first experiment with inoculation. Not only did he have a profound understanding of human anatomy due to his medical training, but he also understood animal biology and its role in human-animal trans-species boundaries in disease transmission. At the time, there was no way of knowing how important this connection would be to the history and discovery of vaccinations. We see this connection now; many present-day vaccinations include animal parts from cows, rabbits, and chicken eggs, which can be attributed to the work of Jenner and his cowpox/smallpox vaccination.

Jenner married Catherine Kingscote (died 1815 from tuberculosis) in March 1788. He might have met her while he and other fellows were experimenting with balloons. Jenner's trial balloon descended into Kingscote Park, Gloucestershire, owned by Anthony Kingscote, one of whose daughters was Catherine.

He earned his MD from the University of St Andrews in 1792. He is credited with advancing the understanding of angina pectoris. In his correspondence with Heberden, he wrote: "How much the heart must suffer from the coronary arteries not being able to perform their functions".

Inoculation was already a standard practice but involved serious risks, one of which was the fear that those inoculated would then transfer the disease to those around them due to their becoming carriers of the disease. In 1721, Lady Mary Wortley Montagu had imported variolation to Britain after having observed it in Constantinople. While Johnnie Notions had great success with his self-devised inoculation (and was reputed not to have lost a single patient), his method's practice was limited to the Shetland Isles. Voltaire wrote that at this time 60% of the population caught smallpox and 20% of the population died of it. Voltaire also states that the Circassians used the inoculation from times immemorial, and the custom may have been borrowed by the Turks from the Circassians.

By 1768, English physician John Fewster had realised that prior infection with cowpox rendered a person immune to smallpox. In the years following 1770, at least five investigators in England and Germany (Sevel, Jensen, Jesty 1774, Rendell, Plett 1791) successfully tested in humans a cowpox vaccine against smallpox. For example, Dorset farmer Benjamin Jesty successfully vaccinated and presumably induced immunity with cowpox in his wife and two children during a smallpox epidemic in 1774, but it was not until Jenner's work that the procedure became widely understood. Jenner may have been aware of Jesty's procedures and success. A similar observation was later made in France by Jacques Antoine Rabaut-Pommier in 1780.

Noting the common observation that milkmaids were generally immune to smallpox, Jenner postulated that the pus in the blisters that milkmaids received from cowpox (a disease similar to smallpox, but much less virulent) protected them from smallpox.
On 14 May 1796, Jenner tested his hypothesis by inoculating James Phipps, an eight-year-old boy who was the son of Jenner's gardener. He scraped pus from cowpox blisters on the hands of Sarah Nelmes, a milkmaid who had caught cowpox from a cow called Blossom, whose hide now hangs on the wall of the St. George's Medical School library (now in Tooting). Phipps was the 17th case described in Jenner's first paper on vaccination.

Jenner inoculated Phipps in both arms that day, subsequently producing in Phipps a fever and some uneasiness, but no full-blown infection. Later, he injected Phipps with variolous material, the routine method of immunization at that time. No disease followed. The boy was later challenged with variolous material and again showed no sign of infection.

Donald Hopkins has written, "Jenner's unique contribution was not that he inoculated a few persons with cowpox, but that he then proved [by subsequent challenges] that they were immune to smallpox. Moreover, he demonstrated that the protective cowpox pus could be effectively inoculated from person to person, not just directly from cattle." Jenner successfully tested his hypothesis on 23 additional subjects.

Jenner continued his research and reported it to the Royal Society, which did not publish the initial paper. After revisions and further investigations, he published his findings on the 23 cases, including his 11 months old son Robert. Some of his conclusions were correct, some erroneous; modern microbiological and microscopic methods would make his studies easier to reproduce. The medical establishment deliberated at length over his findings before accepting them. Eventually, vaccination was accepted, and in 1840, the British government banned variolationthe use of smallpox to induce immunityand provided vaccination using cowpox free of charge ("see" Vaccination Act).

The success of his discovery soon spread around Europe and was used "en masse" in the Spanish Balmis Expedition (1803–1806), a three-year-long mission to the Americas, the Philippines, Macao, China, led by Dr. Francisco Javier de Balmis with the aim of giving thousands the smallpox vaccine. The expedition was successful, and Jenner wrote: "I don’t imagine the annals of history furnish an example of philanthropy so noble, so extensive as this". Napoleon, who at the time was at war with Britain, had all his French troops vaccinated, awarded Jenner a medal, and at the request of Jenner, he released two English prisoners of war and permitted their return home. Napoleon remarked he could not "refuse anything to one of the greatest benefactors of mankind".
Jenner's continuing work on vaccination prevented him from continuing his ordinary medical practice. He was supported by his colleagues and the King in petitioning Parliament, and was granted £10,000 in 1802 for his work on vaccination. In 1807, he was granted another £20,000 after the Royal College of Physicians confirmed the widespread efficacy of vaccination.

Jenner was also elected a foreign honorary member of the American Academy of Arts and Sciences in 1802, and a foreign member of the Royal Swedish Academy of Sciences in 1806. In 1803 in London, he became president of the Jennerian Society, concerned with promoting vaccination to eradicate smallpox. The Jennerian ceased operations in 1809. Jenner became a member of the Medical and Chirurgical Society on its founding in 1805 (now the Royal Society of Medicine) and presented several papers there. In 1808, with government aid, the National Vaccine Establishment was founded, but Jenner felt dishonoured by the men selected to run it and resigned his directorship.

Returning to London in 1811, Jenner observed a significant number of cases of smallpox after vaccination. He found that in these cases the severity of the illness was notably diminished by previous vaccination. In 1821, he was appointed physician extraordinary to King George IV, and was also made mayor of Berkeley and justice of the peace. He continued to investigate natural history, and in 1823, the last year of his life, he presented his "Observations on the Migration of Birds" to the Royal Society.

Jenner was found in a state of apoplexy on 25 January 1823, with his right side paralysed. He did not recover and died the next day of an apparent stroke, his second, on 26 January 1823, aged 73. He was buried in the family vault at the Church of St Mary, Berkeley. He was survived by his son Robert Fitzharding (1797–1854) and his daughter Catherine (1794–1833), his elder son Edward (1789–1810) having died of tuberculosis at age 21.

Neither fanatic nor lax, Jenner was a Christian who in his personal correspondence showed himself quite spiritual; he treasured the Bible. Some days before his death, he stated to a friend: "I am not surprised that men are not grateful to me; but I wonder that they are not grateful to God for the good which He has made me the instrument of conveying to my fellow creatures".
However, his contemporary Rabbi Israel Lipschitz in his classic commentary on the Mishnah, the "Tiferes Yisrael", wrote that Jenner was one of the "righteous of the nations", deserving a lofty place in the World to Come, for having saved millions of people from smallpox.

In 1979, the World Health Organization declared smallpox an eradicated disease. This was the result of coordinated public health efforts, but vaccination was an essential component. Although the disease was declared eradicated, some pus samples still remain in laboratories in Centers for Disease Control and Prevention in Atlanta in the US, and in State Research Center of Virology and Biotechnology VECTOR in Koltsovo, Novosibirsk Oblast, Russia.

Jenner's vaccine laid the foundation for contemporary discoveries in immunology. In 2002, Jenner was named in the BBC's list of the 100 Greatest Britons following a UK-wide vote. The lunar crater Jenner is named in his honour. Jenner was recognized in the TV show ""The Walking Dead"". In "TS-19", a CDC scientist is named Edwin Jenner.







</doc>
<doc id="9508" url="https://en.wikipedia.org/wiki?curid=9508" title="Encyclopædia Britannica">
Encyclopædia Britannica

The (Latin for "British Encyclopaedia") is a general knowledge English-language online encyclopaedia. It was formerly published by Encyclopædia Britannica, Inc., and other publishers (for previous editions). It was written by about 100 full-time editors and more than 4,000 contributors. The 2010 version of the 15th edition, which spans 32 volumes and 32,640 pages, was the last printed edition.

The "Britannica" is the English-language encyclopaedia that was in print for the longest time: it lasted 244 years. It was first published between 1768 and 1771 in the Scottish capital of Edinburgh, as three volumes. (This first edition is available in facsimile.) The encyclopaedia grew in size: the second edition was 10 volumes, and by its fourth edition (1801–1810) it had expanded to 20 volumes. Its rising stature as a scholarly work helped recruit eminent contributors, and the 9th (1875–1889) and 11th editions (1911) are landmark encyclopaedias for scholarship and literary style. Starting with the 11th edition and following its acquisition by an American firm, the "Britannica" shortened and simplified articles to broaden its appeal to the North American market. In 1933, the "Britannica" became the first encyclopaedia to adopt "continuous revision", in which the encyclopaedia is continually reprinted, with every article updated on a schedule. In March 2012, Encyclopædia Britannica, Inc. announced it would no longer publish printed editions, and would focus instead on "Encyclopædia Britannica Online".

The 15th edition had a three-part structure: a 12-volume of short articles (generally fewer than 750 words), a 17-volume of long articles (two to 310 pages), and a single volume to give a hierarchical outline of knowledge. The was meant for quick fact-checking and as a guide to the ; readers are advised to study the outline to understand a subject's context and to find more detailed articles. Over 70 years, the size of the "Britannica" has remained steady, with about 40 million words on half a million topics. Though published in the United States since 1901, the "Britannica" has for the most part maintained British English spelling.

Since 1985, the "Britannica" has had four parts: the , the , the , and a two-volume index. The "Britannica" articles are found in the and , which encompass 12 and 17 volumes, respectively, each volume having roughly one thousand pages. The 2007 has 699 in-depth articles, ranging in length from 2 to 310 pages and having references and named contributors. In contrast, the 2007 has roughly 65,000 articles, the vast majority (about 97%) of which contain fewer than 750 words, no references, and no named contributors. The articles are intended for quick fact-checking and to help in finding more thorough information in the . The articles are meant both as authoritative, well-written articles on their subjects and as storehouses of information not covered elsewhere. The longest article (310 pages) is on the United States, and resulted from the merger of the articles on the individual states. A 2013 "Global Edition" of "Britannica" contained approximately forty thousand articles.

Information can be found in the "Britannica" by following the cross-references in the and ; however, these are sparse, averaging one cross-reference per page. Hence, readers are recommended to consult instead the alphabetical index or the , which organizes the "Britannica" contents by topic.

The core of the is its "Outline of Knowledge", which aims to provide a logical framework for all human knowledge. Accordingly, the Outline is consulted by the "Britannica" editors to decide which articles should be included in the and . The Outline is also intended to be a study guide, to put subjects in their proper perspective, and to suggest a series of "Britannica" articles for the student wishing to learn a topic in depth. However, libraries have found that it is scarcely used, and reviewers have recommended that it be dropped from the encyclopaedia. The also has color transparencies of human anatomy and several appendices listing the staff members, advisors, and contributors to all three parts of the "Britannica".

Taken together, the and comprise roughly 40 million words and 24,000 images. The two-volume index has 2,350 pages, listing the 228,274 topics covered in the "Britannica", together with 474,675 subentries under those topics. The "Britannica" generally prefers British spelling over American; for example, it uses "colour" (not "color"), "centre" (not "center"), and "encyclopaedia" (not "encyclopedia"). However, there are exceptions to this rule, such as "defense" rather than "defence". Common alternative spellings are provided with cross-references such as "Color: "see" Colour."

Since 1936, the articles of the "Britannica" have been revised on a regular schedule, with at least 10% of them considered for revision each year. According to one Britannica website, 46% of its articles were revised over the past three years; however, according to another Britannica website, only 35% of the articles were revised.

The alphabetization of articles in the and follows strict rules. Diacritical marks and non-English letters are ignored, while numerical entries such as "1812, War of" are alphabetized as if the number had been written out ("Eighteen-twelve, War of"). Articles with identical names are ordered first by persons, then by places, then by things. Rulers with identical names are organized first alphabetically by country and then by chronology; thus, Charles III of France precedes Charles I of England, listed in "Britannica" as the ruler of Great Britain and Ireland. (That is, they are alphabetized as if their titles were "Charles, France, 3" and "Charles, Great Britain and Ireland, 1".) Similarly, places that share names are organized alphabetically by country, then by ever-smaller political divisions.

In March 2012, the company announced that the 2010 edition would be the last printed version. This was announced as a move by the company to adapt to the times and focus on its future using digital distribution. The peak year for the printed encyclopaedia was 1990 when 120,000 sets were sold, but it dropped to 40,000 in 1996. 12,000 sets of the 2010 edition were printed, of which 8,000 had been sold . By late April 2012, the remaining copies of the 2010 edition had sold out at Britannica's online store. , a replica of Britannica's 1768 first edition is sold on the online store.

"Britannica Junior" was first published in 1934 as 12 volumes. It was expanded to 15 volumes in 1947, and renamed "Britannica Junior Encyclopædia" in 1963. It was taken off the market after the 1984 printing.

A British "Children's Britannica" edited by John Armitage was issued in London in 1960. Its contents were determined largely by the eleven-plus standardized tests given in Britain. Britannica introduced the "Children's Britannica" to the US market in 1988, aimed at ages seven to 14.

In 1961, a 16 volume "Young Children's Encyclopaedia" was issued for children just learning to read.

"My First Britannica" is aimed at children ages six to 12, and the "Britannica Discovery Library" is for children aged three to six (issued 1974 to 1991).

There have been, and are, several abridged "Britannica" encyclopaedias. The single-volume "Britannica Concise Encyclopædia" has 28,000 short articles condensing the larger 32-volume "Britannica"; there are authorized translations in languages such as Chinese and Vietnamese. "Compton's by Britannica", first published in 2007, incorporating the former "Compton's Encyclopedia", is aimed at 10- to 17-year-olds and consists of 26 volumes and 11,000 pages.

Since 1938, Encyclopædia Britannica, Inc. has published annually a "Book of the Year" covering the past year's events. A given edition of the "Book of the Year" is named in terms of the year of its publication, though the edition actually covers the events of the previous year. Articles dating back to the 1994 edition are included online. The company also publishes several specialized reference works, such as "Shakespeare: The Essential Guide to the Life and Works of the Bard" (Wiley, 2006).

The "Britannica Ultimate Reference Suite 2012 DVD" contains over 100,000 articles. This includes regular "Britannica" articles, as well as others drawn from the "Britannica Student Encyclopædia", and the "Britannica Elementary Encyclopædia." The package includes a range of supplementary content including maps, videos, sound clips, animations and web links. It also offers study tools and dictionary and thesaurus entries from Merriam-Webster.

"Britannica" Online is a website with more than 120,000 articles and is updated regularly. It has daily features, updates and links to news reports from "The New York Times" and the BBC. , roughly 60% of Encyclopædia Britannica's revenue came from online operations, of which around 15% came from subscriptions to the consumer version of the websites. , subscriptions were available on a yearly, monthly or weekly basis. Special subscription plans are offered to schools, colleges and libraries; such institutional subscribers constitute an important part of Britannica's business. Beginning in early 2007, the "Britannica" made articles freely available if they are hyperlinked from an external site. Non-subscribers are served pop-ups and advertising.

On 20 February 2007, Encyclopædia Britannica, Inc. announced that it was working with mobile phone search company AskMeNow to launch a mobile encyclopaedia. Users will be able to send a question via text message, and AskMeNow will search "Britannica" 28,000-article concise encyclopaedia to return an answer to the query. Daily topical features sent directly to users' mobile phones are also planned.

On 3 June 2008, an initiative to facilitate collaboration between online expert and amateur scholarly contributors for Britannica's online content (in the spirit of a wiki), with editorial oversight from Britannica staff, was announced. Approved contributions would be credited, though contributing automatically grants Encyclopædia Britannica, Inc. perpetual, irrevocable license to those contributions.

On 22 January 2009, Britannica's president, Jorge Cauz, announced that the company would be accepting edits and additions to the online "Britannica" website from the public. The published edition of the encyclopaedia will not be affected by the changes. Individuals wishing to edit the "Britannica" website will have to register under their real name and address prior to editing or submitting their content. All edits submitted will be reviewed and checked and will have to be approved by the encyclopaedia's professional staff. Contributions from non-academic users will sit in a separate section from the expert-generated "Britannica" content, as will content submitted by non-"Britannica" scholars. Articles written by users, if vetted and approved, will also only be available in a special section of the website, separate from the professional articles. Official "Britannica" material would carry a "Britannica Checked" stamp, to distinguish it from the user-generated content.

On 14 September 2010, Encyclopædia Britannica, Inc. announced a partnership with mobile phone development company Concentric Sky to launch a series of iPhone products aimed at the K-12 market. On 20 July 2011, Encyclopædia Britannica, Inc. announced that Concentric Sky had ported the Britannica Kids product line to Intel's Intel Atom-based Netbooks and on 26 October 2011 that it had launched its encyclopedia as an iPad app. In 2010, Britannica released Britannica ImageQuest, a database of images.

In March 2012, it was announced that the company would cease printing the encyclopaedia set, and that it would focus more on its online version.

On 7 June 2018, Britannica released a Google Chrome extension, Britannica Insights, which shows snippets of information from Britannica Online in a sidebar for Google Search results. The Britannica sidebar does not replace Google's sidebar and is instead placed above Google's sidebar.

The 2007 print version of the "Britannica" has 4,411 contributors, many eminent in their fields, such as Nobel laureate economist Milton Friedman, astronomer Carl Sagan, and surgeon Michael DeBakey. Roughly a quarter of the contributors are deceased, some as long ago as 1947 (Alfred North Whitehead), while another quarter are retired or emeritus. Most (approximately 98%) contribute to only a single article; however, 64 contributed to three articles, 23 contributed to four articles, 10 contributed to five articles, and 8 contributed to more than five articles. An exceptionally prolific contributor is Christine Sutton of the University of Oxford, who contributed 24 articles on particle physics.

While "Britannica" authors have included writers such as Albert Einstein, Marie Curie, and Leon Trotsky, as well as notable independent encyclopaedists such as Isaac Asimov, some have been criticized for lack of expertise. In 1911 the historian George L. Burr wrote:

 in the fifteenth edition of "Britannica", Dale Hoiberg, a sinologist, was listed as "Britannica's" Senior Vice President and editor-in-chief. Among his predecessors as editors-in-chief were Hugh Chisholm (1902–1924), James Louis Garvin (1926–1932), Franklin Henry Hooper (1932–1938), Walter Yust (1938–1960), Harry Ashmore (1960–1963), Warren E. Preece (1964–1968, 1969–1975), Sir William Haley (1968–1969), Philip W. Goetz (1979–1991), and Robert McHenry (1992–1997). Anita Wolff was listed as the Deputy Editor and Theodore Pappas as Executive Editor. Prior Executive Editors include John V. Dodge (1950–1964) and Philip W. Goetz.

Paul T. Armstrong remains the longest working employee of Encyclopædia Britannica. He began his career there in 1934, eventually earning the positions of treasurer, vice president, and chief financial officer in his 58 years with the company, before retiring in 1992.

The 2007 editorial staff of the "Britannica" included five Senior Editors and nine Associate Editors, supervised by Dale Hoiberg and four others. The editorial staff helped to write the articles of the and some sections of the . The preparation and publication of the required trained staff. According to the final page of the 2007 , the staff were organized into ten departments:


Some of these departments were organized hierarchically. For example, the copy editors were divided into four copy editors, two senior copy editors, four supervisors, plus a coordinator and a director. Similarly, the Editorial department was headed by Dale Hoiberg and assisted by four others; they oversaw the work of five senior editors, nine associate editors, and one executive assistant.

Britannica had 14 editors in 2019: Adam Augustyn, Patricia Bauer, Brian Duignan, Alison Eldridge, Erik Gregersen, Amy McKenna, Melissa Petruzzello, John P. Rafferty, Michael Ray, Kara Rogers, Amy Tikkanen, Jeff Wallenfeldt, Adam Zeidan, and Alicja Zelazko.

The "Britannica" has an Editorial Board of Advisors, which includes 12 distinguished scholars: non-fiction author Nicholas Carr, religion scholar Wendy Doniger, political economist Benjamin M. Friedman, Council on Foreign Relations President Emeritus Leslie H. Gelb, computer scientist David Gelernter, Physics Nobel laureate Murray Gell-Mann, Carnegie Corporation of New York President Vartan Gregorian, philosopher Thomas Nagel, cognitive scientist Donald Norman, musicologist Don Michael Randel, Stewart Sutherland, Baron Sutherland of Houndwood, President of the Royal Society of Edinburgh, and cultural anthropologist Michael Wesch.

The and its "Outline of Knowledge" were produced by dozens of editorial advisors under the direction of Mortimer J. Adler. Roughly half of these advisors have since died, including some of the Outline's chief architects – Rene Dubos (d. 1982), Loren Eiseley (d. 1977), Harold D. Lasswell (d. 1978), Mark Van Doren (d. 1972), Peter Ritchie Calder (d. 1982) and Mortimer J. Adler (d. 2001). The also lists just under 4,000 advisors who were consulted for the unsigned articles.

In January 1996, the "Britannica" was purchased from the Benton Foundation by billionaire Swiss financier Jacqui Safra, who serves as its current Chair of the Board. In 1997, Don Yannias, a long-time associate and investment advisor of Safra, became CEO of Encyclopædia Britannica, Inc. In 1999, a new company, Britannica.com Inc., was created to develop digital versions of the "Britannica"; Yannias assumed the role of CEO in the new company, while his former position at the parent company remained vacant for two years. Yannias' tenure at Britannica.com Inc. was marked by missteps, considerable lay-offs, and financial losses. In 2001, Yannias was replaced by Ilan Yeshua, who reunited the leadership of the two companies. Yannias later returned to investment management, but remains on the "Britannica" Board of Directors.

In 2003, former management consultant Jorge Aguilar-Cauz was appointed President of Encyclopædia Britannica, Inc. Cauz is the senior executive and reports directly to the "Britannica's" Board of Directors. Cauz has been pursuing alliances with other companies and extending the "Britannica" brand to new educational and reference products, continuing the strategy pioneered by former CEO Elkan Harrison Powell in the mid-1930s.

Under Safra's ownership, the company has experienced financial difficulties and has responded by reducing the price of its products and implementing drastic cost cuts. According to a 2003 report in the "New York Post", the "Britannica" management has eliminated employee 401(k) accounts and encouraged the use of free images. These changes have had negative impacts, as freelance contributors have waited up to six months for checks and the "Britannica" staff have gone years without pay rises.

In the fall of 2017, Karthik Krishnan was appointed global chief executive officer of the Encyclopædia Britannica Group. Krishnan brought a varied perspective to the role based on several high-level positions in digital media, including RELX (Reed Elsevier, FT SE 100) and Rodale, in which he was responsible for "driving business and cultural transformation and accelerating growth".

Taking the reins of the company as it was preparing to mark its 250th anniversary and define the next phase of its digital strategy for consumers and K-12 schools, Krishnan launched a series of new initiatives in his first year.

First was Britannica Insights, a free, downloadable software extension to the Google Chrome browser that served up edited, fact-checked Britannica information with queries on search engines such as Google, Yahoo, and Bing. Its purpose, the company said, was to "provide trusted, verified information" in conjunction with search results that were thought to be increasingly unreliable in the era of misinformation and "fake news."

The product was quickly followed by Britannica School Insights, which provided similar content for subscribers to Britannica's online classroom solutions, and a partnership with YouTube in which verified Britannica content appeared on the site as an antidote to user-generated video content that could be false or misleading.  

Krishnan, himself an educator at New York University's Stern School of Business, believes in the "transformative power of education" and set steering the company toward solidifying its place among leaders in educational technology and supplemental curriculum. Krishnan aimed at providing more useful and relevant solutions to customer needs, extending and renewing Britannica's historical emphasis on "Utility", which had been the watchword of its first edition in 1768.

Krishnan also is active in civic affairs, with organizations such as the Urban Enterprise Initiative and Urban Upbound, whose board he serves on.

As the "Britannica" is a general encyclopaedia, it does not seek to compete with specialized encyclopaedias such as the "Encyclopaedia of Mathematics" or the "Dictionary of the Middle Ages", which can devote much more space to their chosen topics. In its first years, the "Britannica" main competitor was the general encyclopaedia of Ephraim Chambers and, soon thereafter, "Rees's Cyclopædia" and Coleridge's "Encyclopædia Metropolitana". In the 20th century, successful competitors included "Collier's Encyclopedia", the "Encyclopedia Americana", and the "World Book Encyclopedia". Nevertheless, from the 9th edition onwards, the "Britannica" was widely considered to have the greatest authority of any general English-language encyclopaedia, especially because of its broad coverage and eminent authors. The print version of the "Britannica" was significantly more expensive than its competitors.

Since the early 1990s, the "Britannica" has faced new challenges from digital information sources. The Internet, facilitated by the development of search engines, has grown into a common source of information for many people, and provides easy access to reliable original sources and expert opinions, thanks in part to initiatives such as Google Books, MIT's release of its educational materials and the open PubMed Central library of the National Library of Medicine. In general, the Internet tends to provide more current coverage than print media, due to the ease with which material on the Internet can be updated. In rapidly changing fields such as science, technology, politics, culture and modern history, the "Britannica" has struggled to stay up to date, a problem first analysed systematically by its former editor Walter Yust. Eventually, the "Britannica" turned to focus more on its online edition.

The has been compared with other print encyclopaedias, both qualitatively and quantitatively. A well-known comparison is that of Kenneth Kister, who gave a qualitative and quantitative comparison of the "Britannica" with two comparable encyclopaedias, "Collier's Encyclopedia" and the "Encyclopedia Americana". For the quantitative analysis, ten articles were selected at random—circumcision, Charles Drew, Galileo, Philip Glass, heart disease, IQ, panda bear, sexual harassment, Shroud of Turin and Uzbekistan—and letter grades of A–D or F were awarded in four categories: coverage, accuracy, clarity, and recency. In all four categories and for all three encyclopaedias, the four average grades fell between B− and B+, chiefly because none of the encyclopaedias had an article on sexual harassment in 1994. In the accuracy category, the "Britannica" received one "D" and seven "A"s, "Encyclopedia Americana" received eight "A"s, and "Collier's" received one "D" and seven "A"s; thus, "Britannica" received an average score of 92% for accuracy to "Americana"s 95% and "Collier's" 92%. In the timeliness category, "Britannica" averaged an 86% to "Americana"'s 90% and "Collier's" 85%.

The most notable competitor of the "Britannica" among CD/DVD-ROM digital encyclopaedias was "Encarta", now discontinued, a modern, multimedia encyclopaedia that incorporated three print encyclopaedias: "Funk & Wagnalls", "Collier's" and the "New Merit Scholar's Encyclopedia". "Encarta" was the top-selling multimedia encyclopaedia, based on total US retail sales from January 2000 to February 2006. Both occupied the same price range, with the "2007 Encyclopædia Britannica Ultimate" CD or DVD costing US$40–50 and the Microsoft Encarta Premium 2007 DVD costing US$45. The "Britannica" contains 100,000 articles and "Merriam-Webster's Dictionary and Thesaurus" (US only), and offers Primary and Secondary School editions. "Encarta" contained 66,000 articles, a user-friendly Visual Browser, interactive maps, math, language and homework tools, a US and UK dictionary, and a youth edition. Like "Encarta", the "Britannica" has been criticized for being biased towards United States audiences; the United Kingdom-related articles are updated less often, maps of the United States are more detailed than those of other countries, and it lacks a UK dictionary. Like the "Britannica", "Encarta" was available online by subscription, although some content could be accessed free.

The dominant internet encyclopaedia and main alternative to "Britannica" is Wikipedia. The key differences between the two lie in accessibility; the model of participation they bring to an encyclopedic project; their respective style sheets and editorial policies; relative ages; the number of subjects treated; the number of languages in which articles are written and made available; and their underlying economic models: unlike "Britannica", Wikipedia is a not-for-profit and is not connected with traditional profit- and contract-based publishing distribution networks.

The 699 printed articles are generally written by identified contributors, and the roughly 65,000 printed articles are the work of the editorial staff and identified outside consultants. Thus, a "Britannica" article either has known authorship or a set of possible authors (the editorial staff). With the exception of the editorial staff, most of the "Britannica" contributors are experts in their field—some are Nobel laureates. By contrast, the articles of Wikipedia are written by people of unknown degrees of expertise: most do not claim any particular expertise, and of those who do, many are anonymous and have no verifiable credentials. It is for this lack of institutional vetting, or certification, that former "Britannica" editor-in-chief Robert McHenry notes his belief that Wikipedia cannot hope to rival the "Britannica" in accuracy.

In 2005, the journal "Nature" chose articles from both websites in a wide range of science topics and sent them to what it called "relevant" field experts for peer review. The experts then compared the competing articles—one from each site on a given topic—side by side, but were not told which article came from which site. "Nature" got back 42 usable reviews.

In the end, the journal found just eight serious errors, such as general misunderstandings of vital concepts: four from each site. It also discovered many factual errors, omissions or misleading statements: 162 in Wikipedia and 123 in "Britannica", an average of 3.86 mistakes per article for Wikipedia and 2.92 for "Britannica". Although "Britannica "was revealed as the more accurate encyclopedia, with fewer errors, Encyclopædia Britannica, Inc. in its detailed 20-page rebuttal called "Nature"'s study flawed and misleading and called for a "prompt" retraction. It noted that two of the articles in the study were taken from a "Britannica" yearbook and not the encyclopaedia, and another two were from "Compton's Encyclopedia" (called the "Britannica Student Encyclopedia" on the company's website). The rebuttal went on to mention that some of the articles presented to reviewers were combinations of several articles, and that other articles were merely excerpts but were penalized for factual omissions. The company also noted that several of what "Nature" called errors were minor spelling variations, and that others were matters of interpretation. "Nature" defended its story and declined to retract, stating that, as it was comparing Wikipedia with the web version of "Britannica", it used whatever relevant material was available on "Britannica"s website.

Interviewed in February 2009, the managing director of "Britannica UK" said: 

Since the 3rd edition, the "Britannica" has enjoyed a popular and critical reputation for general excellence. The 3rd and the 9th editions were pirated for sale in the United States, beginning with "Dobson's Encyclopaedia". On the release of the 14th edition, "Time" magazine dubbed the "Britannica" the "Patriarch of the Library". In a related advertisement, naturalist William Beebe was quoted as saying that the "Britannica" was "beyond comparison because there is no competitor." References to the "Britannica" can be found throughout English literature, most notably in one of Sir Arthur Conan Doyle's favourite Sherlock Holmes stories, "The Red-Headed League". The tale was highlighted by the Lord Mayor of London, Gilbert Inglefield, at the bicentennial of the "Britannica".

The "Britannica" has a reputation for summarising knowledge. To further their education, some people have devoted themselves to reading the entire "Britannica", taking anywhere from three to 22 years to do so. When Fat'h Ali became the Shah of Persia in 1797, he was given a set of the "Britannica's" 3rd edition, which he read completely; after this feat, he extended his royal title to include "Most Formidable Lord and Master of the ". Writer George Bernard Shaw claimed to have read the complete 9th edition—except for the science articles—and Richard Evelyn Byrd took the "Britannica" as reading material for his five-month stay at the South Pole in 1934, while Philip Beaver read it during a sailing expedition. More recently, A.J. Jacobs, an editor at "Esquire" magazine, read the entire 2002 version of the 15th edition, describing his experiences in the well-received 2004 book, "". Only two people are known to have read two independent editions: the author C. S. Forester and Amos Urban Shirk, an American businessman who read the 11th and 14th editions, devoting roughly three hours per night for four and a half years to read the 11th. Several editors-in-chief of the "Britannica" are likely to have read their editions completely, such as William Smellie (1st edition), William Robertson Smith (9th edition), and Walter Yust (14th edition).

The CD/DVD-ROM version of the "Britannica", "Encyclopædia Britannica Ultimate Reference Suite", received the 2004 Distinguished Achievement Award from the Association of Educational Publishers. On 15 July 2009, was awarded a spot as one of "Top Ten Superbrands in the UK" by a panel of more than 2,000 independent reviewers, as reported by the BBC.

Topics are chosen in part by reference to the "Outline of Knowledge". The bulk of the "Britannica" is devoted to geography (26% of the ), biography (14%), biology and medicine (11%), literature (7%), physics and astronomy (6%), religion (5%), art (4%), Western philosophy (4%), and law (3%). A complementary study of the found that geography accounted for 25% of articles, science 18%, social sciences 17%, biography 17%, and all other humanities 25%. Writing in 1992, one reviewer judged that the "range, depth, and catholicity of coverage [of the "Britannica"] are unsurpassed by any other general Encyclopaedia."

The "Britannica" does not cover topics in equivalent detail; for example, the whole of Buddhism and most other religions is covered in a single article, whereas 14 articles are devoted to Christianity, comprising nearly half of all religion articles. However, the "Britannica" has been lauded as the "least" biased of general Encyclopaedias marketed to Western readers and praised for its biographies of important women of all eras.

On rare occasions, the "Britannica" has been criticized for its editorial choices. Given its roughly constant size, the encyclopaedia has needed to reduce or eliminate some topics to accommodate others, resulting in controversial decisions. The initial 15th edition (1974–1985) was faulted for having reduced or eliminated coverage of children's literature, military decorations, and the French poet Joachim du Bellay; editorial mistakes were also alleged, such as inconsistent sorting of Japanese biographies. Its elimination of the index was condemned, as was the apparently arbitrary division of articles into the and . Summing up, one critic called the initial 15th edition a "qualified failure...[that] cares more for juggling its format than for preserving." More recently, reviewers from the American Library Association were surprised to find that most educational articles had been eliminated from the 1992 , along with the article on psychology.

Some very few "Britannica"-appointed contributors are mistaken. A notorious instance from the "Britannica's" early years is the rejection of Newtonian gravity by George Gleig, the chief editor of the 3rd edition (1788–1797), who wrote that gravity was caused by the classical element of fire. The "Britannica" has also staunchly defended a scientific approach to cultural topics, as it did with William Robertson Smith's articles on religion in the 9th edition, particularly his article stating that the Bible was not historically accurate (1875).

The "Britannica" has received criticism, especially as editions become outdated. It is expensive to produce a completely new edition of the "Britannica", and its editors delay for as long as fiscally sensible (usually about 25 years). For example, despite continuous revision, the 14th edition became outdated after 35 years (1929–1964). When American physicist Harvey Einbinder detailed its failings in his 1964 book, "The Myth of the Britannica", the encyclopaedia was provoked to produce the 15th edition, which required 10 years of work. It is still difficult to keep the "Britannica" current; one recent critic writes, "it is not difficult to find articles that are out-of-date or in need of revision", noting that the longer articles are more likely to be outdated than the shorter articles. Information in the is sometimes inconsistent with the corresponding article(s), mainly because of the failure to update one or the other. The bibliographies of the articles have been criticized for being more out-of-date than the articles themselves.

In 2010 an inaccurate entry about the Irish Civil War was discussed in the Irish press following a decision of the Department of Education and Science to pay for online access.

Writing about the 3rd edition (1788–1797), "Britannica"s chief editor George Gleig observed that "perfection seems to be incompatible with the nature of works constructed on such a plan, and embracing such a variety of subjects." In March 2006, the "Britannica" wrote, "we in no way mean to imply that "Britannica" is error-free; we have never made such a claim." The sentiment is expressed by its original editor, William Smellie:

However, Jorge Cauz (president of Encyclopædia Britannica Inc.) asserted in 2012 that ""Britannica" [...] will always be factually correct."

Past owners have included, in chronological order, the Edinburgh, Scotland printers Colin Macfarquhar and Andrew Bell, Scottish bookseller Archibald Constable, Scottish publisher A & C Black, Horace Everett Hooper, Sears Roebuck and William Benton.

The present owner of Encyclopædia Britannica Inc. is Jacqui Safra, a Brazilian billionaire and actor. Recent advances in information technology and the rise of electronic encyclopaedias such as Encyclopædia Britannica Ultimate Reference Suite, "Encarta" and Wikipedia have reduced the demand for print encyclopaedias. To remain competitive, Encyclopædia Britannica, Inc. has stressed the reputation of the "Britannica", reduced its price and production costs, and developed electronic versions on CD-ROM, DVD, and the World Wide Web. Since the early 1930s, the company has promoted spin-off reference works.

The "Britannica" has been issued in 15 editions, with multi-volume supplements to the 3rd and 4th editions (see the Table below). The 5th and 6th editions were reprints of the 4th, the 10th edition was only a supplement to the 9th, just as the 12th and 13th editions were supplements to the 11th. The 15th underwent massive re-organization in 1985, but the updated, current version is still known as the 15th. The 14th and 15th editions were edited every year throughout their runs, so that later printings of each were entirely different from early ones.

Throughout history, the "Britannica" has had two aims: to be an excellent reference book, and to provide educational material. In 1974, the 15th edition adopted a third goal: to systematize all human knowledge. The history of the "Britannica" can be divided into five eras, punctuated by changes in management, or re-organization of the dictionary.

In the first era (1st–6th editions, 1768–1826), the "Britannica" was managed and published by its founders, Colin Macfarquhar and Andrew Bell, by Archibald Constable, and by others. The "Britannica" was first published between December 1768 and 1771 in Edinburgh as the "Encyclopædia Britannica, or, A Dictionary of Arts and Sciences, compiled upon a New Plan". In part, it was conceived in reaction to the French "Encyclopédie" of Denis Diderot and Jean le Rond d'Alembert (published 1751–72), which had been inspired by Chambers's "Cyclopaedia" (first edition 1728). It went on sale 10 December.

The "Britannica" of this period was primarily a Scottish enterprise, and it is one of the most enduring legacies of the Scottish Enlightenment. In this era, the "Britannica" moved from being a three-volume set (1st edition) compiled by one young editor—William Smellie—to a 20-volume set written by numerous authorities. Several other encyclopaedias competed throughout this period, among them editions of Abraham Rees's "Cyclopædia" and Coleridge's "Encyclopædia Metropolitana" and David Brewster's "Edinburgh Encyclopædia".

During the second era (7th–9th editions, 1827–1901), the "Britannica" was managed by the Edinburgh publishing firm A & C Black. Although some contributors were again recruited through friendships of the chief editors, notably Macvey Napier, others were attracted by the "Britannica's" reputation. The contributors often came from other countries and included the world's most respected authorities in their fields. A general index of all articles was included for the first time in the 7th edition, a practice maintained until 1974.

Production of the 9th edition was overseen by Thomas Spencer Baynes, the first English-born editor-in-chief. Dubbed the "Scholar's Edition", the 9th edition is the most scholarly of all "Britannicas". After 1880, Baynes was assisted by William Robertson Smith. No biographies of living persons were included. James Clerk Maxwell and Thomas Huxley were special advisors on science. However, by the close of the 19th century, the 9th edition was outdated, and the "Britannica" faced financial difficulties.

In the third era (10th–14th editions, 1901–1973), the "Britannica" was managed by American businessmen who introduced direct marketing and door-to-door sales. The American owners gradually simplified articles, making them less scholarly for a mass market. The 10th edition was an eleven-volume supplement (including one each of maps and an index) to the 9th, numbered as volumes 25–35, but the 11th edition was a completely new work, and is still praised for excellence; its owner, Horace Hooper, lavished enormous effort on its perfection.

When Hooper fell into financial difficulties, the "Britannica" was managed by Sears Roebuck for 18 years (1920–1923, 1928–1943). In 1932, the vice-president of Sears, Elkan Harrison Powell, assumed presidency of the "Britannica"; in 1936, he began the policy of continuous revision. This was a departure from earlier practice, in which the articles were not changed until a new edition was produced, at roughly 25-year intervals, some articles unchanged from earlier editions. Powell developed new educational products that built upon the "Britannica"s reputation.

In 1943, Sears donated the to the University of Chicago. William Benton, then a vice president of the University, provided the working capital for its operation. The stock was divided between Benton and the University, with the University holding an option on the stock. Benton became chairman of the board and managed the "Britannica" until his death in 1973. Benton set up the Benton Foundation, which managed the "Britannica" until 1996, and whose sole beneficiary was the University of Chicago. In 1968, near the end of this era, the "Britannica" celebrated its bicentennial.

In the fourth era (1974–94), the "Britannica" introduced its 15th edition, which was re-organized into three parts: the , the , and the . Under Mortimer J. Adler (member of the Board of Editors of Encyclopædia Britannica since its inception in 1949, and its chair from 1974; director of editorial planning for the 15th edition of "Britannica" from 1965), the "Britannica" sought not only to be a good reference work and educational tool, but to systematize all human knowledge. The absence of a separate index and the grouping of articles into parallel encyclopaedias (the and ) provoked a "firestorm of criticism" of the initial 15th edition. In response, the 15th edition was completely re-organized and indexed for a re-release in 1985. This second version of the 15th edition continued to be published and revised until the 2010 print version. The official title of the 15th edition is the "New Encyclopædia Britannica", although it has also been promoted as "Britannica 3".

On 9 March 1976 the US Federal Trade Commission entered an opinion and order enjoining Encyclopædia Britannica, Inc. from using: a) deceptive advertising practices in recruiting sales agents and obtaining sales leads, and b) deceptive sales practices in the door-to-door presentations of its sales agents.

In the fifth era (1994–present), digital versions have been developed and released on optical media and online. In 1996, the "Britannica" was bought by Jacqui Safra at well below its estimated value, owing to the company's financial difficulties. Encyclopædia Britannica, Inc. split in 1999. One part retained the company name and developed the print version, and the other, Britannica.com Inc., developed digital versions. Since 2001, the two companies have shared a CEO, Ilan Yeshua, who has continued Powell's strategy of introducing new products with the "Britannica" name. In March 2012, Britannica's president, Jorge Cauz, announced that it would not produce any new print editions of the encyclopaedia, with the 2010 15th edition being the last. The company will focus only on the online edition and other educational tools.

"Britannica"s final print edition was in 2010, a 32-volume set. "Britannica Global Edition" was also printed in 2010. It contained 30 volumes and 18,251 pages, with 8,500 photographs, maps, flags, and illustrations in smaller "compact" volumes. It contained over 40,000 articles written by scholars from across the world, including Nobel Prize winners. Unlike the 15th edition, it did not contain and sections, but ran A through Z as all editions up to the 14th had. The following is "Britannica"s description of the work:

The "Britannica" was dedicated to the reigning British monarch from 1788 to 1901 and then, upon its sale to an American partnership, to the British monarch and the President of the United States. Thus, the 11th edition is "dedicated by Permission to His Majesty George the Fifth, King of Great Britain and Ireland and of the British Dominions beyond the Seas, Emperor of India, and to William Howard Taft, President of the United States of America." The order of the dedications has changed with the relative power of the United States and Britain, and with relative sales; the 1954 version of the 14th edition is "Dedicated by Permission to the Heads of the Two English-Speaking Peoples, Dwight David Eisenhower, President of the United States of America, and Her Majesty, Queen Elizabeth the Second." Consistent with this tradition, the 2007 version of the current 15th edition was "dedicated by permission to the current President of the United States of America, George W. Bush, and Her Majesty, Queen Elizabeth II", while the 2010 version of the current 15th edition is "dedicated by permission to Barack Obama, President of the United States of America, and Her Majesty Queen Elizabeth II."




</doc>
<doc id="9509" url="https://en.wikipedia.org/wiki?curid=9509" title="Endometrium">
Endometrium

The endometrium is the inner epithelial layer, along with its mucous membrane, of the mammalian uterus. It has a basal layer and a functional layer; the functional layer thickens and then is shed during menstruation in humans and some other mammals, including apes, Old World monkeys, some species of bat, and the elephant shrew. In most other mammals, the endometrium is reabsorbed in the estrous cycle. During pregnancy, the glands and blood vessels in the endometrium further increase in size and number. Vascular spaces fuse and become interconnected, forming the placenta, which supplies oxygen and nutrition to the embryo and fetus. The speculated presence of an endometrial microbiota
has been argued against.

The endometrium consists of a single layer of columnar epithelium plus the stroma on which it rests. The stroma is a layer of connective tissue that varies in thickness according to hormonal influences. In the uterus, simple tubular glands reach from the endometrial surface through to the base of the stroma, which also carries a rich blood supply provided by the spiral arteries. In a woman of reproductive age, two layers of endometrium can be distinguished. These two layers occur only in the endometrium lining the cavity of the uterus, and not in the lining of the Fallopian tubes.
In the absence of progesterone, the arteries supplying blood to the functional layer constrict, so that cells in that layer become ischaemic and die, leading to menstruation.

It is possible to identify the phase of the menstrual cycle by reference to either the ovarian cycle or the uterine cycle by observing microscopic differences at each phase—for example in the ovarian cycle:

About 20,000 protein coding genes are expressed in human cells and some 70% of these genes are expressed in the normal endometrium. Just over 100 of these genes are more specifically expressed in the endometrium with only a handful genes being highly endometrium specific. The corresponding specific proteins are expressed in the glandular and stromal cells of the endometrial mucosa. The expression of many of these proteins vary depending on the menstrual cycle, for example the progesterone receptor and thyrotropin-releasing hormone both expressed in the proliferative phase, and PAEP expressed in the secretory phase. Other proteins such as the HOX11 protein that is required for female fertility, is expressed in endometrial stroma cells throughout the menstrual cycle. Certain specific proteins such as the estrogen receptor are also expressed in other types of female tissue types, such as the cervix, fallopian tubes, ovaries and breast.

The uterus and endometrium was for a long time thought to be sterile. The cervical plug of mucosa was seen to prevent the entry of any microorganisms ascending from the vagina. In the 1980s this view was challenged when it was shown that uterine infections could arise from weaknesses in the barrier of the cervical plug. Organisms from the vaginal microbiota could enter the uterus during uterine contractions in the menstrual cycle. Further studies sought to identify microbiota specific to the uterus which would be of help in identifying cases of unsuccessful IVF and miscarriages. Their findings were seen to be unreliable due to the possibility of cross-contamination in the sampling procedures used. The well-documented presence of "Lactobacillus" species, for example, was easily explained by an increase in the vaginal population being able to seep into the cervical mucous. Another study highlighted the flaws of the earlier studies including cross-contamination. It was also argued that the evidence from studies using germ-free offspring of axenic animals (germ-free) clearly showed the sterility of the uterus. The authors concluded that in light of these findings there was no existence of a microbiome.

The normal dominance of Lactobacilli in the vagina is seen as a marker for vaginal health. However, in the uterus this much lower population is seen as invasive in a closed environment that is highly regulated by female sex hormones, and that could have unwanted consequences. In studies of endometriosis "Lactobacillus" is not the dominant type and there are higher levels of "Streptococcus" and "Staphylococcus" species. Half of the cases of bacterial vaginitis showed a polymicrobial biofilm attached to the endometrium.

The endometrium is the innermost lining layer of the uterus, and functions to prevent adhesions between the opposed walls of the myometrium, thereby maintaining the patency of the uterine cavity. During the menstrual cycle or estrous cycle, the endometrium grows to a thick, blood vessel-rich, glandular tissue layer. This represents an optimal environment for the implantation of a blastocyst upon its arrival in the uterus. The endometrium is central, echogenic (detectable using ultrasound scanners), and has an average thickness of 6.7 mm.

During pregnancy, the glands and blood vessels in the endometrium further increase in size and number. Vascular spaces fuse and become interconnected, forming the placenta, which supplies oxygen and nutrition to the embryo and fetus.

The endometrial lining undergoes cyclic regeneration. Humans, apes, and some other species display the menstrual cycle, whereas most other mammals are subject to an estrous cycle. In both cases, the endometrium initially proliferates under the influence of estrogen. However, once ovulation occurs, the ovary (specifically the corpus luteum) will produce much larger amounts of progesterone. This changes the proliferative pattern of the endometrium to a secretory lining. Eventually, the secretory lining provides a hospitable environment for one or more blastocysts.

Upon fertilization, the egg may implant into the uterine wall and provide feedback to the body with human chorionic gonadotropin (HCG). HCG provides continued feedback throughout pregnancy by maintaining the corpus luteum, which will continue its role of releasing progesterone and estrogen. The endometrial lining is either reabsorbed (estrous cycle) or shed (menstrual cycle). In the latter case, the process of shedding involves the breaking down of the lining, the tearing of small connective blood vessels, and the loss of the tissue and blood that had constituted it through the vagina. The entire process occurs over a period of several days. Menstruation may be accompanied by a series of uterine contractions; these help expel the menstrual endometrium.

In case of implantation, however, the endometrial lining is neither absorbed nor shed. Instead, it remains as "decidua". The decidua becomes part of the placenta; it provides support and protection for the gestation.

If there is inadequate stimulation of the lining, due to lack of hormones, the endometrium remains thin and inactive. In humans, this will result in amenorrhea, or the absence of a menstrual period. After menopause, the lining is often described as being atrophic. In contrast, endometrium that is chronically exposed to estrogens, but not to progesterone, may become hyperplastic. Long-term use of oral contraceptives with highly potent progestins can also induce endometrial atrophy.

In humans, the cycle of building and shedding the endometrial lining lasts an average of 28 days. The endometrium develops at different rates in different mammals. Various factors including the seasons, climate, and stress can affect its development. The endometrium itself produces certain hormones at different stages of the cycle and this affects other parts of the reproductive system.

Chorionic tissue can result in marked endometrial changes, known as an Arias-Stella reaction, that have an appearance similar to cancer. Historically, this change was diagnosed as endometrial cancer and it is important only in so far as it should not be misdiagnosed as cancer.


Thin endometrium may be defined as an endometrial thickness of less than 8 mm. It usually occurs after menopause. Treatments that can improve endometrial thickness include Vitamin E, L-arginine and sildenafil citrate.

Gene expression profiling using cDNA microarray can be used for the diagnosis of endometrial disorders.
The European Menopause and Andropause Society (EMAS) released Guidelines with detailed information to assess the endometrium.

An endometrial thickness of less than 7 mm decreases the pregnancy rate in in vitro fertilization by an odds ratio of approximately 0.4 compared to an EMT of over 7 mm. However, such low thickness rarely occurs, and any routine use of this parameter is regarded as not justified.

Observation of the endometrium by transvaginal ultrasonography is used when administering fertility medication, such as in in vitro fertilization. At the time of embryo transfer, it is favorable to have an endometrium of a thickness of between 7 and 14 mm with a "triple-line" configuration, which means that the endometrium contains a hyperechoic (usually displayed as light) line in the middle surrounded by two more hypoechoic (darker) lines. A "triple-line" endometrium reflects the separation of the stratum basalis and functionalis layers, and is also observed in the periovulatory period secondary to rising estradiol levels, and disappears after ovulation.




</doc>
<doc id="9510" url="https://en.wikipedia.org/wiki?curid=9510" title="Electronic music">
Electronic music

Electronic music is music that employs electronic musical instruments, digital instruments and circuitry-based music technology. A distinction can be made between sound produced using electromechanical means (electroacoustic music) and that produced using electronics only. Electromechanical instruments have mechanical elements, such as strings, hammers and electric elements, such as magnetic pickups, power amplifiers and loudspeakers. Examples of electromechanical sound producing devices include the telharmonium, Hammond organ and the electric guitar, which are typically made loud enough for performers and audiences to hear with an instrument amplifier and speaker cabinet. Pure electronic instruments do not have vibrating strings, hammers or other sound-producing mechanisms. Devices such as the theremin, synthesizer and computer can produce electronic sounds.

The first electronic devices for performing music were developed at the end of the 19th century and shortly afterward Italian futurists explored sounds that had not been considered musical. During the 1920s and 1930s, electronic instruments were introduced and the first compositions for electronic instruments were made. By the 1940s, magnetic audio tape allowed musicians to tape sounds and then modify them by changing the tape speed or direction, leading to the development of electroacoustic tape music in the 1940s, in Egypt and France. Musique concrète, created in Paris in 1948, was based on editing together recorded fragments of natural and industrial sounds. Music produced solely from electronic generators was first produced in Germany in 1953. Electronic music was also created in Japan and the United States beginning in the 1950s. An important new development was the advent of computers to compose music. Algorithmic composition with computers was first demonstrated in the 1950s (although algorithmic composition per se without a computer had occurred much earlier, for example Mozart's Musikalisches Würfelspiel).

In the 1960s, live electronics were pioneered in America and Europe, Japanese electronic musical instruments began influencing the music industry and Jamaican dub music emerged as a form of popular electronic music. In the early 1970s, the monophonic Minimoog synthesizer and Japanese drum machines helped popularize synthesized electronic music.

In the 1970s, electronic music began to have a significant influence on popular music, with the adoption of polyphonic synthesizers, electronic drums, drum machines and turntables, through the emergence of genres such as disco, krautrock, new wave, synth-pop, hip hop and EDM. In the 1980s, electronic music became more dominant in popular music, with a greater reliance on synthesizers and the adoption of programmable drum machines such as the Roland TR-808 and bass synthesizers such as the TB-303. In the early 1980s, digital technologies for synthesizers including digital synthesizers such as the Yamaha DX7 became popular and a group of musicians and music merchants developed the Musical Instrument Digital Interface (MIDI).

Electronically produced music became popular by the 1990s, because of the advent of affordable music technology. Contemporary electronic music includes many varieties and ranges from experimental art music to popular forms such as electronic dance music. Pop electronic music is most recognizable in its 4/4 form and more connected with the mainstream than preceding forms which were popular in niche markets.

At the turn of the 20th century, experimentation with emerging electronics led to the first electronic musical instruments. These initial inventions were not sold, but were instead used in demonstrations and public performances. The audiences were presented with reproductions of existing music instead of new compositions for the instruments. While some were considered novelties and produced simple tones, the Telharmonium accurately synthesized the sound of orchestral instruments. It achieved viable public interest and made commercial progress into streaming music through telephone networks.

Critics of musical conventions at the time saw promise in these developments. Ferruccio Busoni encouraged the composition of microtonal music allowed for by electronic instruments. He predicted the use of machines in future music, writing the influential "Sketch of a New Esthetic of Music". Futurists such as Francesco Balilla Pratella and Luigi Russolo began composing music with acoustic noise to evoke the sound of machinery. They predicted expansions in timbre allowed for by electronics in the influential manifesto "The Art of Noises".

Developments of the vacuum tube led to electronic instruments that were smaller, amplified, and more practical for performance. In particular, the theremin, ondes Martenot and trautonium were commercially produced by the early 1930s.

From the late 1920s, the increased practicality of electronic instruments influenced composers such as Joseph Schillinger to adopt them. They were typically used within orchestras, and most composers wrote parts for the theremin that could otherwise be performed with string instruments.

Avant-garde composers criticized the predominant use of electronic instruments for conventional purposes. The instruments offered expansions in pitch resources that were exploited by advocates of microtonal music such as Charles Ives, Dimitrios Levidis, Olivier Messiaen and Edgard Varèse. Further, Percy Grainger used the theremin to abandon fixed tonation entirely, while Russian composers such as Gavriil Popov treated it as a source of noise in otherwise-acoustic noise music.

Developments in early recording technology paralleled that of electronic instruments. The first means of recording and reproducing audio was invented in the late 19th century with the mechanical phonograph. Record players became a common household item, and by the 1920s composers were using them to play short recordings in performances.

The introduction of electrical recording in 1925 was followed by increased experimentation with record players. Paul Hindemith and Ernst Toch composed several pieces in 1930 by layering recordings of instruments and vocals at adjusted speeds. Influenced by these techniques, John Cage composed "Imaginary Landscape No. 1" in 1939 by adjusting the speeds of recorded tones.

Concurrently, composers began to experiment with newly developed sound-on-film technology. Recordings could be spliced together to create sound collages, such as those by Tristan Tzara, Kurt Schwitters, Filippo Tommaso Marinetti, Walter Ruttmann and Dziga Vertov. Further, the technology allowed sound to be graphically created and modified. These techniques were used to compose soundtracks for several films in Germany and Russia, in addition to the popular "Dr. Jekyll and Mr. Hyde" in the United States. Experiments with graphical sound were continued by Norman McLaren from the late 1930s.

The first practical audio tape recorder was unveiled in 1935. Improvements to the technology were made using the AC biasing technique, which significantly improved recording fidelity. As early as 1942, test recordings were being made in stereo. Although these developments were initially confined to Germany, recorders and tapes were brought to the United States following the end of World War II. These were the basis for the first commercially produced tape recorder in 1948.

In 1944, prior to the use of magnetic tape for compositional purposes, Egyptian composer Halim El-Dabh, while still a student in Cairo, used a cumbersome wire recorder to record sounds of an ancient "zaar" ceremony. Using facilities at the Middle East Radio studios El-Dabh processed the recorded material using reverberation, echo, voltage controls, and re-recording. What resulted is believed to be the earliest tape music composition. The resulting work was entitled "The Expression of Zaar" and it was presented in 1944 at an art gallery event in Cairo. While his initial experiments in tape-based composition were not widely known outside of Egypt at the time, El-Dabh is also known for his later work in electronic music at the Columbia-Princeton Electronic Music Center in the late 1950s.

Following his work with Studio d'Essai at Radiodiffusion Française (RDF), during the early 1940s, Pierre Schaeffer is credited with originating the theory and practice of musique concrète. In the late 1940s, experiments in sound based composition using shellac record players were first conducted by Schaeffer. In 1950, the techniques of musique concrete were expanded when magnetic tape machines were used to explore sound manipulation practices such as speed variation (pitch shift) and tape splicing .

On 5 October 1948, RDF broadcast Schaeffer's "Etude aux chemins de fer". This was the first "movement" of "Cinq études de bruits", and marked the beginning of studio realizations and musique concrète (or acousmatic art). Schaeffer employed a disk-cutting lathe, four turntables, a four-channel mixer, filters, an echo chamber, and a mobile recording unit. Not long after this, Pierre Henry began collaborating with Schaeffer, a partnership that would have profound and lasting effects on the direction of electronic music. Another associate of Schaeffer, Edgard Varèse, began work on "Déserts", a work for chamber orchestra and tape. The tape parts were created at Pierre Schaeffer's studio, and were later revised at Columbia University.

In 1950, Schaeffer gave the first public (non-broadcast) concert of musique concrète at the École Normale de Musique de Paris. "Schaeffer used a PA system, several turntables, and mixers. The performance did not go well, as creating live montages with turntables had never been done before." Later that same year, Pierre Henry collaborated with Schaeffer on "Symphonie pour un homme seul" (1950) the first major work of musique concrete. In Paris in 1951, in what was to become an important worldwide trend, RTF established the first studio for the production of electronic music. Also in 1951, Schaeffer and Henry produced an opera, "Orpheus", for concrete sounds and voices.

Karlheinz Stockhausen worked briefly in Schaeffer's studio in 1952, and afterward for many years at the WDR Cologne's Studio for Electronic Music.

1954 saw the advent of what would now be considered authentic electric plus acoustic compositions—acoustic instrumentation augmented/accompanied by recordings of manipulated or electronically generated sound. Three major works were premiered that year: Varèse's "Déserts", for chamber ensemble and tape sounds, and two works by Otto Luening and Vladimir Ussachevsky: "Rhapsodic Variations for the Louisville Symphony" and "A Poem in Cycles and Bells", both for orchestra and tape. Because he had been working at Schaeffer's studio, the tape part for Varèse's work contains much more concrete sounds than electronic. "A group made up of wind instruments, percussion and piano alternates with the mutated sounds of factory noises and ship sirens and motors, coming from two loudspeakers."

At the German premiere of "Déserts" in Hamburg, which was conducted by Bruno Maderna, the tape controls were operated by Karlheinz Stockhausen. The title "Déserts" suggested to Varèse not only "all physical deserts (of sand, sea, snow, of outer space, of empty streets), but also the deserts in the mind of man; not only those stripped aspects of nature that suggest bareness, aloofness, timelessness, but also that remote inner space no telescope can reach, where man is alone, a world of mystery and essential loneliness."

In Cologne, what would become the most famous electronic music studio in the world, was officially opened at the radio studios of the NWDR in 1953, though it had been in the planning stages as early as 1950 and early compositions were made and broadcast in 1951. The brain child of Werner Meyer-Eppler, Robert Beyer, and Herbert Eimert (who became its first director), the studio was soon joined by Karlheinz Stockhausen and Gottfried Michael Koenig. In his 1949 thesis "Elektronische Klangerzeugung: Elektronische Musik und Synthetische Sprache", Meyer-Eppler conceived the idea to synthesize music entirely from electronically produced signals; in this way, "elektronische Musik" was sharply differentiated from French "musique concrète", which used sounds recorded from acoustical sources.

In 1954, Stockhausen composed his "Elektronische Studie II"—the first electronic piece to be published as a score. In 1955, more experimental and electronic studios began to appear. Notable were the creation of the Studio di fonologia musicale di Radio Milano, a studio at the NHK in Tokyo founded by Toshiro Mayuzumi, and the Philips studio at Eindhoven, the Netherlands, which moved to the University of Utrecht as the Institute of Sonology in 1960.

"With Stockhausen and Mauricio Kagel in residence, it became a year-round hive of charismatic avante-gardism " on two occasions combining electronically generated sounds with relatively conventional orchestras—in "Mixtur" (1964) and "Hymnen, dritte Region mit Orchester" (1967). Stockhausen stated that his listeners had told him his electronic music gave them an experience of "outer space", sensations of flying, or being in a "fantastic dream world".
More recently, Stockhausen turned to producing electronic music in his own studio in Kürten, his last work in the medium being "Cosmic Pulses" (2007).

The earliest group of electronic musical instruments in Japan, Yamaha Magna Organ was built in 1935, however after the World War II, Japanese composers such as Minao Shibata knew of the development of electronic musical instruments. By the late 1940s, Japanese composers began experimenting with electronic music and institutional sponsorship enabled them to experiment with advanced equipment. Their infusion of Asian music into the emerging genre would eventually support Japan's popularity in the development of music technology several decades later.

Following the foundation of electronics company Sony in 1946, composers Toru Takemitsu and Minao Shibata independently explored possible uses for electronic technology to produce music. Takemitsu had ideas similar to musique concrète, which he was unaware of, while Shibata foresaw the development of synthesizers and predicted a drastic change in music. Sony began producing popular magnetic tape recorders for government and public use.

The avant-garde collective Jikken Kōbō (Experimental Workshop), founded in 1950, was offered access to emerging audio technology by Sony. The company hired Toru Takemitsu to demonstrate their tape recorders with compositions and performances of electronic tape music. The first electronic tape pieces by the group were "Toraware no Onna" ("Imprisoned Woman") and "Piece B", composed in 1951 by Kuniharu Akiyama. Many of the electroacoustic tape pieces they produced were used as incidental music for radio, film, and theatre. They also held concerts employing a slide show synchronized with a recorded soundtrack. Composers outside of the Jikken Kōbō, such as Yasushi Akutagawa, Saburo Tominaga and Shirō Fukai, were also experimenting with radiophonic tape music between 1952 and 1953.

Musique concrète was introduced to Japan by Toshiro Mayuzumi, who was influenced by a Pierre Schaeffer concert. From 1952, he composed tape music pieces for a comedy film, a radio broadcast, and a radio drama. However, Schaeffer's concept of "sound object" was not influential among Japanese composers, who were mainly interested in overcoming the restrictions of human performance. This led to several Japanese electroacoustic musicians making use of serialism and twelve-tone techniques, evident in Yoshirō Irino's 1951 dodecaphonic piece "Concerto da
Camera", in the organization of electronic sounds in Mayuzumi's "X, Y, Z for Musique Concrète", and later in Shibata's electronic music by 1956.

Modelling the NWDR studio in Cologne, NHK established an electronic music studio in Tokyo in 1955, which became one of the world's leading electronic music facilities. The NHK Studio was equipped with technologies such as tone-generating and audio processing equipment, recording and radiophonic equipment, ondes Martenot, Monochord and Melochord, sine-wave oscillators, tape recorders, ring modulators, band-pass filters, and four- and eight-channel mixers. Musicians associated with the studio included Toshiro Mayuzumi, Minao Shibata, Joji Yuasa, Toshi Ichiyanagi, and Toru Takemitsu. The studio's first electronic compositions were completed in 1955, including Mayuzumi's five-minute pieces "Studie I: Music for Sine Wave by Proportion of Prime Number", "Music for Modulated Wave by Proportion of Prime Number" and "Invention for Square Wave and Sawtooth Wave" produced using the studio's various tone-generating capabilities, and Shibata's 20-minute stereo piece "Musique Concrète for Stereophonic Broadcast".

In the United States, electronic music was being created as early as 1939, when John Cage published "Imaginary Landscape, No. 1", using two variable-speed turntables, frequency recordings, muted piano, and cymbal, but no electronic means of production. Cage composed five more "Imaginary Landscapes" between 1942 and 1952 (one withdrawn), mostly for percussion ensemble, though No. 4 is for twelve radios and No. 5, written in 1952, uses 42 recordings and is to be realized as a magnetic tape. According to Otto Luening, Cage also performed a "William Mix" at Donaueschingen in 1954, using eight loudspeakers, three years after his alleged collaboration. "Williams Mix" was a success at the Donaueschingen Festival, where it made a "strong impression".

The Music for Magnetic Tape Project was formed by members of the New York School (John Cage, Earle Brown, Christian Wolff, David Tudor, and Morton Feldman), and lasted three years until 1954. Cage wrote of this collaboration: "In this social darkness, therefore, the work of Earle Brown, Morton Feldman, and Christian Wolff continues to present a brilliant light, for the reason that at the several points of notation, performance, and audition, action is provocative."

Cage completed "Williams Mix" in 1953 while working with the Music for Magnetic Tape Project. The group had no permanent facility, and had to rely on borrowed time in commercial sound studios, including the studio of Louis and Bebe Barron.

In the same year Columbia University purchased its first tape recorder—a professional Ampex machine—for the purpose of recording concerts. Vladimir Ussachevsky, who was on the music faculty of Columbia University, was placed in charge of the device, and almost immediately began experimenting with it.

Herbert Russcol writes: "Soon he was intrigued with the new sonorities he could achieve by recording musical instruments and then superimposing them on one another." Ussachevsky said later: "I suddenly realized that the tape recorder could be treated as an instrument of sound transformation." On Thursday, May 8, 1952, Ussachevsky presented several demonstrations of tape music/effects that he created at his Composers Forum, in the McMillin Theatre at Columbia University. These included "Transposition, Reverberation, Experiment, Composition", and "Underwater Valse". In an interview, he stated: "I presented a few examples of my discovery in a public concert in New York together with other compositions I had written for conventional instruments."
Otto Luening, who had attended this concert, remarked: "The equipment at his disposal consisted of an Ampex tape recorder . . . and a simple box-like device designed by the brilliant young engineer, Peter Mauzey, to create feedback, a form of mechanical reverberation. Other equipment was borrowed or purchased with personal funds."

Just three months later, in August 1952, Ussachevsky traveled to Bennington, Vermont at Luening's invitation to present his experiments. There, the two collaborated on various pieces. Luening described the event: "Equipped with earphones and a flute, I began developing my first tape-recorder composition. Both of us were fluent improvisors and the medium fired our imaginations."
They played some early pieces informally at a party, where "a number of composers almost solemnly congratulated us saying, 'This is it' ('it' meaning the music of the future)."

Word quickly reached New York City. Oliver Daniel telephoned and invited the pair to "produce a group of short compositions for the October concert sponsored by the American Composers Alliance and Broadcast Music, Inc., under the direction of Leopold Stokowski at the Museum of Modern Art in New York. After some hesitation, we agreed. . . . Henry Cowell placed his home and studio in Woodstock, New York, at our disposal. With the borrowed equipment in the back of Ussachevsky's car, we left Bennington for Woodstock and stayed two weeks. . . . In late September, 1952, the travelling laboratory reached Ussachevsky's living room in New York, where we eventually completed the compositions."

Two months later, on October 28, Vladimir Ussachevsky and Otto Luening presented the first Tape Music concert in the United States. The concert included Luening's "Fantasy in Space" (1952)—"an impressionistic virtuoso piece" using manipulated recordings of flute—and "Low Speed" (1952), an "exotic composition that took the flute far below its natural range." Both pieces were created at the home of Henry Cowell in Woodstock, NY. After several concerts caused a sensation in New York City, Ussachevsky and Luening were invited onto a live broadcast of NBC's Today Show to do an interview demonstration—the first televised electroacoustic performance. Luening described the event: "I improvised some [flute] sequences for the tape recorder. Ussachevsky then and there put them through electronic transformations."

The score for "Forbidden Planet", by Louis and Bebe Barron, was entirely composed using custom built electronic circuits and tape recorders in 1956 (but no synthesizers in the modern sense of the word).

The world's first computer to play music was CSIRAC, which was designed and built by Trevor Pearcey and Maston Beard. Mathematician Geoff Hill programmed the CSIRAC to play popular musical melodies from the very early 1950s. In 1951 it publicly played the Colonel Bogey March, of which no known recordings exist, only the accurate reconstruction. However, CSIRAC played standard repertoire and was not used to extend musical thinking or composition practice. CSIRAC was never recorded, but the music played was accurately reconstructed. The oldest known recordings of computer-generated music were played by the Ferranti Mark 1 computer, a commercial version of the Baby Machine from the University of Manchester in the autumn of 1951. The music program was written by Christopher Strachey.

The impact of computers continued in 1956. Lejaren Hiller and Leonard Isaacson composed "Illiac Suite" for string quartet, the first complete work of computer-assisted composition using algorithmic composition. "... Hiller postulated that a computer could be taught the rules of a particular style and then called on to compose accordingly." Later developments included the work of Max Mathews at Bell Laboratories, who developed the influential MUSIC I program in 1957, one of the first computer programs to play electronic music. Vocoder technology was also a major development in this early era. In 1956, Stockhausen composed "Gesang der Jünglinge", the first major work of the Cologne studio, based on a text from the "Book of Daniel". An important technological development of that year was the invention of the Clavivox synthesizer by Raymond Scott with subassembly by Robert Moog.

In 1957, Kid Baltan (Dick Raaymakers) and Tom Dissevelt released their debut album, "Song Of The Second Moon", recorded at the Philips studio in the Netherlands. The public remained interested in the new sounds being created around the world, as can be deduced by the inclusion of Varèse's "Poème électronique", which was played over four hundred loudspeakers at the Philips Pavilion of the 1958 Brussels World Fair. That same year, Mauricio Kagel, an Argentine composer, composed "Transición II". The work was realized at the WDR studio in Cologne. Two musicians performed on a piano, one in the traditional manner, the other playing on the strings, frame, and case. Two other performers used tape to unite the presentation of live sounds with the future of prerecorded materials from later on and its past of recordings made earlier in the performance.

In 1958, Columbia-Princeton developed the RCA Mark II Sound Synthesizer, the first programmable synthesizer. Prominent composers such as Vladimir Ussachevsky, Otto Luening, Milton Babbitt, Charles Wuorinen, Halim El-Dabh, Bülent Arel and Mario Davidovsky used the RCA Synthesizer extensively in various compositions. One of the most influential composers associated with the early years of the studio was Egypt's Halim El-Dabh who, after having developed the earliest known electronic tape music in 1944, became more famous for "Leiyla and the Poet", a 1959 series of electronic compositions that stood out for its immersion and seamless fusion of electronic and folk music, in contrast to the more mathematical approach used by serial composers of the time such as Babbitt. El-Dabh's "Leiyla and the Poet", released as part of the album "Columbia-Princeton Electronic Music Center" in 1961, would be cited as a strong influence by a number of musicians, ranging from Neil Rolnick, Charles Amirkhanian and Alice Shields to rock musicians Frank Zappa and The West Coast Pop Art Experimental Band.

These were fertile years for electronic music—not just for academia, but for independent artists as synthesizer technology became more accessible. By this time, a strong community of composers and musicians working with new sounds and instruments was established and growing. 1960 witnessed the composition of Luening's "Gargoyles" for violin and tape as well as the premiere of Stockhausen's "Kontakte" for electronic sounds, piano, and percussion. This piece existed in two versions—one for 4-channel tape, and the other for tape with human performers. "In "Kontakte", Stockhausen abandoned traditional musical form based on linear development and dramatic climax. This new approach, which he termed 'moment form,' resembles the 'cinematic splice' techniques in early twentieth century film."

The theremin had been in use since the 1920s but it attained a degree of popular recognition through its use in science-fiction film soundtrack music in the 1950s (e.g., Bernard Herrmann's classic score for "The Day the Earth Stood Still").

In the UK in this period, the BBC Radiophonic Workshop (established in 1958) came to prominence, thanks in large measure to their work on the BBC science-fiction series "Doctor Who". One of the most influential British electronic artists in this period was Workshop staffer Delia Derbyshire, who is now famous for her 1963 electronic realisation of the iconic "Doctor Who" theme, composed by Ron Grainer.

In 1961 Josef Tal established the "Centre for Electronic Music in Israel" at The Hebrew University, and in 1962 Hugh Le Caine arrived in Jerusalem to install his "Creative Tape Recorder" in the centre. In the 1990s Tal conducted, together with Dr Shlomo Markel, in cooperation with the Technion – Israel Institute of Technology, and VolkswagenStiftung a research project (Talmark) aimed at the development of a novel musical notation system for electronic music.

Milton Babbitt composed his first electronic work using the synthesizer—his "Composition for Synthesizer" (1961)—which he created using the RCA synthesizer at the Columbia-Princeton Electronic Music Center.

The collaborations also occurred across oceans and continents. In 1961, Ussachevsky invited Varèse to the Columbia-Princeton Studio (CPEMC). Upon arrival, Varese embarked upon a revision of "Déserts". He was assisted by Mario Davidovsky and Bülent Arel.

The intense activity occurring at CPEMC and elsewhere inspired the establishment of the San Francisco Tape Music Center in 1963 by Morton Subotnick, with additional members Pauline Oliveros, Ramon Sender, Anthony Martin, and Terry Riley.

Later, the Center moved to Mills College, directed by Pauline Oliveros, where it is today known as the Center for Contemporary Music.

Simultaneously in San Francisco, composer Stan Shaff and equipment designer Doug McEachern, presented the first “Audium” concert at San Francisco State College (1962), followed by a work at the San Francisco Museum of Modern Art (1963), conceived of as in time, controlled movement of sound in space. Twelve speakers surrounded the audience, four speakers were mounted on a rotating, mobile-like construction above. In an SFMOMA performance the following year (1964), "San Francisco Chronicle" music critic Alfred Frankenstein commented, "the possibilities of the space-sound continuum have seldom been so extensively explored". In 1967, the first Audium, a "sound-space continuum" opened, holding weekly performances through 1970. In 1975, enabled by seed money from the National Endowment for the Arts, a new Audium opened, designed floor to ceiling for spatial sound composition and performance. “In contrast, there are composers who manipulated sound space by locating multiple speakers at various locations in a performance space and then switching or panning the sound between the sources. In this approach, the composition of spatial manipulation is dependent on the location of the speakers and usually exploits the acoustical properties of the enclosure. Examples include Varese's "Poeme Electronique" (tape music performed in the Philips Pavilion of the 1958 World Fair, Brussels) and Stanley Schaff's "Audium" installation, currently active in San Francisco” Through weekly programs (over 4,500 in 40 years), Shaff “sculpts” sound, performing now-digitized spatial works live through 176 speakers.

A well-known example of the use of Moog's full-sized Moog modular synthesizer is the "Switched-On Bach" album by Wendy Carlos, which triggered a craze for synthesizer music.

In 1969 David Tudor brought a Moog modular synthesizer and Ampex tape machines to the National Institute of Design in Ahmedabad with the support of the Sarabhai family, forming the foundation of India's first electronic music studio. Here a group of composers Jinraj Joshipura, Gita Sarabhai, SC Sharma, IS Mathur and Atul Desai developed experimental sound compositions between 1969–1973

Along with the Moog modular synthesizer, other makes of this period included ARP and Buchla.

Pietro Grossi was an Italian pioneer of computer composition and tape music, who first experimented with electronic techniques in the early sixties. Grossi was a cellist and composer, born in Venice in 1917. He founded the S 2F M (Studio de Fonologia Musicale di Firenze) in 1963 in order to experiment with electronic sound and composition.

Musical melodies were first generated by the computer CSIRAC in Australia in 1950. There were newspaper reports from America and England (early and recently) that computers may have played music earlier, but thorough research has debunked these stories as there is no evidence to support the newspaper reports (some of which were obviously speculative). Research has shown that people "speculated" about computers playing music, possibly because computers would make noises, but there is no evidence that they actually did it.

The world's first computer to play music was CSIRAC, which was designed and built by Trevor Pearcey and Maston Beard in the 1950s. Mathematician Geoff Hill programmed the CSIRAC to play popular musical melodies from the very early 1950s. In 1951 it publicly played the "Colonel Bogey March" of which no known recordings exist.
However, CSIRAC played standard repertoire and was not used to extend musical thinking or composition practice which is current computer-music practice.

The first music to be performed in England was a performance of the British National Anthem that was programmed by Christopher Strachey on the Ferranti Mark I, late in 1951. Later that year, short extracts of three pieces were recorded there by a BBC outside broadcasting unit: the National Anthem, "Ba, Ba Black Sheep, and "In the Mood" and this is recognised as the earliest recording of a computer to play music. This recording can be heard at this Manchester University site. Researchers at the University of Canterbury, Christchurch declicked and restored this recording in 2016 and the results may be heard on SoundCloud.

Laurie Spiegel is also notable for her development of "Music Mouse—an Intelligent Instrument" (1986) for Macintosh, Amiga, and Atari computers. The intelligent-instrument name refers to the program's built-in knowledge of chord and scale convention and stylistic constraints. She continued to update the program through Macintosh OS 9, and , it remained available for purchase or demo download from her Web site.

The late 1950s, 1960s and 1970s also saw the development of large mainframe computer synthesis. Starting in 1957, Max Mathews of Bell Labs developed the MUSIC programs, culminating in MUSIC V, a direct digital synthesis language

In Europe in 1964, Karlheinz Stockhausen composed "Mikrophonie I" for tam-tam, hand-held microphones, filters, and potentiometers, and "Mixtur" for orchestra, four sine-wave generators, and four ring modulators. In 1965 he composed "Mikrophonie II" for choir, Hammond organ, and ring modulators.

In 1966–67, Reed Ghazala discovered and began to teach "circuit bending"—the application of the creative short circuit, a process of chance short-circuiting, creating experimental electronic instruments, exploring sonic elements mainly of timbre and with less regard to pitch or rhythm, and influenced by John Cage's aleatoric music concept.

In the 1950s, Japanese electronic musical instruments began influencing the international music industry. Ikutaro Kakehashi, who founded Ace Tone in 1960, developed his own version of electronic percussion that had been already popular on the overseas electronic organ. At NAMM 1964, he revealed it as the R-1 Rhythm Ace, a hand-operated percussion device that played electronic drum sounds manually as the user pushed buttons, in a similar fashion to modern electronic drum pads.
In 1963, Korg released the Donca-Matic DA-20, an electro-mechanical drum machine. In 1965, Nippon Columbia patented a fully electronic drum machine. Korg released the Donca-Matic DC-11 electronic drum machine in 1966, which they followed with the Korg Mini Pops, which was developed as an option for the Yamaha Electone electric organ. Korg's Stageman and Mini Pops series were notable for "natural metallic percussion" sounds and incorporating controls for drum "breaks and fill-ins."

In 1967, Ace Tone founder Ikutaro Kakehashi patented a preset rhythm-pattern generator using diode matrix circuit similar to the Seeburg's prior filed in 1964 (See Drum machine#History), which he released as the FR-1 Rhythm Ace drum machine the same year. It offered 16 preset patterns, and four buttons to manually play each instrument sound (cymbal, claves, cowbell and bass drum). The rhythm patterns could also be cascaded together by pushing multiple rhythm buttons simultaneously, and the possible combination of rhythm patterns were more than a hundred. Ace Tone's Rhythm Ace drum machines found their way into popular music from the late 1960s, followed by Korg drum machines in the 1970s. Kakehashi later left Ace Tone and founded Roland Corporation in 1972, with and becoming highly influential for the next several decades. The company would go on to have a big impact on popular music, and do more to shape popular electronic music than any other company.
Turntablism has origins in the invention of direct-drive turntables. Early belt-drive turntables were unsuitable for turntablism, since they had a slow start-up time, and they were prone to wear-and-tear and breakage, as the belt would break from backspin or scratching. The first direct-drive turntable was invented by Shuichi Obata, an engineer at Matsushita (now Panasonic), based in Osaka, Japan. It eliminated belts, and instead employed a motor to directly drive a platter on which a vinyl record rests. In 1969, Matsushita released it as the SP-10, the first direct-drive turntable on the market, and the first in their influential Technics series of turntables. It was succeeded by the Technics SL-1100 and SL-1200 in the early 1970s, and they were widely adopted by hip hop musicians, with the SL-1200 remaining the most widely used turntable in DJ culture for several decades.

In Jamaica, a form of popular electronic music emerged in the 1960s, dub music, rooted in sound system culture. Dub music was pioneered by studio engineers, such as Sylvan Morris, King Tubby, Errol Thompson, Lee "Scratch" Perry, and Scientist, producing reggae-influenced experimental music with electronic sound technology, in recording studios and at sound system parties. Their experiments included forms of tape-based composition comparable to aspects of "musique concrète", an emphasis on repetitive rhythmic structures (often stripped of their harmonic elements) comparable to minimalism, the electronic manipulation of spatiality, the sonic electronic manipulation of pre-recorded musical materials from mass media, deejays toasting over pre-recorded music comparable to live electronic music, remixing music, turntablism, and the mixing and scratching of vinyl.

Despite the limited electronic equipment available to dub pioneers such as King Tubby and Lee "Scratch" Perry, their experiments in remix culture were musically cutting-edge. King Tubby, for example, was a sound system proprietor and electronics technician, whose small front-room studio in the Waterhouse ghetto of western Kingston was a key site of dub music creation.

In the late 1960s, pop and rock musicians, including the Beach Boys and the Beatles, began to use electronic instruments, like the theremin and Mellotron, to supplement and define their sound. In his book "Electronic and Experimental Music", Thom Holmes recognises the Beatles' 1966 recording "Tomorrow Never Knows" as the song that "ushered in a new era in the use of electronic music in rock and pop music" due to the band's incorporation of tape loops and reversed and speed-manipulated tape sounds. By the end of the decade, the Moog synthesizer took a leading place in the sound of emerging progressive rock with bands including Pink Floyd, Yes, Emerson, Lake & Palmer, and Genesis making them part of their sound. Gershon Kingsley's "Popcorn" was the first international electronic dance hit in 1969. Instrumental prog rock was particularly significant in continental Europe, allowing bands like Kraftwerk, Tangerine Dream, Can, and Faust to circumvent the language barrier. Their synthesiser-heavy "krautrock", along with the work of Brian Eno (for a time the keyboard player with Roxy Music), would be a major influence on subsequent electronic rock.

Ambient dub was pioneered by King Tubby and other Jamaican sound artists, using DJ-inspired ambient electronics, complete with drop-outs, echo, equalization and psychedelic electronic effects. It featured layering techniques and incorporated elements of world music, deep basslines and harmonic sounds. Techniques such as a long echo delay were also used. Other notable artists within the genre include Dreadzone, Higher Intelligence Agency, The Orb, Ott, Loop Guru, Woob and Transglobal Underground.

Electronic rock was also produced by several Japanese musicians, including Isao Tomita's "Electric Samurai: Switched on Rock" (1972), which featured Moog synthesizer renditions of contemporary pop and rock songs, and Osamu Kitajima's progressive rock album "Benzaiten" (1974). The mid-1970s saw the rise of electronic art music musicians such as Jean Michel Jarre, Vangelis, Tomita and Klaus Schulze were a significant influence on the development of new-age music.

Dub music influenced electronic musical techniques later adopted by hip hop music, when Jamaican immigrant DJ Kool Herc in the early 1970s introduced Jamaica's sound system culture and dub music techniques to America. One such technique that became popular in hip hop culture was playing two copies of the same record on two turntables in alternation, extending the b-dancers' favorite section. The turntable eventually went on to become the most visible electronic musical instrument, and occasionally the most virtuosic, in the 1980s and 1990s.

After the arrival of punk rock, a form of basic electronic rock emerged, increasingly using new digital technology to replace other instruments. Pioneering bands included Ultravox with their 1977 track "Hiroshima Mon Amour" on "Ha!-Ha!-Ha!", Gary Numan, Depeche Mode and The Human League. Yellow Magic Orchestra in particular helped pioneer synth-pop with their self-titled album (1978) and "Solid State Survivor" (1979). The definition of MIDI and the development of digital audio made the development of purely electronic sounds much easier. These developments led to the growth of synth-pop, which after it was adopted by the New Romantic movement, allowed synthesizers to dominate the pop and rock music of the early 80s. Key acts included Duran Duran, Depeche Mode, Spandau Ballet, A Flock of Seagulls, Culture Club, Talk Talk, Japan, and Eurythmics. Synth-pop sometimes used synthesizers to replace all other instruments, until the style began to fall from popularity in the mid-1980s.

Released in 1970 by Moog Music, the Mini-Moog was among the first widely available, portable and relatively affordable synthesizers. It became once the most widely used synthesizer at that time in both popular and electronic art music.
Patrick Gleeson, playing live with Herbie Hancock in the beginning of the 1970s, pioneered the use of synthesizers in a touring context, where they were subject to stresses the early machines were not designed for.

In 1974, the WDR studio in Cologne acquired an EMS Synthi 100 synthesizer, which a number of composers used to produce notable electronic works—including Rolf Gehlhaar's "Fünf deutsche Tänze" (1975), Karlheinz Stockhausen's "Sirius" (1975–76), and John McGuire's "Pulse Music III" (1978).

In 1975, the Japanese company Yamaha licensed the algorithms for frequency modulation synthesis (FM synthesis) from John Chowning, who had experimented with it at Stanford University since 1971. Yamaha's engineers began adapting Chowning's algorithm for use in a digital synthesizer, adding improvements such as the "key scaling" method to avoid the introduction of distortion that normally occurred in analog systems during frequency modulation. However, the first commercial digital synthesizer to be released would be the Australian Fairlight company's Fairlight CMI (Computer Musical Instrument) in 1979, as the first practical polyphonic digital synthesizer/sampler system.

In 1980, Yamaha eventually released the first FM digital synthesizer, the Yamaha GS-1, but at an expensive price. In 1983, Yamaha introduced the first stand-alone digital synthesizer, the DX7, which also used FM synthesis and would become one of the best-selling synthesizers of all time. The DX7 was known for its recognizable bright tonalities that was partly due to an overachieving sampling rate of 57 kHz.

Barry Vercoe describes one of his experiences with early computer sounds:

IRCAM in Paris became a major center for computer music research and realization and development of the Sogitec 4X computer system, featuring then revolutionary real-time digital signal processing. Pierre Boulez's "Répons" (1981) for 24 musicians and 6 soloists used the 4X to transform and route soloists to a loudspeaker system.

STEIM is a center for research and development of new musical instruments in the electronic performing arts, located in Amsterdam, Netherlands. STEIM has existed since 1969. It was founded by Misha Mengelberg, Louis Andriessen, Peter Schat, Dick Raaymakers, , Reinbert de Leeuw, and Konrad Boehmer. This group of Dutch composers had fought for the reformation of Amsterdam's feudal music structures; they insisted on Bruno Maderna's appointment as musical director of the Concertgebouw Orchestra and enforced the first public fundings for experimental and improvised electronic music in The Netherlands.

In 1980, a group of musicians and music merchants met to standardize an interface that new instruments could use to communicate control instructions with other instruments and computers. This standard was dubbed Musical Instrument Digital Interface (MIDI) and resulted from a collaboration between leading manufacturers, initially Sequential Circuits, Oberheim, Roland—and later, other participants that included Yamaha, Korg, and Kawai. A paper was authored by Dave Smith of Sequential Circuits and proposed to the Audio Engineering Society in 1981. Then, in August 1983, the MIDI Specification 1.0 was finalized.

MIDI technology allows a single keystroke, control wheel motion, pedal movement, or command from a microcomputer to activate every device in the studio remotely and in synchrony, with each device responding according to conditions predetermined by the composer.

MIDI instruments and software made powerful control of sophisticated instruments easily affordable by many studios and individuals. Acoustic sounds became reintegrated into studios via sampling and sampled-ROM-based instruments.

Miller Puckette developed graphic signal-processing software for 4X called Max (after Max Mathews) and later ported it to Macintosh (with Dave Zicarelli extending it for Opcode) for real-time MIDI control, bringing algorithmic composition availability to most composers with modest computer programming background.

The early 1980s saw the rise of bass synthesizers, the most influential being the Roland TB-303, a bass synthesizer and sequencer released in late 1981 that later became a fixture in electronic dance music, particularly acid house. One of the first to use it was Charanjit Singh in 1982, though it wouldn't be popularized until Phuture's "Acid Tracks" in 1987. Music sequencers began being used around the mid 20th century, and Tomita's albums in mid-1970s being later examples. In 1978, Yellow Magic Orchestra were using computer-based technology in conjunction with a synthesiser to produce popular music, making their early use of the microprocessor-based Roland MC-8 Microcomposer sequencer.

Drum machines, also known as rhythm machines, also began being used around the late-1950s, with a later example being Osamu Kitajima's progressive rock album "Benzaiten" (1974), which used a rhythm machine along with electronic drums and a synthesizer. In 1977, Ultravox's "Hiroshima Mon Amour" was one of the first singles to use the metronome-like percussion of a Roland TR-77 drum machine. In 1980, Roland Corporation released the TR-808, one of the first and most popular programmable drum machines. The first band to use it was Yellow Magic Orchestra in 1980, and it would later gain widespread popularity with the release of Marvin Gaye's "Sexual Healing" and Afrika Bambaataa's "Planet Rock" in 1982. The TR-808 was a fundamental tool in the later Detroit techno scene of the late 1980s, and was the drum machine of choice for Derrick May and Juan Atkins.

The characteristic lo-fi sound of chip music was initially the result of early sound cards' technical limitations; however, the sound has since become sought after in its own right.

The trend has continued to the present day with modern nightclubs worldwide regularly playing electronic dance music (EDM). Today, electronic dance music has radio stations, websites, and publications like "Mixmag" dedicated solely to the genre. Moreover, the genre has found commercial and cultural significance in the United States and North America, thanks to the wildly popular big room house/EDM sound that has been incorporated into U.S. pop music and the rise of large-scale commercial raves such as Electric Daisy Carnival, Tomorrowland (festival) and Ultra Music Festival.

Other recent developments included the Tod Machover (MIT and IRCAM) composition "Begin Again Again" for "hypercello", an interactive system of sensors measuring physical movements of the cellist. Max Mathews developed the "Conductor" program for real-time tempo, dynamic and timbre control of a pre-input electronic score. Morton Subotnick released a multimedia CD-ROM "All My Hummingbirds Have Alibis".

As computer technology has become more accessible and music software has advanced, interacting with music production technology is now possible using means that bear no relationship to traditional musical performance practices: for instance, laptop performance ("laptronica"), live coding and Algorave. In general, the term Live PA refers to any live performance of electronic music, whether with laptops, synthesizers, or other devices.

Beginning around the year 2000, a number of software-based virtual studio environments emerged, with products such as Propellerhead's Reason and Ableton Live finding popular appeal. Such tools provide viable and cost-effective alternatives to typical hardware-based production studios, and thanks to advances in microprocessor technology, it is now possible to create high quality music using little more than a single laptop computer. Such advances have democratized music creation, leading to a massive increase in the amount of home-produced electronic music available to the general public via the internet. Software based instruments and effect units (so called "plugins") can be incorporated in a computer-based studio using the VST platform. Some of these instruments are more or less exact replicas of existing hardware (such as the Roland D-50, ARP Odyssey, Yamaha DX7 or Korg M1). In many cases, these software-based instruments are sonically indistinguishable from their physical counterpart.

Circuit bending is the modification of battery powered toys and synthesizers to create new unintended sound effects. It was pioneered by Reed Ghazala in the 1960s and Reed coined the name "circuit bending" in 1992.

Following the circuit bending culture, musicians also began to build their own modular synthesizers, causing a renewed interest for the early 1960s designs. Eurorack became a popular system.





</doc>
<doc id="9514" url="https://en.wikipedia.org/wiki?curid=9514" title="Edvard Grieg">
Edvard Grieg

Edvard Hagerup Grieg ( , ; 15 June 18434 September 1907) was a Norwegian composer and pianist. He is widely considered one of the leading Romantic era composers, and his music is part of the standard classical repertoire worldwide. His use and development of Norwegian folk music in his own compositions brought the music of Norway to international consciousness, as well as helping to develop a national identity, much as Jean Sibelius did in Finland and Bedřich Smetana did in Bohemia.

Grieg is the most celebrated person from the city of Bergen, with numerous statues depicting his image, and many cultural entities named after him: the city's largest concert building (Grieg Hall), its most advanced music school (Grieg Academy) and its professional choir (Edvard Grieg Kor). The Edvard Grieg Museum at Grieg's former home, Troldhaugen, is dedicated to his legacy.

Edvard Hagerup Grieg was born in Bergen, Norway (then part of Sweden–Norway). His parents were Alexander Grieg (1806–1875), a merchant and vice-consul in Bergen; and Gesine Judithe Hagerup (1814–1875), a music teacher and daughter of solicitor and politician Edvard Hagerup. The family name, originally spelled Greig, is associated with the Scottish Clann Ghriogair (Clan Gregor). After the Battle of Culloden in 1746, Grieg's great-grandfather, Alexander Greig, travelled widely, settling in Norway about 1770, and establishing business interests in Bergen. Grieg's first cousin, twice removed, was Canadian pianist Glenn Gould, whose mother was a Grieg.

Edvard Grieg was raised in a musical family. His mother was his first piano teacher and taught him to play at the age of six. Grieg studied in several schools, including Tanks Upper Secondary School.

In the summer of 1858, Grieg met the eminent Norwegian violinist Ole Bull, who was a family friend; Bull's brother was married to Grieg's aunt. Bull recognized the 15-year-old boy's talent and persuaded his parents to send him to the Leipzig Conservatory, the piano department of which was directed by Ignaz Moscheles.

Grieg enrolled in the conservatory, concentrating on the piano, and enjoyed the many concerts and recitals given in Leipzig. He disliked the discipline of the conservatory course of study. An exception was the organ, which was mandatory for piano students. About his study in the conservatory, he wrote to his biographer, Aimar Grønvold, in 1881: "I must admit, unlike Svendsen, that I left Leipzig Conservatory just as stupid as I entered it. Naturally, I did learn something there, but my individuality was still a closed book to me."

In the spring of 1860, he survived two life-threatening lung diseases, pleurisy and tuberculosis. Throughout his life, Grieg's health was impaired by a destroyed left lung and considerable deformity of his thoracic spine. He suffered from numerous respiratory infections, and ultimately developed combined lung and heart failure. Grieg was admitted many times to spas and sanatoria both in Norway and abroad. Several of his doctors became his friends.

In 1861, Grieg made his debut as a concert pianist in Karlshamn, Sweden. In 1862, he finished his studies in Leipzig and held his first concert in his home town, where his programme included Beethoven's "Pathétique" sonata.
In 1863, Grieg went to Copenhagen, Denmark, and stayed there for three years. He met the Danish composers J. P. E. Hartmann and Niels Gade. He also met his fellow Norwegian composer Rikard Nordraak (composer of the Norwegian national anthem), who became a good friend and source of inspiration. Nordraak died in 1866, and Grieg composed a funeral march in his honor.

On 11 June 1867, Grieg married his first cousin, Nina Hagerup (1845–1935), a lyric soprano. The next year, their only child, Alexandra, was born. Alexandra died in 1869 from meningitis. In the summer of 1868, Grieg wrote his Piano Concerto in A minor while on holiday in Denmark. Edmund Neupert gave the concerto its premiere performance on 3 April 1869 in the Casino Theatre in Copenhagen. Grieg himself was unable to be there due to conducting commitments in Christiania (now Oslo).

In 1868, Franz Liszt, who had not yet met Grieg, wrote a testimonial for him to the Norwegian Ministry of Education, which led to Grieg's obtaining a travel grant. The two men met in Rome in 1870. On Grieg's first visit, they went over Grieg's Violin Sonata No. 1, which pleased Liszt greatly. On his second visit in April, Grieg brought with him the manuscript of his Piano Concerto, which Liszt proceeded to sightread (including the orchestral arrangement). Liszt's rendition greatly impressed his audience, although Grieg gently pointed out to him that he played the first movement too quickly. Liszt also gave Grieg some advice on orchestration (for example, to give the melody of the second theme in the first movement to a solo trumpet).

In 1874–76, Grieg composed incidental music for the premiere of Henrik Ibsen's play "Peer Gynt", at the request of the author.

Grieg had close ties with the Bergen Philharmonic Orchestra (Harmonien), and later became Music Director of the orchestra from 1880 to 1882. In 1888, Grieg met Tchaikovsky in Leipzig. Grieg was struck by the greatness of Tchaikovsky. Tchaikovsky thought very highly of Grieg's music, praising its beauty, originality and warmth.

On 6 December 1897, Grieg and his wife performed some of his music at a private concert at Windsor Castle for Queen Victoria and her court.

Grieg was awarded two honorary doctorates, first by the University of Cambridge in 1894 and the next from the University of Oxford in 1906.

The Norwegian government provided Grieg with a pension as he reached retirement age. In the spring of 1903, Grieg made nine 78-rpm gramophone recordings of his piano music in Paris. All of these discs have been reissued on both LPs and CDs, despite limited fidelity. Grieg recorded player piano music rolls for the Hupfeld Phonola piano-player system and Welte-Mignon reproducing system, all of which survive and can be heard today. He also worked with the Aeolian Company for its 'Autograph Metrostyle' piano roll series wherein he indicated the tempo mapping for many of his pieces.

In 1899, Grieg cancelled his concerts in France in protest of the Dreyfus Affair, an anti-semitic scandal that was then roiling French politics. Regarding this scandal, Grieg had written that he hoped that the French might, "Soon return to the spirit of 1789, when the French republic declared that it would defend basic human rights." As a result of his position on the affair, he became the target of much French hate mail of that day.

In 1906, he met the composer and pianist Percy Grainger in London. Grainger was a great admirer of Grieg's music and a strong empathy was quickly established. In a 1907 interview, Grieg stated: "I have written Norwegian Peasant Dances that no one in my country can play, and here comes this Australian who plays them as they ought to be played! He is a genius that we Scandinavians cannot do other than love."

Edvard Grieg died at the Municipal Hospital in Bergen, Norway, on 4 September 1907 at age 64 from heart failure. He had suffered a long period of illness. His last words were "Well, if it must be so."

The funeral drew between 30,000 and 40,000 people to the streets of his home town to honor him. Following his wish, his own "Funeral March in Memory of Rikard Nordraak" was played with orchestration by his friend Johan Halvorsen, who had married Grieg's niece. In addition, the "Funeral March" movement from Chopin's Piano Sonata No. 2 was played. Grieg was cremated, and his ashes were entombed in a mountain crypt near his house, Troldhaugen. After the death of his wife, her ashes were placed alongside his.

Edvard Grieg and his wife were Unitarians and Nina attended the Unitarian church in Copenhagen after his death.

A century after his death, Grieg's legacy extends beyond the field of music. There is a large statue of Grieg in Seattle, while one of the largest hotels in Bergen (his hometown) is named Quality Hotel Edvard Grieg (with over 370 rooms), and a large crater on the planet Mercury is named after Grieg.

Some of Grieg's early works include a symphony (which he later suppressed) and a piano sonata. He also wrote three violin sonatas and a cello sonata.
Grieg also composed the incidental music for Henrik Ibsen's play "Peer Gynt", which includes the famous excerpt titled, "In the Hall of the Mountain King". In this piece of music, the adventures of the anti-hero, Peer Gynt, are related, including the episode in which he steals a bride at her wedding. The angry guests chase him, and Peer falls, hitting his head on a rock. He wakes up in a mountain surrounded by trolls. The music of "In the Hall of the Mountain King" represents the angry trolls taunting Peer and gets louder each time the theme repeats. The music ends with Peer escaping from the mountain.

In an 1874 letter to his friend Frants Beyer, Grieg expressed his unhappiness with Dance of the Mountain King's Daughter, one of the movements he composed for "Peer Gynt", writing "I have also written something for the scene in the hall of the mountain King – something that I literally can't bear listening to because it absolutely reeks of cow-pies, exaggerated Norwegian nationalism, and trollish self-satisfaction! But I have a hunch that the irony will be discernible."

Grieg's "Holberg Suite" was originally written for the piano, and later arranged by the composer for string orchestra. Grieg wrote songs in which he set lyrics by poets Heinrich Heine, Johann Wolfgang von Goethe, Henrik Ibsen, Hans Christian Andersen, Rudyard Kipling and others. Russian composer Nikolai Myaskovsky used a theme by Grieg for the variations with which he closed his Third String Quartet. Norwegian pianist Eva Knardahl recorded the composer's complete piano music on 13 LPs for BIS Records from 1977 to 1980. The recordings were reissued in 2006 on 12 compact discs, also on BIS Records. Grieg himself recorded many of these piano works before his death in 1907.



Notes
Bibliography









</doc>
<doc id="9515" url="https://en.wikipedia.org/wiki?curid=9515" title="Emancipation Proclamation">
Emancipation Proclamation

The Emancipation Proclamation, or Proclamation 95, was a presidential proclamation and executive order issued by United States President Abraham Lincoln on September 22, 1862, during the Civil War. The Proclamation read: 

The Proclamation changed the legal status under federal law of more than 3.5 million enslaved African Americans in the secessionist Confederate states from slave to free. As soon as a slave escaped the control of the Confederate government, either by running away across Union lines or through the advance of federal troops, the slave was permanently free. Ultimately, the Union victory brought the proclamation into effect in all of the former Confederacy. 

The proclamation was directed to all of the areas in rebellion and all segments of the executive branch (including the Army and Navy) of the United States. It proclaimed the freedom of slaves in the ten states in rebellion. Even though it excluded areas not in rebellion, it still applied to more than 3.5 million of the 4 million slaves in the country. Around 25,000 to 75,000 slaves were immediately emancipated in those regions of the Confederacy where the US Army was already in place. It could not be enforced in the areas still in rebellion, but as the Union army took control of Confederate regions, the Proclamation provided the legal framework for the liberation of more than three and a half million slaves in those regions. Prior to the Proclamation, in accordance with the Fugitive Slave Act of 1850, escaped slaves were either returned to their masters or held in camps as contraband for later return. The Emancipation Proclamation outraged white Southerners and their sympathizers, who saw it as the beginning of a race war. It energized abolitionists, and undermined those Europeans that wanted to intervene to help the Confederacy. The Proclamation lifted the spirits of African Americans both free and slave. It led many slaves to escape from their masters and get to Union lines to obtain their freedom, and to join the Union Army. 

The Emancipation Proclamation was never challenged in court. To ensure the abolition of slavery in all of the U.S., Lincoln also insisted that Reconstruction plans for Southern states require abolition in new state laws (which occurred during the war in Tennessee, Arkansas, and Louisiana); Lincoln encouraged border states to adopt abolition (which occurred during the war in Maryland, Missouri, and West Virginia) and pushed for passage of the Thirteenth Amendment. Congress passed the 13th Amendment by the necessary two-thirds vote on January 31, 1865, and it was ratified by the states on December 6, 1865, ending legal slavery in the United States.

The United States Constitution of 1787 did not use the word "slavery" but included several provisions about unfree persons. The Three-Fifths Compromise (in Article I, Section 2) allocated Congressional representation based "on the whole Number of free Persons" and "three-fifths of all other Persons". Under the Fugitive Slave Clause (Article IV, Section 2), "no person held to service or labour in one state" would be freed by escaping to another. allowed Congress to pass legislation to outlaw the "Importation of Persons", but not until 1808. However, for purposes of the Fifth Amendment—which states that, "No person shall ... be deprived of life, liberty, or property, without due process of law"—slaves were understood as property. Although abolitionists used the Fifth Amendment to argue against slavery, it became part of the legal basis for treating slaves as property with "Dred Scott v. Sandford" (1857). Socially, slavery was also supported in law and in practice by a pervasive culture of white supremacy. Nonetheless, between 1777 and 1804, every Northern state provided for the immediate or gradual abolition of slavery, except the border states of Maryland and Delaware. Maryland did not abolish slavery until 1864, and Delaware was one of the last states to hold onto slavery; it was still legal in Delaware when the thirteenth amendment was issued. No Southern state did so, and the slave population of the South continued to grow, peaking at almost four million people at the beginning of the American Civil War, when most slave states sought to break away from the United States.

Lincoln understood that the Federal government's power to end slavery in peacetime was limited by the Constitution which before 1865, committed the issue to individual states. Against the background of the American Civil War, however, Lincoln issued the Proclamation under his authority as "Commander-in-Chief of the Army and Navy" under Article II, section 2 of the United States Constitution. As such, he claimed to have the martial power to free persons held as slaves in those states that were in rebellion "as a fit and necessary war measure for suppressing said rebellion". He did not have Commander-in-Chief authority over the four slave-holding states that were not in rebellion: Missouri, Kentucky, Maryland and Delaware, and so those states were not named in the Proclamation. The fifth border jurisdiction, West Virginia, where slavery remained legal but was in the process of being abolished, was, in January 1863, still part of the legally recognized, "reorganized" state of Virginia, based in Alexandria, which was in the Union (as opposed to the Confederate state of Virginia, based in Richmond).

The Proclamation applied in the ten states that were still in rebellion in 1863, and thus did not cover the nearly 500,000 slaves in the slave-holding border states (Missouri, Kentucky, Maryland or Delaware) which were Union states. Those slaves were freed by later separate state and federal actions.

The state of Tennessee had already mostly returned to Union control, under a recognized Union government, so it was not named and was exempted. Virginia was named, but exemptions were specified for the 48 counties then in the process of forming the new state of West Virginia, and seven additional counties and two cities in the Union-controlled Tidewater region of Virginia. Also specifically exempted were New Orleans and 13 named parishes of Louisiana, which were mostly under federal control at the time of the Proclamation. These exemptions left unemancipated an additional 300,000 slaves.

The Emancipation Proclamation has been ridiculed, notably in an influential passage by Richard Hofstadter for "freeing" only the slaves over which the Union had no power. These slaves were freed due to Lincoln's "war powers". This act cleared up the issue of contraband slaves. It automatically clarified the status of over 100,000 now-former slaves. Some 20,000 to 50,000 slaves were freed the day it went into effect in parts of nine of the ten states to which it applied (Texas being the exception). In every Confederate state (except Tennessee and Texas), the Proclamation went into immediate effect in Union-occupied areas and at least 20,000 slaves were freed at once on January 1, 1863.

The Proclamation provided the legal framework for the emancipation of nearly all four million slaves as the Union armies advanced, and committed the Union to ending slavery, which was a controversial decision even in the North. Hearing of the Proclamation, more slaves quickly escaped to Union lines as the Army units moved South. As the Union armies advanced through the Confederacy, thousands of slaves were freed each day until nearly all (approximately 3.9 million, according to the 1860 Census) were freed by July 1865.

While the Proclamation had freed most slaves as a war measure, it had not made slavery illegal. Of the states that were exempted from the Proclamation, Maryland, Missouri, Tennessee, and West Virginia prohibited slavery before the war ended. In 1863, President Lincoln proposed a moderate plan for the Reconstruction of the captured Confederate State of Louisiana. Only 10% of the state's electorate had to take the loyalty oath. The state was also required to accept the Proclamation and abolish slavery in its new constitution. Identical Reconstruction plans would be adopted in Arkansas and Tennessee. By December 1864, the Lincoln plan abolishing slavery had been enacted in Louisiana, as well as in Arkansas and Tennessee. However, in Delaware and Kentucky, slavery continued to be legal until December 18, 1865, when the Thirteenth Amendment went into effect.

The Fugitive Slave Act of 1850 required individuals to return runaway slaves to their owners. During the war, Union generals such as Benjamin Butler declared that slaves in occupied areas were contraband of war and accordingly refused to return them. This decision was controversial because it implied recognition of the Confederacy as a separate, independent sovereign state under international law, a notion that Lincoln steadfastly denied. As a result, he did not promote the contraband designation. In addition, as contraband, these people were legally designated as "property" when they crossed Union lines and their ultimate status was uncertain.

In December 1861, Lincoln sent his first annual message to Congress (the State of the Union Address, but then typically given in writing and not referred to as such). In it he praised the free labor system, as respecting human rights over property rights; he endorsed legislation to address the status of contraband slaves and slaves in loyal states, possibly through buying their freedom with federal taxes, and also the funding of strictly voluntary colonization efforts. In January 1862, Thaddeus Stevens, the Republican leader in the House, called for total war against the rebellion to include emancipation of slaves, arguing that emancipation, by forcing the loss of enslaved labor, would ruin the rebel economy. On March 13, 1862, Congress approved a "Law Enacting an Additional Article of War", which stated that from that point onward it was forbidden for Union Army officers to return fugitive slaves to their owners. On April 10, 1862, Congress declared that the federal government would compensate slave owners who freed their slaves. Slaves in the District of Columbia were freed on April 16, 1862, and their owners were compensated.

On June 19, 1862, Congress prohibited slavery in all current and future United States territories (though not in the states), and President Lincoln quickly signed the legislation. By this act, they repudiated the 1857 opinion of the Supreme Court of the United States in the "Dred Scott Case" that Congress was powerless to regulate slavery in U.S. territories. This joint action by Congress and President Lincoln also rejected the notion of popular sovereignty that had been advanced by Stephen A. Douglas as a solution to the slavery controversy, while completing the effort first legislatively proposed by Thomas Jefferson in 1784 to confine slavery within the borders of existing states.

In July, Congress passed and Lincoln signed the Confiscation Act of 1862, containing provisions for court proceedings to liberate slaves held by convicted "rebels", or of slaves of rebels that had escaped to Union lines. The Act applied in cases of criminal convictions, to those who were slaves of "disloyal" masters, and to slaves in rebel territory that was captured by the Union forces. Unlike the first Confiscation Act, the second one explicitly said that all slaves covered under the law would be permanently freed, stating "all slaves of persons who shall hereafter be engaged in rebellion against the government of the United States, or who shall in any way give aid or comfort thereto, escaping from such persons and taking refuge within the lines of the army; and all slaves captured from such persons or deserted by them and coming under the control of the government of the United States; and all slaves of such person found on [or] being within any place occupied by rebel forces and afterwards occupied by the forces of the United States, shall be deemed captives of war, and shall be forever free of their servitude, and not again held as slaves." However, Lincoln's position continued to be that Congress lacked power to free all slaves within the borders of rebel held states, but Lincoln as commander in chief could do so if he deemed it a proper military measure, and that Lincoln had already drafted plans to do.

Abolitionists had long been urging Lincoln to free all slaves. In the summer of 1862, Republican editor Horace Greeley of the highly influential New York Tribune wrote a famous editorial entitled "The Prayer of Twenty Millions" demanding a more aggressive attack on the Confederacy and faster emancipation of the slaves: "On the face of this wide earth, Mr. President, there is not one ... intelligent champion of the Union cause who does not feel ... that the rebellion, if crushed tomorrow, would be renewed if slavery were left in full vigor and that every hour of deference to slavery is an hour of added and deepened peril to the Union." Lincoln responded in his from August 22, 1862, in terms of the limits imposed by his duty as president to save the Union:

Lincoln scholar Harold Holzer wrote in this context about Lincoln's letter: "Unknown to Greeley, Lincoln composed this after he had already drafted a preliminary Emancipation Proclamation, which he had determined to issue after the next Union military victory. Therefore, this letter, was in truth, an attempt to position the impending announcement in terms of saving the Union, not freeing slaves as a humanitarian gesture. It was one of Lincoln's most skillful public relations efforts, even if it has cast longstanding doubt on his sincerity as a liberator." Historian Richard Striner argues that "for years" Lincoln's letter has been misread as "Lincoln only wanted to save the Union." However, within the context of Lincoln's entire career and pronouncements on slavery this interpretation is wrong, according to Striner. Rather, Lincoln was softening the strong Northern white supremacist opposition to his imminent emancipation by tying it to the cause of the Union. This opposition would fight for the Union but not to end slavery, so Lincoln gave them the means and motivation to do both, at the same time. In his 2014 book, "Lincoln's Gamble", journalist and historian Todd Brewster asserted that Lincoln's desire to reassert the saving of the Union as his sole war goal was in fact crucial to his claim of legal authority for emancipation. Since slavery was protected by the Constitution, the only way that he could free the slaves was as a tactic of war—not as the mission itself. But that carried the risk that when the war ended, so would the justification for freeing the slaves. Late in 1862, Lincoln asked his Attorney General, Edward Bates, for an opinion as to whether slaves freed through a war-related proclamation of emancipation could be re-enslaved once the war was over. Bates had to work through the language of the Dred Scott decision to arrive at an answer, but he finally concluded that they could indeed remain free. Still, a complete end to slavery would require a constitutional amendment.

Conflicting advice, to free all slaves, or not free them at all, was presented to Lincoln in public and private. Thomas Nast, a cartoon artist during the Civil War and the late 1800s considered "Father of the American Cartoon", composed many works including a two-sided spread that showed the transition from slavery into civilization after President Lincoln signed the Proclamation. Nast believed in equal opportunity and equality for all people, including enslaved Africans or free blacks. A mass rally in Chicago on September 7, 1862, demanded an immediate and universal emancipation of slaves. A delegation headed by William W. Patton met the president at the White House on September 13. Lincoln had declared in peacetime that he had no constitutional authority to free the slaves. Even used as a war power, emancipation was a risky political act. Public opinion as a whole was against it. There would be strong opposition among Copperhead Democrats and an uncertain reaction from loyal border states. Delaware and Maryland already had a high percentage of free blacks: 91.2% and 49.7%, respectively, in 1860.

Lincoln first discussed the proclamation with his cabinet in July 1862. He drafted his "preliminary proclamation" and read it to Secretary of State William Seward, and Secretary of Navy Gideon Welles, on July 13. Seward and Welles were at first speechless, then Seward referred to possible anarchy throughout the South and resulting foreign intervention; Welles apparently said nothing. On July 22, Lincoln presented it to his entire cabinet as something he had determined to do and he asked their opinion on wording. Although Secretary of War Edwin Stanton supported it, Seward advised Lincoln to issue the proclamation after a major Union victory, or else it would appear as if the Union was giving "its last shriek of retreat".

In September 1862, the Battle of Antietam gave Lincoln the victory he needed to issue the Emancipation. In the battle, though the Union suffered heavier losses than the Confederates and General McClellan allowed the escape of Robert E. Lee's retreating troops, Union forces turned back a Confederate invasion of Maryland, eliminating more than a quarter of Lee's army in the process. On September 22, 1862, five days after Antietam occurred, and while living at the Soldier's Home, Lincoln called his cabinet into session and issued the Preliminary Emancipation Proclamation. According to Civil War historian James M. McPherson, Lincoln told Cabinet members that he had made a covenant with God, that if the Union drove the Confederacy out of Maryland, he would issue the Emancipation Proclamation. Lincoln had first shown an early draft of the proclamation to Vice President Hannibal Hamlin, an ardent abolitionist, who was more often kept in the dark on presidential decisions. The final proclamation was issued January 1, 1863. Although implicitly granted authority by Congress, Lincoln used his powers as Commander-in-Chief of the Army and Navy, "as a necessary war measure" as the basis of the proclamation, rather than the equivalent of a statute enacted by Congress or a constitutional amendment. Some days after issuing the final Proclamation, Lincoln wrote to Major General John McClernand: "After the commencement of hostilities I struggled nearly a year and a half to get along without touching the "institution"; and when finally I conditionally determined to touch it, I gave a hundred days fair notice of my purpose, to all the States and people, within which time they could have turned it wholly aside, by simply again becoming good citizens of the United States. They chose to disregard it, and I made the peremptory proclamation on what appeared to me to be a military necessity. And being made, it must stand."

Initially, the Emancipation Proclamation effectively freed only a small percentage of the slaves, those who were behind Union lines in areas not exempted. Most slaves were still behind Confederate lines or in exempted Union-occupied areas. Secretary of State William H. Seward commented, "We show our sympathy with slavery by emancipating slaves where we cannot reach them and holding them in bondage where we can set them free." Had any slave state ended its secession attempt before January 1, 1863, it could have kept slavery, at least temporarily. The Proclamation only gave the Lincoln Administration the legal basis to free the slaves in the areas of the South that were still in rebellion on January 1, 1863. It effectively destroyed slavery as the Union armies advanced south and conquered the entire Confederacy.

The Emancipation Proclamation also allowed for the enrollment of freed slaves into the United States military. During the war nearly 200,000 blacks, most of them ex-slaves, joined the Union Army. Their contributions gave the North additional manpower that was significant in winning the war. The Confederacy did not allow slaves in their army as soldiers until the last month before its defeat.

Though the counties of Virginia that were soon to form West Virginia were specifically exempted from the Proclamation (Jefferson County being the only exception), a condition of the state's admittance to the Union was that its constitution provide for the gradual abolition of slavery (an immediate emancipation of all slaves was also adopted there in early 1865). Slaves in the border states of Maryland and Missouri were also emancipated by separate state action before the Civil War ended. In Maryland, a new state constitution abolishing slavery in the state went into effect on November 1, 1864. The Union-occupied counties of eastern Virginia and parishes of Louisiana, which had been exempted from the Proclamation, both adopted state constitutions that abolished slavery in April 1864. In early 1865, Tennessee adopted an amendment to its constitution prohibiting slavery. Slaves in Kentucky and Delaware were not emancipated until the Thirteenth Amendment was ratified.

The Proclamation was issued in two parts. The first part, issued on September 22, 1862, was a preliminary announcement outlining the intent of the second part, which officially went into effect 100 days later on January 1, 1863, during the second year of the Civil War. It was Abraham Lincoln's declaration that all slaves would be permanently freed in all areas of the Confederacy that had not already returned to federal control by January 1863. The ten affected states were individually named in the second part (South Carolina, Mississippi, Florida, Alabama, Georgia, Louisiana, Texas, Virginia, Arkansas, North Carolina). Not included were the Union slave states of Maryland, Delaware, Missouri and Kentucky. Also not named was the state of Tennessee, in which a Union-controlled military government had already been set up, based in the capital, Nashville. Specific exemptions were stated for areas also under Union control on January 1, 1863, namely 48 counties that would soon become West Virginia, seven other named counties of Virginia including Berkeley and Hampshire counties, which were soon added to West Virginia, New Orleans and 13 named parishes nearby.

Union-occupied areas of the Confederate states where the proclamation was put into immediate effect by local commanders included Winchester, Virginia, Corinth, Mississippi, the Sea Islands along the coasts of the Carolinas and Georgia, Key West, Florida, and Port Royal, South Carolina.

It has been inaccurately claimed that the Emancipation Proclamation did not free a single slave; historian Lerone Bennett Jr. alleged that the proclamation was a hoax deliberately designed not to free any slaves. However, as a result of the Proclamation, many slaves were freed during the course of the war, beginning with the day it took effect; eyewitness accounts at places such as Hilton Head Island, South Carolina, and Port Royal, South Carolina record celebrations on January 1 as thousands of blacks were informed of their new legal status of freedom. Estimates of how many thousands of slaves were freed immediately by the Emancipation Proclamation are varied. One contemporary estimate put the 'contraband' population of Union-occupied North Carolina at 10,000, and the Sea Islands of South Carolina also had a substantial population. Those 20,000 slaves were freed immediately by the Emancipation Proclamation." This Union-occupied zone where freedom began at once included parts of eastern North Carolina, the Mississippi Valley, northern Alabama, the Shenandoah Valley of Virginia, a large part of Arkansas, and the Sea Islands of Georgia and South Carolina. Although some counties of Union-occupied Virginia were exempted from the Proclamation, the lower Shenandoah Valley, and the area around Alexandria were covered. Emancipation was immediately enforced as Union soldiers advanced into the Confederacy. Slaves fled their masters and were often assisted by Union soldiers.

Booker T. Washington, as a boy of 9 in Virginia, remembered the day in early 1865:
The Proclamation represented a shift in the war objectives of the North—reuniting the nation was no longer the only goal. It represented a major step toward the ultimate abolition of slavery in the United States and a "new birth of freedom".

Runaway slaves who had escaped to Union lines had previously been held by the Union Army as "contraband of war" under the Confiscation Acts; when the proclamation took effect, they were told at midnight that they were free to leave. The Sea Islands off the coast of Georgia had been occupied by the Union Navy earlier in the war. The whites had fled to the mainland while the blacks stayed. An early program of Reconstruction was set up for the former slaves, including schools and training. Naval officers read the proclamation and told them they were free.

Slaves had been part of the "engine of war" for the Confederacy. They produced and prepared food; sewed uniforms; repaired railways; worked on farms and in factories, shipping yards, and mines; built fortifications; and served as hospital workers and common laborers. News of the Proclamation spread rapidly by word of mouth, arousing hopes of freedom, creating general confusion, and encouraging thousands to escape to Union lines. George Washington Albright, a teenage slave in Mississippi, recalled that like many of his fellow slaves, his father escaped to join Union forces. According to Albright, plantation owners tried to keep the Proclamation from slaves but news of it came through the "grapevine". The young slave became a "runner" for an informal group they called the "4Ls" ("Lincoln's Legal Loyal League") bringing news of the proclamation to secret slave meetings at plantations throughout the region.

Robert E. Lee saw the Emancipation Proclamation as a way for the Union to bolster the number of soldiers it could place on the field, making it imperative for the Confederacy to increase their own numbers. Writing on the matter after the sack of Fredericksburg, Lee wrote "In view of the vast increase of the forces of the enemy, of the savage and brutal policy he has proclaimed, which leaves us no alternative but success or degradation worse than death, if we would save the honor of our families from pollution, our social system from destruction, let every effort be made, every means be employed, to fill and maintain the ranks of our armies, until God, in his mercy, shall bless us with the establishment of our independence." Lee's request for a drastic increase of troops would go unfulfilled.

The Proclamation was immediately denounced by Copperhead Democrats who opposed the war and advocated restoring the union by allowing slavery. Horatio Seymour, while running for the governorship of New York, cast the Emancipation Proclamation as a call for slaves to commit extreme acts of violence on all white southerners, saying it was "a proposal for the butchery of women and children, for scenes of lust and rapine, and of arson and murder, which would invoke the interference of civilized Europe". The Copperheads also saw the Proclamation as an unconstitutional abuse of presidential power. Editor Henry A. Reeves wrote in Greenport's "Republican Watchman" that "In the name of freedom of Negroes, [the proclamation] imperils the liberty of white men; to test a utopian theory of equality of races which Nature, History and Experience alike condemn as monstrous, it overturns the Constitution and Civil Laws and sets up Military Usurpation in their Stead."

Racism remained pervasive on both sides of the conflict and many in the North supported the war only as an effort to force the South to stay in the Union. The promises of many Republican politicians that the war was to restore the Union and not about black rights or ending slavery, were now declared lies by their opponents citing the Proclamation. Copperhead David Allen spoke to a rally in Columbiana, Ohio, stating, "I have told you that this war is carried on for the Negro. There is the proclamation of the President of the United States. Now fellow Democrats I ask you if you are going to be forced into a war against your Brithren of the Southern States for the Negro. I answer No!" The Copperheads saw the Proclamation as irrefutable proof of their position and the beginning of a political rise for their members; in Connecticut, H. B. Whiting wrote that the truth was now plain even to "those stupid thick-headed persons who persisted in thinking that the President was a conservative man and that the war was for the restoration of the Union under the Constitution".

War Democrats who rejected the Copperhead position within their party, found themselves in a quandary. While throughout the war they had continued to espouse the racist positions of their party and their disdain of the concerns of slaves, they did see the Proclamation as a viable military tool against the South, and worried that opposing it might demoralize troops in the Union army. The question would continue to trouble them and eventually lead to a split within their party as the war progressed.

Lincoln further alienated many in the Union two days after issuing the preliminary copy of the Emancipation Proclamation by suspending habeas corpus. His opponents linked these two actions in their claims that he was becoming a despot. In light of this and a lack of military success for the Union armies, many War Democrat voters who had previously supported Lincoln turned against him and joined the Copperheads in the off-year elections held in October and November.

In the 1862 elections, the Democrats gained 28 seats in the House as well as the governorship of New York. Lincoln's friend Orville Hickman Browning told the president that the Proclamation and the suspension of habeas corpus had been "disastrous" for his party by handing the Democrats so many weapons. Lincoln made no response. Copperhead William Javis of Connecticut pronounced the election the "beginning of the end of the utter downfall of Abolitionism in the United States".

Historians James M. McPherson and Allan Nevins state that though the results looked very troubling, they could be seen favorably by Lincoln; his opponents did well only in their historic strongholds and "at the national level their gains in the House were the smallest of any minority party's in an off-year election in nearly a generation. Michigan, California, and Iowa all went Republican... Moreover, the Republicans picked up five seats in the Senate." McPherson states "If the election was in any sense a referendum on emancipation and on Lincoln's conduct of the war, a majority of Northern voters endorsed these policies."

The initial Confederate response was one of expected outrage. The Proclamation was seen as vindication for the rebellion, and proof that Lincoln would have abolished slavery even if the states had remained in the Union. In an August 1863 letter to President Lincoln, U.S. Army general Ulysses S. Grant observed that the Proclamation, combined with the usage of black soldiers by the U.S. Army, profoundly angered the Confederacy, saying that "the emancipation of the Negro, is the heaviest blow yet given the Confederacy. The South rave a great deal about it and profess to be very angry." A few months after the Proclamation took effect, the Confederacy passed a law in May 1863 demanding "full and ample retaliation" against the U.S. for such measures. The Confederacy stated that the black U.S. soldiers captured while fighting against the Confederacy would be tried as slave insurrectionists in civil courts—a capital offense with automatic sentence of death. Less than a year after the law's passage, the Confederates massacred black U.S. soldiers at Fort Pillow.

Confederate General Robert Lee called the Proclamation a "savage and brutal policy he has proclaimed, which leaves us no alternative but success or degradation worse than death"

However, some Confederates welcomed the Proclamation, as they believed it would strengthen pro-slavery sentiment in the Confederacy and, thus, lead to greater enlistment of white men into the Confederate army. According to one Confederate man from Kentucky, "The Proclamation is worth three hundred thousand soldiers to our Government at least... It shows exactly what this war was brought about for and the intention of its damnable authors." Even some Union soldiers concurred with this view and expressed reservations about the Proclamation, not on principle, but rather because they were afraid it would increase the Confederacy's determination to fight on and maintain slavery. One Union soldier from New York stated worryingly after the Proclamation's passage, "I know enough of the Southern spirit that I think they will fight for the institution of slavery even to extermination."

As a result of the Proclamation, the price of slaves in the Confederacy increased in the months after its issuance, with one Confederate from South Carolina opining in 1865 that "now is the time for Uncle to buy some negro women and children."

As Lincoln had hoped, the proclamation turned foreign popular opinion in favor of the Union by gaining the support of anti-slavery countries and countries that had already abolished slavery (especially the developed countries in Europe such as the United Kingdom or France). This shift ended the Confederacy's hopes of gaining official recognition.

Since the Emancipation Proclamation made the eradication of slavery an explicit Union war goal, it linked support for the South to support for slavery. Public opinion in Britain would not tolerate support for slavery. As Henry Adams noted, "The Emancipation Proclamation has done more for us than all our former victories and all our diplomacy." In Italy, Giuseppe Garibaldi hailed Lincoln as "the heir of the aspirations of John Brown". On August 6, 1863, Garibaldi wrote to Lincoln: "Posterity will call you the great emancipator, a more enviable title than any crown could be, and greater than any merely mundane treasure".

Mayor Abel Haywood, a representative for workers from Manchester, England, wrote to Lincoln saying, "We joyfully honor you for many decisive steps toward practically exemplifying your belief in the words of your great founders: 'All men are created free and equal.'" The Emancipation Proclamation served to ease tensions with Europe over the North's conduct of the war, and combined with the recent failed Southern offensive at Antietam, to remove any practical chance for the Confederacy to receive foreign support in the war.

Lincoln's Gettysburg Address in November 1863 made indirect reference to the Proclamation and the ending of slavery as a war goal with the phrase "new birth of freedom". The Proclamation solidified Lincoln's support among the rapidly growing abolitionist element of the Republican Party and ensured that they would not block his re-nomination in 1864.

In December 1863, Lincoln issued his "Proclamation of Amnesty and Reconstruction", which dealt with the ways the rebel states could reconcile with the Union. Key provisions required that the states accept the "Emancipation Proclamation" and thus the freedom of their slaves, and accept the Confiscation Acts, as well as the Act banning of slavery in United States territories.

Near the end of the war, abolitionists were concerned that the Emancipation Proclamation would be construed solely as a war measure, Lincoln's original intent, and would no longer apply once fighting ended. They were also increasingly anxious to secure the freedom of all slaves, not just those freed by the Emancipation Proclamation. Thus pressed, Lincoln staked a large part of his 1864 presidential campaign on a constitutional amendment to abolish slavery uniformly throughout the United States. Lincoln's campaign was bolstered by separate votes in both Maryland and Missouri to abolish slavery in those states. Maryland's new constitution abolishing slavery took effect in November 1864. Slavery in Missouri was ended by executive proclamation of its governor, Thomas C. Fletcher, on January 11, 1865.

Winning re-election, Lincoln pressed the lame duck 38th Congress to pass the proposed amendment immediately rather than wait for the incoming 39th Congress to convene. In January 1865, Congress sent to the state legislatures for ratification what became the Thirteenth Amendment, banning slavery in all U.S. states and territories. The amendment was ratified by the legislatures of enough states by December 6, 1865, and proclaimed 12 days later. There were about 40,000 slaves in Kentucky and 1,000 in Delaware who were liberated then.

As the years went on and American life continued to be deeply unfair towards blacks, cynicism towards Lincoln and the Emancipation Proclamation increased. Perhaps the strongest attack was Lerone Bennett's "" (2000), which claimed that Lincoln was a white supremacist who issued the Emancipation Proclamation in lieu of the real racial reforms for which radical abolitionists pushed. In his "Lincoln's Emancipation Proclamation", Allen C. Guelzo noted the professional historians' lack of substantial respect for the document, since it has been the subject of few major scholarly studies. He argued that Lincoln was the US's "last Enlightenment politician" and as such was dedicated to removing slavery strictly within the bounds of law.

Other historians have given more credit to Lincoln for what he accomplished within the tensions of his cabinet and a society at war, for his own growth in political and moral stature, and for the promise he held out to the slaves. More might have been accomplished if he had not been assassinated. As Eric Foner wrote:
Lincoln was not an abolitionist or Radical Republican, a point Bennett reiterates innumerable times. He did not favor immediate abolition before the war, and held racist views typical of his time. But he was also a man of deep convictions when it came to slavery, and during the Civil War displayed a remarkable capacity for moral and political growth.

Kal Ashraf wrote:
Perhaps in rejecting the critical dualism–Lincoln as individual emancipator pitted against collective self-emancipators–there is an opportunity to recognise the greater persuasiveness of the combination. In a sense, yes: a racist, flawed Lincoln did something heroic, and not in lieu of collective participation, but next to, and enabled, by it. To venerate a singular –Great Emancipator' may be as reductive as dismissing the significance of Lincoln's actions. Who he was as a man, no one of us can ever really know. So it is that the version of Lincoln we keep is also the version we make.

Dr. Martin Luther King Jr. made many references to the Emancipation Proclamation during the civil rights movement. These include a speech made at an observance of the hundredth anniversary of the issuing of the Proclamation made in New York City on September 12, 1962 where he placed it alongside the Declaration of Independence as an "imperishable" contribution to civilization, and "All tyrants, past, present and future, are powerless to bury the truths in these declarations". He lamented that despite a history where the United States "proudly professed the basic principles inherent in both documents", it "sadly practiced the antithesis of these principles". He concluded "There is but one way to commemorate the Emancipation Proclamation. That is to make its declarations of freedom real; to reach back to the origins of our nation when our message of equality electrified an unfree world, and reaffirm democracy by deeds as bold and daring as the issuance of the Emancipation Proclamation."

King's most famous invocation of the Emancipation Proclamation was in a speech from the steps of the Lincoln Memorial at the 1963 March on Washington for Jobs and Freedom (often referred to as the "I Have a Dream" speech). King began the speech saying "Five score years ago, a great American, in whose symbolic shadow we stand, signed the Emancipation Proclamation. This momentous decree came as a great beacon light of hope to millions of Negro slaves who had been seared in the flames of withering injustice. It came as a joyous daybreak to end the long night of captivity. But one hundred years later, we must face the tragic fact that the Negro is still not free. One hundred years later, the life of the Negro is still sadly crippled by the manacles of segregation and the chains of discrimination."

In the early 1960s, Dr. Martin Luther King Jr. and his associates developed a strategy to call on President John F. Kennedy to bypass a Southern segregationist opposition in the Congress by issuing an executive order to put an end to segregation. This envisioned document was referred to as the "Second Emancipation Proclamation".

On June 11, 1963, President Kennedy appeared on national television to address the issue of civil rights. Kennedy, who had been routinely criticized as timid by some of the leaders of the civil rights movement, told Americans that two black students had been peacefully enrolled in the University of Alabama with the aid of the National Guard despite the opposition of Governor George Wallace.

John Kennedy called it a "moral issue" Invoking the centennial of the Emancipation Proclamation he said

In the same speech, Kennedy announced he would introduce comprehensive civil rights legislation to the United States Congress which he did a week later (he continued to push for its passage until his assassination in November 1963). Historian Peniel E. Joseph holds Lyndon Johnson's ability to get that bill, the Civil Rights Act of 1964, passed on July 2, 1964 was aided by "the moral forcefulness of the June 11 speech" which turned "the narrative of civil rights from a regional issue into a national story promoting racial equality and democratic renewal".

During the civil rights movement of the 1960s, Lyndon B. Johnson invoked the Emancipation Proclamation holding it up as a promise yet to be fully implemented.

As Vice President while speaking from Gettysburg on May 30, 1963 (Memorial Day), at the centennial of the Emancipation Proclamation, Johnson connected it directly with the ongoing civil rights struggles of the time saying "One hundred years ago, the slave was freed. One hundred years later, the Negro remains in bondage to the color of his skin... In this hour, it is not our respective races which are at stake—it is our nation. Let those who care for their country come forward, North and South, white and Negro, to lead the way through this moment of challenge and decision... Until justice is blind to color, until education is unaware of race, until opportunity is unconcerned with color of men's skins, emancipation will be a proclamation but not a fact. To the extent that the proclamation of emancipation is not fulfilled in fact, to that extent we shall have fallen short of assuring freedom to the free."

As president, Johnson again invoked the proclamation in a speech presenting the Voting Rights Act at a joint session of Congress on Monday, March 15, 1965. This was one week after violence had been inflicted on peaceful civil rights marchers during the Selma to Montgomery marches. Johnson said "... it's not just Negroes, but really it's all of us, who must overcome the crippling legacy of bigotry and injustice. And we shall overcome. As a man whose roots go deeply into Southern soil, I know how agonizing racial feelings are. I know how difficult it is to reshape the attitudes and the structure of our society. But a century has passed—more than 100 years—since the Negro was freed. And he is not fully free tonight. It was more than 100 years ago that Abraham Lincoln—a great President of another party—signed the Emancipation Proclamation. But emancipation is a proclamation and not a fact. A century has passed—more than 100 years—since equality was promised, and yet the Negro is not equal. A century has passed since the day of promise, and the promise is unkept. The time of justice has now come, and I tell you that I believe sincerely that no force can hold it back. It is right in the eyes of man and God that it should come, and when it does, I think that day will brighten the lives of every American."

In episode 86 of "The Andy Griffith Show", Andy asks Barney to explain the Emancipation Proclamation to Opie who is struggling with history at school. Barney brags about his history expertise, yet it is apparent he cannot answer Andy's question. He finally becomes frustrated and explains it is a proclamation for certain people who wanted emancipation.

In "", Chef asks the military commander if he has "ever heard of the Emancipation Proclamation?" To which the military commander replies: "I don't listen to hip-hop."

The Emancipation Proclamation is celebrated around the world including on stamps of nations such as the Republic of Togo. The United States commemorative was issued on August 16, 1963, the opening day of the Century of Negro Progress Exposition in Chicago, Illinois. Designed by Georg Olden, an initial printing of 120 million stamps was authorized.




</doc>
<doc id="9516" url="https://en.wikipedia.org/wiki?curid=9516" title="Erwin Rommel">
Erwin Rommel

Johannes Erwin Eugen Rommel (15 November 1891 – 14 October 1944) was a German general and military theorist. Popularly known as the Desert Fox, he served as field marshal in the Wehrmacht (armed forces) of Nazi Germany during World War II, as well as serving in the Reichswehr of the Weimar Republic, and the army of Imperial Germany.

Rommel was a highly decorated officer in World War I and was awarded the "Pour le Mérite" for his actions on the Italian Front. In 1937 he published his classic book on military tactics, "Infantry Attacks", drawing on his experiences in that war. In World War II, he distinguished himself as the commander of the 7th Panzer Division during the 1940 invasion of France. His leadership of German and Italian forces in the North African campaign established his reputation as one of the ablest tank commanders of the war, and earned him the nickname "der Wüstenfuchs", "the Desert Fox". Among his British adversaries he had a reputation for chivalry, and his phrase "war without hate" has been used to describe the North African campaign. A number of historians have since rejected the phrase as myth and uncovered numerous examples of war crimes and abuses both towards enemy soldiers and native populations in Africa during the conflict. Other historians note that there is no clear evidence Rommel was involved or aware of these crimes (although Caron and Müllner point out that his military successes allowed these crimes to happen) with some pointing out that the war in the desert, as fought by Rommel and his opponents, still came as close to a clean fight as there was in World War II. He later commanded the German forces opposing the Allied cross-channel invasion of Normandy in June 1944. A number of historians connect Rommel himself with war crimes, although this is not the opinion of the majority.

With the Nazis gaining power in Germany, Rommel gradually came to accept the new regime, with historians giving different accounts on the specific period and his motivations. He is generally considered a supporter and close friend of Adolf Hitler, at least until near the end of the war, if not necessarily sympathetic to the party and the paramilitary forces associated with it. His stance towards Nazi ideology and his level of knowledge of the Holocaust remain matters of debate among scholars. In 1944, Rommel was implicated in the 20 July plot to assassinate Hitler. Because of Rommel's status as a national hero, Hitler desired to eliminate him quietly instead of immediately executing him, as many other plotters were. Rommel was given a choice between committing suicide, in return for assurances that his reputation would remain intact and that his family would not be persecuted following his death, or facing a trial that would result in his disgrace and execution; he chose the former and committed suicide using a cyanide pill. Rommel was given a state funeral, and it was announced that he had succumbed to his injuries from the strafing of his staff car in Normandy.

Rommel has become a larger-than-life figure in both Allied and Nazi propaganda, and in postwar popular culture, with numerous authors considering him an apolitical, brilliant commander and a victim of the Third Reich, although this assessment is contested by other authors as the Rommel myth. Rommel's reputation for conducting a clean war was used in the interest of the West German rearmament and reconciliation between the former enemies – the United Kingdom and the United States on one side and the new Federal Republic of Germany on the other. Several of Rommel's former subordinates, notably his chief of staff Hans Speidel, played key roles in German rearmament and integration into NATO in the postwar era. The German Army's largest military base, the Field Marshal Rommel Barracks, Augustdorf, is named in his honour.

Rommel was born on 15 November 1891, in Southern Germany at Heidenheim, from Ulm, in the Kingdom of Württemberg, then part of the German Empire. He was the third of five children to Erwin Rommel Senior (1860–1913) and his wife Helene von Lutz, whose father, Karl von Luz, headed the local government council. As a young man, Rommel's father had been an artillery lieutenant. Rommel had one older sister who was an art teacher and his favourite sibling, one older brother named Manfred who died in infancy and two younger brothers, of whom one became a successful dentist and the other an opera singer.

At age 18 Rommel joined the local 124th Württemberg Infantry Regiment as a "Fähnrich" (ensign), in 1910, studying at the Officer Cadet School in Danzig. He graduated in November 1911 and was commissioned as a lieutenant in January 1912 and was assigned to the 124th Infantry in Weingarten. He was posted to Ulm in March 1914 to the 46th Field Artillery Regiment, XIII (Royal Württemberg) Corps, as a battery commander. He returned to the 124th when war was declared. While at Cadet School, Rommel met his future wife, 17-year-old Lucia (Lucie) Maria Mollin (1894–1971), of Polish and Italian descent.

During World War I, Rommel fought in France as well as in the Romanian (notably at the Second Battle of the Jiu Valley) and Italian campaigns. He successfully employed the tactics of penetrating enemy lines with heavy covering fire coupled with rapid advances, as well as moving forward rapidly to a flanking position to arrive at the rear of hostile positions, to achieve tactical surprise. His first combat experience was on 22 August 1914 as a platoon commander near Verdun, when – catching a French garrison unprepared – Rommel and three men opened fire on them without ordering the rest of his platoon forward. The armies continued to skirmish in open engagements throughout September, as the static trench warfare typical of the First World War was still in the future. For his actions in September 1914 and January 1915, Rommel was awarded the Iron Cross, Second Class. Rommel was promoted to "Oberleutnant" (first lieutenant) and transferred to the newly created Royal Wurttemberg Mountain Battalion of the "Alpenkorps" in September 1915, as a company commander. In November 1916 in Danzig, Rommel and Lucia married.

In August 1917, his unit was involved in the battle for Mount Cosna, a heavily fortified objective on the border between Hungary and Romania, which they took after two weeks of difficult uphill fighting. The Mountain Battalion was next assigned to the Isonzo front, in a mountainous area in Italy. The offensive, known as the Battle of Caporetto, began on 24 October 1917. Rommel's battalion, consisting of three rifle companies and a machine gun unit, was part of an attempt to take enemy positions on three mountains: Kolovrat, Matajur, and Stol. In two and a half days, from 25 to 27 October, Rommel and his 150 men captured 81 guns and 9,000 men (including 150 officers), at the loss of six dead and 30 wounded. Rommel achieved this remarkable success by taking advantage of the terrain to outflank the Italian forces, attacking from unexpected directions or behind enemy lines, and taking the initiative to attack when he had orders to the contrary. In one instance, the Italian forces, taken by surprise and believing that their lines had collapsed, surrendered after a brief firefight. In this battle, Rommel helped pioneer infiltration tactics, a new form of maneuver warfare just being adopted by German armies, and later by foreign armies, and described by some as Blitzkrieg without tanks. He played no role in the early adoption of Blitzkrieg in World War II though. Acting as advance guard in the capture of Longarone on 9 November, Rommel again decided to attack with a much smaller force. Convinced that they were surrounded by an entire German division, the 1st Italian Infantry Division – 10,000 men – surrendered to Rommel. For this and his actions at Matajur, he received the order of Pour le Mérite.

In January 1918, Rommel was promoted to "Hauptmann" (captain) and assigned to a staff position in the 64th Army Corps, where he served for the remainder of the war.

Rommel remained with the 124th Regiment until October 1920. The regiment was involved in quelling riots and civil disturbances that were occurring throughout Germany at this time. Wherever possible Rommel avoided the use of force in these confrontations. In 1919 he was briefly sent to Friedrichshafen on Lake Constance, where he restored order by "sheer force of personality" in the 32nd Internal Security Company, which was composed of rebellious and pro-communist sailors. He decided against storming the nearby city of Lindau, which had been taken by revolutionary communists. Instead, Rommel negotiated with the city council and managed to return it to the legitimate government through diplomatic means. This was followed by his defence of Schwäbisch Gmünd, again bloodless. He was then posted to the Ruhr, where a red army was responsible for fomenting unrest. Historian praises Rommel as a coolheaded and moderate mind, exceptional amid the many takeovers of revolutionary cities by regular and irregular units and the associated massive violence.

According to Reuth, this period left an indelible impression on Rommel's mind that "Everyone in this Republic was fighting each other," along with direct experience of people attempting to convert Germany into a socialist republic on Soviet lines. There are similarities with Hitler's experiences: like Rommel, Hitler had known the solidarity of trench warfare and had then participated in the Reichswehr's suppression of the First and Second Bavarian Soviet Republics. The need for national unity thus became a decisive legacy of the first World War. Brighton notes that while both believed in the Stab-in-the-back myth, Rommel was able to succeed using peaceful methods because he saw the problem in empty stomachs rather than in Judeo-Bolshevism – which right-wing soldiers such as Hitler blamed for the chaos in Germany.

On 1 October 1920 Rommel was appointed to a company command with the 13th Infantry Regiment in Stuttgart, a post he held for the next nine years. He was then assigned as an instructor at the Dresden Infantry School from 1929 to 1933, and during this time was promoted to major, in April 1932. While at Dresden, he wrote a manual on infantry training, published in 1934. In October 1933 he was promoted to "Oberstleutnant" (lieutenant colonel) and given his next command, the 3rd "Jäger" Battalion, 17th Infantry Regiment, stationed at Goslar. Here he first met Hitler, who inspected his troops on 30 September 1934. In September 1935 Rommel was moved to the War Academy at Potsdam as an instructor, for the next three years. His book "Infanterie greift an" ("Infantry Attacks"), a description of his wartime experiences along with his analysis, was published in 1937. It became a bestseller, which, according to Scheck, later "enormously influenced" many armies of the world; Adolf Hitler was one of many people who owned a copy.

Hearing of Rommel's reputation as an outstanding military instructor, in February 1937 Hitler assigned him as the War Ministry liaison officer to the Hitler Youth in charge of military training. Here he clashed with Baldur von Schirach, the Hitler Youth leader, over the training that the boys should receive. Trying to fulfill a mission assigned to him by the Ministry of War, Rommel had twice proposed a plan that would have effectively subordinated Hitler Youth to the army, removing it from NSDAP control. That went against Schirach's express wishes. Schirach appealed directly to Hitler; consequently, Rommel was quietly removed from the project in 1938. He had been promoted to "Oberst" (colonel), on 1 August 1937, and in 1938 he was appointed commandant of the Theresian Military Academy at Wiener Neustadt. In October 1938 Hitler specially requested that Rommel be seconded to command the "Führerbegleitbatallion" (his escort battalion). This unit accompanied Hitler whenever he travelled outside of Germany. During this period Rommel indulged his interest in engineering and mechanics by learning about the inner workings and maintenance of internal combustion engines and heavy machine guns. He memorized logarithm tables in his spare time and enjoyed skiing and other outdoor sports. Ian F. Beckett writes that by 1938 Rommel drifted towards uncritical acceptance of Nazi regime quoting Rommel's letter to his wife in which he stated "The German Wehrmacht is the sword of the new German world view" as a reaction to speech by Hitler. During his visit to Switzerland in 1938 he reported that Swiss soldiers he met showed "remarkable understanding of our Jewish problem". Butler comments that he did share the view (popular in Germany and many European countries during that time) that the Jews were loyal to themselves as a people more than their nations. Despite this, other pieces of evidence show that he considered the Nazi racial ideologies to be rubbish. Samuel Mitcham states that "Yet after years of propaganda even Rommel was infected with the anti-Semitic virus, at least to a minor degree. ... Rommel did not approve of Jewish clannishness, and he was suspicious of Jews for the wealth they had acquired", but was more focused on his family and career than this issue Searle comments that Rommel knew the official stand of the regime, but in this case, the phrase was ambiguous and there is no evidence after or before this event that he ever sympathised with the antisemitism of the Nazi movement. Rommel's son Manfred Rommel stated in documentary "The Real Rommel", published in 2001 by Channel 4 that his father would "look the other way" when faced with anti-Jewish violence on the streets. According to the documentary Rommel also requested proof of "Aryan descent" from the Italian boyfriend of his illegitimate daughter Gertrud. According to Remy, during the time Rommel was posted in Goslar, he repeatedly clashed with the SA who terrorized the Jews and dissident Goslar citizens. After the Röhm Purge, he mistakenly believed that the worst was now over, although there were still restrictions on Jewish businesses and agitation against their community. According to Remy, Manfred Rommel recounts that his father knew about and privately disagreed with the government's anti-semitism, but by this time, he had not actively campaigned for them. Uri Avnery notes that he protected the Jews in his district even as a low-ranking officer. Manfred Rommel tells the Stuttgarter Nachrichten that their family lived in isolated military lands but knew about the discrimination against the Jews outside. They could not imagine the enormity of the impending atrocities, about which they only knew much later.

Rommel at this time supported Hitler and his nationalist rhetoric stating among other things about Hitler "he was called upon by God" and that "[he speaks] like a prophet"

Rommel was promoted to "Generalmajor" on 23 August 1939 and assigned as commander of the "Führerbegleitbatallion", tasked with guarding Hitler and his field headquarters during the invasion of Poland, which began on 1 September. According to Remy, Rommel's private letters at this time show that he did not understand Hitler's true nature and intentions, as he quickly went from predicting a swift peaceful settlement of tensions to approving Hitler's reaction ("bombs will be retaliated with bombs") to the Gleiwitz incident (a false flag operation staged by Hitler and used as a pretext for the invasion). Hitler took a personal interest in the campaign, often moving close to the front in the "Führersonderzug" (headquarters train). Rommel attended Hitler's daily war briefings and accompanied him everywhere, making use of the opportunity to observe first-hand the use of tanks and other motorized units. On 26 September Rommel returned to Berlin to set up a new headquarters for his unit in the Reich Chancellery. Rommel returned briefly to occupied Warsaw on 5 October to prepare for the German victory parade. In a letter to his wife he claimed after several days of blockade of movement and exposure to danger in the ruined city, the inhabitants were now rescued.

Following the campaign in Poland, Rommel began lobbying for command of one of Germany's panzer divisions, of which there were then only ten. Rommel's successes in World War I were based on surprise and manoeuvre, two elements for which the new panzer units were ideally suited. Rommel received a promotion to a general's rank from Hitler ahead of more senior officers. Rommel obtained the command he aspired to, despite having been earlier turned down by the army's personnel office, which had offered him command of a mountain division instead. According to Caddick-Adams, he was backed by Hitler, the influential Fourteenth Army commander Wilhelm List (a fellow Württemberger middle-class "military outsider") and likely Guderian as well.

Going against military protocol, this promotion added to Rommel's growing reputation as one of Hitler's favoured commanders, although his later outstanding leadership in France quelled complaints about his self-promotion and political scheming. The 7th Panzer Division had recently been converted to an armoured division consisting of 218 tanks in three battalions (thus, one tank regiment, instead of the two assigned to a standard panzer division), with two rifle regiments, a motorcycle battalion, an engineer battalion, and an anti-tank battalion. Upon taking command on 10 February 1940, Rommel quickly set his unit to practising the maneuvers they would need in the upcoming campaign.

The invasion began on 10 May 1940. By the third day Rommel and the advance elements of his division, together with a detachment of the 5th Panzer Division under Colonel Hermann Werner, had reached the River Meuse, where they found the bridges had already been destroyed (Guderian and Reinhardt reached the river on the same day). Rommel was active in the forward areas, directing the efforts to make a crossing, which were initially unsuccessful because of suppressive fire by the French on the other side of the river. Rommel brought up tanks and flak units to provide counter-fire and had nearby houses set on fire to create a smokescreen. He sent infantry across in rubber boats, appropriated the bridging tackle of the 5th Panzer Division, personally grabbed a light machine gun to fight off a French counterattack supported by tanks, and went into the water himself, encouraging the sappers and helping lash together the pontoons. By 16 May Rommel reached Avesnes, and contravening all orders and doctrine, he pressed on to Cateau. That night, the French II Army Corps was shattered and on 17 May, Rommel's forces took 10,000 prisoners, losing 36 men in the process. He was surprised to find out only his vanguard had followed his tempestuous surge. The High Command and Hitler had been extremely nervous about his disappearance, although they awarded him the Knight's Cross. Rommel's (and Guderian's) successes and the new possibilities offered by the new tank arm were welcomed by a small number of generals, but worried and paralysed the rest.

On 20 May, Rommel reached Arras. General Hermann Hoth received orders that the town should be bypassed and its British garrison thus isolated. He ordered the 5th Panzer Division to move to the west and the 7th Panzer Division to the east, flanked by the SS Division "Totenkopf". The following day, the British launched a counterattack, meeting the SS "Totenkopf" with two infantry battalions supported by heavily armoured Matilda Mk I and Matilda II tanks in the Battle of Arras. The German 37 mm anti-tank gun proved ineffective against the heavily armoured Matildas. The 25th Panzer Regiment and a battery of anti-aircraft guns were called in to support, and the British withdrew.

On 24 May, Field Marshal von Rundstedt and Field Marshal von Kluge issued a halt order, which Hitler approved. The reason for this decision is still a matter of debate. The halt order was lifted on 26 May. 7th Panzer continued its advance, reaching Lille on 27 May. For the assault, Hoth placed the 5th Panzer Division's Panzer Brigade under Rommel's command. The Siege of Lille continued until 31 May, when the French garrison of 40,000 men surrendered. 7th Panzer was given six days' leave, during which Rommel was summoned to Berlin to meet with Hitler. He was the only divisional commander present at the planning session for "Fall Rot" (Case Red), the second phase of the invasion of France. By this time the evacuation of the BEF was complete; over 338,000 Allied troops had been evacuated across the Channel, though they had to leave behind all their heavy equipment and vehicles.

Rommel, resuming his advance on 5 June, drove for the River Seine to secure the bridges near Rouen. Advancing in two days, the division reached Rouen to find it defended by three French tanks; after they managed to destroy a number of German tanks before being taken out; the German force enraged by this resistance forbade fire brigade access to the burning district of old Normal capital, and as a result most of the historic quarter was reduced to ashes According to David Fraser, Rommel instructed the German artillery to bombard the city as a "fire demonstration" According to one witness report the smoke from burning Rouen was intense enough that it reached Paris Daniel Allen Butler states that the bridges to the city were already destroyed. Historians note that after the fall of the city both French civilians of Black African descent and colonial troops were executed on 9 June, without mentioning the unit of the perpetrators. The number of Black civilians and prisoners mass murdered is estimated at around 100 According to Butler and Showalter, Rouen fell to the 5th Panzer Division, while Rommel advanced from the Seine towards the Channel. On 10 June, Rommel reached the coast near Dieppe, sending Hoth the message "Bin an der Küste" or "Am on the coast". On 17 June, 7th Panzer was ordered to advance on Cherbourg, where additional British evacuations were under way. The division advanced in 24 hours, and after two days of shelling, the French garrison surrendered on 19 June. The speed and surprise that it was consistently able to achieve, to the point at which both the enemy and the "Oberkommando des Heeres" (OKH; German High Command) at times lost track of its whereabouts, earned the 7th Panzers the nickname "Gespensterdivision" (Ghost Division).

After the armistice with the French was signed on 22 June, the division was placed in reserve, being sent first to the Somme and then to Bordeaux to re-equip and prepare for "Unternehmen Seelöwe" (Operation Sea Lion), the planned invasion of Britain. This invasion was later cancelled, as Germany was not able to acquire the air superiority needed for a successful outcome, while the "Kriegsmarine" was massively outnumbered by the Royal Navy.

In France, Rommel ordered the execution of one French officer who refused three times to cooperate when being taken prisoner; there are disputes as to whether this execution was justified. Bewley remarks that the shooting of a prisoner who does not behave as a prisoner is a legal option; however, this act was brutal because the officer did not have a gun. Caddick-Adams comments that this would make Rommel a war criminal condemned by his own hand, and that other authors overlook this episode. Butler notes that the officer refused to surrender three times and thus died in a courageous but foolhardy way. French historian Petitfrère remarks that Rommel was in a hurry and had no time for useless palavers, although this act was still debatable. Telp remarks that, "For all his craftiness, Rommel was chivalrous by nature and not prone to order or condone acts of needless violence ... He treated prisoners of war with consideration. On one occasion, he was forced to order the shooting of a French lieutenant-colonel for refusing to obey his captors." Scheck says, "Although there is no evidence incriminating Rommel himself, his unit did fight in areas where German massacres of black French prisoners of war were extremely common in June 1940."

There are reports that during the fighting in France, Rommel's 7th Panzer Division committed atrocities against surrendering French troops and captured prisoners of war. The atrocities, according to Martin S. Alexander, included the murder of 50 surrendering officers and men at Quesnoy and the nearby Airaines. According to Richardot, on 7 June, the commanding French officer Charles N'Tchoréré and his company surrendered to the 7th Panzer Division. He was then executed by the 25th Infantry Regiment (the 7th Panzer Division did not have a 25th Infantry Regiment). Journalist Alain Aka states simply that he was executed by one of Rommel's soldiers and his body was driven over by tank. Erwan Bergot reports that he was killed by the SS. Historian John Morrow states he was shot in the neck by a Panzer officer, without mentioning the unit of the perpetrators of this crime. The website of the National Federation of Volunteer Servicemen (F.N.C.V., France) states that N'Tchoréré was pushed against the wall and, despite protests from his comrades and newly liberated German prisoners, was shot by the SS. Elements of the division are considered by Scheck to have been "likely" responsible for the execution of POWs in Hangest-sur-Somme, while Scheck reports that they were too far away to have been involved in the massacres at Airaines and nearby villages. Scheck says that the German units fighting there came from the 46th and 2nd Infantry Division, and possibly from the 6th and 27th Infantry Division as well. Scheck also writes that there were no SS units in the area. Morrow, citing Scheck, says that the 7th Panzer Division carried out "cleansing operations". French historian counts the number of victims of the 7th Panzer Division in Airaines at 109, mostly French-African soldiers from Senegal. Historian Daniel Butler agrees that it was possible that the massacre at Le Quesnoy happened given the existence of Nazis, such as Hanke, in Rommel's division, while stating that in comparison with other German units, few sources regarding such actions of the men of the 7th Panzer exist. Butler believes that "it's almost impossible to imagine" Rommel authorising or countenancing such actions. He also writes that "Some accusers have twisted a remark in Rommel's own account of the action in the village of Le Quesnoy as proof that he at least tacitly condoned the executions—'any enemy troops were either wiped out or forced to withdraw'—but the words themselves as well as the context of the passage hardly support the contention." Showalter writes: "In fact, the garrison of Le Quesnoy, most of them Senegalese, took heavy toll of the German infantry in house-to-house fighting. Unlike other occasions in 1940, when Germans and Africans met, there was no deliberate massacre of survivors. Nevertheless, the riflemen took few prisoners, and the delay imposed by the tirailleurs forced the Panzers to advance unsupported until Rommel was ordered to halt for fear of coming under attack by Stukas." Claus Telp comments that Airaines was not in the sector of the 7th, but at Hangest and Martainville, elements of the 7th might have shot some prisoners and used British Colonel Broomhall as a human shield (although Telp is of the opinion that it was unlikely that Rommel approved of, or even knew about, these two incidents). Historian David Stone notes that acts of shooting surrendered prisoners were carried out by Rommel's 7th Panzer Division and observes contradictory statements in Rommel's account of the events; Rommel initially wrote that "any enemy troops were wiped out or forced to withdraw" but also added that "many prisoners taken were hopelessly drunk." Stone attributes the massacres of soldiers from the 53ème Regiment d'Infanterie Coloniale (N'Tchoréré's unit) on 7 June to the 5th Infantry Division.

On 6 February 1941, Rommel was appointed commander of the new "Deutsches Afrika Korps" (DAK), consisting of the 5th Light Division (later renamed 21st Panzer Division) and of the 15th Panzer Division. He was promoted to "Generalleutnant" three days later and flew to Tripoli on 12 February. The DAK had been sent to Libya in Operation Sonnenblume to support Italian troops who had been roundly defeated by British Commonwealth forces in Operation Compass. His efforts in the Western Desert Campaign earned Rommel the nickname the "Desert Fox" from British journalists. Allied troops in Africa were commanded by General Archibald Wavell, Commander-in-Chief, Middle East Command.

Giordana Terracina writes that: "On April 3, the Italians recaptured Benghazi and a few months later the Afrika Korps led by Rommel was sent to Libya and began the deportation of the Jews of Cyrenaica in the concentration camp of Giado and other smaller towns in Tripolitania. This measure was accompanied by shooting, also in Benghazi, of some Jews guilty of having welcomed the British troops, on their arrival, treating them as liberators."
Some of the Jewish prisoners were later transferred to Italy where they were used for exhausting forced labour on German fortifications, Giordana cites a testimony of one Jewish camp survivor, Sion Burbea, who states that he witnessed Rommel inspecting their work together with general Albert Kesselring According to the witness, the inspection happened on a certain day after 26 October 1943 (when they were transferred to the line "Gustav"). Terracina says it must have happened before 20 November 1943, when Rommel was recalled to Germany. According to other historians, Rommel's Italian responsibility ended on 19 October 1943, when Northern Italy was left under Kesselring's authority, and Rommel received his new mission as Inspector General of Defence in the West on 5 November. According to Remy, on this same day, Rommel was already back in Germany discussing the fortifications with Hitler and Speer, before returning to Italy briefly to prepare for the move to France. By 21 November 1943, Rommel and his Army Group B headquarters were in France.

According to Yad Vashem's International School for Holocaust Studies, the Jews were deported in 1940 and 1941 to concentration camps and forced labour camps by the Italian authorities. Maurice Roumani writes that: "German influence in Libya had been felt since 1938. However, Germany' s direct involvement in the colonial authorities' affairs and management did not completely materialize until 1941. Libyan Jews noted that in daily matters, the Germans largely acted out of pragmatic economic interest rather than adopting the political and ideological practices known elsewhere." Relying upon Jews for goods needed for military activities, they perceived the Jews in Libya as similar to the Muslims, "by the end of their time in Libya". The situation only became radicalized for the Jews when Italy entered the war in 1940. Deportation to Giado, the worst experience that happened to Libyan Jews, was implemented by Italian authorities under the order of Mussolini when he deemed Libyan Jews as traitors in 1942.

According to German historian Wolfgang Proske, Rommel forbade his soldiers to buy anything from the Jewish population of Tripoli, used Jewish slave labour and commanded Jews to clear out minefields by walking on them ahead of his forces. According to Proske, some of the Libyan Jews were eventually sent to concentration camps. Historians Christian Schweizer and Peter Lieb note that: "Over the last few years, even though the social science teacher Wolfgang Proske has sought to participate in the discussion [on Rommel] with very strong opinions, his biased submissions are not scientifically received." The "Heidenheimer Zeitung" notes that Proske was the publisher of his main work "Täter, Helfer, Trittbrettfahrer – NS-Belastete von der Ostalb", after failing to have it published by another publisher.

According to the BBC, on 9 October 1942, Italian racial laws were extended to Libya, and by the end of the war, hundreds of Jews used as slave labour would perish from ill treatment.

Historian Jens Hoppe notes that Libya was the colony of an Axis power and thus it was unlike Tunisia, which was directly under Nazi Germany's control. In November 1942, Rudolf Rahn, the Plenopotentỉary Minister of the Reich notified Admiral Esteva that the Jewish question would be under his jurisdiction. The Germans then hold a meeting to decide the deployment of Jewish forced labour, with the significant authority being Rahn, Rauff and Nehring. Libyan Jews deported to Tunisia were under the control of the "Sicherheitsdienst", led by Rauff, and the Wehrmacht's use of Jewish forced labour in Tunisia began under Nehring on 6 December 1942. According to Rahn, it was von Arnim (who had led the Axis forces in North Africa since 8 December) who assigned Jewish labour companies to individual units. In Libya, it was Bastico, the antisemitic governor of Libya and commander-in-chief of Italian forces in North Africa, who ordered the use of Jewish forced labour and controlled the camps.

According to the publication Jewish Communities of the World edited by Anthony Lerman, in 1942 under the German occupation, the Benghazi quarter that housed Jewish population was plundered and 2000 Jews were deported across the desert, out of which circa a fifth have perished Malka Hillel Shulewitz in "Forgotten Millions: The Modern Jewish Exodus from Arab" writes that up to 1945, the only anti-Jewish riots since centuries in Libya happened during German occupation and plunder in Banghazi The Illustrated Atlas of Jewish Civilization: 4,000 Years of Jewish History by Martin Gilbert state that that German occupation led to first anti-Jewish pogrom in 1942 and subsequent plunder of the Jewish district alongside of expulsion of Jews The Moment magazine in an article "Once upon a time in Libya" published in May 1987 stated that "on orders from the German military commander, the Axis forces, in 1942, plundered Jewish shops and deported 2,600 Benghazi Jews to Giado".
Historians like Mark Avrum Ehrlich and Jacques Roumani describes the pogrom and riots in 1941 as "Italian-led". According to "The Encyclopedia of Jewish Life Before and During the Holocaust: A-J", most of the looting against the Jews in Benghazi after British withdrawal in 1941 was done by local Italian residents. As the Germans appeared in 1941, Jews initially feared the Germans but when things calmed down, they sold merchandise to the German and thus improved their business situation. In 1942 though, except for a few wealthy families, the Jews were sent by Italians to concentration camps in Giado, Gharian and Yefren, under the order of Mussolini. According to Robert Rozett and Georges Bensoussan, from 1938 (when Italian anti-Jewish legislation was introduced), most of the harsher measures against Libyan Jews were prevented because they had a powerful protector in Italo Balbo, the Governor. The situation became worse after Balbo died in an aircraft accident. In 1941, when the Italians regained control, they accused the Jews of betrayal. Bensoussan says that 870 British Jews and 1,600 French Jews were expelled by the Italian Minister of Colonies.

Christian Gerlach writes that: "There is no evidence of German extermination efforts against the 100,000—130,000 Jews in Libya and Tunisia – Italian and French colonies, respectively – where German troops operated in 1942—43. This was in contrast to the fact that in the protocol of the Wannsee conference French northern Africa was included in the figures of Jews to be targeted. Measures, which began in November 1942, were largely restricted in Tunisia to German- and Italian- organized forced labor and official plunder; and in Libya to the Italian internment of foreign Jews and those from the region of Cyrenaica." Gerlach estimates the number of Jews who died due to internment and bad living conditions at 1500 in Tunisia in 1943 and 500 in Libya in 1941–1942, stating that unknown number of foreign interned Jews also died in Libya and Algeria.

Rommel and his troops were technically subordinate to Italian commander-in-chief General Italo Gariboldi. Disagreeing with the orders of the "Oberkommando der Wehrmacht" (OKW, German armed forces high command) to assume a defensive posture along the front line at Sirte, Rommel resorted to subterfuge and insubordination to take the war to the British. According to Remy, the General Staff tried to slow him down but Hitler encouraged him to advance—an expression of the conflict that had existed between Hitler and the army leadership since the invasion of Poland. He decided to launch a limited offensive on 24 March with the 5th Light Division, supported by two Italian divisions. This thrust was not anticipated by the British, who had Ultra intelligence showing that Rommel had orders to remain on the defensive until at least May, when the 15th Panzer Division were due to arrive.
The British Western Desert Force had meanwhile been weakened by the transfer in mid-February of three divisions for the Battle of Greece. They fell back to Mersa El Brega and started constructing defensive works. Rommel continued his attack against these positions to prevent the British from building up their fortifications. After a day of fierce fighting on 31 March, the Germans captured Mersa El Brega. Splitting his force into three groups, Rommel resumed the advance on 3 April. Benghazi fell that night as the British pulled out of the city. Gariboldi, who had ordered Rommel to stay in Mersa El Brega, was furious. Rommel was equally forceful in his response, telling Gariboldi, "One cannot permit unique opportunities to slip by for the sake of trifles." A signal arrived from General Franz Halder reminding Rommel that he was to halt in Mersa El Brega. Knowing Gariboldi could not speak German, Rommel told him the message gave him complete freedom of action. Gariboldi backed down.

On 4 April, Rommel was advised by his supply officers that fuel was running short, which could result in a delay of up to four days. The problem was Rommel's fault, as he had not advised his supply officers of his intentions, and no fuel dumps had been set up. Rommel ordered the 5th Light Division to unload all of their lorries and to return to El Agheila to collect fuel and ammunition. Driving through the night, they were able to reduce the halt to a single day. Fuel supply was problematic throughout the campaign, as no petrol was available locally; it had to be brought from Europe by tanker and then carried by road to where it was needed. Food and fresh water were also in short supply, and it was difficult to move tanks and other equipment off-road through the sand. Cyrenaica was captured by 8 April, except for the port city of Tobruk, which was besieged on 11 April.

The siege of Tobruk was not technically a siege, as the defenders were still able to move supplies and reinforcements into the city via the port. Rommel knew that by capturing the port he could greatly reduce the length of his supply lines and increase his overall port capacity, which was insufficient even for day-to-day operations and only half that needed for offensive operations. The city, which had been heavily fortified by the Italians during their 30-year occupation, was garrisoned by the 18th Infantry Brigade of the Australian 7th Division, the Australian 9th Division, HQ 3rd Armoured Brigade, several thousand British infantrymen, and one regiment of Indian infantry, for a total of 36,000 men. The commanding officer was Australian Lieutenant General Leslie Morshead. Hoping to catch the defenders off-guard, Rommel launched a failed attack on 14 April.

Rommel requested reinforcements, but the OKW, then completing preparations for Operation Barbarossa, refused. General Friedrich Paulus, head of the Operations Branch of OKH, arrived on 25 April to review the situation. He was present for a second failed attack on the city on 30 April. On 4 May Paulus ordered that no further attempts should be made to take Tobruk via a direct assault. This order was not open to interpretation, and Rommel had no choice but to comply. Aware of this order from intelligence reports, Churchill urged Wavell to seize the initiative. While awaiting further reinforcements and a shipment of 300 tanks that were already on their way, Wavell launched a limited offensive code named Operation Brevity on 15 May. The British briefly seized Sollum, Fort Capuzzo, and the important Halfaya Pass, a bottleneck along the coast near the border between Libya and Egypt. Rommel soon forced them to withdraw. On 15 June Wavell launched Operation Battleaxe. The attack was defeated in a four-day battle at Sollum and Halfaya Pass, resulting in the loss of 98 British tanks. The Germans lost 12 tanks, while capturing and seriously damaging over 20 British tanks. The defeat resulted in Churchill replacing Wavell with General Claude Auchinleck as theatre commander. Rommel appointed Heinrich Kirchheim as commander of 5th Light Division on 16 May, became displeased and replaced him with Johann von Ravenstein on 30 May 1941.

In August, Rommel was appointed commander of the newly created Panzer Group Africa, with Fritz Bayerlein as his chief of staff. The Afrika Korps, comprising the 15th Panzer Division and the 5th Light Division, now reinforced and redesignated 21st Panzer Division, was put under command of Generalleutnant Ludwig Crüwell. In addition to the Afrika Korps, Rommel's Panzer Group had the 90th Light Division and four Italian divisions, three infantry divisions investing Tobruk, and one holding Bardia. The two Italian armoured divisions, "Ariete" and "Trieste", were still under Italian control. They formed the Italian XX Motorized Corps under the command of General Gastone Gambara. Two months later Hitler decided he must have German officers in better control of the Mediterranean theatre, and appointed Field Marshal Albert Kesselring as Commander in Chief, South. Kesselring was ordered to get control of the air and sea between Africa and Italy.

Following his success in Battleaxe, Rommel returned his attention to the capture of Tobruk. He made preparations for a new offensive, to be launched between 15 and 20 November. Meanwhile, Auchinleck reorganised Allied forces and strengthened them to two corps, XXX and XIII, which formed the British Eighth Army, which was placed under the command of Alan Cunningham. Auchinleck had 770 tanks and double the number of Axis aircraft. Rommel opposed him with the 15th and 21st Panzer Divisions with a total of 260 tanks, the 90th Light Infantry division, five Italian infantry divisions, and one Italian armoured division of 278 tanks.

Auchinleck launched Operation Crusader, a major offensive to relieve Tobruk, on 18 November 1941. The XIII Corps on the right were assigned to attack Sidi Omar, Capuzzo, Sollum, and Bardia; the XXX Corps (which included most of the armour) were to move on the left southern flank to a position about south of Tobruk, with the expectation that Rommel would find this move so threatening that he would move his armour there in response. Once Rommel's tanks were written down, the British 70th Infantry Division would break out of Tobruk to link up with XXX Corps. Rommel reluctantly decided on 20 November to call off his planned attack on Tobruk.

Some elements of the 7th Armoured Division were stopped on 19 January by the Italian "Ariete" Armoured Division at Bir el Gobi, but they also managed to capture the airfields at Sidi Rezegh, from Tobruk. Engaging the Allied tanks located there became Rommel's primary objective. Noting that the British armour was separated into three groups incapable of mutual support, he concentrated his Panzers so as to gain local superiority. The expected breakout from Tobruk, which took place on 20 November, was stopped by the Italians. The airfield at Sidi Rezegh was retaken by 21st Panzer on 22 November. In four days of fighting, the Eighth Army lost 530 tanks and Rommel only 100. The German forces near Halfaya Pass were cut off on 23 November.

Wanting to exploit the British halt and their apparent disorganisation, on 24 November Rommel counterattacked near the Egyptian border in an operation that became known as the "dash to the wire". Unknown to Rommel, his troops passed within of a major British supply dump. Cunningham asked Auchinleck for permission to withdraw into Egypt, but Auchinleck refused, and soon replaced Cunningham as commander of Eighth Army with Major General Neil Ritchie. The German counterattack stalled as it outran its supplies and met stiffening resistance, and was criticised by the German High Command and some of Rommel's staff officers.

While Rommel drove into Egypt, the remaining Commonwealth forces east of Tobruk threatened the weak Axis lines there. Unable to reach Rommel for several days, Rommel's Chief of Staff, Siegfried Westphal, ordered the 21st Panzer Division withdrawn to support the siege of Tobruk. On 27 November the British attack on Tobruk linked up with the defenders, and Rommel, having suffered losses that could not easily be replaced, had to concentrate on regrouping the divisions that had attacked into Egypt. By 7 December Rommel fell back to a defensive line at Gazala, just west of Tobruk, all the while under heavy attack from the Desert Air Force. The Bardia garrison surrendered on 2 January and Halfaya on 17 January 1942. The Allies kept up the pressure, and Rommel was forced to retreat all the way back to the starting positions he had held in March, reaching El Agheila in December 1941. The British had retaken almost all of Cyrenaica, but Rommel's retreat dramatically shortened his supply lines.

On 5 January 1942 the Afrika Korps received 55 tanks and new supplies and Rommel started planning a counterattack. On 21 January, Rommel launched the attack. Caught by surprise by the Afrika Korps, the Allies lost over 110 tanks and other heavy equipment. The Axis forces retook Benghazi on 29 January and Timimi on 3 February, with the Allies pulling back to a defensive line just before the Tobruk area south of the coastal town of Gazala. Rommel placed a thin screen of mobile forces before them, and held the main force of the Panzerarmee well back near Antela and Mersa Brega. Between December 1941 and June 1942, Rommel had excellent information about the disposition and intentions of the Commonwealth forces. Bonner Fellers, the US diplomat in Egypt, was sending detailed reports to the US State Department using a compromised code.
Following Kesselring's successes in creating local air superiority around the British naval and air bases at Malta in April 1942, an increased flow of supplies reached the Axis forces in Africa. With his forces strengthened, Rommel contemplated a major offensive operation for the end of May. He knew the British were planning offensive operations as well, and he hoped to pre-empt them. While out on reconnaissance on 6 April, he was severely bruised in the abdomen when his vehicle was the target of artillery fire. The British had 900 tanks in the area, 200 of which were new Grant tanks. Unlike the British, the Axis forces had no armoured reserve; all operable equipment was put into immediate service. Rommel's Panzer Army Africa had a force of 320 German tanks; 50 of these were the light Panzer II model. In addition, 240 Italian tanks were in service, but these were under-gunned and poorly armoured.

Early in the afternoon of 26 May 1942, Rommel attacked first and the Battle of Gazala commenced. Italian infantry supplemented with small numbers of armoured forces assaulted the centre of the Gazala fortifications. To give the impression that this was the main assault, spare aircraft engines mounted on trucks were used to create huge clouds of dust. Ritchie was not convinced by this display, and left the 4th and 22nd Armoured Brigades in position at the south end of the Commonwealth position. Under the cover of darkness, the bulk of Rommel's motorized and armoured forces (15th and 21st Panzers, 90th Light Division, and the Italian Ariete and Trieste Divisions) drove south to skirt the left flank of the British, coming up behind them and attacking to the north the following morning. Throughout the day a running armour battle occurred, where both sides took heavy losses. The Grant tanks proved to be impossible to knock out except at close range.

Renewing the attack on the morning of 28 May, Rommel concentrated on encircling and destroying separate units of the British armour. Repeated British counterattacks threatened to cut off and destroy the Afrika Korps. Running low on fuel, Rommel assumed a defensive posture, forming "the Cauldron". He made use of the extensive British minefields to shield his western flank. Meanwhile, Italian infantry cleared a path through the mines to provide supplies. On 30 May Rommel resumed the offensive, attacking westwards to link with elements of Italian X Corps, which had cleared a path through the Allied minefields to establish a supply line. On 1 June, Rommel accepted the surrender of some 3,000 soldiers of the 150th Brigade. On 2 June he demanded surrender of French soldiers at Bir Hakeim, threatening to "exterminate" them if they do not comply. On 6 June, 90th Light Division and the "Trieste" Division assaulted the Free French strongpoint in the Battle of Bir Hakeim, but the defenders continued to thwart the attack until finally evacuating on 10 June. With his communications and the southern strongpoint of the British line thus secured, Rommel shifted his attack north again, relying on the British minefields of the Gazala lines to protect his left flank. Threatened with being completely cut off, the British began a retreat eastward toward Egypt on 14 June, the so-called "Gazala Gallop".

On 15 June Axis forces reached the coast, cutting off the escape for the Commonwealth forces still occupying the Gazala positions. With this task completed, Rommel struck for Tobruk while the enemy was still confused and disorganised. Tobruk's defenders were at this point the 2nd South African Infantry Division, 4th Antiaircraft Brigade, 11th Indian Infantry, 32nd Army Tank, and 201st Guards Brigades, all under command of Major General Hendrik Klopper. The assault on Tobruk began at dawn on 20 June, and Klopper surrendered at dawn the following day. With Tobruk, Rommel achieved the capture of 32,000 defenders, the port, and huge quantities of supplies. Only at the fall of Singapore, earlier that year, had more British Commonwealth troops been captured at one time. On 22 June, Hitler promoted Rommel to Generalfeldmarschall for this victory.

Following his success at Gazala and Tobruk, Rommel wanted to seize the moment and not allow 8th Army a chance to regroup. He strongly argued that the Panzerarmee should advance into Egypt and drive on to Alexandria and the Suez Canal, as this would place almost all the Mediterranean coastline in Axis hands, ease conditions on the Eastern Front, and potentially lead to the capture from the south of the oil fields in the Caucasus and Middle East. Indeed, Allied strategists feared that if Rommel captured Egypt, he would next overrun the Middle East before possibly linking up with the forces besieging the Caucasus. However, Hitler viewed the North African campaign primarily as a way to assist his Italian allies, not as an objective in and of itself. He would not consider sending Rommel the reinforcements and supplies he needed to take and hold Egypt, as this would have required diverting men and supplies from his primary focus: the Eastern Front.

Rommel's success at Tobruk worked against him, as Hitler no longer felt it was necessary to proceed with Operation Herkules, the proposed attack on Malta. Auchinleck relieved Ritchie of command of the Eighth Army on 25 June, and temporarily took command himself. Rommel knew that delay would only benefit the British, who continued to receive supplies at a faster rate than Rommel could hope to achieve. He pressed an attack on the heavily fortified town of Mersa Matruh, which Auchinleck had designated as the fall-back position, surrounding it on 28 June. The 2nd New Zealand Division and 50th (Northumbrian) Infantry Division were almost caught, with 50th Division fleeing on the 27th and 2nd Division escaping after a short engagement during the pre-dawn hours of 28 June. The four divisions of X Corps were caught in the encirclement, and were ordered by Auchinleck to attempt a breakout. The 29th Indian Infantry Brigade was nearly destroyed, losing 6,000 troops and 40 tanks. The fortress fell on 29 June. In addition to stockpiles of fuel and other supplies, the British abandoned hundreds of tanks and trucks. Those that were functional were put into service by the Panzerarmee.

After the Fall of Tobruk, the Allied POWs were quickly delivered to the POW camps. All POWs had to endure extremely hard living condition. Non-European soldiers were mistreated and several were shot if they were giving the captors troubles. Karen Horn in her publication "Narratives from North Africa: South African prisoner-of-war experience following the fall of Tobruk" describes a witness statement reporting execution of a group of black soldiers by German guards transporting them to prisoner of war camps. Horn writes that both German and Italian forces did not view black and coloured prisoners as regular troops; and "we do know that German and Italian treatment of black Allied soldiers was for the most part dreadful". According to Horn, black soldiers were threatened with death if they refused to work, which would constitute violation of Geneva Convention, and describes other types of mistreatment such as giving their food rations to one biscuit per day and giving them minimum water rations. Furthermore, another witness report describes how Indian and black soldiers were barred from seeking cover in shelters during Allied bombings. Throughout the forced labour the captured soldiers were subjected to assaults by both German and Italian guards supervising their work.

According to Karen Horn, Rommel himself was considered by the South African POWs as an efficient and proper soldier. This impression was one of the factors that helped the POWs to identify with the German captors to a degree, whom they would less likely defy than the Italians. Maurice Remy writes that prisoners in North Africa were the responsibility of the Italians (under whose administration the POW camps were operated) anyway. According to Remy, no incident of assault by the soldiers of the Afrika Korps themselves against the prisoners (in the process of delivering them to the Italian side) is currently known. His position on the matter of POWs did not show effect on the way they were treated in camps though. Despite his insistence that the black and white prisoners should be in the same compounds and accorded the same treatment, the black and white POWs were only kept together at the early state of detention, with the black POWs being singled out for harsher tasks and maltreatment. Although, in segregating the prisoners, it was the Italian side that followed the Geneva Convention which discouraged gathering of prisoners of different races and nationalities. The South African soldier Job Maseko recalls that, after Rommel visited his camp and asked Maseko about the prisoner's conditions, the commandant Major Schroeder (who had warned Maseko against talking) imposed even more brutal methods.

Rommel continued his pursuit of the Eighth Army, which had fallen back to heavily prepared defensive positions at El Alamein. This region is a natural choke point, where the Qattara Depression creates a relatively short line to defend that could not be outflanked to the south because of the steep escarpment. During this time Germans prepared numerous propaganda postcards and leaflets for Egyptian and Syrian population urging them to "chase English out of the cities", warning them about "Jewish peril" and with one leaflet printed in 296,000 copies and aimed at Syria stating among others "Because Marshal Rommel, at the head of the brave Axis troops, is already rattling the last gates of England's power! Arabs! Help your friends achieve their goal:abolishing the English-Jewish-American tyranny!" On 1 July the First Battle of El Alamein began. Rommel had around 100 available tanks. The Allies were able to achieve local air superiority, with heavy bombers attacking the 15th and 21st Panzers, who had also been delayed by a sandstorm. The 90th Light Division veered off course and were pinned down by South African artillery fire. Rommel continued to attempt to advance for two more days, but repeated sorties by the Desert Air Force meant he could make no progress. On 3 July, he wrote in his diary that his strength had "faded away". Attacks by 21st Panzer on 13 and 14 July were repulsed, and an Australian attack on 16–17 July was held off with difficulty. Throughout the first half of July, Auchinleck concentrated attacks on the Italian 60th Infantry Division Sabratha at Tel el Eisa. The ridge was captured by the 26th Australian Brigade on 16 July. Both sides suffered similar losses throughout the month, but the Axis supply situation remained less favourable. Rommel realised that the tide was turning. A break in the action took place at the end of July as both sides rested and regrouped.
Preparing for a renewed drive, the British replaced Auchinleck with General Harold Alexander on 8 August. Bernard Montgomery was made the new commander of Eighth Army that same day. The Eighth Army had initially been assigned to General William Gott, but he was killed when his plane was shot down on 7 August. Rommel knew that a British convoy carrying over 100,000 tons of supplies was due to arrive in September. He decided to launch an attack at the end of August with the 15th and 21st Panzer Division, 90th Light Division, and the Italian XX Motorized Corps in a drive through the southern flank of the El Alamein lines. Expecting an attack sooner rather than later, Montgomery fortified the Alam el Halfa ridge with the 44th Division, and positioned the 7th Armoured Division about to the south.

The Battle of Alam el Halfa was launched on 30 August. The terrain left Rommel with no choice but to follow a similar tactic as he had at previous battles: the bulk of the forces attempted to sweep around from the south while secondary attacks were launched on the remainder of the front. It took much longer than anticipated to get through the minefields in the southern sector, and the tanks got bogged down in unexpected patches of quicksand (Montgomery had arranged for Rommel to acquire a falsified map of the terrain). Under heavy fire from British artillery and aircraft, and in the face of well prepared positions that Rommel could not hope to outflank for lack of fuel, the attack stalled. By 2 September, Rommel realized the battle was unwinnable, and decided to withdraw.

Montgomery had made preparations to cut the Germans off in their retreat, but in the afternoon of 2 September he visited Corps commander Brian Horrocks and gave orders to allow the Germans to retire. This was to preserve his own strength intact for the main battle which was to come. On the night of 3 September the 2nd New Zealand Division and 7th Armoured Division positioned to the north engaged in an assault, but they were repelled in a fierce rearguard action by the 90th Light Division. Montgomery called off further action to preserve his strength and allow for further desert training for his forces. In the attack Rommel had suffered 2,940 casualties and lost 50 tanks, a similar number of guns, and 400 lorries, vital for supplies and movement. The British losses, except tank losses of 68, were much less, further adding to the numerical inferiority of Panzer Army Afrika. The Desert Air Force inflicted the highest proportions of damage on Rommel's forces. He now realized the war in Africa could not be won. Physically exhausted and suffering from a liver infection and low blood pressure, Rommel flew home to Germany to recover his health. General Georg Stumme was left in command in Rommel's absence.

Improved decoding by British intelligence (see Ultra) meant that the Allies had advance knowledge of virtually every Mediterranean convoy, and only 30 percent of shipments were getting through. In addition, Mussolini diverted supplies intended for the front to his garrison at Tripoli and refused to release any additional troops to Rommel. The increasing Allied air superiority and lack of fuel meant Rommel was forced to take a more defensive posture than he would have liked for the second Battle of El Alamein. The German defences to the west of the town included a minefield deep with the main defensive line – itself several thousand yards deep – to its west. This, Rommel hoped, would allow his infantry to hold the line at any point until motorized and armoured units in reserve could move up and counterattack any Allied breaches. The British offensive began on 23 October. Stumme, in command in Rommel's absence, died of an apparent heart attack while examining the front on 24 October, and Rommel was ordered to return from his medical leave, arriving on the 25th. Montgomery's intention was to clear a narrow path through the minefield at the northern part of the defenses, at the area called Kidney Ridge, with a feint to the south. By the end of 25 October, the 15th Panzer, the defenders in this sector, had only 31 serviceable tanks remaining of their initial force of 119. Rommel brought the 21st Panzer and Ariete Divisions north on 26 October, to bolster the sector. On 28 October, Montgomery shifted his focus to the coast, ordering his 1st and 10th Armoured Divisions to attempt to swing around and cut off Rommel's line of retreat. Meanwhile, Rommel concentrated his attack on the Allied salient at Kidney Ridge, inflicting heavy losses. However, Rommel had only 150 operational tanks remaining, and Montgomery had 800, many of them Shermans.

Montgomery, seeing his armoured brigades losing tanks at an alarming rate, stopped major attacks until the early hours of 2 November, when he opened Operation Supercharge, with a massive artillery barrage. This was followed by penetration at the salient by two armoured and two infantry divisions. Rommel's counterattack at 11:00 inflicted severe casualties on the Commonwealth troops, but by 20:00, with only 35 tanks remaining, he ordered his forces to disengage and begin to withdraw. At midnight, he informed the OKW of his decision, and received a reply directly from Hitler the following afternoon: he ordered Rommel and his troops to hold their position to the last man. Rommel, who believed that the lives of his soldiers should never be squandered needlessly, was stunned. While he (like all members of the Wehrmacht) had pledged an oath of absolute obedience to Hitler, he thought this order was pointless, even madness, and had to be disobeyed. Rommel initially complied with the order, but after discussions with Kesselring and others, he issued orders for a retreat on 4 November. The delay proved costly in terms of his ability to get his forces out of Egypt. He later said the decision to delay was what he most regretted from his time in Africa. Meanwhile, the British 1st and 7th Armoured Division had broken through the German defences and were preparing to swing north and surround the Axis forces. On the evening of the 4th, Rommel finally received word from Hitler authorizing the withdrawal. By this time it was impossible for Rommel to save his non-motorized units.

As Rommel attempted to withdraw his forces before the British could cut off his retreat, he fought a series of delaying actions. Heavy rains slowed movements and grounded the Desert Air Force, which aided the withdrawal. According to Kourt von Esebeck, those German parts of Panzerarmee Africa that were motorized slipped away from El Alamein, all vehicles had been taken away from Italian forces, leaving them behind, but were under pressure from the pursuing Eighth Army. According to officers of the Italian X Corps, they were not deliberately abandoned and an effort to save all divisions would only have led to destruction of more units. A series of short delaying actions was fought over the coastal highway, but no line could be held for any length of time, as Rommel lacked the armour and fuel to defend his open southern flank. Rommel continued to retreat west, abandoning Halfaya Pass, Sollum, Mersa Brega and El Agheila. The line Rommel was aiming for was 'Gabes gap' in Tunisia. Luftwaffe Field Marshal Kesselring strongly criticized Rommel's decision to retreat all the way to Tunisia, as each airfield the Germans abandoned extended the range of the Allied bombers and fighters. Rommel defended his decision, pointing out that if he tried to assume a defensive position the Allies would destroy his forces and take the airfields anyway; the retreat saved the lives of his remaining men and shortened his supply lines. By now, Rommel's remaining forces fought in reduced strength combat groups, whereas the Allied forces had great numerical superiority and control of the air. Upon his arrival in Tunisia, Rommel noted with some bitterness the reinforcements, including the 10th Panzer Division, arriving in Tunisia following the Allied invasion of Morocco.

Robert Satloff writes in his book "Among the Righteous: Lost Stories from the Holocaust's Long Reach into Arab Lands" that as the German and Italian forces retreated across Libya towards Tunisia, the Jewish population became victim upon which they released their anger and frustration. According to Satloff Afrika Korps soldiers plundered Jewish property all along the Libyan coast. This violence and persecution only came to an end with the arrival of General Montgomery in Tripoli on 23 January 1943. According to Maurice Remy, although there were antisemitic individuals in the Afrika Korps, actual cases of abuse are not known, even against the Jewish soldiers of the Eighth Army. Remy quotes Isaac Levy, the Senior Jewish Chaplain of the Eighth Army, as saying that he had never seen "any sign or hint that the soldiers [of the Afrika Korps] are antisemitic.". "The Telegraph" comments: "Accounts suggest that it was not Field Marshal Erwin Rommel but the ruthless SS colonel Walter Rauff who stripped Tunisian Jews of their wealth."

According to several historians, allegations and stories that associate Rommel and the Afrika Korps with the harassing and plundering of Jewish gold and property in Tunisia are usually known under the name "Rommel's treasure" or "Rommel's gold".
Michael FitzGerald comments that the treasure should be named more accurately as Rauff's gold, as Rommel had nothing to do with its acquisition or removal. Jean-Christoph Caron comments that the treasure legend has a real core and that Jewish property was looted by the SS in Tunisia and later might have been hidden or sunken around the port city of Corsica, where Rauff was stationed in 1943. The person who gave birth to the full-blown legend was the SS soldier Walter Kirner, who presented a false map to the French authorities. Caron and Jörg Müllner, his co-author of the ZDF documentary "Rommel's treasure" (Rommels Schatz) tell Die Welt that "Rommel had nothing to do with the treasure, but his name is assocỉated with everything that happened in the war in Africa."

Having reached Tunisia, Rommel launched an attack against the U.S. II Corps which was threatening to cut his lines of supply north to Tunis. Rommel inflicted a sharp defeat on the American forces at the Kasserine Pass in February, his last battlefield victory of the war, and his first engagement against the United States Army.

Rommel immediately turned back against the British forces, occupying the Mareth Line (old French defences on the Libyan border). While Rommel was at Kasserine at the end of January 1943, the Italian General Giovanni Messe was appointed commander of Panzer Army Africa, renamed the Italo-German Panzer Army in recognition of the fact that it consisted of one German and three Italian corps. Though Messe replaced Rommel, he diplomatically deferred to him, and the two coexisted in what was theoretically the same command. On 23 February "Armeegruppe Afrika" was created with Rommel in command. It included the Italo-German Panzer Army under Messe (renamed 1st Italian Army) and the German 5th Panzer Army in the north of Tunisia under General Hans-Jürgen von Arnim.

The last Rommel offensive in North Africa was on 6 March 1943, when he attacked Eighth Army at the Battle of Medenine. The attack was made with 10th, 15th, and 21st Panzer Divisions. Alerted by Ultra intercepts, Montgomery deployed large numbers of anti-tank guns in the path of the offensive. After losing 52 tanks, Rommel called off the assault. On 9 March he returned to Germany. Command was handed over to General Hans-Jürgen von Arnim. Rommel never returned to Africa. The fighting there continued on for another two months, until 13 May 1943, when General Messe surrendered the "Armeegruppe Afrika" to the Allies.

Having arrived in Tunisia German forces ordered establishment of Judenrat and terrorised the local Jewish population into slave labour
Mark Wills writes that the newly arrived German force forcefully conscripted 2000 young Jewish men, with 5000 rounded up in next 6 months. This forced labour was used in extremely dangerous situations near targets of bombing raids, facing hunger and violence.

Der Spiegel writes that: "The SS had established a network of labor camps in Tunisia. More than 2,500 Tunisian Jews died in six months of German rule, and the regular army was also involved in executions." Caron writes on "Der Spiegel" that the camps were organized in early December 1942 by Nehring, the commander in Tunisia, and Rauff, while Rommel was retreating. As commander of the German Afrika Korps, Nehring would continue to use Tunesian forced labour. Historian Clemens Vollnhals writes that use of Jews by Afrika Korps as forced labour is barely known, but it did happen alongside persecution of Jewish population(although on smaller scale than in Europe) and some of the labourers have died. According to Caddick-Adams, no Waffen-SS served under Rommel in Africa at any time and most of the activities of Rauff's detachment happened after Rommel's departure. Shepherd notes that during this time Rommel was retreating and there is no evident that he had contact with the Einsatzkommando Klaus-Michael Mallmann, Martin Cüppers Smith Addressing the call of some authors to contextualize Rommel's actions in Italy and North Africa, Wolfgang Mährle notes that while it is undeniable that Rommel played the role of a "Generalfeldmarschall" in a criminal war, this only illustrates in a limited way his personal attitude and the actions resulted from that.

On 23 July 1943 Rommel was moved to Greece as commander of Army Group E to counter a possible British invasion. He arrived in Greece on 25 July, but was recalled to Berlin the same day because of the overthrow of Mussolini. Rommel was to be posted to Italy as commander of the newly formed Army Group B. On 16 August 1943 Rommel's headquarters moved to Lake Garda in northern Italy and formally assumed command of the army group, which consisted of the 44th Infantry Division, the 26th Panzer Division and the 1st SS Panzer Division Leibstandarte SS Adolf Hitler. When Italy announced its armistice with the Allies on 8 September, his forces took part in Operation Achse, disarming the Italian forces.

Hitler met with Rommel and Kesselring to discuss future operations in Italy on 30 September 1943. Rommel insisted on a defensive line north of Rome, while Kesselring was more optimistic and advocated holding a line south of Rome. Hitler preferred Kesselring's recommendation, and therefore revoked his previous decision for the subordination of Kesselring's forces to Rommel's army group. On 19 October Hitler decided that Kesselring would be the overall commander of the forces in Italy, sidelining Rommel.

Rommel had wrongly predicted that the collapse of the German line in Italy would be fast. On 21 November Hitler gave Kesselring overall command of the Italian theater, moving Rommel and Army Group B to Normandy in France with responsibility for defending the French coast against the long anticipated Allied invasion.

On 4 November 1943, Rommel became General Inspector of the Western Defences. He was given a staff that befitted an army group commander, and the powers to travel, examine and make suggestions on how to improve the defences, but not a single soldier. Hitler, who was having a disagreement with him over military matters, intended to use Rommel as a psychological trump card.

There was broad disagreement in the German High Command as to how best to meet the expected allied invasion of Northern France. The Commander-in-Chief West, Gerd von Rundstedt, believed there was no way to stop the invasion near the beaches because of the Allied navies' firepower, as had been experienced at Salerno. He argued that the German armour should be held in reserve well inland near Paris, where they could be used to counter-attack in force in a more traditional military doctrine. The allies could be allowed to extend themselves deep into France, where a battle for control would be fought, allowing the Germans to envelop the allied forces in a pincer movement, cutting off their avenue of retreat. He feared the piecemeal commitment of their armoured forces would cause them to become caught in a battle of attrition which they could not hope to win.

The notion of holding the armour inland to use as a mobile reserve force from which they could mount a powerful counterattack applied the classic use of armoured formations as seen in France 1940. These tactics were still effective on the Eastern Front, where control of the air was important but did not dominate the action. Rommel's own experiences at the end of the North African campaign revealed to him that the Germans would not be allowed to preserve their armour from air attack for this type of massed assault. Rommel believed their only opportunity would be to oppose the landings directly at the beaches, and to counterattack there before the invaders could become well established. Though there had been some defensive positions established and gun emplacements made, the Atlantic Wall was a token defensive line. Rundstedt had confided to Rommel that it was for propaganda purposes only.

Upon arriving in Northern France Rommel was dismayed by the lack of completed works. According to Ruge, Rommel was in a staff position and could not issue orders, but he took every effort to explain his plan to commanders down to the platoon level, who took up his words eagerly, but "more or less open" opposition from the above slowed down the process. Finally, Rundstedt, who only respected Rommel grudgingly (he called him Field Marshal Cub), intervened and supported Rommel's request for being made a commander. It was granted on 15 January 1944, when "much valuable time had been lost."

He set out to improve the fortifications along the Atlantic Wall with great energy and engineering skill. This was a compromise: Rommel now commanded the 7th and 15th armies; he also had authority over a 20-kilometer-wide strip of coastal land between Zuiderzee and the mouth of the Loire. The chain of command was convoluted: the airforce and navy had their own chiefs, as did the South and Southwest France and the Panzer group; Rommel also needed Hitler's permissions to use the tank divisions. Undeterred, Rommel had millions of mines laid and thousands of tank traps and obstacles set up on the beaches and throughout the countryside, including in fields suitable for glider aircraft landings, the so-called Rommel's asparagus.(The Allies would later counter these with Hobart's Funnies) In April 1944 Rommel promised Hitler that the preparations would be complete by 1 May, but by the time of the Allied invasion the preparations were far from finished. The quality of some of the troops manning them was poor and many bunkers lacked sufficient stocks of ammunition.

Rundstedt expected the Allies to invade in the Pas-de-Calais because it was the shortest crossing point from Britain, its port facilities were essential to supplying a large invasion force, and the distance from Calais to Germany was relatively short. Rommel and Hitler's views on the matter is a matter of debate between authors, with both seeming to change their positions.

Hitler vacillated between the two strategies. In late April, he ordered the I SS Panzer Corps placed near Paris, far enough inland to be useless to Rommel, but not far enough for Rundstedt. Rommel moved those armoured formations under his command as far forward as possible, ordering General Erich Marcks, commanding the 84th Corps defending the Normandy section, to move his reserves into the frontline. Although Rommel was the dominating personality in Normandy with Rundstedt willing to delegate most of the responsibilities to him (the central reserve was Rundstedt's idea but he did not oppose to some form of coastal defense, and gradually came under the influence of Rommel's thinking), Rommel's strategy of an armor-supported coastal defense line was opposed by some officers, most notably Leo Geyr von Schweppenburg, who was supported by Guderian. Hitler compromised and gave Rommel three divisions (the 2nd, the 21st and the 116th Panzer), let Rundstedt retain four and turned the other three to Army Group G, pleasing no one.

The Allies staged elaborate deceptions for D-Day (see Operation Fortitude), giving the impression that the landings would be at Calais. Although Hitler himself expected a Normandy invasion for a while, Rommel and most Army commanders in France believed there would be two invasions, with the main invasion coming at the Pas-de-Calais. Rommel drove defensive preparations all along the coast of Northern France, particularly concentrating fortification building in the River Somme estuary. By D-Day on 6 June 1944 nearly all the German staff officers, including Hitler's staff, believed that Pas-de-Calais was going to be the main invasion site, and continued to believe so even after the landings in Normandy had occurred.

The 5 June storm in the channel seemed to make a landing very unlikely, and a number of the senior officers were away from their units for training exercises and various other efforts. On 4 June the chief meteorologist of the 3 Air Fleet reported that weather in the channel was so poor there could be no landing attempted for two weeks. On 5 June Rommel left France and on 6 June he was at home celebrating his wife's birthday. He was recalled and returned to his headquarters at 10pm. Meanwhile, earlier in the day, Rundstedt had requested the reserves be transferred to his command. At 10am Keitel advised that Hitler declined to release the reserves but that Rundstedt could move the 12th SS Panzer Division Hitlerjugend closer to the coast, with the Panzer-Lehr-Division placed on standby. Later in the day, Rundstedt received authorisation to move additional units in preparation for a counterattack, which Rundstedt decided to launch on 7 June. Upon arrival, Rommel concurred with the plan. By nightfall, Rundstedt, Rommel and Speidel continued to believe that the Normandy landing might have been a diversionary attack, as the Allied deception measures still pointed towards Calais. The 7 June counterattack did not take place because Allied air bombardments prevented the 12th SS's timely arrival. All this made the German command structure in France in disarray during the opening hours of the D-Day invasion.

Facing relatively small-scale German counterattacks, the Allies secured five beachheads by nightfall of 6 June, landing 155,000 troops. The Allies pushed ashore and expanded their beachhead despite strong German resistance. Rommel believed that if his armies pulled out of range of Allied naval fire, it would give them a chance to regroup and re-engage them later with a better chance of success. While he managed to convince Rundstedt, they still needed to win over Hitler. At a meeting with Hitler at his Wolfsschlucht II headquarters in Margival in northern France on 17 June, Rommel warned Hitler about the inevitable collapse in the German defences, but was rebuffed and told to focus on military operations.

By mid-July the German position was crumbling. On 17 July 1944, as Rommel was returning from visiting the headquarters of the I SS Panzer Corps, a fighter piloted by Charley Fox of 412 Squadron, or of No. 602 Squadron RAF or Johannes Jacobus le Roux of No. 602 Squadron RAF strafed his staff car near Sainte-Foy-de-Montgommery. The driver sped up and attempted to get off the main roadway, but a 20 mm round shattered his left arm, causing the vehicle to veer off of the road and crash into trees. Rommel was thrown from the car, suffering injuries to the left side of his face from glass shards and three fractures to his skull. He was hospitalised with major head injuries (assumed to be almost certainly fatal).

The role that Rommel played in the military's resistance against Hitler or the 20 July plot is difficult to ascertain, as most of the leaders who were directly involved did not survive and limited documentation on the conspirators' plans and preparations exists. One piece of evidence that points to the possibility that Rommel came to support the assassination plan was General Eberbach's confession to his son (eavesdropped on by British agencies) while in British captivity, which stated that Rommel explicitly said to him that Hitler and his close associates had to be killed because this would be the only way out for Germany. This conversation occurred about a month before Rommel was coerced into committing suicide. Other notable evidence includes the papers of Rudolf Hartmann (who survived the later purge) and Carl-Heinrich von Stülpnagel, who were among the leaders of the military resistance (alongside General Hans Speidel, Colonel Karl-Richard Koßmann, Colonel Eberhard Finckh and Lieutenant Colonel Caesar von Hofacker). These papers, accidentally discovered by historian Christian Schweizer in 2018 while doing research on Rudolf Hartmann, include Hartmann's eyewitness account of a conversation between Rommel and Carl-Heinrich von Stülpnagel in May 1944, as well as photos of the mid-May 1944 meeting between the inner circle of the resistance and Rommel at Kossmann's house. According to Hartmann, by the end of May, in another meeting at Hartmann's quarters in Mareil–Marly, Rommel showed "decisive determination" and clear approval of the inner circle's plan.

According to a post-war account by Karl Strölin, three of Rommel's friends—the "Oberbürgermeister" of Stuttgart, Strölin (who had served with Rommel in the First World War), Alexander von Falkenhausen and Carl Heinrich von Stülpnagel—began efforts to bring Rommel into the anti-Hitler conspiracy in early 1944. According to Strölin, sometime in February, Rommel agreed to lend his support to the resistance. On 15 April 1944 Rommel's new chief of staff, Hans Speidel, arrived in Normandy and reintroduced Rommel to Stülpnagel. Speidel had previously been connected to Carl Goerdeler, the civilian leader of the resistance, but not to the plotters led by Claus von Stauffenberg, and came to Stauffenberg's attention only upon his appointment to Rommel's headquarters. The conspirators felt they needed the support of a field marshal on active duty. Erwin von Witzleben, who would have become commander-in-chief of the Wehrmacht had the plot succeeded, was a field marshal, but had been inactive since 1942. The conspirators gave instructions to Speidel to bring Rommel into their circle.

Speidel met with former foreign minister Konstantin von Neurath and Strölin on 27 May in Germany, ostensibly at Rommel's request, although the latter was not present. Neurath and Strölin suggested opening immediate surrender negotiations in the West, and, according to Speidel, Rommel agreed to further discussions and preparations. Around the same timeframe, the plotters in Berlin were not aware that Rommel had allegedly decided to take part in the conspiracy. On 16 May, they informed Allen Dulles, through whom they hoped to negotiate with the Western Allies, that Rommel could not be counted on for support.

At least initially, Rommel opposed assassinating Hitler. According to some authors, he gradually changed his attitude. After the war, his widow—among others—maintained that Rommel believed an assassination attempt would spark civil war in Germany and Austria, and Hitler would have become a martyr for a lasting cause. Instead, Rommel reportedly suggested that Hitler be arrested and brought to trial for his crimes; he did not attempt to implement this plan when Hitler visited Margival, France, on 17 June. The arrest plan would have been highly improbable, as Hitler's security was extremely tight. Rommel would have known this, having commanded Hitler's army protection detail in 1939. He was in favour of peace negotiations, and repeatedly urged Hitler to negotiate with the Allies, which is dubbed by some as "hopelessly naive", considering no one would trust Hitler, and "as naive as it was idealistic, the attitude he showed to the man he had sworn loyalty". According to Reuth, the reason Lucie Rommel did not want her husband to be associated with any conspiracy was that even after the war, the German population neither grasped nor wanted to comprehend the reality of the genocide, thus conspirators were still treated as traitors and outcasts. On the other hand, the resistance depended on the reputation of Rommel to win over the population. Some officers who had worked with Rommel also recognized the relationship between Rommel and the resistance: Westphal said that Rommel did not want any more senseless sacrifices. Butler, using Ruge's recollections, reports that when told by Hitler himself that "no one will make peace with me", Rommel told Hitler that if he was the obstacle for peace, he should resign or kill himself, but Hitler insisted on fanatical defense. Reuth, based on Jodl's testimony, reports that Rommel forcefully presented the situation and asked for political solutions from Hitler, who rebuffed that Rommel should leave politics to him. Brighton comments that Rommel seemed devoted, even though he did not have much faith in Hitler anymore, considering he kept informing Hitler in person and by letter about his changing beliefs, despite facing a military dilemma as well as a personal struggle. Lieb remarks that Rommel's attitude in describing the situation honestly and requiring political solutions was almost without precedent and contrary to the attitude of many other generals. Remy comments that Rommel put himself and his family (which he had briefly considered evacuating to France, but refrained from doing so) at risk for the resistance out of a combination of his concern for the fate of Germany, his indignation at atrocities and the influence of people around him.

On 15 July, Rommel wrote a letter to Hitler giving him a "last chance" to end the hostilities with the Western Allies, urging Hitler to "draw the proper conclusions without delay". What Rommel did not know was that the letter took two weeks to reach Hitler because of Kluge's precautions. Various authors report that many German generals in Normandy, including some SS officers like Hausser, Bittrich, Dietrich (a hard-core Nazi and Hitler's long-time supporter) and Rommel's former opponent Geyr von Schweppenburg pledged support to him, even against Hitler's orders, while Kluge supported him with much hesitation. Von Rundstedt encouraged Rommel to carry out his plans but refused to do anything himself, remarking that it had to be a man who was still young and loved by the people, while von Manstein was also approached by Rommel but categorically refused, although he did not report them to Hitler either. Peter Hoffmann reports that he also attracted into his orbit officials who had previously refused to support the conspiracy, like Julius Dorpmüller and Karl Kaufmann (According to Russell A. Hart, reliable details of the conversations are now lost, although they certainly met.)

On 17 July, Rommel was incapacitated by an Allied air attack, which many authors describe as a fateful event that drastically altered the outcome of the bomb plot. Writer Ernst Jünger commented: "The blow that felled Rommel ... robbed the plan of the shoulders that were to be entrusted the double weight of war and civil war - the only man who had enough naivety to counter the simple terror that those he was about to go against possessed."

After the failed bomb attack of 20 July, many conspirators were arrested and the dragnet expanded to thousands. Rommel was first implicated when Stülpnagel, after his suicide attempt, repeatedly muttered "Rommel" in delirium. Under torture, Hofacker named Rommel as one of the participants. Additionally, Goerdeler had written down Rommel's name on a list as potential Reich President (according to Stroelin, they had not managed to announce this intention to Rommel yet and he probably never heard of it until the end of his life). On 27 September, Martin Bormann submitted to Hitler a memorandum which claimed that "the late General Stülpnagel, Colonel von Hofacker, Kluge's nephew who has been executed, Lieutenant Colonel Rathgens, and several ... living defendants have testified that Field Marshal Rommel was perfectly in the picture about the assassination plan and has promised to be at the disposal of the New Government." Gestapo agents were sent to Rommel's house in Ulm and placed him under surveillance.

Historian Peter Lieb considers the memorandum, as well as Eberbach's conversation and the testimonies of surviving resistant members (including Hartmann) to be the three key sources that indicate Rommel's support of the assassination plan. He further notes that while Speidel had an interest in promoting his own post-war career, his testimonies should not be dismissed, considering his bravery as an early resistance figure. Remy writes that even more important than Rommel's attitude to the assassination is the fact Rommel had his own plan to end the war. He began to contemplate this plan some months after El Alamein and carried it out with a lonely decision and conviction, and in the end, had managed to bring military leaders in the West to his side.

Rommel's case was turned over to the "Court of Military Honour"—a drumhead court-martial convened to decide the fate of officers involved in the conspiracy. The court included Generalfeldmarshall Wilhelm Keitel, Generalfeldmarshall Gerd von Rundstedt, Generaloberst Heinz Guderian, General der Infanterie Walther Schroth and Generalleutnant Karl-Wilhelm Specht, with General der Infanterie Karl Kriebel and Generalleutnant Heinrich Kirchheim (whom Rommel had fired after Tobruk in 1941) as deputy members and Generalmajor Ernst Maisel as protocol officer. The Court acquired information from Speidel, Hofacker and others that implicated Rommel, with Keitel and Ernst Kaltenbrunner assuming that he had taken part in the subversion. Keitel and Guderian then made the decision that favoured Speidel's case and at the same time shifted the blame to Rommel. By normal procedure, this would lead to Rommel's being brought to Roland Freisler's People's Court, a kangaroo court that always decided in favour of the prosecution. However, Hitler knew that having Rommel branded and executed as a traitor would severely damage morale on the home front. He thus decided to offer Rommel the chance to take his own life.

Two generals from Hitler's headquarters, Wilhelm Burgdorf and Ernst Maisel, visited Rommel at his home on 14 October 1944. Burgdorf informed him of the charges and offered him three options: he could choose to defend himself personally to Hitler in Berlin, or if he refused to do so (which would be taken as an admission of guilt), he would either face the People's Court—which would have been tantamount to a death sentence—or choose a quiet suicide. In the former case, his family would have suffered even before the all-but-certain conviction and execution, and his staff would have been arrested and executed as well. In the latter case, the government would claim that he died a hero and bury him with full military honours, and his family would receive full pension payments. Burgdorf had brought a cyanide capsule.

Rommel denied involvement in the plot, declaring his love for Hitler, and saying that he would gladly serve his "Fatherland" again. "Der Spiegel" notes that he was talking to the "messengers of death" – Burgdorf and Maisel – and some would claim that he was acting out of helpless defense – although "Der Spiegel" thought his love for Hitler was sincere. Remy, however, suggests that Rommel was trying in some way to apologise to Hitler about whom he had conflicting emotions, which Maisel realised and found "disgusting" and "a hypocrisy", because Maisel – a loyal, unapologetic Hitler supporter, even after the war – could not understand how someone could try to kill someone he loved; Rommel's previous replies about his role in the attempt had made Maisel believe that he was part of the plot.

Before the two officers came, Rommel had told his family and friends that he would not reach Berlin alive, considering the fact that his appearing before a court "would be the end of Hitler", too. He now realized that an SS detachment had surrounded his village and he could not contact even his headquarters. With that in mind, Rommel opted to commit suicide, and explained his decision to his wife and son. Wearing his Afrika Korps jacket and carrying his field marshal's baton, Rommel went to Burgdorf's Opel, driven by SS Master Sergeant Heinrich Doose, and was driven out of the village. After stopping, Doose and Maisel walked away from the car, leaving Rommel with Burgdorf. Five minutes later Burgdorf gestured to the two men to return to the car, and Doose noticed that Rommel was slumped over, having taken the cyanide. He died before being taken to the Wagner-Schule field hospital. Ten minutes later, the group telephoned Rommel's wife to inform her of his death. Witnesses were struck by the smile of deep contempt on the dead man's face, never seen in life, and his widow thought it was for Hitler.

The official story of Rommel's death, as reported to the public, stated that Rommel had died of either a heart attack or a cerebral embolism—a complication of the skull fractures he had suffered in the earlier strafing of his staff car. To strengthen the story still further, Hitler ordered an official day of mourning in commemoration. As previously promised, Rommel was given a state funeral. The fact that his state funeral was held in Ulm instead of Berlin had, according to his son, been stipulated by Rommel. Hitler sent Field Marshal von Rundstedt, who was unaware that Rommel had died as a result of Hitler's orders, as his representative at Rommel's funeral. The body was cremated so no incriminating evidence would be left. The truth behind Rommel's death became known to the Allies when intelligence officer Charles Marshall interviewed Rommel's widow, Lucia Rommel, as well as from a letter by Rommel's son Manfred in April 1945.

Rommel's grave is located in Herrlingen, a short distance west of Ulm. For decades after the war on the anniversary of his death, veterans of the Africa campaign, including former opponents, would gather at his tomb in Herrlingen.

On the Italian front in the First World War Rommel was a successful tactician in fast-developing mobile battle, and this shaped his subsequent style as a military commander. He found that taking initiative and not allowing the enemy forces to regroup led to victory. Some authors, like Porch, comment that his enemies were often less organised, second-rate, or depleted, and his tactics were less effective against adequately led, trained and supplied opponents and proved insufficient in the later years of the war. Others point out that through his career, he frequently fought while out-numbered and out-gunned, sometimes overwhelmingly so, while having to deal with internal opponents in Germany who hoped that he would fail.

Rommel is praised by numerous authors as a great leader of men. The historian and journalist Basil Liddell Hart concludes that he was a strong leader worshipped by his troops, respected by his adversaries and deserving to be named as one of the "Great Captains of History". Owen Connelly concurs, writing that "No better exemplar of military leadership can be found" and quoting Friedrich von Mellenthin on the inexplicable mutual understanding that existed between Rommel and his troops. Hitler, though, remarked that, "Unfortunately Field-Marshal Rommel is a very great leader full of drive in times of success, but an absolute pessimist when he meets the slightest problems." Telp criticises Rommel for not extending the benevolence he showed in promoting his own officers' careers to his peers, who he ignored or slighted in his reports.

Taking his opponents by surprise and creating uncertainty in their minds were key elements in Rommel's approach to offensive warfare: he took advantage of sand storms and the dark of night to conceal the movement of his forces. He was aggressive and often directed battle from the front or piloted a reconnaissance aircraft over the lines to get a view of the situation. When the British mounted a commando raid deep behind German lines in an effort to kill Rommel and his staff on the eve of their Crusader offensive, Rommel was indignant that the British expected to find his headquarters 250 miles behind his front. Mellenthin and Harald Kuhn write that at times in North Africa his absence from a position of communication made command of the battles of the Afrika Korps difficult. Mellenthin lists Rommel's counterattack during Operation Crusader as one such instance. Butler concurred, saying that leading from the front is a good concept but Rommel took it so far – he frequently directed the actions of a single company or battalion – that he made communication and coordination between units problematic, as well as risking his life to the extent that he could easily have been killed even by his own artillery. Kesselring also complained about Rommel cruising about the battlefield like a division or corps commander; but Gause and Westphal, supporting Rommel, replied that in the African desert only this method would work and that it was useless to try to restrain Rommel anyway. His staff officers, although admiring towards their leader, complained about the self-destructive Spartan lifestyle that made life harder, diminished his effectiveness and forced them to "bab[y] him as unobtrusively as possible".

For his leadership during the French campaign Rommel received both praise and criticism. Many, such as General Georg Stumme, who had previously commanded 7th Panzer Division, were impressed with the speed and success of Rommel's drive. Others were reserved or critical: Kluge, his commanding officer, argued that Rommel's decisions were impulsive and that he claimed too much credit, by falsifying diagrams or by not acknowledging contributions of other units, especially the Luftwaffe. Some pointed out that Rommel's division took the highest casualties in the campaign. Others point out that in exchange for 2,160 casualties and 42 tanks, it captured more than 100,000 prisoners and destroyed nearly two divisions' worth of enemy tanks (about 450 tanks), vehicles and guns.

Rommel spoke German with a pronounced southern German or Swabian accent. He was not a part of the Prussian aristocracy that dominated the German high command, and as such was looked upon somewhat suspiciously by the Wehrmacht's traditional power structure. Rommel felt a commander should be physically more robust than the troops he led, and should always show them an example. He expected his subordinate commanders to do the same.

Rommel was direct, unbending, tough in his manners, to superiors and subordinates alike, disobedient even to Hitler whenever he saw fit, although gentle and diplomatic to the lower ranks (German and Italian alike) and POWs. Despite being publicity-friendly, he was also shy, introverted, clumsy and overly formal even to his closest aides, judging people only on their merits, although loyal and considerate to those who had proved reliability, and he displayed a surprisingly passionate and devoted side to a very small few (including Hitler) with whom he had dropped the seemingly impenetrable barriers. Many of these traits seemed to manifest even at a very young age.

Rommel's relationship with the Italian High Command in North Africa was generally poor. Although he was nominally subordinate to the Italians, he enjoyed a certain degree of autonomy from them; since he was directing their troops in battle as well as his own, this was bound to cause hostility among Italian commanders. Conversely, as the Italian command had control over the supplies of the forces in Africa, they resupplied Italian units preferentially, which was a source of resentment for Rommel and his staff. Rommel's direct and abrasive manner did nothing to smooth these issues.

While certainly much less proficient than Rommel in their leadership, aggressiveness, tactical outlook and mobile warfare skills, Italian commanders were competent in logistics, strategy and artillery doctrine: their troops were ill-equipped but well-trained. As such, the Italian commanders were repeatedly at odds with Rommel over concerns with issues of supply. Field Marshal Kesselring was assigned Supreme Commander Mediterranean, at least in part to alleviate command problems between Rommel and the Italians. This effort resulted only in partial success, with Kesselring's own relationship with the Italians being unsteady and Kesselring claiming Rommel ignored him as readily as he ignored the Italians. Rommel often went directly to Hitler with his needs and concerns, taking advantage of the favoritism that the Führer displayed towards him and adding to the distrust that Kesselring and the German High Command already had of him.

According to Scianna, opinion among the Italian military leaders was not unanimous. In general, Rommel was a target of criticism and a scapegoat for defeat rather than a glorified figure, with certain generals also trying to replace him as the heroic leader or hijack the Rommel myth for their own benefit. Nevertheless, he never became a hated figure, although the "abandonment myth", despite being repudiated by officers of the X Corps themselves, was long-lived. Many found Rommel's chaotic leadership and emotional character hard to work with, yet the Italians held him in higher regard than other German senior commanders, militarily and personally.

Very different, however, was the perception of Rommel by Italian common soldiers and NCOs, who, like the German field troops, had the deepest trust and respect for him. Paolo Colacicchi, an officer in the Italian Tenth Army recalled that Rommel "became sort of a myth to the Italian soldiers" and that the Bersaglieri baptised him "Rommelito" (This may also have been a reference to both men's small stature: "Rommelito" means "little Rommel" while Romulus means "the little boy from Rome". Rommel himself held a much more generous view about the Italian soldier than about their leadership, towards whom his disdain, deeply rooted in militarism, was not atypical, although unlike Kesselring he was incapable of concealing it. Unlike many of his superiors and subordinates who held racist views, he was usually "kindly disposed" to the Italians in general.

James J. Sadkovich states examples of Rommel for abandoning his Italian units, refusing cooperation, rarely acknowledging their achievements and other improper behaviour towards his Italian allies, Giuseppe Mancinell who was liaison between German and Italian command accused Rommel of blaming Italians for his own errors. Sadkovich names Rommel as "arrogantly ethnocentric" and disdainful towards Italians However, others point out that the Italians under Rommel, in comparison with many of their compatriots in other areas, were better led, supplied, and trained, fighting well as a result, with a ratio of wounded and killed Italians similar to that of the Germans. In one case, a false accusation of Rommel's supposed mistreatment of Italians made by Goering was refuted by Mussolini himself. In 1943, Jodl stated that the only German commander numerous officers and soldiers in Italy would willingly subordinate themselves to would be Rommel.

Many authors describe Rommel as having a reputation of being a chivalrous, humane, and professional officer, and that he earned the respect of both his own troops and his enemies.
Gerhard Schreiber quotes Rommel's orders, issued together with Kesselring: "Sentimentality concerning the Badoglio following gangs ("Banden" in the original, indicating a mob-like crowd) in the uniforms of the former ally is misplaced. Whoever fights against the German soldier has lost any right to be treated well and shall experience toughness reserved for the rabble which betrays friends. Every member of the German troop has to adopt this stance."
Schreiber writes that this exceptionally harsh and, according to him, "hate fuelled" order brutalised the war and was clearly aimed at Italian soldiers, not just partisans.Dennis Showalter writes that "Rommel was not involved in Italy's partisan war, though the orders he issued prescribing death for Italian soldiers taken in arms and Italian civilians sheltering escaped British prisoners do not suggest he would have behaved significantly different from his Wehrmacht counterparts."

According to Maurice Remy, orders issued by Hitler during Rommel's stay in a hospital resulted in massacres in the course of Operation Achse, disarming the Italian forces after the armistice with the Allies in 1943, but according to Remy Rommel treated his Italian opponents with his usual fairness, requiring that the prisoners should be accorded the same conditions as German civilians. Remy opines that an order in which Rommel, in fact protesting against Hitler's directives, called for no "sentimental scruples" against "Badoglio-dependent bandits in uniforms of the once brothers-in-arms" should not be taken out of context. Peter Lieb agrees that the order did not radicalize the war and that the disarmament in Rommel's area of responsibility happened without major bloodshed. Italian internees were sent to Germany for forced labour, but Rommel was unaware of this. Klaus Schmider comments that the writings of Lieb and others succeed in vindicating Rommel "both with regards to his likely complicity in the July plot as well as his repeated refusal to carry out illegal orders."

In the Normandy campaign both Allied and German troops murdered prisoners of war on occasion during June and July 1944. But Rommel withheld Hitler's Commando Order to execute captured commandos from his Army Group B, with his units reporting that they were treating commandos as regular POWs. It is likely that he had acted similarly in North Africa. Historian Szymon Datner argues that Rommel may have been simply trying to conceal the atrocities of Nazi Germany from the Allies. Other authors argue that generosity to opponents was a natural trait of the man. Telp states that Rommel was chivalrous by nature and not prone to order needless violence. Robert Forczyk considers Rommel a true great captain with chivalry. Remy states that although Rommel had heard rumours about massacres while fighting in Africa, his personality, combined with special circumstances, meant that he was not fully confronted with the reality of atrocities before 1944. When Rommel learned about the atrocities that "SS Division Leibstandarte" committed in Italy in September 1943, he allegedly forbade his son from joining the Waffen-SS.

Some authors cite, among other cases, Rommel's naive reaction to events in Poland while he was there: he paid a visit to his wife's uncle, famous Polish priest and patriotic leader, who was murdered within days, but Rommel never understood this and, at his wife's urgings, kept writing letter after letter to Himmler's adjutants asking them to keep track and take care of their relative. Knopp and Mosier agree that he was naive politically, citing his request for a Jewish Gauleiter in 1943. Despite this, Lieb finds it hard to believe that a man in Rommel's position could have known nothing about atrocities, while accepting that locally he was separated from the places where these atrocities occurred. "Der Spiegel" comments that Rommel was simply in denial about what happened around him. Alaric Searle points out that it was the early diplomatic successes and bloodless expansion that blinded Rommel to the true nature of his beloved Führer, whom he then naively continued to support. Scheck believes it may be forever unclear whether Rommel recognized the unprecedented depraved character of the regime.

Historian Richard J. Evans has stated that German soldiers in Tunisia raped Jewish women, and the success of Rommel's forces in capturing or securing Allied, Italian and Vichy French territory in North Africa led to many Jews in these areas being killed by other German institutions as part of the Holocaust. Anti-Jewish and Anti-Arab violence erupted in North Africa when Rommel and Ettore Bastico regained territory there in February 1941 and then again in April
1942. While committed by Italian forces, Patrick Bernhard writes "the Germans were aware of Italian reprisals behind the front lines. Yet, perhaps surprisingly, they seem to have exercised little control over events. The German consul general in Tripoli consulted with Italian state and party officials about possible countermeasures against the natives, but this was the full extent of German involvement. Rommel did not directly intervene, though he advised the Italian authorities to do whatever was necessary to eliminate the danger of riots and espionage; for the German general, the rear areas were to be kept "quiet" at all costs. Thus, although he had no direct hand in the atrocities, Rommel made himself complicit in war crimes by failing to point out that international laws of war strictly prohibited certain forms of retaliation. By giving carte blanche to the Italians, Rommel implicitly condoned, and perhaps even encouraged, their war crimes". In his article "Im Rücken Rommels. Kriegsverbrechen, koloniale Massengewalt und Judenverfolgung in Nordafrika", Bernhard writes that North African campaign was hardly "war without hate" as Rommel described it, and points out rapes of women, ill treatment and executions of captured POWs, as well as racially motivated murders of Arabs, Berbers and Jews, in addition to establishment of concentration camps. Bernhard again cites discussion among the German and Italian authorities about Rommel's position regarding countermeasures against local resurrection (according to them, Rommel wanted to eliminate the danger at all costs) to show that Rommel fundamentally approved of Italian policy in the matter. Bernhard opines that Rommel had informal power over the matter because his military success brought him influence on the Italian authorities. United States Holocaust Memorial Museum describes relationship between Rommel and the proposed Einsatzgruppen Egypt as "problematic". The Museum states that this unit was to be tasked with murdering Jewish population of North Africa, Palestine, and it was to be attached directly to Rommel's Afrika Korps. According to museum Rauff met with Rommel's staff in 1942 as part of preparations for this plan.The Museum states that Rommel was certainly aware that planning was taking place,even if his reaction to it isn't recorded, and while the main proposed Einsatzgruppen were never set in action, smaller units did murder Jews in North Africa.

On the other hand, Christopher Gabel remarks that Richards Evans seems to attempt to prove that Rommel was a war criminal by association but fails to produce evidence that he had actual or constructive knowledge about said crimes. Ben H. Shepherd comments that Rommel showed insight and restraint when dealing with the nomadic Arabs, the only civilians who occasionally intervened into the war and thus risked reprisals as a result. Shepherd cites a request by Rommel to the Italian High Command, in which he complained about excesses against the Arabic population and noted that reprisals without identifying the real culprits were never expedient. According to Caron, German soldiers did steal livestocks from the Arabs though. Aisa Bu Graiem, who worked as foreman and cook for the Luftwaffe recalls that when some Arabs complained, Rommel politely told them that his soldiers did not have enough to eat, but when the war ended they would be compensated. The documentary "Rommel's War" ("Rommels Krieg"), made by Caron and Müllner with advice from Sönke Neitzel, states that even though it is not clear whether Rommel knew about the crimes (in Africa) or not, "his military success made possible forced labor, torture and robbery. Rommel's war is always part of Hitler's war of worldviews, whether Rommel wanted it or not." More specifically, several German historians have revealed existence of plans to exterminate Jews in Egypt and Palestine, if Rommel had succeeded in his goal of invading the Middle East during 1942 by SS unit embedded to Afrika Korps. According to Mallmann and Cüppers, a post-war CIA report described Rommel as having met with Walther Rauff, who was responsible for the unit, and been disgusted after learning about the plan from him and as having sent him on his way; but they conclude that such a meeting is hardly possible as Rauff was sent to report to Rommel at Tobruk on 20 July and Rommel was then 500 km away conducting the First El Alamein. On 29 July, Rauff's unit was sent to Athens, expecting to enter Africa when Rommel crossed the Nile. However, in view of the Axis' deteriorating situation in Africa it returned to Germany in September. Historian Jean-Christoph Caron opines that there is no evidence that Rommel knew or would have supported Rauff's mission; he also believes Rommel bore no direct responsibility regarding the SS's looting of gold in Tunisia. Historian Haim Saadon, Director of the Center of Research on North African Jewry in WWII, goes further, stating that there was no extermination plan: Rauff's documents show that his foremost concern was helping the Wehrmacht to win, and he came up with the idea of forced labour camps in the process. By the time these labour camps were in operation, according to Ben H. Shepherd, Rommel had already been retreating and there is no proof of his contact with the Einsatzkommando. "Haaretz" comments that the CIA report is most likely correct regarding both the interaction between Rommel and Rauff and Rommel's objections to the plan: Rauff's assistant Theodor Saevecke, and declassified information from Rauff's file, both report the same story. "Haaretz" also remarks that Rommel's influence probably softened the Nazi authorities' attitude to the Jews and to the civilian population generally in North Africa.
Rolf-Dieter Müller comments that the war in North Africa, while as bloody as any other war, differed considerable from the war of annihilation in eastern Europe, because it was limited to a narrow coastline and hardly affected the population. Showalter writes that: "From the desert campaign’s beginning, both sides consciously sought to wage a "clean" war—war without hate, as Rommel put it in his reflections. Explanations include the absence of civilians and the relative absence of Nazis; the nature of the environment, which conveyed a "moral simplicity and transparency"; and the control of command on both sides by prewar professionals, producing a British tendency to depict war in the imagery of a game, and the corresponding German pattern of seeing it as a test of skill and a proof of virtue. The nature of the fighting as well diminished the last-ditch, close-quarter actions that are primary nurturers of mutual bitterness. A battalion overrun by tanks usually had its resistance broken so completely that nothing was to be gained by a broken-backed final stand."

Joachim Käppner writes that while the conflict in North Africa was not as bloody as in Eastern Europe,the Afrika Korps committed some war crimes

Historian Martin Kitchen states that the reputation of the Afrika Korps was preserved by circumstances: The sparsely populated desert areas did not lend themselves to ethnic cleansing; the German forces never reached the large Jewish populations in Egypt and Palestine; and in the urban areas of Tunisia and Tripolitania the Italian government constrained the German efforts to discriminate against or eliminate Jews who were Italian citizens. Despite this, the North African Jews themselves believed that it was Rommel who prevented the "Final Solution" from being carried out against them when German might dominated North Africa from Egypt to Morocco. According to Curtis and Remy, 120,000 Jews lived in Algeria, 200,000 in Morocco, about 80,000 in Tunisia. Remy writes that this number was unchanged following the German invasion of Tunisia in 1942 while Curtis notes that 5000 of these Jews would be sent to forced labour camps. and 26,000 in Libya. According to Marshall, Rommel sharply protested the Jewish policies and other immoralities and was an opponent of the Gestapo. He also refused to comply with Hitler's order to execute Jewish POWs. Bryan Mark Rigg writes: "The only place in the army where one might find a place of refuge was in the Deutsches Afrika-Korps (DAK) under the leadership of the "Desert Fox," Field Marshal Erwin Rommel. According to this study's files, his half-Jews were not as affected by the racial laws as most others serving on the European continent." He notes, though, that "Perhaps Rommel failed to enforce the order to discharge half-Jews because he was unaware of it". Captain Horst van Oppenfeld (a staff officer to Colonel Claus von Stauffenberg and a quarter-Jew) says that Rommel did not concern himself with the racial decrees and he had never experienced any trouble caused by his ancestry during his time in the DAK even if Rommel never personally interfered on his behalf.) Another quarter-Jew, Fritz Bayerlein, became a famous general and Rommel's chief-of-staff, despite also being a bisexual, which made his situation even more precarious.

At his 17 June 1944 meeting with Hitler at Margival he protested against the massacre of the citizens of the French town of Oradour-sur-Glane, committed by the 2nd SS Panzer Division "Das Reich", and asked to be allowed to punish the division.

Building the Atlantic Wall was officially the responsibility of the Organisation Todt, which was not under Rommel's command, but he enthusiastically joined the task, protesting slave labour and suggesting that they should recruit French civilians and pay them good wages. Despite this, French civilians and Italian prisoners of war held by the Germans were forced by officials under the Vichy government, the Todt Organization and the SS forces to work on building some of the defences Rommel requested, in appalling conditions according to historian Will Fowler. Although they got basic wages, the workers complained because it was too little and there was no heavy equipment. Robin Neillands and Roderick De Normann report that German soldiers as well as Russian and Polish renegades were used, to avoid using forced labour. German troops worked almost round-the-clock under very harsh conditions, with Rommel's rewards being accordions. (Rommel was himself an eccentric and horrible violinist.) Lieb reports that Rommel felt pity when he saw the suffering of the French in his inspection tour and probably helped to save the lives of thousands of locals.

By the time of Second World War, French colonial troops were seen as symbol of French depravity in Nazi propaganda; Canadian historian Myron Echenberg writes that Rommel, just like Hitler, viewed black French soldiers with particular disdain According to author Ward Rutherford Rommel also held racist views towards British colonial troops from India; Rutherford in his "The biography of Field Marshal Erwin Rommel" writes : "Not even his most sycophantic apologists have been able to evade the conclusion, fully demonstrated by later behavior, that Rommel was a racist who, for example, thought it desperately unfair that the British should employ 'black' – by which he meant Indian – troops against a white adversary." Vaughn Raspberry writes that Rommel and other officers considered it an insult to fight black Africans seeing them as "inferior races"

Bruce Watson comments that whatever racism Rommel might have had at the beginning, it became washed away when fighting in the desert. When he saw it that they were fighting well, he gave the 4th Division of the Indian Army high praise. Rommel and the Germans acknowledge the Gurkhas' fighting ability, although their style leaned more towards ferocity. Once he witnessed German soldiers with throats cut by a khukri knife. Originally, he did not want Chandra Bose's Indian formation (composed of the Allied Indian soldiers), captured by his own troops, to work under his own command. In Normandy though, when they had already become the Indische Freiwilligen Legion der Waffen SS, he visited them and praised them for their efforts (while they still suffered general disrespect within the Wehrmacht). A review on Rutherford's book by the "Pakistan Army Journal" says that the statement is one of many that Rutherford uses, which lack support in authority and analysis. Rommel saying that using the Indians was unfair also should be put in perpspective, considering the disbandment of the battle-hardened 4th Division by the Allies. The BBC, relying upon the research of Dominique Lormier, writes that Rommel praised the colonial troops: "The (French) colonial troops fought with extraordinary determination. The anti-tank teams and tank crews performed with courage and caused serious losses." The BBC remarks that this might be conventional honour as generals usually bestow on the opponents, if only to make their victory more impressive. Reuth comments that Rommel ensured that he and his command would act decently (shown by his treatment towards the Free French prisoners considered partisans by Hitler, the Jews and the coloured men), while distancing himself from Hitler's racist war in the East and deluding himself that Hitler was good, only the Party big shots were evil. The black South African soldiers recount that when they were POWs captured by Rommel, initially they slept and queued for food separately from the whites, until Rommel saw this and told them that brave soldiers should all queue together. Finding this strange coming from a man fighting for Hitler, they adopted this behaviour until they were back to the Union of South Africa, where they were separated again.

There are reports that Rommel acknowledged the Maori soldiers' fighting skills, yet at the same time he complained about their methods which were unfair from the European perspective. When he asked the Commander of the 6th New Zealand Brigade about his division's massacres of the wounded and POWs, the Commander attributed these incidents to the Maoris in his unit. Hew Strachan notes that lapses in practicing the warriors' code of war were usually attributed to ethnic groups outside of Europe with the implication that those from within knew better how to behave (although Strachan opines that such attributions were perhaps true). Nevertheless, according to the website of the 28th Maori Battalion, Rommel always treated them fairly and also showed understanding in the matter of war crimes.

Rick Atkinson criticises Rommel for gaining a looted stamp collection (a bribe from Sepp Dietrich) and a villa taken from Jews. Lucas, Matthews and Remy though describe the contemptuous and angry reaction of Rommel towards Dietrich's act and the lootings and other brutal behaviours of the SS that he had discovered in Italy. Claudia Hecht also explains that although the Stuttgart and Ulm authorities did arrange for the Rommel family to use a villa whose Jewish owners had been forced out two years earlier, for a brief period after their own house had been destroyed by Allied bombing, ownership of it was never transferred to them. Butler notes that Rommel was one of the few who refused large estates and gifts of cash Hitler gave to his generals.

Curiously, recent research by Norman Ohler claims that Rommel's behaviours were heavily influenced by Pervitin which he reportedly took in heavy doses, to such an extent that Ohler refers to him as "the Crystal Fox" ("Kristallfuchs") – playing off the nickname "Desert Fox" famously given to him by the British.

At the beginning, although Hitler and Goebbels took particular notice of Rommel, the Nazi elites had no intent to create one major war symbol (partly out of fear that he would offset Hitler), generating huge propaganda campaigns for not only Rommel but also Gerd von Rundstedt, Walther von Brauchitsch, Eduard Dietl, Sepp Dietrich (the latter two were party members and also strongly supported by Hitler), etc. Nevertheless, a multitude factors—including Rommel's unusual charisma, his talents both in military matters and public relations, the efforts of Goebbels's propaganda machine, and the Allies' participation in mythologizing his life (either for political benefits, sympathy for someone who evoked a romantic archetype, or genuine admiration for his actions)—gradually contributed to Rommel's fame. Spiegel wrote, "Even back then his fame outshone that of all other commanders."

Rommel's victories in France were featured in the German press and in the February 1941 film "Victory in the West," in which Rommel personally helped direct a segment reenacting the crossing of the Somme River. Rommel's victories in 1941 were played up by the Nazi propaganda, even though his successes in North Africa were achieved in arguably one of Germany's least strategically important theaters of World War II. In November 1941, Reich Minister of Propaganda Joseph Goebbels wrote about "the urgent need" to have Rommel "elevated to a kind of popular hero." Rommel, with his innate abilities as a military commander and love of the spotlight, was a perfect fit for the role Goebbels designed for him.

In North Africa, Rommel received help in cultivating his image from Alfred Ingemar Berndt, a senior official at the Reich Propaganda Ministry who had volunteered for military service. Seconded by Goebbels, Berndt was assigned to Rommel's staff and became one of his closest aides. Berndt often acted as liaison between Rommel, the Propaganda Ministry, and the Führer Headquarters. He directed Rommel's photo shoots and filed radio dispatches describing the battles.

In the spring of 1941, Rommel's name began to appear in the British media. In the autumn of 1941 and early winter of 1941/1942, he was mentioned in the British press almost daily. Toward the end of the year, the Reich propaganda machine also used Rommel's successes in Africa as a diversion from the Wehrmacht's challenging situation in the Soviet Union with the stall of Operation Barbarossa. The American press soon began to take notice of Rommel as well, following the country's entry into the war on 11 December 1941, writing that "The British (...) admire him because he beat them and were surprised to have beaten in turn such a capable general." General Auchinleck distributed a directive to his commanders seeking to dispel the notion that Rommel was a "superman". Rommel, no matter how hard the situation was, made a deliberate effort at always spending some time with soldiers and patients, his own and POWs alike, which contributed greatly to his reputation of not only being a great commander but also "a decent chap" among the troops.

The attention of the Western and especially the British press thrilled Goebbels, who wrote in his diary in early 1942: "Rommel continues to be the recognized darling of even the enemies' news agencies." The Field Marshal was pleased by the media attention, although he knew the downsides of having a reputation. Hitler took note of the British propaganda as well, commenting in the summer of 1942 that Britain's leaders must have hoped "to be able to explain their defeat to their own nation more easily by focusing on Rommel".

The Field Marshal was the German commander most frequently covered in the German media, and the only one to be given a press conference, which took place in October 1942. The press conference was moderated by Goebbels and was attended by both domestic and foreign media. Rommel declared: "Today we (...) have the gates of Egypt in hand, and with the intent to act!" Keeping the focus on Rommel distracted the German public from Wehrmacht losses elsewhere as the tide of the war began to turn. He became a symbol that was used to reinforce the German public's faith in an ultimate Axis victory.

In the wake of the successful British offensive in November 1942 and other military reverses, the Propaganda Ministry directed the media to emphasize Rommel's invincibility. The charade was maintained until the spring of 1943, even as the German situation in Africa became increasingly precarious. To ensure that the inevitable defeat in Africa would not be associated with Rommel's name, Goebbels had the Supreme High Command announce in May 1943 that Rommel was on a two-month leave for health reasons. Instead, the campaign was presented by Berndt, who resumed his role in the Propaganda Ministry, as a ruse to tie down the British Empire while Germany was turning Europe into an impenetrable fortress with Rommel at the helm of this success. After the radio program ran in May 1943, Rommel sent Berndt a case of cigars as a sign of his gratitude.
Although Rommel then entered a period without a significant command, he remained a household name in Germany, synonymous with the aura of invincibility. Hitler then made Rommel part of his defensive strategy for Fortress Europe ("Festung Europa") by sending him to the West to inspect fortifications along the Atlantic Wall. Goebbels supported the decision, noting in his diary that Rommel was "undoubtedly the suitable man" for the task. The propaganda minister expected the move to reassure the German public and at the same time to have a negative impact on the Allied forces' morale.

In France, a Wehrmacht propaganda company frequently accompanied Rommel on his inspection trips to document his work for both domestic and foreign audiences. In May 1944 the German newsreels reported on Rommel's speech at a Wehrmacht conference, where he stated his conviction that "every single German soldier will make his contribution against the Anglo-American spirit that it deserves for its criminal and bestial air war campaign against our homeland." The speech led to an upswing in morale and sustained confidence in Rommel.

When Rommel was seriously wounded on 17 July 1944, the Propaganda Ministry undertook efforts to conceal the injury so as not to undermine domestic morale. Despite those, the news leaked to the British press. To counteract the rumors of a serious injury and even death, Rommel was required to appear at 1 August press conference. On 3 August, the German press published an official report that Rommel had been injured in a car accident. Rommel noted in his diary his dismay at this twisting of the truth, belatedly realising how much the Reich propaganda was using him for its own ends.

Rommel was interested in propaganda beyond the promotion of his own image. In 1944, after visiting Rommel in France and reading his proposals on counteracting Allied propaganda, Alfred-Ingemar Berndt remarked: "He is also interested in this propaganda business and wants to develop it by all means. He has even thought and brought out practical suggestions for each program and subject."

Rommel saw the propaganda and education values in his and his nation's deeds (He also did value justice itself; according to Admiral Ruge's diary, Rommel told Ruge: "Justice is the indispensable foundation of a nation. Unfortunately, the higher-ups are not clean. The slaughterings are grave sins.") The key to the successful creating of an image, according to Rommel, was leading by example: "The men tend to feel no kind of contact with a commander who, they know, is sitting somewhere in headquarters. What they want is what might be termed a physical contact with him. In moments of panic, fatigue, or disorganization, or when something out of the ordinary has to be demanded from them, the personal example of the commander works wonders, especially if he has had the wit to create some sort of legend around himself." He urged Axis authorities to treat the Arab with the utmost respect to prevent uprisings behind the front. He protested the use of propaganda at the cost of explicit military benefits though. Ruge suggests that his chief treated his own fame as a kind of weapon.

The political scientist and historian Randall Hansen suggests that Rommel chose his whole command style for the purpose of spreading meritocracy and egalitarianism, as well as Nazi ideals he shared with Hitler because of their common non-aristocratic background. His egalitarianism extended to people of other races: in replying to white South African officers' demands that the black POWs should be housed in separated compounds, he refused, commenting that the black soldiers wore the same uniforms and had fought alongside the whites and thus were their equals. On the other hand, Watson comments that, regarding the Afrika Korps, any Nazi indoctrination was minimised, allowing Rommel the freedom to reinvent his army in his own style. Rommel's proposals were not always practical: in 1943, he surprised Hitler by proposing that a Jew should be made into a Gauleiter to prove to the world that Germany was innocent of accusations that Rommel had heard from the enemy's propaganda regarding the mistreatment of Jews. Hitler replied, "Dear Rommel, you understand nothing about my thinking at all."

Rommel was not a member of the Nazi Party. Rommel and Hitler had a close and genuine, if complicated, personal relationship. Rommel, as other Wehrmacht officers, welcomed the Nazi rise to power. Numerous historians state that Rommel was one of Hitler's favorite generals and that his close relationship with the dictator benefited both his inter-war and war-time career. Robert Citino describes Rommel as "not apolitical" and writes that he owed his career to Hitler, to whom Rommel's attitude was "worshipful", with Messenger agreeing that Rommel owed his tank command, his hero status and other promotions to Hitler's interference and support.

Kesselring described Rommel's own power over Hitler as "hypnotic". In 1944, Rommel himself told Ruge and his wife that Hitler had a kind of irresistible magnetic aura ("magnetismus") and was always seemingly in an intoxicated condition. Maurice Remy identifies that the point at which their relationship became a personal one was 1939, when Rommel proudly announced to his friend Kurt Hesse that he had "sort of forced Hitler to go with me (to the Hradschin Castle in Prague, in an open top car, without another bodyguard), under my personal protection ... He had entrusted himself to me and would never forget me for my excellent advice."

The close relationship between Rommel and Hitler continued following the Western campaign; after Rommel sent to him a specially prepared diary on the 7th Division, he received a letter of thanks from the dictator. (According to Speer, he would normally send extremely unclear reports which annoyed Hitler greatly.) According to Maurice Remy, the relationship, which Remy calls "a dream marriage", only showed the first crack in 1942, and later gradually turned into, in the words of German writer Ernst Jünger (in contact with Rommel in Normandy), "hassliebe" (a love-hate relationship). Ruge's diary and Rommel's letters to his wife show his mood fluctuating wildly regarding Hitler: while he showed disgust towards the atrocities and disappointment towards the situation, he was overjoyed to welcome a visit from Hitler, only to return to depression the next day when faced with reality.

Hitler displayed the same emotions. Amid growing doubts and differences, he would remain eager for Rommel's calls (they had almost daily, hour-long, highly animated conversations, with the preferred topic being technical innovations): he once almost grabbed the telephone out of Linge's hand. But, according to Linge, seeing Rommel's disobedience Hitler also realized his mistake in building up Rommel, whom not only the Afrika Korps but also the German people in general now considered the German God. Hitler tried to fix the dysfunctional relationship many times without results, with Rommel calling his attempts "Sunlamp Treatment", although later he said that "Once I have loved the Führer, and I still do." Remy and "Der Spiegel" remark that the statement was very much genuine, while Watson notes that Rommel believed he deserved to die for his treasonable plan.

Rommel was an ambitious man who took advantage of his proximity to Hitler and willingly accepted the propaganda campaigns designed for him by Goebbels. On one hand, he wanted personal promotion and the realization of his ideals. On the other hand, being elevated by the traditional system that gave preferential treatment to aristocratic officers would be betrayal of his aspiration "to remain a man of the troops". In 1918, Rommel refused an invitation to a prestigious officer training course, and with it, the chance to be promoted to general. Additionally, he had no inclination towards the political route, preferring to remain a soldier ("Nur-Soldat"). He was thus attracted by the Common Man theme which promised to level German society, the glorification of the national community, and the idea of a soldier of common background who served the Fatherland with talent and got rewarded by another common man who embodied the will of the German people. While he had much indignation towards Germany's contemporary class problem, this self-association with the Common Man went along well with his desire to simulate the knights of the past, who also led from the front. (The dominant parent in Rommel's life was his mother Helene, a minor "von" and a loving but ambitious and class-conscious mother who strongly stirred him towards a military career) While Rommel was greatly attached to his profession ("the body and soul of war", a fellow officer commented), he seemed to equally enjoy the idea of peace, as shown by his words to his wife in August 1939: "You can trust me, we have taken part in one World War, but as long as our generation live, there will not be a second", as well as his letter sent to her the night before the Invasion of Poland, in which he expressed (in Maurice Remy's phrase) "boundless optimism": "I still believe the atmosphere will not become more bellicose." Butler remarks that Rommel was center in his politics, leaning a little to the left in his attitude.

Messenger argues that Rommel's attitude towards Hitler changed only after the Allied invasion of Normandy, when Rommel came to realise that the war could not be won, while Maurice Remy suggests that Rommel never truly broke away from the relationship with Hitler but praises him for "always [having] the courage to oppose him whenever his conscience required so". The historian Peter Lieb states that it was not clear whether the threat of defeat was the only reason Rommel wanted to switch sides. The relationship seemed to go significantly downhill after a conversation in July 1943, in which Hitler told Rommel that if they did not win the war, the Germans could rot. Rommel even began to think that it was lucky that his Afrika Korps was now safe as POWs and could escape Hitler's Wagnerian ending. Die Welt comments that Hitler chose Rommel as his favourite because he was apolitical, and that the combination of his military expertise and circumstances allowed Rommel to remain clean.

Rommel's political inclinations were a controversial matter even among the contemporary Nazi elites. Rommel himself, while showing support to some facets of the Nazi ideology and enjoying the propaganda the Nazi machine built around him, was enraged by the Nazi media's effort to portray him as an early Party member and son of a mason, forcing them to correct this misinformation. The Nazi elites were not comfortable with the idea of a national icon who did not wholeheartedly support the regime. Hitler and Goebbels, his main supporters, tended to defend him. When Rommel was being considered for appointment as Commander-in-Chief of the Army in the summer of 1942, Goebbels wrote in his diary that Rommel "is ideologically sound, is not just sympathetic to the National Socialists. He is a National Socialist; he is a troop leader with a gift for improvisation, personally courageous and extraordinarily inventive. These are the kinds of soldiers we need." Despite this, they gradually saw that his grasp of political realities and his views could be very different from theirs. Hitler knew, though, that Rommel's optimistic and combative character was indispensable for his war efforts. When Rommel lost faith in the final victory and Hitler's leadership, Hitler and Goebbels tried to find an alternative in Manstein to remedy the fighting will and "political direction" of other generals but did not succeed.

Meanwhile, officials who did not like Rommel, such as Bormann and Schirach, whispered to each other that he was not a Nazi at all. Rommel's relationship to the Nazi elites, other than Hitler and Goebbels, was mostly hostile, although even powerful people like Bormann and Himmler had to tread carefully around Rommel. Himmler, who played a decisive role in Rommel's death, tried to blame Keitel and Jodl for the deed. And in fact the deed was initiated by them. They deeply resented Rommel's meteoric rise and had long feared that he would become the Commander-in-Chief. (Hitler also played innocent by trying to erect a monument for the national hero, on 7 March 1945) Franz Halder, after concocting several schemes to rein in Rommel through people like Paulus and Gause to no avail (even willing to undermine German operations and strategy in the process for the sole purpose of embarrassing him), concluded that Rommel was a madman with whom no one dared to cross swords because of "his brutal methods and his backing from the highest levels". (Rommel imposed a high number of courts martial, but according to Westphal, he never signed the final order. Owen Connelly comments that he could afford easy discipline because of his charisma). Rommel for his part was highly critical of Himmler, Halder, the High Command and particularly Goering who Rommel at one point called his "bitterest enemy". Hitler realized that Rommel attracted the elites' negative emotions to himself, in the same way he generated optimism in the common people. Depending on the case, Hitler manipulated or exacerbated the situation in order to benefit himself, although he originally had no intent of pushing Rommel to the point of destruction. (Even when informed of Rommel's involvement in the plot, hurt and vengeful, Hitler at first wanted to retire Rommel, and eventually offered him a last-minute chance to explain himself and refute the claims, which Rommel apparently did not take advantage of.) Ultimately Rommel's enemies worked together to bring him down.

Maurice Remy concludes that, unwillingly and probably without ever realising it, Rommel was part of a murderous regime, although he never actually grasped the core of National Socialism. Peter Lieb sees Rommel as a person who could not be put into a single drawer, although problematic by modern moral standards, and suggests people should personally decide for themselves whether Rommel should remain a role model or not. He was a Nazi general in some aspects, considering his support for the leader cult (Führerkult) and the Volksgemeinschaft, but he was not an antisemite, nor a war criminal, nor a radical ideological fighter. Samuel W. Mitcham states that Rommel "after years of propaganda" was antisemitic and worried about "Jewish problem", Jewish "clannishness" and supposed Jewish wealth in Germany, Mitcham however states that main concern for Rommel was his career and family, and he didn't devote much focus to the issue, and being stationed in Africa knew little about their treatment in Europe. Historian Cornelia Hecht remarks "It is really hard to know who the man behind the myth was," noting that in numerous letters he wrote to his wife during their almost 30-year marriage, he commented little on political issues as well as his personal life as a husband and a father.

According to some revisionist authors, an assessment of Rommel's role in history has been hampered by views of Rommel that were formed, at least in part, for political reasons, creating what these historians have called the "Rommel myth". The interpretation considered by some historians to be a myth is the depiction of the Field Marshal as an apolitical, brilliant commander and a victim of the Third Reich who participated in the 20 July plot against Adolf Hitler. There are a notable number of authors who refer to "Rommel Myth" or "Rommel Legend" in a neutral or positive manner though.

The seeds of the myth can be found first in Rommel's drive for success as a young officer in World War I and then in his popular 1937 book "Infantry Attacks," which was written in a style that diverged from the German military literature of the time and became a bestseller.

The myth then took shape during the opening years of World War II, as a component of Nazi propaganda to praise the Wehrmacht and instill optimism in the German public, with Rommel's willing participation. When Rommel came to North Africa, it was picked up and disseminated in the West by the British press as the Allies sought to explain their continued inability to defeat the Axis forces in North Africa. The British military and political figures contributed to the heroic image of the man as Rommel resumed offensive operations in January 1942 against the British forces weakened by redeployments to the Far East. During parliamentary debate following the fall of Tobruk, Churchill described Rommel as an "extraordinary bold and clever opponent" and a "great field commander".

According to "Der Spiegel" following the war's end, West Germany yearned for father figures who were needed to replace the former ones who had been unmasked as criminals. Rommel was chosen because he embodied the decent soldier, cunning yet fair-minded, and if guilty by association, not so guilty that he became unreliable, and additionally, former comrades reported that he was close to the Resistance. While everyone else was disgraced, his star became brighter than ever, and he made the historically unprecedented leap over the threshold between eras: from Hitler's favourite general to the young republic's hero. Cornelia Hecht notes that despite the change of times, Rommel has become the symbol of different regimes and concepts, which is paradoxical, whoever the man he really was. Ulrich vom Hagen reports that Rommel, for the admiration shown towards him by all sides after the war, was used as a unity symbol that led to the "elegant settlement" of the conflict between fascistic, small-bourgeois elements and the aristocratic traditionalists during the early years after the formation of the Bundeswehr. Simon Ball describes how various elements in the German and British armies and governments extensively used Rommel's image in dealing with their inner struggles, promoting aspects of his that each group associated with themselves. Eric Dorman-Smith claimed that it was a "pity we could not have combined with Rommel to clean up the whole mess on both sides". Already in September 1944, the officer Heinz Eugen Eberbach (later a leading figure in the Bundeswehr) anticipated that the Allied victors would have to turn to Rommel and men like him, because he was accepted by both the old regime and the working class, whom the English would not be able to win over by telling them: "The entire previous system is rotten to the core".

At the same time, the Western Allies, and particularly the British, depicted Rommel as the "good German". His reputation for conducting a clean war was used in the interest of the West German rearmament and reconciliation between the former enemies—Britain and the United States on one side and the new Federal Republic of Germany on the other. When Rommel's alleged involvement in the plot to kill Hitler became known after the war, his stature was enhanced in the eyes of his former adversaries. Rommel was often cited in Western sources as a patriotic German willing to stand up to Hitler. Churchill wrote about him in 1950: "[Rommel] (...) deserves our respect because, although a loyal German soldier, he came to hate Hitler and all his works and took part in the conspiracy of 1944 to rescue Germany by displacing the maniac and tyrant."

The German rearmament of the early 1950s was highly dependent on the moral rehabilitation that the Wehrmacht needed. The journalist and historian Basil Liddell Hart, an early proponent of these two interconnected initiatives, provided the first widely available source on Rommel in his 1948 book on Hitler's generals, updated in 1951, portraying Rommel in a positive light and as someone who stood apart from the regime.

The other foundational text was the influential and laudatory 1950 biography "" by Brigadier Desmond Young. Young extensively interviewed Rommel's widow and collaborated with several individuals who had been close to Rommel, including Hans Speidel. The manner of Rommel's death had led to the assumption that he had not been a supporter of Nazism, to which Young subscribed. The reception of "The Desert Fox" in Britain was enthusiastic, with the book going through eight editions in a year. Young's biography was another step in the development of the Rommel myth – with Rommel emerging as an active, if not a leading, plotter. Speidel contributed as well, starting from the early 1950s to bring up Rommel's and his own role in the plot, boosting his [Speidel's] suitability for a future role in the new military force of the Federal Republic, the Bundeswehr, and then in NATO.

Further in 1953 was the publication of Rommel's writings of the war period as "The Rommel Papers," edited by Liddell Hart. The book contributed to the perception of Rommel as a brilliant commander; in an introduction, Liddell Hart drew comparisons between Rommel and Lawrence of Arabia, "two masters of desert warfare". Liddell Hart had a personal interest in the work: by having coaxed Rommel's widow to include material favorable to himself, he could present Rommel as his "pupil". The controversy was described by the political scientist John Mearsheimer, who concluded that, by "manipulating history", Liddell Hart was in a position to show that he was at the root of the dramatic German success in 1940.

According to Mark Connelly, Young and Liddell Hart laid the foundation for the Anglo-American myth, which consisted of three themes: Rommel's ambivalence towards Nazism; his military genius; and the emphasis of the chivalrous nature of the fighting in North Africa. Their works lent support to the image of the "clean Wehrmacht" and were generally not questioned, since they came from British authors, rather than German revisionists.

Historian Bruce Allen Watson offers his interpretation of the myth, encompassing the foundation laid down by the Nazi propaganda machine. According to Watson, the most dominant element is Rommel the Superior Soldier; the second being Rommel the Common Man; and the last one Rommel the Martyr. The German news magazine "Der Spiegel" described the myth in 2007 as "Gentleman warrior, military genius".

During recent years, historians' opinions on Rommel have become more diversified, with some aspects of his image being the target of revisionism more frequently than the others. According to the prominent German historian Hans-Ulrich Wehler, the modern consensus agrees with post-war sources that Rommel treated the Allied captives decently, and he personally thinks that the movie "Rommel" does not overstate his conscience. Also according to Wehler, scholars in England and the US still show a lot of admiration towards Rommel the military commander. Some authors, notably Wolfgang Proske, see Rommel as a criminal whose memorials should be removed, although these represent the unorthodox minority (which is admitted by Proske). Perry and Massari note that the majority of historians continue to describe Rommel as a brilliant, chivalrous commander.

Modern historians who agree with the image of the apolitical, chivalrous genius also have different opinions regarding details. Smith and Bierman opine that Rommel might be considered an honourable man in his limited way but in a deeply dishonourable cause, and that he played the game of war with no more hatred for his opponent than a rugby team captain might feel for his opposite number. Butler states that Rommel's idealistic character led to grave misjudgements because he refused to let anything compromise it, and also that although he had a sense of strategy that developed greatly during the war, he lacked a philosophy of war. German historian Wolfgang Proske states that Rommel's chivalry only showed itself to opponents that Nazi ideology viewed as Aryan; in other cases he followed the racist principles of Nazis

According to some modern scholars, he was much more complex than the figure that has been firmly established in post-war reputation.
Caddick-Adams writes that Rommel was a "complicated man of many contradictions," while Beckett notes that "Rommel's myth (...) has proved remarkably resilient" and that more work is needed to put him in proper historical context. Watson opines that historians often portray Rommel as someone they want him to be, "coward ... hero, fool, villain or hypocrite," and that he seemed to be all of these things, except coward, with perhaps a naive loyalty. Hansen counters that Rommel was hardly naive, always judged military and political situations with cold objectivity, and shared a lot of characteristics with Hitler, an opinion shared by psychoanalyst and historian Geoffrey Cocks who writes that Rommel "embodies the modern synergy of technical expertise and self-promotion ... arriviste, ... professionally ambitious, adept at cultivating a mass media image ... like Hitler".

There is also, especially in Germany, an increasing tendency to portray Rommel as someone who cannot be explained in concrete details yet. However, these modern authors, while respecting the man and his mythical aura, are not afraid to show his questionable traits or point out the horrible (including the possible) consequences of his "politically extremely naive" actions that perhaps would not be fitting of a role model, and allow living witnesses who might portray Rommel in a negative light to speak in documentaries about him, to the extent some, like , consider excessive and unbalanced (Storbeck states that there are many other witnesses who will provide the opposite views, and also questions the use of an extremely ill Manfred Rommel to achieve a portrayal filmmakers want).

Rommel was famous in his lifetime, including among his adversaries. His tactical prowess and decency in the treatment of Allied prisoners earned him the respect of opponents including Claude Auchinleck, Archibald Wavell, George S. Patton, and Bernard Montgomery.

Rommel's military reputation has been controversial. While nearly all military practitioners acknowledge Rommel's excellent tactical skills and personal bravery, some, such as U.S. major general and military historian David T. Zabecki of the United States Naval Institute, considers Rommel's performance as an operational level commander to be highly overrated. He argues that other officers share this belief. General Klaus Naumann, who served as Chief of Staff of the Bundeswehr, agrees with the military historian Charles Messenger that Rommel had challenges at the operational level, and states that Rommel's violation of the unity of command principle, bypassing the chain of command in Africa, was unacceptable and contributed to the eventual operational and strategic failure in North Africa. The German biographer Wolf Heckmann describes Rommel as "the most overrated commander of an army in world history".

Nevertheless, there is also a notable number of officers who admire his methods, like Norman Schwarzkopf who describes Rommel as a "genius at battles of movement" and explains that "Look at Rommel. Look at North Africa, the Arab-Israeli wars, and all the rest of them. A war in the desert is a war of mobility and lethality. It's not a war where straight lines are drawn in the sand and [you] say, 'I will defend here or die." Ariel Sharon deemed the German military model used by Rommel to be superior to the British model used by Montgomery. His compatriot Moshe Dayan likewise considered Rommel a model and icon. Wesley Clark states that "Rommel's military reputation, though, has lived on, and still sets the standard for a style of daring, charismatic leadership to which most officers aspire." During the recent desert wars, Rommel's military theories and experiences attracted great interest from policy makers and military instructors. Chinese military leader Sun Li-jen had the laudatory nickname "Rommel of the East". The Bundeswehr and Germany's NATO partners recognize Rommel as the modern knight of the Bundeswehr, a highly successful operator of military arts and an apolitical, chivalrous soldier (with several leaders of the Bundeswehr like , Hartmut Bagger and declaring him as their personal role model). This ideal of modern knighthood is connected and combined with the anachronistic Miles Christianus model, the more recent "Miles Protector" model, the "Soldier-Statesman" concept, and the traditional monofunctional combatant.

Certain modern military historians, such as Larry T. Addington, Niall Barr, Douglas Porch and Robert Citino, are skeptical of Rommel as an operational, let alone strategic level commander. They point to Rommel's lack of appreciation for Germany's strategic situation, his misunderstanding of the relative importance of his theatre to the German High Command, his poor grasp of logistical realities, and, according to the historian Ian Beckett, his "penchant for glory hunting". Citino credits Rommel's limitations as an operational level commander as "materially contributing" to the eventual demise of the Axis forces in North Africa, while Addington focuses on the struggle over strategy, whereby Rommel's initial brilliant success resulted in "catastrophic effects" for Germany in North Africa. Porch highlights Rommel's "offensive mentality", symptomatic of the Wehrmacht commanders as a whole in the belief that the tactical and operational victories would lead to strategic success. Compounding the problem was the Wehrmacht's institutional tendency to discount logistics, industrial output and their opponents' capacity to learn from past mistakes.

The historian Geoffrey P. Megargee points out Rommel's playing the German and Italian command structures against each other to his advantage. Rommel used the confused structure (the OKW (Supreme Command of the Wehrmacht), the OKH (Supreme High Command of the Army) and the Italian Supreme Command) to disregard orders that he disagreed with or to appeal to whatever authority he felt would be most sympathetic to his requests.
Some historians take issue with Rommel's absence from Normandy on the day of the Allied invasion, 6 June 1944. He had left France on 5 June and was at home on the 6th celebrating his wife's birthday. (According to Rommel, he planned to proceed to see Hitler the next day to discuss the situation in Normandy). Zabecki calls his decision to leave the theatre in view of an imminent invasion "an incredible lapse of command responsibility". Lieb remarks that Rommel displayed real mental agility, but the lack of an energetic commander, together with other problems, caused the battle largely not to be conducted in his concept (which is the opposite of the German doctrine), although the result was still better than Geyr's plan. Lieb also opines that while his harshest critics (who mostly came from the General Staff) often said that Rommel was overrated or not suitable for higher commands, envy was a big factor here.
T.L. McMahon argues that Rommel no doubt possessed operational vision, however Rommel did not have the strategic resources to effect his operational choices while his forces provided the tactical ability to accomplish his goals, and the German staff and system of staff command were designed for commanders who led from the front, and in some cases he might have chosen the same options as Montgomery (a reputedly strategy-oriented commander) had he been put in the same conditions. According to Steven Zaloga, tactical flexibility was a great advantage of the German system, but in the final years of the war, Hitler and his cronies like Himmler and Goering had usurped more and more authority at the strategic level, leaving professionals like Rommel increasing constraints on their actions. Martin Blumenson considers Rommel a general with a compelling view of strategy and logistics, which was demonstrated through his many arguments with his superiors over such matters, although Blumenson also thinks that what distinguished Rommel was his boldness, his intuitive feel for the battlefield.(Upon which Schwarzkopf also comments "Rommel had a feel for the battlefield like no other man.")

Joseph Forbes comments that: "The complex, conflict-filled interaction between Rommel and his superiors over logistics, objectives and priorities should not be used to detract from Rommel's reputation as a remarkable military leader", because Rommel was not given powers over logistics, and because if only generals who attain strategic-policy goals are great generals, such highly regarded commanders as Robert E. Lee, Hannibal, Charles XII would have to be excluded from that list. General Siegfried F. Storbeck, Deputy Inspector General of the Bundeswehr (1987–1991), remarks that, Rommel's leadership style and offensive thinking, although carrying inherent risks like losing the overview of the situation and creating overlapping of authority, have been proved effective, and have been analysed and incorporated in the training of officers by "us, our Western allies, the Warsaw Pact, and even the Israel Defense Forces". Maurice Remy and Samuel W. Mitcham both defend his strategic decision regarding Malta as, although risky, the only logical choice. Mitcham also takes note of the fact that the British C-in-C actually feared that the German leadership would embark on Rommel's strategic plans regarding the Suez Canal instead of that of Hitler.

Rommel was among the few Axis commanders (the others being Isoroku Yamamoto and Reinhard Heydrich) who were targeted for assassination by Allied planners. Two attempts were made, the first being Operation Flipper in North Africa in 1941, and the second being Operation Gaff in Normandy in 1944.

While at Cadet School in 1911, Rommel met and became engaged to 17-year-old Lucia (Lucie) Maria Mollin (1894–1971). While stationed in Weingarten in 1913, Rommel developed a relationship with Walburga Stemmer, which produced a daughter, Gertrude, born 8 December 1913. Because of elitism in the officer corps, Stemmer's working-class background made her unsuitable as an officer's wife, and Rommel felt honour-bound to uphold his previous commitment to Mollin. With Mollin's cooperation, he accepted financial responsibility for the child. Rommel and Mollin were married in November 1916 in Danzig. Rommel's marriage was a happy one, and he wrote his wife at least one letter every day while he was in the field.

After the end of the First World War, the couple settled initially in Stuttgart, and Stemmer and her child lived with them. Gertrude was referred to as Rommel's niece, a fiction that went unquestioned because of the enormous number of women widowed during the war. Walburga died suddenly in October 1928, and Gertrude remained a member of the household until Rommel's death in 1944. The incident with Walburga seemed to affect Rommel for the rest of his life: he would always keep women distant. A son, Manfred Rommel, was born on 24 December 1928, later served as Mayor of Stuttgart from 1974 to 1996.


The German Army's largest base, the Field Marshal Rommel Barracks, Augustdorf, is named in his honour; at the dedication in 1961 his widow Lucie and son Manfred Rommel were guests of honour. The Rommel Barracks, Dornstadt, was also named for him in 1965. A third base named for him, the Field Marshal Rommel Barracks, Osterode, closed in 2004. A German Navy Lütjens-class destroyer, "Rommel", was named for him in 1969 and christened by his widow; the ship was decommissioned in 1998.

Numerous streets in Germany, especially in Rommel's home state of Baden-Württemberg, are named in his honor, including the street near where his last home was located. The Rommel Memorial was erected in Heidenheim in 1961. The Rommel Museum opened in 1989 in the Villa Lindenhof in Herrlingen; there is also a Rommel Museum in Mersa Matruh in Egypt which opened in 1977, and which is located in one of Rommel's former headquarters; various other localities and establishments in Mersa Matruh, including Rommel Beach, are also named for Rommel. The reason for the naming is that he respected the Bedouins' traditions and the sanctity of their homes (he always kept his troops at least 2 kilometers from their houses) and refused to poison the wells against the Allies, fearing doing so would harm the population.

In Italy, the annual marathon tour "Rommel Trail", which is sponsored by the Protezione Civile and the autonomous region of Friuli Venezia Giulia through its tourism agency, celebrates Rommel and the Battle of Caporetto. The naming and sponsoring (at that time by the center-left PD) was criticized by the politician Giuseppe Civati in 2017.


Informational notes
Citations
Bibliography

Further reading


</doc>
<doc id="9518" url="https://en.wikipedia.org/wiki?curid=9518" title="Edmund Husserl">
Edmund Husserl

Edmund Gustav Albrecht Husserl ( , ; ; 8 April 1859 – 27 April 1938) was a German philosopher who established the school of phenomenology. In his early work, he elaborated critiques of historicism and of psychologism in logic based on analyses of intentionality. In his mature work, he sought to develop a systematic foundational science based on the so-called phenomenological reduction. Arguing that transcendental consciousness sets the limits of all possible knowledge, Husserl redefined phenomenology as a transcendental-idealist philosophy. Husserl's thought profoundly influenced 20th-century philosophy, and he remains a notable figure in contemporary philosophy and beyond.

Husserl studied mathematics, taught by Karl Weierstrass and Leo Königsberger, and philosophy taught by Franz Brentano and Carl Stumpf. He taught philosophy as a "Privatdozent" at Halle from 1887, then as professor, first at Göttingen from 1901, then at Freiburg from 1916 until he retired in 1928, after which he remained highly productive. In 1933, due to racial laws, having been born to a Jewish family, he was expelled from the library of the University of Freiburg, and months later resigned from the Deutsche Akademie. Following an illness, he died in Freiburg in 1938.

Husserl was born in 1859 in Prostějov, a town in the Margraviate of Moravia, which was then in the Austrian Empire, and which today is Prostějov in the Czech Republic. He was born into a Jewish family, the second of four children. His father was a milliner. His childhood was spent in Prostějov, where he attended the secular elementary school. Then Husserl traveled to Vienna to study at the "Realgymnasium" there, followed next by the Staatsgymnasium in Olomouc (Ger.: Olmütz).

At the University of Leipzig from 1876 to 1878, Husserl studied mathematics, physics, and astronomy. At Leipzig he was inspired by philosophy lectures given by Wilhelm Wundt, one of the founders of modern psychology. Then he moved to the Frederick William University of Berlin (the present-day Humboldt University of Berlin) in 1878 where he continued his study of mathematics under Leopold Kronecker and the renowned Karl Weierstrass. In Berlin he found a mentor in Thomas Masaryk, then a former philosophy student of Franz Brentano and later the first president of Czechoslovakia. There Husserl also attended Friedrich Paulsen's philosophy lectures. In 1881 he left for the University of Vienna to complete his mathematics studies under the supervision of Leo Königsberger (a former student of Weierstrass). At Vienna in 1883 he obtained his PhD with the work "Beiträge zur Variationsrechnung" ("Contributions to the Calculus of Variations").

Evidently as a result of his becoming familiar with the New Testament during his twenties, Husserl asked to be baptized into the Lutheran Church in 1886. Husserl's father Adolf had died in 1884. Herbert Spiegelberg writes, "While outward religious practice never entered his life any more than it did that of most academic scholars of the time, his mind remained open for the religious phenomenon as for any other genuine experience." At times Husserl saw his goal as one of moral "renewal". Although a steadfast proponent of a radical and rational "autonomy" in all things, Husserl could also speak "about his vocation and even about his mission under God's will to find new ways for philosophy and science," observes Spiegelberg.

Following his PhD in mathematics, Husserl returned to Berlin to work as the assistant to Karl Weierstrass. Yet already Husserl had felt the desire to pursue philosophy. Then professor Weierstrass became very ill. Husserl became free to return to Vienna where, after serving a short military duty, he devoted his attention to philosophy. In 1884 at the University of Vienna he attended the lectures of Franz Brentano on philosophy and philosophical psychology. Brentano introduced him to the writings of Bernard Bolzano, Hermann Lotze, J. Stuart Mill, and David Hume. Husserl was so impressed by Brentano that he decided to dedicate his life to philosophy; indeed, Franz Brentano is often credited as being his most important influence, e.g., with regard to intentionality. Following academic advice, two years later in 1886 Husserl followed Carl Stumpf, a former student of Brentano, to the University of Halle, seeking to obtain his habilitation which would qualify him to teach at the university level. There, under Stumpf's supervision, he wrote "Über den Begriff der Zahl" ("On the Concept of Number") in 1887, which would serve later as the basis for his first important work, "Philosophie der Arithmetik" (1891).

In 1887 Husserl married Malvine Steinschneider, a union that would last over fifty years. In 1892 their daughter Elizabeth was born, in 1893 their son Gerhart, and in 1894 their son Wolfgang. Elizabeth would marry in 1922, and Gerhart in 1923; Wolfgang, however, became a casualty of the First World War. Gerhart would become a philosopher of law, contributing to the subject of comparative law, teaching in the United States and after the war in Austria.

Following his marriage Husserl began his long teaching career in philosophy. He started when he was in 1887 as a "Privatdozent" at the University of Halle. In 1891 he published his "Philosophie der Arithmetik. Psychologische und logische Untersuchungen" which, drawing on his prior studies in mathematics and philosophy, proposed a psychological context as the basis of mathematics. It drew the adverse notice of Gottlob Frege, who criticized its psychologism.

In 1901 Husserl with his family moved to the University of Göttingen, where he taught as "extraordinarius professor". Just prior to this a major work of his, "Logische Untersuchungen" (Halle, 1900–1901), was published. Volume One contains seasoned reflections on "pure logic" in which he carefully refutes "psychologism". This work was well received and became the subject of a seminar given by Wilhelm Dilthey; Husserl in 1905 traveled to Berlin to visit Dilthey. Two years later in Italy he paid a visit to Franz Brentano his inspiring old teacher and to Constantin Carathéodory the mathematician. Kant and Descartes were also now influencing his thought. In 1910 he became joint editor of the journal "Logos". During this period Husserl had delivered lectures on "internal time consciousness", which several decades later his former student Heidegger edited for publication.

In 1912 at Freiburg the journal "Jahrbuch für Philosophie und Phänomenologische Forschung" ("Yearbook for Philosophy and Phenomenological Research") was founded by Husserl and his school, and which published articles of their phenomenological movement from 1913 to 1930. His important work "Ideen" was published in its first issue (Vol. 1, Issue 1, 1913). Before beginning "Ideen" Husserl's thought had reached the stage where "each subject is 'presented' to itself, and to each all others are 'presentiated' ("Vergegenwärtigung"), not as parts of nature but as pure consciousness." "Ideen" advanced his transition to a "transcendental interpretation" of phenomenology, a view later criticized by, among others, Jean-Paul Sartre. In "Ideen" Paul Ricœur sees the development of Husserl's thought as leading "from the psychological cogito to the transcendental cogito." As phenomenology further evolves, it leads (when viewed from another vantage point in Husserl's 'labyrinth') to "transcendental subjectivity". Also in "Ideen" Husserl explicitly elaborates the phenomenological and eidetic reductions. In 1913 Karl Jaspers visited Husserl at Göttingen.

In October 1914 both his sons were sent to fight on the Western Front of World War I and the following year one of them, Wolfgang Husserl, was badly injured. On 8 March 1916, on the battlefield of Verdun, Wolfgang was killed in action. The next year his other son Gerhart Husserl was wounded in the war but survived. His own mother Julia died. In November 1917 one of his outstanding students and later a noted philosophy professor in his own right, Adolf Reinach, was killed in the war while serving in Flanders.

Husserl had transferred in 1916 to the University of Freiburg (in Freiburg im Breisgau) where he continued bringing his work in philosophy to fruition, now as a full professor. Edith Stein served as his personal assistant during his first few years in Freiburg, followed later by Martin Heidegger from 1920 to 1923. The mathematician Hermann Weyl began corresponding with him in 1918. Husserl gave four lectures on Phenomenological method at University College, London in 1922. The University of Berlin in 1923 called on him to relocate there, but he declined the offer. In 1926 Heidegger dedicated his book "Sein und Zeit" ("Being and Time") to him "in grateful respect and friendship." Husserl remained in his professorship at Freiburg until he requested retirement, teaching his last class on 25 July 1928. A "Festschrift" to celebrate his seventieth birthday was presented to him on 8 April 1929.

Despite retirement, Husserl gave several notable lectures. The first, at Paris in 1929, led to "Méditations cartésiennes" (Paris 1931). Husserl here reviews the phenomenological epoché (or phenomenological reduction), presented earlier in his pivotal "Ideen" (1913), in terms of a further reduction of experience to what he calls a 'sphere of ownness.' From within this sphere, which Husserl enacts in order to show the impossibility of solipsism, the transcendental ego finds itself always already paired with the lived body of another ego, another monad. This 'a priori' interconnection of bodies, given in perception, is what founds the interconnection of consciousnesses known as transcendental intersubjectivity, which Husserl would go on to describe at length in volumes of unpublished writings. There has been a debate over whether or not Husserl's description of ownness and its movement into intersubjectivity is sufficient to reject the charge of solipsism, to which Descartes, for example, was subject. One argument against Husserl's description works this way: instead of infinity and the Deity being the ego's gateway to the Other, as in Descartes, Husserl's ego in the "Cartesian Meditations" itself becomes transcendent. It remains, however, alone (unconnected). Only the ego's grasp "by analogy" of the Other (e.g., by conjectural reciprocity) allows the possibility for an 'objective' intersubjectivity, and hence for community.

In 1933, the racial laws of the new Nazi regime were enacted. On 6 April Husserl was banned from using the library at the University of Freiburg, or any other academic library; the following week, after a public outcry, he was reinstated. Yet his colleague Heidegger was elected Rector of the university on 21–22 April, and joined the Nazi Party. By contrast, in July Husserl resigned from the Deutsche Akademie.
Later Husserl lectured at Prague in 1935 and Vienna in 1936, which resulted in a very differently styled work that, while innovative, is no less problematic: "Die Krisis" (Belgrade 1936). Husserl describes here the cultural crisis gripping Europe, then approaches a philosophy of history, discussing Galileo, Descartes, several British philosophers, and Kant. The apolitical Husserl before had specifically avoided such historical discussions, pointedly preferring to go directly to an investigation of consciousness. Merleau-Ponty and others question whether Husserl here does not undercut his own position, in that Husserl had attacked in principle historicism, while specifically designing his phenomenology to be rigorous enough to transcend the limits of history. On the contrary, Husserl may be indicating here that historical traditions are merely features given to the pure ego's intuition, like any other. A longer section follows on the "lifeworld" ["Lebenswelt"], one not observed by the objective logic of science, but a world seen in our subjective experience. Yet a problem arises similar to that dealing with 'history' above, a chicken-and-egg problem. Does the lifeworld contextualize and thus compromise the gaze of the pure ego, or does the phenomenological method nonetheless raise the ego up transcendent? These last writings presented the fruits of his professional life. Since his university retirement Husserl had "worked at a tremendous pace, producing several major works."

After suffering a fall the autumn of 1937, the philosopher became ill with pleurisy. Edmund Husserl died at Freiburg on 27 April 1938, having just turned 79. His wife Malvine survived him. Eugen Fink, his research assistant, delivered his eulogy. Gerhard Ritter was the only Freiburg faculty member to attend the funeral, as an anti-Nazi protest.

Husserl was incorrectly rumoured to have been denied the use of the library at Freiburg as a result of the anti-Jewish legislation of April 1933. However, among other disabilities Husserl was unable to publish his works in Nazi Germany [see above footnote to "Die Krisis" (1936)]. It was also rumoured that his former pupil Martin Heidegger informed Husserl that he was discharged, but it was actually the previous rector.
Apparently Husserl and Heidegger had moved apart during the 1920s, which became clearer after 1928 when Husserl retired and Heidegger succeeded to his university chair. In the summer of 1929 Husserl had studied carefully selected writings of Heidegger, coming to the conclusion that on several of their key positions they differed: e.g., Heidegger substituted "Dasein" ["Being-there"] for the pure ego, thus transforming phenomenology into an anthropology, a type of psychologism strongly disfavored by Husserl. Such observations of Heidegger, along with a critique of Max Scheler, were put into a lecture Husserl gave to various "Kant Societies" in Frankfurt, Berlin, and Halle during 1931 entitled "Phänomenologie und Anthropologie".

In the war-time 1941 edition of Heidegger's primary work, "Being and Time" ("Sein und Zeit", first published in 1927), the original dedication to Husserl was removed. This was not due to a negation of the relationship between the two philosophers, however, but rather was the result of a suggested censorship by Heidegger's publisher who feared that the book might otherwise be banned by the Nazi regime. The dedication can still be found in a footnote on page 38, thanking Husserl for his guidance and generosity. Husserl, of course, had died three years earlier. In post-war editions of "Sein und Zeit" the dedication to Husserl is restored. The complex, troubled, and sundered philosophical relationship between Husserl and Heidegger has been widely discussed.

On 4 May 1933, Professor Edmund Husserl addressed the recent regime change in Germany and its consequences:The future alone will judge which was the true Germany in 1933, and who were the true Germans—those who subscribe to the more or less materialistic-mythical racial prejudices of the day, or those Germans pure in heart and mind, heirs to the great Germans of the past whose tradition they revere and perpetuate.After his death, Husserl's manuscripts, amounting to approximately 40,000 pages of ""Gabelsberger"" stenography and his complete research library, were in 1939 smuggled to the Catholic University of Louvain in Belgium by the Franciscan priest Herman Van Breda. There they were deposited at Leuven to form the "Husserl-Archives" of the Higher Institute of Philosophy. Much of the material in his research manuscripts has since been published in the Husserliana critical edition series.

In his first works Husserl tries to combine mathematics, psychology and philosophy with a main goal to provide a sound foundation for mathematics. He analyzes the psychological process needed to obtain the concept of number and then tries to build up a systematical theory on this analysis. To achieve this he uses several methods and concepts taken from his teachers. From Weierstrass he derives the idea that we generate the concept of number by counting a certain collection of objects.

From Brentano and Stumpf he takes over the distinction between "proper" and "improper" presenting. In an example Husserl explains this in the following way: if you are standing in front of a house, you have a proper, direct presentation of that house, but if you are looking for it and ask for directions, then these directions (e.g. the house on the corner of this and that street) are an indirect, improper presentation. In other words, you can have a proper presentation of an object if it is actually present, and an improper (or symbolic, as he also calls it) one if you only can indicate that object through signs, symbols, etc. Husserl's "Logical Investigations" (1900–1901) is considered the starting point for the formal theory of wholes and their parts known as mereology.

Another important element that Husserl took over from Brentano is intentionality, the notion that the main characteristic of consciousness is that it is always intentional. While often simplistically summarised as "aboutness" or the relationship between mental acts and the external world, Brentano defined it as the main characteristic of "mental phenomena", by which they could be distinguished from "physical phenomena". Every mental phenomenon, every psychological act, has a content, is directed at an object (the "intentional object"). Every belief, desire, etc. has an object that it is about: the believed, the wanted. Brentano used the expression "intentional inexistence" to indicate the status of the objects of thought in the mind. The property of being intentional, of having an intentional object, was the key feature to distinguish mental phenomena and physical phenomena, because physical phenomena lack intentionality altogether.

Some years after the 1900–1901 publication of his main work, the "Logische Untersuchungen" ("Logical Investigations"), Husserl made some key conceptual elaborations which led him to assert that in order to study the structure of consciousness, one would have to distinguish between the act of consciousness and the phenomena at which it is directed (the objects as intended). Knowledge of essences would only be possible by "bracketing" all assumptions about the existence of an external world. This procedure he called "epoché". These new concepts prompted the publication of the "Ideen" ("Ideas") in 1913, in which they were at first incorporated, and a plan for a second edition of the "Logische Untersuchungen".

From the "Ideen" onward, Husserl concentrated on the ideal, essential structures of consciousness. The metaphysical problem of establishing the reality of what we perceive, as distinct from the perceiving subject, was of little interest to Husserl in spite of his being a transcendental idealist. Husserl proposed that the world of objects—and of ways in which we direct ourselves toward and perceive those objects—is normally conceived of in what he called the "natural attitude", which is characterized by a belief that objects exist distinct from the perceiving subject and exhibit properties that we see as emanating from them (this attitude is also called physicalist objectivism). Husserl proposed a radical new phenomenological way of looking at objects by examining how we, in our many ways of being intentionally directed toward them, actually "constitute" them (to be distinguished from materially creating objects or objects merely being figments of the imagination); in the Phenomenological standpoint, the object ceases to be something simply "external" and ceases to be seen as providing indicators about what it is, and becomes a grouping of perceptual and functional aspects that imply one another under the idea of a particular object or "type". The notion of objects as real is not expelled by phenomenology, but "bracketed" as a way in which we regard objectsinstead of a feature that inheres in an object's essence founded in the relation between the object and the perceiver. In order to better understand the world of appearances and objects, phenomenology attempts to identify the invariant features of how objects are perceived and pushes attributions of reality into their role as an attribution about the things we perceive (or an assumption underlying how we perceive objects). The major dividing line in Husserl's thought is the turn to transcendental idealism.

In a later period, Husserl began to wrestle with the complicated issues of intersubjectivity, specifically, how communication about an object can be assumed to refer to the same ideal entity ("Cartesian Meditations", Meditation V). Husserl tries new methods of bringing his readers to understand the importance of phenomenology to scientific inquiry (and specifically to psychology) and what it means to "bracket" the natural attitude. "The Crisis of the European Sciences" is Husserl's unfinished work that deals most directly with these issues. In it, Husserl for the first time attempts a historical overview of the development of Western philosophy and science, emphasizing the challenges presented by their increasingly (one-sidedly) empirical and naturalistic orientation. Husserl declares that mental and spiritual reality possess their own reality independent of any physical basis, and that a science of the mind ('') must be established on as scientific a foundation as the natural sciences have managed: "It is my conviction that intentional phenomenology has for the first time made spirit as spirit the field of systematic scientific experience, thus effecting a total transformation of the task of knowledge."

Husserl's thought is revolutionary in several ways, most notably in the distinction between "natural" and "phenomenological" modes of understanding. In the former, sense-perception in correspondence with the material realm constitutes the known reality, and understanding is premised on the accuracy of the perception and the objective knowability of what is called the "real world". Phenomenological understanding strives to be rigorously "presuppositionless" by means of what Husserl calls "phenomenological reduction". This reduction is not conditioned but rather transcendental: in Husserl's terms, pure consciousness of absolute Being. In Husserl's work, consciousness of any given thing calls for discerning its meaning as an "intentional object". Such an object does not simply strike the senses, to be interpreted or misinterpreted by mental reason; it has already been selected and grasped, grasping being an etymological connotation, of "percipere", the root of "perceive".

From "Logical Investigations" (1900/1901) to "Experience and Judgment" (published in 1939), Husserl expressed clearly the difference between meaning and object. He identified several different kinds of names. For example, there are names that have the role of properties that uniquely identify an object. Each of these names expresses a meaning and designates the same object. Examples of this are "the victor in Jena" and "the loser in Waterloo", or "the equilateral triangle" and "the equiangular triangle"; in both cases, both names express different meanings, but designate the same object. There are names which have no meaning, but have the role of designating an object: "Aristotle", "Socrates", and so on. Finally, there are names which designate a variety of objects. These are called "universal names"; their meaning is a "concept" and refers to a series of objects (the extension of the concept). The way we know sensible objects is called "sensible intuition".

Husserl also identifies a series of "formal words" which are necessary to form sentences and have no sensible correlates. Examples of formal words are "a", "the", "more than", "over", "under", "two", "group", and so on. Every sentence must contain formal words to designate what Husserl calls "formal categories". There are two kinds of categories: meaning categories and formal-ontological categories. Meaning categories relate judgments; they include forms of conjunction, disjunction, forms of plural, among others. Formal-ontological categories relate objects and include notions such as set, cardinal number, ordinal number, part and whole, relation, and so on. The way we know these categories is through a faculty of understanding called "categorial intuition".

Through sensible intuition our consciousness constitutes what Husserl calls a "situation of affairs" ("Sachlage"). It is a passive constitution where objects themselves are presented to us. To this situation of affairs, through categorial intuition, we are able to constitute a "state of affairs" ("Sachverhalt"). One situation of affairs through objective acts of consciousness (acts of constituting categorially) can serve as the basis for constituting multiple states of affairs. For example, suppose "a" and "b" are two sensible objects in a certain situation of affairs. We can use it as basis to say, ""a"<"b"" and ""b">"a"", two judgments which designate the same state of affairs. For Husserl a sentence has a proposition or judgment as its meaning, and refers to a state of affairs which has a situation of affairs as a reference base.

Husserl believed that "truth-in-itself" has as ontological correlate "being-in-itself", just as meaning categories have formal-ontological categories as correlates. Logic is a formal theory of judgment, that studies the formal "a priori" relations among judgments using meaning categories. Mathematics, on the other hand, is formal ontology; it studies all the possible forms of being (of objects). Hence for both logic and mathematics, the different formal categories are the objects of study, not the sensible objects themselves. The problem with the psychological approach to mathematics and logic is that it fails to account for the fact that this approach is about formal categories, and not simply about abstractions from sensibility alone. The reason why we do not deal with sensible objects in mathematics is because of another faculty of understanding called "categorial abstraction." Through this faculty we are able to get rid of sensible components of judgments, and just focus on formal categories themselves.

Thanks to "eidetic reduction" (or "essential intuition"), we are able to grasp the possibility, impossibility, necessity and contingency among concepts and among formal categories. Categorial intuition, along with categorial abstraction and eidetic reduction, are the basis for logical and mathematical knowledge.

Husserl criticized the logicians of his day for not focusing on the relation between subjective processes that give us objective knowledge of pure logic. All subjective activities of consciousness need an ideal correlate, and objective logic (constituted noematically) as it is constituted by consciousness needs a noetic correlate (the subjective activities of consciousness).

Husserl stated that logic has three strata, each further away from consciousness and psychology than those that precede it.

The ontological correlate to the third stratum is the "theory of manifolds". In formal ontology, it is a free investigation where a mathematician can assign several meanings to several symbols, and all their possible valid deductions in a general and indeterminate manner. It is, properly speaking, the most universal mathematics of all. Through the posit of certain indeterminate objects (formal-ontological categories) as well as any combination of mathematical axioms, mathematicians can explore the apodeictic connections between them, as long as consistency is preserved.

According to Husserl, this view of logic and mathematics accounted for the objectivity of a series of mathematical developments of his time, such as "n"-dimensional manifolds (both Euclidean and non-Euclidean), Hermann Grassmann's theory of extensions, William Rowan Hamilton's Hamiltonians, Sophus Lie's theory of transformation groups, and Cantor's set theory.

Jacob Klein was one student of Husserl who pursued this line of inquiry, seeking to "desedimentize" mathematics and the mathematical sciences.

After obtaining his PhD in mathematics, Husserl began analyzing the foundations of mathematics from a psychological point of view. In his habilitation thesis, "On the Concept of Number" (1886) and in his "Philosophy of Arithmetic" (1891), Husserl sought, by employing Brentano's descriptive psychology, to define the natural numbers in a way that advanced the methods and techniques of Karl Weierstrass, Richard Dedekind, Georg Cantor, Gottlob Frege, and other contemporary mathematicians. Later, in the first volume of his "Logical Investigations", the "Prolegomena of Pure Logic", Husserl, while attacking the psychologistic point of view in logic and mathematics, also appears to reject much of his early work, although the forms of psychologism analysed and refuted in the "Prolegomena" did not apply directly to his "Philosophy of Arithmetic". Some scholars question whether Frege's negative review of the "Philosophy of Arithmetic" helped turn Husserl towards modern Platonism, but he had already discovered the work of Bernard Bolzano independently around 1890/91. In his "Logical Investigations", Husserl explicitly mentioned Bolzano, G. W. Leibniz and Hermann Lotze as inspirations for his newer position.

Husserl's review of Ernst Schröder, published before Frege's landmark 1892 article, clearly distinguishes sense from reference; thus Husserl's notions of noema and object also arose independently. Likewise, in his criticism of Frege in the "Philosophy of Arithmetic", Husserl remarks on the distinction between the content and the extension of a concept. Moreover, the distinction between the subjective mental act, namely the content of a concept, and the (external) object, was developed independently by Brentano and his school, and may have surfaced as early as Brentano's 1870s lectures on logic.

Scholars such as J. N. Mohanty, Claire Ortiz Hill, and Guillermo E. Rosado Haddock, among others, have argued that Husserl's so-called change from psychologism to Platonism came about independently of Frege's review.
For example, the review falsely accuses Husserl of subjectivizing everything, so that no objectivity is possible, and falsely attributes to him a notion of abstraction whereby objects disappear until we are left with numbers as mere ghosts. Contrary to what Frege states, in Husserl's "Philosophy of Arithmetic" we already find two different kinds of representations: subjective and objective. Moreover, objectivity is clearly defined in that work. Frege's attack seems to be directed at certain foundational doctrines then current in Weierstrass's Berlin School, of which Husserl and Cantor cannot be said to be orthodox representatives.

Furthermore, various sources indicate that Husserl changed his mind about psychologism as early as 1890, a year before he published the "Philosophy of Arithmetic". Husserl stated that by the time he published that book, he had already changed his mind—that he had doubts about psychologism from the very outset. He attributed this change of mind to his reading of Leibniz, Bolzano, Lotze, and David Hume. Husserl makes no mention of Frege as a decisive factor in this change. In his "Logical Investigations", Husserl mentions Frege only twice, once in a footnote to point out that he had retracted three pages of his criticism of Frege's "The Foundations of Arithmetic", and again to question Frege's use of the word "Bedeutung" to designate "reference" rather than "meaning" (sense).

In a letter dated 24 May 1891, Frege thanked Husserl for sending him a copy of the "Philosophy of Arithmetic" and Husserl's review of Ernst Schröder's "Vorlesungen über die Algebra der Logik". In the same letter, Frege used the review of Schröder's book to analyze Husserl's notion of the sense of reference of concept words. Hence Frege recognized, as early as 1891, that Husserl distinguished between sense and reference. Consequently, Frege and Husserl independently elaborated a theory of sense and reference before 1891.

Commentators argue that Husserl's notion of noema has nothing to do with Frege's notion of sense, because "noemata" are necessarily fused with noeses which are the conscious activities of consciousness. "Noemata" have three different levels:
Consequently, in intentional activities, even non-existent objects can be constituted, and form part of the whole noema. Frege, however, did not conceive of objects as forming parts of senses: If a proper name denotes a non-existent object, it does not have a reference, hence concepts with no objects have no truth value in arguments. Moreover, Husserl did not maintain that predicates of sentences designate concepts. According to Frege the reference of a sentence is a truth value; for Husserl it is a "state of affairs." Frege's notion of "sense" is unrelated to Husserl's noema, while the latter's notions of "meaning" and "object" differ from those of Frege.

In detail, Husserl's conception of logic and mathematics differs from that of Frege, who held that arithmetic could be derived from logic. For Husserl this is not the case: mathematics (with the exception of geometry) is the ontological correlate of logic, and while both fields are related, neither one is strictly reducible to the other.

Reacting against authors such as J. S. Mill, Christoph von Sigwart and his own former teacher Brentano, Husserl criticised their psychologism in mathematics and logic, i.e. their conception of these abstract and "a priori" sciences as having an essentially empirical foundation and a prescriptive or descriptive nature. According to psychologism, logic would not be an autonomous discipline, but a branch of psychology, either proposing a prescriptive and practical "art" of correct judgement (as Brentano and some of his more orthodox students did) or a description of the factual processes of human thought. Husserl pointed out that the failure of anti-psychologists to defeat psychologism was a result of being unable to distinguish between the foundational, theoretical side of logic, and the applied, practical side. Pure logic does not deal at all with "thoughts" or "judgings" as mental episodes but about "a priori" laws and conditions for any theory and any judgments whatsoever, conceived as propositions in themselves.

Since "truth-in-itself" has "being-in-itself" as ontological correlate, and since psychologists reduce truth (and hence logic) to empirical psychology, the inevitable consequence is scepticism. Psychologists have also not been successful in showing how from induction or psychological processes we can justify the absolute certainty of logical principles, such as the principles of identity and non-contradiction. It is therefore futile to base certain logical laws and principles on uncertain processes of the mind.

This confusion made by psychologism (and related disciplines such as biologism and anthropologism) can be due to three specific prejudices:

1. The first prejudice is the supposition that logic is somehow normative in nature. Husserl argues that logic is theoretical, i.e., that logic itself proposes "a priori" laws which are themselves the basis of the normative side of logic. Since mathematics is related to logic, he cites an example from mathematics: If we have a formula like "(a + b)(a – b) = a² – b²" it does not tell us how to think mathematically. It just expresses a truth. A proposition that says: "The product of the sum and the difference of a and b "should" give us the difference of the squares of a and b" does express a normative proposition, but this normative statement "is based on" the theoretical statement "(a + b)(a – b) = a² – b²".

2. For psychologists, the acts of judging, reasoning, deriving, and so on, are all psychological processes. Therefore, it is the role of psychology to provide the foundation of these processes. Husserl states that this effort made by psychologists is a "metábasis eis állo génos" (Gr. μετάβασις εἰς ἄλλο γένος, "a transgression to another field"). It is a metábasis because psychology cannot provide any foundations for "a priori" laws which themselves are the basis for all the ways we should think correctly. Psychologists have the problem of confusing intentional activities with the object of these activities. It is important to distinguish between the act of judging and the judgment itself, the act of counting and the number itself, and so on. Counting five objects is undeniably a psychological process, but the number 5 is not.

3. Judgments can be true or not true. Psychologists argue that judgments are true because they become "evidently" true to us. This evidence, a psychological process that "guarantees" truth, is indeed a psychological process. Husserl responds by saying that truth itself, as well as logical laws, always remain valid regardless of psychological "evidence" that they are true. No psychological process can explain the "a priori" objectivity of these logical truths.

From this criticism to psychologism, the distinction between psychological acts and their intentional objects, and the difference between the normative side of logic and the theoretical side, derives from a Platonist conception of logic. This means that we should regard logical and mathematical laws as being independent of the human mind, and also as an autonomy of meanings. It is essentially the difference between the real (everything subject to time) and the ideal or irreal (everything that is atemporal), such as logical truths, mathematical entities, mathematical truths and meanings in general.

David Carr commented on Husserl's following in his 1970 dissertation at Yale: "It is well known that Husserl was always disappointed at the tendency of his students to go their own way, to embark upon fundamental revisions of phenomenology rather than engage in the communal task" as originally intended by the radical new science. Notwithstanding, he did attract philosophers to phenomenology.

Martin Heidegger is the best known of Husserl's students, the one whom Husserl chose as his successor at Freiburg. Heidegger's magnum opus "Being and Time" was dedicated to Husserl. They shared their thoughts and worked alongside each other for over a decade at the University of Freiburg, Heidegger being Husserl's assistant during 1920–1923. Heidegger's early work followed his teacher, but with time he began to develop new insights distinctively variant. Husserl became increasingly critical of Heidegger's work, especially in 1929, and included pointed criticism of Heidegger in lectures he gave during 1931. Heidegger, while acknowledging his debt to Husserl, followed a political position offensive and harmful to Husserl after the Nazis came to power in 1933, Husserl being of Jewish origin and Heidegger infamously being then a Nazi proponent. Academic discussion of Husserl and Heidegger is extensive.

At Göttingen in 1913 Adolf Reinach (1884–1917) "was now Husserl's right hand. He was above all the mediator between Husserl and the students, for he understood extremely well how to deal with other persons, whereas Husserl was pretty much helpless in this respect." He was an original editor of Husserl's new journal, "Jahrbuch"; one of his works (giving a phenomenological analysis of the law of obligations) appeared in its first issue. Reinach was widely admired and a remarkable teacher. Husserl, in his 1917 obituary, wrote, "He wanted to draw only from the deepest sources, he wanted to produce only work of enduring value. And through his wise restrain he succeeded in this."

Edith Stein was Husserl's student at Göttingen and Freiburg while she wrote her doctoral thesis "The Empathy Problem as it Developed Historically and Considered Phenomenologically" (1916). She then became his assistant at Freiburg in 1916–18. She later adapted her phenomenology to the modern school of modern Thomism".

Ludwig Landgrebe became assistant to Husserl in 1923. From 1939 he collaborated with Eugen Fink at the Husserl-Archives in Leuven. In 1954 he became leader of the Husserl-Archives. Landgrebe is known as one of Husserl's closest associates, but also for his independent views relating to history, religion and politics as seen from the viewpoints of existentialist philosophy and metaphysics.

Eugen Fink was a close associate of Husserl during the 1920s and 1930s. He wrote the "Sixth Cartesian Meditation" which Husserl said was the truest expression and continuation of his own work. Fink delivered the eulogy for Husserl in 1938.

Roman Ingarden, an early student of Husserl at Freiburg, corresponded with Husserl into the mid-1930s. Ingarden did not accept, however, the later transcendental idealism of Husserl which he thought would lead to relativism. Ingarden has written his work in German and Polish. In his "Spór o istnienie świata" (Ger.: "Der Streit um die Existenz der Welt", Eng.: "Dispute about existence of the world") he created his own realistic position, which also helped to spread phenomenology in Poland.

Max Scheler met Husserl in Halle in 1901 and found in his phenomenology a methodological breakthrough for his own philosophy. Scheler, who was at Göttingen when Husserl taught there, was one of the original few editors of the journal "Jahrbuch für Philosophie und Phänomenologische Forschung" (1913). Scheler's work "Formalism in Ethics and Nonformal Ethics of Value" appeared in the new journal (1913 and 1916) and drew acclaim. The personal relationship between the two men, however, became strained, due to Scheler's legal troubles, and Scheler returned to Munich. Although Scheler later criticised Husserl's idealistic logical approach and proposed instead a "phenomenology of love", he states that he remained "deeply indebted" to Husserl throughout his work.

Nicolai Hartmann was once thought to be at the center of phenomenology, but perhaps no longer. In 1921 the prestige of Hartmann the Neo-Kantian, who was Professor of Philosophy at Marburg, was added to the Movement; he "publicly declared his solidarity with the actual work of "die Phänomenologie"." Yet Hartmann's connections were with Max Scheler and the Munich circle; Husserl himself evidently did not consider him as a phenomenologist. His philosophy, however, is said to include an innovative use of the method.

Emmanuel Levinas in 1929 gave a presentation at one of Husserl's last seminars in Freiburg. Also that year he wrote on Husserl's "Ideen" (1913) a long review published by a French journal. With Gabrielle Peiffer, Levinas translated into French Husserl's "Méditations cartésiennes" (1931). He was at first impressed with Heidegger and began a book on him, but broke off the project when Heidegger became involved with the Nazis. After the war he wrote on Jewish spirituality; most of his family had been murdered by the Nazis in Lithuania. Levinas then began to write works that would become widely known and admired.

Alfred Schutz's "Phenomenology of the Social World" seeks to rigorously ground Max Weber's interpretive sociology in Husserl's phenomenology. Husserl was impressed by this work and asked Schutz to be his assistant.

Jean-Paul Sartre was also largely influenced by Husserl, although he later came to disagree with key points in his analyses. Sartre rejected Husserl's transcendental interpretations begun in his "Ideen" (1913) and instead followed Heidegger's ontology.

Maurice Merleau-Ponty's "Phenomenology of Perception" is influenced by Edmund Husserl's work on perception, intersubjectivity, intentionality, space, and temporality, including Husserl's theory of retention and protention. Merleau-Ponty's description of 'motor intentionality' and sexuality, for example, retain the important structure of the noetic/noematic correlation of "Ideen I", yet further concretize what it means for Husserl when consciousness particularizes itself into modes of intuition. Merleau-Ponty's most clearly Husserlian work is, perhaps, "the Philosopher and His Shadow." Depending on the interpretation of Husserl's accounts of eidetic intuition, given in Husserl's "Phenomenological Psychology" and "Experience and Judgment", it may be that Merleau-Ponty did not accept the "eidetic reduction" nor the "pure essence" said to result. Merleau-Ponty was the first student to study at the Husserl-archives in Leuven.

Gabriel Marcel explicitly rejected existentialism, due to Sartre, but not phenomenology, which has enjoyed a wide following among French Catholics. He appreciated Husserl, Scheler, and (but with apprehension) Heidegger. His expressions like "ontology of sensability" when referring to the body, indicate influence by phenomenological thought.

Kurt Gödel is known to have read "Cartesian Meditations". He expressed very strong appreciation for Husserl's work, especially with regard to "bracketing" or "epoché".

Hermann Weyl's interest in intuitionistic logic and impredicativity appears to have resulted from his reading of Husserl. He was introduced to Husserl's work through his wife, Helene Joseph, herself a student of Husserl at Göttingen.

Colin Wilson has used Husserl's ideas extensively in developing his "New Existentialism," particularly in regards to his "intentionality of consciousness," which he mentions in a number of his books.

Rudolf Carnap was also influenced by Husserl, not only concerning Husserl's notion of essential insight that Carnap used in his "Der Raum", but also his notion of "formation rules" and "transformation rules" is founded on Husserl's philosophy of logic.

Karol Wojtyla, who would later become Pope John Paul II, was influenced by Husserl. Phenomenology appears in his major work, "The Acting Person" (1969). Originally published in Polish, it was translated by Andrzej Potocki and edited by Anna-Teresa Tymieniecka in the "Analecta Husserliana". "The Acting Person" combines phenomenological work with Thomistic ethics.
Paul Ricœur has translated many works of Husserl into French and has also written many of his own studies of the philosopher. Among other works, Ricœur employed phenomenology in his "Freud and Philosophy" (1965).

Jacques Derrida wrote several critical studies of Husserl early in his academic career. These included his dissertation, "The Problem of Genesis in Husserl's Philosophy," and also his introduction to "The Origin of Geometry". Derrida continued to make reference to Husserl in works such as "Of Grammatology".

Stanisław Leśniewski and Kazimierz Ajdukiewicz were inspired by Husserl's formal analysis of language. Accordingly, they employed phenomenology in the development of categorial grammar.

José Ortega y Gasset visited Husserl at Freiburg in 1934. He credited phenomenology for having 'liberated him' from a narrow neo-Kantian thought. While perhaps not a phenomenologist himself, he introduced the philosophy to Iberia and Latin America.

Wilfrid Sellars, an influential figure in the so-called "Pittsburgh School" (Robert Brandom, John McDowell) had been a student of Marvin Farber, a pupil of Husserl, and was influenced by phenomenology through him:

Hans Blumenberg received his habilitation in 1950, with a dissertation on 'Ontological distance', an inquiry into the crisis of Husserl's phenomenology.

Roger Scruton, despite some disagreements with Husserl, drew upon his work in "Sexual Desire" (1986).

The influence of the Husserlian phenomenological tradition in the 21st century extends beyond the confines of the European and North American legacies. It has already started to impact (indirectly) scholarship in Eastern and Oriental thought, including research on the impetus of philosophical thinking in the history of ideas in Islam.










</doc>
<doc id="9531" url="https://en.wikipedia.org/wiki?curid=9531" title="Electrical engineering">
Electrical engineering

Electrical engineering is an engineering discipline concerned with the study, design and application of equipment, devices and systems which use electricity, electronics, and electromagnetism. It emerged as an identifiable occupation in the latter half of the 19th century after commercialization of the electric telegraph, the telephone, and electrical power generation, distribution and use.

Electrical engineering is now divided into a wide range of fields, including computer engineering, systems engineering, power engineering, telecommunications, radio-frequency engineering, signal processing, instrumentation, and electronics. Many of these disciplines overlap with other engineering branches, spanning a huge number of specializations including hardware engineering, power electronics, electromagnetics and waves, microwave engineering, nanotechnology, electrochemistry, renewable energies, mechatronics, and electrical materials science.

Electrical engineers typically hold a degree in electrical engineering or electronic engineering. Practising engineers may have professional certification and be members of a professional body or an international standards organization. These include the International Electrotechnical Commission (IEC), the Institute of Electrical and Electronics Engineers (IEEE) and the Institution of Engineering and Technology (IET) "(formerly the IEE)".

Electrical engineers work in a very wide range of industries and the skills required are likewise variable. These range from circuit theory to the management skills of a project manager. The tools and equipment that an individual engineer may need are similarly variable, ranging from a simple voltmeter to sophisticated design and manufacturing software.

Electricity has been a subject of scientific interest since at least the early 17th century. William Gilbert was a prominent early electrical scientist, and was the first to draw a clear distinction between magnetism and static electricity. He is credited with establishing the term "electricity". He also designed the versorium: a device that detects the presence of statically charged objects. In 1762 Swedish professor Johan Wilcke invented a device later named electrophorus that produced a static electric charge. By 1800 Alessandro Volta had developed the voltaic pile, a forerunner of the electric battery.

In the 19th century, research into the subject started to intensify. Notable developments in this century include the work of Hans Christian Ørsted who discovered in 1820 that an electric current produces a magnetic field that will deflect a compass needle, of William Sturgeon who, in 1825 invented the electromagnet, of Joseph Henry and Edward Davy who invented the electrical relay in 1835, of Georg Ohm, who in 1827 quantified the relationship between the electric current and potential difference in a conductor, of Michael Faraday (the discoverer of electromagnetic induction in 1831), and of James Clerk Maxwell, who in 1873 published a unified theory of electricity and magnetism in his treatise "Electricity and Magnetism".

In 1782 Georges-Louis Le Sage developed and presented in Berlin probably the world's first form of electric telegraphy, using 24 different wires, one for each letter of the alphabet. This telegraph connected two rooms. It was an electrostatic telegraph that moved gold leaf through electrical conduction.

In 1795, Francisco Salva Campillo proposed an electrostatic telegraph system. Between 1803–1804, he worked on electrical telegraphy and in 1804, he presented his report at the Royal Academy of Natural Sciences and Arts of Barcelona. Salva's electrolyte telegraph system was very innovative though it was greatly influenced by and based upon two new discoveries made in Europe in 1800 – Alessandro Volta's electric battery for generating an electric current and William Nicholson and Anthony Carlyle's electrolysis of water. Electrical telegraphy may be considered the first example of electrical engineering. Electrical engineering became a profession in the later 19th century. Practitioners had created a global electric telegraph network and the first professional electrical engineering institutions were founded in the UK and USA to support the new discipline. Francis Ronalds created an electric telegraph system in 1816 and documented his vision of how the world could be transformed by electricity. Over 50 years later, he joined the new Society of Telegraph Engineers (soon to be renamed the Institution of Electrical Engineers) where he was regarded by other members as the first of their cohort. By the end of the 19th century, the world had been forever changed by the rapid communication made possible by the engineering development of land-lines, submarine cables, and, from about 1890, wireless telegraphy.

Practical applications and advances in such fields created an increasing need for standardised units of measure. They led to the international standardization of the units volt, ampere, coulomb, ohm, farad, and henry. This was achieved at an international conference in Chicago in 1893. The publication of these standards formed the basis of future advances in standardisation in various industries, and in many countries, the definitions were immediately recognized in relevant legislation.

During these years, the study of electricity was largely considered to be a subfield of physics since the early electrical technology was considered electromechanical in nature. The Technische Universität Darmstadt founded the world's first department of electrical engineering in 1882 and introduced the first degree course in electrical engineering in 1883. The first electrical engineering degree program in the United States was started at Massachusetts Institute of Technology (MIT) in the physics department under Professor Charles Cross, though it was Cornell University to produce the world's first electrical engineering graduates in 1885. The first course in electrical engineering was taught in 1883 in Cornell's Sibley College of Mechanical Engineering and Mechanic Arts. It was not until about 1885 that Cornell President Andrew Dickson White established the first Department of Electrical Engineering in the United States. In the same year, University College London founded the first chair of electrical engineering in Great Britain. Professor Mendell P. Weinbach at University of Missouri soon followed suit by establishing the electrical engineering department in 1886. Afterwards, universities and institutes of technology gradually started to offer electrical engineering programs to their students all over the world.
During these decades use of electrical engineering increased dramatically. In 1882, Thomas Edison switched on the world's first large-scale electric power network that provided 110 volts — direct current (DC) — to 59 customers on Manhattan Island in New York City. In 1884, Sir Charles Parsons invented the steam turbine allowing for more efficient electric power generation. Alternating current, with its ability to transmit power more efficiently over long distances via the use of transformers, developed rapidly in the 1880s and 1890s with transformer designs by Károly Zipernowsky, Ottó Bláthy and Miksa Déri (later called ZBD transformers), Lucien Gaulard, John Dixon Gibbs and William Stanley, Jr.. Practical AC motor designs including induction motors were independently invented by Galileo Ferraris and Nikola Tesla and further developed into a practical three-phase form by Mikhail Dolivo-Dobrovolsky and Charles Eugene Lancelot Brown. Charles Steinmetz and Oliver Heaviside contributed to the theoretical basis of alternating current engineering. The spread in the use of AC set off in the United States what has been called the "war of the currents" between a George Westinghouse backed AC system and a Thomas Edison backed DC power system, with AC being adopted as the overall standard.

During the development of radio, many scientists and inventors contributed to radio technology and electronics. The mathematical work of James Clerk Maxwell during the 1850s had shown the relationship of different forms of electromagnetic radiation including the possibility of invisible airborne waves (later called "radio waves"). In his classic physics experiments of 1888, Heinrich Hertz proved Maxwell's theory by transmitting radio waves with a spark-gap transmitter, and detected them by using simple electrical devices. Other physicists experimented with these new waves and in the process developed devices for transmitting and detecting them. In 1895, Guglielmo Marconi began work on a way to adapt the known methods of transmitting and detecting these "Hertzian waves" into a purpose built commercial wireless telegraphic system. Early on, he sent wireless signals over a distance of one and a half miles. In December 1901, he sent wireless waves that were not affected by the curvature of the Earth. Marconi later transmitted the wireless signals across the Atlantic between Poldhu, Cornwall, and St. John's, Newfoundland, a distance of .

Millimetre wave communication was first investigated by Jagadish Chandra Bose during 18941896, when he reached an extremely high frequency of up to 60GHz in his experiments. He also introduced the use of semiconductor junctions to detect radio waves, when he patented the radio crystal detector in 1901.

In 1897, Karl Ferdinand Braun introduced the cathode ray tube as part of an oscilloscope, a crucial enabling technology for electronic television. John Fleming invented the first radio tube, the diode, in 1904. Two years later, Robert von Lieben and Lee De Forest independently developed the amplifier tube, called the triode.

In 1920, Albert Hull developed the magnetron which would eventually lead to the development of the microwave oven in 1946 by Percy Spencer. In 1934, the British military began to make strides toward radar (which also uses the magnetron) under the direction of Dr Wimperis, culminating in the operation of the first radar station at Bawdsey in August 1936.

In 1941, Konrad Zuse presented the Z3, the world's first fully functional and programmable computer using electromechanical parts. In 1943, Tommy Flowers designed and built the Colossus, the world's first fully functional, electronic, digital and programmable computer. In 1946, the ENIAC (Electronic Numerical Integrator and Computer) of John Presper Eckert and John Mauchly followed, beginning the computing era. The arithmetic performance of these machines allowed engineers to develop completely new technologies and achieve new objectives.

In 1948 Claude Shannon publishes "A Mathematical Theory of Communication" which mathematically describes the passage of information with uncertainty(electrical noise.

The first working transistor was a point-contact transistor invented by John Bardeen and Walter Houser Brattain while working under William Shockley at the Bell Telephone Laboratories (BTL) in 1947. They then invented the bipolar junction transistor in 1948. While early junction transistors were relatively bulky devices that were difficult to manufacture on a mass-production basis, they opened the door for more compact devices.

The surface passivation process, which electrically stabilized silicon surfaces via thermal oxidation, was developed by Mohamed M. Atalla at BTL in 1957. This led to the development of the monolithic integrated circuit chip. The first integrated circuits were the hybrid integrated circuit invented by Jack Kilby at Texas Instruments in 1958 and the monolithic integrated circuit chip invented by Robert Noyce at Fairchild Semiconductor in 1959.

The MOSFET (metal-oxide-semiconductor field-effect transistor, or MOS transistor) was invented by Mohamed Atalla and Dawon Kahng at BTL in 1959. It was the first truly compact transistor that could be miniaturised and mass-produced for a wide range of uses. It revolutionized the electronics industry, becoming the most widely used electronic device in the world. The MOSFET is the basic element in most modern electronic equipment, and has been central to the electronics revolution, the microelectronics revolution, and the Digital Revolution. The MOSFET has thus been credited as the birth of modern electronics, and possibly the most important invention in electronics.

The MOSFET made it possible to build high-density integrated circuit chips. Atalla first proposed the concept of the MOS integrated circuit (MOS IC) chip in 1960, followed by Kahng in 1961. The earliest experimental MOS IC chip to be fabricated was built by Fred Heiman and Steven Hofstein at RCA Laboratories in 1962. MOS technology enabled Moore's law, the doubling of transistors on an IC chip every two years, predicted by Gordon Moore in 1965. Silicon-gate MOS technology was developed by Federico Faggin at Fairchild in 1968. Since then, the MOSFET has been the basic building block of modern electronics. The mass-production of silicon MOSFETs and MOS integrated circuit chips, along with continuous MOSFET scaling miniaturization at an exponential pace (as predicted by Moore's law), has since led to revolutionary changes in technology, economy, culture and thinking.

The Apollo program which culminated in landing astronauts on the Moon with Apollo 11 in 1969 was enabled by NASA's adoption of advances in semiconductor electronic technology, including MOSFETs in the Interplanetary Monitoring Platform (IMP) and silicon integrated circuit chips in the Apollo Guidance Computer (AGC).

The development of MOS integrated circuit technology in the 1960s led to the invention of the microprocessor in the early 1970s. The first single-chip microprocessor was the Intel 4004, released in 1971. It began with the "Busicom Project" as Masatoshi Shima's three-chip CPU design in 1968, before Sharp's Tadashi Sasaki conceived of a single-chip CPU design, which he discussed with Busicom and Intel in 1968. The Intel 4004 was then designed and realized by Federico Faggin at Intel with his silicon-gate MOS technology, along with Intel's Marcian Hoff and Stanley Mazor and Busicom's Masatoshi Shima. The microprocessor led to the development of microcomputers and personal computers, and the microcomputer revolution.

Electrical engineering has many subdisciplines, the most common of which are listed below. Although there are electrical engineers who focus exclusively on one of these subdisciplines, many deal with a combination of them. Sometimes certain fields, such as electronic engineering and computer engineering, are considered separate disciplines in their own right.

Power engineering deals with the generation, transmission, and distribution of electricity as well as the design of a range of related devices. These include transformers, electric generators, electric motors, high voltage engineering, and power electronics. In many regions of the world, governments maintain an electrical network called a power grid that connects a variety of generators together with users of their energy. Users purchase electrical energy from the grid, avoiding the costly exercise of having to generate their own. Power engineers may work on the design and maintenance of the power grid as well as the power systems that connect to it. Such systems are called "on-grid" power systems and may supply the grid with additional power, draw power from the grid, or do both. Power engineers may also work on systems that do not connect to the grid, called "off-grid" power systems, which in some cases are preferable to on-grid systems. The future includes Satellite controlled power systems, with feedback in real time to prevent power surges and prevent blackouts.

Control engineering focuses on the modeling of a diverse range of dynamic systems and the design of controllers that will cause these systems to behave in the desired manner. To implement such controllers, electrical engineers may use electronic circuits, digital signal processors, microcontrollers, and programmable logic controllers (PLCs). Control engineering has a wide range of applications from the flight and propulsion systems of commercial airliners to the cruise control present in many modern automobiles. It also plays an important role in industrial automation.

Control engineers often utilize feedback when designing control systems. For example, in an automobile with cruise control the vehicle's speed is continuously monitored and fed back to the system which adjusts the motor's power output accordingly. Where there is regular feedback, control theory can be used to determine how the system responds to such feedback.

Electronic engineering involves the design and testing of electronic circuits that use the properties of components such as resistors, capacitors, inductors, diodes, and transistors to achieve a particular functionality. The tuned circuit, which allows the user of a radio to filter out all but a single station, is just one example of such a circuit. Another example to research is a pneumatic signal conditioner.

Prior to the Second World War, the subject was commonly known as "radio engineering" and basically was restricted to aspects of communications and radar, commercial radio, and early television. Later, in post-war years, as consumer devices began to be developed, the field grew to include modern television, audio systems, computers, and microprocessors. In the mid-to-late 1950s, the term "radio engineering" gradually gave way to the name "electronic engineering".

Before the invention of the integrated circuit in 1959, electronic circuits were constructed from discrete components that could be manipulated by humans. These discrete circuits consumed much space and power and were limited in speed, although they are still common in some applications. By contrast, integrated circuits packed a large number—often millions—of tiny electrical components, mainly transistors, into a small chip around the size of a coin. This allowed for the powerful computers and other electronic devices we see today.

Microelectronics engineering deals with the design and microfabrication of very small electronic circuit components for use in an integrated circuit or sometimes for use on their own as a general electronic component. The most common microelectronic components are semiconductor transistors, although all main electronic components (resistors, capacitors etc.) can be created at a microscopic level.

Nanoelectronics is the further scaling of devices down to nanometer levels. Modern devices are already in the nanometer regime, with below 100 nm processing having been standard since around 2002.

Microelectronic components are created by chemically fabricating wafers of semiconductors such as silicon (at higher frequencies, compound semiconductors like gallium arsenide and indium phosphide) to obtain the desired transport of electronic charge and control of current. The field of microelectronics involves a significant amount of chemistry and material science and requires the electronic engineer working in the field to have a very good working knowledge of the effects of quantum mechanics.

Signal processing deals with the analysis and manipulation of signals. Signals can be either analog, in which case the signal varies continuously according to the information, or digital, in which case the signal varies according to a series of discrete values representing the information. For analog signals, signal processing may involve the amplification and filtering of audio signals for audio equipment or the modulation and demodulation of signals for telecommunications. For digital signals, signal processing may involve the compression, error detection and error correction of digitally sampled signals.

Signal Processing is a very mathematically oriented and intensive area forming the core of digital signal processing and it is rapidly expanding with new applications in every field of electrical engineering such as communications, control, radar, audio engineering, broadcast engineering, power electronics, and biomedical engineering as many already existing analog systems are replaced with their digital counterparts. Analog signal processing is still important in the design of many control systems.

DSP processor ICs are found in many types of modern electronic devices, such as digital television sets, radios, Hi-Fi audio equipment, mobile phones, multimedia players, camcorders and digital cameras, automobile control systems, noise cancelling headphones, digital spectrum analyzers, missile guidance systems, radar systems, and telematics systems. In such products, DSP may be responsible for noise reduction, speech recognition or synthesis, encoding or decoding digital media, wirelessly transmitting or receiving data, triangulating position using GPS, and other kinds of image processing, video processing, audio processing, and speech processing.

Telecommunications engineering focuses on the transmission of information across a communication channel such as a coax cable, optical fiber or free space. Transmissions across free space require information to be encoded in a carrier signal to shift the information to a carrier frequency suitable for transmission; this is known as modulation. Popular analog modulation techniques include amplitude modulation and frequency modulation. The choice of modulation affects the cost and performance of a system and these two factors must be balanced carefully by the engineer.

Once the transmission characteristics of a system are determined, telecommunication engineers design the transmitters and receivers needed for such systems. These two are sometimes combined to form a two-way communication device known as a transceiver. A key consideration in the design of transmitters is their power consumption as this is closely related to their signal strength. Typically, if the power of the transmitted signal is insufficient once the signal arrives at the receiver's antenna(s), the information contained in the signal will be corrupted by noise.

Instrumentation engineering deals with the design of devices to measure physical quantities such as pressure, flow, and temperature. The design of such instruments requires a good understanding of physics that often extends beyond electromagnetic theory. For example, flight instruments measure variables such as wind speed and altitude to enable pilots the control of aircraft analytically. Similarly, thermocouples use the Peltier-Seebeck effect to measure the temperature difference between two points.

Often instrumentation is not used by itself, but instead as the sensors of larger electrical systems. For example, a thermocouple might be used to help ensure a furnace's temperature remains constant. For this reason, instrumentation engineering is often viewed as the counterpart of control.

Computer engineering deals with the design of computers and computer systems. This may involve the design of new hardware, the design of PDAs, tablets, and supercomputers, or the use of computers to control an industrial plant. Computer engineers may also work on a system's software. However, the design of complex software systems is often the domain of software engineering, which is usually considered a separate discipline. Desktop computers represent a tiny fraction of the devices a computer engineer might work on, as computer-like architectures are now found in a range of devices including video game consoles and DVD players.

Mechatronics is an engineering discipline which deals with the convergence of electrical and mechanical systems. Such combined systems are known as electromechanical systems and have widespread adoption. Examples include automated manufacturing systems, heating, ventilation and air-conditioning systems, and various subsystems of aircraft and automobiles.

"Electronic systems design" is the subject within electrical engineering that deals with the multi-disciplinary design issues of complex electrical and mechanical systems.

The term "mechatronics" is typically used to refer to macroscopic systems but futurists have predicted the emergence of very small electromechanical devices. Already, such small devices, known as Microelectromechanical systems (MEMS), are used in automobiles to tell airbags when to deploy, in digital projectors to create sharper images, and in inkjet printers to create nozzles for high definition printing. In the future it is hoped the devices will help build tiny implantable medical devices and improve optical communication.

Biomedical engineering is another related discipline, concerned with the design of medical equipment. This includes fixed equipment such as ventilators, MRI scanners, and electrocardiograph monitors as well as mobile equipment such as cochlear implants, artificial pacemakers, and artificial hearts.

Aerospace engineering and robotics an example is the most recent electric propulsion and ion propulsion.

Electrical engineers typically possess an academic degree with a major in electrical engineering, electronics engineering, electrical engineering technology, or electrical and electronic engineering. The same fundamental principles are taught in all programs, though emphasis may vary according to title. The length of study for such a degree is usually four or five years and the completed degree may be designated as a Bachelor of Science in Electrical/Electronics Engineering Technology, Bachelor of Engineering, Bachelor of Science, Bachelor of Technology, or Bachelor of Applied Science, depending on the university. The bachelor's degree generally includes units covering physics, mathematics, computer science, project management, and a variety of topics in electrical engineering. Initially such topics cover most, if not all, of the subdisciplines of electrical engineering. At some schools, the students can then choose to emphasize one or more subdisciplines towards the end of their courses of study.

At many schools, electronic engineering is included as part of an electrical award, sometimes explicitly, such as a Bachelor of Engineering (Electrical and Electronic), but in others, electrical and electronic engineering are both considered to be sufficiently broad and complex that separate degrees are offered.

Some electrical engineers choose to study for a postgraduate degree such as a Master of Engineering/Master of Science (MEng/MSc), a Master of Engineering Management, a Doctor of Philosophy (PhD) in Engineering, an Engineering Doctorate (Eng.D.), or an Engineer's degree. The master's and engineer's degrees may consist of either research, coursework or a mixture of the two. The Doctor of Philosophy and Engineering Doctorate degrees consist of a significant research component and are often viewed as the entry point to academia. In the United Kingdom and some other European countries, Master of Engineering is often considered to be an undergraduate degree of slightly longer duration than the Bachelor of Engineering rather than a standalone postgraduate degree.

In most countries, a bachelor's degree in engineering represents the first step towards professional certification and the degree program itself is certified by a professional body. After completing a certified degree program the engineer must satisfy a range of requirements (including work experience requirements) before being certified. Once certified the engineer is designated the title of Professional Engineer (in the United States, Canada and South Africa), Chartered Engineer or Incorporated Engineer (in India, Pakistan, the United Kingdom, Ireland and Zimbabwe), Chartered Professional Engineer (in Australia and New Zealand) or European Engineer (in much of the European Union).
The advantages of licensure vary depending upon location. For example, in the United States and Canada "only a licensed engineer may seal engineering work for public and private clients". This requirement is enforced by state and provincial legislation such as Quebec's Engineers Act. In other countries, no such legislation exists. Practically all certifying bodies maintain a code of ethics that they expect all members to abide by or risk expulsion. In this way these organizations play an important role in maintaining ethical standards for the profession. Even in jurisdictions where certification has little or no legal bearing on work, engineers are subject to contract law. In cases where an engineer's work fails he or she may be subject to the tort of negligence and, in extreme cases, the charge of criminal negligence. An engineer's work must also comply with numerous other rules and regulations, such as building codes and legislation pertaining to environmental law.

Professional bodies of note for electrical engineers include the Institute of Electrical and Electronics Engineers (IEEE) and the Institution of Engineering and Technology (IET). The IEEE claims to produce 30% of the world's literature in electrical engineering, has over 360,000 members worldwide and holds over 3,000 conferences annually. The IET publishes 21 journals, has a worldwide membership of over 150,000, and claims to be the largest professional engineering society in Europe. Obsolescence of technical skills is a serious concern for electrical engineers. Membership and participation in technical societies, regular reviews of periodicals in the field and a habit of continued learning are therefore essential to maintaining proficiency. An MIET(Member of the Institution of Engineering and Technology) is recognised in Europe as an Electrical and computer (technology) engineer.

In Australia, Canada, and the United States electrical engineers make up around 0.25% of the labor force.

From the Global Positioning System to electric power generation, electrical engineers have contributed to the development of a wide range of technologies. They design, develop, test, and supervise the deployment of electrical systems and electronic devices. For example, they may work on the design of telecommunication systems, the operation of electric power stations, the lighting and wiring of buildings, the design of household appliances, or the electrical control of industrial machinery.
Fundamental to the discipline are the sciences of physics and mathematics as these help to obtain both a qualitative and quantitative description of how such systems will work. Today most engineering work involves the use of computers and it is commonplace to use computer-aided design programs when designing electrical systems. Nevertheless, the ability to sketch ideas is still invaluable for quickly communicating with others.
Although most electrical engineers will understand basic circuit theory (that is the interactions of elements such as resistors, capacitors, diodes, transistors, and inductors in a circuit), the theories employed by engineers generally depend upon the work they do. For example, quantum mechanics and solid state physics might be relevant to an engineer working on VLSI (the design of integrated circuits), but are largely irrelevant to engineers working with macroscopic electrical systems. Even circuit theory may not be relevant to a person designing telecommunication systems that use off-the-shelf components. Perhaps the most important technical skills for electrical engineers are reflected in university programs, which emphasize strong numerical skills, computer literacy, and the ability to understand the technical language and concepts that relate to electrical engineering.

A wide range of instrumentation is used by electrical engineers. For simple control circuits and alarms, a basic multimeter measuring voltage, current, and resistance may suffice. Where time-varying signals need to be studied, the oscilloscope is also an ubiquitous instrument. In RF engineering and high frequency telecommunications, spectrum analyzers and network analyzers are used. In some disciplines, safety can be a particular concern with instrumentation. For instance, medical electronics designers must take into account that much lower voltages than normal can be dangerous when electrodes are directly in contact with internal body fluids. Power transmission engineering also has great safety concerns due to the high voltages used; although voltmeters may in principle be similar to their low voltage equivalents, safety and calibration issues make them very different. Many disciplines of electrical engineering use tests specific to their discipline. Audio electronics engineers use audio test sets consisting of a signal generator and a meter, principally to measure level but also other parameters such as harmonic distortion and noise. Likewise, information technology have their own test sets, often specific to a particular data format, and the same is true of television broadcasting.
For many engineers, technical work accounts for only a fraction of the work they do. A lot of time may also be spent on tasks such as discussing proposals with clients, preparing budgets and determining project schedules. Many senior engineers manage a team of technicians or other engineers and for this reason project management skills are important. Most engineering projects involve some form of documentation and strong written communication skills are therefore very important.

The workplaces of engineers are just as varied as the types of work they do. Electrical engineers may be found in the pristine lab environment of a fabrication plant, on board a Naval ship, the offices of a consulting firm or on site at a mine. During their working life, electrical engineers may find themselves supervising a wide range of individuals including scientists, electricians, computer programmers, and other engineers.

Electrical engineering has an intimate relationship with the physical sciences. For instance, the physicist Lord Kelvin played a major role in the engineering of the first transatlantic telegraph cable. Conversely, the engineer Oliver Heaviside produced major work on the mathematics of transmission on telegraph cables. Electrical engineers are often required on major science projects. For instance, large particle accelerators such as CERN need electrical engineers to deal with many aspects of the project including the power distribution, the instrumentation, and the manufacture and installation of the superconducting electromagnets.





</doc>
<doc id="9532" url="https://en.wikipedia.org/wiki?curid=9532" title="Electromagnetism">
Electromagnetism

Electromagnetism is a branch of physics involving the study of the electromagnetic force, a type of physical interaction that occurs between electrically charged particles. The electromagnetic force is carried by electromagnetic fields composed of electric fields and magnetic fields, and it is responsible for electromagnetic radiation such as light. It is one of the four fundamental interactions (commonly called forces) in nature, together with the strong interaction, the weak interaction, and gravitation. At high energy the weak force and electromagnetic force are unified as a single electroweak force.

Electromagnetic phenomena are defined in terms of the electromagnetic force, sometimes called the Lorentz force, which includes both electricity and magnetism as different manifestations of the same phenomenon. The electromagnetic force plays a major role in determining the internal properties of most objects encountered in daily life. The electromagnetic attraction between atomic nuclei and their orbital electrons holds atoms together. Electromagnetic forces are responsible for the chemical bonds between atoms which create molecules, and intermolecular forces. The electromagnetic force governs all chemical processes, which arise from interactions between the electrons of neighboring atoms.

There are numerous mathematical descriptions of the electromagnetic field. In classical electrodynamics, electric fields are described as electric potential and electric current. In Faraday's law, magnetic fields are associated with electromagnetic induction and magnetism, and Maxwell's equations describe how electric and magnetic fields are generated and altered by each other and by charges and currents.

The theoretical implications of electromagnetism, particularly the establishment of the speed of light based on properties of the "medium" of propagation (permeability and permittivity), led to the development of special relativity by Albert Einstein in 1905.

Originally, electricity and magnetism were considered to be two separate forces. This view changed with the publication of James Clerk Maxwell's 1873 "A Treatise on Electricity and Magnetism" in which the interactions of positive and negative charges were shown to be mediated by one force. There are four main effects resulting from these interactions, all of which have been clearly demonstrated by experiments:
While preparing for an evening lecture on 21 April 1820, Hans Christian Ørsted made a surprising observation. As he was setting up his materials, he noticed a compass needle deflected away from magnetic north when the electric current from the battery he was using was switched on and off. This deflection convinced him that magnetic fields radiate from all sides of a wire carrying an electric current, just as light and heat do, and that it confirmed a direct relationship between electricity and magnetism.

At the time of discovery, Ørsted did not suggest any satisfactory explanation of the phenomenon, nor did he try to represent the phenomenon in a mathematical framework. However, three months later he began more intensive investigations. Soon thereafter he published his findings, proving that an electric current produces a magnetic field as it flows through a wire. The CGS unit of magnetic induction (oersted) is named in honor of his contributions to the field of electromagnetism.
His findings resulted in intensive research throughout the scientific community in electrodynamics. They influenced French physicist André-Marie Ampère's developments of a single mathematical form to represent the magnetic forces between current-carrying conductors. Ørsted's discovery also represented a major step toward a unified concept of energy.

This unification, which was observed by Michael Faraday, extended by James Clerk Maxwell, and partially reformulated by Oliver Heaviside and Heinrich Hertz, is one of the key accomplishments of 19th-century mathematical physics. It has had far-reaching consequences, one of which was the understanding of the nature of light. Unlike what was proposed by the electromagnetic theory of that time, light and other electromagnetic waves are at present seen as taking the form of quantized, self-propagating oscillatory electromagnetic field disturbances called photons. Different frequencies of oscillation give rise to the different forms of electromagnetic radiation, from radio waves at the lowest frequencies, to visible light at intermediate frequencies, to gamma rays at the highest frequencies.

Ørsted was not the only person to examine the relationship between electricity and magnetism. In 1802, Gian Domenico Romagnosi, an Italian legal scholar, deflected a magnetic needle using a Voltaic pile. The factual setup of the experiment is not completely clear, so if current flowed across the needle or not. An account of the discovery was published in 1802 in an Italian newspaper, but it was largely overlooked by the contemporary scientific community, because Romagnosi seemingly did not belong to this community.
An earlier (1735), and often neglected, connection between electricity and magnetism was reported by a Dr. Cookson. The account stated:A tradesman at Wakefield in Yorkshire, having put up a great number of knives and forks in a large box ... and having placed the box in the corner of a large room, there happened a sudden storm of thunder, lightning, &c. ... The owner emptying the box on a counter where some nails lay, the persons who took up the knives, that lay on the nails, observed that the knives took up the nails. On this the whole number was tried, and found to do the same, and that, to such a degree as to take up large nails, packing needles, and other iron things of considerable weight ... E. T. Whittaker suggested in 1910 that this particular event was responsible for lightning to be "credited with the power of magnetizing steel; and it was doubtless this which led Franklin in 1751 to attempt to magnetize a sewing-needle by means of the discharge of Leyden jars." 

The electromagnetic force is one of the four known fundamental forces. The other fundamental forces are:
All other forces (e.g., friction, contact forces) are derived from these four fundamental forces and they are known as Non-fundamental forces.

The electromagnetic force is responsible for practically all phenomena one encounters in daily life above the nuclear scale, with the exception of gravity. Roughly speaking, all the forces involved in interactions between atoms can be explained by the electromagnetic force acting between the electrically charged atomic nuclei and electrons of the atoms. Electromagnetic forces also explain how these particles carry momentum by their movement. This includes the forces we experience in "pushing" or "pulling" ordinary material objects, which result from the intermolecular forces that act between the individual molecules in our bodies and those in the objects. The electromagnetic force is also involved in all forms of chemical phenomena.

A necessary part of understanding the intra-atomic and intermolecular forces is the effective force generated by the momentum of the electrons' movement, such that as electrons move between interacting atoms they carry momentum with them. As a collection of electrons becomes more confined, their minimum momentum necessarily increases due to the Pauli exclusion principle. The behaviour of matter at the molecular scale including its density is determined by the balance between the electromagnetic force and the force generated by the exchange of momentum carried by the electrons themselves.

In 1600, William Gilbert proposed, in his "De Magnete", that electricity and magnetism, while both capable of causing attraction and repulsion of objects, were distinct effects. Mariners had noticed that lightning strikes had the ability to disturb a compass needle. The link between lightning and electricity was not confirmed until Benjamin Franklin's proposed experiments in 1752. One of the first to discover and publish a link between man-made electric current and magnetism was Gian Romagnosi, who in 1802 noticed that connecting a wire across a voltaic pile deflected a nearby compass needle. However, the effect did not become widely known until 1820, when Ørsted performed a similar experiment. Ørsted's work influenced Ampère to produce a theory of electromagnetism that set the subject on a mathematical foundation.

A theory of electromagnetism, known as classical electromagnetism, was developed by various physicists during the period between 1820 and 1873 when it culminated in the publication of a treatise by James Clerk Maxwell, which unified the preceding developments into a single theory and discovered the electromagnetic nature of light. In classical electromagnetism, the behavior of the electromagnetic field is described by a set of equations known as Maxwell's equations, and the electromagnetic force is given by the Lorentz force law.

One of the peculiarities of classical electromagnetism is that it is difficult to reconcile with classical mechanics, but it is compatible with special relativity. According to Maxwell's equations, the speed of light in a vacuum is a universal constant that is dependent only on the electrical permittivity and magnetic permeability of free space. This violates Galilean invariance, a long-standing cornerstone of classical mechanics. One way to reconcile the two theories (electromagnetism and classical mechanics) is to assume the existence of a luminiferous aether through which the light propagates. However, subsequent experimental efforts failed to detect the presence of the aether. After important contributions of Hendrik Lorentz and Henri Poincaré, in 1905, Albert Einstein solved the problem with the introduction of special relativity, which replaced classical kinematics with a new theory of kinematics compatible with classical electromagnetism. (For more information, see History of special relativity.)

In addition, relativity theory implies that in moving frames of reference, a magnetic field transforms to a field with a nonzero electric component and conversely, a moving electric field transforms to a nonzero magnetic component, thus firmly showing that the phenomena are two sides of the same coin. Hence the term "electromagnetism". (For more information, see Classical electromagnetism and special relativity and Covariant formulation of classical electromagnetism.)

The Maxwell equations are "linear," in that a change in the sources (the charges and currents) results in a proportional change of the fields. Nonlinear dynamics can occur when electromagnetic fields couple to matter that follows nonlinear dynamical laws. This is studied, for example, in the subject of magnetohydrodynamics, which combines Maxwell theory with the Navier–Stokes equations.

Electromagnetic units are part of a system of electrical units based primarily upon the magnetic properties of electric currents, the fundamental SI unit being the ampere. The units are:


In the electromagnetic cgs system, electric current is a fundamental quantity defined via Ampère's law and takes the permeability as a dimensionless quantity (relative permeability) whose value in a vacuum is unity. As a consequence, the square of the speed of light appears explicitly in some of the equations interrelating quantities in this system.

Formulas for physical laws of electromagnetism (such as Maxwell's equations) need to be adjusted depending on what system of units one uses. This is because there is no one-to-one correspondence between electromagnetic units in SI and those in CGS, as is the case for mechanical units. Furthermore, within CGS, there are several plausible choices of electromagnetic units, leading to different unit "sub-systems", including Gaussian, "ESU", "EMU", and Heaviside–Lorentz. Among these choices, Gaussian units are the most common today, and in fact the phrase "CGS units" is often used to refer specifically to CGS-Gaussian units.




</doc>
<doc id="9534" url="https://en.wikipedia.org/wiki?curid=9534" title="Euphemism">
Euphemism

A euphemism () is an innocuous word or expression used in place of one that may be found offensive or suggest something unpleasant. Some euphemisms are intended to amuse, while others use bland, inoffensive terms for concepts that the user wishes to downplay. Euphemisms may be used to mask profanity or refer to taboo topics such as disability, sex, excretion, or death in a polite way.

"Euphemism" comes from the Greek word () which refers to the use of 'words of good omen'; it is a compound of (), meaning 'good, well', and (), meaning 'prophetic speech; rumour, talk'. "Eupheme" is a reference to the female Greek spirit of words of praise and positivity, etc. The term "euphemism" itself was used as a euphemism by the ancient Greeks; with the meaning "to keep a holy silence" (speaking well by not speaking at all).

Reasons for using euphemisms vary by context and intent. Commonly, euphemisms are used to avoid directly addressing subjects that might be deemed negative or embarrassing, e.g. death, sex, excretory bodily functions. They may be created for innocent, well-intentioned purposes or nefariously and cynically, intentionally to deceive and confuse.

Euphemisms are also used to mitigate, soften or downplay the gravity of large-scale injustices, war crimes, or other events that warrant a pattern of avoidance in official statements or documents. For instance, one reason for the comparative scarcity of written evidence documenting the exterminations at Auschwitz, relative to their sheer number, is "directives for the extermination process obscured in bureaucratic euphemisms".

Euphemisms are sometimes used to lessen the opposition to a political move. For example, according to linguist Ghil'ad Zuckermann, Israeli Prime Minister Benjamin Netanyahu used the neutral Hebrew lexical item פעימות "peimót" ("beatings (of the heart)"), rather than נסיגה "nesigá" ("withdrawal"), to refer to the stages in the Israeli withdrawal from the West Bank (see Wye River Memorandum), in order to lessen the opposition of right-wing Israelis to such a move. The lexical item פעימות "peimót", which literally means "beatings (of the heart)" is thus a euphemism for "withdrawal".

Euphemism may be used as a rhetorical strategy, in which case its goal is to change the valence of a description.

The act of labeling a term as a euphemism can in itself be controversial, as in the following two examples:

There is some disagreement over whether certain terms are or are not euphemisms. For example, sometimes the phrase "visually impaired" is labeled as a politically correct euphemism for "blind". However, "visual impairment" can be a broader term, including, for example, people who have partial sight in one eye, those with uncorrectable mild to moderate poor vision, or even those who wear glasses, groups that would be excluded by the word "blind" or even "partially blind".

The coining and usage of euphemisms reveals what the coiner or speaker/writer considers, perhaps only sub-consciously, to be shameful, and may thus be an indication of prejudices or opinions held.

Phonetic euphemism is used to replace profanities, diminishing their intensity. Modifications include:

To alter the pronunciation or spelling of a taboo word (such as a swear word) to form a euphemism is known as "taboo deformation", or a minced oath. In American English, words that are unacceptable on television, such as "fuck", may be represented by deformations such as "freak", even in children's cartoons. Feck is a minced oath originating in Hiberno-English and popularised outside of Ireland by the British sitcom "Father Ted". Some examples of Cockney rhyming slang may serve the same purpose: to call a person a "berk" sounds less offensive than to call a person a "cunt", though "berk" is short for Berkeley Hunt, which rhymes with "cunt".

Euphemisms formed from understatements include: "asleep" for dead and "drinking" for consuming alcohol. "Tired and emotional" is a notorious euphemism for "drunk", used by British parliamentarians of one another.

Pleasant, positive, worthy, neutral or non-descript terms are substituted for explicit or unpleasant ones, with many substituted terms deliberately coined by socio-political progressive movements, cynically by planned marketing, public relations or advertising initiatives, including:

Over time it becomes socially unacceptable to use the former word, as one is effectively down-grading the matter concerned to its former lower status, and the euphemism becomes dominant, due to a wish not to offend.


The use of a term with a softer connotation, though it shares the same meaning. For instance, "screwed up" is a euphemism for "fucked up"; "hook-up" and "laid" are euphemisms for sexual intercourse.

Expressions or words from a foreign language may be imported for use as a replacement for an offensive word. For example, the French word "enceinte" was sometimes used instead of the English word "pregnant"; "abattoir" for "slaughter-house", although in French the word retains its explicit violent meaning "a place for beating down", conveniently lost on non-French speakers. "Entrepreneur" for "business-man", adds glamour; "douche" (French: shower) for vaginal irrigation device; "bidet" (French: little pony) for "vessel for intimate ablutions". Ironically, whilst in English physical "handicap" is almost always substituted for a modern euphemism, in French the English word "handicap" is used as a euphemism for their problematic words "infirmité" or "invalidité".

Periphrasis, or circumlocution, is one of the most common: to "speak around" a given word, implying it without saying it. Over time, circumlocutions become recognized as established euphemisms for particular words or ideas.

Bureaucracies frequently spawn euphemisms intentionally, as doublespeak expressions. For example, in the past, the US military used the term "sunshine units" for contamination by radioactive isotopes. An effective death sentence in the Soviet Union during the Great Purge often used the clause "imprisonment without right to correspondence": the person sentenced would be shot soon after conviction. As early as 1939, Nazi official Reinhard Heydrich used the term "Sonderbehandlung" ("special treatment") to mean summary execution of persons viewed as "disciplinary problems" by the Nazis even before commencing the systematic extermination of the Jews. Heinrich Himmler, aware that the word had come to be known to mean murder, replaced that euphemism with one in which Jews would be "guided" (to their deaths) through the slave-labor and extermination camps after having been "evacuated" to their doom. Such was part of the formulation of "Endlösung der Judenfrage" (the "Final Solution to the Jewish Question"), which became known to the outside world during the Nuremberg Trials.

Frequently, over time, euphemisms themselves become taboo words, through the linguistic process known as semantic change (specifically pejoration) described by W. V. O. Quine, and more recently dubbed the "euphemism treadmill" by Harvard professor Steven Pinker. For instance, "toilet" is an 18th-century euphemism, replacing the older euphemism "house-of-office", which in turn replaced the even older euphemisms "privy-house" and "bog-house". The act of human defecation is possibly the most needy candidate for the euphemism in all eras. In the 20th century, where the old euphemisms "lavatory" (a place where one washes) or "toilet" (a place where one dresses) had grown from long usage (e.g. in the United States) too synonymous with the crude act they sought to deflect, they were sometimes replaced with "bathroom" (a place where one bathes) "washroom" (a place where one washes) or "restroom" (a place where one rests) or even by the extreme form "powder-room" (a place where one applies facial cosmetics). The form "water closet", which in turn became euphemised to "W.C.", is a less deflective form.

Another example in American English is the replacement of colored with Negro (euphemism by foreign language), then the "honest" non-euphemistic form "Black" making a brief appearance (due to 1960s political forces attempting to normalise black skin) before being suppressed again by the present euphemistic "African American". Venereal disease, which associated shameful bacterial infection with a seemingly worthy ailment emanating from Venus the goddess of love, soon lost its deflective force in the post-classical education era, as "VD", which has now been replaced by the three-letter acronym "STI".

The word "shit" appears to have originally been a euphemism for defecation in Pre-Germanic, as the Proto-Indo-European root *"", from which it was derived, meant 'to cut off'.

Doublespeak is a term sometimes used for deliberate euphemistic misuse of words to distort or reverse their meaning, as in a "Ministry of Peace" which wages war, and a "Ministry of Love" which imprisons and tortures. It is a portmanteau of the terms "Newspeak" and "doublethink", which originate from George Orwell's novel "Nineteen Eighty-Four".

The word "euphemism" itself can be used as a euphemism. In the animated TV special "Halloween Is Grinch Night" (see Dr. Seuss), a child asks to go to the "euphemism", where "euphemism" is being used as a euphemism for "outhouse". This euphemistic use of "euphemism" also occurred in the play "Who's Afraid of Virginia Woolf?" where a character requests, "Martha, will you show her where we keep the, uh, euphemism?"

The song Makin' Whoopee from the 1928 musical Whoopee! introduced us to a new euphemism for sexual intercourse. The phrase "make whoopee" was often used on the popular game show The Newlywed Game starting in the late 1960s, whenever the host asked a question about sexual relations. This successfully avoided the network censors.

In Wes Anderson's film "Fantastic Mr. Fox", the replacement of swear words by the word "cuss" became a humorous motif throughout the film.

In Tom Hanks's web series "Electric City", the use of profanity has been censored by the word "expletive". "[Expletive deleted]" entered public discourse after its notorious use in censoring transcripts of the Watergate tapes.

In Isaac Asimov's "Foundation" series, the curses of the scientist Ebling Mis have all been replaced with the word "unprintable". In fact, there is only one case of his curses being referred to as such, leading some readers to mistakenly assume that the euphemism is Ebling's, rather than Asimov's. The same word has also been used in his short story "Flies".

George Carlin has stated in audio books and his stand-up shows that euphemisms soften everyday language and take the life out of it.

In "Battlestar Galactica" (2004), use of the words "frak" and "frakking" was directly substituted for the English slang words "fuck" and "fucking", confounding the censors. Other science fiction series have similarly used word substitution to avoid censorship, such as "frell" instead of "fuck" in "Farscape", "gorram" and "rutting" instead of "goddamn" and "fucking" in "Firefly", and "frag" instead of "fuck" in "Babylon 5". "The Good Place" takes this word substitution to its logical extreme, replacing all profanities with similar-sounding English words under the premise that such words may not be spoken in a perfect afterlife in order to avoid making anyone uncomfortable; "son of a bitch" becomes "son of a bench", "bullshit" becomes "bullshirt", and "fuck" becomes "fork".

In "The Sims" series, the word WooHoo is used as a euphemistic slang for various activities of sexual intercourse in the series.



</doc>
<doc id="9536" url="https://en.wikipedia.org/wiki?curid=9536" title="Edmund Spenser">
Edmund Spenser

Edmund Spenser (; 1552/1553 – 13 January 1599) was an English poet best known for "The Faerie Queene", an epic poem and fantastical allegory celebrating the Tudor dynasty and Elizabeth I. He is recognized as one of the premier craftsmen of nascent Modern English verse, and is often considered one of the greatest poets in the English language.

Edmund Spenser was born in East Smithfield, London, around the year 1552, though there is still some ambiguity as to the exact date of his birth. His parenthood is obscure, but he was probably the son of John Spenser, a journeyman clothmaker. As a young boy, he was educated in London at the Merchant Taylors' School and matriculated as a sizar at Pembroke College, Cambridge. While at Cambridge he became a friend of Gabriel Harvey and later consulted him, despite their differing views on poetry. In 1578, he became for a short time secretary to John Young, Bishop of Rochester. In 1579, he published "The Shepheardes Calender" and around the same time married his first wife, Machabyas Childe. They had two children, Sylvanus (d. 1638) and Katherine.

In July 1580, Spenser went to Ireland in service of the newly appointed Lord Deputy, Arthur Grey, 14th Baron Grey de Wilton. Spenser served under Lord Grey with Walter Raleigh at the Siege of Smerwick massacre. When Lord Grey was recalled to England, Spenser stayed on in Ireland, having acquired other official posts and lands in the Munster Plantation. Raleigh acquired other nearby Munster estates confiscated in the Second Desmond Rebellion. Some time between 1587 and 1589, Spenser acquired his main estate at Kilcolman, near Doneraile in North Cork. He later bought a second holding to the south, at Rennie, on a rock overlooking the river Blackwater in North Cork. Its ruins are still visible today. A short distance away grew a tree, locally known as "Spenser's Oak" until it was destroyed in a lightning strike in the 1960s. Local legend has it that he penned some of "The Faerie Queene" under this tree.

In 1590, Spenser brought out the first three books of his most famous work, "The Faerie Queene", having travelled to London to publish and promote the work, with the likely assistance of Raleigh. He was successful enough to obtain a life pension of £50 a year from the Queen. He probably hoped to secure a place at court through his poetry, but his next significant publication boldly antagonised the queen's principal secretary, Lord Burghley (William Cecil), through its inclusion of the satirical "Mother Hubberd's Tale". He returned to Ireland.

In 1591, Spenser published a translation in verse of Joachim Du Bellay's sonnets, "Les Antiquités de Rome", which had been published in 1558. Spenser's version, "Ruines of Rome: by Bellay", may also have been influenced by Latin poems on the same subject, written by Jean or Janis Vitalis and published in 1576.

By 1594, Spenser's first wife had died, and in that year he married a much younger Elizabeth Boyle, sister of Richard Boyle, 1st Earl of Cork. He addressed to her the sonnet sequence "Amoretti". The marriage itself was celebrated in "Epithalamion". They had a son named Peregrine.

In 1596, Spenser wrote a prose pamphlet titled "A View of the Present State of Ireland". This piece, in the form of a dialogue, circulated in manuscript, remaining unpublished until the mid-seventeenth century. It is probable that it was kept out of print during the author's lifetime because of its inflammatory content. The pamphlet argued that Ireland would never be totally "pacified" by the English until its indigenous language and customs had been destroyed, if necessary by violence.

In 1598, during the Nine Years' War, Spenser was driven from his home by the native Irish forces of Aodh Ó Néill. His castle at Kilcolman was burned, and Ben Jonson, who may have had private information, asserted that one of his infant children died in the blaze.

In the year after being driven from his home, 1599, Spenser travelled to London, where he died at the age of forty-six – "for want of bread", according to Ben Jonson; one of Jonson's more doubtful statements, since Spenser had a payment to him authorised by the government and was due his pension. His coffin was carried to his grave in Poets' Corner in Westminster Abbey by other poets, who threw many pens and pieces of poetry into his grave with many tears. His second wife survived him and remarried twice. His sister Sarah, who had accompanied him to Ireland, married into the Travers family, and her descendants were prominent landowners in Cork for centuries.

Thomas Fuller, in "Worthies of England", included a story where the Queen told her treasurer, William Cecil, to pay Spenser one hundred pounds for his poetry. The treasurer, however, objected that the sum was too much. She said, "Then give him what is reason". Without receiving his payment in due time, Spenser gave the Queen this quatrain on one of her progresses:

<poem>
I was promis'd on a time,
To have a reason for my rhyme:
From that time unto this season,
I receiv'd nor rhyme nor reason.
</poem>

She immediately ordered the treasurer pay Spenser the original £100.

This story seems to have attached itself to Spenser from Thomas Churchyard, who apparently had difficulty in getting payment of his pension, the only other pension Elizabeth awarded to a poet. Spenser seems to have had no difficulty in receiving payment when it was due as the pension was being collected for him by his publisher, Ponsonby.

"The Shepheardes Calender" is Edmund Spenser's first major work, which appeared in 1579. It emulates Virgil's "Eclogues" of the first century BCE and the "Eclogues" of Mantuan by Baptista Mantuanus, a late medieval, early renaissance poet. An eclogue is a short pastoral poem that is in the form of a dialogue or soliloquy. Although all the months together form an entire year, each month stands alone as a separate poem. Editions of the late 16th and early 17th centuries include woodcuts for each month/poem, and thereby have a slight similarity to an emblem book which combines a number of self-contained pictures and texts, usually a short vignette, saying, or allegory with an accompanying illustration.

Spenser's masterpiece is the epic poem "The Faerie Queene". The first three books of "The Faerie Queene" were published in 1590, and a second set of three books were published in 1596. Spenser originally indicated that he intended the poem to consist of twelve books, so the version of the poem we have today is incomplete. Despite this, it remains one of the longest poems in the English language. It is an allegorical work, and can be read (as Spenser presumably intended) on several levels of allegory, including as praise of Queen Elizabeth I. In a completely allegorical context, the poem follows several knights in an examination of several virtues. In Spenser's "A Letter of the Authors", he states that the entire epic poem is "cloudily enwrapped in allegorical devises", and that the aim behind "The Faerie Queene" was to "fashion a gentleman or noble person in virtuous and gentle discipline".

Spenser published numerous relatively short poems in the last decade of the sixteenth century, almost all of which consider love or sorrow. In 1591, he published "Complaints", a collection of poems that express complaints in mournful or mocking tones. Four years later, in 1595, Spenser published "Amoretti and Epithalamion". This volume contains eighty-nine sonnets commemorating his courtship of Elizabeth Boyle. In "Amoretti", Spenser uses subtle humour and parody while praising his beloved, reworking Petrarchism in his treatment of longing for a woman. "Epithalamion", similar to "Amoretti", deals in part with the unease in the development of a romantic and sexual relationship. It was written for his wedding to his young bride, Elizabeth Boyle. Some have speculated that the attention to disquiet in general reflects Spenser's personal anxieties at the time, as he was unable to complete his most significant work, "The Faerie Queene". In the following year Spenser released "Prothalamion", a wedding song written for the daughters of a duke, allegedly in hopes to gain favour in the court.

Spenser used a distinctive verse form, called the Spenserian stanza, in several works, including "The Faerie Queene". The stanza's main meter is iambic pentameter with a final line in iambic hexameter (having six feet or stresses, known as an Alexandrine), and the rhyme scheme is . He also used his own rhyme scheme for the sonnet. In a Spenserian sonnet, the last line of every quatrain is linked with the first line of the next one, yielding the rhyme scheme .

Though Spenser was well read in classical literature, scholars have noted that his poetry does not rehash tradition, but rather is distinctly his. This individuality may have resulted, to some extent, from a lack of comprehension of the classics. Spenser strove to emulate such ancient Roman poets as Virgil and Ovid, whom he studied during his schooling, but many of his best-known works are notably divergent from those of his predecessors. The language of his poetry is purposely archaic, reminiscent of earlier works such as "The Canterbury Tales" of Geoffrey Chaucer and "Il Canzoniere" of Francesco Petrarca, whom Spenser greatly admired.

An Anglican and a devotee of the Protestant Queen Elizabeth, Spenser was particularly offended by the anti-Elizabethan propaganda that some Catholics circulated. Like most Protestants near the time of the Reformation, Spenser saw a Catholic church full of corruption, and he determined that it was not only the wrong religion but the anti-religion. This sentiment is an important backdrop for the battles of "The Faerie Queene".

Spenser was called "the Poet's Poet" by Charles Lamb, and was admired by John Milton, William Blake, William Wordsworth, John Keats, Lord Byron, Alfred Tennyson and others. Among his contemporaries Walter Raleigh wrote a commendatory poem to "The Faerie Queene" in 1590, in which he claims to admire and value Spenser's work more so than any other in the English language. John Milton in his "Areopagitica" mentions "our sage and serious poet Spenser, whom I dare be known to think a better teacher than Scotus or Aquinas". In the eighteenth century, Alexander Pope compared Spenser to "a mistress, whose faults we see, but love her with them all."

In his work "A View of the Present State of Irelande" (1596), Spenser discussed future plans to subjugate Ireland, the most recent rising, led by Hugh O'Neill, having demonstrated the futility of previous efforts. The work is partly a defence of Lord Arthur Grey de Wilton, who was appointed Lord Deputy of Ireland in 1580, and who greatly influenced Spenser's thinking on Ireland.

The goal of this piece was to show that Ireland was in great need of reform. Spenser believed that "Ireland is a diseased portion of the State, it must first be cured and reformed, before it could be in a position to appreciate the good sound laws and blessings of the nation". In "A View of the Present State of Ireland", Spenser categorises the "evils" of the Irish people into three prominent categories: laws, customs and religion. These three elements work together in creating the disruptive and degraded people. One example given in the work is the native law system called "Brehon law" which trumps the established law given by the English monarchy. This system has its own court and way of dealing with infractions. It has been passed down through the generations and Spenser views this system as a native backward custom which must be destroyed. (Brehon law methods of dealing with murder by imposing an "éraic", or fine, on the murderer's whole family particularly horrified the English, in whose Protestant view a murderer should die for his act.)

Spenser wished devoutly that the Irish language should be eradicated, writing that if children learn Irish before English, "Soe that the speach being Irish, the hart must needes be Irishe; for out of the aboundance of the hart, the tonge speaketh".

He pressed for a scorched earth policy in Ireland, noting that the destruction of crops and animals had been successful in crushing the Second Desmond Rebellion (1579–83), when, despite the rich and bountiful land:

"'Out of everye corner of the woode and glenns they came creepinge forth upon theire handes, for theire legges could not beare them; they looked Anatomies [of] death, they spake like ghostes, crying out of theire graves; they did eate of the carrions, happye wheare they could find them, yea, and one another soone after, in soe much as the verye carcasses they spared not to scrape out of theire graves; and if they found a plott of water-cresses or shamrockes, theyr they flocked as to a feast… in a shorte space there were none almost left, and a most populous and plentyfull countrye suddenly lefte voyde of man or beast: yett sure in all that warr, there perished not manye by the sworde, but all by the extreamytie of famine ... they themselves had wrought'"


1590:

1591:

1592:

1595:

1596:
Posthumous:


Washington University in St. Louis professor Joseph Lowenstein, with the assistance of several undergraduate students, has been involved in creating, editing, and annotating a digital archive of the first publication of poet Edmund Spenser's collective works in 100 years. A large grant from the National Endowment for the Humanities has been given to support this ambitious project centralized at Washington University with support from other colleges in the United States.

|- style="text-align:center;" 


</doc>
<doc id="9540" url="https://en.wikipedia.org/wiki?curid=9540" title="Electricity generation">
Electricity generation

Electricity generation is the process of generating electric power from sources of primary energy. For utilities in the electric power industry, it is the stage prior to its delivery to end users (transmission, distribution, etc.) or its storage (using, for example, the pumped-storage method).

Electricity is not freely available in nature, so it must be "produced" (that is, transforming other forms of energy to electricity). Production is carried out in power stations (also called "power plants"). Electricity is most often generated at a power plant by electromechanical generators, primarily driven by heat engines fueled by combustion or nuclear fission but also by other means such as the kinetic energy of flowing water and wind. Other energy sources include solar photovoltaics and geothermal power.

The fundamental principles of electricity generation were discovered in the 1820s and early 1830s by British scientist Michael Faraday. His method, still used today, is for electricity to be generated by the movement of a loop of wire, or Faraday disc, between the poles of a magnet. Central power stations became economically practical with the development of alternating current (AC) power transmission, using power transformers to transmit power at high voltage and with low loss.

Commercial electricity production started in 1870 with the coupling of the dynamo to the hydraulic turbine. The mechanical production of electric power began the Second Industrial Revolution and made possible several inventions using electricity, with the major contributors being Thomas Alva Edison and Nikola Tesla. Previously the only way to produce electricity was by chemical reactions or using battery cells, and the only practical use of electricity was for the telegraph.

Electricity generation at central power stations started in 1882, when a steam engine driving a dynamo at Pearl Street Station produced a DC current that powered public lighting on Pearl Street, New York. The new technology was quickly adopted by many cities around the world, which adapted their gas-fueled street lights to electric power. Soon after electric lights would be used in public buildings, in businesses, and to power public transport, such as trams and trains.

The first power plants used water power or coal. Today a variety of energy sources are used, such as coal, nuclear, natural gas, hydroelectric, wind, and oil, as well as solar energy, tidal power, and geothermal sources.

Several fundamental methods exist to convert other forms of energy into electrical energy. Utility-scale generation is achieved by rotating electric generators or by photovoltaic systems. A small proportion of electric power distributed by utilities is provided by batteries. Other forms of electricity generation used in niche applications include the triboelectric effect, the piezoelectric effect, the thermoelectric effect, and betavoltaics.

Electric generators transform kinetic energy into electricity. This is the most used form for generating electricity and is based on Faraday's law. It can be seen experimentally by rotating a magnet within closed loops of conducting material (e.g. copper wire). Almost all commercial electrical generation is done using electromagnetic induction, in which mechanical energy forces a generator to rotate:

Electrochemistry is the direct transformation of chemical energy into electricity, as in a battery. Electrochemical electricity generation is important in portable and mobile applications. Currently, most electrochemical power comes from batteries. Primary cells, such as the common zinc–carbon batteries, act as power sources directly, but secondary cells (i.e. rechargeable batteries) are used for storage systems rather than primary generation systems. Open electrochemical systems, known as fuel cells, can be used to extract power either from natural fuels or from synthesized fuels. Osmotic power is a possibility at places where salt and fresh water merge.

The photovoltaic effect is the transformation of light into electrical energy, as in solar cells. Photovoltaic panels convert sunlight directly to DC electricity. Power inverters can then convert that to AC electricity if needed. Although sunlight is free and abundant, solar power electricity is still usually more expensive to produce than large-scale mechanically generated power due to the cost of the panels. Low-efficiency silicon solar cells have been decreasing in cost and multijunction cells with close to 30% conversion efficiency are now commercially available. Over 40% efficiency has been demonstrated in experimental systems. Until recently, photovoltaics were most commonly used in remote sites where there is no access to a commercial power grid, or as a supplemental electricity source for individual homes and businesses. Recent advances in manufacturing efficiency and photovoltaic technology, combined with subsidies driven by environmental concerns, have dramatically accelerated the deployment of solar panels. Installed capacity is growing by 40% per year led by increases in Germany, Japan, United States, China, and India.

The selection of electricity production modes and their economic viability varies in accordance with demand and region. The economics vary considerably around the world, resulting in widespread residential selling prices, e.g. the price in Iceland is 5.54 cents per kWh while in some island nations it is 40 cents per kWh. Hydroelectric plants, nuclear power plants, thermal power plants and renewable sources have their own pros and cons, and selection is based upon the local power requirement and the fluctuations in demand. All power grids have varying loads on them but the daily minimum is the base load, often supplied by plants which run continuously. Nuclear, coal, oil, gas and some hydro plants can supply base load. If well construction costs for natural gas are below $10 per MWh, generating electricity from natural gas is cheaper than generating power by burning coal.

Thermal energy may be economical in areas of high industrial density, as the high demand cannot be met by local renewable sources. The effect of localized pollution is also minimized as industries are usually located away from residential areas. These plants can also withstand variation in load and consumption by adding more units or temporarily decreasing the production of some units.
Nuclear power plants can produce a huge amount of power from a single unit. However, disasters in Japan have raised concerns over the safety of nuclear power, and the capital cost of nuclear plants is very high.
Hydroelectric power plants are located in areas where the potential energy from falling water can be harnessed for moving turbines and the generation of power. It may not be an economically viable single source of production where the ability to store the flow of water is limited and the load varies too much during the annual production cycle.

Due to advancements in technology, and with mass production, renewable sources other than hydroelectricity (solar power, wind energy, tidal power, etc.) experienced decreases in cost of production, and the energy is now in many cases as expensive or less expensive than fossil fuels. Many governments around the world provide subsidies to offset the higher cost of any new power production, and to make the installation of renewable energy systems economically feasible.

Electric generators were known in simple forms from the discovery of electromagnetic induction in the 1830s. In general, some form of prime mover such as an engine or the turbines described above, drives a rotating magnetic field past stationary coils of wire thereby turning mechanical energy into electricity. The only commercial scale electricity production that does not employ a generator is solar PV.

Almost all commercial electrical power on Earth is generated with a turbine, driven by wind, water, steam or burning gas. The turbine drives a generator, thus transforming its mechanical energy into electrical energy by electromagnetic induction. There are many different methods of developing mechanical energy, including heat engines, hydro, wind and tidal power. Most electric generation is driven by heat engines. The combustion of fossil fuels supplies most of the energy to these engines, with a significant fraction from nuclear fission and some from renewable sources. The modern steam turbine (invented by Sir Charles Parsons in 1884) currently generates about 80% of the electric power in the world using a variety of heat sources. Turbine types include:
Although turbines are most common in commercial power generation, smaller generators can be powered by gasoline or diesel engines. These may used for backup generation or as a prime source of power within isolated villages.

Total worldwide gross production of electricity in 2016 was 25,082 TWh. Sources of electricity were coal and peat 38.3%, natural gas 23.1%, hydroelectric 16.6%, nuclear power 10.4%, oil 3.7%, solar/wind/geothermal/tidal/other 5.6%, biomass and waste 2.3%.

Total energy consumed at all power plants for the generation of electricity was which was 36% of the total for primary energy sources (TPES) of 2008. Electricity output (gross) was , efficiency was 39%, and the balance of 61% was generated heat. A small part, or about 3% of the input total, of the heat was utilized at co-generation heat and power plants. The in-house consumption of electricity and power transmission losses were .
The amount supplied to the final consumer was which was 33% of the total energy consumed at power plants and heat and power co-generation (CHP) plants.

Note that the vertical axes of these two charts are not to the same scale.

The United States has long been the largest producer and consumer of electricity, with a global share in 2005 of at least 25%, followed by China, Japan, Russia, and India. In 2011, China overtook the United States to become the largest producer of electricity.

Data source of values (electric power generated) is IEA/OECD.
Listed countries are top 20 by population or top 20 by GDP (PPP) and Saudi Arabia based on CIA World Factbook 2009.

Solar PV* is Photovoltaics 
Bio other* = 198 TWh (Biomass) + 69 TWh (Waste) + 4 TWh (other)

Variations between countries generating electrical power affect concerns about the environment. In France only 10% of electricity is generated from fossil fuels, the US is higher at 70% and China is at 80%. The cleanliness of electricity depends on its source. Most scientists agree that emissions of pollutants and greenhouse gases from fossil fuel-based electricity generation account for a significant portion of world greenhouse gas emissions; in the United States, electricity generation accounts for nearly 40% of emissions, the largest of any source. Transportation emissions are close behind, contributing about one-third of U.S. production of carbon dioxide.
In the United States, fossil fuel combustion for electric power generation is responsible for 65% of all emissions of sulfur dioxide, the main component of acid rain. Electricity generation is the fourth highest combined source of NOx, carbon monoxide, and particulate matter in the US.
In July 2011, the UK parliament tabled a motion that "levels of (carbon) emissions from nuclear power were approximately three times lower per kilowatt hour than those of solar, four times lower than clean coal and 36 times lower than conventional coal".


</doc>
<doc id="9541" url="https://en.wikipedia.org/wiki?curid=9541" title="Design of experiments">
Design of experiments

The design of experiments (DOE, DOX, or experimental design) is the design of any task that aims to describe and explain the variation of information under conditions that are hypothesized to reflect the variation. The term is generally associated with experiments in which the design introduces conditions that directly affect the variation, but may also refer to the design of quasi-experiments, in which natural conditions that influence the variation are selected for observation.

In its simplest form, an experiment aims at predicting the outcome by introducing a change of the preconditions, which is represented by one or more independent variables, also referred to as "input variables" or "predictor variables." The change in one or more independent variables is generally hypothesized to result in a change in one or more dependent variables, also referred to as "output variables" or "response variables." The experimental design may also identify control variables that must be held constant to prevent external factors from affecting the results. Experimental design involves not only the selection of suitable independent, dependent, and control variables, but planning the delivery of the experiment under statistically optimal conditions given the constraints of available resources. There are multiple approaches for determining the set of design points (unique combinations of the settings of the independent variables) to be used in the experiment.

Main concerns in experimental design include the establishment of validity, reliability, and replicability. For example, these concerns can be partially addressed by carefully choosing the independent variable, reducing the risk of measurement error, and ensuring that the documentation of the method is sufficiently detailed. Related concerns include achieving appropriate levels of statistical power and sensitivity.

Correctly designed experiments advance knowledge in the natural and social sciences and engineering. Other applications include marketing and policy making. The study of the design of experiments is an important topic in metascience.

A theory of statistical inference was developed by Charles S. Peirce in "Illustrations of the Logic of Science" (1877–1878) and "A Theory of Probable Inference" (1883), two publications that emphasized the importance of randomization-based inference in statistics.

Charles S. Peirce randomly assigned volunteers to a blinded, repeated-measures design to evaluate their ability to discriminate weights.
Peirce's experiment inspired other researchers in psychology and education, which developed a research tradition of randomized experiments in laboratories and specialized textbooks in the 1800s.

Charles S. Peirce also contributed the first English-language publication on an optimal design for regression models in 1876. A pioneering optimal design for polynomial regression was suggested by Gergonne in 1815. In 1918, Kirstine Smith published optimal designs for polynomials of degree six (and less).

The use of a sequence of experiments, where the design of each may depend on the results of previous experiments, including the possible decision to stop experimenting, is within the scope of sequential analysis, a field that was pioneered by Abraham Wald in the context of sequential tests of statistical hypotheses. Herman Chernoff wrote an overview of optimal sequential designs, while adaptive designs have been surveyed by S. Zacks. One specific type of sequential design is the "two-armed bandit", generalized to the multi-armed bandit, on which early work was done by Herbert Robbins in 1952.

A methodology for designing experiments was proposed by Ronald Fisher, in his innovative books: "The Arrangement of Field Experiments" (1926) and "The Design of Experiments" (1935). Much of his pioneering work dealt with agricultural applications of statistical methods. As a mundane example, he described how to test the lady tasting tea hypothesis, that a certain lady could distinguish by flavour alone whether the milk or the tea was first placed in the cup. These methods have been broadly adapted in biological, psychological, and agricultural research.







This example of design experiments is attributed to Harold Hotelling, building on examples from Frank Yates. The experiments designed in this example involve combinatorial designs.

Weights of eight objects are measured using a pan balance and set of standard weights. Each weighing measures the weight difference between objects in the left pan and any objects in the right pan by adding calibrated weights to the lighter pan until the balance is in equilibrium. Each measurement has a random error. The average error is zero; the standard deviations of the probability distribution of the errors is the same number σ on different weighings; errors on different weighings are independent. Denote the true weights by

We consider two different experiments:


The question of design of experiments is: which experiment is better?

The variance of the estimate "X" of "θ" is "σ" if we use the first experiment. But if we use the second experiment, the variance of the estimate given above is "σ"/8. Thus the second experiment gives us 8 times as much precision for the estimate of a single item, and estimates all items simultaneously, with the same precision. What the second experiment achieves with eight would require 64 weighings if the items are weighed separately. However, note that the estimates for the items obtained in the second experiment have errors that correlate with each other.

Many problems of the design of experiments involve combinatorial designs, as in this example and others.

False positive conclusions, often resulting from the pressure to publish or the author's own confirmation bias, are an inherent hazard in many fields. A good way to prevent biases potentially leading to false positives in the data collection phase is to use a double-blind design. When a double-blind design is used, participants are randomly assigned to experimental groups but the researcher is unaware of what participants belong to which group. Therefore, the researcher can not affect the participants' response to the intervention. 
Experimental designs with undisclosed degrees of freedom are a problem. This can lead to conscious or unconscious "p-hacking": trying multiple things until you get the desired result. It typically involves the manipulation – perhaps unconsciously – of the process of statistical analysis and the degrees of freedom until they return a figure below the p<.05 level of statistical significance. So the design of the experiment should include a clear statement proposing the analyses to be undertaken. P-hacking can be prevented by preregistering researches, in which researchers have to send their data analysis plan to the journal they wish to publish their paper in before they even start their data collection, so no data manipulation is possible (https://osf.io). Another way to prevent this is taking the double-blind design to the data-analysis phase, where the data are sent to a data-analyst unrelated to the research who scrambles up the data so there is no way to know which participants belong to before they are potentially taken away as outliers.

Clear and complete documentation of the experimental methodology is also important in order to support replication of results.

An experimental design or randomized clinical trial requires careful consideration of several factors before actually doing the experiment. An experimental design is the laying out of a detailed experimental plan in advance of doing the experiment. Some of the following topics have already been discussed in the principles of experimental design section:


The independent variable of a study often has many levels or different groups. In a true experiment, researchers can have an experimental group, which is where their intervention testing the hypothesis is implemented, and a control group, which has all the same element as the experimental group, without the interventional element. Thus, when everything else except for one intervention is held constant, researchers can certify with some certainty that this one element is what caused the observed change. In some instances, having a control group is not ethical. This is sometimes solved using two different experimental groups. In some cases, independent variables cannot be manipulated, for example when testing the difference between two groups who have a different disease, or testing the difference between genders (obviously variables that would be hard or unethical to assign participants to). In these cases, a quasi-experimental design may be used.

In the pure experimental design, the independent (predictor) variable is manipulated by the researcher – that is – every participant of the research is chosen randomly from the population, and each participant chosen is assigned randomly to conditions of the independent variable. Only when this is done is it possible to certify with high probability that the reason for the differences in the outcome variables are caused by the different conditions. Therefore, researchers should choose the experimental design over other design types whenever possible. However, the nature of the independent variable does not always allow for manipulation. In those cases, researchers must be aware of not certifying about causal attribution when their design doesn't allow for it. For example, in observational designs, participants are not assigned randomly to conditions, and so if there are differences found in outcome variables between conditions, it is likely that there is something other than the differences between the conditions that causes the differences in outcomes, that is – a third variable. The same goes for studies with correlational design. (Adér & Mellenbergh, 2008).

It is best that a process be in reasonable statistical control prior to conducting designed experiments. When this is not possible, proper blocking, replication, and randomization allow for the careful conduct of designed experiments.
To control for nuisance variables, researchers institute control checks as additional measures. Investigators should ensure that uncontrolled influences (e.g., source credibility perception) do not skew the findings of the study. A manipulation check is one example of a control check. Manipulation checks allow investigators to isolate the chief variables to strengthen support that these variables are operating as planned.

One of the most important requirements of experimental research designs is the necessity of eliminating the effects of spurious, intervening, and antecedent variables. In the most basic model, cause (X) leads to effect (Y). But there could be a third variable (Z) that influences (Y), and X might not be the true cause at all. Z is said to be a spurious variable and must be controlled for. The same is true for intervening variables (a variable in between the supposed cause (X) and the effect (Y)), and anteceding variables (a variable prior to the supposed cause (X) that is the true cause). When a third variable is involved and has not been controlled for, the relation is said to be a zero order relationship. In most practical applications of experimental research designs there are several causes (X1, X2, X3). In most designs, only one of these causes is manipulated at a time.

Some efficient designs for estimating several main effects were found independently and in near succession by Raj Chandra Bose and K. Kishen in 1940 at the Indian Statistical Institute, but remained little known until the Plackett–Burman designs were published in "Biometrika" in 1946. About the same time, C. R. Rao introduced the concepts of orthogonal arrays as experimental designs. This concept played a central role in the development of Taguchi methods by Genichi Taguchi, which took place during his visit to Indian Statistical Institute in early 1950s. His methods were successfully applied and adopted by Japanese and Indian industries and subsequently were also embraced by US industry albeit with some reservations.

In 1950, Gertrude Mary Cox and William Gemmell Cochran published the book "Experimental Designs," which became the major reference work on the design of experiments for statisticians for years afterwards.

Developments of the theory of linear models have encompassed and surpassed the cases that concerned early writers. Today, the theory rests on advanced topics in linear algebra, algebra and combinatorics.

As with other branches of statistics, experimental design is pursued using both frequentist and Bayesian approaches: In evaluating statistical procedures like experimental designs, frequentist statistics studies the sampling distribution while Bayesian statistics updates a probability distribution on the parameter space.

Some important contributors to the field of experimental designs are C. S. Peirce, R. A. Fisher, F. Yates, R. C. Bose, A. C. Atkinson, R. A. Bailey, D. R. Cox, G. E. P. Box, W. G. Cochran, W. T. Federer, V. V. Fedorov, A. S. Hedayat, J. Kiefer, O. Kempthorne, J. A. Nelder, Andrej Pázman, Friedrich Pukelsheim, D. Raghavarao, C. R. Rao, Shrikhande S. S., J. N. Srivastava, William J. Studden, G. Taguchi and H. P. Wynn.

The textbooks of D. Montgomery, R. Myers, and G. Box/W. Hunter/J.S. Hunter have reached generations of students and practitioners.

Some discussion of experimental design in the context of system identification (model building for static or dynamic models) is given in and 

Laws and ethical considerations preclude some carefully designed 
experiments with human subjects. Legal constraints are dependent on 
jurisdiction. Constraints may involve 
institutional review boards, informed consent 
and confidentiality affecting both clinical (medical) trials and 
behavioral and social science experiments.
In the field of toxicology, for example, experimentation is performed 
on laboratory "animals" with the goal of defining safe exposure limits 
for "humans". Balancing
the constraints are views from the medical field. Regarding the randomization of patients, 
"... if no one knows which therapy is better, there is no ethical 
imperative to use one therapy or another." (p 380) Regarding 
experimental design, "...it is clearly not ethical to place subjects 
at risk to collect data in a poorly designed study when this situation 
can be easily avoided...". (p 393)



</doc>
<doc id="9545" url="https://en.wikipedia.org/wiki?curid=9545" title="Empirical research">
Empirical research

Empirical research is research using empirical evidence. It is also a way of gaining knowledge by means of direct and indirect observation or experience. Empiricism values such research more than other kinds. Empirical evidence (the record of one's direct observations or experiences) can be analyzed quantitatively or qualitatively. Quantifying the evidence or making sense of it in qualitative form, a researcher can answer empirical questions, which should be clearly defined and answerable with the evidence collected (usually called data). Research design varies by field and by the question being investigated. Many researchers combine qualitative and quantitative forms of analysis to better answer questions which cannot be studied in laboratory settings, particularly in the social sciences and in education.

In some fields, quantitative research may begin with a research question (e.g., "Does listening to vocal music during the learning of a word list have an effect on later memory for these words?") which is tested through experimentation. Usually, researcher has a certain theory regarding the topic under investigation. Based on this theory, statements or hypotheses will be proposed (e.g., "Listening to vocal voice has a negative effect on learning a word list."). From these hypotheses, predictions about specific events are derived (e.g., "People who study a word list while listening to vocal music will remember fewer words on a later memory test than people who study a word list in silence."). These predictions can then be tested with a suitable experiment. Depending on the outcomes of the experiment, the theory on which the hypotheses and predictions were based will be supported or not, or may need to be modified and then subjected to further testing.

The term empirical was originally used to refer to certain ancient Greek practitioners of medicine who rejected adherence to the dogmatic doctrines of the day, preferring instead to rely on the observation of phenomena as perceived in experience. Later empiricism referred to a theory of knowledge in philosophy which adheres to the principle that knowledge arises from experience and evidence gathered specifically using the senses. In scientific use, the term empirical refers to the gathering of data using only evidence that is observable by the senses or in some cases using calibrated scientific instruments. What early philosophers described as empiricist and empirical research have in common is the dependence on observable data to formulate and test theories and come to conclusions.

The researcher attempts to describe accurately the interaction between the instrument (or the human senses) and the entity being observed. If instrumentation is involved, the researcher is expected to calibrate his/her instrument by applying it to known standard objects and documenting the results before applying it to unknown objects. In other words, it describes the research that has not taken place before and their results.

In practice, the accumulation of evidence for or against any particular theory involves planned research designs for the collection of empirical data, and academic rigor plays a large part of judging the merits of research design. Several typologies for such designs have been suggested, one of the most popular of which comes from Campbell and Stanley. They are responsible for popularizing the widely cited distinction among pre-experimental, experimental, and quasi-experimental designs and are staunch advocates of the central role of randomized experiments in educational research.

Accurate analysis of data using standardized statistical methods in scientific studies is critical to determining the validity of empirical research. Statistical formulas such as regression, uncertainty coefficient, t-test, chi square, and various types of ANOVA (analyses of variance) are fundamental to forming logical, valid conclusions. If empirical data reach significance under the appropriate statistical formula, the research hypothesis is supported. If not, the null hypothesis is supported (or, more accurately, not rejected), meaning no effect of the independent variable(s) was observed on the dependent variable(s).

The outcome of empirical research using statistical hypothesis testing is never "proof". It can only "support" a hypothesis, "reject" it, or do neither. These methods yield only probabilities. Among scientific researchers, empirical "evidence" (as distinct from empirical "research") refers to objective evidence that appears the same regardless of the observer. For example, a thermometer will not display different temperatures for each individual who observes it. Temperature, as measured by an accurate, well calibrated thermometer, is empirical evidence. By contrast, non-empirical evidence is subjective, depending on the observer. Following the previous example, observer A might truthfully report that a room is warm, while observer B might truthfully report that the same room is cool, though both observe the same reading on the thermometer. The use of empirical evidence negates this effect of personal (i.e., subjective) experience or time.

The varying perception of empiricism and rationalism shows concern with the limit to which there is dependency on experience of sense as an effort of gaining knowledge. According to rationalism, there are a number of different ways in which sense experience is gained independently for the knowledge and concepts. According to empiricism, sense experience is considered as the main source of every piece of knowledge and the concepts. 
In general, rationalists are known for the development of their own views following two different way. First, the key argument can be placed that there are cases in which the content of knowledge or concepts end up outstripping the information. This outstripped information is provided by the sense experience (Hjørland, 2010, 2). Second, there is construction of accounts as to how reasoning helps in the provision of addition knowledge about a specific or broader scope. Empiricists are known to be presenting complementary senses related to thought. 

First there is development of accounts of how there is provision of information by experience that is cited by rationalists. This is insofar for having it in the initial place. At times, empiricists tend to be opting skepticism as an option of rationalism. If experience is not helpful in the provision of knowledge or concept cited by rationalists, then they do not exist (Pearce, 2010, 35). Second, empiricists hold the tendency of attacking the accounts of rationalists while considering reasoning to be an important source of knowledge or concepts. The overall disagreement between empiricists and rationalists show primary concerns in how there is gaining of knowledge with respect to the sources of knowledge and concept. In some of the cases, disagreement at the point of gaining knowledge results in the provision of conflicting responses to other aspects as well. There might be a disagreement in the overall feature of warrant, while limiting the knowledge and thought. Empiricists are known for sharing the view that there is no existence of innate knowledge and rather that is derivation of knowledge out of experience. These experiences are either reasoned using the mind or sensed through the five senses human possess (Bernard, 2011, 5). On the other hand, rationalists are known to be sharing the view that there is existence of innate knowledge and this is different for the objects of innate knowledge being chosen. 

In order to follow rationalism, there must be adoption of one of the three claims related to the theory that are deduction or intuition, innate knowledge, and innate concept. The more there is removal of concept from mental operations and experience, there can be performance over experience with increased plausibility in being innate. Further ahead, empiricism in context with a specific subject provides a rejection of corresponding version related to innate knowledge and deduction or intuition (Weiskopf, 2008, 16). Insofar as there is acknowledgement of concepts and knowledge within the area of subject, the knowledge has major dependence on experience through human senses.

A.D. de Groot's empirical cycle:




</doc>
<doc id="9546" url="https://en.wikipedia.org/wiki?curid=9546" title="Engineering statistics">
Engineering statistics

Engineering statistics combines engineering and statistics using scientific methods for analyzing data. Engineering statistics involves data concerning manufacturing processes such as: component dimensions, tolerances, type of material, and fabrication process control. There are many methods used in engineering analysis and they are often displayed as histograms to give a visual of the data as opposed to being just numerical. Examples of methods are:

Engineering statistics dates back to 1000 B.C. when the Abacus was developed as means to calculate numerical data. In the 1600s, the development of information processing to systematically analyze and process data began. In 1654, the Slide Rule technique was developed by Robert Bissaker for advanced data calculations. In 1833, a British mathematician named Charles Babbage designed the idea of an automatic computer which inspired developers at Harvard University and IBM to design the first mechanical automatic-sequence-controlled calculator called MARK I. The integration of computers and calculators into the industry brought about a more efficient means of analyzing data and the beginning of engineering statistics.



</doc>
<doc id="9549" url="https://en.wikipedia.org/wiki?curid=9549" title="Edgar Allan Poe">
Edgar Allan Poe

Edgar Allan Poe (; born Edgar Poe; January 19, 1809 – October 7, 1849) was an American writer, poet, editor, and literary critic. Poe is best known for his poetry and short stories, particularly his tales of mystery and the macabre. He is widely regarded as a central figure of Romanticism in the United States and of American literature as a whole, and he was one of the country's earliest practitioners of the short story. He is also generally considered the inventor of the detective fiction genre and is further credited with contributing to the emerging genre of science fiction. Poe was the first well-known American writer to earn a living through writing alone, resulting in a financially difficult life and career.

Poe was born in Boston, the second child of actors David and Elizabeth "Eliza" Poe. His father abandoned the family in 1810, and his mother died the following year. Thus orphaned, Poe was taken in by John and Frances Allan of Richmond, Virginia. They never formally adopted him, but he was with them well into young adulthood. Tension developed later as Poe and John Allan repeatedly clashed over Poe's debts, including those incurred by gambling, and the cost of Poe's education. Poe attended the University of Virginia but left after a year due to lack of money. He quarreled with Allan over the funds for his education and enlisted in the United States Army in 1827 under an assumed name. It was at this time that his publishing career began with the anonymous collection "Tamerlane and Other Poems" (1827), credited only to "a Bostonian". Poe and Allan reached a temporary rapprochement after the death of Allan's wife in 1829. Poe later failed as an officer cadet at West Point, declaring a firm wish to be a poet and writer, and he ultimately parted ways with Allan.

Poe switched his focus to prose and spent the next several years working for literary journals and periodicals, becoming known for his own style of literary criticism. His work forced him to move among several cities, including Baltimore, Philadelphia, and New York City. He married his 13-year-old cousin, Virginia Clemm, in 1836, but Virginia died of tuberculosis in 1847. In January 1845, Poe published his poem "The Raven" to instant success. He planned for years to produce his own journal "The Penn" (later renamed "The Stylus"), but before it could be produced, he died in Baltimore on October 7, 1849, at age 40. The cause of his death is unknown and has been variously attributed to disease, alcoholism, substance abuse, suicide, and other causes.

Poe and his works influenced literature around the world, as well as specialized fields such as cosmology and cryptography. He and his work appear throughout popular culture in literature, music, films, and television. A number of his homes are dedicated museums today. The Mystery Writers of America present an annual award known as the Edgar Award for distinguished work in the mystery genre.

Edgar Poe was born in Boston, Massachusetts on January 19, 1809, the second child of English-born actress Elizabeth Arnold Hopkins Poe and actor David Poe Jr. He had an elder brother named William Henry Leonard Poe and a younger sister named Rosalie Poe. Their grandfather, David Poe Sr., emigrated from County Cavan, Ireland around 1750. Edgar may have been named after a character in William Shakespeare's "King Lear", which the couple were performing in 1809. His father abandoned the family in 1810, and his mother died a year later from consumption (pulmonary tuberculosis). Poe was then taken into the home of John Allan, a successful merchant in Richmond, Virginia who dealt in a variety of goods, including cloth, wheat, tombstones, tobacco, and slaves. The Allans served as a foster family and gave him the name "Edgar Allan Poe", though they never formally adopted him.

The Allan family had Poe baptized into the Episcopal Church in 1812. John Allan alternately spoiled and aggressively disciplined his foster son. The family sailed to the United Kingdom in 1815, and Poe attended the grammar school for a short period in Irvine, Ayrshire, Scotland (where Allan was born) before rejoining the family in London in 1816. There he studied at a boarding school in Chelsea until summer 1817. He was subsequently entered at the Reverend John Bransby's Manor House School at Stoke Newington, then a suburb north of London.

Poe moved with the Allans back to Richmond in 1820. In 1824, he served as the lieutenant of the Richmond youth honor guard as the city celebrated the visit of the Marquis de Lafayette. In March 1825, Allan's uncle and business benefactor William Galt died, who was said to be one of the wealthiest men in Richmond, leaving Allan several acres of real estate. The inheritance was estimated at $750,000 (). By summer 1825, Allan celebrated his expansive wealth by purchasing a two-story brick house called Moldavia.

Poe may have become engaged to Sarah Elmira Royster before he registered at the University of Virginia in February 1826 to study ancient and modern languages. The university was in its infancy, established on the ideals of its founder Thomas Jefferson. It had strict rules against gambling, horses, guns, tobacco, and alcohol, but these rules were mostly ignored. Jefferson had enacted a system of student self-government, allowing students to choose their own studies, make their own arrangements for boarding, and report all wrongdoing to the faculty. The unique system was still in chaos, and there was a high dropout rate. During his time there, Poe lost touch with Royster and also became estranged from his foster father over gambling debts. He claimed that Allan had not given him sufficient money to register for classes, purchase texts, and procure and furnish a dormitory. Allan did send additional money and clothes, but Poe's debts increased. Poe gave up on the university after a year but did not feel welcome returning to Richmond, especially when he learned that his sweetheart Royster had married another man, Alexander Shelton. He traveled to Boston in April 1827, sustaining himself with odd jobs as a clerk and newspaper writer, and he started using the pseudonym Henri Le Rennet during this period.

Poe was unable to support himself, so he enlisted in the United States Army as a private on May 27, 1827, using the name "Edgar A. Perry". He claimed that he was even though he was 18. He first served at Fort Independence in Boston Harbor for five dollars a month. That same year, he released his first book, a 40-page collection of poetry titled "Tamerlane and Other Poems", attributed with the byline "by a Bostonian". Only 50 copies were printed, and the book received virtually no attention. Poe's regiment was posted to Fort Moultrie in Charleston, South Carolina and traveled by ship on the brig "Waltham" on November 8, 1827. Poe was promoted to "artificer", an enlisted tradesman who prepared shells for artillery, and had his monthly pay doubled. He served for two years and attained the rank of Sergeant Major for Artillery (the highest rank that a non-commissioned officer could achieve); he then sought to end his five-year enlistment early. Poe revealed his real name and his circumstances to his commanding officer, Lieutenant Howard, who would only allow Poe to be discharged if he reconciled with Allan. Poe wrote a letter to Allan, who was unsympathetic and spent several months ignoring Poe's pleas; Allan may not have written to Poe even to make him aware of his foster mother's illness. Frances Allan died on February 28, 1829, and Poe visited the day after her burial. Perhaps softened by his wife's death, Allan agreed to support Poe's attempt to be discharged in order to receive an appointment to the United States Military Academy at West Point, New York.

Poe was finally discharged on April 15, 1829, after securing a replacement to finish his enlisted term for him. Before entering West Point, he moved back to Baltimore for a time to stay with his widowed aunt Maria Clemm, her daughter Virginia Eliza Clemm (Poe's first cousin), his brother Henry, and his invalid grandmother Elizabeth Cairnes Poe. In September of that year Poe received “the very first words of encouragement I ever remember to have heard” in a review of his poetry by influential critic John Neal, prompting Poe to dedicate one of the poems to Neal in his second book "Al Aaraaf, Tamerlane and Minor Poems", published in Baltimore in 1829.

Poe traveled to West Point and matriculated as a cadet on July 1, 1830. In October 1830, Allan married his second wife Louisa Patterson. The marriage and bitter quarrels with Poe over the children born to Allan out of extramarital affairs led to the foster father finally disowning Poe. Poe decided to leave West Point by purposely getting court-martialed. On February 8, 1831, he was tried for gross neglect of duty and disobedience of orders for refusing to attend formations, classes, or church. He tactically pleaded not guilty to induce dismissal, knowing that he would be found guilty.

Poe left for New York in February 1831 and released a third volume of poems, simply titled "Poems." The book was financed with help from his fellow cadets at West Point, many of whom donated 75 cents to the cause, raising a total of $170. They may have been expecting verses similar to the satirical ones that Poe had been writing about commanding officers. It was printed by Elam Bliss of New York, labeled as "Second Edition," and including a page saying, "To the U.S. Corps of Cadets this volume is respectfully dedicated". The book once again reprinted the long poems "Tamerlane" and "Al Aaraaf" but also six previously unpublished poems, including early versions of "To Helen", "Israfel", and "The City in the Sea". Poe returned to Baltimore to his aunt, brother, and cousin in March 1831. His elder brother Henry had been in ill health, in part due to problems with alcoholism, and he died on August 1, 1831.

After his brother's death, Poe began more earnest attempts to start his career as a writer, but he chose a difficult time in American publishing to do so. He was one of the first Americans to live by writing alone and was hampered by the lack of an international copyright law. American publishers often produced unauthorized copies of British works rather than paying for new work by Americans. The industry was also particularly hurt by the Panic of 1837. There was a booming growth in American periodicals around this time, fueled in part by new technology, but many did not last beyond a few issues. Publishers often refused to pay their writers or paid them much later than they promised, and Poe repeatedly resorted to humiliating pleas for money and other assistance.
After his early attempts at poetry, Poe had turned his attention to prose, likely based on John Neal's critiques in "The Yankee" magazine. He placed a few stories with a Philadelphia publication and began work on his only drama "Politian". The "Baltimore Saturday Visiter" awarded him a prize in October 1833 for his short story "MS. Found in a Bottle". The story brought him to the attention of John P. Kennedy, a Baltimorean of considerable means who helped Poe place some of his stories and introduced him to Thomas W. White, editor of the "Southern Literary Messenger" in Richmond. Poe became assistant editor of the periodical in August 1835, but White discharged him within a few weeks for being drunk on the job. Poe returned to Baltimore where he obtained a license to marry his cousin Virginia on September 22, 1835, though it is unknown if they were married at that time. He was 26 and she was 13.

Poe was reinstated by White after promising good behavior, and he went back to Richmond with Virginia and her mother. He remained at the "Messenger" until January 1837. During this period, Poe claimed that its circulation increased from 700 to 3,500. He published several poems, book reviews, critiques, and stories in the paper. On May 16, 1836, he and Virginia held a Presbyterian wedding ceremony at their Richmond boarding house, with a witness falsely attesting Clemm's age as 21.

Poe's novel "The Narrative of Arthur Gordon Pym of Nantucket" was published and widely reviewed in 1838. In the summer of 1839, Poe became assistant editor of "Burton's Gentleman's Magazine". He published numerous articles, stories, and reviews, enhancing his reputation as a trenchant critic which he had established at the "Messenger". Also in 1839, the collection "Tales of the Grotesque and Arabesque" was published in two volumes, though he made little money from it and it received mixed reviews. Poe left "Burton's" after about a year and found a position as assistant at "Graham's Magazine".

In June 1840, Poe published a prospectus announcing his intentions to start his own journal called "The Stylus", although he originally intended to call it "The Penn", as it would have been based in Philadelphia. He bought advertising space for his prospectus in the June 6, 1840 issue of Philadelphia's "Saturday Evening Post": ""Prospectus of the Penn Magazine, a Monthly Literary journal to be edited and published in the city of Philadelphia by Edgar A. Poe."" The journal was never produced before Poe's death.

Around this time, Poe attempted to secure a position within the administration of President John Tyler, claiming that he was a member of the Whig Party. He hoped to be appointed to the United States Custom House in Philadelphia with help from President Tyler's son Robert, an acquaintance of Poe's friend Frederick Thomas. Poe failed to show up for a meeting with Thomas to discuss the appointment in mid-September 1842, claiming to have been sick, though Thomas believed that he had been drunk. Poe was promised an appointment, but all positions were filled by others.
One evening in January 1842, Virginia showed the first signs of consumption, now known as tuberculosis, while singing and playing the piano, which Poe described as breaking a blood vessel in her throat. She only partially recovered, and Poe began to drink more heavily under the stress of her illness. He left "Graham's" and attempted to find a new position, for a time angling for a government post. He returned to New York where he worked briefly at the "Evening Mirror" before becoming editor of the "Broadway Journal", and later its owner. There Poe alienated himself from other writers by publicly accusing Henry Wadsworth Longfellow of plagiarism, though Longfellow never responded. On January 29, 1845, his poem "The Raven" appeared in the "Evening Mirror" and became a popular sensation. It made Poe a household name almost instantly, though he was paid only $9 for its publication. It was concurrently published in "" under the pseudonym "Quarles".

The "Broadway Journal" failed in 1846, and Poe moved to a cottage in Fordham, New York, in what is now the Bronx. That home is now known as the Edgar Allan Poe Cottage, relocated to a park near the southeast corner of the Grand Concourse and Kingsbridge Road. Nearby, Poe befriended the Jesuits at St. John's College, now Fordham University. Virginia died at the cottage on January 30, 1847. Biographers and critics often suggest that Poe's frequent theme of the "death of a beautiful woman" stems from the repeated loss of women throughout his life, including his wife.

Poe was increasingly unstable after his wife's death. He attempted to court poet Sarah Helen Whitman who lived in Providence, Rhode Island. Their engagement failed, purportedly because of Poe's drinking and erratic behavior. There is also strong evidence that Whitman's mother intervened and did much to derail their relationship. Poe then returned to Richmond and resumed a relationship with his childhood sweetheart Sarah Elmira Royster.

On October 3, 1849, Poe was found delirious on the streets of Baltimore, "in great distress, and… in need of immediate assistance", according to Joseph W. Walker, who found him. He was taken to the Washington Medical College, where he died on Sunday, October 7, 1849, at 5:00 in the morning. Poe was not coherent long enough to explain how he came to be in his dire condition and, oddly, was wearing clothes that were not his own. He is said to have repeatedly called out the name "Reynolds" on the night before his death, though it is unclear to whom he was referring. Some sources say that Poe's final words were, "Lord help my poor soul". All medical records have been lost, including Poe's death certificate.

Newspapers at the time reported Poe's death as "congestion of the brain" or "cerebral inflammation", common euphemisms for death from disreputable causes such as alcoholism. The actual cause of death remains a mystery. Speculation has included "delirium tremens", heart disease, epilepsy, syphilis, meningeal inflammation, cholera, and rabies. One theory dating from 1872 suggests that cooping was the cause of Poe's death, a form of electoral fraud in which citizens were forced to vote for a particular candidate, sometimes leading to violence and even murder.

Immediately after Poe's death, his literary rival Rufus Wilmot Griswold wrote a slanted high-profile obituary under a pseudonym, filled with falsehoods that cast him as a lunatic and a madman, and which described him as a person who "walked the streets, in madness or melancholy, with lips moving in indistinct curses, or with eyes upturned in passionate prayers, (never for himself, for he felt, or professed to feel, that he was already damned)".

The long obituary appeared in the "New York Tribune" signed "Ludwig" on the day that Poe was buried. It was soon further published throughout the country. The piece began, "Edgar Allan Poe is dead. He died in Baltimore the day before yesterday. This announcement will startle many, but few will be grieved by it." "Ludwig" was soon identified as Griswold, an editor, critic, and anthologist who had borne a grudge against Poe since 1842. Griswold somehow became Poe's literary executor and attempted to destroy his enemy's reputation after his death.

Griswold wrote a biographical article of Poe called "Memoir of the Author", which he included in an 1850 volume of the collected works. There he depicted Poe as a depraved, drunken, drug-addled madman and included Poe's letters as evidence. Many of his claims were either lies or distorted half-truths. For example, it is seriously disputed that Poe really was a drug addict. Griswold's book was denounced by those who knew Poe well, including John Neal, who published an article defending Poe and attacking Griswold as “a Rhadamanthus, who is not to be bilked of his fee, a thimble-full of newspaper notoriety.” Griswold's book nevertheless became a popularly accepted biographical source. This occurred in part because it was the only full biography available and was widely reprinted, and in part because readers thrilled at the thought of reading works by an "evil" man. Letters that Griswold presented as proof were later revealed to be forgeries.

After Poe's death, Griswold convinced Poe's mother-in-law to sign away the rights to his works. Griswold went on to publish the collected works attached with his own fabricated biography of Poe that invented stories of his drunkenness, immorality and instability.

Poe's best known fiction works are Gothic, adhering to the genre's conventions to appeal to the public taste. His most recurring themes deal with questions of death, including its physical signs, the effects of decomposition, concerns of premature burial, the reanimation of the dead, and mourning. Many of his works are generally considered part of the dark romanticism genre, a literary reaction to transcendentalism which Poe strongly disliked. He referred to followers of the transcendental movement as "Frog-Pondians", after the pond on Boston Common, and ridiculed their writings as "metaphor—run mad," lapsing into "obscurity for obscurity's sake" or "mysticism for mysticism's sake". Poe once wrote in a letter to Thomas Holley Chivers that he did not dislike transcendentalists, "only the pretenders and sophists among them".

Beyond horror, Poe also wrote satires, humor tales, and hoaxes. For comic effect, he used irony and ludicrous extravagance, often in an attempt to liberate the reader from cultural conformity. "Metzengerstein" is the first story that Poe is known to have published and his first foray into horror, but it was originally intended as a burlesque satirizing the popular genre. Poe also reinvented science fiction, responding in his writing to emerging technologies such as hot air balloons in "The Balloon-Hoax".

Poe wrote much of his work using themes aimed specifically at mass-market tastes. To that end, his fiction often included elements of popular pseudosciences, such as phrenology and physiognomy.

Poe's writing reflects his literary theories, which he presented in his criticism and also in essays such as "The Poetic Principle". He disliked didacticism and allegory, though he believed that meaning in literature should be an undercurrent just beneath the surface. Works with obvious meanings, he wrote, cease to be art. He believed that work of quality should be brief and focus on a specific single effect. To that end, he believed that the writer should carefully calculate every sentiment and idea.

Poe describes his method in writing "The Raven" in the essay "The Philosophy of Composition", and he claims to have strictly followed this method. It has been questioned whether he really followed this system, however. T. S. Eliot said: "It is difficult for us to read that essay without reflecting that if Poe plotted out his poem with such calculation, he might have taken a little more pains over it: the result hardly does credit to the method." Biographer Joseph Wood Krutch described the essay as "a rather highly ingenious exercise in the art of rationalization".

During his lifetime, Poe was mostly recognized as a literary critic. Fellow critic James Russell Lowell called him "the most discriminating, philosophical, and fearless critic upon imaginative works who has written in America", suggesting—rhetorically—that he occasionally used prussic acid instead of ink. Poe's caustic reviews earned him the reputation of being a "tomahawk man". A favorite target of Poe's criticism was Boston's acclaimed poet Henry Wadsworth Longfellow, who was often defended by his literary friends in what was later called "The Longfellow War". Poe accused Longfellow of "the heresy of the didactic", writing poetry that was preachy, derivative, and thematically plagiarized. Poe correctly predicted that Longfellow's reputation and style of poetry would decline, concluding, "We grant him high qualities, but deny him the Future".

Poe was also known as a writer of fiction and became one of the first American authors of the 19th century to become more popular in Europe than in the United States. Poe is particularly respected in France, in part due to early translations by Charles Baudelaire. Baudelaire's translations became definitive renditions of Poe's work throughout Europe.

Poe's early detective fiction tales featuring C. Auguste Dupin laid the groundwork for future detectives in literature. Sir Arthur Conan Doyle said, "Each [of Poe's detective stories] is a root from which a whole literature has developed... Where was the detective story until Poe breathed the breath of life into it?" The Mystery Writers of America have named their awards for excellence in the genre the "Edgars". Poe's work also influenced science fiction, notably Jules Verne, who wrote a sequel to Poe's novel "The Narrative of Arthur Gordon Pym of Nantucket" called "An Antarctic Mystery", also known as "The Sphinx of the Ice Fields". Science fiction author H. G. Wells noted, ""Pym" tells what a very intelligent mind could imagine about the south polar region a century ago". In 2013, "The Guardian" cited "Pym" as one of the greatest novels ever written in the English language, and noted its influence on later authors such as Doyle, Henry James, B. Traven, and David Morrell.

Horror author and historian H. P. Lovecraft was heavily influenced by Poe’s horror tales, dedicating an entire section of his long essay, “Supernatural Horror in Literature”, to his influence on the genre. In his letters, Lovecraft stated, “When I write stories, Edgar Allan Poe is my model.” Alfred Hitchcock once said, "It's because I liked Edgar Allan Poe's stories so much that I began to make suspense films".

Like many famous artists, Poe's works have spawned imitators. One trend among imitators of Poe has been claims by clairvoyants or psychics to be "channeling" poems from Poe's spirit. One of the most notable of these was Lizzie Doten, who published "Poems from the Inner Life" in 1863, in which she claimed to have "received" new compositions by Poe's spirit. The compositions were re-workings of famous Poe poems such as "The Bells", but which reflected a new, positive outlook.
Even so, Poe has received not only praise, but criticism as well. This is partly because of the negative perception of his personal character and its influence upon his reputation. William Butler Yeats was occasionally critical of Poe and once called him "vulgar". Transcendentalist Ralph Waldo Emerson reacted to "The Raven" by saying, "I see nothing in it", and derisively referred to Poe as "the jingle man". Aldous Huxley wrote that Poe's writing "falls into vulgarity" by being "too poetical"—the equivalent of wearing a diamond ring on every finger.

It is believed that only twelve copies have survived of Poe's first book "Tamerlane and Other Poems". In December 2009, one copy sold at Christie's auctioneers in New York City for $662,500, a record price paid for a work of American literature.

"", an essay written in 1848, included a cosmological theory that presaged the Big Bang theory by 80 years, as well as the first plausible solution to Olbers' paradox.
Poe eschewed the scientific method in "Eureka" and instead wrote from pure intuition. For this reason, he considered it a work of art, not science, but insisted that it was still true and considered it to be his career masterpiece. Even so, "Eureka" is full of scientific errors. In particular, Poe's suggestions ignored Newtonian principles regarding the density and rotation of planets.

Poe had a keen interest in cryptography. He had placed a notice of his abilities in the Philadelphia paper "Alexander's Weekly (Express) Messenger", inviting submissions of ciphers which he proceeded to solve. In July 1841, Poe had published an essay called "A Few Words on Secret Writing" in "Graham's Magazine". Capitalizing on public interest in the topic, he wrote "The Gold-Bug" incorporating ciphers as an essential part of the story. Poe's success with cryptography relied not so much on his deep knowledge of that field (his method was limited to the simple substitution cryptogram) as on his knowledge of the magazine and newspaper culture. His keen analytical abilities, which were so evident in his detective stories, allowed him to see that the general public was largely ignorant of the methods by which a simple substitution cryptogram can be solved, and he used this to his advantage. The sensation that Poe created with his cryptography stunts played a major role in popularizing cryptograms in newspapers and magazines.

Two ciphers he published in 1841 under the name "W. B. Tyler" were not solved until 1992 and 2000 respectively. One was a quote from Joseph Addison's play "Cato"; the other is still unidentified.

Poe had an influence on cryptography beyond increasing public interest during his lifetime. William Friedman, America's foremost cryptologist, was heavily influenced by Poe. Friedman's initial interest in cryptography came from reading "The Gold-Bug" as a child, an interest that he later put to use in deciphering Japan's PURPLE code during World War II.

The historical Edgar Allan Poe has appeared as a fictionalized character, often representing the "mad genius" or "tormented artist" and exploiting his personal struggles. Many such depictions also blend in with characters from his stories, suggesting that Poe and his characters share identities. Often, fictional depictions of Poe use his mystery-solving skills in such novels as "The Poe Shadow" by Matthew Pearl.

No childhood home of Poe is still standing, including the Allan family's Moldavia estate. The oldest standing home in Richmond, the Old Stone House, is in use as the Edgar Allan Poe Museum, though Poe never lived there. The collection includes many items that Poe used during his time with the Allan family, and also features several rare first printings of Poe works. 13 West Range is the dorm room that Poe is believed to have used while studying at the University of Virginia in 1826; it is preserved and available for visits. Its upkeep is now overseen by a group of students and staff known as the Raven Society.

The earliest surviving home in which Poe lived is in Baltimore, preserved as the Edgar Allan Poe House and Museum. Poe is believed to have lived in the home at the age of 23 when he first lived with Maria Clemm and Virginia (as well as his grandmother and possibly his brother William Henry Leonard Poe). It is open to the public and is also the home of the Edgar Allan Poe Society. Of the several homes that Poe, his wife Virginia, and his mother-in-law Maria rented in Philadelphia, only the last house has survived. The Spring Garden home, where the author lived in 1843–1844, is today preserved by the National Park Service as the Edgar Allan Poe National Historic Site. Poe's final home is preserved as the Edgar Allan Poe Cottage in the Bronx.

In Boston, a commemorative plaque on Boylston Street is several blocks away from the actual location of Poe's birth. The house which was his birthplace at 62 Carver Street no longer exists; also, the street has since been renamed "Charles Street South". A "square" at the intersection of Broadway, Fayette, and Carver Streets had once been named in his honor, but it disappeared when the streets were rearranged. In 2009, the intersection of Charles and Boylston Streets (two blocks north of his birthplace) was designated "Edgar Allan Poe Square". 

In March 2014, fundraising was completed for construction of a permanent memorial sculpture, known as "Poe Returning to Boston," at this location. The winning design by Stefanie Rocknak depicts a life-sized Poe striding against the wind, accompanied by a flying raven; his suitcase lid has fallen open, leaving a "paper trail" of literary works embedded in the sidewalk behind him. The public unveiling on October 5, 2014 was attended by former U.S. poet laureate Robert Pinsky.

Other Poe landmarks include a building on the Upper West Side where Poe temporarily lived when he first moved to New York. A plaque suggests that Poe wrote "The Raven" here. On Sullivan's Island in Charleston, South Carolina, the setting of Poe's tale "The Gold-Bug" and where Poe served in the Army in 1827 at Fort Moultrie, there is a restaurant called Poe's Tavern. In Fell's Point, Baltimore, a bar still stands where legend says that Poe was last seen drinking before his death. Now known as "The Horse You Came in On", local lore insists that a ghost whom they call "Edgar" haunts the rooms above.

Early daguerreotypes of Poe continue to arouse great interest among literary historians. Notable among them are:

Between 1949 and 2009, a bottle of cognac and three roses were left at Poe's original grave marker every January 19 by an unknown visitor affectionately referred to as the "Poe Toaster". Sam Porpora was a historian at the Westminster Church in Baltimore where Poe is buried, and he claimed on August 15, 2007 that he had started the tradition in 1949. Porpora said that the tradition began in order to raise money and enhance the profile of the church. His story has not been confirmed, and some details which he gave to the press are factually inaccurate. The Poe Toaster's last appearance was on January 19, 2009, the day of Poe's bicentennial.

Tales

Poetry
Other works




</doc>
<doc id="9550" url="https://en.wikipedia.org/wiki?curid=9550" title="Electricity">
Electricity

Electricity is the set of physical phenomena associated with the presence and motion of matter that has a property of electric charge. Electricity is related to magnetism, both being part of the phenomenon of electromagnetism, as described by Maxwell's equations. Various common phenomena are related to electricity, including lightning, static electricity, electric heating, electric discharges and many others.

The presence of an electric charge, which can be either positive or negative, produces an electric field. The movement of electric charges is an electric current and produces a magnetic field.

When a charge is placed in a location with a non-zero electric field, a force will act on it. The magnitude of this force is given by Coulomb's law. If the charge moves, the electric field would be doing work on the electric charge. Thus we can speak of electric potential at a certain point in space, which is equal to the work done by an external agent in carrying a unit of positive charge from an arbitrarily chosen reference point to that point without any acceleration and is typically measured in volts.

Electricity is at the heart of many modern technologies, being used for:

Electrical phenomena have been studied since antiquity, though progress in theoretical understanding remained slow until the seventeenth and eighteenth centuries. The theory of electromagnetism was developed in the 19th century, and by the end of that century electricity was being put to industrial and residential use by electrical engineers. The rapid expansion in electrical technology at this time transformed industry and society, becoming a driving force for the Second Industrial Revolution. Electricity's extraordinary versatility means it can be put to an almost limitless set of applications which include transport, heating, lighting, communications, and computation. Electrical power is now the backbone of modern industrial society.

Long before any knowledge of electricity existed, people were aware of shocks from electric fish. Ancient Egyptian texts dating from 2750 BCE referred to these fish as the "Thunderer of the Nile", and described them as the "protectors" of all other fish. Electric fish were again reported millennia later by ancient Greek, Roman and Arabic naturalists and physicians. Several ancient writers, such as Pliny the Elder and Scribonius Largus, attested to the numbing effect of electric shocks delivered by electric catfish and electric rays, and knew that such shocks could travel along conducting objects. Patients suffering from ailments such as gout or headache were directed to touch electric fish in the hope that the powerful jolt might cure them. 

Ancient cultures around the Mediterranean knew that certain objects, such as rods of amber, could be rubbed with cat's fur to attract light objects like feathers. Thales of Miletus made a series of observations on static electricity around 600 BCE, from which he believed that friction rendered amber magnetic, in contrast to minerals such as magnetite, which needed no rubbing. Thales was incorrect in believing the attraction was due to a magnetic effect, but later science would prove a link between magnetism and electricity. According to a controversial theory, the Parthians may have had knowledge of electroplating, based on the 1936 discovery of the Baghdad Battery, which resembles a galvanic cell, though it is uncertain whether the artifact was electrical in nature.
Electricity would remain little more than an intellectual curiosity for millennia until 1600, when the English scientist William Gilbert wrote "De Magnete", in which he made a careful study of electricity and magnetism, distinguishing the lodestone effect from static electricity produced by rubbing amber. He coined the New Latin word "electricus" ("of amber" or "like amber", from ἤλεκτρον, "elektron", the Greek word for "amber") to refer to the property of attracting small objects after being rubbed. This association gave rise to the English words "electric" and "electricity", which made their first appearance in print in Thomas Browne's "Pseudodoxia Epidemica" of 1646.

Further work was conducted in the 17th and early 18th centuries by Otto von Guericke, Robert Boyle, Stephen Gray and C. F. du Fay. Later in the 18th century, Benjamin Franklin conducted extensive research in electricity, selling his possessions to fund his work. In June 1752 he is reputed to have attached a metal key to the bottom of a dampened kite string and flown the kite in a storm-threatened sky. A succession of sparks jumping from the key to the back of his hand showed that lightning was indeed electrical in nature. He also explained the apparently paradoxical behavior of the Leyden jar as a device for storing large amounts of electrical charge in terms of electricity consisting of both positive and negative charges.

In 1791, Luigi Galvani published his discovery of bioelectromagnetics, demonstrating that electricity was the medium by which neurons passed signals to the muscles. Alessandro Volta's battery, or voltaic pile, of 1800, made from alternating layers of zinc and copper, provided scientists with a more reliable source of electrical energy than the electrostatic machines previously used. The recognition of electromagnetism, the unity of electric and magnetic phenomena, is due to Hans Christian Ørsted and André-Marie Ampère in 1819–1820. Michael Faraday invented the electric motor in 1821, and Georg Ohm mathematically analysed the electrical circuit in 1827. Electricity and magnetism (and light) were definitively linked by James Clerk Maxwell, in particular in his "On Physical Lines of Force" in 1861 and 1862.

While the early 19th century had seen rapid progress in electrical science, the late 19th century would see the greatest progress in electrical engineering. Through such people as Alexander Graham Bell, Ottó Bláthy, Thomas Edison, Galileo Ferraris, Oliver Heaviside, Ányos Jedlik, William Thomson, 1st Baron Kelvin, Charles Algernon Parsons, Werner von Siemens, Joseph Swan, Reginald Fessenden, Nikola Tesla and George Westinghouse, electricity turned from a scientific curiosity into an essential tool for modern life.

In 1887, Heinrich Hertz discovered that electrodes illuminated with ultraviolet light create electric sparks more easily. In 1905, Albert Einstein published a paper that explained experimental data from the photoelectric effect as being the result of light energy being carried in discrete quantized packets, energising electrons. This discovery led to the quantum revolution. Einstein was awarded the Nobel Prize in Physics in 1921 for "his discovery of the law of the photoelectric effect". The photoelectric effect is also employed in photocells such as can be found in solar panels and this is frequently used to make electricity commercially.

The first solid-state device was the "cat's-whisker detector" first used in the 1900s in radio receivers. A whisker-like wire is placed lightly in contact with a solid crystal (such as a germanium crystal) to detect a radio signal by the contact junction effect. In a solid-state component, the current is confined to solid elements and compounds engineered specifically to switch and amplify it. Current flow can be understood in two forms: as negatively charged electrons, and as positively charged electron deficiencies called holes. These charges and holes are understood in terms of quantum physics. The building material is most often a crystalline semiconductor.

Solid-state electronics came into its own with the emergence of transistor technology. The first working transistor, a germanium-based point-contact transistor, was invented by John Bardeen and Walter Houser Brattain at Bell Labs in 1947, followed by the bipolar junction transistor in 1948. These early transistors were relatively bulky devices that were difficult to manufacture on a mass-production basis. They were followed by the silicon-based MOSFET (metal-oxide-semiconductor field-effect transistor, or MOS transistor), invented by Mohamed M. Atalla and Dawon Kahng at Bell Labs in 1959. It was the first truly compact transistor that could be miniaturised and mass-produced for a wide range of uses, leading to the silicon revolution. Solid-state devices started becoming prevalent from the 1960s, with the transition from vacuum tubes to semiconductor diodes, transistors, integrated circuit (IC) chips, MOSFETs, and light-emitting diode (LED) technology.

The most common electronic device is the MOSFET, which has become the most widely manufactured device in history. Common solid-state MOS devices include microprocessor chips and semiconductor memory. A special type of semiconductor memory is flash memory, which is used in USB flash drives and mobile devices, as well as solid-state drive (SSD) technology to replace mechanically rotating magnetic disc hard disk drive (HDD) technology.

The presence of charge gives rise to an electrostatic force: charges exert a force on each other, an effect that was known, though not understood, in antiquity. A lightweight ball suspended from a string can be charged by touching it with a glass rod that has itself been charged by rubbing with a cloth. If a similar ball is charged by the same glass rod, it is found to repel the first: the charge acts to force the two balls apart. Two balls that are charged with a rubbed amber rod also repel each other. However, if one ball is charged by the glass rod, and the other by an amber rod, the two balls are found to attract each other. These phenomena were investigated in the late eighteenth century by Charles-Augustin de Coulomb, who deduced that charge manifests itself in two opposing forms. This discovery led to the well-known axiom: "like-charged objects repel and opposite-charged objects attract".

The force acts on the charged particles themselves, hence charge has a tendency to spread itself as evenly as possible over a conducting surface. The magnitude of the electromagnetic force, whether attractive or repulsive, is given by Coulomb's law, which relates the force to the product of the charges and has an inverse-square relation to the distance between them. The electromagnetic force is very strong, second only in strength to the strong interaction, but unlike that force it operates over all distances. In comparison with the much weaker gravitational force, the electromagnetic force pushing two electrons apart is 10 times that of the gravitational attraction pulling them together.

Study has shown that the origin of charge is from certain types of subatomic particles which have the property of electric charge. Electric charge gives rise to and interacts with the electromagnetic force, one of the four fundamental forces of nature. The most familiar carriers of electrical charge are the electron and proton. Experiment has shown charge to be a conserved quantity, that is, the net charge within an electrically isolated system will always remain constant regardless of any changes taking place within that system. Within the system, charge may be transferred between bodies, either by direct contact, or by passing along a conducting material, such as a wire. The informal term static electricity refers to the net presence (or 'imbalance') of charge on a body, usually caused when dissimilar materials are rubbed together, transferring charge from one to the other.

The charge on electrons and protons is opposite in sign, hence an amount of charge may be expressed as being either negative or positive. By convention, the charge carried by electrons is deemed negative, and that by protons positive, a custom that originated with the work of Benjamin Franklin. The amount of charge is usually given the symbol "Q" and expressed in coulombs; each electron carries the same charge of approximately −1.6022×10 coulomb. The proton has a charge that is equal and opposite, and thus +1.6022×10  coulomb. Charge is possessed not just by matter, but also by antimatter, each antiparticle bearing an equal and opposite charge to its corresponding particle.

Charge can be measured by a number of means, an early instrument being the gold-leaf electroscope, which although still in use for classroom demonstrations, has been superseded by the electronic electrometer.

The movement of electric charge is known as an electric current, the intensity of which is usually measured in amperes. Current can consist of any moving charged particles; most commonly these are electrons, but any charge in motion constitutes a current. Electric current can flow through some things, electrical conductors, but will not flow through an electrical insulator.

By historical convention, a positive current is defined as having the same direction of flow as any positive charge it contains, or to flow from the most positive part of a circuit to the most negative part. Current defined in this manner is called conventional current. The motion of negatively charged electrons around an electric circuit, one of the most familiar forms of current, is thus deemed positive in the "opposite" direction to that of the electrons. However, depending on the conditions, an electric current can consist of a flow of charged particles in either direction, or even in both directions at once. The positive-to-negative convention is widely used to simplify this situation.
The process by which electric current passes through a material is termed electrical conduction, and its nature varies with that of the charged particles and the material through which they are travelling. Examples of electric currents include metallic conduction, where electrons flow through a conductor such as metal, and electrolysis, where ions (charged atoms) flow through liquids, or through plasmas such as electrical sparks. While the particles themselves can move quite slowly, sometimes with an average drift velocity only fractions of a millimetre per second, the electric field that drives them itself propagates at close to the speed of light, enabling electrical signals to pass rapidly along wires.

Current causes several observable effects, which historically were the means of recognising its presence. That water could be decomposed by the current from a voltaic pile was discovered by Nicholson and Carlisle in 1800, a process now known as electrolysis. Their work was greatly expanded upon by Michael Faraday in 1833. Current through a resistance causes localised heating, an effect James Prescott Joule studied mathematically in 1840. One of the most important discoveries relating to current was made accidentally by Hans Christian Ørsted in 1820, when, while preparing a lecture, he witnessed the current in a wire disturbing the needle of a magnetic compass. He had discovered electromagnetism, a fundamental interaction between electricity and magnetics. The level of electromagnetic emissions generated by electric arcing is high enough to produce electromagnetic interference, which can be detrimental to the workings of adjacent equipment.

In engineering or household applications, current is often described as being either direct current (DC) or alternating current (AC). These terms refer to how the current varies in time. Direct current, as produced by example from a battery and required by most electronic devices, is a unidirectional flow from the positive part of a circuit to the negative. If, as is most common, this flow is carried by electrons, they will be travelling in the opposite direction. Alternating current is any current that reverses direction repeatedly; almost always this takes the form of a sine wave. Alternating current thus pulses back and forth within a conductor without the charge moving any net distance over time. The time-averaged value of an alternating current is zero, but it delivers energy in first one direction, and then the reverse. Alternating current is affected by electrical properties that are not observed under steady state direct current, such as inductance and capacitance. These properties however can become important when circuitry is subjected to transients, such as when first energised.

The concept of the electric field was introduced by Michael Faraday. An electric field is created by a charged body in the space that surrounds it, and results in a force exerted on any other charges placed within the field. The electric field acts between two charges in a similar manner to the way that the gravitational field acts between two masses, and like it, extends towards infinity and shows an inverse square relationship with distance. However, there is an important difference. Gravity always acts in attraction, drawing two masses together, while the electric field can result in either attraction or repulsion. Since large bodies such as planets generally carry no net charge, the electric field at a distance is usually zero. Thus gravity is the dominant force at distance in the universe, despite being much weaker.

An electric field generally varies in space, and its strength at any one point is defined as the force (per unit charge) that would be felt by a stationary, negligible charge if placed at that point. The conceptual charge, termed a 'test charge', must be vanishingly small to prevent its own electric field disturbing the main field and must also be stationary to prevent the effect of magnetic fields. As the electric field is defined in terms of force, and force is a vector, having both magnitude and direction, so it follows that an electric field is a vector field.

The study of electric fields created by stationary charges is called electrostatics. The field may be visualised by a set of imaginary lines whose direction at any point is the same as that of the field. This concept was introduced by Faraday, whose term 'lines of force' still sometimes sees use. The field lines are the paths that a point positive charge would seek to make as it was forced to move within the field; they are however an imaginary concept with no physical existence, and the field permeates all the intervening space between the lines. Field lines emanating from stationary charges have several key properties: first, that they originate at positive charges and terminate at negative charges; second, that they must enter any good conductor at right angles, and third, that they may never cross nor close in on themselves.

A hollow conducting body carries all its charge on its outer surface. The field is therefore zero at all places inside the body. This is the operating principal of the Faraday cage, a conducting metal shell which isolates its interior from outside electrical effects.

The principles of electrostatics are important when designing items of high-voltage equipment. There is a finite limit to the electric field strength that may be withstood by any medium. Beyond this point, electrical breakdown occurs and an electric arc causes flashover between the charged parts. Air, for example, tends to arc across small gaps at electric field strengths which exceed 30 kV per centimetre. Over larger gaps, its breakdown strength is weaker, perhaps 1 kV per centimetre. The most visible natural occurrence of this is lightning, caused when charge becomes separated in the clouds by rising columns of air, and raises the electric field in the air to greater than it can withstand. The voltage of a large lightning cloud may be as high as 100 MV and have discharge energies as great as 250 kWh.

The field strength is greatly affected by nearby conducting objects, and it is particularly intense when it is forced to curve around sharply pointed objects. This principle is exploited in the lightning conductor, the sharp spike of which acts to encourage the lightning stroke to develop there, rather than to the building it serves to protect

The concept of electric potential is closely linked to that of the electric field. A small charge placed within an electric field experiences a force, and to have brought that charge to that point against the force requires work. The electric potential at any point is defined as the energy required to bring a unit test charge from an infinite distance slowly to that point. It is usually measured in volts, and one volt is the potential for which one joule of work must be expended to bring a charge of one coulomb from infinity. This definition of potential, while formal, has little practical application, and a more useful concept is that of electric potential difference, and is the energy required to move a unit charge between two specified points. An electric field has the special property that it is "conservative", which means that the path taken by the test charge is irrelevant: all paths between two specified points expend the same energy, and thus a unique value for potential difference may be stated. The volt is so strongly identified as the unit of choice for measurement and description of electric potential difference that the term voltage sees greater everyday usage.

For practical purposes, it is useful to define a common reference point to which potentials may be expressed and compared. While this could be at infinity, a much more useful reference is the Earth itself, which is assumed to be at the same potential everywhere. This reference point naturally takes the name earth or ground. Earth is assumed to be an infinite source of equal amounts of positive and negative charge, and is therefore electrically uncharged—and unchargeable.

Electric potential is a scalar quantity, that is, it has only magnitude and not direction. It may be viewed as analogous to height: just as a released object will fall through a difference in heights caused by a gravitational field, so a charge will 'fall' across the voltage caused by an electric field. As relief maps show contour lines marking points of equal height, a set of lines marking points of equal potential (known as equipotentials) may be drawn around an electrostatically charged object. The equipotentials cross all lines of force at right angles. They must also lie parallel to a conductor's surface, otherwise this would produce a force that will move the charge carriers to even the potential of the surface.

The electric field was formally defined as the force exerted per unit charge, but the concept of potential allows for a more useful and equivalent definition: the electric field is the local gradient of the electric potential. Usually expressed in volts per metre, the vector direction of the field is the line of greatest slope of potential, and where the equipotentials lie closest together.

Ørsted's discovery in 1821 that a magnetic field existed around all sides of a wire carrying an electric current indicated that there was a direct relationship between electricity and magnetism. Moreover, the interaction seemed different from gravitational and electrostatic forces, the two forces of nature then known. The force on the compass needle did not direct it to or away from the current-carrying wire, but acted at right angles to it. Ørsted's words were that "the electric conflict acts in a revolving manner." The force also depended on the direction of the current, for if the flow was reversed, then the force did too.

Ørsted did not fully understand his discovery, but he observed the effect was reciprocal: a current exerts a force on a magnet, and a magnetic field exerts a force on a current. The phenomenon was further investigated by Ampère, who discovered that two parallel current-carrying wires exerted a force upon each other: two wires conducting currents in the same direction are attracted to each other, while wires containing currents in opposite directions are forced apart. The interaction is mediated by the magnetic field each current produces and forms the basis for the international definition of the ampere.
This relationship between magnetic fields and currents is extremely important, for it led to Michael Faraday's invention of the electric motor in 1821. Faraday's homopolar motor consisted of a permanent magnet sitting in a pool of mercury. A current was allowed through a wire suspended from a pivot above the magnet and dipped into the mercury. The magnet exerted a tangential force on the wire, making it circle around the magnet for as long as the current was maintained.

Experimentation by Faraday in 1831 revealed that a wire moving perpendicular to a magnetic field developed a potential difference between its ends. Further analysis of this process, known as electromagnetic induction, enabled him to state the principle, now known as Faraday's law of induction, that the potential difference induced in a closed circuit is proportional to the rate of change of magnetic flux through the loop. Exploitation of this discovery enabled him to invent the first electrical generator in 1831, in which he converted the mechanical energy of a rotating copper disc to electrical energy. Faraday's disc was inefficient and of no use as a practical generator, but it showed the possibility of generating electric power using magnetism, a possibility that would be taken up by those that followed on from his work.

The ability of chemical reactions to produce electricity, and conversely the ability of electricity to drive chemical reactions has a wide array of uses.

Electrochemistry has always been an important part of electricity. From the initial invention of the Voltaic pile, electrochemical cells have evolved into the many different types of batteries, electroplating and electrolysis cells. Aluminium is produced in vast quantities this way, and many portable devices are electrically powered using rechargeable cells.

An electric circuit is an interconnection of electric components such that electric charge is made to flow along a closed path (a circuit), usually to perform some useful task.

The components in an electric circuit can take many forms, which can include elements such as resistors, capacitors, switches, transformers and electronics. Electronic circuits contain active components, usually semiconductors, and typically exhibit non-linear behaviour, requiring complex analysis. The simplest electric components are those that are termed passive and linear: while they may temporarily store energy, they contain no sources of it, and exhibit linear responses to stimuli.

The resistor is perhaps the simplest of passive circuit elements: as its name suggests, it resists the current through it, dissipating its energy as heat. The resistance is a consequence of the motion of charge through a conductor: in metals, for example, resistance is primarily due to collisions between electrons and ions. Ohm's law is a basic law of circuit theory, stating that the current passing through a resistance is directly proportional to the potential difference across it. The resistance of most materials is relatively constant over a range of temperatures and currents; materials under these conditions are known as 'ohmic'. The ohm, the unit of resistance, was named in honour of Georg Ohm, and is symbolised by the Greek letter Ω. 1 Ω is the resistance that will produce a potential difference of one volt in response to a current of one amp.

The capacitor is a development of the Leyden jar and is a device that can store charge, and thereby storing electrical energy in the resulting field. It consists of two conducting plates separated by a thin insulating dielectric layer; in practice, thin metal foils are coiled together, increasing the surface area per unit volume and therefore the capacitance. The unit of capacitance is the farad, named after Michael Faraday, and given the symbol "F": one farad is the capacitance that develops a potential difference of one volt when it stores a charge of one coulomb. A capacitor connected to a voltage supply initially causes a current as it accumulates charge; this current will however decay in time as the capacitor fills, eventually falling to zero. A capacitor will therefore not permit a steady state current, but instead blocks it.

The inductor is a conductor, usually a coil of wire, that stores energy in a magnetic field in response to the current through it. When the current changes, the magnetic field does too, inducing a voltage between the ends of the conductor. The induced voltage is proportional to the time rate of change of the current. The constant of proportionality is termed the inductance. The unit of inductance is the henry, named after Joseph Henry, a contemporary of Faraday. One henry is the inductance that will induce a potential difference of one volt if the current through it changes at a rate of one ampere per second. The inductor's behaviour is in some regards converse to that of the capacitor: it will freely allow an unchanging current, but opposes a rapidly changing one.

Electric power is the rate at which electric energy is transferred by an electric circuit. The SI unit of power is the watt, one joule per second.

Electric power, like mechanical power, is the rate of doing work, measured in watts, and represented by the letter "P". The term "wattage" is used colloquially to mean "electric power in watts." The electric power in watts produced by an electric current "I" consisting of a charge of "Q" coulombs every "t" seconds passing through an electric potential (voltage) difference of "V" is
where

Electricity generation is often done with electric generators, but can also be supplied by chemical sources such as electric batteries or by other means from a wide variety of sources of energy. Electric power is generally supplied to businesses and homes by the electric power industry. Electricity is usually sold by the kilowatt hour (3.6 MJ) which is the product of power in kilowatts multiplied by running time in hours. Electric utilities measure power using electricity meters, which keep a running total of the electric energy delivered to a customer. Unlike fossil fuels, electricity is a low entropy form of energy and can be converted into motion or many other forms of energy with high efficiency.

Electronics deals with electrical circuits that involve active electrical components such as vacuum tubes, transistors, diodes, optoelectronics, sensors and integrated circuits, and associated passive interconnection technologies. The nonlinear behaviour of active components and their ability to control electron flows makes amplification of weak signals possible and electronics is widely used in information processing, telecommunications, and signal processing. The ability of electronic devices to act as switches makes digital information processing possible. Interconnection technologies such as circuit boards, electronics packaging technology, and other varied forms of communication infrastructure complete circuit functionality and transform the mixed components into a regular working system.

Today, most electronic devices use semiconductor components to perform electron control. The study of semiconductor devices and related technology is considered a branch of solid state physics, whereas the design and construction of electronic circuits to solve practical problems come under electronics engineering.

Faraday's and Ampère's work showed that a time-varying magnetic field acted as a source of an electric field, and a time-varying electric field was a source of a magnetic field. Thus, when either field is changing in time, then a field of the other is necessarily induced. Such a phenomenon has the properties of a wave, and is naturally referred to as an electromagnetic wave. Electromagnetic waves were analysed theoretically by James Clerk Maxwell in 1864. Maxwell developed a set of equations that could unambiguously describe the interrelationship between electric field, magnetic field, electric charge, and electric current. He could moreover prove that such a wave would necessarily travel at the speed of light, and thus light itself was a form of electromagnetic radiation. Maxwell's Laws, which unify light, fields, and charge are one of the great milestones of theoretical physics.

Thus, the work of many researchers enabled the use of electronics to convert signals into high frequency oscillating currents, and via suitably shaped conductors, electricity permits the transmission and reception of these signals via radio waves over very long distances.

In the 6th century BC, the Greek philosopher Thales of Miletus experimented with amber rods and these experiments were the first studies into the production of electrical energy. While this method, now known as the triboelectric effect, can lift light objects and generate sparks, it is extremely inefficient. It was not until the invention of the voltaic pile in the eighteenth century that a viable source of electricity became available. The voltaic pile, and its modern descendant, the electrical battery, store energy chemically and make it available on demand in the form of electrical energy. The battery is a versatile and very common power source which is ideally suited to many applications, but its energy storage is finite, and once discharged it must be disposed of or recharged. For large electrical demands electrical energy must be generated and transmitted continuously over conductive transmission lines.

Electrical power is usually generated by electro-mechanical generators driven by steam produced from fossil fuel combustion, or the heat released from nuclear reactions; or from other sources such as kinetic energy extracted from wind or flowing water. The modern steam turbine invented by Sir Charles Parsons in 1884 today generates about 80 percent of the electric power in the world using a variety of heat sources. Such generators bear no resemblance to Faraday's homopolar disc generator of 1831, but they still rely on his electromagnetic principle that a conductor linking a changing magnetic field induces a potential difference across its ends. The invention in the late nineteenth century of the transformer meant that electrical power could be transmitted more efficiently at a higher voltage but lower current. Efficient electrical transmission meant in turn that electricity could be generated at centralised power stations, where it benefited from economies of scale, and then be despatched relatively long distances to where it was needed.
Since electrical energy cannot easily be stored in quantities large enough to meet demands on a national scale, at all times exactly as much must be produced as is required. This requires electricity utilities to make careful predictions of their electrical loads, and maintain constant co-ordination with their power stations. A certain amount of generation must always be held in reserve to cushion an electrical grid against inevitable disturbances and losses.

Demand for electricity grows with great rapidity as a nation modernises and its economy develops. The United States showed a 12% increase in demand during each year of the first three decades of the twentieth century, a rate of growth that is now being experienced by emerging economies such as those of India or China. Historically, the growth rate for electricity demand has outstripped that for other forms of energy.

Environmental concerns with electricity generation have led to an increased focus on generation from renewable sources, in particular from wind and solar. While debate can be expected to continue over the environmental impact of different means of electricity production, its final form is relatively clean.

Electricity is a very convenient way to transfer energy, and it has been adapted to a huge, and growing, number of uses. The invention of a practical incandescent light bulb in the 1870s led to lighting becoming one of the first publicly available applications of electrical power. Although electrification brought with it its own dangers, replacing the naked flames of gas lighting greatly reduced fire hazards within homes and factories. Public utilities were set up in many cities targeting the burgeoning market for electrical lighting. In the late 20th century and in modern times, the trend has started to flow in the direction of deregulation in the electrical power sector.

The resistive Joule heating effect employed in filament light bulbs also sees more direct use in electric heating. While this is versatile and controllable, it can be seen as wasteful, since most electrical generation has already required the production of heat at a power station. A number of countries, such as Denmark, have issued legislation restricting or banning the use of resistive electric heating in new buildings. Electricity is however still a highly practical energy source for heating and refrigeration, with air conditioning/heat pumps representing a growing sector for electricity demand for heating and cooling, the effects of which electricity utilities are increasingly obliged to accommodate.

Electricity is used within telecommunications, and indeed the electrical telegraph, demonstrated commercially in 1837 by Cooke and Wheatstone, was one of its earliest applications. With the construction of first transcontinental, and then transatlantic, telegraph systems in the 1860s, electricity had enabled communications in minutes across the globe. Optical fibre and satellite communication have taken a share of the market for communications systems, but electricity can be expected to remain an essential part of the process.

The effects of electromagnetism are most visibly employed in the electric motor, which provides a clean and efficient means of motive power. A stationary motor such as a winch is easily provided with a supply of power, but a motor that moves with its application, such as an electric vehicle, is obliged to either carry along a power source such as a battery, or to collect current from a sliding contact such as a pantograph. Electrically powered vehicles are used in public transportation, such as electric buses and trains, and an increasing number of battery-powered electric cars in private ownership.

Electronic devices make use of the transistor, perhaps one of the most important inventions of the twentieth century, and a fundamental building block of all modern circuitry. A modern integrated circuit may contain several billion miniaturised transistors in a region only a few centimetres square.

A voltage applied to a human body causes an electric current through the tissues, and although the relationship is non-linear, the greater the voltage, the greater the current. The threshold for perception varies with the supply frequency and with the path of the current, but is about 0.1 mA to 1 mA for mains-frequency electricity, though a current as low as a microamp can be detected as an electrovibration effect under certain conditions. If the current is sufficiently high, it will cause muscle contraction, fibrillation of the heart, and tissue burns. The lack of any visible sign that a conductor is electrified makes electricity a particular hazard. The pain caused by an electric shock can be intense, leading electricity at times to be employed as a method of torture. Death caused by an electric shock is referred to as electrocution. Electrocution is still the means of judicial execution in some jurisdictions, though its use has become rarer in recent times.

Electricity is not a human invention, and may be observed in several forms in nature, a prominent manifestation of which is lightning. Many interactions familiar at the macroscopic level, such as touch, friction or chemical bonding, are due to interactions between electric fields on the atomic scale. The Earth's magnetic field is thought to arise from a natural dynamo of circulating currents in the planet's core. Certain crystals, such as quartz, or even sugar, generate a potential difference across their faces when subjected to external pressure. This phenomenon is known as piezoelectricity, from the Greek "piezein" (πιέζειν), meaning to press, and was discovered in 1880 by Pierre and Jacques Curie. The effect is reciprocal, and when a piezoelectric material is subjected to an electric field, a small change in physical dimensions takes place.

§Bioelectrogenesis in microbial life is a prominent phenomenon in soils and sediment ecology resulting from anaerobic respiration. The microbial fuel cell mimics this ubiquitous natural phenomenon.

Some organisms, such as sharks, are able to detect and respond to changes in electric fields, an ability known as electroreception, while others, termed electrogenic, are able to generate voltages themselves to serve as a predatory or defensive weapon. The order Gymnotiformes, of which the best known example is the electric eel, detect or stun their prey via high voltages generated from modified muscle cells called electrocytes. All animals transmit information along their cell membranes with voltage pulses called action potentials, whose functions include communication by the nervous system between neurons and muscles. An electric shock stimulates this system, and causes muscles to contract. Action potentials are also responsible for coordinating activities in certain plants.

In 1850, William Gladstone asked the scientist Michael Faraday why electricity was valuable. Faraday answered, “One day sir, you may tax it.”

In the 19th and early 20th century, electricity was not part of the everyday life of many people, even in the industrialised Western world. The popular culture of the time accordingly often depicted it as a mysterious, quasi-magical force that can slay the living, revive the dead or otherwise bend the laws of nature. This attitude began with the 1771 experiments of Luigi Galvani in which the legs of dead frogs were shown to twitch on application of animal electricity. "Revitalization" or resuscitation of apparently dead or drowned persons was reported in the medical literature shortly after Galvani's work. These results were known to Mary Shelley when she authored "Frankenstein" (1819), although she does not name the method of revitalization of the monster. The revitalization of monsters with electricity later became a stock theme in horror films.

As the public familiarity with electricity as the lifeblood of the Second Industrial Revolution grew, its wielders were more often cast in a positive light, such as the workers who "finger death at their gloves' end as they piece and repiece the living wires" in Rudyard Kipling's 1907 poem "Sons of Martha". Electrically powered vehicles of every sort featured large in adventure stories such as those of Jules Verne and the "Tom Swift" books. The masters of electricity, whether fictional or real—including scientists such as Thomas Edison, Charles Steinmetz or Nikola Tesla—were popularly conceived of as having wizard-like powers.

With electricity ceasing to be a novelty and becoming a necessity of everyday life in the later half of the 20th century, it required particular attention by popular culture only when it "stops" flowing, an event that usually signals disaster. The people who "keep" it flowing, such as the nameless hero of Jimmy Webb’s song "Wichita Lineman" (1968), are still often cast as heroic, wizard-like figures.





</doc>
<doc id="9553" url="https://en.wikipedia.org/wiki?curid=9553" title="Empedocles">
Empedocles

Empedocles (; , "Empedoklēs"; , fl. 444–443 BC) was a Greek pre-Socratic philosopher and a native citizen of Akragas, a Greek city in Sicily. Empedocles' philosophy is best known for originating the cosmogonic theory of the four classical elements. He also proposed forces he called Love and Strife which would mix and separate the elements, respectively.

Influenced by Pythagoras (died c. 495 BC) and the Pythagoreans, Empedocles challenged the practice of animal sacrifice and killing animals for food. He developed a distinctive doctrine of reincarnation. He is generally considered the last Greek philosopher to have recorded his ideas in verse. Some of his work survives, more than is the case for any other pre-Socratic philosopher. Empedocles' death was mythologized by ancient writers, and has been the subject of a number of literary treatments.

Empedocles (Empedokles) was a native citizen of Akragas in Sicily. He came from a rich and noble family. Very little is known about his life. His grandfather, also called Empedokles, had won a victory in the horse-race at Olympia in [the 71st Olympiad] OL. LXXI (496–95 BC). His father's name, according to the best accounts, was Meton.

All we can be said to know about the dates of Empedocles is, that his grandfather was still alive in 496 BC; that he himself was active at Akragas after 472 BC, the date of Theron’s death; and that he died later than 444 BC.

Empedocles "broke up the assembly of the Thousand. perhaps some oligarchical association or club." He is said to have been magnanimous in his support of the poor; severe in persecuting the overbearing conduct of the oligarchs; and he even declined the sovereignty of the city when it was offered to him.

According to John Burnet: "there is another side to his public character ... He claimed to be a god, and to receive the homage of his fellow-citizens in that capacity. The truth is, Empedokles was not a mere statesman; he had a good deal of the 'medicine-man' about him. ... We can see what this means from the fragments of the "Purifications". Empedokles was a preacher of the new religion which sought to secure release from the 'wheel of birth' by purity and abstinence. Orphicism seems to have been strong at Akragas in the days of Theron, and there are even some verbal coincidences between the poems of Empedokles and the Orphicsing Odes which Pindar addressed to that prince."

His brilliant oratory, his penetrating knowledge of nature, and the reputation of his marvellous powers, including the curing of diseases, and averting epidemics, produced many myths and stories surrounding his name. In his poem "Purifications" he claimed miraculous powers, including the destruction of evil, the curing of old age, and the controlling of wind and rain.

Empedocles was acquainted or connected by friendship with the physicians Pausanias (his eromenos) and Acron; with various Pythagoreans; and even, it is said, with Parmenides and Anaxagoras. The only pupil of Empedocles who is mentioned is the sophist and rhetorician Gorgias.

Timaeus and Dicaearchus spoke of the journey of Empedocles to the Peloponnese, and of the admiration, which was paid to him there; others mentioned his stay at Athens, and in the newly founded colony of Thurii, 446 BC; there are also fanciful reports of him travelling far to the east to the lands of the Magi.

The contemporary "Life of Empedocles" by Xanthus has been lost.

According to Aristotle, he died at the age of sixty (), even though other writers have him living up to the age of one hundred and nine. Likewise, there are myths concerning his death: a tradition, which is traced to Heraclides Ponticus, represented him as having been removed from the Earth; whereas others had him perishing in the flames of Mount Etna.

According to Burnet: "We are told that Empedokles leapt into the crater of Etna that he might be deemed a god. This appears to be a malicious version of a tale set on foot by his adherents that he had been snatched up to heaven in the night. Both stories would easily get accepted; for there was no local tradition. Empedokles did not die in Sicily, but in the Peloponnese, or, perhaps, at Thourioi. It is not at all unlikely that he visited Athens. ... Timaios refuted the common stories [about Empedokles] at some length. (Diog. viii. 71 sqq.; Ritter and. Preller [162].). He was quite positive that Empedokles never returned to Sicily after he went to Olympia to have his poem recited to the Hellenes. The plan for the colonisation of Thourioi would, of course, be discussed at Olympia, and we know that Greeks from the Peloponnese and elsewhere joined it. He may very well have gone to Athens in connexion with this."

Empedocles is considered the last Greek philosopher to write in verse. There is a debate about whether the surviving fragments of his teaching should be attributed to two separate poems, "Purifications" and "On Nature", with different subject matter, or whether they may all derive from one poem with two titles, or whether one title refers to part of the whole poem. Some scholars argue that the title "Purifications" refers to the first part of a larger work called (as a whole) "On Nature". There is also a debate about which fragments should be attributed to each of the poems, if there are two poems, or if part of it is called "Purifications"; because ancient writers rarely mentioned which poem they were quoting. 

Empedocles was undoubtedly acquainted with the didactic poems of Xenophanes and Parmenides—allusions to the latter can be found in the fragments—but he seems to have surpassed them in the animation and richness of his style, and in the clearness of his descriptions and diction. Aristotle called him the father of rhetoric, and, although he acknowledged only the meter as a point of comparison between the poems of Empedocles and the epics of Homer, he described Empedocles as Homeric and powerful in his diction. Lucretius speaks of him with enthusiasm, and evidently viewed him as his model. The two poems together comprised 5000 lines. About 550 lines of his poetry survive.

In the old editions of Empedocles, only about 100 lines were typically ascribed to his "Purifications", which was taken to be a poem about ritual purification, or the poem that contained all his religious and ethical thought. Early editors supposed that it was a poem that offered a mythical account of the world which may, nevertheless, have been part of Empedocles' philosophical system. According to Diogenes Laërtius it began with the following verses:
<poem>
Friends who inhabit the mighty town by tawny Acragas
which crowns the citadel, caring for good deeds,
greetings; I, an immortal God, no longer mortal,
wander among you, honoured by all,
adorned with holy diadems and blooming garlands.
To whatever illustrious towns I go,
I am praised by men and women, and accompanied
by thousands, who thirst for deliverance,
some ask for prophecies, and some entreat,
for remedies against all kinds of disease.
</poem>
In the older editions, it is to this work that editors attributed the story about souls, where we are told that there were once spirits who lived in a state of bliss, but having committed a crime (the nature of which is unknown) they were punished by being forced to become mortal beings, reincarnated from body to body. Humans, animals, and even plants are such spirits. The moral conduct recommended in the poem may allow us to become like gods again. If, as is now widely held, this title "Purifications" refers to the poem "On Nature", or to a part of that poem, this story will have been at the beginning of the main work on nature and the cosmic cycle. The relevant verses are also sometimes attributed to the proem of "On Nature", even by those who think that there was a separate poem called "Purifications".

There are about 450 lines of his poem "On Nature" extant, including 70 lines which have been reconstructed from some papyrus scraps known as the "Strasbourg Papyrus". The poem originally consisted of 2000 lines of hexameter verse, and was addressed to Pausanias. It was this poem which outlined his philosophical system. In it, Empedocles explains not only the nature and history of the universe, including his theory of the four classical elements, but he describes theories on causation, perception, and thought, as well as explanations of terrestrial phenomena and biological processes.

Although acquainted with the theories of the Eleatics and the Pythagoreans, Empedocles did not belong to any one definite school. An eclectic in his thinking, he combined much that had been suggested by Parmenides, Pythagoras and the Ionian schools. He was a firm believer in Orphic mysteries, as well as a scientific thinker and a precursor of physics. Aristotle mentions Empedocles among the Ionic philosophers, and he places him in very close relation to the atomist philosophers and to Anaxagoras.

According to House (1956)

Empedocles, like the Ionian philosophers and the atomists, continued the tradition of tragic thought which tried to find the basis of the relationship of the one and many. Each of the various philosophers, following Parmenides, derived from the Eleatics, the conviction that an existence could not pass into non-existence, and vice versa. Yet, each one had his peculiar way of describing this relation of Divine and mortal thought and thus of the relation of the One and the Many. In order to account for change in the world, in accordance with the ontological requirements of the Eleatics, they viewed changes as the result of mixture and separation of unalterable fundamental realities. Empedocles held that the four elements (Water, Air, Earth, and Fire) were those unchangeable fundamental realities, which were themselves transfigured into successive worlds by the powers of Love and Strife (Heraclitus had explicated the Logos or the "unity of opposites").

Empedocles established four ultimate elements which make all the structures in the world—fire, air, water, earth. Empedocles called these four elements "roots", which he also identified with the mythical names of Zeus, Hera, Nestis, and Aidoneus (e.g., "Now hear the fourfold roots of everything: enlivening Hera, Hades, shining Zeus. And Nestis, moistening mortal springs with tears"). Empedocles never used the term "element" (, "stoicheion"), which seems to have been first used by Plato. According to the different proportions in which these four indestructible and unchangeable elements are combined with each other the difference of the structure is produced. It is in the aggregation and segregation of elements thus arising, that Empedocles, like the atomists, found the real process which corresponds to what is popularly termed growth, increase or decrease. Nothing new comes or can come into being; the only change that can occur is a change in the juxtaposition of element with element. This theory of the four elements became the standard dogma for the next two thousand years.

The four elements, however, are simple, eternal, and unalterable, and as change is the consequence of their mixture and separation, it was also necessary to suppose the existence of moving powers that bring about mixture and separation. The four elements are both eternally brought into union and parted from one another by two divine powers, Love and ("Philotes" and "Neikos"). Love () is responsible for the attraction of different forms of what we now call matter, and Strife () is the cause of their separation. If the four elements make up the universe, then Love and Strife explain their variation and harmony. Love and Strife are attractive and repulsive forces, respectively, which are plainly observable in human behavior, but also pervade the universe. The two forces wax and wane in their dominance, but neither force ever wholly escapes the imposition of the other.

According to Burnet: "Empedokles sometimes gave an efficient power to Love and Strife, and sometimes put them on a level with the other four. The fragments leave no room for doubt that they were thought of as spatial and corporeal. ... Love is said to be "equal in length and breadth" to the others, and Strife is described as equal to each of them in weight (fr. 17). These physical speculations were part of a history of the universe which also dealt with the origin and development of life."

As the best and original state, there was a time when the pure elements and the two powers co-existed in a condition of rest and inertness in the form of a sphere. The elements existed together in their purity, without mixture and separation, and the uniting power of Love predominated in the sphere: the separating power of Strife guarded the extreme edges of the sphere. Since that time, strife gained more sway and the bond which kept the pure elementary substances together in the sphere was dissolved. The elements became the world of phenomena we see today, full of contrasts and oppositions, operated on by both Love and Strife. The sphere of Empedocles being the embodiment of pure existence is the embodiment or representative of God. Empedocles assumed a cyclical universe whereby the elements return and prepare the formation of the sphere for the next period of the universe.

Empedocles attempted to explain the separation of elements, the formation of earth and sea, of Sun and Moon, of atmosphere. He also dealt with the first origin of plants and animals, and with the physiology of humans. As the elements entered into combinations, there appeared strange results—heads without necks, arms without shoulders. Then as these fragmentary structures met, there were seen horned heads on human bodies, bodies of oxen with human heads, and figures of double sex. But most of these products of natural forces disappeared as suddenly as they arose; only in those rare cases where the parts were found to be adapted to each other did the complex structures last. Thus the organic universe sprang from spontaneous aggregations that suited each other as if this had been intended. Soon various influences reduced creatures of double sex to a male and a female, and the world was replenished with organic life. It is possible to see this theory as an anticipation of Charles Darwin's theory of natural selection, although Empedocles was not trying to explain evolution.

Empedocles is credited with the first comprehensive theory of light and vision. He put forward the idea that we see objects because light streams out of our eyes and touches them. While flawed, this became the fundamental basis on which later Greek philosophers and mathematicians like Euclid would construct some of the most important theories of light, vision, and optics.

Knowledge is explained by the principle that elements in the things outside us are perceived by the corresponding elements in ourselves. Like is known by like. The whole body is full of pores and hence respiration takes place over the whole frame. In the organs of sense these pores are specially adapted to receive the effluences which are continually rising from bodies around us; thus perception occurs. In vision, certain particles go forth from the eye to meet similar particles given forth from the object, and the resultant contact constitutes vision. Perception is not merely a passive reflection of external objects.

Empedocles noted the limitation and narrowness of human perceptions. We see only a part but fancy that we have grasped the whole. But the senses cannot lead to truth; thought and reflection must look at the thing from every side. It is the business of a philosopher, while laying bare the fundamental difference of elements, to show the identity that exists between what seem unconnected parts of the universe.

In a famous fragment, Empedocles attempted to explain the phenomenon of respiration by means of an elaborate analogy with the clepsydra, an ancient device for conveying liquids from one vessel to another. This fragment has sometimes been connected to a passage in Aristotle's "Physics" where Aristotle refers to people who twisted wineskins and captured air in clepsydras to demonstrate that void does not exist. There is however, no evidence that Empedocles performed any experiment with clepsydras. The fragment certainly implies that Empedocles knew about the corporeality of air, but he says nothing whatever about the void. The clepsydra was a common utensil and everyone who used it must have known, in some sense, that the invisible air could resist liquid.

Like Pythagoras, Empedocles believed in the transmigration of the soul/metempsychosis, that souls can be reincarnated between humans, animals and even plants. For Empedocles, all living things were on the same spiritual plane; plants and animals are links in a chain where humans are a link too. Empedocles was a vegetarian and advocated vegetarianism, since the bodies of animals are the dwelling places of punished souls. Wise people, who have learned the secret of life, are next to the divine, and their souls, free from the cycle of reincarnations, are able to rest in happiness for eternity.

Diogenes Laërtius records the legend that Empedocles died by throwing himself into Mount Etna in Sicily, so that the people would believe his body had vanished and he had turned into an immortal god; the volcano, however, threw back one of his bronze sandals, revealing the deceit. Another legend maintains that he threw himself into the volcano to prove to his disciples that he was immortal; he believed he would come back as a god after being consumed by the fire. Horace also refers to the death of Empedocles in his work "Ars Poetica" and admits poets the right to destroy themselves.

In "Icaro-Menippus", a comedic dialogue written by the second century satirist Lucian of Samosata, Empedocles' final fate is re-evaluated. Rather than being incinerated in the fires of Mount Etna, he was carried up into the heavens by a volcanic eruption. Although a bit singed by the ordeal, Empedocles survives and continues his life on the Moon, surviving by feeding on dew.

Empedocles' death has inspired two major modern literary treatments. Empedocles' death is the subject of Friedrich Hölderlin's play "Tod des Empedokles" ("The Death of Empedocles"), two versions of which were written between the years 1798 and 1800. A third version was made public in 1826. In Matthew Arnold's poem "Empedocles on Etna", a narrative of the philosopher's last hours before he jumps to his death in the crater first published in 1852, Empedocles predicts:
<poem>
To the elements it came from
Everything will return.
Our bodies to earth,
Our blood to water,
Heat to fire,
Breath to air.
</poem>

In his "History of Western Philosophy", Bertrand Russell humorously quotes an unnamed poet on the subject – "Great Empedocles, that ardent soul, Leapt into Etna, and was roasted whole."

In "J R" by William Gaddis, Karl Marx's famous dictum ("From each according to his abilities, to each according to his needs") is misattributed to Empedocles.

In 2006, a massive underwater volcano off the coast of Sicily was named Empedocles.

In 2016, Scottish musician Momus wrote and sang the song "The Death of Empedokles" for his album "Scobberlotchers".






</doc>
<doc id="9555" url="https://en.wikipedia.org/wiki?curid=9555" title="Ericaceae">
Ericaceae

The Ericaceae are a family of flowering plants, commonly known as the heath or heather family, found most commonly in acid and infertile growing conditions. The family is large, with c. 4250 known species spread across 124 genera, making it the 14th most species-rich family of flowering plants. The many well-known and economically important members of the Ericaceae include the cranberry, blueberry, huckleberry, rhododendron (including azaleas), and various common heaths and heathers ("Erica", "Cassiope", "Daboecia", and "Calluna" for example).

The Ericaceae contain a morphologically diverse range of taxa, including herbs, dwarf shrubs, shrubs, and trees. Their leaves are usually evergreen, alternate or whorled, simple and without stipules. Their flowers are hermaphrodite and show considerable variability. The petals are often fused (sympetalous) with shapes ranging from narrowly tubular to funnelform or widely urn-shaped. The corollas are usually radially symmetrical (actinomorphic) and urn-shaped, but many flowers of the genus "Rhododendron" are somewhat bilaterally symmetrical (zygomorphic). Anthers open by pores.

Michel Adanson used the term Vaccinia to describe a similar family, but Antoine Laurent de Jussieu first used the term Ericaceae. The name comes from the type genus "Erica", which appears to be derived from the Greek word "ereike". The exact meaning is difficult to interpret, but some sources show it as meaning 'heather'. The name may have been used informally to refer to the plants before Linnaean times, and simply been formalised when Linnaeus described "Erica" in 1753, and then again when Jussieu described the Ericaceae in 1789.

Historically, the Ericaceae included both subfamilies and tribes. In 1971, Stevens, who outlined the history from 1876 and in some instances 1839, recognised six subfamilies (Rhododendroideae, Ericoideae, Vaccinioideae, Pyroloideae, Monotropoideae, and Wittsteinioideae), and further subdivided four of the subfamilies into tribes, the Rhododendroideae having seven tribes (Bejarieae, Rhodoreae, Cladothamneae, Epigaeae, Phyllodoceae, and Diplarcheae). Within tribe Rhodoreae, five genera were described, "Rhododendron" L. (including "Azalea" L. pro parte), "Therorhodion" Small, "Ledum" L., "Tsusiophyllum" Max., "Menziesia" J. E. Smith, that were eventually transferred into "Rhododendron", along with Diplarche from the monogeneric tribe Diplarcheae.

In 2002, systematic research resulted in the inclusion of the formerly recognised families Empetraceae, Epacridaceae, Monotropaceae, Prionotaceae, and Pyrolaceae into the Ericaceae based on a combination of molecular, morphological, anatomical, and embryological data, analysed within a phylogenetic framework. The move significantly increased the morphological and geographical range found within the group. One possible classification of the resulting family includes 9 subfamilies, 126 genera, and about 4000 species:


The Ericaceae have a nearly worldwide distribution. They are absent from continental Antarctica, parts of the high Arctic, central Greenland, northern and central Australia, and much of the lowland tropics and neotropics.

The family is largely composed of plants that can tolerate acidic, infertile conditions. Like other stress-tolerant plants, many Ericaceae have mycorrhizal fungi to assist with extracting nutrients from infertile soils, as well as evergreen foliage to conserve absorbed nutrients. This trait is not found in the Clethraceae and Cyrillaceae, the two families most closely related to the Ericaceae. Most Ericaceae (excluding the Monotropoideae, and some Styphelioideae) form a distinctive accumulation of mycorrhizae, in which fungi grow in and around the roots and provide the plant with nutrients. The Pyroloideae are mixotrophic and gain sugars from the mycorrhizae, as well as nutrients.

In many parts of the world, a "heath" or "heathland" is an environment characterised by an open dwarf-shrub community found on low-quality acidic soils, generally dominated by plants in the Ericaceae. A common example is "Erica tetralix". This plant family is also typical of peat bogs and blanket bogs; examples include "Rhododendron groenlandicum" and "Kalmia polifolia". In eastern North America, members of this family often grow in association with an oak canopy, in a habitat known as an oak-heath forest.

In heathland, plants in the family Ericaceae serve as hostplants to the butterfly "Plebejus argus""."

Some evidence suggests eutrophic rainwater can convert ericoid heaths with species such as "Erica tetralix" to grasslands. Nitrogen is particularly suspect in this regard, and may be causing measurable changes to the distribution and abundance of some ericaceous species.




</doc>
<doc id="9559" url="https://en.wikipedia.org/wiki?curid=9559" title="Electrical network">
Electrical network

An electrical network is an interconnection of electrical components (e.g., batteries, resistors, inductors, capacitors, switches, transistors) or a model of such an interconnection, consisting of electrical elements (e.g., voltage sources, current sources, resistances, inductances, capacitances). An electrical circuit is a network consisting of a closed loop, giving a return path for the current. Linear electrical networks, a special type consisting only of sources (voltage or current), linear lumped elements (resistors, capacitors, inductors), and linear distributed elements (transmission lines), have the property that signals are linearly superimposable. They are thus more easily analyzed, using powerful frequency domain methods such as Laplace transforms, to determine DC response, AC response, and transient response.

A resistive circuit is a circuit containing only resistors and ideal current and voltage sources. Analysis of resistive circuits is less complicated than analysis of circuits containing capacitors and inductors. If the sources are constant (DC) sources, the result is a DC circuit. The effective resistance and current distribution properties of arbitrary resistor networks can be modeled in terms of their graph measures and geometrical properties.

A network that contains active electronic components is known as an "electronic circuit". Such networks are generally nonlinear and require more complex design and analysis tools.

An active network contains at least one voltage source or current source that can supply energy to the network indefinitely. A passive network does not contain an active source.

An active network contains one or more sources of electromotive force. Practical examples of such sources include a battery or a generator. Active elements can inject power to the circuit, provide power gain, and control the current flow within the circuit.

Passive networks do not contain any sources of electromotive force. They consist of passive elements like resistors and capacitors.

A network is linear if its signals obey the principle of superposition; otherwise it is non-linear. Passive networks are generally taken to be linear, but there are exceptions. For instance, an inductor with an iron core can be driven into saturation if driven with a large enough current. In this region, the behaviour of the inductor is very non-linear.

Discrete passive components (resistors, capacitors and inductors) are called "lumped elements" because all of their, respectively, resistance, capacitance and inductance is assumed to be located ("lumped") at one place. This design philosophy is called the lumped-element model and networks so designed are called "lumped-element circuits". This is the conventional approach to circuit design. At high enough frequencies the lumped assumption no longer holds because there is a significant fraction of a wavelength across the component dimensions. A new design model is needed for such cases called the distributed-element model. Networks designed to this model are called "distributed-element circuits".

A distributed-element circuit that includes some lumped components is called a "semi-lumped" design. An example of a semi-lumped circuit is the combline filter.

Sources can be classified as independent sources and dependent sources.

An ideal independent source maintains the same voltage or current regardless of the other elements present in the circuit. Its value is either constant (DC) or sinusoidal (AC). The strength of voltage or current is not changed by any variation in the connected network.

Dependent sources depend upon a particular element of the circuit for delivering the power or voltage or current depending upon the type of source it is.

A number of electrical laws apply to all electrical networks. These include:

Other more complex laws may be needed if the network contains nonlinear or reactive components. Non-linear self-regenerative heterodyning systems can be approximated. Applying these laws results in a set of simultaneous equations that can be solved either algebraically or numerically.

To design any electrical circuit, either analog or digital, electrical engineers need to be able to predict the voltages and currents at all places within the circuit. Simple linear circuits can be analyzed by hand using complex number theory. In more complex cases the circuit may be analyzed with specialized computer programs or estimation techniques such as the piecewise-linear model.

Circuit simulation software, such as HSPICE (an analog circuit simulator), and languages such as VHDL-AMS and verilog-AMS allow engineers to design circuits without the time, cost and risk of error involved in building circuit prototypes.

More complex circuits can be analyzed numerically with software such as SPICE or GNUCAP, or symbolically using software such as SapWin.

When faced with a new circuit, the software first tries to find a steady state solution, that is, one where all nodes conform to Kirchhoff's current law "and" the voltages across and through each element of the circuit conform to the voltage/current equations governing that element.

Once the steady state solution is found, the "operating points" of each element in the circuit are known. For a small signal analysis, every non-linear element can be linearized around its operation point to obtain the small-signal estimate of the voltages and currents. This is an application of Ohm's Law. The resulting linear circuit matrix can be solved with Gaussian elimination.

Software such as the PLECS interface to Simulink uses piecewise-linear approximation of the equations governing the elements of a circuit. The circuit is treated as a completely linear network of ideal diodes. Every time a diode switches from on to off or vice versa, the configuration of the linear network changes. Adding more detail to the approximation of equations increases the accuracy of the simulation, but also increases its running time.









</doc>
<doc id="9561" url="https://en.wikipedia.org/wiki?curid=9561" title="Euler (disambiguation)">
Euler (disambiguation)


Euler may also refer to:





</doc>
<doc id="9566" url="https://en.wikipedia.org/wiki?curid=9566" title="Empty set">
Empty set

In mathematics, the empty set is the unique set having no elements; its size or cardinality (count of elements in a set) is zero. Some axiomatic set theories ensure that the empty set exists by including an axiom of empty set, while in other theories, its existence can be deduced. Many possible properties of sets are vacuously true for the empty set.

In some textbooks and popularizations, the empty set is referred to as the "null set". However, "null set" is a distinct notion within the context of measure theory, in which it describes a set of measure zero (which is not necessarily empty). The empty set may also be called the "void set". It is commonly denoted by the symbols formula_1, formula_2 or formula_3.

Common notations for the empty set include "{}", "formula_2", and "∅". The latter two symbols were introduced by the Bourbaki group (specifically André Weil) in 1939, inspired by the letter Ø in the Danish and Norwegian alphabets. In the past, "0" was occasionally used as a symbol for the empty set, but this is now considered to be an improper use of notation.

The symbol ∅ is available at Unicode point U+2205. It can be coded in HTML as &empty; and as &#8709;. It can be coded in LaTeX as \varnothing. The symbol formula_2 is coded in LaTeX as \emptyset.

When writing in languages such as Danish and Norwegian, where the empty set character may be confused with the alphabetic letter Ø (as when using the symbol in linguistics), the Unicode character U+29B0 REVERSED EMPTY SET ⦰ may be used instead.

In standard axiomatic set theory, by the principle of extensionality, two sets are equal if they have the same elements. As a result, there can be only one set with no elements, hence the usage of "the empty set" rather than "an empty set".

The following lists document of some of the most notable properties related to the empty set. For more on the mathematical symbols used therein, see "List of mathematical symbols".

For any set "A":

The empty set has the following properties:

The connection between the empty set and zero goes further, however: in the standard set-theoretic definition of natural numbers, sets are used to model the natural numbers. In this context, zero is modelled by the empty set.

For any property "P":

Conversely, if for some property "P" and some set "V", the following two statements hold:
then "V" = ∅.

By the definition of subset, the empty set is a subset of any set "A". That is, "every" element "x" of formula_1 belongs to "A". Indeed, if it were not true that every element of formula_1 is in "A", then there would be at least one element of formula_1 that is not present in "A". Since there are "no" elements of formula_1 at all, there is no element of formula_1 that is not in "A". Any statement that begins "for every element of formula_1" is not making any substantive claim; it is a vacuous truth. This is often paraphrased as "everything is true of the elements of the empty set."

When speaking of the sum of the elements of a finite set, one is inevitably led to the convention that the sum of the elements of the empty set is zero. The reason for this is that zero is the identity element for addition. Similarly, the product of the elements of the empty set should be considered to be one (see empty product), since one is the identity element for multiplication.

A derangement is a permutation of a set without fixed points. The empty set can be considered a derangement of itself, because it has only one permutation (formula_21), and it is vacuously true that no element (of the empty set) can be found that retains its original position.

Since the empty set has no member when it is considered as a subset of any ordered set, every member of that set will be an upper bound and lower bound for the empty set. For example, when considered as a subset of the real numbers, with its usual ordering, represented by the real number line, every real number is both an upper and lower bound for the empty set. When considered as a subset of the extended reals formed by adding two "numbers" or "points" to the real numbers (namely negative infinity, denoted formula_22 which is defined to be less than every other extended real number, and positive infinity, denoted formula_23 which is defined to be greater than every other extended real number), we have that: 

and

That is, the least upper bound (sup or supremum) of the empty set is negative infinity, while the greatest lower bound (inf or infimum) is positive infinity. By analogy with the above, in the domain of the extended reals, negative infinity is the identity element for the maximum and supremum operators, while positive infinity is the identity element for the minimum and infimum operators.

In any topological space "X", the empty set is open by definition, as is "X". Since the complement of an open set is closed and the empty set and "X" are complements of each other, the empty set is also closed, making it a clopen set. Moreover, the empty set is compact by the fact that every finite set is compact.

The closure of the empty set is empty. This is known as "preservation of nullary unions."

If "A" is a set, then there exists precisely one function "f" from ∅ to "A", the empty function. As a result, the empty set is the unique initial object of the category of sets and functions.

The empty set can be turned into a topological space, called the empty space, in just one way: by defining the empty set to be open. This empty topological space is the unique initial object in the category of topological spaces with continuous maps. In fact, it is a strict initial object: only the empty set has a function to the empty set.

In the von Neumann construction of the ordinals, 0 is defined as the empty set, and the successor of an ordinal is defined as formula_26. Thus, we have formula_27, formula_28, formula_29, and so on. The von Neumann construction, along with the axiom of infinity, which guarantees the existence of at least one infinite set, can be used to construct the set of natural numbers, formula_30, such that the Peano axioms of arithmetic are satisfied.

In Zermelo set theory, the existence of the empty set is assured by the axiom of empty set, and its uniqueness follows from the axiom of extensionality. However, the axiom of empty set can be shown redundant in at least two ways:

While the empty set is a standard and widely accepted mathematical concept, it remains an ontological curiosity, whose meaning and usefulness are debated by philosophers and logicians.

The empty set is not the same thing as "nothing"; rather, it is a set with nothing "inside" it and a set is always "something". This issue can be overcome by viewing a set as a bag—an empty bag undoubtedly still exists. Darling (2004) explains that the empty set is not nothing, but rather "the set of all triangles with four sides, the set of all numbers that are bigger than nine but smaller than eight, and the set of all opening moves in chess that involve a king."

The popular syllogism
is often used to demonstrate the philosophical relation between the concept of nothing and the empty set. Darling writes that the contrast can be seen by rewriting the statements "Nothing is better than eternal happiness" and "[A] ham sandwich is better than nothing" in a mathematical tone. According to Darling, the former is equivalent to "The set of all things that are better than eternal happiness is formula_1" and the latter to "The set {ham sandwich} is better than the set formula_1". The first compares elements of sets, while the second compares the sets themselves.

Jonathan Lowe argues that while the empty set:

it is also the case that:

George Boolos argued that much of what has been heretofore obtained by set theory can just as easily be obtained by plural quantification over individuals, without reifying sets as singular entities having other entities as members.




</doc>
<doc id="9567" url="https://en.wikipedia.org/wiki?curid=9567" title="Egoism">
Egoism

Egoism is the philosophy concerned with the role of the self, or , as the motivation and goal of one's own action. Different theories on egoism encompass a range of disparate ideas and can be categorized into descriptive or normative forms. That is, they may be interested in either describing that people "do" act in self-interest or prescribing that they "should".

The "New Catholic Encyclopedia" states of egoism that it "incorporates in itself certain basic truths: it is natural for man to love himself; he should moreover do so, since each one is ultimately responsible for himself; pleasure, the development of one's potentialities, and the acquisition of power are normally desirable." The moral censure of self-interest is a common subject of critique in egoist philosophy, with such judgments being examined as means of control and the result of power relations. Egoism may also reject that insight into one's internal motivation can arrive extrinsically, such as from psychology or sociology, though, for example, this is not present in the philosophy of Friedrich Nietzsche.

The term egoism is derived from the French , from the Latin (first person singular personal pronoun; "I") with the French ("-ism"). As such, the term shares early etymology with egotism.

The descriptive variants of egoism are concerned with self-interest as a factual description of human motivation and, in its furthest application, that all human motivation stems from the desires and interest of the ego. In these theories, action which is self-interested may be simply termed "egoistic".

The view that people "tend" to act in their own self-interest is called default egoism, whereas psychological egoism is the doctrine that holds that "all" motivations are rooted solely in psychological self-interest. That is, in its strong form, that even seemingly altruistic actions are only disguised as such and are always self-serving. Its weaker form instead holds that, even if altruistic motivation is possible, the willed action necessarily becomes egoistic in serving the ego's will. In contrast to this and philosophical egoism, biological egoism (also called evolutionary egoism) describes motivations rooted solely in reproductive self-interest (i.e. reproductive fitness). Furthermore, selfish gene theory holds that it is the self-interest of genetic information that conditions human behaviour.

In his "On the Genealogy of Morals", Friedrich Nietzsche traces the origins of master–slave morality to fundamentally egoistic value judgments. In the aristocratic valuation, excellence and virtue come as a form of superiority over the common masses, which the priestly valuation, in "ressentiment" of power, seeks to invert—where the powerless and pitiable become the moral ideal. This upholding of unegoistic actions is therefore seen as stemming from a desire to reject the superiority or excellency of others. He holds that all normative systems which operate in the role often associated with morality favor the interests of some people, often, though not necessarily, at the expense of others.

Theories which hold egoism to be normative stipulate that the ego ought to promote its own interests above other values. Where this ought is held to be a pragmatic judgment it is termed rational egoism and where it is held to be a moral judgment it is termed ethical egoism. The "Stanford Encyclopedia of Philosophy" states that "ethical egoism might also apply to things other than acts, such as rules or character traits" but that such variants are uncommon. Furthermore, conditional egoism is a consequentialist form of ethical egoism which holds that egoism is morally right if it leads to morally acceptable ends. John F. Welsh, in his work "Max Stirner's Dialectical Egoism: A New Interpretation", coins the term dialectical egoism to describe an interepretation of the egoist philosophy of Max Stirner as being fundamentally dialectical.

Normative egoism, as in the case of Stirner, need not reject that some modes of behavior are to be valued above others—such as Stirner's affirmation that non-restriction and autonomy are to be most highly valued. Contrary theories, however, may just as easily favour egoistic domination of others.

In 1851, French philosopher Auguste Comte coined the term altruism (; ) as an antonym for egoism. The term entered English in 1853 and was popularized by advocates of Comte's moral philosophy—principally, that self-regard must be replaced with only the regard for others.

Comte argues that only two human motivations exist, egoistic and altruistic, and that the two cannot be mediated. That is, one must always predominate the other. For Comte, the total subordination of the self to altruism is a necessary condition to social and individual benefit. Friedrich Nietzsche, rather than rejecting the "practice" of altruism, warns that despite there being neither much altruism nor equality in the world, there is almost universal endorsement of their value and, notoriously, even by those who are its worst enemies in practice. Egoism commonly views the subordination of the self to altruism as either a form of domination that limits freedom, an unethical or irrational principle, or an extention of some egoistic root cause.

In evolutionary theory, biological altruism is the observed occurance of an organism acting to the benefit of others at the cost of its own reproductive fitness. While biological egoism does grant that an organism may act to the benefit of others, it describes only such when in accordance with reproductive self-interest. Kin altruism and selfish gene theory are examples of this division. On biological altruism, the "Stanford Encyclopedia of Philosophy" states the following:

This is a central topic within contemporary discourse of psychological egoism.

Max Stirner's rejection of absolutes and abstract concepts often places him among the first philosophical nihilists. Furthermore, his philosophy has been disputedly recognised as among the precursors to nihilism, existentialism, poststructuralism and postmodernism. The "Stanford Encyclopedia of Philosophy", states:

Russian philosophers Dmitry Pisarev and Nikolay Chernyshevsky, both advocates of rational egoism, were also major proponents of Russian nihilism.

The terms "nihilism" and "anti-nihilism" have both been used to categorise the philosophy of Friedrich Nietzsche. His thought has similarly been linked to forms of both descriptive and normative egoism.

Nietzsche, in attacking the widely held moral abhorrence for egoistic action, seeks to free higher human beings from their belief that this morality is good for them. He rejects Christian and Kantian ethics as merely the disguised egoism of slave morality.

Max Stirner's philosophy strongly rejects modernity and is highly critical of the increasing dogmatism and oppressive social institutions that embody it. In order that it might be surpassed, egoist principles are upheld as a necessary advancement beyond the modern world. The "Stanford Encyclopedia" states that Stirner's historical analyses serve to "undermine historical narratives which portray the modern development of humankind as the progressive realisation of freedom, but also to support an account of individuals in the modern world as increasingly oppressed". This critique of humanist discourses especially has linked Stirner to more contemporary poststructuralist thought.

Since normative egoism rejects the moral obligation to subordinate the ego to a ruling class, it is predisposed to certain political implications. The "Internet Encyclopedia of Philosophy" states:
In contrast with this however, such an ethic may not morally obligate against the egoistic exercise of power over others. On these grounds, Friedrich Nietzsche criticizes egalitarian morality and political projects as incondusive to the development of human excellence. Max Stirner's own conception, the union of egoists as detailed in his work "The Ego and Its Own", saw a proposed form of societal relations whereby limitations on egoistic action are rejected. When posthumously adopted by the anarchist movement, this became the foundation for egoist anarchism.

Stirner's variant of property theory is similarly dialectical, where the concept of ownership is only that personal distinction made between what is one's property and what is not. Consequentially, it is the exercise of control over property which constitutes the nonabstract possession of it. In contrast to this, Ayn Rand incorporates capitalist property rights into her egoist theory.

Egoist philosopher Nikolai Gavrilovich Chernyshevskii was the dominant intellectual figure behind the 1860–1917 revolutionary movement in Russia, which resulted in the assassination of Tsar Alexander II eight years before his death in 1889. Dmitry Pisarev was a similarly radical influence within the movement, though he did not personally advocate political revolution.

Philosophical egoism has also found wide appeal among anarchist revolutionaries and thinkers, such as John Henry Mackay, Benjamin Tucker, Emile Armand, Han Ryner Gérard de Lacaze-Duthiers, Renzo Novatore, Miguel Giménez Igualada, and Lev Chernyi. Though he did not involve in any revolutionary movements himself, the entire school of individualist anarchism owes much of its intellectual heritage to Max Stirner.

Egoist philosophy may be misrepresented as a principally revolutionary field of thought. However, neither Hobbesian nor Nietzshean theories of egoism approve of political revolution. Anarchism and revolutionary socialism were also strongly rejected by Ayn Rand and her followers.

The philosophies of both Nietzsche and Stirner were heavily appropriated by fascist and proto-fascist ideologies. Nietzsche in particular has infamously been misrepresented as a predecessor to Nazism and a substantial academic effort was necessary to disassociate his ideas from their aforementioned appropriation.

Despite this, the influence of Stirner's philosophy has been primarily anti-authoritarian. More contemporarily, Rand's thought has lent influence to the alt-right movement.




</doc>
<doc id="9569" url="https://en.wikipedia.org/wiki?curid=9569" title="Endomorphism">
Endomorphism

In mathematics, an endomorphism is a morphism from a mathematical object to itself. An endomorphism that is also an isomorphism is an automorphism. For example, an endomorphism of a vector space is a linear map , and an endomorphism of a group is a group homomorphism . In general, we can talk about endomorphisms in any category. In the category of sets, endomorphisms are functions from a set "S" to itself.

In any category, the composition of any two endomorphisms of is again an endomorphism of . It follows that the set of all endomorphisms of forms a monoid, the full transformation monoid, and denoted (or to emphasize the category ).

An invertible endomorphism of is called an automorphism. The set of all automorphisms is a subset of with a group structure, called the automorphism group of and denoted . In the following diagram, the arrows denote implication:

Any two endomorphisms of an abelian group, , can be added together by the rule . Under this addition, and with multiplication being defined as function composition, the endomorphisms of an abelian group form a ring (the endomorphism ring). For example, the set of endomorphisms of is the ring of all matrices with integer entries. The endomorphisms of a vector space or module also form a ring, as do the endomorphisms of any object in a preadditive category. The endomorphisms of a nonabelian group generate an algebraic structure known as a near-ring. Every ring with one is the endomorphism ring of its regular module, and so is a subring of an endomorphism ring of an abelian group; however there are rings that are not the endomorphism ring of any abelian group.

In any concrete category, especially for vector spaces, endomorphisms are maps from a set into itself, and may be interpreted as unary operators on that set, acting on the elements, and allowing to define the notion of orbits of elements, etc.

Depending on the additional structure defined for the category at hand (topology, metric, ...), such operators can have properties like continuity, boundedness, and so on. More details should be found in the article about operator theory.

An endofunction is a function whose domain is equal to its codomain. A homomorphic endofunction is an endomorphism.

Let be an arbitrary set. Among endofunctions on one finds permutations of and constant functions associating to every in the same element in . Every permutation of has the codomain equal to its domain and is bijective and invertible. If has more than one element, a constant function on has an image that is a proper subset of its codomain, and thus is not bijective (and hence not invertible). The function associating to each natural number the floor of has its image equal to its codomain and is not invertible.

Finite endofunctions are equivalent to directed pseudoforests. For sets of size there are endofunctions on the set.

Particular examples of bijective endofunctions are the involutions; i.e., the functions coinciding with their inverses.




</doc>
<doc id="9574" url="https://en.wikipedia.org/wiki?curid=9574" title="Eric Hoffer">
Eric Hoffer

Eric Hoffer (July 15, 1902 – May 21, 1983) was an American moral and social philosopher. He was the author of ten books and was awarded the Presidential Medal of Freedom in February 1983. His first book, "The True Believer" (1951), was widely recognized as a classic, receiving critical acclaim from both scholars and laymen, although Hoffer believed that "The Ordeal of Change" (1963) was his finest work.

Hoffer was born in 1902 in The Bronx, New York, to Knut and Elsa (Goebel) Hoffer. His parents were immigrants from Alsace, then part of Imperial Germany. By age five, Hoffer could already read in both English and his parents' native German. When he was five, his mother fell down the stairs with him in her arms. He later recalled, "I lost my sight at the age of seven. Two years before, my mother and I fell down a flight of stairs. She did not recover and died in that second year after the fall. I lost my sight and, for a time, my memory." Hoffer spoke with a pronounced German accent all his life, and spoke the language fluently. He was raised by a live-in relative or servant, a German immigrant named Martha. His eyesight inexplicably returned when he was 15. Fearing he might lose it again, he seized on the opportunity to read as much as he could. His recovery proved permanent, but Hoffer never abandoned his reading habit.

Hoffer was a young man when he also lost his father. The cabinetmaker's union paid for Knut Hoffer's funeral and gave Hoffer about $300 insurance money. He took a bus to Los Angeles and spent the next 10 years wandering, as he remembered, “up and down the land, dodging hunger and grieving over the world.” Hoffer eventually landed on Skid Row, reading, occasionally writing, and working at odd jobs.

In 1931, he considered suicide by drinking a solution of oxalic acid, but he could not bring himself to do it. He left Skid Row and became a migrant worker, following the harvests in California. He acquired a library card where he worked, dividing his time "between the books and the brothels." He also prospected for gold in the mountains. Snowed in for the winter, he read the "Essays" by Michel de Montaigne. Montaigne impressed Hoffer deeply, and Hoffer often made reference to him. He also developed a respect for America's underclass, which he said was "lumpy with talent."

He wrote a novel, "Four Years in Young Hank's Life," and a novella, "Chance and Mr. Kunze," both partly autobiographical. He also penned a long article based on his experiences in a federal work camp, "Tramps and Pioneers." It was never published, but a truncated version appeared in "Harper's Magazine" after he became well known.

Hoffer tried to enlist in the US Army at age 40 during World War II, but he was rejected due to a hernia. Instead, he began work as a longshoreman on the docks of San Francisco in 1943. At the same time, he began to write seriously.

Hoffer left the docks in 1964, and shortly after became an adjunct professor at the University of California, Berkeley. He later retired from public life in 1970. “I'm going to crawl back into my hole where I started,” he said. “I don't want to be a public person or anybody's spokesman... Any man can ride a train. Only a wise man knows when to get off.”In 1970, he endowed the Lili Fabilli and Eric Hoffer Laconic Essay Prize for students, faculty, and staff at the University of California, Berkeley.

Hoffer called himself an atheist but had sympathetic views of religion and described it as a positive force.

He died at his home in San Francisco in 1983 at the age of 80.

Hoffer was influenced by his modest roots and working-class surroundings, seeing in it vast human potential. In a letter to Margaret Anderson in 1941, he wrote:
He once remarked, "my writing grows out of my life just as a branch from a tree." When he was called an intellectual, he insisted that he simply was a longshoreman. Hoffer has been dubbed by some authors a "longshoreman philosopher."

Hoffer, who was an only child, never married. He fathered a child with Lili Fabilli Osborne, named Eric Osborne, who was born in 1955 and raised by Lili Osborne and her husband, Selden Osborne. Lili Fabilli Osborne had become acquainted with Hoffer through her husband, a fellow longshoreman and acquaintance of Hoffer's. Despite the affair and Lili Osborne later co-habitating with Hoffer, Selden Osborne and Hoffer remained on good terms.

Hoffer referred to Eric Osborne as his son or godson. Lili Fabilli Osborne died in 2010 at the age of 93. Prior to her death, Osborne was the executor of Hoffer's estate, and vigorously controlled the rights to his intellectual property.

In his 2012 book "Eric Hoffer: The Longshoreman Philosopher," journalist Tom Bethell revealed doubts about Hoffer's account of his early life. Although Hoffer claimed his parents were from Alsace-Lorraine, Hoffer himself spoke with a pronounced Bavarian accent. He claimed to have been born and raised in the Bronx but had no Bronx accent. His lover and executor Lili Fabilli stated that she always thought Hoffer was an immigrant. Her son, Eric Fabilli, said that Hoffer's life may have been comparable to that of B. Traven and considered hiring a genealogist to investigate Hoffer's early life, to which Hoffer reportedly replied, "Are you "sure" you want to know?" Pescadero land-owner Joe Gladstone, a family friend of the Fabilli's who also knew Hoffer, said of Hoffer's account of his early life: "I don't believe a word of it." To this day, no one ever has claimed to have known Hoffer in his youth, and no records apparently exist of his parents, nor indeed of Hoffer himself until he was about forty, when his name appeared in a census.

Hoffer came to public attention with the 1951 publication of his first book, "The True Believer: Thoughts on the Nature of Mass Movements", which consists of a preface and 125 sections, which are divided into 18 chapters. Hoffer analyzes the phenomenon of "mass movements," a general term that he applies to revolutionary parties, nationalistic movements, and religious movements. He summarizes his thesis in §113: "A movement is pioneered by men of words, materialized by fanatics and consolidated by men of actions."

Hoffer argues that fanatical and extremist cultural movements, whether religious, social, or national, arise when large numbers of frustrated people, believing their own individual lives to be worthless or spoiled, join a movement demanding radical change. But the real attraction for this population is an escape from the self, not a realization of individual hopes: "A mass movement attracts and holds a following not because it can satisfy the desire for self-advancement, but because it can satisfy the passion for self-renunciation."

Hoffer consequently argues that the appeal of mass movements is interchangeable: in the Germany of the 1920s and the 1930s, for example, the Communists and National Socialists were ostensibly enemies, but sometimes enlisted each other's members, since they competed for the same kind of marginalized, angry, frustrated people. For the "true believer," Hoffer argues that particular beliefs are less important than escaping from the burden of the autonomous self.

Harvard historian Arthur M. Schlesinger Jr. said of "The True Believer": "This brilliant and original inquiry into the nature of mass movements is a genuine contribution to our social thought."

Subsequent to the publication of "The True Believer" (1951), Eric Hoffer touched upon Asia and American interventionism in several of his essays. In "The Awakening of Asia" (1954), published in "The Reporter" and later his book "The Ordeal of Change" (1963), Hoffer discusses the reasons for unrest on the continent. In particular, he argues that the root cause of social discontent in Asia was not government corruption, "communist agitation," or the legacy of European colonial "oppression and exploitation," but rather that a "craving for pride" was the central problem in Asia, suggesting a problem that could not be relieved through typical American intervention.

For centuries, Hoffer notes, Asia had "submitted to one conqueror after another." Throughout these centuries, Asia had "been misruled, looted, and bled by both foreign and native oppressors without" so much as "a peep" from the general population. Though not without negative effect, corrupt governments and the legacy of European imperialism represented nothing new under the sun. Indeed, the European colonial authorities had been "fairly beneficent" in Asia.

To be sure, Communism exerted an appeal of sorts. For the Asian "pseudo-intellectual," it promised elite status and the phony complexities of "doctrinaire double talk." For the ordinary Asian, it promised partnership with the seemingly emergent Soviet Union in a "tremendous, unprecedented undertaking" to build a better tomorrow.

According to Hoffer, however, Communism in Asia was dwarfed by the desire for pride. To satisfy such desire, Asians would willingly and irrationally sacrifice their economic well-being and their lives as well.

Unintentionally, the West had created this appetite, causing "revolutionary unrest" in Asia. The West had done so by eroding the traditional communal bonds that once had woven the individual to the patriarchal family, clan, tribe, "cohesive rural or urban unit," and "religious or political body."

Without the security and spiritual meaning produced by such bonds, Asians had been liberated from tradition only to find themselves now atomized, isolated, exposed, and abandoned, "left orphaned and empty in a cold world."

Certainly, Europe had undergone a similar destruction of tradition, but it had occurred centuries earlier at the end of the medieval period and produced better results thanks to different circumstances.

For the Asians of the 1950s, the circumstances differed markedly. Most were illiterate and impoverished, living in a world that included no expansive physical or intellectual vistas. Dangerously, the "articulate minority" of the Asian population inevitably disconnected themselves from the ordinary people, thereby failing to acquire "a sense of usefulness and of worth" that came by "taking part in the world's work." As a result, they were "condemned to the life of chattering posturing pseudo-intellectuals" and coveted "the illusion of weight and importance."

Most significantly, Hoffer asserts that the disruptive awakening of Asia came about as a result of an unbearable sense of weakness. Indeed, Hoffer discusses the problem of weakness, asserting that while "power corrupts the few... weakness corrupts the many."

Hoffer notes that "the resentment of the weak does not spring from any injustice done them but from the sense of their inadequacy and impotence." In short, the weak "hate not wickedness" but themselves for being weak. Consequently, self-loathing produces explosive effects that cannot be mitigated through social engineering schemes, such as programs of wealth redistribution. In fact, American "generosity" is counterproductive, perceived in Asia simply as an example of Western "oppression."

In the wake of the Korean War, Hoffer does not recommend exporting at gunpoint either American political institutions or mass democracy. In fact, Hoffer advances the possibility that winning over the multitudes of Asia may not even be desirable. If on the other hand, necessity truly dictates that for "survival" the United States must persuade the "weak" of Asia to "our side," Hoffer suggests the wisest course of action would be to master "the art or technique of sharing hope, pride, and as a last resort, hatred with others."

During the Vietnam War, despite his objections to the antiwar movement and acceptance of the notion that the war was somehow necessary to prevent a third world war, Hoffer remained skeptical concerning American interventionism, specifically the intelligence with which the war was being conducted in Southeast Asia. After the United States became involved in the war, Hoffer wished to avoid defeat in Vietnam because of his fear that such a defeat would transform American society for ill, opening the door to those who would preach a stab-in-the-back myth and allow for the rise of an American version of Hitler.

In "The Temper of Our Time" (1967), Hoffer implies that the United States as a rule should avoid interventions in the first place: "the better part of statesmanship might be to know clearly and precisely what not to do, and leave action to the improvisation of chance." In fact, Hoffer indicates that "it might be wise to wait for enemies to defeat themselves," as they might fall upon each other with the United States out of the picture. The view was somewhat borne out with the Cambodian-Vietnamese War and Chinese-Vietnamese War of the late 1970s.

In May 1968, about a year after the Six-Day War, he wrote an article for the "Los Angeles Times" titled "Israel's Peculiar Position:"
Hoffer asks why "everyone expects the Jews to be the only real Christians in this world" and why Israel should sue for peace after its victory.

Hoffer believed that rapid change is not necessarily a positive thing for a society and that too rapid change can cause a regression in maturity for those who were brought up in a different society. He noted that in America in the 1960s, many young adults were still living in extended adolescence. Seeking to explain the attraction of the New Left protest movements, he characterized them as the result of widespread affluence, which "is robbing a modern society of whatever it has left of puberty rites to routinize the attainment of manhood." He saw the puberty rites as essential for self-esteem and noted that mass movements and juvenile mindsets tend to go together, to the point that anyone, no matter what age, who joins a mass movement immediately begins to exhibit juvenile behavior.

Hoffer further noted that working-class Americans rarely joined protest movements and subcultures since they had entry into meaningful labor as an effective rite of passage out of adolescence while both the very poor who lived on welfare and the affluent were, in his words, "prevented from having a share in the world's work, and of proving their manhood by doing a man's work and getting a man's pay" and thus remained in a state of extended adolescence. Lacking in necessary self-esteem, they were prone to joining mass movements as a form of compensation. Hoffer suggested that the need for meaningful work as a rite of passage into adulthood could be fulfilled with a two-year civilian national service program (like programs during the Great Depression such as the Civilian Conservation Corps): "The routinization of the passage from boyhood to manhood would contribute to the solution of many of our pressing problems. I cannot think of any other undertaking that would dovetail so many of our present difficulties into opportunities for growth."

Hoffer appeared on public television in 1964 and then in two one-hour conversations on CBS with Eric Sevareid in the late 1960s.

Hoffer's papers, including 131 of the notebooks he carried in his pockets, were acquired in 2000 by the Hoover Institution Archives. The papers fill of shelf space. Because Hoffer cultivated an aphoristic style, the unpublished notebooks (dated from 1949 to 1977) contain very significant work. Although available for scholarly study since at least 2003, little of their contents has been published. A selection of fifty aphorisms, focusing on the development of unrealized human talents through the creative process, appeared in the July 2005 issue of "Harper's Magazine".


Australian foreign minister Julie Bishop extensively referred to Hoffer's book "The True Believer" when in a 2015 speech she closely compared the psychological underpinnings of ISIS with that of Nazism.





</doc>
