<doc id="874" url="https://en.wikipedia.org/wiki?curid=874" title="Ancient Egypt">
Ancient Egypt

Ancient Egypt was a civilization of ancient North Africa, concentrated along the lower reaches of the Nile River, situated in the place that is now the country Egypt. Ancient Egyptian civilization followed prehistoric Egypt and coalesced around 3100BC (according to conventional Egyptian chronology) with the political unification of Upper and Lower Egypt under Menes (often identified with Narmer). The history of ancient Egypt occurred as a series of stable kingdoms, separated by periods of relative instability known as Intermediate Periods: the Old Kingdom of the Early Bronze Age, the Middle Kingdom of the Middle Bronze Age and the New Kingdom of the Late Bronze Age.

Egypt reached the pinnacle of its power in the New Kingdom, ruling much of Nubia and a sizable portion of the Near East, after which it entered a period of slow decline. During the course of its history Egypt was invaded or conquered by a number of foreign powers, including the Hyksos, the Libyans, the Nubians, the Assyrians, the Achaemenid Persians, and the Macedonians under the command of Alexander the Great. The Greek Ptolemaic Kingdom, formed in the aftermath of Alexander's death, ruled Egypt until 30BC, when, under Cleopatra, it fell to the Roman Empire and became a Roman province.

The success of ancient Egyptian civilization came partly from its ability to adapt to the conditions of the Nile River valley for agriculture. The predictable flooding and controlled irrigation of the fertile valley produced surplus crops, which supported a more dense population, and social development and culture. With resources to spare, the administration sponsored mineral exploitation of the valley and surrounding desert regions, the early development of an independent writing system, the organization of collective construction and agricultural projects, trade with surrounding regions, and a military intended to assert Egyptian dominance. Motivating and organizing these activities was a bureaucracy of elite scribes, religious leaders, and administrators under the control of a pharaoh, who ensured the cooperation and unity of the Egyptian people in the context of an elaborate system of religious beliefs.

The many achievements of the ancient Egyptians include the quarrying, surveying and construction techniques that supported the building of monumental pyramids, temples, and obelisks; a system of mathematics, a practical and effective system of medicine, irrigation systems and agricultural production techniques, the first known planked boats, Egyptian faience and glass technology, new forms of literature, and the earliest known peace treaty, made with the Hittites. Ancient Egypt has left a lasting legacy. Its art and architecture were widely copied, and its antiquities carried off to far corners of the world. Its monumental ruins have inspired the imaginations of travelers and writers for centuries. A newfound respect for antiquities and excavations in the early modern period by Europeans and Egyptians led to the scientific investigation of Egyptian civilization and a greater appreciation of its cultural legacy.

The Nile has been the lifeline of its region for much of human history. The fertile floodplain of the Nile gave humans the opportunity to develop a settled agricultural economy and a more sophisticated, centralized society that became a cornerstone in the history of human civilization. Nomadic modern human hunter-gatherers began living in the Nile valley through the end of the Middle Pleistocene some 120,000 years ago. By the late Paleolithic period, the arid climate of Northern Africa became increasingly hot and dry, forcing the populations of the area to concentrate along the river region.
In Predynastic and Early Dynastic times, the Egyptian climate was much less arid than it is today. Large regions of Egypt were covered in treed savanna and traversed by herds of grazing ungulates. Foliage and fauna were far more prolific in all environs and the Nile region supported large populations of waterfowl. Hunting would have been common for Egyptians, and this is also the period when many animals were first domesticated.

By about 5500 BC, small tribes living in the Nile valley had developed into a series of cultures demonstrating firm control of agriculture and animal husbandry, and identifiable by their pottery and personal items, such as combs, bracelets, and beads. The largest of these early cultures in upper (Southern) Egypt was the Badarian culture, which probably originated in the Western Desert; it was known for its high quality ceramics, stone tools, and its use of copper.
The Badari was followed by the Naqada culture: the Amratian (Naqada I), the Gerzeh (Naqada II), and Semainean (Naqada III). These brought a number of technological improvements. As early as the Naqada I Period, predynastic Egyptians imported obsidian from Ethiopia, used to shape blades and other objects from flakes. In Naqada II times, early evidence exists of contact with the Near East, particularly Canaan and the Byblos coast. Over a period of about 1,000 years, the Naqada culture developed from a few small farming communities into a powerful civilization whose leaders were in complete control of the people and resources of the Nile valley. Establishing a power center at Nekhen (in Greek, Hierakonpolis), and later at Abydos, Naqada III leaders expanded their control of Egypt northwards along the Nile. They also traded with Nubia to the south, the oases of the western desert to the west, and the cultures of the eastern Mediterranean and Near East to the east, initiating a period of Egypt-Mesopotamia relations.

The Naqada culture manufactured a diverse selection of material goods, reflective of the increasing power and wealth of the elite, as well as societal personal-use items, which included combs, small statuary, painted pottery, high quality decorative stone vases, cosmetic palettes, and jewelry made of gold, lapis, and ivory. They also developed a ceramic glaze known as faience, which was used well into the Roman Period to decorate cups, amulets, and figurines. During the last predynastic phase, the Naqada culture began using written symbols that eventually were developed into a full system of hieroglyphs for writing the ancient Egyptian language.

 The Early Dynastic Period was approximately contemporary to the early Sumerian-Akkadian civilisation of Mesopotamia and of ancient Elam. The third-centuryBC Egyptian priest Manetho grouped the long line of kings from Menes to his own time into 30 dynasties, a system still used today. He began his official history with the king named "Meni" (or "Menes" in Greek) who was believed to have united the two kingdoms of Upper and Lower Egypt.

The transition to a unified state happened more gradually than ancient Egyptian writers represented, and there is no contemporary record of Menes. Some scholars now believe, however, that the mythical Menes may have been the king Narmer, who is depicted wearing royal regalia on the ceremonial "Narmer Palette," in a symbolic act of unification. In the Early Dynastic Period, which began about 3000BC, the first of the Dynastic kings solidified control over lower Egypt by establishing a capital at Memphis, from which he could control the labour force and agriculture of the fertile delta region, as well as the lucrative and critical trade routes to the Levant. The increasing power and wealth of the kings during the early dynastic period was reflected in their elaborate mastaba tombs and mortuary cult structures at Abydos, which were used to celebrate the deified king after his death. The strong institution of kingship developed by the kings served to legitimize state control over the land, labour, and resources that were essential to the survival and growth of ancient Egyptian civilization.

Major advances in architecture, art, and technology were made during the Old Kingdom, fueled by the increased agricultural productivity and resulting population, made possible by a well-developed central administration. Some of ancient Egypt's crowning achievements, the Giza pyramids and Great Sphinx, were constructed during the Old Kingdom. Under the direction of the vizier, state officials collected taxes, coordinated irrigation projects to improve crop yield, drafted peasants to work on construction projects, and established a justice system to maintain peace and order. 

With the rising importance of central administration in Egypt a new class of educated scribes and officials arose who were granted estates by the king in payment for their services. Kings also made land grants to their mortuary cults and local temples, to ensure that these institutions had the resources to worship the king after his death. Scholars believe that five centuries of these practices slowly eroded the economic vitality of Egypt, and that the economy could no longer afford to support a large centralized administration. As the power of the kings diminished, regional governors called nomarchs began to challenge the supremacy of the office of king. This, coupled with severe droughts between 2200 and 2150BC, is believed to have caused the country to enter the 140-year period of famine and strife known as the First Intermediate Period.

After Egypt's central government collapsed at the end of the Old Kingdom, the administration could no longer support or stabilize the country's economy. Regional governors could not rely on the king for help in times of crisis, and the ensuing food shortages and political disputes escalated into famines and small-scale civil wars. Yet despite difficult problems, local leaders, owing no tribute to the king, used their new-found independence to establish a thriving culture in the provinces. Once in control of their own resources, the provinces became economically richer—which was demonstrated by larger and better burials among all social classes. In bursts of creativity, provincial artisans adopted and adapted cultural motifs formerly restricted to the royalty of the Old Kingdom, and scribes developed literary styles that expressed the optimism and originality of the period.

Free from their loyalties to the king, local rulers began competing with each other for territorial control and political power. By 2160BC, rulers in Herakleopolis controlled Lower Egypt in the north, while a rival clan based in Thebes, the Intef family, took control of Upper Egypt in the south. As the Intefs grew in power and expanded their control northward, a clash between the two rival dynasties became inevitable. Around 2055BC the northern Theban forces under Nebhepetre Mentuhotep II finally defeated the Herakleopolitan rulers, reuniting the Two Lands. They inaugurated a period of economic and cultural renaissance known as the Middle Kingdom.

The kings of the Middle Kingdom restored the country's stability and prosperity, thereby stimulating a resurgence of art, literature, and monumental building projects. Mentuhotep II and his Eleventh Dynasty successors ruled from Thebes, but the vizier Amenemhat I, upon assuming the kingship at the beginning of the Twelfth Dynasty around 1985BC, shifted the kingdom's capital to the city of Itjtawy, located in Faiyum. From Itjtawy, the kings of the Twelfth Dynasty undertook a far-sighted land reclamation and irrigation scheme to increase agricultural output in the region. Moreover, the military reconquered territory in Nubia that was rich in quarries and gold mines, while laborers built a defensive structure in the Eastern Delta, called the "Walls-of-the-Ruler", to defend against foreign attack.

With the kings having secured the country militarily and politically and with vast agricultural and mineral wealth at their disposal, the nation's population, arts, and religion flourished. In contrast to elitist Old Kingdom attitudes towards the gods, the Middle Kingdom displayed an increase in expressions of personal piety. Middle Kingdom literature featured sophisticated themes and characters written in a confident, eloquent style. The relief and portrait sculpture of the period captured subtle, individual details that reached new heights of technical sophistication.

The last great ruler of the Middle Kingdom, Amenemhat III, allowed Semitic-speaking Canaanite settlers from the Near East into the Delta region to provide a sufficient labour force for his especially active mining and building campaigns. These ambitious building and mining activities, however, combined with severe Nile floods later in his reign, strained the economy and precipitated the slow decline into the Second Intermediate Period during the later Thirteenth and Fourteenth dynasties. During this decline, the Canaanite settlers began to assume greater control of the Delta region, eventually coming to power in Egypt as the Hyksos.

Around 1785BC, as the power of the Middle Kingdom kings weakened, a Western Asian people called the Hyksos, who had already settled in the Delta, seized control of Egypt and established their capital at Avaris, forcing the former central government to retreat to Thebes. The king was treated as a vassal and expected to pay tribute. The Hyksos ("foreign rulers") retained Egyptian models of government and identified as kings, thereby integrating Egyptian elements into their culture. They and other invaders introduced new tools of warfare into Egypt, most notably the composite bow and the horse-drawn chariot.

After retreating south, the native Theban kings found themselves trapped between the Canaanite Hyksos ruling the north and the Hyksos' Nubian allies, the Kushites, to the south. After years of vassalage, Thebes gathered enough strength to challenge the Hyksos in a conflict that lasted more than 30 years, until 1555BC. The kings Seqenenre Tao II and Kamose were ultimately able to defeat the Nubians to the south of Egypt, but failed to defeat the Hyksos. That task fell to Kamose's successor, Ahmose I, who successfully waged a series of campaigns that permanently eradicated the Hyksos' presence in Egypt. He established a new dynasty and, in the New Kingdom that followed, the military became a central priority for the kings, who sought to expand Egypt's borders and attempted to gain mastery of the Near East.

The New Kingdom pharaohs established a period of unprecedented prosperity by securing their borders and strengthening diplomatic ties with their neighbours, including the Mitanni Empire, Assyria, and Canaan. Military campaigns waged under Tuthmosis I and his grandson Tuthmosis III extended the influence of the pharaohs to the largest empire Egypt had ever seen. Beginning with Merneptah the rulers of Egypt adopted the title of pharaoh.

Between their reigns, Hatshepsut, a queen who established herself as pharaoh, launched many building projects, including restoration of temples damaged by the Hyksos, and sent trading expeditions to Punt and the Sinai. When Tuthmosis III died in 1425BC, Egypt had an empire extending from Niya in north west Syria to the Fourth Cataract of the Nile in Nubia, cementing loyalties and opening access to critical imports such as bronze and wood.

The New Kingdom pharaohs began a large-scale building campaign to promote the god Amun, whose growing cult was based in Karnak. They also constructed monuments to glorify their own achievements, both real and imagined. The Karnak temple is the largest Egyptian temple ever built.

Around 1350BC, the stability of the New Kingdom was threatened when Amenhotep IV ascended the throne and instituted a series of radical and chaotic reforms. Changing his name to Akhenaten, he touted the previously obscure sun deity Aten as the supreme deity, suppressed the worship of most other deities, and moved the capital to the new city of Akhetaten (modern-day Amarna). He was devoted to his new religion and artistic style. After his death, the cult of the Aten was quickly abandoned and the traditional religious order restored. The subsequent pharaohs, Tutankhamun, Ay, and Horemheb, worked to erase all mention of Akhenaten's heresy, now known as the Amarna Period.
Around 1279BC, Ramesses II, also known as Ramesses the Great, ascended the throne, and went on to build more temples, erect more statues and obelisks, and sire more children than any other pharaoh in history. A bold military leader, Ramesses II led his army against the Hittites in the Battle of Kadesh (in modern Syria) and, after fighting to a stalemate, finally agreed to the first recorded peace treaty, around 1258BC.

Egypt's wealth, however, made it a tempting target for invasion, particularly by the Libyan Berbers to the west, and the Sea Peoples, a conjectured confederation of seafarers from the Aegean Sea. Initially, the military was able to repel these invasions, but Egypt eventually lost control of its remaining territories in southern Canaan, much of it falling to the Assyrians. The effects of external threats were exacerbated by internal problems such as corruption, tomb robbery, and civil unrest. After regaining their power, the high priests at the temple of Amun in Thebes accumulated vast tracts of land and wealth, and their expanded power splintered the country during the Third Intermediate Period.

Following the death of Ramesses XI in 1078BC, Smendes assumed authority over the northern part of Egypt, ruling from the city of Tanis. The south was effectively controlled by the High Priests of Amun at Thebes, who recognized Smendes in name only. During this time, Libyans had been settling in the western delta, and chieftains of these settlers began increasing their autonomy. Libyan princes took control of the delta under Shoshenq I in 945BC, founding the so-called Libyan or Bubastite dynasty that would rule for some 200 years. Shoshenq also gained control of southern Egypt by placing his family members in important priestly positions. Libyan control began to erode as a rival dynasty in the delta arose in Leontopolis, and Kushites threatened from the south. Around 727BC the Kushite king Piye invaded northward, seizing control of Thebes and eventually the Delta, which established the 25th dynasty. During the 25th dynasty, Pharaoh Taharqa created an empire nearly as large as the New Kingdom's. 25th dynasty Pharaohs built, or restored, temples and monuments throughout the Nile valley, including at Memphis, Karnak, Kawa, and Jebel Barkal. During this period, the Nile valley saw the first widespread construction of pyramids (many in modern Sudan) since the Middle Kingdom.
Egypt's far-reaching prestige declined considerably toward the end of the Third Intermediate Period. Its foreign allies had fallen under the Assyrian sphere of influence, and by 700BC war between the two states became inevitable. Between 671 and 667BC the Assyrians began the Assyrian conquest of Egypt. The reigns of both Taharqa and his successor, Tanutamun, were filled with constant conflict with the Assyrians, against whom Egypt enjoyed several victories. Ultimately, the Assyrians pushed the Kushites back into Nubia, occupied Memphis, and sacked the temples of Thebes.

The Assyrians left control of Egypt to a series of vassals who became known as the Saite kings of the Twenty-Sixth Dynasty. By 653BC, the Saite king Psamtik I was able to oust the Assyrians with the help of Greek mercenaries, who were recruited to form Egypt's first navy. Greek influence expanded greatly as the city-state of Naukratis became the home of Greeks in the Nile Delta. The Saite kings based in the new capital of Sais witnessed a brief but spirited resurgence in the economy and culture, but in 525BC, the powerful Persians, led by Cambyses II, began their conquest of Egypt, eventually capturing the pharaoh Psamtik III at the battle of Pelusium. Cambyses II then assumed the formal title of pharaoh, but ruled Egypt from Iran, leaving Egypt under the control of a satrapy. A few successful revolts against the Persians marked the 5th centuryBC, but Egypt was never able to permanently overthrow the Persians.

Following its annexation by Persia, Egypt was joined with Cyprus and Phoenicia in the sixth satrapy of the Achaemenid Persian Empire. This first period of Persian rule over Egypt, also known as the Twenty-Seventh dynasty, ended in 402BC, when Egypt regained independence under a series of native dynasties. The last of these dynasties, the Thirtieth, proved to be the last native royal house of ancient Egypt, ending with the kingship of Nectanebo II. A brief restoration of Persian rule, sometimes known as the Thirty-First Dynasty, began in 343BC, but shortly after, in 332BC, the Persian ruler Mazaces handed Egypt over to Alexander the Great without a fight.

In 332BC, Alexander the Great conquered Egypt with little resistance from the Persians and was welcomed by the Egyptians as a deliverer. The administration established by Alexander's successors, the Macedonian Ptolemaic Kingdom, was based on an Egyptian model and based in the new capital city of Alexandria. The city showcased the power and prestige of Hellenistic rule, and became a seat of learning and culture, centered at the famous Library of Alexandria. The Lighthouse of Alexandria lit the way for the many ships that kept trade flowing through the city—as the Ptolemies made commerce and revenue-generating enterprises, such as papyrus manufacturing, their top priority.

Hellenistic culture did not supplant native Egyptian culture, as the Ptolemies supported time-honored traditions in an effort to secure the loyalty of the populace. They built new temples in Egyptian style, supported traditional cults, and portrayed themselves as pharaohs. Some traditions merged, as Greek and Egyptian gods were syncretized into composite deities, such as Serapis, and classical Greek forms of sculpture influenced traditional Egyptian motifs. Despite their efforts to appease the Egyptians, the Ptolemies were challenged by native rebellion, bitter family rivalries, and the powerful mob of Alexandria that formed after the death of Ptolemy IV. In addition, as Rome relied more heavily on imports of grain from Egypt, the Romans took great interest in the political situation in the country. Continued Egyptian revolts, ambitious politicians, and powerful opponents from the Near East made this situation unstable, leading Rome to send forces to secure the country as a province of its empire.

Egypt became a province of the Roman Empire in 30BC, following the defeat of Marc Antony and Ptolemaic Queen Cleopatra VII by Octavian (later Emperor Augustus) in the Battle of Actium. The Romans relied heavily on grain shipments from Egypt, and the Roman army, under the control of a prefect appointed by the Emperor, quelled rebellions, strictly enforced the collection of heavy taxes, and prevented attacks by bandits, which had become a notorious problem during the period. Alexandria became an increasingly important center on the trade route with the orient, as exotic luxuries were in high demand in Rome.

Although the Romans had a more hostile attitude than the Greeks towards the Egyptians, some traditions such as mummification and worship of the traditional gods continued. The art of mummy portraiture flourished, and some Roman emperors had themselves depicted as pharaohs, though not to the extent that the Ptolemies had. The former lived outside Egypt and did not perform the ceremonial functions of Egyptian kingship. Local administration became Roman in style and closed to native Egyptians.

From the mid-first century AD, Christianity took root in Egypt and it was originally seen as another cult that could be accepted. However, it was an uncompromising religion that sought to win converts from Egyptian Religion and Greco-Roman religion and threatened popular religious traditions. This led to the persecution of converts to Christianity, culminating in the great purges of Diocletian starting in 303, but eventually Christianity won out. In 391 the Christian Emperor Theodosius introduced legislation that banned pagan rites and closed temples. Alexandria became the scene of great anti-pagan riots with public and private religious imagery destroyed. As a consequence, Egypt's native religious culture was continually in decline. While the native population continued to speak their language, the ability to read hieroglyphic writing slowly disappeared as the role of the Egyptian temple priests and priestesses diminished. The temples themselves were sometimes converted to churches or abandoned to the desert.

In the fourth century, as the Roman Empire divided, Egypt found itself in the Eastern Empire with its capital at Constantinople. In the waning years of the Empire, Egypt fell to the Sasanian Persian army (618–628), was recaptured by the Roman Emperor Heraclius (629–639), and then was finally captured by Muslim Rashidun army in 639–641, ending Roman rule.

The pharaoh was the absolute monarch of the country and, at least in theory, wielded complete control of the land and its resources. The king was the supreme military commander and head of the government, who relied on a bureaucracy of officials to manage his affairs. In charge of the administration was his second in command, the vizier, who acted as the king's representative and coordinated land surveys, the treasury, building projects, the legal system, and the archives. At a regional level, the country was divided into as many as 42 administrative regions called nomes each governed by a nomarch, who was accountable to the vizier for his jurisdiction. The temples formed the backbone of the economy. Not only were they houses of worship, but were also responsible for collecting and storing the kingdom's wealth in a system of granaries and treasuries administered by overseers, who redistributed grain and goods.

Much of the economy was centrally organized and strictly controlled. Although the ancient Egyptians did not use coinage until the Late period, they did use a type of money-barter system, with standard sacks of grain and the "deben", a weight of roughly of copper or silver, forming a common denominator. Workers were paid in grain; a simple laborer might earn 5 sacks (200 kg or 400 lb) of grain per month, while a foreman might earn 7 sacks (250 kg or 550 lb). Prices were fixed across the country and recorded in lists to facilitate trading; for example a shirt cost five copper deben, while a cow cost 140deben. Grain could be traded for other goods, according to the fixed price list. During the fifth centuryBC coined money was introduced into Egypt from abroad. At first the coins were used as standardized pieces of precious metal rather than true money, but in the following centuries international traders came to rely on coinage.

Egyptian society was highly stratified, and social status was expressly displayed. Farmers made up the bulk of the population, but agricultural produce was owned directly by the state, temple, or noble family that owned the land. Farmers were also subject to a labor tax and were required to work on irrigation or construction projects in a corvée system. Artists and craftsmen were of higher status than farmers, but they were also under state control, working in the shops attached to the temples and paid directly from the state treasury. Scribes and officials formed the upper class in ancient Egypt, known as the "white kilt class" in reference to the bleached linen garments that served as a mark of their rank. The upper class prominently displayed their social status in art and literature. Below the nobility were the priests, physicians, and engineers with specialized training in their field. It is unclear whether slavery as understood today existed in ancient Egypt, there is difference of opinions among authors.

The ancient Egyptians viewed men and women, including people from all social classes, as essentially equal under the law, and even the lowliest peasant was entitled to petition the vizier and his court for redress. Although slaves were mostly used as indentured servants, they were able to buy and sell their servitude, work their way to freedom or nobility, and were usually treated by doctors in the workplace. Both men and women had the right to own and sell property, make contracts, marry and divorce, receive inheritance, and pursue legal disputes in court. Married couples could own property jointly and protect themselves from divorce by agreeing to marriage contracts, which stipulated the financial obligations of the husband to his wife and children should the marriage end. Compared with their counterparts in ancient Greece, Rome, and even more modern places around the world, ancient Egyptian women had a greater range of personal choices, legal rights, and opportunities for achievement. Women such as Hatshepsut and Cleopatra VII even became pharaohs, while others wielded power as Divine Wives of Amun. Despite these freedoms, ancient Egyptian women did not often take part in official roles in the administration, aside from the royal high priestesses, apparently served only secondary roles in the temples (not much data for many dynasties), and were not so likely to be as educated as men. 

The head of the legal system was officially the pharaoh, who was responsible for enacting laws, delivering justice, and maintaining law and order, a concept the ancient Egyptians referred to as Ma'at. Although no legal codes from ancient Egypt survive, court documents show that Egyptian law was based on a common-sense view of right and wrong that emphasized reaching agreements and resolving conflicts rather than strictly adhering to a complicated set of statutes. Local councils of elders, known as "Kenbet" in the New Kingdom, were responsible for ruling in court cases involving small claims and minor disputes. More serious cases involving murder, major land transactions, and tomb robbery were referred to the "Great Kenbet", over which the vizier or pharaoh presided. Plaintiffs and defendants were expected to represent themselves and were required to swear an oath that they had told the truth. In some cases, the state took on both the role of prosecutor and judge, and it could torture the accused with beatings to obtain a confession and the names of any co-conspirators. Whether the charges were trivial or serious, court scribes documented the complaint, testimony, and verdict of the case for future reference.

Punishment for minor crimes involved either imposition of fines, beatings, facial mutilation, or exile, depending on the severity of the offense. Serious crimes such as murder and tomb robbery were punished by execution, carried out by decapitation, drowning, or impaling the criminal on a stake. Punishment could also be extended to the criminal's family. Beginning in the New Kingdom, oracles played a major role in the legal system, dispensing justice in both civil and criminal cases. The procedure was to ask the god a "yes" or "no" question concerning the right or wrong of an issue. The god, carried by a number of priests, rendered judgment by choosing one or the other, moving forward or backward, or pointing to one of the answers written on a piece of papyrus or an ostracon.

A combination of favorable geographical features contributed to the success of ancient Egyptian culture, the most important of which was the rich fertile soil resulting from annual inundations of the Nile River. The ancient Egyptians were thus able to produce an abundance of food, allowing the population to devote more time and resources to cultural, technological, and artistic pursuits. Land management was crucial in ancient Egypt because taxes were assessed based on the amount of land a person owned.

Farming in Egypt was dependent on the cycle of the Nile River. The Egyptians recognized three seasons: "Akhet" (flooding), "Peret" (planting), and "Shemu" (harvesting). The flooding season lasted from June to September, depositing on the river's banks a layer of mineral-rich silt ideal for growing crops. After the floodwaters had receded, the growing season lasted from October to February. Farmers plowed and planted seeds in the fields, which were irrigated with ditches and canals. Egypt received little rainfall, so farmers relied on the Nile to water their crops. From March to May, farmers used sickles to harvest their crops, which were then threshed with a flail to separate the straw from the grain. Winnowing removed the chaff from the grain, and the grain was then ground into flour, brewed to make beer, or stored for later use.

The ancient Egyptians cultivated emmer and barley, and several other cereal grains, all of which were used to make the two main food staples of bread and beer. Flax plants, uprooted before they started flowering, were grown for the fibers of their stems. These fibers were split along their length and spun into thread, which was used to weave sheets of linen and to make clothing. Papyrus growing on the banks of the Nile River was used to make paper. Vegetables and fruits were grown in garden plots, close to habitations and on higher ground, and had to be watered by hand. Vegetables included leeks, garlic, melons, squashes, pulses, lettuce, and other crops, in addition to grapes that were made into wine.

The Egyptians believed that a balanced relationship between people and animals was an essential element of the cosmic order; thus humans, animals and plants were believed to be members of a single whole. Animals, both domesticated and wild, were therefore a critical source of spirituality, companionship, and sustenance to the ancient Egyptians. Cattle were the most important livestock; the administration collected taxes on livestock in regular censuses, and the size of a herd reflected the prestige and importance of the estate or temple that owned them. In addition to cattle, the ancient Egyptians kept sheep, goats, and pigs. Poultry, such as ducks, geese, and pigeons, were captured in nets and bred on farms, where they were force-fed with dough to fatten them. The Nile provided a plentiful source of fish. Bees were also domesticated from at least the Old Kingdom, and provided both honey and wax.

The ancient Egyptians used donkeys and oxen as beasts of burden, and they were responsible for plowing the fields and trampling seed into the soil. The slaughter of a fattened ox was also a central part of an offering ritual. Horses were introduced by the Hyksos in the Second Intermediate Period. Camels, although known from the New Kingdom, were not used as beasts of burden until the Late Period. There is also evidence to suggest that elephants were briefly utilized in the Late Period but largely abandoned due to lack of grazing land. Dogs, cats, and monkeys were common family pets, while more exotic pets imported from the heart of Africa, such as Sub-Saharan African lions, were reserved for royalty. Herodotus observed that the Egyptians were the only people to keep their animals with them in their houses. During the Late Period, the worship of the gods in their animal form was extremely popular, such as the cat goddess Bastet and the ibis god Thoth, and these animals were kept in large numbers for the purpose of ritual sacrifice.

Egypt is rich in building and decorative stone, copper and lead ores, gold, and semiprecious stones. These natural resources allowed the ancient Egyptians to build monuments, sculpt statues, make tools, and fashion jewelry. Embalmers used salts from the Wadi Natrun for mummification, which also provided the gypsum needed to make plaster. Ore-bearing rock formations were found in distant, inhospitable wadis in the Eastern Desert and the Sinai, requiring large, state-controlled expeditions to obtain natural resources found there. There were extensive gold mines in Nubia, and one of the first maps known is of a gold mine in this region. The Wadi Hammamat was a notable source of granite, greywacke, and gold. Flint was the first mineral collected and used to make tools, and flint handaxes are the earliest pieces of evidence of habitation in the Nile valley. Nodules of the mineral were carefully flaked to make blades and arrowheads of moderate hardness and durability even after copper was adopted for this purpose. Ancient Egyptians were among the first to use minerals such as sulfur as cosmetic substances.

The Egyptians worked deposits of the lead ore galena at Gebel Rosas to make net sinkers, plumb bobs, and small figurines. Copper was the most important metal for toolmaking in ancient Egypt and was smelted in furnaces from malachite ore mined in the Sinai. Workers collected gold by washing the nuggets out of sediment in alluvial deposits, or by the more labor-intensive process of grinding and washing gold-bearing quartzite. Iron deposits found in upper Egypt were utilized in the Late Period. High-quality building stones were abundant in Egypt; the ancient Egyptians quarried limestone all along the Nile valley, granite from Aswan, and basalt and sandstone from the wadis of the Eastern Desert. Deposits of decorative stones such as porphyry, greywacke, alabaster, and carnelian dotted the Eastern Desert and were collected even before the First Dynasty. In the Ptolemaic and Roman Periods, miners worked deposits of emeralds in Wadi Sikait and amethyst in Wadi el-Hudi.

The ancient Egyptians engaged in trade with their foreign neighbors to obtain rare, exotic goods not found in Egypt. In the Predynastic Period, they established trade with Nubia to obtain gold and incense. They also established trade with Palestine, as evidenced by Palestinian-style oil jugs found in the burials of the First Dynasty pharaohs. An Egyptian colony stationed in southern Canaan dates to slightly before the First Dynasty. Narmer had Egyptian pottery produced in Canaan and exported back to Egypt.

By the Second Dynasty at latest, ancient Egyptian trade with Byblos yielded a critical source of quality timber not found in Egypt. By the Fifth Dynasty, trade with Punt provided gold, aromatic resins, ebony, ivory, and wild animals such as monkeys and baboons. Egypt relied on trade with Anatolia for essential quantities of tin as well as supplementary supplies of copper, both metals being necessary for the manufacture of bronze. The ancient Egyptians prized the blue stone lapis lazuli, which had to be imported from far-away Afghanistan. Egypt's Mediterranean trade partners also included Greece and Crete, which provided, among other goods, supplies of olive oil. In exchange for its luxury imports and raw materials, Egypt mainly exported grain, gold, linen, and papyrus, in addition to other finished goods including glass and stone objects.

The Egyptian language is a northern Afro-Asiatic language closely related to the Berber and Semitic languages. It has the second longest known history of any language (after Sumerian), having been written from c. 3200BC to the Middle Ages and remaining as a spoken language for longer. The phases of ancient Egyptian are Old Egyptian, Middle Egyptian (Classical Egyptian), Late Egyptian, Demotic and Coptic. Egyptian writings do not show dialect differences before Coptic, but it was probably spoken in regional dialects around Memphis and later Thebes.

Ancient Egyptian was a synthetic language, but it became more analytic later on. Late Egyptian developed prefixal definite and indefinite articles, which replaced the older inflectional suffixes. There was a change from the older verb–subject–object word order to subject–verb–object. The Egyptian hieroglyphic, hieratic, and demotic scripts were eventually replaced by the more phonetic Coptic alphabet. Coptic is still used in the liturgy of the Egyptian Orthodox Church, and traces of it are found in modern Egyptian Arabic.

Ancient Egyptian has 25 consonants similar to those of other Afro-Asiatic languages. These include pharyngeal and emphatic consonants, voiced and voiceless stops, voiceless fricatives and voiced and voiceless affricates. It has three long and three short vowels, which expanded in Late Egyptian to about nine. The basic word in Egyptian, similar to Semitic and Berber, is a triliteral or biliteral root of consonants and semiconsonants. Suffixes are added to form words. The verb conjugation corresponds to the person. For example, the triconsonantal skeleton is the semantic core of the word 'hear'; its basic conjugation is ', 'he hears'. If the subject is a noun, suffixes are not added to the verb: ', 'the woman hears'.

Adjectives are derived from nouns through a process that Egyptologists call "nisbation" because of its similarity with Arabic. The word order is in verbal and adjectival sentences, and in nominal and adverbial sentences. The subject can be moved to the beginning of sentences if it is long and is followed by a resumptive pronoun. Verbs and nouns are negated by the particle "n", but "nn" is used for adverbial and adjectival sentences. Stress falls on the ultimate or penultimate syllable, which can be open (CV) or closed (CVC).

Hieroglyphic writing dates from c. 3000BC, and is composed of hundreds of symbols. A hieroglyph can represent a word, a sound, or a silent determinative; and the same symbol can serve different purposes in different contexts. Hieroglyphs were a formal script, used on stone monuments and in tombs, that could be as detailed as individual works of art. In day-to-day writing, scribes used a cursive form of writing, called hieratic, which was quicker and easier. While formal hieroglyphs may be read in rows or columns in either direction (though typically written from right to left), hieratic was always written from right to left, usually in horizontal rows. A new form of writing, Demotic, became the prevalent writing style, and it is this form of writing—along with formal hieroglyphs—that accompany the Greek text on the Rosetta Stone.

Around the first century AD, the Coptic alphabet started to be used alongside the Demotic script. Coptic is a modified Greek alphabet with the addition of some Demotic signs. Although formal hieroglyphs were used in a ceremonial role until the fourth century, towards the end only a small handful of priests could still read them. As the traditional religious establishments were disbanded, knowledge of hieroglyphic writing was mostly lost. Attempts to decipher them date to the Byzantine and Islamic periods in Egypt, but only in the 1820s, after the discovery of the Rosetta Stone and years of research by Thomas Young and Jean-François Champollion, were hieroglyphs substantially deciphered.

Writing first appeared in association with kingship on labels and tags for items found in royal tombs. It was primarily an occupation of the scribes, who worked out of the "Per Ankh" institution or the House of Life. The latter comprised offices, libraries (called House of Books), laboratories and observatories. Some of the best-known pieces of ancient Egyptian literature, such as the Pyramid and Coffin Texts, were written in Classical Egyptian, which continued to be the language of writing until about 1300BC. Late Egyptian was spoken from the New Kingdom onward and is represented in Ramesside administrative documents, love poetry and tales, as well as in Demotic and Coptic texts. During this period, the tradition of writing had evolved into the tomb autobiography, such as those of Harkhuf and Weni. The genre known as "Sebayt" ("instructions") was developed to communicate teachings and guidance from famous nobles; the Ipuwer papyrus, a poem of lamentations describing natural disasters and social upheaval, is a famous example.

The Story of Sinuhe, written in Middle Egyptian, might be the classic of Egyptian literature. Also written at this time was the Westcar Papyrus, a set of stories told to Khufu by his sons relating the marvels performed by priests. The Instruction of Amenemope is considered a masterpiece of Near Eastern literature. Towards the end of the New Kingdom, the vernacular language was more often employed to write popular pieces like the Story of Wenamun and the Instruction of Any. The former tells the story of a noble who is robbed on his way to buy cedar from Lebanon and of his struggle to return to Egypt. From about 700BC, narrative stories and instructions, such as the popular Instructions of Onchsheshonqy, as well as personal and business documents were written in the demotic script and phase of Egyptian. Many stories written in demotic during the Greco-Roman period were set in previous historical eras, when Egypt was an independent nation ruled by great pharaohs such as Ramesses II.

Most ancient Egyptians were farmers tied to the land. Their dwellings were restricted to immediate family members, and were constructed of mud-brick designed to remain cool in the heat of the day. Each home had a kitchen with an open roof, which contained a grindstone for milling grain and a small oven for baking the bread. Walls were painted white and could be covered with dyed linen wall hangings. Floors were covered with reed mats, while wooden stools, beds raised from the floor and individual tables comprised the furniture.
The ancient Egyptians placed a great value on hygiene and appearance. Most bathed in the Nile and used a pasty soap made from animal fat and chalk. Men shaved their entire bodies for cleanliness; perfumes and aromatic ointments covered bad odors and soothed skin. Clothing was made from simple linen sheets that were bleached white, and both men and women of the upper classes wore wigs, jewelry, and cosmetics. Children went without clothing until maturity, at about age 12, and at this age males were circumcised and had their heads shaved. Mothers were responsible for taking care of the children, while the father provided the family's income.

Music and dance were popular entertainments for those who could afford them. Early instruments included flutes and harps, while instruments similar to trumpets, oboes, and pipes developed later and became popular. In the New Kingdom, the Egyptians played on bells, cymbals, tambourines, drums, and imported lutes and lyres from Asia. The sistrum was a rattle-like musical instrument that was especially important in religious ceremonies.

The ancient Egyptians enjoyed a variety of leisure activities, including games and music. Senet, a board game where pieces moved according to random chance, was particularly popular from the earliest times; another similar game was mehen, which had a circular gaming board. “Hounds and Jackals” also known as 58 holes is another example of board games played in ancient Egypt. The first complete set of this game was discovered from a Theban tomb of the Egyptian pharaoh Amenemhat IV that dates to the 13th Dynasty. Juggling and ball games were popular with children, and wrestling is also documented in a tomb at Beni Hasan. The wealthy members of ancient Egyptian society enjoyed hunting and boating as well.

The excavation of the workers' village of Deir el-Medina has resulted in one of the most thoroughly documented accounts of community life in the ancient world, which spans almost four hundred years. There is no comparable site in which the organization, social interactions, and working and living conditions of a community have been studied in such detail.

Egyptian cuisine remained remarkably stable over time; indeed, the cuisine of modern Egypt retains some striking similarities to the cuisine of the ancients. The staple diet consisted of bread and beer, supplemented with vegetables such as onions and garlic, and fruit such as dates and figs. Wine and meat were enjoyed by all on feast days while the upper classes indulged on a more regular basis. Fish, meat, and fowl could be salted or dried, and could be cooked in stews or roasted on a grill.

The architecture of ancient Egypt includes some of the most famous structures in the world: the Great Pyramids of Giza and the temples at Thebes. Building projects were organized and funded by the state for religious and commemorative purposes, but also to reinforce the wide-ranging power of the pharaoh. The ancient Egyptians were skilled builders; using only simple but effective tools and sighting instruments, architects could build large stone structures with great accuracy and precision that is still envied today.

The domestic dwellings of elite and ordinary Egyptians alike were constructed from perishable materials such as mud bricks and wood, and have not survived. Peasants lived in simple homes, while the palaces of the elite and the pharaoh were more elaborate structures. A few surviving New Kingdom palaces, such as those in Malkata and Amarna, show richly decorated walls and floors with scenes of people, birds, water pools, deities and geometric designs. Important structures such as temples and tombs that were intended to last forever were constructed of stone instead of mud bricks. The architectural elements used in the world's first large-scale stone building, Djoser's mortuary complex, include post and lintel supports in the papyrus and lotus motif.

The earliest preserved ancient Egyptian temples, such as those at Giza, consist of single, enclosed halls with roof slabs supported by columns. In the New Kingdom, architects added the pylon, the open courtyard, and the enclosed hypostyle hall to the front of the temple's sanctuary, a style that was standard until the Greco-Roman period. The earliest and most popular tomb architecture in the Old Kingdom was the mastaba, a flat-roofed rectangular structure of mudbrick or stone built over an underground burial chamber. The step pyramid of Djoser is a series of stone mastabas stacked on top of each other. Pyramids were built during the Old and Middle Kingdoms, but most later rulers abandoned them in favor of less conspicuous rock-cut tombs. The use of the pyramid form continued in private tomb chapels of the New Kingdom and in the royal pyramids of Nubia.

The ancient Egyptians produced art to serve functional purposes. For over 3500 years, artists adhered to artistic forms and iconography that were developed during the Old Kingdom, following a strict set of principles that resisted foreign influence and internal change. These artistic standards—simple lines, shapes, and flat areas of color combined with the characteristic flat projection of figures with no indication of spatial depth—created a sense of order and balance within a composition. Images and text were intimately interwoven on tomb and temple walls, coffins, stelae, and even statues. The Narmer Palette, for example, displays figures that can also be read as hieroglyphs. Because of the rigid rules that governed its highly stylized and symbolic appearance, ancient Egyptian art served its political and religious purposes with precision and clarity.

Ancient Egyptian artisans used stone as a medium for carving statues and fine reliefs, but used wood as a cheap and easily carved substitute. Paints were obtained from minerals such as iron ores (red and yellow ochres), copper ores (blue and green), soot or charcoal (black), and limestone (white). Paints could be mixed with gum arabic as a binder and pressed into cakes, which could be moistened with water when needed.

Pharaohs used reliefs to record victories in battle, royal decrees, and religious scenes. Common citizens had access to pieces of funerary art, such as shabti statues and books of the dead, which they believed would protect them in the afterlife. During the Middle Kingdom, wooden or clay models depicting scenes from everyday life became popular additions to the tomb. In an attempt to duplicate the activities of the living in the afterlife, these models show laborers, houses, boats, and even military formations that are scale representations of the ideal ancient Egyptian afterlife.

Despite the homogeneity of ancient Egyptian art, the styles of particular times and places sometimes reflected changing cultural or political attitudes. After the invasion of the Hyksos in the Second Intermediate Period, Minoan-style frescoes were found in Avaris. The most striking example of a politically driven change in artistic forms comes from the Amarna period, where figures were radically altered to conform to Akhenaten's revolutionary religious ideas. This style, known as Amarna art, was quickly abandoned after Akhenaten's death and replaced by the traditional forms.

Beliefs in the divine and in the afterlife were ingrained in ancient Egyptian civilization from its inception; pharaonic rule was based on the divine right of kings. The Egyptian pantheon was populated by gods who had supernatural powers and were called on for help or protection. However, the gods were not always viewed as benevolent, and Egyptians believed they had to be appeased with offerings and prayers. The structure of this pantheon changed continually as new deities were promoted in the hierarchy, but priests made no effort to organize the diverse and sometimes conflicting myths and stories into a coherent system. These various conceptions of divinity were not considered contradictory but rather layers in the multiple facets of reality.
Gods were worshiped in cult temples administered by priests acting on the king's behalf. At the center of the temple was the cult statue in a shrine. Temples were not places of public worship or congregation, and only on select feast days and celebrations was a shrine carrying the statue of the god brought out for public worship. Normally, the god's domain was sealed off from the outside world and was only accessible to temple officials. Common citizens could worship private statues in their homes, and amulets offered protection against the forces of chaos. After the New Kingdom, the pharaoh's role as a spiritual intermediary was de-emphasized as religious customs shifted to direct worship of the gods. As a result, priests developed a system of oracles to communicate the will of the gods directly to the people.

The Egyptians believed that every human being was composed of physical and spiritual parts or "aspects". In addition to the body, each person had a "šwt" (shadow), a "ba" (personality or soul), a "ka" (life-force), and a "name". The heart, rather than the brain, was considered the seat of thoughts and emotions. After death, the spiritual aspects were released from the body and could move at will, but they required the physical remains (or a substitute, such as a statue) as a permanent home. The ultimate goal of the deceased was to rejoin his "ka" and "ba" and become one of the "blessed dead", living on as an "akh", or "effective one". For this to happen, the deceased had to be judged worthy in a trial, in which the heart was weighed against a "feather of truth." If deemed worthy, the deceased could continue their existence on earth in spiritual form. If they were not deemed worthy, their heart was eaten by Ammit the Devourer and they were erased from the Universe.

The ancient Egyptians maintained an elaborate set of burial customs that they believed were necessary to ensure immortality after death. These customs involved preserving the body by mummification, performing burial ceremonies, and interring with the body goods the deceased would use in the afterlife. Before the Old Kingdom, bodies buried in desert pits were naturally preserved by desiccation. The arid, desert conditions were a boon throughout the history of ancient Egypt for burials of the poor, who could not afford the elaborate burial preparations available to the elite. Wealthier Egyptians began to bury their dead in stone tombs and use artificial mummification, which involved removing the internal organs, wrapping the body in linen, and burying it in a rectangular stone sarcophagus or wooden coffin. Beginning in the Fourth Dynasty, some parts were preserved separately in canopic jars.

By the New Kingdom, the ancient Egyptians had perfected the art of mummification; the best technique took 70 days and involved removing the internal organs, removing the brain through the nose, and desiccating the body in a mixture of salts called natron. The body was then wrapped in linen with protective amulets inserted between layers and placed in a decorated anthropoid coffin. Mummies of the Late Period were also placed in painted cartonnage mummy cases. Actual preservation practices declined during the Ptolemaic and Roman eras, while greater emphasis was placed on the outer appearance of the mummy, which was decorated.

Wealthy Egyptians were buried with larger quantities of luxury items, but all burials, regardless of social status, included goods for the deceased. Funerary texts were often included in the grave, and, beginning in the New Kingdom, so were shabti statues that were believed to perform manual labor for them in the afterlife. Rituals in which the deceased was magically re-animated accompanied burials. After burial, living relatives were expected to occasionally bring food to the tomb and recite prayers on behalf of the deceased.

The ancient Egyptian military was responsible for defending Egypt against foreign invasion, and for maintaining Egypt's domination in the ancient Near East. The military protected mining expeditions to the Sinai during the Old Kingdom and fought civil wars during the First and Second Intermediate Periods. The military was responsible for maintaining fortifications along important trade routes, such as those found at the city of Buhen on the way to Nubia. Forts also were constructed to serve as military bases, such as the fortress at Sile, which was a base of operations for expeditions to the Levant. In the New Kingdom, a series of pharaohs used the standing Egyptian army to attack and conquer Kush and parts of the Levant.

Typical military equipment included bows and arrows, spears, and round-topped shields made by stretching animal skin over a wooden frame. In the New Kingdom, the military began using chariots that had earlier been introduced by the Hyksos invaders. Weapons and armor continued to improve after the adoption of bronze: shields were now made from solid wood with a bronze buckle, spears were tipped with a bronze point, and the Khopesh was adopted from Asiatic soldiers. The pharaoh was usually depicted in art and literature riding at the head of the army; it has been suggested that at least a few pharaohs, such as Seqenenre Tao II and his sons, did do so. However, it has also been argued that "kings of this period did not personally act as frontline war leaders, fighting alongside their troops." Soldiers were recruited from the general population, but during, and especially after, the New Kingdom, mercenaries from Nubia, Kush, and Libya were hired to fight for Egypt.

In technology, medicine, and mathematics, ancient Egypt achieved a relatively high standard of productivity and sophistication. Traditional empiricism, as evidenced by the Edwin Smith and Ebers papyri (c. 1600BC), is first credited to Egypt. The Egyptians created their own alphabet and decimal system.

Even before the Old Kingdom, the ancient Egyptians had developed a glassy material known as faience, which they treated as a type of artificial semi-precious stone. Faience is a non-clay ceramic made of silica, small amounts of lime and soda, and a colorant, typically copper. The material was used to make beads, tiles, figurines, and small wares. Several methods can be used to create faience, but typically production involved application of the powdered materials in the form of a paste over a clay core, which was then fired. By a related technique, the ancient Egyptians produced a pigment known as Egyptian Blue, also called blue frit, which is produced by fusing (or sintering) silica, copper, lime, and an alkali such as natron. The product can be ground up and used as a pigment.

The ancient Egyptians could fabricate a wide variety of objects from glass with great skill, but it is not clear whether they developed the process independently. It is also unclear whether they made their own raw glass or merely imported pre-made ingots, which they melted and finished. However, they did have technical expertise in making objects, as well as adding trace elements to control the color of the finished glass. A range of colors could be produced, including yellow, red, green, blue, purple, and white, and the glass could be made either transparent or opaque.

The medical problems of the ancient Egyptians stemmed directly from their environment. Living and working close to the Nile brought hazards from malaria and debilitating schistosomiasis parasites, which caused liver and intestinal damage. Dangerous wildlife such as crocodiles and hippos were also a common threat. The lifelong labors of farming and building put stress on the spine and joints, and traumatic injuries from construction and warfare all took a significant toll on the body. The grit and sand from stone-ground flour abraded teeth, leaving them susceptible to abscesses (though caries were rare).

The diets of the wealthy were rich in sugars, which promoted periodontal disease. Despite the flattering physiques portrayed on tomb walls, the overweight mummies of many of the upper class show the effects of a life of overindulgence. Adult life expectancy was about 35 for men and 30 for women, but reaching adulthood was difficult as about one-third of the population died in infancy.

Ancient Egyptian physicians were renowned in the ancient Near East for their healing skills, and some, such as Imhotep, remained famous long after their deaths. Herodotus remarked that there was a high degree of specialization among Egyptian physicians, with some treating only the head or the stomach, while others were eye-doctors and dentists. Training of physicians took place at the "Per Ankh" or "House of Life" institution, most notably those headquartered in Per-Bastet during the New Kingdom and at Abydos and Saïs in the Late period. Medical papyri show empirical knowledge of anatomy, injuries, and practical treatments.

Wounds were treated by bandaging with raw meat, white linen, sutures, nets, pads, and swabs soaked with honey to prevent infection, while opium, thyme, and belladona were used to relieve pain. The earliest records of burn treatment describe burn dressings that use the milk from mothers of male babies. Prayers were made to the goddess Isis. Moldy bread, honey, and copper salts were also used to prevent infection from dirt in burns. Garlic and onions were used regularly to promote good health and were thought to relieve asthma symptoms. Ancient Egyptian surgeons stitched wounds, set broken bones, and amputated diseased limbs, but they recognized that some injuries were so serious that they could only make the patient comfortable until death occurred.

Early Egyptians knew how to assemble planks of wood into a ship hull and had mastered advanced forms of shipbuilding as early as 3000BC. The Archaeological Institute of America reports that the oldest planked ships known are the Abydos boats. A group of 14 discovered ships in Abydos were constructed of wooden planks "sewn" together. Discovered by Egyptologist David O'Connor of New York University, woven straps were found to have been used to lash the planks together, and reeds or grass stuffed between the planks helped to seal the seams. Because the ships are all buried together and near a mortuary belonging to Pharaoh Khasekhemwy, originally they were all thought to have belonged to him, but one of the 14 ships dates to 3000BC, and the associated pottery jars buried with the vessels also suggest earlier dating. The ship dating to 3000BC was long and is now thought to perhaps have belonged to an earlier pharaoh, perhaps one as early as Hor-Aha.

Early Egyptians also knew how to assemble planks of wood with treenails to fasten them together, using pitch for caulking the seams. The "Khufu ship", a vessel sealed into a pit in the Giza pyramid complex at the foot of the Great Pyramid of Giza in the Fourth Dynasty around 2500BC, is a full-size surviving example that may have filled the symbolic function of a solar barque. Early Egyptians also knew how to fasten the planks of this ship together with mortise and tenon joints.

Large seagoing ships are known to have been heavily used by the Egyptians in their trade with the city states of the eastern Mediterranean, especially Byblos (on the coast of modern-day Lebanon), and in several expeditions down the Red Sea to the Land of Punt. In fact one of the earliest Egyptian words for a seagoing ship is a "Byblos Ship", which originally defined a class of Egyptian seagoing ships used on the Byblos run; however, by the end of the Old Kingdom, the term had come to include large seagoing ships, whatever their destination.

In 2011 archaeologists from Italy, the United States, and Egypt excavating a dried-up lagoon known as Mersa Gawasis have unearthed traces of an ancient harbor that once launched early voyages like Hatshepsut's Punt expedition onto the open ocean. Some of the site's most evocative evidence for the ancient Egyptians' seafaring prowess include large ship timbers and hundreds of feet of ropes, made from papyrus, coiled in huge bundles. And in 2013 a team of Franco-Egyptian archaeologists discovered what is believed to be the world's oldest port, dating back about 4500 years, from the time of King Cheops on the Red Sea coast near Wadi el-Jarf (about 110 miles south of Suez).

In 1977, an ancient north–south canal dating to the Middle Kingdom of Egypt was discovered extending from Lake Timsah to the Ballah Lakes. It was dated to the Middle Kingdom of Egypt by extrapolating dates of ancient sites constructed along its course.

The earliest attested examples of mathematical calculations date to the predynastic Naqada period, and show a fully developed numeral system. The importance of mathematics to an educated Egyptian is suggested by a New Kingdom fictional letter in which the writer proposes a scholarly competition between himself and another scribe regarding everyday calculation tasks such as accounting of land, labor, and grain. Texts such as the Rhind Mathematical Papyrus and the Moscow Mathematical Papyrus show that the ancient Egyptians could perform the four basic mathematical operations—addition, subtraction, multiplication, and division—use fractions, calculate the areas of rectangles, triangles, and circles and compute the volumes of boxes, columns and pyramids. They understood basic concepts of algebra and geometry, and could solve simple sets of simultaneous equations.

Mathematical notation was decimal, and based on hieroglyphic signs for each power of ten up to one million. Each of these could be written as many times as necessary to add up to the desired number; so to write the number eighty or eight hundred, the symbol for ten or one hundred was written eight times respectively. Because their methods of calculation could not handle most fractions with a numerator greater than one, they had to write fractions as the sum of several fractions. For example, they resolved the fraction "two-fifths" into the sum of "one-third" + "one-fifteenth". Standard tables of values facilitated this. Some common fractions, however, were written with a special glyph—the equivalent of the modern two-thirds is shown on the right.

Ancient Egyptian mathematicians knew the Pythagorean theorem as an empirical formula. They were aware, for example, that a triangle had a right angle opposite the hypotenuse when its sides were in a 3–4–5 ratio. They were able to estimate the area of a circle by subtracting one-ninth from its diameter and squaring the result:

a reasonable approximation of the formula .

The golden ratio seems to be reflected in many Egyptian constructions, including the pyramids, but its use may have been an unintended consequence of the ancient Egyptian practice of combining the use of knotted ropes with an intuitive sense of proportion and harmony.

A team led by Johannes Krause managed the first reliable sequencing of the genomes of 90 mummified individuals in 2017 from northern Egypt (buried near modern-day Cairo), which constituted "the first reliable data set obtained from ancient Egyptians using high-throughput DNA sequencing methods." Whilst not conclusive, because of the non-exhaustive time frame and restricted location that the mummies represent, their study nevertheless showed that these ancient Egyptians "closely resembled ancient and modern Near Eastern populations, especially those in the Levant, and had almost no DNA from sub-Saharan Africa. What's more, the genetics of the mummies remained remarkably consistent even as different powers—including Nubians, Greeks, and Romans—conquered the empire." Later, however, something did alter the genomes of Egyptians. Some 15% to 20% of modern Egyptians' DNA reflects sub-Saharan ancestry, but the ancient mummies had only 6–15% sub-Saharan DNA. They called for additional research to be undertaken. Other genetic studies show much greater levels of sub-Saharan African ancestry in the current-day populations of southern as opposed to northern Egypt, and anticipate that mummies from southern Egypt would contain greater levels of sub-Saharan African ancestry than Lower Egyptian mummies.

The culture and monuments of ancient Egypt have left a lasting legacy on the world. The cult of the goddess Isis, for example, became popular in the Roman Empire, as obelisks and other relics were transported back to Rome. The Romans also imported building materials from Egypt to erect Egyptian-style structures. Early historians such as Herodotus, Strabo, and Diodorus Siculus studied and wrote about the land, which Romans came to view as a place of mystery.

During the Middle Ages and the Renaissance, Egyptian pagan culture was in decline after the rise of Christianity and later Islam, but interest in Egyptian antiquity continued in the writings of medieval scholars such as Dhul-Nun al-Misri and al-Maqrizi. In the seventeenth and eighteenth centuries, European travelers and tourists brought back antiquities and wrote stories of their journeys, leading to a wave of Egyptomania across Europe. This renewed interest sent collectors to Egypt, who took, purchased, or were given many important antiquities. Napoleon arranged the first studies in Egyptology when he brought some 150 scientists and artists to study and document Egypt's natural history, which was published in the "Description de l'Égypte".

In the 20th century, the Egyptian Government and archaeologists alike recognized the importance of cultural respect and integrity in excavations. The Supreme Council of Antiquities now approves and oversees all excavations, which are aimed at finding information rather than treasure. The council also supervises museums and monument reconstruction programs designed to preserve the historical legacy of Egypt.




</doc>
<doc id="875" url="https://en.wikipedia.org/wiki?curid=875" title="Analog Brothers">
Analog Brothers

Analog Brothers were an experimental hip hop band featuring Tracy 'Ice Oscillator' Marrow (Ice-T) on keyboards, drums and vocals, Keith 'Keith Korg' Thornton (Ultramagnetic MCs' Kool Keith) on bass, strings and vocals, Marc 'Mark Moog' Giveand (Raw Breed's Marc Live) on drums, violins and vocals, Christopher 'Silver Synth' Rodgers (Black Silver) on synthesizer, lazar bell and vocals, and Rex Colonel 'Rex Roland JX3P' Doby Jr. (Pimpin' Rex) on keyboards, vocals and production. Its album "Pimp to Eat" featured guest appearances by various members of Rhyme Syndicate, Odd Oberheim, Jacky Jasper (who appears as Jacky Jasper on the song "We Sleep Days" and H-Bomb on "War"), D.J. Cisco from S.M., Synth-A-Size Sisters and Teflon.

While the group only recorded one album together as the Analog Brothers, a few bootlegs of its live concert performances, including freestyles with original lyrics, have occasionally surfaced online. After "Pimp to Eat", the Analog Brothers continued performing together in various line ups. Kool Keith and Marc Live joined with Jacky Jasper to release two albums as KHM. Marc Live rapped with Ice T's group SMG. Marc also formed a group with Black Silver called Live Black, but while five of their tracks were released on a demo CD sold at concerts, Live Black's first album has yet to be released.

In 2008, Ice-T and Black Silver toured together as Black Ice, and released an album together called "Urban Legends".

In 2013 Black Silver and newest member to Analog Brothers, Kiew Kurzweil (Kiew Nikon of Kinetic) collaborated on the joint album called "Slang Banging (Return to Analog)" with production by Junkadelic Music. In addition to all this, the Analog Brothers continue to make frequent appearances on each other's solo albums.



</doc>
<doc id="876" url="https://en.wikipedia.org/wiki?curid=876" title="Motor neuron disease">
Motor neuron disease

Motor neuron diseases or motor neurone diseases (MNDs) are a group of rare neurodegenerative disorders that selectively affect motor neurons, the cells which control voluntary muscles of the body. They include amyotrophic lateral sclerosis (ALS), progressive bulbar palsy (PBP), pseudobulbar palsy, progressive muscular atrophy (PMA), primary lateral sclerosis (PLS), and monomelic amyotrophy (MMA), as well as some rarer variants resembling ALS.

Motor neuron diseases affect both children and adults. While each motor neuron disease affects patients differently, they all cause movement-related symptoms, mainly muscle weakness. Most of these diseases seem to occur randomly without known causes, but some forms are inherited. Studies into these inherited forms have led to discoveries of various genes (e.g. "SOD1") that are thought be important in understanding how the disease occurs.

Symptoms of motor neuron diseases can be first seen at birth or can come on slowly later in life. Most of these diseases worsen over time; while some, such as ALS, shorten one's life expectancy, others do not. Currently, there are no approved treatments for the majority of motor neuron disorders, and care is mostly symptomatic.

Signs and symptoms depend on the specific disease, but motor neuron diseases typically manifest as a group of movement-related symptoms. They come on slowly, and worsen over the course of more than three months. Various patterns of muscle weakness are seen, and muscle cramps and spasms may occur. One can have difficulty breathing with climbing stairs (exertion), difficulty breathing when lying down (orthopnea), or even respiratory failure if breathing muscles become involved. Bulbar symptoms, including difficulty speaking (dysarthria), difficulty swallowing (dysphagia), and excessive saliva production (sialorrhea), can also occur. Sensation, or the ability to feel, is typically not affected. Emotional disturbance (e.g. pseudobulbar affect) and cognitive and behavioural changes (e.g. problems in word fluency, decision-making, and memory) are also seen. There can be lower motor neuron findings (e.g. muscle wasting, muscle twitching), upper motor neuron findings (e.g. brisk reflexes, Babinski reflex, Hoffman's reflex, increased muscle tone), or both.

Motor neuron diseases are seen both in children and in adults. Those that affect children tend to be inherited or familial, and their symptoms are either present at birth or appear before learning to walk. Those that affect adults tend to appear after age 40. The clinical course depends on the specific disease, but most progress or worsen over the course of months. Some are fatal (e.g. ALS), while others are not (e.g. PLS).

Various patterns of muscle weakness occur in different motor neuron diseases. Weakness can be symmetric or asymmetric, and it can occur in body parts that are distal, proximal, or both... According to Statland et al., there are three main weakness patterns that are seen in motor neuron diseases, which are:


Motor neuron diseases are on a spectrum in terms of upper and lower motor neuron involvement. Some have just lower or upper motor neuron findings, while others have a mix of both. Lower motor neuron (LMN) findings include muscle atrophy and fasciculations, and upper motor neuron (UMN) findings include hyperreflexia, spasticity, muscle spasm, and abnormal reflexes.

Pure upper motor neuron diseases, or those with just UMN findings, include PLS.

Pure lower motor neuron diseases, or those with just LMN findings, include PMA.

Motor neuron diseases with both UMN and LMN findings include both familial and sporadic ALS.

Most cases are sporadic and their causes are usually not known. It is thought that environmental, toxic, viral, or genetic factors may be involved.

TARDBP (TAR DNA-binding protein 43), also referred to as TDP-43, is a critical component of the non-homologous end joining (NHEJ) enzymatic pathway that repairs DNA double-strand breaks in pluripotent stem cell-derived motor neurons. TDP-43 is rapidly recruited to double-strand breaks where it acts as a scaffold for the recruitment of the XRCC4-DNA ligase protein complex that then acts to repair double-strand breaks. About 95% of ALS patients have abnormalities in the nucleus-cytoplasmic localization in spinal motor neurons of TDP43. In TDP-43 depleted human neural stem cell-derived motor neurons, as well as in sporadic ALS patients’ spinal cord specimens there is significant double-strand break accumulation and reduced levels of NHEJ.

In adults, men are more commonly affected than women.

Differential diagnosis can be challenging due to the number of overlapping symptoms, shared between several motor neuron diseases. Frequently, the diagnosis is based on clinical findings (i.e. LMN vs. UMN signs and symptoms, patterns of weakness), family history of MND, and a variation of tests, many of which are used to rule out disease mimics, which can manifest with identical symptoms.

Please refer to individual articles for the diagnostic methods used in each individual motor neuron disease.

Motor neuron disease describes a collection of clinical disorders, characterized by progressive muscle weakness and the degeneration of the motor neuron on electrophysiological testing. As discussed above, the term "motor neuron disease" has varying meanings in different countries. Similarly, the literature inconsistently classifies which degenerative motor neuron disorders can be included under the umbrella term "motor neuron disease". The four main types of MND are marked (*) in the table below.

All types of MND can be differentiated by two defining characteristics:


Sporadic or acquired MNDs occur in patients with no family history of degenerative motor neuron disease. Inherited or genetic MNDs adhere to one of the following inheritance patterns: autosomal dominant, autosomal recessive, or X-linked. Some disorders, like ALS, can occur sporadically (85%) or can have a genetic cause (15%) with the same clinical symptoms and progression of disease.

UMNs are motor neurons that project from the cortex down to the brainstem or spinal cord. LMNs originate in the anterior horns of the spinal cord and synapse on peripheral muscles. Both motor neurons are necessary for the strong contraction of a muscle, but damage to an UMN can be distinguished from damage to a LMN by physical exam.


There are no known curative treatments for the majority of motor neuron disorders. Please refer to the articles on individual disorders for more details.

The table below lists life expectancy for patients who are diagnosed with MND. Please refer to individual articles for more detail.
In the United States, the term "motor neuron disease" is often used to denote amyotrophic lateral sclerosis (Lou Gehrig's disease), the most common disorder in the group. In the United Kingdom, the term is spelled "motor neurone disease" and is frequently used for the entire group, but can also refer specifically to ALS.

While MND refers to a specific subset of similar diseases, there are numerous other diseases of motor neurons that are referred to collectively as "motor neuron disorders", for instance the diseases belonging to the spinal muscular atrophies group. However, they are not classified as "motor neuron diseases" by the 11th edition of the International Statistical Classification of Diseases and Related Health Problems (ICD-11), which is the definition followed in this article.



</doc>
<doc id="877" url="https://en.wikipedia.org/wiki?curid=877" title="Abjad">
Abjad

An abjad () is a type of writing system in which (in contrast to true alphabets) each symbol or glyph stands for a consonant, in effect leaving it to readers to infer or otherwise supply an appropriate vowel. So-called impure abjads represent vowels with either optional diacritics, a limited number of distinct vowel glyphs, or both. The name "abjad" is based on the Arabic alphabet's first (in its original order) four letters — corresponding to a, b, j, d — to replace the more common terms "consonantary" and "consonantal alphabet", in describing the family of scripts classified as "West Semitic."

The name "abjad" (' ) is derived from pronouncing the first letters of the Arabic alphabet order, in its original order. The ordering (') of Arabic letters used to match that of the older Phoenician, Hebrew and Semitic proto-alphabets: specifically, aleph, bet, gimel, dalet.

According to the formulations of Peter T. Daniels, abjads differ from alphabets in that only consonants, not vowels, are represented among the basic graphemes. Abjads differ from abugidas, another category defined by Daniels, in that in abjads, the vowel sound is "implied" by phonology, and where vowel marks exist for the system, such as nikkud for Hebrew and ḥarakāt for Arabic, their use is optional and not the dominant (or literate) form. Abugidas mark all vowels (other than the "inherent" vowel) with a diacritic, a minor attachment to the letter, or a standalone glyph. Some abugidas use a special symbol to "suppress" the inherent vowel so that the consonant alone can be properly represented. In a syllabary, a grapheme denotes a complete syllable, that is, either a lone vowel sound or a combination of a vowel sound with one or more consonant sounds.

The antagonism of abjad versus alphabet, as it was formulated by Daniels, has been rejected by some other scholars because abjad is also used as a term not only for the Arabic numeral system but, which is most important in terms of historical grammatology, also as term for the alphabetic device (i.e. letter order) of ancient Northwest Semitic scripts in opposition to the 'south Arabian' order. This caused fatal effects on terminology in general and especially in (ancient) Semitic philology. Also, it suggests that consonantal alphabets, in opposition to, for instance, the Greek alphabet, were not yet true alphabets and not yet entirely complete, lacking something important to be a fully working script system. It has also been objected that, as a set of letters, an alphabet is not the mirror of what should be there in a language from a phonological point of view; rather, it is the data stock of what provides maximum efficiency with least effort from a semantic point of view.

The first abjad to gain widespread usage was the Phoenician abjad. Unlike other contemporary scripts, such as cuneiform and Egyptian hieroglyphs, the Phoenician script consisted of only a few dozen symbols. This made the script easy to learn, and seafaring Phoenician merchants took the script throughout the then-known world.

The Phoenician abjad was a radical simplification of phonetic writing, since hieroglyphics required the writer to pick a hieroglyph starting with the same sound that the writer wanted to write in order to write phonetically, much as "man'yōgana" (Chinese characters used solely for phonetic use) was used to represent Japanese phonetically before the invention of kana.

Phoenician gave rise to a number of new writing systems, including the widely used Aramaic abjad and the Greek alphabet. The Greek alphabet evolved into the modern western alphabets, such as Latin and Cyrillic, while Aramaic became the ancestor of many modern abjads and abugidas of Asia.

Impure abjads have characters for some vowels, optional vowel diacritics, or both. The term pure abjad refers to scripts entirely lacking in vowel indicators. However, most modern abjads, such as Arabic, Hebrew, Aramaic, and Pahlavi, are "impure" abjadsthat is, they also contain symbols for some of the vowel phonemes, although the said non-diacritic vowel letters are also used to write certain consonants, particularly approximants that sound similar to long vowels. A "pure" abjad is exemplified (perhaps) by very early forms of ancient Phoenician, though at some point (at least by the 9th century BC) it and most of the contemporary Semitic abjads had begun to overload a few of the consonant symbols with a secondary function as vowel markers, called "matres lectionis". This practice was at first rare and limited in scope but became increasingly common and more developed in later times.

In the 9th century BC the Greeks adapted the Phoenician script for use in their own language. The phonetic structure of the Greek language created too many ambiguities when vowels went unrepresented, so the script was modified. They did not need letters for the guttural sounds represented by "aleph", "he", "heth" or "ayin", so these symbols were assigned vocalic values. The letters "waw" and "yod" were also adapted into vowel signs; along with "he", these were already used as "matres lectionis" in Phoenician. The major innovation of Greek was to dedicate these symbols exclusively and unambiguously to vowel sounds that could be combined arbitrarily with consonants (as opposed to syllabaries such as Linear B which usually have vowel symbols but cannot combine them with consonants to form arbitrary syllables).

Abugidas developed along a slightly different route. The basic consonantal symbol was considered to have an inherent "a" vowel sound. Hooks or short lines attached to various parts of the basic letter modify the vowel. In this way, the South Arabian alphabet evolved into the Ge'ez alphabet between the 5th century BC and the 5th century AD. Similarly, the Brāhmī script developed around the 3rd century BC (from the Aramaic abjad, it has been hypothesized).

The other major family of abugidas, Canadian Aboriginal syllabics, was initially developed in the 1840s by missionary and linguist James Evans for the Cree and Ojibwe languages. Evans used features of Devanagari script and Pitman shorthand to create his initial abugida. Later in the 19th century, other missionaries adapted Evans' system to other Canadian aboriginal languages. Canadian syllabics differ from other abugidas in that the vowel is indicated by rotation of the consonantal symbol, with each vowel having a consistent orientation.

The abjad form of writing is well-adapted to the morphological structure of the Semitic languages it was developed to write. This is because words in Semitic languages are formed from a root consisting of (usually) three consonants, the vowels being used to indicate inflectional or derived forms. For instance, according to Classical Arabic and Modern Standard Arabic, from the Arabic root "Dh-B-Ḥ" (to slaughter) can be derived the forms ' (he slaughtered), ' (you (masculine singular) slaughtered), ' (he slaughters), and ' (slaughterhouse). In most cases, the absence of full glyphs for vowels makes the common root clearer, allowing readers to guess the meaning of unfamiliar words from familiar roots (especially in conjunction with context clues) and improving word recognition while reading for practiced readers.

By contrast, the Arabic and Hebrew scripts sometimes perform the role of true alphabets rather than abjads when used to write certain Indo-European languages, including Kurdish, Bosnian, and Yiddish.



The Science of Arabic Letters, Abjad and Geometry, by Jorge Lupin 


</doc>
<doc id="878" url="https://en.wikipedia.org/wiki?curid=878" title="Abugida">
Abugida

An abugida (from Ge'ez: አቡጊዳ "’abugida"), or alphasyllabary, is a segmental writing system in which consonant–vowel sequences are written as a unit; each unit is based on a consonant letter, and vowel notation is secondary. This contrasts with a full alphabet, in which vowels have status equal to consonants, and with an abjad, in which vowel marking is absent, partial, or optional (although in less formal contexts, all three types of script may be termed alphabets). The terms also contrast them with a syllabary, in which the symbols cannot be split into separate consonants and vowels.

Abugidas include the extensive Brahmic family of scripts of Tibet, South and Southeast Asia, Semitic Ethiopic scripts, and Canadian Aboriginal syllabics.

As is the case for syllabaries, the units of the writing system may consist of the representations both of syllables and of consonants. For scripts of the Brahmic family, the term "akshara" is used for the units.

"’Äbugida" is an Ethiopian name for the Ge‘ez script, taken from four letters of that script, "ä bu gi da", in much the same way that "abecedary" is derived from Latin "a be ce de", "abjad" is derived from the Arabic "a b j d", and "alphabet" is derived from the names of the two first letters in the Greek alphabet, "alpha" and "beta". "Abugida" as a term in linguistics was proposed by Peter T. Daniels in his 1990 typology of writing systems. As Daniels used the word, an abugida is in contrast with a syllabary, where letters with shared consonants or vowels show no particular resemblance to one another, and also with an alphabet proper, where independent letters are used to denote both consonants and vowels. The term "alphasyllabary" was suggested for the Indic scripts in 1997 by William Bright, following South Asian linguistic usage, to convey the idea that "they share features of both alphabet and syllabary."

Abugidas were long considered to be syllabaries, or intermediate between syllabaries and alphabets, and the term "syllabics" is retained in the name of Canadian Aboriginal Syllabics. Other terms that have been used include "neosyllabary" (Février 1959), "pseudo-alphabet" (Householder 1959), "semisyllabary" (Diringer 1968; a word that has other uses) and "syllabic alphabet" (Coulmas 1996; this term is also a synonym for syllabary).

The formal definitions given by Daniels and Bright for abugida and alphasyllabary differ; some writing systems are abugidas but not alphasyllabaries, and some are alphasyllabaries but not abugidas. An abugida is defined as "a type of writing system whose basic characters denote consonants followed by a particular vowel, and in which diacritics denote other vowels". (This 'particular vowel' is referred to as the "inherent" or "implicit" vowel, as opposed to the "explicit" vowels marked by the 'diacritics'.) An alphasyllabary is defined as "a type of writing system in which the vowels are denoted by subsidiary symbols not all of which occur in a linear order (with relation to the consonant symbols) that is congruent with their temporal order in speech". Bright did not require that an alphabet explicitly represent all vowels. ʼPhags-pa is an example of an abugida that is not an alphasyllabary, and modern Lao is an example of an alphasyllabary that is not an abugida, for its vowels are always explicit.

This description is expressed in terms of an abugida. Formally, an alphasyllabary that is not an abugida can be converted to an abugida by adding a purely formal vowel sound that is never used and declaring that to be the inherent vowel of the letters representing consonants. This may formally make the system ambiguous, but in 'practice' this is not a problem, for then the interpretation with the never used inherent vowel sound will always be a wrong interpretation. Note that the actual pronunciation may be complicated by interactions between the sounds apparently written just as the sounds of the letters in the English words "wan, gem" and "war" are affected by neighbouring letters.

The fundamental principles of an abugida apply to words made up of consonant-vowel (CV) syllables. The syllables are written as a linear sequences of the units of the script. Each syllable is either a letter that represents the sound of a consonant and the inherent vowel, or a letter with a modification to indicate the vowel, either by means of diacritics, or by changes in the form of the letter itself. If all modifications are by diacritics and all diacritics follow the direction of the writing of the letters, then the abugida is not an alphasyllabary.

However, most languages have words that are more complicated than a sequence of CV syllables, even ignoring tone.

The first complication is syllables that consist of just a vowel (V). This issue does not arise in some languages because every syllable starts with a consonant. This is common in Semitic languages and in languages of mainland SE Asia, and for such languages this issue need not arise. For some languages, a zero consonant letter is used as though every syllable began with a consonant. For other languages, each vowel has a separate letter that is used for each syllable consisting of just the vowel. These letters are known as "independent vowels", and are found in most Indic scripts. These letters may be quite different from the corresponding diacritics, which by contrast are known as "dependent vowels". As a result of the spread of writing systems, independent vowels may be used to represent syllables beginning with a glottal stop, even for non-initial syllables.

The next two complications are sequences of consonants before a vowel (CCV) and syllables ending in a consonant (CVC). The simplest solution, which is not always available, is to break with the principle of writing words as a sequence of syllables and use a unit representing just a consonant (C). This unit may be represented with:

In a true abugida, the lack of distinctive marking may result from the diachronic loss of the inherent vowel, e.g. by syncope and apocope in Hindi.

When not handled by decomposition into C + CV, CCV syllables are handled by combining the two consonants. In the Indic scripts, the earliest method was simply to arrange them vertically, but the two consonants may merge as a conjunct consonant letters, where two or more letters are graphically joined in a ligature, or otherwise change their shapes. Rarely, one of the consonants may be replaced by a gemination mark, e.g. the Gurmukhi "". When they are arranged vertically, as in Burmese or Khmer, they are said to be 'stacked'. Often there has been a change to writing the two consonants side by side. In the latter case, the fact of combination may be indicated by a diacritic on one of the consonants or a change in the form of one of the consonants, e.g. the half forms of Devanagari. Generally, the reading order is top to bottom or the general reading order of the script, but sometimes the order is reversed.

The division of a word into syllables for the purposes of writing does not always accord with the natural phonetics of the language. For example, Brahmic scripts commonly handle a phonetic sequence CVC-CV as CV-CCV or CV-C-CV. However, sometimes phonetic CVC syllables are handled as single units, and the final consonant may be represented:


More complicated unit structures (e.g. CC or CCVC) are handled by combining the various techniques above.

There are three principal families of abugidas, depending on whether vowels are indicated by modifying consonants by "diacritics, distortion," or "orientation."

Tāna of the Maldives has dependent vowels and a zero vowel sign, but no inherent vowel.

Indic scripts originated in India and spread to Southeast Asia. All surviving Indic scripts are descendants of the Brahmi alphabet. Today they are used in most languages of South Asia (although replaced by Perso-Arabic in Urdu, Kashmiri and some other languages of Pakistan and India), mainland Southeast Asia (Myanmar, Thailand, Laos, and Cambodia), and Indonesian archipelago (Javanese, Balinese, Sundanese, etc.). The primary division is into North Indic scripts used in Northern India, Nepal, Tibet and Bhutan, and Southern Indic scripts used in South India, Sri Lanka and Southeast Asia.
South Indic letter forms are very rounded; North Indic less so, though Odia, Golmol and Litumol of Nepal script are rounded.
Most North Indic scripts' full letters incorporate a horizontal line at the top, with Gujarati and Odia as exceptions; South Indic scripts do not.

Indic scripts indicate vowels through dependent vowel signs (diacritics) around the consonants, often including a sign that explicitly indicates the lack of a vowel. If a consonant has no vowel sign, this indicates a default vowel. Vowel diacritics may appear above, below, to the left, to the right, or around the consonant.

The most widely used Indic script is Devanagari, shared by Hindi, Bhojpuri, Marathi, Konkani, Nepali, and often Sanskrit. A basic letter such as क in Hindi represents a syllable with the default vowel, in this case "ka" (). In some languages, including Hindi, it becomes a final closing consonant at the end of a word, in this case "k". The inherent vowel may be changed by adding vowel mark (diacritics), producing syllables such as कि "ki," कु "ku," के "ke," को "ko."

In many of the Brahmic scripts, a syllable beginning with a cluster is treated as a single character for purposes of vowel marking, so a vowel marker like ि "-i," falling before the character it modifies, may appear several positions before the place where it is pronounced. For example, the game cricket in Hindi is क्रिकेट "cricket;" the diacritic for appears before the consonant cluster , not before the . A more unusual example is seen in the Batak alphabet: Here the syllable "bim" is written "ba-ma-i-(virama)". That is, the vowel diacritic and virama are both written after the consonants for the whole syllable.

In many abugidas, there is also a diacritic to suppress the inherent vowel, yielding the bare consonant. In Devanagari, क् is "k," and ल् is "l". This is called the "virāma" or "halantam" in Sanskrit. It may be used to form consonant clusters, or to indicate that a consonant occurs at the end of a word. Thus in Sanskrit, a default vowel consonant such as क does not take on a final consonant sound. Instead, it keeps its vowel. For writing two consonants without a vowel in between, instead of using diacritics on the first consonant to remove its vowel, another popular method of special conjunct forms is used in which two or more consonant characters are merged to express a cluster, such as Devanagari: क्ल "kla." (Note that some fonts display this as क् followed by ल, rather than forming a conjunct. This expedient is used by ISCII and South Asian scripts of Unicode.) Thus a closed syllable such as "kal" requires two "aksharas" to write.

The Róng script used for the Lepcha language goes further than other Indic abugidas, in that a single "akshara" can represent a closed syllable: Not only the vowel, but any final consonant is indicated by a diacritic. For example, the syllable [sok] would be written as something like s̥̽, here with an underring representing and an overcross representing the diacritic for final . Most other Indic abugidas can only indicate a very limited set of final consonants with diacritics, such as or , if they can indicate any at all.

In Ethiopic (where the term "abugida" originates) the diacritics have been fused to the consonants to the point that they must be considered modifications of the form of the letters. Children learn each modification separately, as in a syllabary; nonetheless, the graphic similarities between syllables with the same consonant is readily apparent, unlike the case in a true syllabary.

Though now an abugida, the Ge'ez script, until the advent of Christianity ("ca." AD 350), had originally been what would now be termed an "abjad". In the Ge'ez abugida (or "fidel"), the base form of the letter (also known as "fidel") may be altered. For example, ሀ "hä" (base form), ሁ "hu" (with a right-side diacritic that doesn't alter the letter), ሂ "hi" (with a subdiacritic that compresses the consonant, so it is the same height), ህ "hə" or (where the letter is modified with a kink in the left arm).

In the family known as Canadian Aboriginal syllabics, which was inspired by the Devanagari script of India, vowels are indicated by changing the orientation of the syllabogram. Each vowel has a consistent orientation; for example, Inuktitut ᐱ "pi," ᐳ "pu," ᐸ "pa;" ᑎ "ti," ᑐ "tu," ᑕ "ta". Although there is a vowel inherent in each, all rotations have equal status and none can be identified as basic. Bare consonants are indicated either by separate diacritics, or by superscript versions of the "aksharas"; there is no vowel-killer mark.

Consonantal scripts ("abjads") are normally written without indication of many vowels. However, in some contexts like teaching materials or scriptures, Arabic and Hebrew are written with full indication of vowels via diacritic marks ("harakat", "niqqud") making them effectively alphasyllabaries. The Brahmic and Ethiopic families are thought to have originated from the Semitic abjads by the addition of vowel marks.
The Arabic scripts used for Kurdish in Iraq and for Uyghur in Xinjiang, China, as well as the Hebrew script of Yiddish, are fully vowelled, but because the vowels are written with full letters rather than diacritics (with the exception of distinguishing between /a/ and /o/ in the latter) and there are no inherent vowels, these are considered alphabets, not abugidas.

The imperial Mongol script called Phagspa was derived from the Tibetan abugida, but all vowels are written in-line rather than as diacritics. However, it retains the features of having an inherent vowel /a/ and having distinct initial vowel letters.

Pahawh Hmong is a non-segmental script that indicates syllable onsets and rimes, such as consonant clusters and vowels with final consonants. Thus it is not segmental and cannot be considered an abugida. However, it superficially resembles an abugida with the roles of consonant and vowel reversed. Most syllables are written with two letters in the order rime–onset (typically vowel-consonant), even though they are pronounced as onset-rime (consonant-vowel), rather like the position of the vowel in Devanagari, which is written before the consonant. Pahawh is also unusual in that, while an inherent rime (with mid tone) is unwritten, it also has an inherent onset . For the syllable , which requires one or the other of the inherent sounds to be overt, it is that is written. Thus it is the rime (vowel) that is basic to the system.

It is difficult to draw a dividing line between abugidas and other segmental scripts. For example, the Meroitic script of ancient Sudan did not indicate an inherent "a" (one symbol stood for both "m" and "ma," for example), and is thus similar to Brahmic family of abugidas. However, the other vowels were indicated with full letters, not diacritics or modification, so the system was essentially an alphabet that did not bother to write the most common vowel.

Several systems of shorthand use diacritics for vowels, but they do not have an inherent vowel, and are thus more similar to Thaana and Kurdish script than to the Brahmic scripts. The Gabelsberger shorthand system and its derivatives modify the "following" consonant to represent vowels. The Pollard script, which was based on shorthand, also uses diacritics for vowels; the placements of the vowel relative to the consonant indicates tone. Pitman shorthand uses straight strokes and quarter-circle marks in different orientations as the principal "alphabet" of consonants; vowels are shown as light and heavy dots, dashes and other marks in one of 3 possible positions to indicate the various vowel-sounds. However, to increase writing speed, Pitman has rules for "vowel indication" using the positioning or choice of consonant signs so that writing vowel-marks can be dispensed with.

As the term "alphasyllabary" suggests, abugidas have been considered an intermediate step between alphabets and syllabaries. Historically, abugidas appear to have evolved from abjads (vowelless alphabets). They contrast with syllabaries, where there is a distinct symbol for each syllable or consonant-vowel combination, and where these have no systematic similarity to each other, and typically develop directly from logographic scripts. Compare the examples above to sets of syllables in the Japanese hiragana syllabary: か "ka", き "ki", く "ku", け "ke", こ "ko" have nothing in common to indicate "k;" while ら "ra", り "ri", る "ru", れ "re", ろ "ro" have neither anything in common for "r", nor anything to indicate that they have the same vowels as the "k" set.

Most Indian and Indochinese abugidas appear to have first been developed from abjads with the Kharoṣṭhī and Brāhmī scripts; the abjad in question is usually considered to be the Aramaic one, but while the link between Aramaic and Kharosthi is more or less undisputed, this is not the case with Brahmi. The Kharosthi family does not survive today, but Brahmi's descendants include most of the modern scripts of South and Southeast Asia.

Ge'ez derived from a different abjad, the Sabean script of Yemen; the advent of vowels coincided with the introduction of Christianity about AD 350. 
The Ethiopic script is the elaboration of an abjad.

The Cree syllabary was invented with full knowledge of the Devanagari system.

The Meroitic script was developed from Egyptian hieroglyphs, within which various schemes of 'group writing' had been used for showing vowels.







</doc>
<doc id="880" url="https://en.wikipedia.org/wiki?curid=880" title="ABBA">
ABBA

ABBA (, ) is a Swedish pop supergroup formed in Stockholm in 1972 by Agnetha Fältskog, Björn Ulvaeus, Benny Andersson, and Anni-Frid Lyngstad. The group's name is an acronym of the first letters of their first names. They became one of the most commercially successful acts in the history of popular music, topping the charts worldwide from 1974 to 1983. ABBA won the Eurovision Song Contest 1974, giving Sweden its first triumph in the contest. They are the most successful group to have taken part in the competition.

During the band's main active years, it was composed of two married couples: Fältskog and Ulvaeus, and Lyngstad and Andersson. With the increase of their popularity, their personal lives suffered, which eventually resulted in the collapse of both marriages. The relationship changes were reflected in the group's music, with latter compositions featuring darker and more introspective lyrics. After ABBA disbanded, Andersson and Ulvaeus achieved success writing music for the stage, while Lyngstad and Fältskog pursued solo careers. Ten years after their disbanding, a compilation, "ABBA Gold" was released, which became a worldwide bestseller.

In 1999, ABBA's music was adapted into the successful musical "Mamma Mia!" that toured worldwide. A film of the same name, released in 2008, became the highest-grossing film in the United Kingdom that year. A sequel, "Mamma Mia! Here We Go Again", was released in 2018. That same year it was announced that the band had recorded two new songs after 35 years of being inactive.

Estimates of ABBA's total record sales are over 150 million, making them one of the best-selling music artists of all time. ABBA were the first group from a non-English-speaking country to achieve consistent success in the charts of English-speaking countries, including the United Kingdom, Ireland, Canada, Australia, New Zealand, South Africa and the United States. They had eight consecutive number-one albums in the UK. The group also enjoyed significant success in Latin America, and recorded a collection of their hit songs in Spanish. ABBA were honoured at the of the Eurovision Song Contest in 2005, when their hit "Waterloo" was chosen as the best song in the competition's history. The group was inducted into the Rock and Roll Hall of Fame in 2010. In 2015, their song "Dancing Queen" was inducted into the Recording Academy's Grammy Hall of Fame.

Benny Andersson (born 16 December 1946 in Stockholm, Sweden) became (at age 18) a member of a popular Swedish pop-rock group, the Hep Stars, that performed covers, amongst other things, of international hits. The Hep Stars were known as "the Swedish Beatles". They also set up Hep House, their equivalent of Apple Corps. Andersson played the keyboard and eventually started writing original songs for his band, many of which became major hits, including "No Response", which hit number three in 1965, and "Sunny Girl", "Wedding", and "Consolation", all of which hit number one in 1966. Andersson also had a fruitful songwriting collaboration with Lasse Berghagen, with whom he wrote his first Svensktoppen entry, "Sagan om lilla Sofie" ("The Story of Little Sophie") in 1968.

Björn Ulvaeus (born 25 April 1945 in Gothenburg, Sweden) also began his musical career at the age of 18 (as a singer and guitarist), when he fronted the Hootenanny Singers, a popular Swedish folk–skiffle group. Ulvaeus started writing English-language songs for his group, and even had a brief solo career alongside. The Hootenanny Singers and the Hep Stars sometimes crossed paths while touring. In June 1966, Ulvaeus and Andersson decided to write a song together. Their first attempt was "Isn't It Easy to Say", a song later recorded by the Hep Stars. Stig Anderson was the manager of the Hootenanny Singers and founder of the Polar Music label. He saw potential in the collaboration, and encouraged them to write more. The two also began playing occasionally with the other's bands on stage and on record, although it was not until 1969 that the pair wrote and produced some of their first real hits together: "Ljuva sextital" ("Sweet Sixties"), recorded by Brita Borg, and the Hep Stars' 1969 hit "Speleman" ("Fiddler").

Andersson wrote and submitted the song "Hej, Clown" for Melodifestivalen 1969, the national festival to select the Swedish entry to the Eurovision Song Contest. The song tied for first place, but re-voting relegated Andersson's song to second place. On that occasion Andersson briefly met his future spouse, singer Anni-Frid Lyngstad, who also participated in the contest. A month later, the two had become a couple. As their respective bands began to break up during 1969, Andersson and Ulvaeus teamed up and recorded their first album together in 1970, called "Lycka" ("Happiness"), which included original songs sung by both men. Their partners were often present in the recording studio, and sometimes added backing vocals; Fältskog even co-wrote a song with the two. Ulvaeus still occasionally recorded and performed with the Hootenanny Singers until the middle of 1974, and Andersson took part in producing their records.

Agnetha Fältskog (born 5 April 1950 in Jönköping, Sweden) sang with a local dance band headed by Bernt Enghardt who sent a demo recording of the band to Karl Gerhard Lundkvist. The demo tape featured a song written and sung by Agnetha: "Jag var så kär" ("I Was So in Love"). Lundkvist was so impressed with her voice that he was convinced she would be a star. After going through considerable effort to locate the singer, he arranged for Agnetha to come to Stockholm and to record two of her own songs. This led to Agnetha at the age of 18 having a number-one record in Sweden with a self-composed song, which later went on to sell over 80,000 copies. She was soon noticed by the critics and songwriters as a talented singer/songwriter of schlager style songs. Fältskog's main inspiration in her early years was singers such as Connie Francis. Along with her own compositions, she recorded covers of foreign hits and performed them on tours in Swedish folkparks. Most of her biggest hits were self-composed, which was quite unusual for a female singer in the 1960s. Agnetha released four solo LPs between 1968 and 1971. She had many successful singles in the Swedish charts.

During filming of a Swedish TV special in May 1969, Fältskog met Ulvaeus and they married on 6 July 1971. Fältskog and Ulvaeus eventually were involved in each other's recording sessions, and soon even Andersson and Lyngstad added backing vocals to Fältskog's third studio album, "Som jag är" ("As I Am") (1970). In 1972, Fältskog starred as Mary Magdalene in the original Swedish production of "Jesus Christ Superstar" and attracted favourable reviews. Between 1967 and 1975, Fältskog released five studio albums.

Anni-Frid "Frida" Lyngstad (born 15 November 1945 in Bjørkåsen in Ballangen, Norway) sang from the age of 13 with various dance bands, and worked mainly in a jazz-oriented cabaret style. She also formed her own band, the Anni-Frid Four. In the middle of 1967, she won a national talent competition with "En ledig dag" ("A Day Off") a Swedish version of the bossa nova song "A Day in Portofino", which is included in the EMI compilation "Frida 1967–1972". The first prize was a recording contract with EMI Sweden and to perform live on the most popular TV shows in the country. This TV performance, amongst many others, is included in the 3½-hour documentary "Frida – The DVD". Lyngstad released several schlager style singles on EMI without much success. When Benny Andersson started to produce her recordings in 1971, she had her first number-one single, "Min egen stad" ("My Own Town"), written by Benny and featuring all the future ABBA members on backing vocals. Lyngstad toured and performed regularly in the folkpark circuit and made appearances on radio and TV. She met Ulvaeus briefly in 1963 during a talent contest, and Fältskog during a TV show in early 1968.

Lyngstad linked up with her future bandmates in 1969. On 1 March 1969, she participated in the Melodifestival, where she met Andersson for the first time. A few weeks later they met again during a concert tour in southern Sweden and they soon became a couple. Andersson produced her single "Peter Pan" in September 1969—her first collaboration with Benny & Björn, as they had written the song. Andersson would then produce Lyngstad's debut studio album, "Frida", which was released in March 1971. Lyngstad also played in several revues and cabaret shows in Stockholm between 1969 and 1973. After ABBA formed, she recorded another successful album in 1975, "Frida ensam", which included a Swedish rendition of "Fernando", a hit on the Swedish radio charts before the English version was released.

An attempt at combining their talents occurred in April 1970 when the two couples went on holiday together to the island of Cyprus. What started as singing for fun on the beach ended up as an improvised live performance in front of the United Nations soldiers stationed on the island. Andersson and Ulvaeus were at this time recording their first album together, "Lycka", which was to be released in September 1970. Fältskog and Lyngstad added backing vocals on several tracks during June, and the idea of their working together saw them launch a stage act, "Festfolket" (which translates from Swedish to "Party People"), on 1 November 1970 in Gothenburg.

The cabaret show attracted generally negative reviews, except for the performance of the Andersson and Ulvaeus hit "Hej, gamle man" ("Hello, Old Man")–the first Björn and Benny recording to feature all four. They also performed solo numbers from respective albums, but the lukewarm reception convinced the foursome to shelve plans for working together for the time being, and each soon concentrated on individual projects again.

"Hej, gamle man", a song about an old Salvation Army soldier, became the quartet's first hit. The record was credited to Björn & Benny and reached number five on the sales charts and number one on Svensktoppen, staying on the latter chart (which was not a chart linked to sales or airplay) for 15 weeks.

It was during 1971 that the four artists began working together more, adding vocals to the others' recordings. Fältskog, Andersson and Ulvaeus toured together in May, while Lyngstad toured on her own. Frequent recording sessions brought the foursome closer together during the summer.

After the 1970 release of "Lycka", two more singles credited to "Björn & Benny" were released in Sweden, "Det kan ingen doktor hjälpa" ("No Doctor Can Help with That") and "Tänk om jorden vore ung" ("Imagine If Earth Was Young"), with more prominent vocals by Fältskog and Lyngstad–and moderate chart success.

Fältskog and Ulvaeus, now married, started performing together with Andersson on a regular basis at the Swedish folkparks in the middle of 1971.

Stig Anderson, founder and owner of Polar Music, was determined to break into the mainstream international market with music by Andersson and Ulvaeus. "One day the pair of you will write a song that becomes a worldwide hit," he predicted. Stig Anderson encouraged Ulvaeus and Andersson to write a song for Melodifestivalen, and after two rejected entries in 1971, Andersson and Ulvaeus submitted their new song "Säg det med en sång" ("Say It with a Song") for the 1972 contest, choosing newcomer Lena Anderson to perform. The song came in third place, encouraging Stig Anderson, and became a hit in Sweden.

The first signs of foreign success came as a surprise, as the Andersson and Ulvaeus single "She's My Kind of Girl" was released through Epic Records in Japan in March 1972, giving the duo a Top 10 hit. Two more singles were released in Japan, "En Carousel" ("En Karusell" in Scandinavia, an earlier version of "Merry-Go-Round") and "Love Has Its Ways" (a song they wrote with Kōichi Morita).

Ulvaeus and Andersson persevered with their songwriting and experimented with new sounds and vocal arrangements. "People Need Love" was released in June 1972, featuring guest vocals by the women, who were now given much greater prominence. Stig Anderson released it as a single, credited to "Björn & Benny, Agnetha & Anni-Frid". The song peaked at number 17 in the Swedish combined single and album charts, enough to convince them they were on to something. The single also became the first record to chart for the quartet in the United States, where it peaked at number 114 on the "Cashbox" singles chart and number 117 on the "Record World" singles chart. Labeled as "Björn & Benny (with Svenska Flicka)", it was released there through Playboy Records. However, according to Stig Anderson, "People Need Love" could have been a much bigger American hit, but a small label like Playboy Records did not have the distribution resources to meet the demand for the single from retailers and radio programmers.

In 1973, the band and their manager Stig Anderson decided to have another try at Melodifestivalen, this time with the song "Ring Ring". The studio sessions were handled by Michael B. Tretow, who experimented with a "wall of sound" production technique that became the wholly new sound. Stig Anderson arranged an English translation of the lyrics by Neil Sedaka and Phil Cody and they thought this would be a surefire winner. However, on 10 February 1973, the song came third in Melodifestivalen; thus it never reached the Eurovision Song Contest itself. Nevertheless, the group released their debut studio album, also called "Ring Ring". The album did well and the "Ring Ring" single was a hit in many parts of Europe and also in South Africa. However, Stig Anderson felt that the true breakthrough could only come with a UK or US hit.

When Agnetha Fältskog gave birth to her daughter Linda in 1973, she was replaced for a short period by Inger Brundin on a trip to West Germany.

In 1973, Stig Anderson, tired of unwieldy names, started to refer to the group privately and publicly as ABBA (a palindrome). At first, this was a play on words, as Abba is also the name of a well-known fish-canning company in Sweden, and itself an abbreviation. However, since the fish-canners were unknown outside Sweden, Anderson came to believe the name would work in international markets. A competition to find a suitable name for the group was held in a Gothenburg newspaper and it was officially announced in the summer that the group were to be known as "ABBA". The group negotiated with the canners for the rights to the name. Fred Bronson reported for "Billboard" that Fältskog told him in a 1988 interview that "[ABBA] had to ask permission and the factory said, 'O.K., as long as you don't make us feel ashamed for what you're doing. "ABBA" is an acronym formed from the first letters of each group member's first name: Agnetha, Björn, Benny, and Anni-Frid. The earliest known example of "ABBA" written on paper is on a recording session sheet from the Metronome Studio in Stockholm dated 16 October 1973. This was first written as "Björn, Benny, Agnetha & Frida", but was subsequently crossed out with "ABBA" written in large letters on top.

Their official logo, distinct with the backward 'B', was designed by Rune Söderqvist, who designed most of ABBA's record sleeves. The ambigram first appeared on the French compilation album, Golden Double Album, released in May 1976 by Disques Vogue, and would henceforth be used for all official releases.

The idea for the official logo was made by the German photographer Wolfgang "Bubi" Heilemann () on a velvet jumpsuit photo shoot for the teenage magazine Bravo. On the photo, the ABBA members held a giant initial letter of his/her name. After the pictures were made, Heilemann found out that Benny Andersson reversed his letter "B"; this prompted discussions about the mirrored "B", and the members of ABBA agreed on the mirrored letter. From 1976 onward, the first "B" in the logo version of the name was "mirror-image" reversed on the band's promotional material, thus becoming the group's registered trademark.

Following their acquisition of the group's catalogue, PolyGram began using variations of the ABBA logo, employing a different font. In 1992, Polygram added a crown emblem to it for the first release of the "ABBA Gold: Greatest Hits" compilation. After Universal Music purchased PolyGram (and, thus, ABBA's label Polar Music International), control of the group's catalogue returned to Stockholm. Since then, the original logo has been reinstated on all official products.

As the group entered the Melodifestivalen with "Ring Ring" but failed to qualify as the 1973 Swedish entry, Stig Anderson immediately started planning for the 1974 contest.

Ulvaeus, Andersson and Stig Anderson believed in the possibilities of using the Eurovision Song Contest as a way to make the music business aware of them as songwriters, as well as the band itself. In late 1973, they were invited by Swedish television to contribute a song for the Melodifestivalen 1974 and from a number of new songs, the upbeat song "Waterloo" was chosen; the group was now inspired by the growing glam rock scene in England.

ABBA won their nation's hearts on Swedish television on 9 February 1974, and with this third attempt were far more experienced and better prepared for the Eurovision Song Contest. Winning the 1974 Eurovision Song Contest on 6 April 1974 (and singing "Waterloo" in English instead of their native tongue) gave ABBA the chance to tour Europe and perform on major television shows; thus the band saw the "Waterloo" single chart in many European countries. Following their success at the Eurovision Song Contest, ABBA spent an evening of glory partying in the appropriately named first-floor Napoleon suite of The Grand Brighton Hotel.

"Waterloo" was ABBA's first number-one single in big markets such as the UK and West Germany. In the United States, the song peaked at number-six on the "Billboard" Hot 100 chart, paving the way for their first album and their first trip as a group there. Albeit a short promotional visit, it included their first performance on American television, "The Mike Douglas Show". The album "Waterloo" only peaked at number 145 on the "Billboard" 200 chart, but received unanimous high praise from the US critics: "Los Angeles Times" called it "a compelling and fascinating debut album that captures the spirit of mainstream pop quite effectively ... an immensely enjoyable and pleasant project", while "Creem" characterised it as "a perfect blend of exceptional, lovable compositions".

ABBA's follow-up single, "Honey, Honey", peaked at number 27 on the US "Billboard" Hot 100, and was a number-two hit in West Germany. However, in the United Kingdom, ABBA's British record label, Epic, decided to re-release a remixed version of "Ring Ring" instead of "Honey, Honey", and a cover version of the latter by Sweet Dreams peaked at number 10. Both records debuted on the UK chart within one week of each other. "Ring Ring" failed to reach the Top 30 in the United Kingdom, increasing growing speculation that the group was simply a Eurovision one-hit wonder.

In November 1974, ABBA embarked on their first European tour, playing dates in Denmark, West Germany and Austria. It was not as successful as the band had hoped, since most of the venues did not sell out. Due to a lack of demand, they were even forced to cancel a few shows, including a sole concert scheduled in Switzerland. The second leg of the tour, which took them through Scandinavia in January 1975, was very different. They played to full houses everywhere and finally got the reception they had aimed for. Live performances continued in the middle of 1975 when ABBA embarked on a fourteen open-air date tour of Sweden and Finland. Their Stockholm show at the Gröna Lund amusement park had an estimated audience of 19,200. Björn Ulvaeus later said that "If you look at the singles we released straight after Waterloo, we were trying to be more like The Sweet, a semi-glam rock group, which was stupid because we were always a pop group."

In late 1974, "So Long" was released as a single in the United Kingdom but it received no airplay from Radio 1 and failed to chart. In the middle of 1975, ABBA released "I Do, I Do, I Do, I Do, I Do", which again received little airplay on Radio 1 but managed to climb the charts, to number 38. Later that year, the release of their self-titled third studio album "ABBA" and single "SOS" brought back their chart presence in the UK, where the single hit number six and the album peaked at number 13. "SOS" also became ABBA's second number-one single in Germany and their third in Australia. Success was further solidified with "Mamma Mia" reaching number-one in the United Kingdom, Germany and Australia. In the United States, "SOS" peaked at number 10 on the Record World Top 100 singles chart and number 15 on the "Billboard" Hot 100 chart, picking up the BMI Award along the way as one of the most played songs on American radio in 1975.

The success of the group in the United States had until that time been limited to single releases. By early 1976, the group already had four Top 30 singles on the US charts, but the album market proved to be tough to crack. The eponymous "ABBA " album generated three American hits, but it only peaked at number 165 on the "Cashbox" album chart and number 174 on the "Billboard" 200 chart. Opinions were voiced, by "Creem" in particular, that in the US ABBA had endured "a very sloppy promotional campaign". Nevertheless, the group enjoyed warm reviews from the American press. "Cashbox" went as far as saying that "there is a recurrent thread of taste and artistry inherent in Abba's marketing, creativity and presentation that makes it almost embarrassing to critique their efforts", while "Creem" wrote: "SOS is surrounded on this LP by so many good tunes that the mind boggles."

In Australia, the airing of the music videos for "I Do, I Do, I Do, I Do, I Do" and "Mamma Mia" on the nationally broadcast TV pop show "Countdown" (which premiered in November 1974) saw the band rapidly gain enormous popularity, and "Countdown" become a key promoter of the group via their distinctive music videos. This started an immense interest for ABBA in Australia, resulting in both the single and album holding down the No. 1 positions on the charts for months.

In March 1976, the band released the compilation album "Greatest Hits". It became their first UK number-one album, and also took ABBA into the Top 50 on the US album charts for the first time, eventually selling more than a million copies there. Also included on "Greatest Hits" was a new single, "Fernando", which went to number-one in at least thirteen countries worldwide, including the United Kingdom, Germany and Australia, and the single went on to sell over 10 million copies worldwide. In Australia, "Fernando" occupied the top position for a then record breaking 14 weeks (and stayed in the chart for 40 weeks), and was the longest-running chart-topper there for over 40 years until it was overtaken by Ed Sheeran's "Shape of You" in May 2017. It still remains as one of the best-selling singles of all time in Australia. Also in 1976, the group received its first international prize, with "Fernando" being chosen as the "Best Studio Recording of 1975". In the United States, "Fernando" reached the Top 10 of the Cashbox Top 100 singles chart and number 13 on the "Billboard" Hot 100. It topped the "Billboard" Adult Contemporary chart, ABBA's first American number-one single on any chart. At the same time, Germany released a compilation named "The Very Best of ABBA", also becoming a number-one album there whereas the "Greatest Hits" compilation followed a few months later to number-two on the German charts, despite all similarities with "The Very Best" album.

The group's fourth studio album, "Arrival", a number-one best-seller in Europe and Australia, represented a new level of accomplishment in both songwriting and studio work, prompting rave reviews from more rock-oriented UK music weeklies such as "Melody Maker" and "New Musical Express", and mostly appreciative notices from US critics. Hit after hit flowed from "Arrival": "Money, Money, Money", another number-one in Germany and Australia, and "Knowing Me, Knowing You", ABBA's sixth consecutive German number-one as well as another UK number-one. The real sensation was "Dancing Queen", not only topping the charts in loyal markets the UK, Germany and Australia, but also reaching number-one in the United States. In South Africa, ABBA had astounding success with "Fernando", "Dancing Queen" and "Knowing Me, Knowing You" being among the top 20 best-selling singles for 1976–77. In 1977, "Arrival" was nominated for the inaugural BRIT Award in the category "Best International Album of the Year". By this time ABBA were popular in the United Kingdom, most of Western Europe, Australia and New Zealand. In "Frida – The DVD", Lyngstad explains how she and Fältskog developed as singers, as ABBA's recordings grew more complex over the years.

The band's popularity in the United States would remain on a comparatively smaller scale, and "Dancing Queen" became the only "Billboard" Hot 100 number-one single ABBA had there (they did, however, get three more singles to the number-one position on other "Billboard" charts, including "Billboard" Adult Contemporary and Hot Dance Club Play). Nevertheless, "Arrival" finally became a true breakthrough release for ABBA on the US album market where it peaked at number 20 on the "Billboard" 200 chart and was certified gold by RIAA.

In January 1977, ABBA embarked on their first major tour. The group's status had changed dramatically and they were clearly regarded as superstars. They opened their much anticipated tour in Oslo, Norway, on 28 January, and mounted a lavishly produced spectacle that included a few scenes from their self-written mini-operetta "The Girl with the Golden Hair". The concert attracted immense media attention from across Europe and Australia. They continued the tour through Western Europe, visiting Gothenburg, Copenhagen, Berlin, Cologne, Amsterdam, Antwerp, Essen, Hanover, and Hamburg and ending with shows in the United Kingdom in Manchester, Birmingham, Glasgow and two sold-out concerts at London's Royal Albert Hall. Tickets for these two shows were available only by mail application and it was later revealed that the box-office received 3.5 million requests for tickets, enough to fill the venue 580 times. Along with praise ("ABBA turn out to be amazingly successful at reproducing their records", wrote "Creem"), there were complaints that "ABBA performed slickly...but with a zero personality coming across from a total of 16 people on stage" ("Melody Maker"). One of the Royal Albert Hall concerts was filmed as a reference for the filming of the Australian tour for what became "", though it is not exactly known how much of the concert was filmed.
After the European leg of the tour, in March 1977, ABBA played 11 dates in Australia before a total of 160,000 people. The opening concert in Sydney at the Sydney Showground on 3 March to an audience of 20,000 was marred by torrential rain with Lyngstad slipping on the wet stage during the concert. However, all four members would later recall this concert as the most memorable of their career. Upon their arrival in Melbourne, a civic reception was held at the Melbourne Town Hall and ABBA appeared on the balcony to greet an enthusiastic crowd of 6,000. In Melbourne, the group gave three concerts at the Sidney Myer Music Bowl with 14,500 at each including the Australian Prime Minister Malcolm Fraser and his family. At the first Melbourne concert, an additional 16,000 people gathered outside the fenced-off area to listen to the concert. In Adelaide, the group performed one concert at West Lakes Football Stadium in front of 20,000 people, with another 10,000 listening outside. During the first of five concerts in Perth, there was a bomb scare with everyone having to evacuate the Entertainment Centre. The trip was accompanied by mass hysteria and unprecedented media attention ("Swedish ABBA stirs box-office in Down Under tour...and the media coverage of the quartet rivals that set to cover the upcoming Royal tour of Australia", wrote "Variety"), and is captured on film in "", directed by Lasse Hallström.

The Australian tour and its subsequent "ABBA: The Movie" produced some ABBA lore, as well. Fältskog's blonde good looks had long made her the band's "pin-up girl", a role she disdained. During the Australian tour, she performed in a skin-tight white jumpsuit, causing one Australian newspaper to use the headline "Agnetha's bottom tops dull show". When asked about this at a news conference, she replied: "Don't they have bottoms in Australia?"

In December 1977, ABBA followed up "Arrival" with the more ambitious fifth album "", released to coincide with the debut of "ABBA: The Movie". Although the album was less well received by UK reviewers, it did spawn more worldwide hits: "The Name of the Game" and "Take a Chance on Me", which both topped the UK charts, and peaked at number 12 and number three, respectively, on the "Billboard" Hot 100 chart in the US. Although "Take a Chance on Me" did not top the American charts, it proved to be ABBA's biggest hit single there, selling more copies than "Dancing Queen". "The Album" also included "Thank You for the Music", the B-side of "Eagle" in countries where the latter had been released as a single, and was belatedly released as an A-side single in the United Kingdom and Ireland in 1983. "Thank You for the Music" has become one of the best loved and best known ABBA songs without being released as a single during the group's lifetime.

By 1978, ABBA were one of the biggest bands in the world. They converted a vacant cinema into the Polar Music Studio, a state-of-the-art studio in Stockholm. The studio was used by several other bands; notably Genesis' "Duke" and Led Zeppelin's "In Through the Out Door" were recorded there. During May 1978, the group went to the United States for a promotional campaign, performing alongside Andy Gibb on Olivia Newton-John's TV show. Recording sessions for the single "Summer Night City" were an uphill struggle, but upon release the song became another hit for the group. The track would set the stage for ABBA's foray into disco with their next album.

On 9 January 1979, the group performed "Chiquitita" at the Music for UNICEF Concert held at the United Nations General Assembly to celebrate UNICEF's Year of the Child. ABBA donated the copyright of this worldwide hit to the UNICEF; see Music for UNICEF Concert. The single was released the following week, and reached number-one in ten countries.

In mid-January 1979, Ulvaeus and Fältskog announced they were getting divorced. The news caused interest from the media and led to speculation about the band's future. ABBA assured the press and their fan base they were continuing their work as a group and that the divorce would not affect them. Nonetheless, the media continued to confront them with this in interviews. To escape the media swirl and concentrate on their writing, Andersson and Ulvaeus secretly travelled to Compass Point Studios in Nassau, Bahamas, where for two weeks they prepared their next album's songs.

The group's sixth studio album, "Voulez-Vous", was released in April 1979, the title track of which was recorded at the famous Criteria Studios in Miami, Florida, with the assistance of recording engineer Tom Dowd amongst others. The album topped the charts across Europe and in Japan and Mexico, hit the Top 10 in Canada and Australia and the Top 20 in the United States. None of the singles from the album reached number one on the UK charts, but "Chiquitita", "Does Your Mother Know", "Angeleyes" (with "Voulez-Vous", released as a double A-side) and "I Have a Dream" were all UK Top 5 hits. In Canada, "I Have a Dream" became ABBA's second number one on the RPM Adult Contemporary chart (after "Fernando" hit the top previously). Also in 1979, the group released their second compilation album, "Greatest Hits Vol. 2", which featured a brand new track: "Gimme! Gimme! Gimme! (A Man After Midnight)", another number-three hit in both the UK and Germany. In Russia during the late 1970s, the group was paid in oil commodities because of an embargo on the ruble.

On 13 September 1979, ABBA began at Northlands Coliseum in Edmonton, Canada, with a full house of 14,000. "The voices of the band, Agnetha's high sauciness combined with round, rich lower tones of Anni-Frid, were excellent...Technically perfect, melodically correct and always in perfect pitch...The soft lower voice of Anni-Frid and the high, edgy vocals of Agnetha were stunning", raved "Edmonton Journal".

During the next four weeks they played a total of 17 sold-out dates, 13 in the United States and four in Canada. The last scheduled ABBA concert in the United States in Washington, D.C. was cancelled due to Fältskog's emotional distress suffered during the flight from New York to Boston, when the group's private plane was subjected to extreme weather conditions and was unable to land for an extended period. They appeared at the Boston Music Hall for the performance 90 minutes late. The tour ended with a show in Toronto, Canada at Maple Leaf Gardens before a capacity crowd of 18,000. "ABBA plays with surprising power and volume; but although they are loud, they're also clear, which does justice to the signature vocal sound...Anyone who's been waiting five years to see Abba will be well satisfied", wrote "Record World".

On 19 October 1979, the tour resumed in Western Europe where the band played 23 sold-out gigs, including six sold-out nights at London's Wembley Arena.

In March 1980, ABBA travelled to Japan where upon their arrival at Narita International Airport, they were besieged by thousands of fans. The group performed eleven concerts to full houses, including six shows at Tokyo's Budokan. This tour was the last "on the road" adventure of their career.
In July 1980, ABBA released the single "The Winner Takes It All", the group's eighth UK chart topper (and their first since 1978). The song is widely misunderstood as being written about Ulvaeus and Fältskog's marital tribulations; Ulvaeus wrote the lyrics, but has stated they were not about his own divorce; Fältskog has repeatedly stated she was not the loser in their divorce. In the United States, the single peaked at number-eight on the "Billboard" Hot 100 chart and became ABBA's second "Billboard" Adult Contemporary number-one. It was also re-recorded by Andersson and Ulvaeus with a slightly different backing track, by French chanteuse Mireille Mathieu at the end of 1980 – as "Bravo tu as gagné", with French lyrics by Alain Boublil. November the same year saw the release of ABBA's seventh album "Super Trouper", which reflected a certain change in ABBA's style with more prominent use of synthesizers and increasingly personal lyrics. It set a record for the most pre-orders ever received for a UK album after one million copies were ordered before release. The second single from the album, "Super Trouper", also hit number-one in the UK, becoming the group's ninth and final UK chart-topper. Another track from the album, "Lay All Your Love on Me", released in 1981 as a Twelve-inch single only in selected territories, managed to top the "Billboard" Hot Dance Club Play chart and peaked at number-seven on the UK singles chart becoming, at the time, the highest ever charting 12-inch release in UK chart history.

Also in 1980, ABBA recorded a compilation of Spanish-language versions of their hits called "Gracias Por La Música". This was released in Spanish-speaking countries as well as in Japan and Australia. The album became a major success, and along with the Spanish version of "Chiquitita", this signalled the group's breakthrough in Latin America. "ABBA Oro: Grandes Éxitos", the Spanish equivalent of "ABBA Gold: Greatest Hits", was released in 1999.

In January 1981, Ulvaeus married Lena Källersjö, and manager Stig Anderson celebrated his 50th birthday with a party. For this occasion, ABBA recorded the track "Hovas Vittne" (a pun on the Swedish name for Jehovah's Witness and Anderson's birthplace, Hova) as a tribute to him, and released it only on 200 red vinyl copies, to be distributed to the guests attending the party. This single has become a sought-after collectable. In mid-February 1981, Andersson and Lyngstad announced they were filing for divorce. Information surfaced that their marriage had been an uphill struggle for years, and Benny had already met another woman, Mona Nörklit, whom he married in November 1981.

Andersson and Ulvaeus had songwriting sessions in early 1981, and recording sessions began in mid-March. At the end of April, the group recorded a TV special, "Dick Cavett Meets ABBA" with the US talk show host Dick Cavett. "The Visitors", ABBA's eighth and final studio album, showed a songwriting maturity and depth of feeling distinctly lacking from their earlier recordings but still placing the band squarely in the pop genre, with catchy tunes and harmonies. Although not revealed at the time of its release, the album's title track, according to Ulvaeus, refers to the secret meetings held against the approval of totalitarian governments in Soviet-dominated states, while other tracks address topics like failed relationships, the threat of war, aging, and loss of innocence. The album's only major single release, "One of Us", proved to be the last of ABBA's nine number-one singles in Germany, this being in December 1981; and the swansong of their sixteen Top 5 singles on the South African chart. "One of Us" was also ABBA's final Top 3 hit in the UK, reaching number-three on the UK Singles Chart.

Although it topped the album charts across most of Europe, including Ireland, the UK and Germany, "The Visitors" was not as commercially successful as its predecessors, showing a commercial decline in previously loyal markets such as France, Australia and Japan. A track from the album, "When All Is Said and Done", was released as a single in North America, Australia and New Zealand, and fittingly became ABBA's final Top 40 hit in the US (debuting on the US charts on 31 December 1981), while also reaching the US Adult Contemporary Top 10, and number-four on the RPM Adult Contemporary chart in Canada. The song's lyrics, as with "The Winner Takes It All" and "One of Us", dealt with the painful experience of separating from a long-term partner, though it looked at the trauma more optimistically. With the now publicised story of Andersson and Lyngstad's divorce, speculation increased of tension within the band. Also released in the United States was the title track of "The Visitors", which hit the Top Ten on the "Billboard" Hot Dance Club Play chart.

In the spring of 1982, songwriting sessions had started and the group came together for more recordings. Plans were not completely clear, but a new album was discussed and the prospect of a small tour suggested. The recording sessions in May and June 1982 were a struggle, and only three songs were eventually recorded: "You Owe Me One", "I Am the City" and "Just Like That". Andersson and Ulvaeus were not satisfied with the outcome, so the tapes were shelved and the group took a break for the summer.

Back in the studio again in early August, the group had changed plans for the rest of the year: they settled for a Christmas release of a double album compilation of all their past single releases to be named "". New songwriting and recording sessions took place, and during October and December, they released the singles "The Day Before You Came"/"Cassandra" and "Under Attack"/"You Owe Me One", the A-sides of which were included on the compilation album. Neither single made the Top 20 in the United Kingdom, though "The Day Before You Came" became a Top 5 hit in many European countries such as Germany, the Netherlands and Belgium. The album went to number one in the UK and Belgium, Top 5 in the Netherlands and Germany and Top 20 in many other countries. "Under Attack", the group's final release before disbanding, was a Top 5 hit in the Netherlands and Belgium.

"I Am the City" and "Just Like That" were left unreleased on "The Singles: The First Ten Years" for possible inclusion on the next projected studio album, though this never came to fruition. "I Am the City" was eventually released on the compilation album "" in 1993, while "Just Like That" has been recycled in new songs with other artists produced by Andersson and Ulvaeus. A reworked version of the verses ended up in the musical "Chess". The chorus section of "Just Like That" was eventually released on a retrospective box set in 1994, as well as in the "ABBA Undeleted" medley featured on disc 9 of "The Complete Studio Recordings". Despite a number of requests from fans, Ulvaeus and Andersson are still refusing to release ABBA's version of "Just Like That" in its entirety, even though the complete version has surfaced on bootlegs.

The group travelled to London to promote "The Singles: The First Ten Years" in the first week of November 1982, appearing on "Saturday Superstore" and "The Late, Late Breakfast Show", and also to West Germany in the second week, to perform on Show Express. On 19 November 1982, ABBA appeared for the last time in Sweden on the TV programme Nöjesmaskinen, and on 11 December 1982, they made their last performance ever, transmitted to the UK on Noel Edmonds' "The Late, Late Breakfast Show", through a live link from a TV studio in Stockholm.

Andersson and Ulvaeus began collaborating with Tim Rice in early 1983 on writing songs for the musical project "Chess", while Fältskog and Lyngstad both concentrated on international solo careers. While Andersson and Ulvaeus were working on the musical, a further co-operation among the three of them came with the musical "Abbacadabra" that was produced in France for television. It was a children's musical utilising 14 ABBA songs. Alain and Daniel Boublil, who wrote "Les Misérables", had been in touch with Stig Anderson about the project, and the TV musical was aired over Christmas on French TV and later a Dutch version was also broadcast. Boublil previously also wrote the French lyric for Mireille Mathieu's version of "The Winner Takes It All".

Lyngstad, who had recently moved to Paris, participated in the French version, and recorded a single, "Belle", a duet with French singer Daniel Balavoine. The song was a cover of ABBA's 1976 instrumental track "Arrival". As the single "Belle" sold well in France, Cameron Mackintosh wanted to stage an English-language version of the show in London, with the French lyrics translated by David Wood and Don Black; Andersson and Ulvaeus got involved in the project, and contributed with one new song, "I Am the Seeker". "Abbacadabra" premiered on 8 December 1983 at the Lyric Hammersmith Theatre in London, to mixed reviews and full houses for eight weeks, closing on 21 January 1984. Lyngstad was also involved in this production, recording "Belle" in English as "Time", a duet with actor and singer B. A. Robertson: the single sold well, and was produced and recorded by Mike Batt. In May 1984, Lyngstad performed "I Have a Dream" with a children's choir at the United Nations Organisation Gala, in Geneva, Switzerland.

All four members made their (at the time, final) public appearance as four friends more than as ABBA in January 1986, when they recorded a video of themselves performing an acoustic version of "Tivedshambo" (which was the first song written by their manager Stig Anderson), for a Swedish TV show honouring Anderson on his 55th birthday. The four had not seen each other for more than two years. That same year they also performed privately at another friend's 40th birthday: their old tour manager, Claes af Geijerstam. They sang a self-written song titled "Der Kleine Franz" that was later to resurface in "Chess". Also in 1986, "ABBA Live" was released, featuring selections of live performances from the group's 1977 and 1979 tours. The four members were guests at the 50th birthday of Görel Hanser in 1999. Hanser was a long-time friend of all four, and also former secretary of Stig Anderson. Honouring Görel, ABBA performed a Swedish birthday song "Med En Enkel Tulipan" a cappella.

Andersson has on several occasions performed ABBA songs. In June 1992, he and Ulvaeus appeared with U2 at a Stockholm concert, singing the chorus of "Dancing Queen", and a few years later during the final performance of the B & B in Concert in Stockholm, Andersson joined the cast for an encore at the piano. Andersson frequently adds an ABBA song to the playlist when he performs with his BAO band. He also played the piano during new recordings of the ABBA songs "Like an Angel Passing Through My Room" with opera singer Anne Sofie von Otter, and "When All Is Said and Done" with Swede Viktoria Tolstoy. In 2002, Andersson and Ulvaeus both performed an a cappella rendition of the first verse of "Fernando" as they accepted their Ivor Novello award in London. Lyngstad performed and recorded an a cappella version of "Dancing Queen" with the Swedish group the Real Group in 1993, and also re-recorded "I Have a Dream" with Swiss singer Dan Daniell in 2003.

ABBA never officially announced the end of the group or an indefinite break, but it was long considered dissolved after their final public performance together in 1982. Their final public performance together as ABBA before their 2016 reunion was on the British TV programme "The Late, Late Breakfast Show" (live from Stockholm) on 11 December 1982. While reminiscing on "The Day Before You Came", Ulvaeus said: "we might have continued for a while longer if that had been a number one". In January 1983, Fältskog started recording sessions for a solo album, as Lyngstad had successfully released her album "Something's Going On" some months earlier. Ulvaeus and Andersson, meanwhile, started songwriting sessions for the musical "Chess". In interviews at the time, Björn and Benny denied the split of ABBA ("Who are we without our ladies? Initials of Brigitte Bardot?"), and Lyngstad and Fältskog kept claiming in interviews that ABBA would come together for a new album repeatedly during 1983 and 1984. Internal strife between the group and their manager escalated and the band members sold their shares in Polar Music during 1983. Except for a TV appearance in 1986, the foursome did not come together publicly again until they were reunited at the Swedish premiere of the "Mamma Mia!" movie on 4 July 2008. The individual members' endeavours shortly before and after their final public performance coupled with the collapse of both marriages and the lack of significant activity in the following few years after that widely suggested that the group had broken up.

In an interview with the "Sunday Telegraph", following the premiere, Ulvaeus and Andersson said that there was nothing that could entice them back on stage again. Ulvaeus said: "We will never appear on stage again. [...] There is simply no motivation to re-group. Money is not a factor and we would like people to remember us as we were. Young, exuberant, full of energy and ambition. I remember Robert Plant saying Led Zeppelin were a cover band now because they cover all their own stuff. I think that hit the nail on the head."
However, on 3 January 2011, Fältskog, long considered to be the most reclusive member of the group and a major obstacle to any reunion, raised the possibility of reuniting for a one-off engagement. She admitted that she has not yet brought the idea up to the other three members. In April 2013, she reiterated her hopes for reunion during an interview with "Die Zeit", stating: "If they ask me, I'll say yes."

In a May 2013 interview, Fältskog, aged 63 at the time, stated that an ABBA reunion would never occur: "I think we have to accept that it will not happen, because we are too old and each one of us has their own life. Too many years have gone by since we stopped, and there's really no meaning in putting us together again." Fältskog further explained that the band members remained on amicable terms: "It's always nice to see each other now and then and to talk a little and to be a little nostalgic." In an April 2014 interview, Fältskog, when asked about whether the band might reunite for a new recording said: "It's difficult to talk about this because then all the news stories will be: 'ABBA is going to record another song!' But as long as we can sing and play, then why not? I would love to, but it's up to Björn and Benny."

The same year the members of ABBA went their separate ways, the French production of a "tribute" show (a children's TV musical named "Abbacadabra" using 14 ABBA songs) spawned new interest in the group's music.

After receiving little attention during the mid-to-late-1980s, ABBA's music experienced a resurgence in the early 1990s due to the UK synth-pop duo Erasure, who released "Abba-esque", a four track extended play release featuring cover versions of ABBA songs which topped several European charts in 1992. As U2 arrived in Stockholm for a concert in June of that year, the band paid homage to ABBA by inviting Björn Ulvaeus and Benny Andersson to join them on stage for a rendition of "Dancing Queen", playing guitar and keyboards. September 1992 saw the release of "", a new compilation album. The single "Dancing Queen" received radio airplay in the UK in the middle of 1992 to promote the album. The song returned to the Top 20 of the UK singles chart in August that year, this time peaking at number 16. With sales of 30 million, "Gold" is the best-selling ABBA album, as well as one of the best-selling albums worldwide. With sales of 5.5 million copies it is the second-highest selling album of all time in the UK, after Queen's "Greatest Hits". The enormous interest in the "ABBA Gold: Greatest Hits" compilation saw the release of "" in 1993.

In 1994, two Australian cult films caught the attention of the world's media, both focusing on admiration for ABBA: "The Adventures of Priscilla, Queen of the Desert" and "Muriel's Wedding". The same year, "Thank You for the Music", a four-disc box set comprising all the group's hits and stand-out album tracks, was released with the involvement of all four members. "By the end of the twentieth century," American critic Chuck Klosterman wrote a decade later, "it was far more contrarian to hate ABBA than to love them."

ABBA were soon recognised and embraced by other acts: Evan Dando of the Lemonheads recorded a cover version of "Knowing Me, Knowing You"; Sinéad O'Connor and Boyzone's Stephen Gately have recorded "Chiquitita"; Tanita Tikaram, Blancmange and Steven Wilson paid tribute to "The Day Before You Came". Cliff Richard covered "Lay All Your Love on Me", while Dionne Warwick, Peter Cetera, and Celebrity Skin recorded their versions of "SOS". US alternative-rock musician Marshall Crenshaw has also been known to play a version of "Knowing Me, Knowing You" in concert appearances, while legendary English Latin pop songwriter Richard Daniel Roman has recognised ABBA as a major influence. Swedish metal guitarist Yngwie Malmsteen covered "Gimme! Gimme! Gimme! (A Man After Midnight)" with slightly altered lyrics.

Two different compilation albums of ABBA songs have been released. "ABBA: A Tribute" coincided with the 25th anniversary celebration and featured 17 songs, some of which were recorded especially for this release. Notable tracks include Go West's "One of Us", Army of Lovers "Hasta Mañana", Information Society's "Lay All Your Love on Me", Erasure's "Take a Chance on Me" (with MC Kinky), and Lyngstad's a cappella duet with the Real Group of "Dancing Queen". A second 12-track album was released in 1999, entitled "ABBAmania", with proceeds going to the Youth Music charity in England. It featured all new cover versions: notable tracks were by Madness ("Money, Money, Money"), Culture Club ("Voulez-Vous"), the Corrs ("The Winner Takes It All"), Steps ("Lay All Your Love on Me", "I Know Him So Well"), and a medley entitled "Thank ABBA for the Music" performed by several artists and as featured on the Brits Awards that same year.

In 1997, an ABBA tribute group was formed, the ABBA Teens, which was subsequently renamed the A-Teens to allow the group some independence. The group's first album, "The ABBA Generation", consisting solely of ABBA covers reimagined as 1990s pop songs, was a worldwide success and so were subsequent albums. The group disbanded in 2004 due to a gruelling schedule and intentions to go solo. In Sweden, the growing recognition of the legacy of Andersson and Ulvaeus resulted in the 1998 "B & B Concerts", a tribute concert (with Swedish singers who had worked with the songwriters through the years) showcasing not only their ABBA years, but hits both before and after ABBA. The concert was a success, and was ultimately released on CD. It later toured Scandinavia and even went to Beijing in the People's Republic of China for two concerts. In 2000, ABBA was reported to have turned down an offer of approximately one billion US dollars to do a reunion tour consisting of 100 concerts.

For the 2004 semi-final of the Eurovision Song Contest, staged in Istanbul 30 years after ABBA had won the contest in Brighton, all four members made cameo appearances in a special comedy video made for the interval act, entitled "Our Last Video Ever". Other well-known stars such as Rik Mayall, Cher and Iron Maiden's Eddie also made appearances in the video. It was not included in the official DVD release of the Eurovision Contest, but was issued as a separate DVD release, retitled "The Last Video" at the request of the former ABBA members. The video was made using puppet models of the members of the band. The video has surpassed 8 million views on YouTube as of November 2019.

In 2005, all four members of ABBA appeared at the Stockholm premiere of the musical "Mamma Mia!". On 22 October 2005, at the , "Waterloo" was chosen as the best song in the competition's history. On 4 July 2008, all four ABBA members were reunited at the Swedish premiere of the film "Mamma Mia!". It was only the second time all of them had appeared together in public since 1986. During the appearance, they re-emphasised that they intended never to officially reunite, citing the opinion of Robert Plant that the re-formed Led Zeppelin was more like a cover band of itself than the original band. Ulvaeus stated that he wanted the band to be remembered as they were during the peak years of their success.

The compilation album "", originally released in 1992, returned to number-one in the UK album charts for the fifth time on 3 August 2008. On 14 August 2008, the "Mamma Mia! The Movie" film soundtrack went to number-one on the US "Billboard" charts, ABBA's first US chart-topping album. During the band's heyday the highest album chart position they had ever achieved in America was number 14. In November 2008, all eight studio albums, together with a ninth of rare tracks, was released as "The Albums". It hit several charts, peaking at number-four in Sweden and reaching the Top 10 in several other European territories.

In 2008, Sony Computer Entertainment Europe, in collaboration with Universal Music Group Sweden AB, released "SingStar ABBA" on both the PlayStation 2 and PlayStation 3 games consoles, as part of the SingStar music video games. The PS2 version features 20 ABBA songs, while 25 songs feature on the PS3 version.

On 22 January 2009, Fältskog and Lyngstad appeared together on stage to receive the Swedish music award ""Rockbjörnen"" (for "lifetime achievement"). In an interview, the two women expressed their gratitude for the honorary award and thanked their fans. On 25 November 2009, PRS for Music announced that the British public voted ABBA as the band they would most like to see re-form. On 27 January 2010, ABBAWORLD, a 25-room touring exhibition featuring interactive and audiovisual activities, debuted at Earls Court Exhibition Centre in London. According to the exhibition's website, ABBAWORLD is "approved and fully supported" by the band members.

"Mamma Mia" was released as one of the first few non-premium song selections for the online RPG game "Bandmaster". On 17 May 2011, "Gimme! Gimme! Gimme!" was added as a non-premium song selection for the Bandmaster Philippines server. On 15 November 2011, Ubisoft released a dancing game called "" for the Wii. In January 2012, Universal Music announced the re-release of ABBA's final album "The Visitors", featuring a previously unheard track "From a Twinkling Star to a Passing Angel".

A book entitled "ABBA: The Official Photo Book" was published in early 2014 to mark the 40th anniversary of the band's Eurovision victory. The book reveals that part of the reason for the band's outrageous costumes was that Swedish tax laws at the time allowed the cost of garish outfits that were not suitable for daily wear to be tax deductible.

A sequel to the 2008 movie "Mamma Mia!", titled "Mamma Mia! Here We Go Again", was announced in May 2017; the film was released on 20 July 2018. Cher, who appeared in the movie, also released "Dancing Queen", an album full of ABBA covers, in September 2018.

In June 2017, a blue plaque outside Brighton Dome was set to commemorate their 1974 Eurovision win.

In May 2020, it was announced that ABBA's entire studio discography would be released on coloured vinyl for the first time, in a box set titled "ABBA: The Studio Albums." The initial release sold out in just a few hours.

On 20 January 2016, all four members of ABBA made a public appearance at "Mamma Mia! The Party" in Stockholm. On 6 June 2016, the quartet appeared together at a private party at Berns Salonger in Stockholm, which was held to celebrate the 50th anniversary of Andersson and Ulvaeus's first meeting. Fältskog and Lyngstad sang the ABBA song "The Way Old Friends Do" before they were joined on stage by Andersson and Ulvaeus.

British manager Simon Fuller announced in a statement in October 2016 that the group would be reuniting to work on a new 'digital entertainment experience'. The project would feature the members in their "life-like" avatar form ("'abbatars"') based on their late 1970s tour and would be set to launch by the spring of 2019. On 27 April 2018, the members announced that they had recorded two new songs, one entitled "I Still Have Faith in You", to feature in a TV special set to air later that year. The other new track is called "Don't Shut Me Down".

In September 2018, Ulvaeus revealed that the two new songs, "I Still Have Faith in You" and "Don't Shut Me Down", as well as the aforementioned TV special now called "", would be released no earlier than March 2019. In January 2019, Ulvaeus revealed that neither song was finished yet, hinting at a final mix date of spring 2019 and the possibility of a third song.

In June 2019, Ulvaeus announced that the first new song and video containing the Abbatars would be released in November 2019. In September, he stated in an interview that there were now five new ABBA songs to be released in 2020. In early 2020, Andersson confirmed that he was aiming for the songs to be released in September 2020.

In April 2020, Ulvaeus gave an interview saying that in the wake of the COVID-19 pandemic, the avatar project has been delayed by six months. As of 2020, five out of the eight original songs written by Benny for the new album have been recorded by the 2 female members, and there is a new music video with new unseen technology that cost £15 million, with the release for the music video being decided. In July 2020, Ulvaeus told podcaster Geoff Lloyd that the release of the new ABBA recordings had been delayed to 2021.

In October 1984, Ulvaeus and Andersson together with lyricist Tim Rice released the musical concept double-album "Chess". The singles "One Night in Bangkok" (with vocals by Murray Head and Anders Glenmark ) and "I Know Him So Well" (a duet by Barbara Dickson and Elaine Paige, and later also recorded by Barbra Streisand and Whitney Houston) were both hugely successful. The former reached number-one in Australia, Germany, Spain and Switzerland; number-two in Austria, France and New Zealand, and number-three in Canada, Norway, Sweden and the US. In May 1986, the musical premiered in London's West End, and ran for almost three years. "Chess" also opened on Broadway in April 1988, but closed within two months due to bad reviews. In Stockholm, the composers staged "Chess på svenska" ("Chess in Swedish") in 2003, with some new material, including the musical numbers ""Han är en man, han är ett barn"" ("He's a Man, He's a Child") and ""Glöm mig om du kan"" ("Forget Me If You Can"). In 2008, the musical was again revived for a successful staging at London's Royal Albert Hall which was subsequently released on DVD, and then in two successful separate touring productions in the United States and United Kingdom, in 2010.

Andersson and Ulvaeus' next project, "Kristina från Duvemåla", an epic Swedish musical, premiered in Malmö, in southern Sweden in October 1995. The musical ran for five years in Stockholm, and an English version has been in development for some considerable time. It has been reported that a Broadway production is in its earliest stages of pre-production. In the meantime, following some earlier workshops, a full presentation of the English translation of the musical in concert, now with the shortened name of ""Kristina"", took place to capacity crowds in September 2009 at New York's Carnegie Hall, and in April 2010 at London's Royal Albert Hall, followed by a CD release of the New York recordings.

Since 1983, besides "Chess" and "Kristina från Duvemåla", Andersson has continued writing songs with Ulvaeus. The pair produced two English-language pop albums with Swedish duo Gemini in 1985 and 1987. In 1987, Andersson also released his first solo album on his own label, Mono Music, called ""Klinga mina klockor"" ("Ring My Bells"), containing material inspired by Swedish folk music – and followed it with a second album titled "November 1989".

During the 1990s, Andersson wrote music for the popular Swedish cabaret quartet Ainbusk Singers, giving them two hits: "Lassie" and ""Älska mig"" ("Love me"), and later produced "Shapes", an English-language album by Josefin Nilsson with all-new material by Andersson and Ulvaeus. Andersson has also regularly written music for films (most notably to Roy Andersson's "Songs from the Second Floor"). In 2001, Andersson formed his own band, Benny Anderssons Orkester (BAO), which released three successful albums in 2001, 2004 and 2007. Andersson has the distinction of remaining the longest in the Swedish Radio Svensktoppen charts; the song ""Du är min man"" ("You Are My Man"), sung by Helen Sjöholm, spent 278 weeks there between 2004 and 2009. Andersson released his third album BAO 3 in October 2007, of new material with his band BAO and vocalists Helen Sjöholm and Tommy Körberg, as well as playing to full houses at two of Sweden's largest concert venues in October and November 2007, with an audience of 14,000.

Andersson and Ulvaeus have been highly involved in the worldwide productions of the musical "Mamma Mia!", alongside Lyngstad who attends premieres. They were also involved in the production of the successful film version of the musical, which opened in July 2008. Andersson produced the soundtrack utilising many of the musicians ABBA used on their albums and tours. Andersson made a cameo appearance in the movie as a "fisherman" piano player in the "Dancing Queen" scene, while Ulvaeus is seen as a Greek god playing a lyre during the closing credits.

Andersson and Ulvaeus have continuously been writing new material; most recently they wrote seven songs for Andersson's 2011 BAO album "O Klang Och Jubeltid", performed as usual by vocalists Sjöholm, Körberg and Moreus. In July 2009, BAO (now renamed the Benny Andersson Band) released their first international album, "The Story of a Heart". The album was a compilation of 14 tracks from Andersson's five Swedish-language releases between 1987 and 2007, including five songs now recorded with lyrics by Ulvaeus in English; the new title song premiered on BBC2's "Ken Bruce Show". A Swedish-language version of the title track, ""Sommaren Du Fick"" ("The Summer You Got"), was released as a single in Sweden prior to the English version, with vocals by Helen Sjöholm. In May 2009, Andersson released a single recorded by the staff at his privately owned Stockholm hotel "Hotel Rival", titled "2nd Best to None", accompanied by a video showing the staff at work.

In 2008, Andersson and Ulvaeus wrote a song for Swedish singer Sissela Kyle, titled ""Jag vill bli gammal"" ("I Wanna Grow Old"), for her Stockholm stage show ""Your Days Are Numbered"", which was never recorded and released, but was performed on television. Ulvaeus also contributed lyrics to ABBA's 1976 instrumental track "Arrival" for Sarah Brightman's cover version recorded for her 2008 album "Winter Symphony". New English lyrics have also been written for Andersson's 1999 song ""Innan Gryningen"" (then also named "Millennium Hymn"), with the new title "The Silence of the Dawn" for Barbara Dickson (performed live, but not yet recorded and released). In 2007, they wrote the song ""Han som har vunnit allt"" ("He Who's Won It All") for actor/singer Anders Ekborg.

Ulvaeus also wrote English lyrics for two older songs from Andersson's solo albums: "After the Rain" ("Efter regnet", 1987) for opera singer Anne Sofie von Otter, for her Andersson tribute album "I Let the Music Speak", and "I Walk with You Mama" ("Stockholm by Night", 1989). Barbara Dickson recorded (but not yet released) a Björn & Benny song entitled "The Day The Wall Came Tumbling Down"; the track was eventually released by Australian "Mamma Mia!" musical star Anne Wood on her album of ABBA covers, "Divine Discontent". , Ulvaeus has mentioned writing new material with Andersson for a BAO Christmas release (also mentioned as a BAO 'box'). Andersson (together with Kristina Lugn and Lars Rudolfsson) composed music for a Swedish language obscure musical, "Hjälp Sökes" ("Help Wanted"), which premiered in February 2013. Andersson has also written music for a documentary film about Olof Palme, re-recording the track "Sorgmarsch" ("Dirge").

In 1980, Fältskog and her then 7-year-old daughter Linda recorded "Nu tändas tusen juleljus", a Swedish Christmas album. Released in 1981, it was Fältskog's first Swedish-language recording for the Polar Music label after she left CBS-Cupol. It peaked at No. 6 on the Swedish album chart in January 1982, and has since been re-released on CD by Polar Music/PolyGram/Universal Music. The album title is derived from one of Scandinavia's best-known Christmas carols, "Nu tändas tusen juleljus" ("Now a thousand Christmas candles are lit").

In 1983, Fältskog released the solo album "Wrap Your Arms Around Me", which achieved platinum sales in Sweden. This included the single "The Heat Is On", which was a hit in Europe and Scandinavia. It reached number-one in Sweden and Norway and peaked at number-two in the Netherlands and Belgium. In the United States, Fältskog earned a "Billboard" Top 30 hit with "Can't Shake Loose". The title track of the album was another successful hit, topping the charts in Belgium and Denmark, reaching the Top 5 in the Netherlands, South Africa and Sweden, and the Top 20 in Germany and France. The album sold 1.2 million copies worldwide. The album was produced Mike Chapman, also known for his work with The Sweet, Mud, Suzi Quatro, Blondie, Pat Benatar and The Knack.
Fältskog's second English-language solo album, "Eyes of a Woman" (produced by Eric Stewart of 10cc), was released in March 1985. It peaked at number two in Sweden (becoming a platinum seller). The first single from the album was her self-penned "I Won't Let You Go". Her duet with Ola Håkansson, "The Way You Are", was a number-one hit in Sweden in 1986 and was awarded double platinum status.

In early 1987, Fältskog recorded a Swedish-language album, "Kom följ med i vår karusell" ("Come Join Us on Our Carousel") with her son Christian and a children's choir. The single "På Söndag" ("On Sunday") received significant airplay on Swedish radio and even made the Swedish Top 10, unique for a song made for kids to enjoy.

Also in 1987, Fältskog released her third English-language solo album, the Peter Cetera-produced "I Stand Alone", which also included the "Billboard" Adult Contemporary duet with Cetera, "I Wasn't the One (Who Said Goodbye)", as well as the European charting singles "The Last Time" and "Let It Shine". The album was extremely successful in Sweden, where it spent eight weeks at number-one and was awarded double-platinum status. Shortly after some minor European promotion for the album in early 1988, Fältskog withdrew from public life and halted her music career. In 1996, she released her autobiography, "As I Am", and a compilation album featuring her solo hits alongside some ABBA classics.

In 2004, Fältskog made a successful comeback, with the release of the critically acclaimed album "My Colouring Book", containing covers of songs that had the most impact on her teenage years in the 1960s. It debuted at number-one in Sweden (achieving triple-platinum status), and was a Top 10 hit in Denmark, Finland and Germany. It also became Fältskog's second solo album to reach the UK Top 20, peaking at number 12. The single "If I Thought You'd Ever Change Your Mind" (a cover of the song recorded by Cilla Black) became Fältskog's biggest solo hit in the UK, reaching number 11, while peaking at number-two in her native Sweden. A second single, "When You Walk in the Room", was released but met with less success. In January 2007, Fältskog sang a live duet on stage with Swedish singer Tommy Körberg at the after party for the final performance of the musical, "Mamma Mia!", in Stockholm, at which Andersson and Ulvaeus were also present.

In May 2013, Fältskog released a solo album entitled "A" through Universal International. In a promotional interview, Fältskog explained that the album was unplanned and it was after she heard the first three songs that she felt that she "had to do this [record the album]". She also revealed that she completed singing lessons prior to recording the album, as she felt her throat was "a bit rusty". Fältskog stated that she would not be undertaking any tours or live performances in support of the album, explaining: "I'm not that young anymore. I don't have the energy to do that, and also I don't want to travel too much." The title of the album was conceived of by the studio production team. "A" proved successful upon release, reaching the Top 10 in many European countries, including Germany, Sweden and the UK (where it peaked at number-six and is Fältskog's highest-charting solo album to date), as well as Australia.

Both female members of ABBA pursued solo careers on the international scene after their work with the group. In 1982, Lyngstad chose Genesis drummer and vocalist Phil Collins to produce the album "Something's Going On" and unveiled the hit single and video "I Know There's Something Going On" in August of that year. The single became a number-one hit in Belgium and Switzerland and was a Top 10 hit in Australia, Austria, Finland, France, Germany, Italy, the Netherlands, Norway, Poland, South Africa and Sweden. The track also proved successful in the US, peaking at No. 13 (and spending almost four months on the "Billboard" Hot 100). Sveriges Television documented this historical event, by filming the whole recording process. The result became a one-hour TV documentary, including interviews with Lyngstad, Collins, Ulvaeus and Andersson as well as all the musicians. This documentary and the promotion videos from the album are included in "Frida – The DVD".

Lyngstad's second international solo album, "Shine" (produced by Steve Lillywhite), was recorded in Paris and released in 1984. This would be Lyngstad's final studio album release for twelve years. It featured "Slowly", the last known Andersson-Ulvaeus composition to have been recorded by one of the former female ABBA vocalists to date. The promotional videos and clips for "Shine" are included in "Frida – The DVD".

In 1992, Lyngstad was chosen to be the chairperson for the environmental organisation ""Artister för miljön"" ("Artists for the Environment") in Sweden. She was chairperson for this organisation until 1995. To mark her interests for the environment, she recorded the Julian Lennon song "Saltwater" and performed it live in Stockholm. She arranged and financed summer camps for poor children in Sweden, focusing on environmental and ecological issues. Her environmental work for the organisation led to the decision to record again. The album "Djupa andetag" ("Deep Breaths") was released in 1996 and became a number-one success in Sweden. The lyrics for the single, "Även en blomma" ("Even a Flower"), deal with environmental issues.

In 2004, Lyngstad recorded a song entitled "The Sun Will Shine Again", written especially for her and released with former Deep Purple member Jon Lord. The couple made several TV performances with the song in Germany.

On 5 December 2005, Universal released her box set, Frida – 4xCD 1xDVD, consisting of the solo albums she recorded for the Polar Label and the 3-hour documentary "Frida – The DVD". On this DVD, which covers her entire singing career, the viewer is guided by Lyngstad herself through the years from her TV debut in Sweden in 1967 to the TV performances she made in Germany in 2004. Many rare clips are included in the set and each performance is explained by Lyngstad herself. The interview with Lyngstad was filmed in the Swiss Alps in mid-2005.

Lyngstad returned to the recording studio in 2010 to record vocals for the Cat Stevens song "Morning Has Broken", for Swedish guitarist Georg Wadenius's album "Reconnection". The album, which featured other guest vocalists, reached number 17 in Sweden.

In 2018, Lyngstad and multi-Grammy winning Jazz trumpeter Arturo Sandoval released a reworking of the ABBA song "Andante, Andante" as a single, which is also featured on Sandoval's album "Ultimate Duets".

ABBA were perfectionists in the studio, working on tracks until they got them right rather than leaving them to come back to later on.

The band created a basic rhythm track with a drummer, guitarist and bass player, and overlaid other arrangements and instruments. Vocals were then added, and orchestra overdubs were usually left until last.

Fältskog and Lyngstad contributed ideas at the studio stage. Andersson and Ulvaeus played them the backing tracks and they made comments and suggestions. According to Fältskog, she and Lyngstad had the final say in how the lyrics were shaped.

Their single "S.O.S." was "heavily influenced by Phil Spector's Wall of Sound and the melodies of the Beach Boys", according to "Billboard" writer Fred Bronson, who also reported that Ulvaeus had said, "Because there was the Latin-American influence, the German, the Italian, the English, the American, all of that. I suppose we were a bit exotic in every territory in an acceptable way."

ABBA was widely noted for the colourful and trend-setting costumes its members wore. The reason for the wild costumes was Swedish tax law. The cost of the clothes was deductible only if they could not be worn other than for performances. Choreography by Graham Tainton also contributed to their performance style.

The videos that accompanied some of the band's biggest hits are often cited as being among the earliest examples of the genre. Most of ABBA's videos (and "ABBA: The Movie") were directed by Lasse Hallström, who would later direct the films "My Life as a Dog", "The Cider House Rules" and "Chocolat".

ABBA made videos because their songs were hits in many different countries and personal appearances were not always possible. This was also done in an effort to minimise travelling, particularly to countries that would have required extremely long flights. Fältskog and Ulvaeus had two young children and Fältskog, who was also afraid of flying, was very reluctant to leave her children for such a long time. ABBA's manager, Stig Anderson, realised the potential of showing a simple video clip on television to publicise a single or album, thereby allowing easier and quicker exposure than a concert tour. Some of these videos have become classics because of the 1970s-era costumes and early video effects, such as the grouping of the band members in different combinations of pairs, overlapping one singer's profile with the other's full face, and the contrasting of one member against another.

In 1976, ABBA participated in an advertising campaign to promote the Matsushita Electric Industrial Co.'s brand, National, in Australia. The campaign was also broadcast in Japan. Five commercial spots, each of approximately one minute, were produced, each presenting the "National Song" performed by ABBA using the melody and instrumental arrangements of "Fernando" and revised lyrics.

In September 2010, band members Andersson and Ulvaeus criticised the right-wing Danish People's Party (DF) for using the ABBA song "Mamma Mia" (with modified lyrics) at rallies. The band threatened to file a lawsuit against the DF, saying they never allowed their music to be used politically and that they had absolutely no interest in supporting the party. Their record label Universal Music later said that no legal action would be taken because an agreement had been reached.

During their active career, from 1972 to 1982, ABBA placed 20 singles on the "Billboard" Hot 100, 14 of which made the Top 40 (13 on the Cashbox Top 100), with 10 making the Top 20 on both charts. A total of four of those singles reached the Top 10, including "Dancing Queen" which reached number one in April 1977. While "Fernando" and "SOS" did not break the Top 10 on the "Billboard" Hot 100 (reaching number 13 and 15 respectively), they did reach the Top 10 on Cashbox ("Fernando") and Record World ("SOS") charts. Both "Dancing Queen" and "Take a Chance on Me" were certified gold by the Recording Industry Association of America for sales of over one million copies each.

The group also had 12 Top 20 singles on the "Billboard" Adult Contemporary chart with two of them, "Fernando" and "The Winner Takes It All", reaching number-one. "Lay All Your Love on Me" was ABBA's fourth number-one single on a "Billboard" chart, topping the Hot Dance Club Play chart.

Nine ABBA albums made their way into the top half of the "Billboard" 200 album chart, with seven reaching the Top 50 and four reaching the Top 20. "ABBA: The Album" was the highest-charting album of the group's career, peaking at No. 14. Five albums received RIAA gold certification (more than 500,000 copies sold), while three acquired platinum status (selling more than one million copies).

The compilation album "" topped the "Billboard" Top Pop Catalog Albums chart in August 2008 (15 years after it was first released in the US in 1993), becoming the group's first number-one album ever on any of the "Billboard" album charts. It has sold 6 million copies there.

On 15 March 2010, ABBA were inducted into the Rock and Roll Hall of Fame by Bee Gees members Barry Gibb and Robin Gibb. The ceremony was held at the Waldorf Astoria Hotel in New York City. The group was represented by Anni-Frid Lyngstad and Benny Andersson.


The members of ABBA were married as follows: Agnetha Fältskog and Björn Ulvaeus from 1971 to 1980: Benny Andersson and Anni-Frid Lyngstad from 1978 to 1981.



Studio albums







</doc>
<doc id="881" url="https://en.wikipedia.org/wiki?curid=881" title="Allegiance">
Allegiance

An allegiance is a duty of fidelity said to be owed, or freely committed, by the people, subjects or citizens to their state or sovereign.

From Middle English "ligeaunce" (see medieval Latin "ligeantia", "a liegance"). The "al-" prefix was probably added through confusion with another legal term, "allegeance", an "allegation" (the French "allegeance" comes from the English). "Allegiance" is formed from "liege," from Old French "liege", "liege, free", of Germanic origin. The connection with Latin "ligare", "to bind," is erroneous.

Traditionally, English legal commentators used the term "allegiance" in two ways. In one sense, it referred to the deference which anyone, even foreigners, was expected to pay to the institutions of the country where one lived. In the other sense, it meant national character and the subjection due to that character.


The English doctrine, which was at one time adopted in the United States, asserted that allegiance was indelible: "Nemo potest exuere patriam". As the law stood prior to 1870, every person who by birth or naturalisation satisfied the conditions set forth, (though he should be removed in infancy to another country where his family resided), owed an allegiance to the British crown which he/she could never resign or lose, except by act of parliament or by the recognition of the independence or the cession of the portion of British territory in which he resided.

This refusal to accept any renunciation of allegiance to the Crown led to conflict with the United States over impressment, which led to further conflicts during the War of 1812, when thirteen Irish American prisoners of war were executed as traitors after the Battle of Queenston Heights; Winfield Scott urged American reprisal, but none was carried out.

Allegiance is the tie which binds the subject to the Sovereign in return for that protection which the Sovereign affords the subject. It was the mutual bond and obligation between monarch and subjects, whereby subjects are called his liege subjects, because they are bound to obey and serve him; and he is called their liege lord, because he should maintain and defend them ("Ex parte Anderson" (1861) 3 El & El 487; 121 ER 525; "China Navigation Co v Attorney-General" (1932) 48 TLR 375; "Attorney-General v Nissan" [1969] 1 All ER 629; "Oppenheimer v Cattermole" [1972] 3 All ER 1106). The duty of the Crown towards its subjects is to govern and protect. The reciprocal duty of the subject towards the Crown is that of allegiance.

At common law, allegiance is a true and faithful obedience of the subject due to his Sovereign. As the subject owes to his king his true and faithful allegiance and obedience, so the Sovereign


Natural allegiance and obedience is an incident inseparable to every subject, for parte Anderson (1861) 3 El & El 487; 121 ER 525). Natural-born subjects owe allegiance wherever they may be. Where territory is occupied in the course of hostilities by an enemy's force, even if the annexation of the occupied country is proclaimed by the enemy, there can be no change of allegiance during the progress of hostilities on the part of a citizen of the occupied country ("R v Vermaak" (1900) 21 NLR 204 (South Africa)).

Allegiance is owed both to the Sovereign as a natural person and to the Sovereign in the political capacity ("Re Stepney Election Petition, Isaacson v Durant" (1886) 17 QBD 54 (per Lord Coleridge CJ)). Attachment to the person of the reigning Sovereign is not sufficient. Loyalty requires affection also to the office of the Sovereign, attachment to royalty, attachment to the law and to the constitution of the realm, and he who would, by force or by fraud, endeavour to prostrate that law and constitution, though he may retain his affection for its head, can boast but an imperfect and spurious species of loyalty ("R v O'Connell" (1844) 7 ILR 261).

There were four kinds of allegiances ("Rittson v Stordy" (1855) 3 Sm & G 230; "De Geer v Stone" (1882) 22 Ch D 243; "Isaacson v Durant" (1886) 54 LT 684; "Gibson, Gavin v Gibson" [1913] 3 KB 379; "Joyce v DPP" [1946] AC 347; "Collingwood v Pace" (1661) O Bridg 410; "Lane v Bennett" (1836) 1 M & W 70; "Lyons Corp v East India Co" (1836) 1 Moo PCC 175; "Birtwhistle v Vardill" (1840) 7 Cl & Fin 895; "R v Lopez, R v Sattler" (1858) Dears & B 525; Ex p Brown (1864) 5 B & S 280);

(a) "Ligeantia naturalis, absoluta, pura et indefinita", and this originally is due by nature and birthright, and is called "alta ligeantia", and those that owe this are called "subditus natus";

(b) "Ligeantia acquisita", not by nature but by acquisition or denization, being called a denizen, or rather denizon, because they are "subditus datus";

(c) "Ligeantia localis", by operation of law, when a friendly alien enters the country, because so long as they are in the country they are within the Sovereign's protection, therefore they owe the Sovereign a local obedience or allegiance ("R v Cowle" (1759) 2 Burr 834; "Low v Routledge" (1865) 1 Ch App 42; "Re Johnson, Roberts v Attorney-General" [1903] 1 Ch 821; "Tingley v Muller" [1917] 2 Ch 144; "Rodriguez v Speyer" [1919] AC 59; "Johnstone v Pedlar" [1921] 2 AC 262; "R v Tucker" (1694) Show Parl Cas 186; "R v Keyn" (1876) 2 Ex D 63; "Re Stepney Election Petn, Isaacson v Durant" (1886) 17 QBD 54);

(d) A legal obedience, where a particular law requires the taking of an oath of allegiance by subject or alien alike.

Natural allegiance was acquired by birth within the Sovereign's dominions (except for the issue of diplomats or of invading forces or of an alien in an enemy occupied territory). The natural allegiance and obedience are an incident inseparable from every subject, for as soon as they are born they owe by birthright allegiance and obedience to the Sovereign ("Ex p. Anderson" (1861) 3 E & E 487). A natural-born subject owes allegiance wherever they may be, so that where territory is occupied in the course of hostilities by an enemy's force, even if the annexation of the occupied country is proclaimed by the enemy, there can be no change of allegiance during the progress of hostilities on the part of a citizen of the occupied country ("R v Vermaak" (1900) 21 NLR 204 (South Africa)).

Acquired allegiance was acquired by naturalisation or denization. Denization, or "ligeantia acquisita", appears to be threefold ("Thomas v Sorrel" (1673) 3 Keb 143);


Local allegiance was due by an alien while in the protection of the Crown. All friendly resident aliens incurred all the obligations of subjects ("The Angelique" (1801) 3 Ch Rob App 7). An alien, coming into a colony also became, temporarily a subject of the Crown, and acquired rights both within and beyond the colony, and these latter rights could not be affected by the laws of that colony ("Routledge v Low" (1868) LR 3 HL 100; 37 LJ Ch 454; 18 LT 874; 16 WR 1081, HL; "Reid v Maxwell" (1886) 2 TLR 790; "Falcon v Famous Players Film Co" [1926] 2 KB 474).

A resident alien owed allegiance even when the protection of the Crown was withdrawn owing to the occupation of an enemy, because the absence of the Crown's protection was temporary and involuntary ("de Jager v Attorney-General of Natal" [1907] AC 326).

Legal allegiance was due when an alien took an oath of allegiance required for a particular office under the Crown.

By the Naturalisation Act 1870, it was made possible for British subjects to renounce their nationality and allegiance, and the ways in which that nationality is lost are defined. So British subjects voluntarily naturalized in a foreign state are deemed aliens from the time of such naturalization, unless, in the case of persons naturalized before the passing of the act, they have declared their desire to remain British subjects within two years from the passing of the act. Persons who from having been born within British territory are British subjects, but who at birth became under the law of any foreign state subjects of such state, and also persons who though born abroad are British subjects by reason of parentage, may by declarations of alienage get rid of British nationality. Emigration to an uncivilized country leaves British nationality unaffected: indeed the right claimed by all states to follow with their authority their subjects so emigrating is one of the usual and recognized means of colonial expansion.

The doctrine that no man can cast off his native allegiance without the consent of his sovereign was early abandoned in the United States, and Chief Justice John Rutledge also declared in Talbot v. Janson, "a man may, at the same time, enjoy the rights of citizenship under two governments." On July 27, 1868, the day before the Fourteenth Amendment was adopted, U.S. Congress declared in the preamble of the Expatriation Act that "the right of expatriation is a natural and inherent right of all people, indispensable to the enjoyment of the rights of life, liberty and the pursuit of happiness," and (Section I) one of "the fundamental principles of this government" (United States Revised Statutes, sec. 1999). Every natural-born citizen of a foreign state who is also an American citizen and every natural-born American citizen who is a citizen of a foreign land owes a double allegiance, one to the United States, and one to his homeland (in the event of an immigrant becoming a citizen of the US), or to his adopted land (in the event of an emigrant natural born citizen of the US becoming a citizen of another nation). If these allegiances come into conflict, he or she may be guilty of treason against one or both. If the demands of these two sovereigns upon his duty of allegiance come into conflict, those of the United States have the paramount authority in American law; likewise, those of the foreign land have paramount authority in their legal system. In such a situation, it may be incumbent on the individual to renounce one of his citizenships to avoid possibly being forced into situations where countervailing duties are required of him, such as might occur in the event of war.

The oath of allegiance is an oath of fidelity to the sovereign taken by all persons holding important public office and as a condition of naturalization. By ancient common law, it might be required of all persons above the age of 12, and it was repeatedly used as a test for the disaffected. In England, it was first imposed by statute in the reign of Elizabeth I (1558) and its form has more than once been altered since. Up to the time of the revolution, the promise was, "to be true and faithful to the king and his heirs, and truth and faith to bear of life and limb and terrene honour, and not to know or hear of any ill or damage intended him without defending him therefrom." This was thought to favour the doctrine of absolute non-resistance, and accordingly, the Convention Parliament enacted the form that has been in use since that time – "I do sincerely promise and swear that I will be faithful and bear true allegiance to His Majesty ..."

In the United States and some other republics, the oath is known as the Pledge of Allegiance. Instead of declaring fidelity to a monarch, the pledge is made to the flag, the republic, and to the core values of the country, specifically liberty and justice. The reciting of the pledge in the United States is voluntary because of the rights guaranteed to the people under the First Amendment to the United States Constitution.

The word used in the Arabic language for allegiance is "bay'at" (Arabic: بيعة), which means "taking hand". The practice is sanctioned in the Quran by Surah 48:10: "Verily, those who give thee their allegiance, they give it but to Allah Himself". The word is used for the oath of allegiance to an emir. It is also used for the initiation ceremony specific to many Sufi orders.




</doc>
<doc id="885" url="https://en.wikipedia.org/wiki?curid=885" title="Altenberg">
Altenberg

Altenberg (German for "old mountain" or "mountain of the old") may refer to:








</doc>
<doc id="887" url="https://en.wikipedia.org/wiki?curid=887" title="MessagePad">
MessagePad

The MessagePad is a discontinued series of personal digital assistant devices developed by Apple Computer Inc. for the Newton platform in 1993. Some electronic engineering and the manufacture of Apple's MessagePad devices was undertaken in Japan by the Sharp Corporation. The devices were based on the ARM 610 RISC processor and all featured handwriting recognition software and were developed and marketed by Apple. The devices ran the Newton OS.

With the MessagePad 120 with Newton OS 2.0, the Newton Keyboard by Apple became available, which can also be used via the dongle on Newton devices with a Newton InterConnect port, most notably the Apple MessagePad 2000/2100 series, as well as the Apple eMate 300.

Newton devices featuring Newton OS 2.1 or higher can be used with the screen turned horizontally ("landscape") as well as vertically ("portrait"). A change of a setting rotates the contents of the display by 90, 180 or 270 degrees. Handwriting recognition still works properly with the display rotated, although display calibration is needed when rotation in any direction is used for the first time or when the Newton device is reset.

In initial versions (Newton OS 1.x) the handwriting recognition gave extremely mixed results for users and was sometimes inaccurate. The original handwriting recognition engine was called Calligrapher, and was licensed from a Russian company called . Calligrapher's design was quite sophisticated; it attempted to learn the user's natural handwriting, using a database of known words to make guesses as to what the user was writing, and could interpret writing anywhere on the screen, whether hand-printed, in cursive, or a mix of the two. By contrast, Palm Pilot's Graffiti had a less sophisticated design than Calligrapher, but was sometimes found to be more accurate and precise due to its reliance on a fixed, predefined stroke alphabet. The stroke alphabet used letter shapes which resembled standard handwriting, but which were modified to be both simple and very easy to differentiate. Palm Computing also released two versions of Graffiti for Newton devices. The Newton version sometimes performed better and could also show strokes as they were being written as input was done on the display itself, rather than on a silkscreen area.

For editing text, Newton had a very intuitive system for handwritten editing, such as scratching out words to be deleted, circling text to be selected, or using written carets to mark inserts.

Later releases of the Newton operating system retained the original recognizer for compatibility, but added a hand-printed-text-only (not cursive) recognizer, called "Rosetta", which was developed by Apple, included in version 2.0 of the Newton operating system, and refined in Newton 2.1. Rosetta is generally considered a significant improvement and many reviewers, testers, and most users consider the Newton 2.1 handwriting recognition software better than any of the alternatives even 10 years after it was introduced. Recognition and computation of handwritten horizontal and vertical formulas such as "1 + 2 =" was also under development but never released. However, users wrote similar programs which could evaluate mathematical formulas using the Newton OS Intelligent Assistant, a unique part of every Newton device.

The handwriting recognition and parts of the user interface for the Newton are best understood in the context of the broad history of pen computing, which is quite extensive.

A vital feature of the Newton handwriting recognition system is the modeless error correction. That is, correction done in situ without using a separate window or widget, using a minimum of gestures. If a word is recognized improperly, the user could double-tap the word and a list of alternatives would pop up in a menu under the stylus. Most of the time, the correct word will be in the list. If not, a button at the bottom of the list allows the user to edit individual characters in that word. Other pen gestures could do such things as transpose letters (also in situ). The correction popup also allowed the user to revert to the original, un-recognized letter shapes - this would be useful in note-taking scenarios if there was insufficient time to make corrections immediately. To conserve memory and storage space, alternative recognition hypotheses would not be saved indefinitely. If the user returned to a note a week later, for example, they would only see the best match. Error correction in many current handwriting systems provides such functionality but adds more steps to the process, greatly increasing the interruption to a user's workflow that a given correction requires.

Text could also be entered by tapping with the stylus on a small on-screen pop-up QWERTY virtual keyboard, although more layouts were developed by users. Newton devices could also accept free-hand "Sketches", "Shapes", and "Ink Text", much like a desktop computer graphics tablet. With "Shapes", Newton could recognize that the user was attempting to draw a circle, a line, a polygon, etc., and it would clean them up into perfect vector representations (with modifiable control points and defined vertices) of what the user was attempting to draw. "Shapes" and "Sketches" could be scaled or deformed once drawn. "Ink text" captured the user's free-hand writing but allowed it to be treated somewhat like recognized text when manipulating for later editing purposes ("ink text" supported word wrap, could be formatted to be bold, italic, etc.). At any time a user could also direct their Newton device to recognize selected "ink text" and turn it into recognized text (deferred recognition). A Newton note (or the notes attached to each contact in Names and each Dates calendar or to-do event) could contain any mix of interleaved text, Ink Text, Shapes, and Sketches.

While the Newton offered handwriting recognition training and would clean up sketches into vector shapes, both were unreliable and required much rewriting and redrawing. The most reliable application of the Newton was collecting and organizing address and phone numbers. While handwritten messages could be stored, they could not be easily filed, sorted or searched. While the technology was a probable cause for the failure of the device (which otherwise met or exceeded expectations), the technology has been instrumental in producing the future generation of handwriting software that realizes the potential and promise that began in the development of Newton-Apple's Ink Handwriting Recognition.

The MessagePad 100 series of devices used Macintosh's proprietary serial ports—round Mini-DIN 8 connectors. The MessagePad 2000/2100 models (as well as the eMate 300) have a small, proprietary "Newton InterConnect" port. However, the development of the Newton hardware/software platform was canceled by Steve Jobs on February 27, 1998, so the InterConnect port, while itself very advanced, can only be used to connect a serial dongle. A prototype multi-purpose InterConnect device containing serial, audio in, audio out, and other ports was also discovered. In addition, all Newton devices have infrared connectivity, initially only the Sharp ASK protocol, but later also IrDA, though the Sharp ASK protocol was kept in for compatibility reasons. Unlike the Palm Pilot, all Newton devices are equipped with a standard PC Card expansion slot (two on the 2000/2100). This allows native modem and even Ethernet connectivity; Newton users have also written drivers for 802.11b wireless networking cards and ATA-type flash memory cards (including the popular CompactFlash format), as well as for Bluetooth cards. Newton can also dial a phone number through the built-in speaker of the Newton device by simply holding a telephone handset up to the speaker and transmitting the appropriate tones. Fax and printing support is also built in at the operating system level, although it requires peripherals such as parallel adapters, PCMCIA cards, or serial modems, the most notable of which is the lightweight Newton Fax Modem released by Apple in 1993. It is powered by 2 AA batteries, and can also be used with a power adapter. It provides data transfer at 2,400 bit/s, and can also send and receive fax messages at 9,600 and 4,800 bit/s respectively.

The original Apple MessagePad and MessagePad 100 used four AAA batteries. They were eventually replaced by AA batteries with the release of the Apple MessagePad 110.

The use of 4 AA NiCd (MessagePad 110, 120 and 130) and 4x AA NiMH cells (MP2x00 series, eMate 300) give a runtime of up to 30 hours (MP2100 with two 20 MB Linear Flash memory PC Cards, no backlight usage) and up to 24 hours with backlight on. While adding more weight to the handheld Newton devices than AAA batteries or custom battery packs, the choice of an easily replaceable/rechargeable cell format gives the user a still unsurpassed runtime and flexibility of power supply. This, together with the flash memory used as internal storage starting with the Apple MessagePad 120 (if all cells lost their power, no data was lost due to the non-volatility of this storage), gave birth to the slogan "Newton never dies, it only gets new batteries".

The Apple MessagePad 2000/2100, with a vastly improved handwriting recognition system, 162 MHz StrongARM SA-110 RISC processor, Newton OS 2.1, and a better, clearer, backlit screen, attracted critical plaudits. 
Fourteen months after Sculley demoed it at the May 1992, Chicago CES, the MessagePad was first offered for sale on August 2, 1993, at the Boston Macworld Expo. The hottest item at the show, it cost $900. 50,000 MessagePads were sold in the device's first three months on the market.

The original Apple MessagePad and MessagePad 100 were limited by the very short lifetime of their inadequate AAA batteries.
Critics also panned the handwriting recognition that was available in the debut models, which had been trumpeted in the Newton's marketing campaign. It was this problem that was skewered in the "Doonesbury" comic strips in which a written text entry is (erroneously) translated as "Egg Freckles?", as well as in the animated series "The Simpsons". However, the word 'freckles' was not included in the Newton dictionary, although a user could add it themselves. Difficulties were in part caused by the long time requirements for the Calligrapher handwriting recognition software to "learn" the user's handwriting; this process could take from two weeks to two months.

Another factor which limited the early Newton devices' appeal was that desktop connectivity was not included in the basic retail package, a problem that was later solved with 2.x Newton devices - these were bundled with a serial cable and the appropriate Newton Connection Utilities software.

Later versions of Newton OS offered improved handwriting recognition, quite possibly a leading reason for the continued popularity of the devices among Newton users. Even given the age of the hardware and software, Newtons still demand a sale price on the used market far greater than that of comparatively aged PDAs produced by other companies. In 2006, CNET compared an Apple MessagePad 2000 to a Samsung Q1, and the Newton was declared better. In 2009, CNET compared an Apple MessagePad 2000 to an iPhone, and the Newton was still declared better.

A chain of dedicated Newton only stores called Newton Source existed from 1994 until 1998. Locations included New York, Los Angeles, San Francisco, Chicago and Boston. The Westwood Village, California, near U.C.L.A. featured the trademark red and yellow light bulb Newton logo in neon. The stores provided an informative educational venue to learn about the Newton platform in a hands on relaxed fashion. The stores had no traditional computer retail counters and featured oval desktops where interested users could become intimately involved with the Newton product range. The stores were a model for the later Apple Stores.

Notes: The eMate 300 actually has ROM chips silk screened with 2.2 on them. Stephanie Mak on her website discusses this:
If one removes all patches to the eMate 300 (by replacing the ROM chip, and then putting in the original one again, as the eMate and the MessagePad 2000/2100 devices erase their memory completely after replacing the chip), the result will be the Newton OS saying that this is version 2.2.00. Also, the Original MessagePad and the MessagePad 100 share the same model number, as they only differ in the ROM chip version. (The OMP has OS versions 1.0 to 1.05, or 1.10 to 1.11, while the MP100 has 1.3 that can be upgraded with various patches.)

There were a number of projects that used the Newton as a portable information device in cultural settings such as museums. For example, Visible Interactive created a walking tour in San Francisco's Chinatown but the most significant effort took place in Malaysia at the Petronas Discovery Center, known as Petrosains .

In 1995, an exhibit design firm, DMCD Inc., was awarded the contract to design a new science museum in the Petronas Towers in Kuala Lumpur. A major factor in the award was the concept that visitors would use a Newton device to access additional information, find out where they were in the museum, listen to audio, see animations, control robots and other media, and to bookmark information for printout at the end of the exhibit.

The device became known as the ARIF, a Malay word for "wise man" or "seer" and it was also an acronym for A Resourceful Informative Friend. Some 400 ARIFS were installed and over 300 are still in use today. The development of the ARIF system was extremely complex and required a team of hardware and software engineers, designers, and writers. ARIF is an ancestor of the PDA systems used in museums today and it boasted features that have not been attempted since.

The Newton was also used in healthcare applications, for example in collecting data directly from patients. Newtons were used as electronic diaries, with patients entering their symptoms and other information concerning their health status on a daily basis. The compact size of the device and its ease of use made it possible for the electronic diaries to be carried around and used in the patients' everyday life setting. This was an early example of electronic patient-reported outcomes (ePRO)






</doc>
<doc id="888" url="https://en.wikipedia.org/wiki?curid=888" title="A. E. van Vogt">
A. E. van Vogt

Alfred Elton van Vogt (; April 26, 1912 – January 26, 2000) was a Canadian-born science fiction author. His fragmented, bizarre narrative style influenced later science fiction writers, notably Philip K. Dick. He is one of the most popular and influential practitioners of science fiction in the mid-twentieth century, the genre's so-called Golden Age, and one of the most complex.

Alfred Vogt (both "Elton" and "van" were added much later) was born on April 26, 1912 on his grandparents' farm in Edenburg, Manitoba, a tiny (and now defunct) Russian Mennonite community east of Gretna, Manitoba, Canada in the Mennonite West Reserve. He was the third of six children born to Heinrich "Henry" Vogt and Aganetha "Agnes" Vogt (née Buhr), both of whom were themselves born in Manitoba, but who grew up in heavily immigrant communities. Until age four, van Vogt and his family spoke only Plautdietsch at home.

For the first dozen or so years of his life, van Vogt's father, Henry Vogt, a lawyer, moved his family several times within western Canada, alighting successively in Neville, Saskatchewan; Morden, Manitoba; and finally Winnipeg, Manitoba. Alfred Vogt found these moves difficult, later remarking:

By the 1920s, living in Winnipeg, father Henry worked as an agent for a steamship company, but the stock market crash of 1929 proved financially disastrous, and the family could not afford to send Alfred to college. During his teen years, Alfred worked as a farmhand and a truck driver, and by the age of 19, he was working in Ottawa for the Canadian census bureau. He began his writing career with stories in the true confession style of pulp magazines such as "True Story". Most of these stories were published anonymously, with the first-person narratives allegedly being written by people (often women) in extraordinary, emotional, and life-changing circumstances.

After a year in Ottawa, he moved back to Winnipeg, where he sold newspaper advertising space and continued to write. While continuing to pen melodramatic "true confessions" stories through 1937, he also began writing short radio dramas for local radio station CKY, as well as conducting interviews published in trade magazines. He added the middle name "Elton" at some point in the mid-1930s, and at least one confessional story (1937's "To Be His Keeper") was sold to the "Toronto Star", who misspelled his name "Alfred Alton Bogt" in the byline. Shortly thereafter, he added the "van" to his surname, and from that point forward he used the name "A. E. van Vogt" both personally and professionally.

By 1938, van Vogt decided to switch to writing science fiction, a genre he enjoyed reading. He was inspired by the August 1938 issue of "Astounding Science Fiction," which he picked up at a newsstand. John W. Campbell's novelette "Who Goes There?" (later adapted into "The Thing from Another World" and "The Thing") inspired van Vogt to write "Vault of the Beast", which he submitted to that same magazine. Campbell, who edited "Astounding" (and had written the story under a pseudonym), sent van Vogt a rejection letter, but one which encouraged van Vogt to try again. Van Vogt sent another story, entitled "Black Destroyer", which was accepted. A revised version of "Vault of the Beast" would be published in 1940.
Van Vogt's first SF publication was inspired by "The Voyage of the Beagle" by Charles Darwin. "The Black Destroyer" was published in July 1939 by John W. Campbell in "Astounding Science Fiction", the centennial year of Darwin's journal. It featured a fierce, carnivorous alien, the coeurl, stalking the crew of an exploration spaceship, and served as the inspiration for multiple science fiction movies, including "Alien" (1979).

Also in 1939, still living in Winnipeg, van Vogt married Edna Mayne Hull, a fellow Manitoban. Hull, who had previously worked as a private secretary, would act as van Vogt's typist, and be credited with writing several SF stories of her own throughout the early 1940s.

The outbreak of World War II in September 1939 caused a change in van Vogt's circumstances. Ineligible for military service due to his poor eyesight, he accepted a clerking job with the Canadian Department of National Defence. This necessitated a move back to Ottawa, where he and his wife would stay for the next year and a half.

Meanwhile, his writing career continued. "Discord in Scarlet" was van Vogt's second story to be published, also appearing as the cover story. It was accompanied by interior illustrations created by Frank Kramer and Paul Orban. (Van Vogt and Kramer thus debuted in the issue of "Astounding" that is sometimes identified as the start of the Golden Age of Science Fiction.)

Van Vogt's first completed novel, and one of his most famous, is "Slan" (Arkham House, 1946), which Campbell serialized in "Astounding" September to December 1940. Using what became one of van Vogt's recurring themes, it told the story of a nine-year-old superman living in a world in which his kind are slain by "Homo sapiens".

Others saw van Vogt's talent from his first story, and in May 1941, van Vogt decided to become a full-time writer, quitting his job at the Canadian Department of National Defence. Freed from the necessity of living in Ottawa, he and his wife lived for a time in the Gatineau region of Quebec before moving to Toronto in the fall of 1941.

Prolific throughout this period, van Vogt wrote many of his more famous short stories and novels in the years from 1941 through 1944. The novels "The Book of Ptath" and "The Weapon Makers" both appeared in magazines in serial form during this era; they were later published in book form after World War II. As well, several (though not all) of the stories that were compiled to make up the novels "The Weapon Shops of Isher", "The Mixed Men" and "The War Against the Rull" were also published during this time.

In November 1944, van Vogt and Hull moved to Hollywood; van Vogt would spend the rest of his life in California. He had been using the name "A. E. van Vogt" in his public life for several years, and as part of the process of obtaining American citizenship in 1945 he finally and formally changed his legal name from Alfred Vogt to Alfred Elton van Vogt. To his friends in the California science fiction community, he was known as "Van".

Van Vogt systematized his writing method, using scenes of 800 words or so where a new complication was added or something resolved. Several of his stories hinge on temporal conundra, a favorite theme. He stated that he acquired many of his writing techniques from three books: "Narrative Technique" by Thomas Uzzell, "The Only Two Ways to Write a Story" by John Gallishaw, and "Twenty Problems of the Fiction Writer" by Gallishaw. He also claimed many of his ideas came from dreams; throughout his writing life he arranged to be awakened every 90 minutes during his sleep period so he could write down his dreams.

Van Vogt was also always interested in the idea of all-encompassing systems of knowledge (akin to modern meta-systems)—the characters in his very first story used a system called "Nexialism" to analyze the alien's behavior. Around this time, he became particularly interested in the general semantics of Alfred Korzybski.

He subsequently wrote a novel merging these overarching themes, "The World of Ā", originally serialized in "Astounding" in 1945. Ā (often rendered as "Null-A"), or non-Aristotelian logic, refers to the capacity for, and practice of, using intuitive, inductive reasoning (compare fuzzy logic), rather than reflexive, or conditioned, deductive reasoning. The novel recounts the adventures of an individual living in an apparent Utopia, where those with superior brainpower make up the ruling class... though all is not as it seems. A sequel, "The Players of Ā" (later re-titled "The Pawns of Null-A") was serialized in 1948–49.

At the same time, in his fiction, van Vogt was consistently sympathetic to absolute monarchy as a form of government. This was the case, for instance, in the "Weapon Shop" series, the "Mixed Men" series, and in single stories such as "Heir Apparent" (1945), whose protagonist was described as a "benevolent dictator". These sympathies were the subject of much critical discussion during van Vogt's career, and afterwards.

Van Vogt published "Enchanted Village" in the July 1950 issue of "Other Worlds Science Stories". It was reprinted in over 20 collections or anthologies, and appeared many times in translation.

In 1950, van Vogt was briefly appointed as head of L. Ron Hubbard's Dianetics operation in California. Van Vogt had first met Hubbard in 1945, and became interested in his Dianetics theories, which were published shortly thereafter. Dianetics was the secular precursor to Hubbard's Church of Scientology; van Vogt would have no association with Scientology, as he did not approve of its mysticism.

The California Dianetics operation went broke nine months later, but never went bankrupt, due to van Vogt's arrangements with creditors. Very shortly after that, van Vogt and his wife opened their own Dianetics center, partly financed by his writings, until he "signed off" around 1961. In practical terms, what this meant was that from 1951 through 1961, van Vogt's focus was on Dianetics, and no new story ideas flowed from his typewriter.

However, during the 1950s, van Vogt retrospectively patched together many of his previously published stories into novels, sometimes creating new interstitial material to help bridge gaps in the narrative. Van Vogt referred to the resulting books as "fix-ups", a term that entered the vocabulary of science-fiction criticism. When the original stories were closely related this was often successful — although some van Vogt fix-ups featured disparate stories thrown together that bore little relation to each other, generally making for a less coherent plot. One of his best-known (and well-regarded) novels, "The Voyage of the Space Beagle" (1950) was a fix-up of four short stories including "Discord in Scarlet"; it was published in at least five European languages by 1955.

Although Van Vogt averaged a new book title every ten months from 1951 to 1961, none of them were new stories. All of van Vogt's books from 1951 to 1961 were fix-ups, or collections of previously published stories, or expansions of previously published short stories to novel length, or republications of his books under new titles. All were based on story material written and originally published between 1939 and 1950. As well, one non-fiction work, "The Hypnotism Handbook", appeared in 1956, though it had apparently been written much earlier.

Some of van Vogt's more well-known work was still produced using the fix-up method. In 1951, he published the fix-up "The Weapon Shops of Isher." In the same decade, van Vogt also produced collections and fixups such as "The Mixed Men" (1952), "The War Against the Rull" (1959), and the two "Clane" novels, "Empire of the Atom" (1957) and "The Wizard of Linn" (1962), which were inspired (like Asimov's Foundation series) by Roman imperial history; specifically, as Damon Knight wrote, the plot of "Empire of the Atom" was "lifted almost bodily" from that of Robert Graves' "I, Claudius".

After more than a decade of running their Dianetics center, Hull and van Vogt closed it in 1961. Nevertheless, van Vogt maintained his association with the overall organization and was still president of the Californian Association of Dianetic Auditors into the 1980s.

Though the constant re-packaging of his older work meant that he had never really been away from the book publishing world, van Vogt had not published any wholly new fiction for almost 12 years when he decided to return to writing in 1962. He did not return immediately to science fiction, however, but instead wrote the only mainstream, non-sf novel of his career.
Van Vogt was profoundly affected by revelations of totalitarian police states that emerged after World War II. Accordingly, he wrote a mainstream novel that he set in Communist China, "The Violent Man" (1962); he said that to research this book he had read 100 books about China. Into this book he incorporated his view of "the violent male type", which he described as a "man who had to be right", a man who "instantly attracts women" and who he said were the men who "run the world". Contemporary reviews were lukewarm at best, and van Vogt thereafter returned to science fiction.

From 1963 through the mid-1980s, van Vogt once again published new material on a regular basis, though fix-ups and reworked material also appeared relatively often. His later novels included fix-ups such as "The Beast" (also known as "Moonbeast") (1963), "Rogue Ship" (1965), "Quest for the Future" (1970) and "Supermind" (1977). He also wrote novels by expanding previously published short stories; works of this type include "The Darkness on Diamondia" (1972) and "Future Glitter" (also known as "Tyranopolis"; 1973).

Novels that were written simply as novels, and not serialized magazine pieces or fix-ups, were very rare in van Vogt's oeuvre, but began to appear regularly beginning in the 1970s. Van Vogt's original novels included "Children of Tomorrow" (1970), "The Battle of Forever" (1971) and "The Anarchistic Colossus" (1977). Over the years, many sequels to his classic works were promised, but only one appeared: "Null-A Three" (1984; originally published in French). Several later books were originally published in Europe, and at least one novel only ever appeared in foreign language editions and was never published in its original English.

When the 1979 film "Alien" appeared, it was noted that the plot closely matched the plots of both "Black Destroyer" and "Discord in Scarlet", both published in "Astounding magazine" in 1939, and then later published in the 1950 book "Voyage of the Space Beagle". Van Vogt sued the production company for plagiarism, and eventually collected an out-of-court settlement of $50,000 from 20th Century Fox. 

In increasingly frail health, van Vogt published his final short story in 1986.

Van Vogt's first wife, Edna Mayne Hull, died in 1975. Van Vogt married Lydia Bereginsky in 1979; they remained together until his death.

On January 26, 2000, A. E. van Vogt died in Los Angeles from Alzheimer's disease. He was survived by his second wife, the former Lydia Bereginsky.

Critical opinion about the quality of van Vogt's work is sharply divided. An early and articulate critic was Damon Knight. In a 1945 chapter-long essay reprinted in "In Search of Wonder," entitled "Cosmic Jerrybuilder: A. E. van Vogt", Knight described van Vogt as "no giant; he is a pygmy who has learned to operate an overgrown typewriter". Knight described "The World of Null-A" as "one of the worst allegedly adult science fiction stories ever published". Concerning van Vogt's writing, Knight said:

About "Empire of the Atom" Knight wrote:

Knight also expressed misgivings about van Vogt's politics. He noted that van Vogt's stories almost invariably present absolute monarchy in a favorable light. In 1974, Knight retracted some of his criticism after finding out about Vogt's writing down his dreams as a part of his working methods:

Knight's criticism greatly damaged van Vogt's reputation. On the other hand, when science fiction author Philip K. Dick was asked which science fiction writers had influenced his work the most, he replied:

Dick also defended van Vogt against Damon Knight's criticisms:

In a review of "Transfinite: The Essential A. E. van Vogt", science fiction writer Paul Di Filippo said:

In "The John W. Campbell Letters", Campbell says, "The son-of-a-gun gets hold of you in the first paragraph, ties a knot around you, and keeps it tied in every paragraph thereafter—including the ultimate last one".

Harlan Ellison (who had begun reading van Vogt as a teenager) wrote, "Van was the first writer to shine light on the restricted ways in which I had been taught to view the universe and the human condition".

Writing in 1984, David Hartwell said:

The literary critic Leslie A. Fiedler said something similar:

American literary critic Fredric Jameson says of van Vogt:

Van Vogt still has his critics. For example, Darrell Schweitzer writing to "The New York Review of Science Fiction" in 1999 quoted a passage from the original van Vogt novelette "The Mixed Men", which he was then reading, and remarked:

The Science Fiction Writers of America named him its 14th Grand Master in 1995 (presented 1996). Also in 1996, van Vogt received a Special Award from the World Science Fiction Convention "for six decades of golden age science fiction". That same year, he was inducted as an inaugural member of the Science Fiction and Fantasy Hall of Fame.

In 1946, van Vogt and his first wife, Edna Mayne Hull, were Guests of Honor at the fourth World Science Fiction Convention.

In 1980, van Vogt received a "Casper Award" (precursor to the Canadian Prix Aurora Awards) for Lifetime Achievement.

The Science Fiction Writers of America named him its 14th Grand Master in 1995 (presented 1996). Great controversy within SFWA accompanied its long wait in bestowing its highest honor (limited to living writers, no more than one annually). Writing an obituary of van Vogt, Robert J. Sawyer, a fellow Canadian writer of science fiction, remarked:

It is generally held that the "damnable SFWA politics" concerns Damon Knight, the founder of the SFWA, who abhorred van Vogt's style and politics and thoroughly demolished his literary reputation in the 1950s.

Harlan Ellison was more explicit in 1999 introduction to "Futures Past: The Best Short Fiction of A. E. van Vogt":

In 1996, van Vogt received a Special Award from the World Science Fiction Convention "for six decades of golden age science fiction". That same year, the Science Fiction and Fantasy Hall of Fame inducted him in its inaugural class of two deceased and two living persons, along with writer Jack Williamson (also living) and editors Hugo Gernsback and John W. Campbell.

The works of van Vogt were translated into French by the surrealist Boris Vian ("The World of Null-A" as "Le Monde des Å" in 1958), and van Vogt's works were "viewed as great literature of the surrealist school". In addition, "Slan" was published in French, translated by Jean Rosenthal, under the title "À la poursuite des Slans", as part of the paperback series 'Editions J'ai Lu: Romans-Texte Integral' in 1973, this edition also listing the following works by van Vogt as having been published in French as part of this series: "Le Monde des Å", "La faune de l'espace", "Les joueurs du Å", "L'empire de l'atome", "Le sorcier de Linn", "Les armureries d'Isher", "Les fabricants d'armes", and "Le livre de Ptath".

Also:





</doc>
<doc id="890" url="https://en.wikipedia.org/wiki?curid=890" title="Anna Kournikova">
Anna Kournikova

Anna Sergeyevna Kournikova (; born 7 June 1981) is a Russian former professional tennis player and American television personality. Her appearance and celebrity status made her one of the best known tennis stars worldwide. At the peak of her fame, fans looking for images of Kournikova made her name one of the most common search strings on Google Search.

Despite never winning a singles title, she reached No. 8 in the world in 2000. She achieved greater success playing doubles, where she was at times the world No. 1 player. With Martina Hingis as her partner, she won Grand Slam titles in Australia in 1999 and 2002, and the WTA Championships in 1999 and 2000. They referred to themselves as the "Spice Girls of Tennis".

Kournikova retired at the age of 21 due to serious back and spinal problems, including a herniated disk. She lives in Miami Beach, Florida, and played in occasional exhibitions and in doubles for the St. Louis Aces of World Team Tennis before the team folded in 2011. She was a new trainer for season 12 of the television show "The Biggest Loser", replacing Jillian Michaels, but did not return for season 13. In addition to her tennis and television work, Kournikova serves as a Global Ambassador for Population Services International's "Five & Alive" program, which addresses health crises facing children under the age of five and their families.

Kournikova was born in Moscow, Russia on 7 June 1981. Her father, Sergei Kournikov (born 1961), a former Greco-Roman wrestling champion, eventually earned a PhD and was a professor at the University of Physical Culture and Sport in Moscow. As of 2001, he was still a part-time martial arts instructor there. Her mother Alla (born 1963) had been a 400-metre runner. Her younger half-brother, Allan, is a youth golf world champion who was featured in the 2013 documentary film "The Short Game".

Sergei Kournikov has said, "We were young and we liked the clean, physical life, so Anna was in a good environment for sport from the beginning".

Kournikova received her first tennis racquet as a New Year gift in 1986 at the age of five. Describing her early regimen, she said, "I played two times a week from age six. It was a children's program. And it was just for fun; my parents didn't know I was going to play professionally, they just wanted me to do something because I had lots of energy. It was only when I started playing well at seven that I went to a professional academy. I would go to school, and then my parents would take me to the club, and I'd spend the rest of the day there just having fun with the kids." In 1986, Kournikova became a member of the Spartak Tennis Club, coached by Larissa Preobrazhenskaya. In 1989, at the age of eight, Kournikova began appearing in junior tournaments, and by the following year, was attracting attention from tennis scouts across the world. She signed a management deal at age ten and went to Bradenton, Florida, to train at Nick Bollettieri's celebrated tennis academy.

Following her arrival in the United States, she became prominent on the tennis scene. At the age of 14, she won the European Championships and the Italian Open Junior tournament. In December 1995, she became the youngest player to win the 18-and-under division of the Junior Orange Bowl tennis tournament. By the end of the year, Kournikova was crowned the ITF Junior World Champion U-18 and Junior European Champion U-18.

Earlier, in September 1995, Kournikova, still at the age of 14, debuted in the WTA Tour, when she received a wildcard into the qualifications at the WTA tournament in Moscow, the Moscow Ladies Open, and played her way through the qualifying rounds before losing in the second round of the main draw to third-seeded Sabine Appelmans. There at the 1995 Moscow Ladies Open Kournikova already reached her first WTA Tour doubles final. Partnering with 1995 Wimbledon girls' champion in both singles and doubles Aleksandra Olsza, she lost the title match to Meredith McGrath and Larisa Savchenko-Neiland.

In February–March 1996, Kournikova won two ITF titles, in Midland, Michigan and Rockford, Illinois. Still only 14 years of age, in April 1996 she debuted at the Fed Cup for Russia, the youngest player ever to participate and win a match.

In 1996, she started playing under a new coach, Ed Nagel. Her six-year tenure with Ed would produce terrific results. At the age of 15, she made her Grand Slam debut, when she reached the fourth round of the 1996 US Open, only to be stopped by then-top ranked player Steffi Graf, the eventual champion. After this tournament, Kournikova's ranking jumped from No. 144 to debut in the Top 100 at No. 69. Kournikova was a member of the Russian delegation to the 1996 Olympic Games in Atlanta, Georgia. In 1996, she was named WTA Newcomer of the Year, and she was ranked No. 57 in the end of the season.

Kournikova entered the 1997 Australian Open as world No. 67, where she lost in the first round to world No. 12, Amanda Coetzer. At the Italian Open, Kournikova lost to Amanda Coetzer in the second round. However, she reached the semi-finals in the doubles partnering with Elena Likhovtseva, before losing to the sixth seeds Mary Joe Fernández and Patricia Tarabini.

At the French Open, Kournikova made it to the third round before losing to world No. 1, Martina Hingis. She also reached the third round in doubles with Likhovtseva. At the Wimbledon Championships, Kournikova became only the second woman in the open era to reach the semi-finals in her Wimbledon debut, the first being Chris Evert in 1972. There she lost to eventual champion Martina Hingis.

At the US Open, she lost in the second round to the eleventh seed Irina Spîrlea. Partnering with Likhovtseva, she reached the third round of the women's doubles event. Kournikova played her last WTA Tour event of 1997 at Porsche Tennis Grand Prix in Filderstadt, losing to Amanda Coetzer in the second round of singles, and in the first round of doubles to Lindsay Davenport and Jana Novotná partnering with Likhovtseva. She broke into the top 50 on 19 May, and was ranked No. 32 in singles and No. 41 in doubles at the end of the season.

In 1998, Kournikova broke into the WTA's top 20 rankings for the first time, when she was ranked No. 16. At the Australian Open, Kournikova lost in the third round to world No. 1 player, Martina Hingis. She also partnered with Larisa Savchenko-Neiland in women's doubles, and they lost to eventual champions Hingis and Mirjana Lučić in the second round. Although she lost in the second round of the Paris Open to Anke Huber in singles, Kournikova reached her second doubles WTA Tour final, partnering with Larisa Savchenko-Neiland. They lost to Sabine Appelmans and Miriam Oremans. Kournikova and Savchenko-Neiland reached their second consecutive final at the Linz Open, losing to Alexandra Fusai and Nathalie Tauziat. At the Miami Open, Kournikova reached her first WTA Tour singles final, before losing to Venus Williams in the final.
Kournikova then reached two consecutive quarterfinals, at Amelia Island and the Italian Open, losing respectively to Lindsay Davenport and Martina Hingis. At the German Open, she reached the semi-finals in both singles and doubles, partnering with Larisa Savchenko-Neiland. At the French Open Kournikova had her best result at this tournament, making it to the fourth round before losing to Jana Novotná. She also reached her first Grand Slam doubles semi-finals, losing with Savchenko-Neiland to Lindsay Davenport and Natasha Zvereva. During her quarterfinals match at the grass-court Eastbourne Open versus Steffi Graf, Kournikova injured her thumb, which would eventually force her to withdraw from the 1998 Wimbledon Championships. However, she won that match, but then withdrew from her semi-finals match against Arantxa Sánchez Vicario. Kournikova returned for the Du Maurier Open and made it to the third round, before losing to Conchita Martínez. At the US Open Kournikova reached the fourth round before losing to Arantxa Sánchez Vicario. Her strong year qualified her for the year-end 1998 WTA Tour Championships, but she lost to Monica Seles in the first round. However, with Seles, she won her first WTA doubles title, in Tokyo, beating Mary Joe Fernández and Arantxa Sánchez Vicario in the final. At the end of the season, she was ranked No. 10 in doubles.

At the start of the 1999 season, Kournikova advanced to the fourth round in singles before losing to Mary Pierce. However, Kournikova won her first doubles Grand Slam title, partnering Martina Hingis. The two defeated Lindsay Davenport and Natasha Zvereva in the final. At the Tier I Family Circle Cup, Kournikova reached her second WTA Tour final, but lost to Martina Hingis. She then defeated Jennifer Capriati, Lindsay Davenport and Patty Schnyder on her route to the Bausch & Lomb Championships semi-finals, losing to Ruxandra Dragomir. At The French Open, Kournikova reached the fourth round before losing to eventual champion Steffi Graf. Once the grass-court season commenced in England, Kournikova lost to Nathalie Tauziat in the semi-finals in Eastbourne. At Wimbledon, Kournikova lost to Venus Williams in the fourth round. She also reached the final in mixed doubles, partnering with Jonas Björkman, but they lost to Leander Paes and Lisa Raymond. Kournikova again qualified for year-end WTA Tour Championships, but lost to Mary Pierce in the first round, and ended the season as World No. 12.
While Kournikova had a successful singles season, she was even more successful in doubles. After their victory at the Australian Open, she and Martina Hingis won tournaments in Indian Wells, Rome, Eastbourne and the WTA Tour Championshiops, and reached the final of The French Open where they lost to Serena and Venus Williams. Partnering with Elena Likhovtseva, Kournikova also reached the final in Stanford. On 22 November 1999 she reached the world No. 1 ranking in doubles, and ended the season at this ranking. Anna Kournikova and Martina Hingis were presented with the WTA Award for Doubles Team of the Year.

Kournikova opened her 2000 season winning the Gold Coast Open doubles tournament partnering with Julie Halard. She then reached the singles semi-finals at the Medibank International Sydney, losing to Lindsay Davenport. At the Australian Open, she reached the fourth round in singles and the semi-finals in doubles. That season, Kournikova reached eight semi-finals (Sydney, Scottsdale, Stanford, San Diego, Luxembourg, Leipzig and Tour Championships), seven quarterfinals (Gold Coast, Tokyo, Amelia Island, Hamburg, Eastbourne, Zürich and Philadelphia) and one final. On 20 November 2000 she broke into top 10 for the first time, reaching No. 8. She was also ranked No. 4 in doubles at the end of the season. Kournikova was once again, more successful in doubles. She reached the final of the US Open in mixed doubles, partnering with Max Mirnyi, but they lost to Jared Palmer and Arantxa Sánchez Vicario. She also won six doubles titles – Gold Coast (with Julie Halard), Hamburg (with Natasha Zvereva), Filderstadt, Zürich, Philadelphia and the Tour Championships (with Martina Hingis).

Her 2001 season was plagued by injuries, including a left foot stress fracture which made her withdrawal from twelve tournaments, including the French Open and Wimbledon. She underwent surgery in April. She reached her second career grand slam quarterfinals, at the Australian Open. Kournikova then withdrew from several events due to continuing problems with her left foot and did not return until Leipzig. With Barbara Schett, she won the doubles title in Sydney. She then lost in the finals in Tokyo, partnering with Iroda Tulyaganova, and at San Diego, partnering with Martina Hingis. Hingis and Kournikova also won the Kremlin Cup. At the end of the 2001 season, she was ranked No. 74 in singles and No. 26 in doubles.
Kournikova was quite successful in 2002. She reached the semi-finals of Auckland, Tokyo, Acapulco and San Diego, and the final of the China Open, losing to Anna Smashnova. This was Kournikova's last singles final. With Martina Hingis, she lost in the final at Sydney, but they won their second Grand Slam title together, the Australian Open. They also lost in the quarterfinals of the US Open. With Chanda Rubin, Kournikova played the semi-finals of Wimbledon, but they lost to Serena and Venus Williams. Partnering Janet Lee, she won the Shanghai title. At the end of 2002 season, she was ranked No. 35 in singles and No. 11 in doubles.

In 2003, Anna Kournikova collected her first Grand Slam match victory in two years at the Australian Open. She defeated Henrieta Nagyová in the first round, and then lost to Justine Henin-Hardenne in the 2nd round. She withdrew from Tokyo due to a sprained back suffered at the Australian Open and did not return to Tour until Miami. On 9 April, in what would be the final WTA match of her career, Kournikova dropped out in the first round of the Family Circle Cup in Charleston, due to a left adductor strain. Her singles world ranking was 67. She reached the semi-finals at the ITF tournament in Sea Island, before withdrawing from a match versus Maria Sharapova due to the adductor injury. She lost in the first round of the ITF tournament in Charlottesville. She did not compete for the rest of the season due to a continuing back injury. At the end of the 2003 season and her professional career, she was ranked No. 305 in singles and No. 176 in doubles.

Kournikova's two Grand Slam doubles titles came in 1999 and 2002, both at the Australian Open in the Women's Doubles event with partner Martina Hingis. Kournikova proved a successful doubles player on the professional circuit, winning 16 tournament doubles titles, including two Australian Opens and being a finalist in mixed doubles at the US Open and at Wimbledon, and reaching the No. 1 ranking in doubles in the WTA Tour rankings. Her pro career doubles record was 200–71. However, her singles career plateaued after 1999. For the most part, she managed to retain her ranking between 10 and 15 (her career high singles ranking was No.8), but her expected finals breakthrough failed to occur; she only reached four finals out of 130 singles tournaments, never in a Grand Slam event, and never won one.

Her singles record is 209–129. Her final playing years were marred by a string of injuries, especially back injuries, which caused her ranking to erode gradually. As a personality Kournikova was among the most common search strings for both articles and images in her prime.

Kournikova has not played on the WTA Tour since 2003, but still plays exhibition matches for charitable causes. In late 2004, she participated in three events organized by Elton John and by fellow tennis players Serena Williams and Andy Roddick. In January 2005, she played in a doubles charity event for the Indian Ocean tsunami with John McEnroe, Andy Roddick, and Chris Evert. In November 2005, she teamed up with Martina Hingis, playing against Lisa Raymond and Samantha Stosur in the WTT finals for charity. Kournikova is also a member of the St. Louis Aces in the World Team Tennis (WTT), playing doubles only.

In September 2008, Kournikova showed up for the 2008 Nautica Malibu Triathlon held at Zuma Beach in Malibu, California. The Race raised funds for children's Hospital Los Angeles. She won that race for women's K-Swiss team. On 27 September 2008, Kournikova played exhibition mixed doubles matches in Charlotte, North Carolina, partnering with Tim Wilkison and Karel Nováček. Kournikova and Wilkison defeated Jimmy Arias and Chanda Rubin, and then Kournikova and Novacek defeated Rubin and Wilkison.

On 12 October 2008, Anna Kournikova played one exhibition match for the annual charity event, hosted by Billie Jean King and Elton John, and raised more than $400,000 for the Elton John AIDS Foundation and Atlanta AIDS Partnership Fund. She played doubles with Andy Roddick (they were coached by David Chang) versus Martina Navratilova and Jesse Levine (coached by Billie Jean King); Kournikova and Roddick won.

Kournikova competed alongside John McEnroe, Tracy Austin and Jim Courier at the "Legendary Night", which was held on 2 May 2009, at the Turning Stone Event Center in Verona, New York. The exhibition included a mixed doubles match of McEnroe and Austin against Courier and Kournikova.

In 2008, she was named a spokesperson for K-Swiss. In 2005, Kournikova stated that if she were 100% fit, she would like to come back and compete again.

In June 2010, Kournikova reunited with her doubles partner Martina Hingis to participate in competitive tennis for the first time in seven years in the Invitational Ladies Doubles event at Wimbledon. On 29 June 2010 they defeated the British pair Samantha Smith and Anne Hobbs.

Kournikova plays right-handed with a two-handed backhand. She is a great player at the net. She can hit forceful groundstrokes and also drop shots.

Her playing style fits the profile for a doubles player, and is complemented by her height. She has been compared to such doubles specialists as Pam Shriver and Peter Fleming.

Kournikova was in a relationship with fellow Russian, Pavel Bure, an NHL ice hockey player. The two met in 1999, when Kournikova was still linked to Bure's former Russian teammate Sergei Fedorov. Bure and Kournikova were reported to have been engaged in 2000 after a reporter took a photo of them together in a Florida restaurant where Bure supposedly asked Kournikova to marry him. As the story made headlines in Russia, where they were both heavily followed in the media as celebrities, Bure and Kournikova both denied any engagement. Kournikova, 10 years younger than Bure, was 18 years old at the time.

Fedorov claimed that he and Kournikova were married in 2001, and divorced in 2003. Kournikova's representatives deny any marriage to Fedorov; however, Fedorov's agent Pat Brisson claims that although he does not know when they got married, he knew "Fedorov was married".

Kournikova started dating singer Enrique Iglesias in late 2001 after she had appeared in his music video for "Escape". She has consistently refused to directly confirm or deny the status of her personal relationships. In June 2008, Iglesias was quoted by the "Daily Star" as having married Kournikova the previous year and subsequently separated. The couple have invested in a $20 million home built on a private island in Miami. They have three children, twins, Nicholas and Lucy born on 16 December 2017 and a daughter born, 30 January 2020.

It was reported in 2010 that Kournikova had become an American citizen.

Most of Kournikova's fame has come from the publicity surrounding her looks and her personal life. During her debut at the 1996 US Open at the age of 15, the western world noticed her beauty, and soon pictures of her appeared in numerous magazines worldwide.

In 2000, Kournikova became the new face for Berlei's shock absorber sports bras, and appeared in the "only the ball should bounce" billboard campaign. Following that, she was cast by the Farrelly brothers for a minor role in the 2000 film "Me, Myself & Irene" starring Jim Carrey and Renée Zellweger. Photographs of her have appeared on covers of various publications, including men's magazines, such as one in the much-publicized 2004 "Sports Illustrated Swimsuit Issue", where she posed in bikinis and swimsuits, as well as in "FHM" and "Maxim".

Kournikova was named one of "People"s 50 Most Beautiful People in 1998 and was voted "hottest female athlete" on ESPN.com. In 2002, she also placed first in "FHM's 100 Sexiest Women in the World" in US and UK editions. By contrast, ESPN – citing the degree of hype as compared to actual accomplishments as a singles player – ranked Kournikova 18th in its "25 Biggest Sports Flops of the Past 25 Years". Kournikova was also ranked No. 1 in the ESPN Classic series "Who's number 1?" when the series featured sport's most overrated athletes.

She continued to be the most searched athlete on the Internet through 2008 even though she had retired from the professional tennis circuit years earlier. After slipping from first to sixth among athletes in 2009, she moved back up to third place among athletes in terms of search popularity in 2010.

In October 2010, Kournikova headed to NBC's "The Biggest Loser" where she led the contestants in a tennis-workout challenge. In May 2011, it was announced that Kournikova would join "The Biggest Loser" as a regular celebrity trainer in season 12. She did not return for season 13.





 


</doc>
<doc id="892" url="https://en.wikipedia.org/wiki?curid=892" title="Alfons Maria Jakob">
Alfons Maria Jakob

Alfons Maria Jakob (2 July 1884 in Aschaffenburg/Bavaria – 17 October 1931 in Hamburg) was a German neurologist who worked in the field of neuropathology.

He was born in Aschaffenburg, Bavaria and educated in medicine at the universities of Munich, Berlin, and Strasbourg, where he received his doctorate in 1908. During the following year, he began clinical work under the psychiatrist Emil Kraepelin and did laboratory work with Franz Nissl and Alois Alzheimer in Munich.

In 1911, by way of an invitation from Wilhelm Weygandt, he relocated to Hamburg, where he worked with Theodor Kaes and eventually became head of the laboratory of anatomical pathology at the psychiatric State Hospital Hamburg-Friedrichsberg. Following the death of Kaes in 1913, Jakob succeeded him as prosector. During World War I he served as an army physician in Belgium, and afterwards returned to Hamburg. In 1919 he obtained his habilitation for neurology and in 1924 became a professor of neurology. Under Jakob's guidance the department grew rapidly. He made significant contributions to knowledge on concussion and secondary nerve degeneration and became a doyen of neuropathology.

Jakob was the author of five monographs and nearly 80 scientific papers. His neuropathological research contributed greatly to the delineation of several diseases, including multiple sclerosis and Friedreich's ataxia. He first recognised and described Alper's disease and Creutzfeldt–Jakob disease (named along with Munich neuropathologist Hans Gerhard Creutzfeldt). He gained experience in neurosyphilis, having a 200-bed ward devoted entirely to that disorder. Jakob made a lecture tour of the United States (1924) and South America (1928), of which, he wrote a paper on the neuropathology of yellow fever.

He suffered from chronic osteomyelitis for the last seven years of his life. This eventually caused a retroperitoneal abscess and paralytic ileus from which he died following operation.




</doc>
<doc id="894" url="https://en.wikipedia.org/wiki?curid=894" title="Agnosticism">
Agnosticism

Agnosticism is the view that the existence of God, of the divine or the supernatural is unknown or unknowable. Another definition provided is the view that "human reason is incapable of providing sufficient rational grounds to justify either the belief that God exists or the belief that God does not exist." 

The English biologist Thomas Henry Huxley coined the word "agnostic" in 1869, and said "It simply means that a man shall not say he knows or believes that which he has no scientific grounds for professing to know or believe."
Earlier thinkers, however, had written works that promoted agnostic points of view, such as Sanjaya Belatthaputta, a 5th-century BCE Indian philosopher who expressed agnosticism about any afterlife; and Protagoras, a 5th-century BCE Greek philosopher who expressed agnosticism about the existence of "the gods".

Agnosticism is the doctrine or tenet of agnostics with regard to the existence of anything beyond and behind material phenomena or to knowledge of a First Cause or God, and is not a religion.

Being a scientist, above all else, Huxley presented agnosticism as a form of demarcation. A hypothesis with no supporting, objective, testable evidence is not an objective, scientific claim. As such, there would be no way to test said hypotheses, leaving the results inconclusive. His agnosticism was not compatible with forming a belief as to the truth, or falsehood, of the claim at hand. Karl Popper would also describe himself as an agnostic. According to philosopher William L. Rowe, in this strict sense, agnosticism is the view that human reason is incapable of providing sufficient rational grounds to justify either the belief that God exists or the belief that God does not exist.

George H. Smith, while admitting that the narrow definition of atheist was the common usage definition of that word, and admitting that the broad definition of agnostic was the common usage definition of that word, promoted broadening the definition of atheist and narrowing the definition of agnostic. Smith rejects agnosticism as a third alternative to theism and atheism and promotes terms such as agnostic atheism (the view of those who do not "believe" in the existence of any deity, but do not claim to "know" if a deity does or does not exist) and agnostic theism (the view of those who do not claim to "know" of the existence of any deity, but still "believe" in such an existence).

"Agnostic" () was used by Thomas Henry Huxley in a speech at a meeting of the Metaphysical Society in 1869 to describe his philosophy, which rejects all claims of spiritual or mystical knowledge.

Early Christian church leaders used the Greek word "gnosis" (knowledge) to describe "spiritual knowledge". Agnosticism is not to be confused with religious views opposing the ancient religious movement of Gnosticism in particular; Huxley used the term in a broader, more abstract sense.
Huxley identified agnosticism not as a creed but rather as a method of skeptical, evidence-based inquiry.

In recent years, scientific literature dealing with neuroscience and psychology has used the word to mean "not knowable".
In technical and marketing literature, "agnostic" can also mean independence from some parameters—for example, "platform agnostic" (referring to cross-platform software)
or "hardware-agnostic".

Scottish Enlightenment philosopher David Hume contended that meaningful statements about the universe are always qualified by some degree of doubt. He asserted that the fallibility of human beings means that they cannot obtain absolute certainty except in trivial cases where a statement is true by definition (e.g. tautologies such as "all bachelors are unmarried" or "all triangles have three corners").


Throughout the history of Hinduism there has been a strong tradition of philosophic speculation and skepticism.

The Rig Veda takes an agnostic view on the fundamental question of how the universe and the gods were created. Nasadiya Sukta ("Creation Hymn") in the tenth chapter of the Rig Veda says:
Aristotle,
Anselm,
Aquinas,
Descartes,
and Gödel
presented arguments attempting to rationally prove the existence of God. The skeptical empiricism of David Hume, the antinomies of Immanuel Kant, and the existential philosophy of Søren Kierkegaard convinced many later philosophers to abandon these attempts, regarding it impossible to construct any unassailable proof for the existence or non-existence of God.

In his 1844 book, "Philosophical Fragments", Kierkegaard writes:
Hume was Huxley's favourite philosopher, calling him "the Prince of Agnostics". Diderot wrote to his mistress, telling of a visit by Hume to the Baron D'Holbach, and describing how a word for the position that Huxley would later describe as agnosticism didn't seem to exist, or at least wasn't common knowledge, at the time.

Raised in a religious environment, Charles Darwin (1809-1882) studied to be an Anglican clergyman. While eventually doubting parts of his faith, Darwin continued to help in church affairs, even while avoiding church attendance. Darwin stated that it would be "absurd to doubt that a man might be an ardent theist and an evolutionist". Although reticent about his religious views, in 1879 he wrote that "I have never been an atheist in the sense of denying the existence of a God. – I think that generally ... an agnostic would be the most correct description of my state of mind."

Agnostic views are as old as philosophical skepticism, but the terms agnostic and agnosticism were created by Huxley (1825-1895) to sum up his thoughts on contemporary developments of metaphysics about the "unconditioned" (William Hamilton) and the "unknowable" (Herbert Spencer). Though Huxley began to use the term "agnostic" in 1869, his opinions had taken shape some time before that date. In a letter of September 23, 1860, to Charles Kingsley, Huxley discussed his views extensively:
And again, to the same correspondent, May 6, 1863:
Of the origin of the name agnostic to describe this attitude, Huxley gave the following account:
In 1889, Huxley wrote:Therefore, although it be, as I believe, demonstrable that we have no real knowledge of the authorship, or of the date of composition of the Gospels, as they have come down to us, and that nothing better than more or less probable guesses can be arrived at on that subject.

William Stewart Ross (1844-1906) wrote under the name of Saladin. He was associated with Victorian Freethinkers and the organization the British Secular Union. He edited the "Secular Review" from 1882; it was renamed "Agnostic Journal and Eclectic Review" and closed in 1907. Ross championed agnosticism in opposition to the atheism of Charles Bradlaugh as an open-ended spiritual exploration.

In "Why I am an Agnostic" (c. 1889) he claims that agnosticism is "the very reverse of atheism".

Bertrand Russell (1872-1970) declared "Why I Am Not a Christian" in 1927, a classic statement of agnosticism.
He calls upon his readers to "stand on their own two feet and look fair and square at the world with a fearless attitude and a free intelligence".

In 1939, Russell gave a lecture on "The existence and nature of God", in which he characterized himself as an atheist. He said:
However, later in the same lecture, discussing modern non-anthropomorphic concepts of God, Russell states:
In Russell's 1947 pamphlet, "Am I An Atheist or an Agnostic?" (subtitled "A Plea For Tolerance in the Face of New Dogmas"), he ruminates on the problem of what to call himself:
In his 1953 essay, "What Is An Agnostic?" Russell states:
Later in the essay, Russell adds:
In 1965 Christian theologian Leslie Weatherhead (1893–1976) published "The Christian Agnostic", in which he argues:
Although radical and unpalatable to conventional theologians, Weatherhead's "agnosticism" falls far short of Huxley's, and short even of "weak agnosticism":

Robert G. Ingersoll (1833-1899), an Illinois lawyer and politician who evolved into a well-known and sought-after orator in 19th-century America, has been referred to as the "Great Agnostic".

In an 1896 lecture titled "Why I Am An Agnostic", Ingersoll related why he was an agnostic:
In the conclusion of the speech he simply sums up the agnostic position as:
In 1885 Ingersoll explained his comparative view of agnosticism and atheism as follows:

Canon Bernard Iddings Bell (1886-1958), a popular cultural commentator, Episcopal priest, and author, lauded the necessity of agnosticism in "Beyond Agnosticism: A Book for Tired Mechanists", calling it the foundation of "all intelligent Christianity." Agnosticism was a temporary mindset in which one rigorously questioned the truths of the age, including the way in which one believed God. His view of Robert Ingersoll and Thomas Paine was that they were not denouncing true Christianity but rather "a gross perversion of it." Part of the misunderstanding stemmed from ignorance of the concepts of God and religion. Historically, a god was any real, perceivable force that ruled the lives of humans and inspired admiration, love, fear, and homage; religion was the practice of it. Ancient peoples worshiped gods with real counterparts, such as Mammon (money and material things), Nabu (rationality), or Ba'al (violent weather); Bell argued that modern peoples were still paying homage—with their lives and their children's lives—to these old gods of wealth, physical appetites, and self-deification. Thus, if one attempted to be agnostic passively, he or she would incidentally join the worship of the world's gods.

In "Unfashionable Convictions (1931)," he criticized the Enlightenment's complete faith in human sensory perception, augmented by scientific instruments, as a means of accurately grasping Reality. Firstly, it was fairly new, an innovation of the Western World, which Aristotle invented and Thomas Aquinas revived among the scientific community. Secondly, the divorce of "pure" science from human experience, as manifested in American Industrialization, had completely altered the environment, often disfiguring it, so as to suggest its insufficiency to human needs. Thirdly, because scientists were constantly producing more data—to the point where no single human could grasp it all at once—it followed that human intelligence was incapable of attaining a complete understanding of universe; therefore, to admit the mysteries of the unobserved universe was to be "actually" scientific.

Bell believed that there were two other ways that humans could perceive and interact with the world. "Artistic experience" was how one expressed meaning through speaking, writing, painting, gesturing—any sort of communication which shared insight into a human's inner reality. "Mystical experience" was how one could "read" people and harmonize with them, being what we commonly call love. In summary, man was a scientist, artist, and lover. Without exercising all three, a person became "lopsided."

Bell considered a humanist to be a person who cannot rightly ignore the other ways of knowing. However, humanism, like agnosticism, was also temporal, and would eventually lead to either scientific materialism or theism. He lays out the following thesis:


Demographic research services normally do not differentiate between various types of non-religious respondents, so agnostics are often classified in the same category as atheists or other non-religious people.

A 2010 survey published in "Encyclopædia Britannica" found that the non-religious people or the agnostics made up about 9.6% of the world's population.
A November–December 2006 poll published in the "Financial Times" gives rates for the United States and five European countries. The rates of agnosticism in the United States were at 14%, while the rates of agnosticism in the European countries surveyed were considerably higher: Italy (20%), Spain (30%), Great Britain (35%), Germany (25%), and France (32%).

A study conducted by the Pew Research Center found that about 16% of the world's people, the third largest group after Christianity and Islam, have no religious affiliation.
According to a 2012 report by the Pew Research Center, agnostics made up 3.3% of the US adult population.
In the "U.S. Religious Landscape Survey", conducted by the Pew Research Center, 55% of agnostic respondents expressed "a belief in God or a universal spirit",
whereas 41% stated that they thought that they felt a tension "being non-religious in a society where most people are religious".

According to the 2011 Australian Bureau of Statistics, 22% of Australians have "no religion", a category that includes agnostics.
Between 64% and 65%
of Japanese and up to 81%
of Vietnamese are atheists, agnostics, or do not believe in a god. An official European Union survey reported that 3% of the EU population is unsure about their belief in a god or spirit.

Agnosticism is criticized from a variety of standpoints. Some religious thinkers see agnosticism as limiting the mind's capacity to know reality to materialism. Some atheists criticize the use of the term agnosticism as functionally indistinguishable from atheism; this results in frequent criticisms of those who adopt the term as avoiding the atheist label.

Theistic critics claim that agnosticism is impossible in practice, since a person can live only either as if God did not exist ("etsi deus non-daretur"), or as if God did exist ("etsi deus daretur").

According to Pope Benedict XVI, strong agnosticism in particular contradicts itself in affirming the power of reason to know scientific truth. He blames the exclusion of reasoning from religion and ethics for dangerous pathologies such as crimes against humanity and ecological disasters.
"Agnosticism", said Ratzinger, "is always the fruit of a refusal of that knowledge which is in fact offered to man ... The knowledge of God has always existed". He asserted that agnosticism is a choice of comfort, pride, dominion, and utility over truth, and is opposed by the following attitudes: the keenest self-criticism, humble listening to the whole of existence, the persistent patience and self-correction of the scientific method, a readiness to be purified by the truth.

The Catholic Church sees merit in examining what it calls "partial agnosticism", specifically those systems that "do not aim at constructing a complete philosophy of the unknowable, but at excluding special kinds of truth, notably religious, from the domain of knowledge". However, the Church is historically opposed to a full denial of the capacity of human reason to know God. The Council of the Vatican declares, "God, the beginning and end of all, can, by the natural light of human reason, be known with certainty from the works of creation".

Blaise Pascal argued that even if there were truly no evidence for God, agnostics should consider what is now known as Pascal's Wager: the infinite expected value of acknowledging God is always greater than the finite expected value of not acknowledging his existence, and thus it is a safer "bet" to choose God.

Peter Kreeft and Ronald Tacelli cited 20 arguments for God's existence, asserting that any demand for evidence testable in a laboratory is in effect asking God, the supreme being, to become man's servant.

According to Richard Dawkins, a distinction between agnosticism and atheism is unwieldy and depends on how close to zero a person is willing to rate the probability of existence for any given god-like entity. About himself, Dawkins continues, "I am agnostic only to the extent that I am agnostic about fairies at the bottom of the garden." Dawkins also identifies two categories of agnostics; "Temporary Agnostics in Practice" (TAPs), and "Permanent Agnostics in Principle" (PAPs). He states that "agnosticism about the existence of God belongs firmly in the temporary or TAP category. Either he exists or he doesn't. It is a scientific question; one day we may know the answer, and meanwhile we can say something pretty strong about the probability" and considers PAP a "deeply inescapable kind of fence-sitting".

A related concept is ignosticism, the view that a coherent definition of a deity must be put forward before the question of the existence of a deity can be meaningfully discussed. If the chosen definition is not coherent, the ignostic holds the noncognitivist view that the existence of a deity is meaningless or empirically untestable. A. J. Ayer, Theodore Drange, and other philosophers see both atheism and agnosticism as incompatible with ignosticism on the grounds that atheism and agnosticism accept "a deity exists" as a meaningful proposition that can be argued for or against.




</doc>
<doc id="896" url="https://en.wikipedia.org/wiki?curid=896" title="Argon">
Argon

Argon is a chemical element with the symbol Ar and atomic number 18. It is in group 18 of the periodic table and is a noble gas. Argon is the third-most abundant gas in the Earth's atmosphere, at 0.934% (9340 ppmv). It is more than twice as abundant as water vapor (which averages about 4000 ppmv, but varies greatly), 23 times as abundant as carbon dioxide (400 ppmv), and more than 500 times as abundant as neon (18 ppmv). Argon is the most abundant noble gas in Earth's crust, comprising 0.00015% of the crust.

Nearly all of the argon in the Earth's atmosphere is radiogenic argon-40, derived from the decay of potassium-40 in the Earth's crust. In the universe, argon-36 is by far the most common argon isotope, as it is the most easily produced by stellar nucleosynthesis in supernovas.

The name "argon" is derived from the Greek word , neuter singular form of meaning "lazy" or "inactive", as a reference to the fact that the element undergoes almost no chemical reactions. The complete octet (eight electrons) in the outer atomic shell makes argon stable and resistant to bonding with other elements. Its triple point temperature of 83.8058 K is a defining fixed point in the International Temperature Scale of 1990.

Argon is produced industrially by the fractional distillation of liquid air. Argon is mostly used as an inert shielding gas in welding and other high-temperature industrial processes where ordinarily unreactive substances become reactive; for example, an argon atmosphere is used in graphite electric furnaces to prevent the graphite from burning. Argon is also used in incandescent, fluorescent lighting, and other gas-discharge tubes. Argon makes a distinctive blue-green gas laser. Argon is also used in fluorescent glow starters.

Argon has approximately the same solubility in water as oxygen and is 2.5 times more soluble in water than nitrogen. Argon is colorless, odorless, nonflammable and nontoxic as a solid, liquid or gas. Argon is chemically inert under most conditions and forms no confirmed stable compounds at room temperature.

Although argon is a noble gas, it can form some compounds under various extreme conditions. Argon fluorohydride (HArF), a compound of argon with fluorine and hydrogen that is stable below , has been demonstrated. Although the neutral ground-state chemical compounds of argon are presently limited to HArF, argon can form clathrates with water when atoms of argon are trapped in a lattice of water molecules. Ions, such as , and excited-state complexes, such as ArF, have been demonstrated. Theoretical calculation predicts several more argon compounds that should be stable but have not yet been synthesized.

"Argon" (Greek , neuter singular form of meaning "lazy" or "inactive") is named in reference to its chemical inactivity. This chemical property of this first noble gas to be discovered impressed the namers. An unreactive gas was suspected to be a component of air by Henry Cavendish in 1785.

Argon was first isolated from air in 1894 by Lord Rayleigh and Sir William Ramsay at University College London by removing oxygen, carbon dioxide, water, and nitrogen from a sample of clean air. They first accomplished this by replicating an experiment of Henry Cavendish's. They trapped a mixture of atmospheric air with additional oxygen in a test-tube (A) upside-down over a large quantity of dilute alkali solution (B), which in Canvendish's original experiment was potassium hydroxide, and conveyed a current through wires insulated by U-shaped glass tubes (CC) which sealed around the platinum wire electrodes, leaving the ends of the wires (DD) exposed to the gas and insulated from the alkali solution. The arc was powered by a battery of five Grove cells and a Ruhmkorff coil of medium size. The alkali absorbed the oxides of nitrogen produced by the arc and also carbon dioxide. They operated the arc until no more reduction of volume of the gas could be seen for at least an hour or two and the spectral lines of nitrogen disappeared when the gas was examined. The remaining oxygen was reacted with alkaline pyrogallate to leave behind an apparently non-reactive gas which they called Argon.

Before isolating the gas, they had determined that nitrogen produced from chemical compounds was 0.5% lighter than nitrogen from the atmosphere. The difference was slight, but it was important enough to attract their attention for many months. They concluded that there was another gas in the air mixed in with the nitrogen. Argon was also encountered in 1882 through independent research of H. F. Newall and W. N. Hartley. Each observed new lines in the emission spectrum of air that did not match known elements.

Until 1957, the symbol for argon was "A", but now it is "Ar".

Argon constitutes 0.934% by volume and 1.288% by mass of the Earth's atmosphere, and air is the primary industrial source of purified argon products. Argon is isolated from air by fractionation, most commonly by cryogenic fractional distillation, a process that also produces purified nitrogen, oxygen, neon, krypton and xenon. The Earth's crust and seawater contain 1.2 ppm and 0.45 ppm of argon, respectively.

The main isotopes of argon found on Earth are (99.6%), (0.34%), and (0.06%). Naturally occurring , with a half-life of 1.25 years, decays to stable (11.2%) by electron capture or positron emission, and also to stable (88.8%) by beta decay. These properties and ratios are used to determine the age of rocks by K–Ar dating.

In the Earth's atmosphere, is made by cosmic ray activity, primarily by neutron capture of followed by two-neutron emission. In the subsurface environment, it is also produced through neutron capture by , followed by proton emission. is created from the neutron capture by followed by an alpha particle emission as a result of subsurface nuclear explosions. It has a half-life of 35 days.

Between locations in the Solar System, the isotopic composition of argon varies greatly. Where the major source of argon is the decay of in rocks, will be the dominant isotope, as it is on Earth. Argon produced directly by stellar nucleosynthesis is dominated by the alpha-process nuclide . Correspondingly, solar argon contains 84.6% (according to solar wind measurements), and the ratio of the three isotopes Ar : Ar : Ar in the atmospheres of the outer planets is 8400 : 1600 : 1. This contrasts with the low abundance of primordial in Earth's atmosphere, which is only 31.5 ppmv (= 9340 ppmv × 0.337%), comparable with that of neon (18.18 ppmv) on Earth and with interplanetary gasses, measured by probes.

The atmospheres of Mars, Mercury and Titan (the largest moon of Saturn) contain argon, predominantly as , and its content may be as high as 1.93% (Mars).

The predominance of radiogenic is the reason the standard atomic weight of terrestrial argon is greater than that of the next element, potassium, a fact that was puzzling when argon was discovered. Mendeleev positioned the elements on his periodic table in order of atomic weight, but the inertness of argon suggested a placement "before" the reactive alkali metal. Henry Moseley later solved this problem by showing that the periodic table is actually arranged in order of atomic number (see History of the periodic table).

Argon's complete octet of electrons indicates full s and p subshells. This full valence shell makes argon very stable and extremely resistant to bonding with other elements. Before 1962, argon and the other noble gases were considered to be chemically inert and unable to form compounds; however, compounds of the heavier noble gases have since been synthesized. The first argon compound with tungsten pentacarbonyl, W(CO)Ar, was isolated in 1975. However it was not widely recognised at that time. In August 2000, another argon compound, argon fluorohydride (HArF), was formed by researchers at the University of Helsinki, by shining ultraviolet light onto frozen argon containing a small amount of hydrogen fluoride with caesium iodide. This discovery caused the recognition that argon could form weakly bound compounds, even though it was not the first. It is stable up to 17 kelvin s (−256 °C). The metastable dication, which is valence-isoelectronic with carbonyl fluoride and phosgene, was observed in 2010. Argon-36, in the form of argon hydride (argonium) ions, has been detected in interstellar medium associated with the Crab Nebula supernova; this was the first noble-gas molecule detected in outer space.

Solid argon hydride (Ar(H)) has the same crystal structure as the MgZn Laves phase. It forms at pressures between 4.3 and 220 GPa, though Raman measurements suggest that the H molecules in Ar(H) dissociate above 175 GPa.

Argon is produced industrially by the fractional distillation of liquid air in a cryogenic air separation unit; a process that separates liquid nitrogen, which boils at 77.3 K, from argon, which boils at 87.3 K, and liquid oxygen, which boils at 90.2 K. About 700,000 tonnes of argon are produced worldwide every year.

Ar, the most abundant isotope of argon, is produced by the decay of K with a half-life of 1.25 years by electron capture or positron emission. Because of this, it is used in potassium–argon dating to determine the age of rocks.

Argon has several desirable properties:

Other noble gases would be equally suitable for most of these applications, but argon is by far the cheapest. Argon is inexpensive, since it occurs naturally in air and is readily obtained as a byproduct of cryogenic air separation in the production of liquid oxygen and liquid nitrogen: the primary constituents of air are used on a large industrial scale. The other noble gases (except helium) are produced this way as well, but argon is the most plentiful by far. The bulk of argon applications arise simply because it is inert and relatively cheap.

Argon is used in some high-temperature industrial processes where ordinarily non-reactive substances become reactive. For example, an argon atmosphere is used in graphite electric furnaces to prevent the graphite from burning.

For some of these processes, the presence of nitrogen or oxygen gases might cause defects within the material. Argon is used in some types of arc welding such as gas metal arc welding and gas tungsten arc welding, as well as in the processing of titanium and other reactive elements. An argon atmosphere is also used for growing crystals of silicon and germanium.
Argon is used in the poultry industry to asphyxiate birds, either for mass culling following disease outbreaks, or as a means of slaughter more humane than electric stunning. Argon is denser than air and displaces oxygen close to the ground during inert gas asphyxiation. Its non-reactive nature makes it suitable in a food product, and since it replaces oxygen within the dead bird, argon also enhances shelf life.

Argon is sometimes used for extinguishing fires where valuable equipment may be damaged by water or foam.

Liquid argon is used as the target for neutrino experiments and direct dark matter searches. The interaction between the hypothetical WIMPs and an argon nucleus produces scintillation light that is detected by photomultiplier tubes. Two-phase detectors containing argon gas are used to detect the ionized electrons produced during the WIMP–nucleus scattering. As with most other liquefied noble gases, argon has a high scintillation light yield (about 51 photons/keV), is transparent to its own scintillation light, and is relatively easy to purify. Compared to xenon, argon is cheaper and has a distinct scintillation time profile, which allows the separation of electronic recoils from nuclear recoils. On the other hand, its intrinsic beta-ray background is larger due to contamination, unless one uses argon from underground sources, which has much less contamination. Most of the argon in the Earth's atmosphere was produced by electron capture of long-lived ( + e → + ν) present in natural potassium within the Earth. The activity in the atmosphere is maintained by cosmogenic production through the knockout reaction (n,2n) and similar reactions. The half-life of is only 269 years. As a result, the underground Ar, shielded by rock and water, has much less contamination. Dark-matter detectors currently operating with liquid argon include DarkSide, WArP, ArDM, microCLEAN and DEAP. Neutrino experiments include ICARUS and MicroBooNE, both of which use high-purity liquid argon in a time projection chamber for fine grained three-dimensional imaging of neutrino interactions.

At Linköping University, Sweden, the inert gas is being utilized in a vacuum chamber in which plasma is introduced to ionize metallic films. This process results in a film usable for manufacturing computer processors. The new process would eliminate the need for chemical baths and use of expensive, dangerous and rare materials.

Argon is used to displace oxygen- and moisture-containing air in packaging material to extend the shelf-lives of the contents (argon has the European food additive code E938). Aerial oxidation, hydrolysis, and other chemical reactions that degrade the products are retarded or prevented entirely. High-purity chemicals and pharmaceuticals are sometimes packed and sealed in argon.

In winemaking, argon is used in a variety of activities to provide a barrier against oxygen at the liquid surface, which can spoil wine by fueling both microbial metabolism (as with acetic acid bacteria) and standard redox chemistry.

Argon is sometimes used as the propellant in aerosol cans.

Argon is also used as a preservative for such products as varnish, polyurethane, and paint, by displacing air to prepare a container for storage.

Since 2002, the American National Archives stores important national documents such as the Declaration of Independence and the Constitution within argon-filled cases to inhibit their degradation. Argon is preferable to the helium that had been used in the preceding five decades, because helium gas escapes through the intermolecular pores in most containers and must be regularly replaced.

Argon may be used as the inert gas within Schlenk lines and gloveboxes. Argon is preferred to less expensive nitrogen in cases where nitrogen may react with the reagents or apparatus.

Argon may be used as the carrier gas in gas chromatography and in electrospray ionization mass spectrometry; it is the gas of choice for the plasma used in ICP spectroscopy. Argon is preferred for the sputter coating of specimens for scanning electron microscopy. Argon gas is also commonly used for sputter deposition of thin films as in microelectronics and for wafer cleaning in microfabrication.

Cryosurgery procedures such as cryoablation use liquid argon to destroy tissue such as cancer cells. It is used in a procedure called "argon-enhanced coagulation", a form of argon plasma beam electrosurgery. The procedure carries a risk of producing gas embolism and has resulted in the death of at least one patient.

Blue argon lasers are used in surgery to weld arteries, destroy tumors, and correct eye defects.

Argon has also been used experimentally to replace nitrogen in the breathing or decompression mix known as Argox, to speed the elimination of dissolved nitrogen from the blood.

Incandescent lights are filled with argon, to preserve the filaments at high temperature from oxidation. It is used for the specific way it ionizes and emits light, such as in plasma globes and calorimetry in experimental particle physics. Gas-discharge lamps filled with pure argon provide lilac/violet light; with argon and some mercury, blue light. Argon is also used for blue and green argon-ion lasers.

Argon is used for thermal insulation in energy-efficient windows. Argon is also used in technical scuba diving to inflate a dry suit because it is inert and has low thermal conductivity.

Argon is used as a propellant in the development of the Variable Specific Impulse Magnetoplasma Rocket (VASIMR). Compressed argon gas is allowed to expand, to cool the seeker heads of some versions of the AIM-9 Sidewinder missile and other missiles that use cooled thermal seeker heads. The gas is stored at high pressure.

Argon-39, with a half-life of 269 years, has been used for a number of applications, primarily ice core and ground water dating. Also, potassium–argon dating and related argon-argon dating is used to date sedimentary, metamorphic, and igneous rocks.

Argon has been used by athletes as a doping agent to simulate hypoxic conditions. In 2014, the World Anti-Doping Agency (WADA) added argon and xenon to the list of prohibited substances and methods, although at this time there is no reliable test for abuse.

Although argon is non-toxic, it is 38% more dense than air and therefore considered a dangerous asphyxiant in closed areas. It is difficult to detect because it is colorless, odorless, and tasteless. A 1994 incident, in which a man was asphyxiated after entering an argon-filled section of oil pipe under construction in Alaska, highlights the dangers of argon tank leakage in confined spaces and emphasizes the need for proper use, storage and handling.




</doc>
<doc id="897" url="https://en.wikipedia.org/wiki?curid=897" title="Arsenic">
Arsenic

Arsenic is a chemical element with the symbol As and atomic number 33. Arsenic occurs in many minerals, usually in combination with sulfur and metals, but also as a pure elemental crystal. Arsenic is a metalloid. It has various allotropes, but only the gray form, which has a metallic appearance, is important to industry.

The primary use of arsenic is in alloys of lead (for example, in car batteries and ammunition). Arsenic is a common n-type dopant in semiconductor electronic devices, and the optoelectronic compound gallium arsenide is the second most commonly used semiconductor after doped silicon. Arsenic and its compounds, especially the trioxide, are used in the production of pesticides, treated wood products, herbicides, and insecticides. These applications are declining due to the toxicity of arsenic and its compounds.

A few species of bacteria are able to use arsenic compounds as respiratory metabolites. Trace quantities of arsenic are an essential dietary element in rats, hamsters, goats, chickens, and presumably other species. A role in human metabolism is not known. However, arsenic poisoning occurs in multicellular life if quantities are larger than needed. Arsenic contamination of groundwater is a problem that affects millions of people across the world.

The United States' Environmental Protection Agency states that all forms of arsenic are a serious risk to human health. The United States' Agency for Toxic Substances and Disease Registry ranked arsenic as number 1 in its 2001 Priority List of Hazardous Substances at Superfund sites. Arsenic is classified as a Group-A carcinogen.

The three most common arsenic allotropes are gray, yellow, and black arsenic, with gray being the most common. Gray arsenic (α-As, space group Rm No. 166) adopts a double-layered structure consisting of many interlocked, ruffled, six-membered rings. Because of weak bonding between the layers, gray arsenic is brittle and has a relatively low Mohs hardness of 3.5. Nearest and next-nearest neighbors form a distorted octahedral complex, with the three atoms in the same double-layer being slightly closer than the three atoms in the next. This relatively close packing leads to a high density of 5.73 g/cm. Gray arsenic is a semimetal, but becomes a semiconductor with a bandgap of 1.2–1.4 eV if amorphized. Gray arsenic is also the most stable form. 
Yellow arsenic is soft and waxy, and somewhat similar to tetraphosphorus (). Both have four atoms arranged in a tetrahedral structure in which each atom is bound to each of the other three atoms by a single bond. This unstable allotrope, being molecular, is the most volatile, least dense, and most toxic. Solid yellow arsenic is produced by rapid cooling of arsenic vapor, . It is rapidly transformed into gray arsenic by light. The yellow form has a density of 1.97 g/cm. Black arsenic is similar in structure to black phosphorus.
Black arsenic can also be formed by cooling vapor at around 100–220 °C and by crystallization of amorphous arsenic in the presence of mercury vapors. It is glassy and brittle. It is also a poor electrical conductor.

Arsenic occurs in nature as a monoisotopic element, composed of one stable isotope, As. As of 2003, at least 33 radioisotopes have also been synthesized, ranging in atomic mass from 60 to 92. The most stable of these is As with a half-life of 80.30 days. All other isotopes have half-lives of under one day, with the exception of As ("t"=65.30 hours), As ("t"=26.0 hours), As ("t"=17.77 days), As ("t"=1.0942 days), and As ("t"=38.83 hours). Isotopes that are lighter than the stable As tend to decay by β decay, and those that are heavier tend to decay by β decay, with some exceptions.

At least 10 nuclear isomers have been described, ranging in atomic mass from 66 to 84. The most stable of arsenic's isomers is As with a half-life of 111 seconds.

Arsenic has a similar electronegativity and ionization energies to its lighter congener phosphorus and as such readily forms covalent molecules with most of the nonmetals. Though stable in dry air, arsenic forms a golden-bronze tarnish upon exposure to humidity which eventually becomes a black surface layer. When heated in air, arsenic oxidizes to arsenic trioxide; the fumes from this reaction have an odor resembling garlic. This odor can be detected on striking arsenide minerals such as arsenopyrite with a hammer. It burns in oxygen to form arsenic trioxide and arsenic pentoxide, which have the same structure as the more well-known phosphorus compounds, and in fluorine to give arsenic pentafluoride. Arsenic (and some arsenic compounds) sublimes upon heating at atmospheric pressure, converting directly to a gaseous form without an intervening liquid state at . The triple point is 3.63 MPa and . Arsenic makes arsenic acid with concentrated nitric acid, arsenous acid with dilute nitric acid, and arsenic trioxide with concentrated sulfuric acid; however, it does not react with water, alkalis, or non-oxidising acids. Arsenic reacts with metals to form arsenides, though these are not ionic compounds containing the As ion as the formation of such an anion would be highly endothermic and even the group 1 arsenides have properties of intermetallic compounds. Like germanium, selenium, and bromine, which like arsenic succeed the 3d transition series, arsenic is much less stable in the group oxidation state of +5 than its vertical neighbors phosphorus and antimony, and hence arsenic pentoxide and arsenic acid are potent oxidizers.

Compounds of arsenic resemble in some respects those of phosphorus which occupies the same group (column) of the periodic table. The most common oxidation states for arsenic are: −3 in the arsenides, which are alloy-like intermetallic compounds, +3 in the arsenites, and +5 in the arsenates and most organoarsenic compounds. Arsenic also bonds readily to itself as seen in the square As ions in the mineral skutterudite. In the +3 oxidation state, arsenic is typically pyramidal owing to the influence of the lone pair of electrons.

One of the simplest arsenic compound is the trihydride, the highly toxic, flammable, pyrophoric arsine (AsH). This compound is generally regarded as stable, since at room temperature it decomposes only slowly. At temperatures of 250–300 °C decomposition to arsenic and hydrogen is rapid. Several factors, such as humidity, presence of light and certain catalysts (namely aluminium) facilitate the rate of decomposition. It oxidises readily in air to form arsenic trioxide and water, and analogous reactions take place with sulfur and selenium instead of oxygen.

Arsenic forms colorless, odorless, crystalline oxides AsO ("white arsenic") and AsO which are hygroscopic and readily soluble in water to form acidic solutions. Arsenic(V) acid is a weak acid and the salts are called arsenates, the most common arsenic contamination of groundwater, and a problem that affects many people. Synthetic arsenates include Scheele's Green (cupric hydrogen arsenate, acidic copper arsenate), calcium arsenate, and lead hydrogen arsenate. These three have been used as agricultural insecticides and poisons.

The protonation steps between the arsenate and arsenic acid are similar to those between phosphate and phosphoric acid. Unlike phosphorous acid, arsenous acid is genuinely tribasic, with the formula As(OH).

A broad variety of sulfur compounds of arsenic are known. Orpiment (AsS) and realgar (AsS) are somewhat abundant and were formerly used as painting pigments. In AsS, arsenic has a formal oxidation state of +2 in AsS which features As-As bonds so that the total covalency of As is still 3. Both orpiment and realgar, as well as AsS, have selenium analogs; the analogous AsTe is known as the mineral kalgoorlieite, and the anion AsTe is known as a ligand in cobalt complexes.

All trihalides of arsenic(III) are well known except the astatide, which is unknown. Arsenic pentafluoride (AsF) is the only important pentahalide, reflecting the lower stability of the +5 oxidation state; even so, it is a very strong fluorinating and oxidizing agent. (The pentachloride is stable only below −50 °C, at which temperature it decomposes to the trichloride, releasing chlorine gas.)

Arsenic is used as the group 5 element in the III-V semiconductors gallium arsenide, indium arsenide, and aluminium arsenide. The valence electron count of GaAs is the same as a pair of Si atoms, but the band structure is completely different which results in distinct bulk properties. Other arsenic alloys include the II-V semiconductor cadmium arsenide.

A large variety of organoarsenic compounds are known. Several were developed as chemical warfare agents during World War I, including vesicants such as lewisite and vomiting agents such as adamsite. Cacodylic acid, which is of historic and practical interest, arises from the methylation of arsenic trioxide, a reaction that has no analogy in phosphorus chemistry. Indeed, cacodyl was the first organometallic compound known (even though arsenic is not a true metal) and was named from the Greek "κακωδἰα" "stink" for its offensive odor; it is very poisonous.

Arsenic comprises about 1.5 ppm (0.00015%) of the Earth's crust, and is the 53rd most abundant element. Typical background concentrations of arsenic do not exceed 3 ng/m in the atmosphere; 100 mg/kg in soil; and 10 μg/L in freshwater.

Minerals with the formula MAsS and MAs (M = Fe, Ni, Co) are the dominant commercial sources of arsenic, together with realgar (an arsenic sulfide mineral) and native (elemental) arsenic. An illustrative mineral is arsenopyrite (FeAsS), which is structurally related to iron pyrite. Many minor As-containing minerals are known. Arsenic also occurs in various organic forms in the environment.

In 2014, China was the top producer of white arsenic with almost 70% world share, followed by Morocco, Russia, and Belgium, according to the British Geological Survey and the United States Geological Survey. Most arsenic refinement operations in the US and Europe have closed over environmental concerns. Arsenic is found in the smelter dust from copper, gold, and lead smelters, and is recovered primarily from copper refinement dust.

On roasting arsenopyrite in air, arsenic sublimes as arsenic(III) oxide leaving iron oxides, while roasting without air results in the production of gray arsenic. Further purification from sulfur and other chalcogens is achieved by sublimation in vacuum, in a hydrogen atmosphere, or by distillation from molten lead-arsenic mixture.

The word "arsenic" has its origin in the Syriac word "(al) zarniqa", from Arabic al-zarnīḵ ‘the orpiment’, based on Persian zar ‘gold’ from the word "zarnikh", meaning "yellow" (literally "gold-colored") and hence "(yellow) orpiment". It was adopted into Greek as "arsenikon" (), a form that is folk etymology, being the neuter form of the Greek word "arsenikos" (), meaning "male", "virile". The Greek word was adopted in Latin as "arsenicum", which in French became "arsenic", from which the English word arsenic is taken. Arsenic sulfides (orpiment, realgar) and oxides have been known and used since ancient times. Zosimos (circa 300 AD) describes roasting "sandarach" (realgar) to obtain "cloud of arsenic" (arsenic trioxide), which he then reduces to gray arsenic. As the symptoms of arsenic poisoning are not very specific, it was frequently used for murder until the advent of the Marsh test, a sensitive chemical test for its presence. (Another less sensitive but more general test is the Reinsch test.) Owing to its use by the ruling class to murder one another and its potency and discreetness, arsenic has been called the "poison of kings" and the "king of poisons".

During the Bronze Age, arsenic was often included in bronze, which made the alloy harder (so-called "arsenical bronze").
The isolation of arsenic was described by Jabir ibn Hayyan before 815 AD. Albertus Magnus (Albert the Great, 1193–1280) later isolated the element from a compound in 1250, by heating soap together with arsenic trisulfide. In 1649, Johann Schröder published two ways of preparing arsenic. Crystals of elemental (native) arsenic are found in nature, although rare.

Cadet's fuming liquid (impure cacodyl), often claimed as the first synthetic organometallic compound, was synthesized in 1760 by Louis Claude Cadet de Gassicourt by the reaction of potassium acetate with arsenic trioxide.

In the Victorian era, "arsenic" ("white arsenic" or arsenic trioxide) was mixed with vinegar and chalk and eaten by women to improve the complexion of their faces, making their skin paler to show they did not work in the fields. Arsenic was also rubbed into the faces and arms of women to "improve their complexion". The accidental use of arsenic in the adulteration of foodstuffs led to the Bradford sweet poisoning in 1858, which resulted in around 20 deaths. Wallpaper production also began to use dyes made from arsenic, which was thought to increase the pigment's brightness.

Two arsenic pigments have been widely used since their discovery – Paris Green and Scheele's Green. After the toxicity of arsenic became widely known, these chemicals were used less often as pigments and more often as insecticides. In the 1860s, an arsenic byproduct of dye production, London Purple was widely used. This was a solid mixture of arsenic trioxide, aniline, lime, and ferrous oxide, insoluble in water and very toxic by inhalation or ingestion But it was later replaced with Paris Green, another arsenic-based dye. With better understanding of the toxicology mechanism, two other compounds were used starting in the 1890s. Arsenite of lime and arsenate of lead were used widely as insecticides until the discovery of DDT in 1942.

The toxicity of arsenic to insects, bacteria, and fungi led to its use as a wood preservative. In the 1930s, a process of treating wood with chromated copper arsenate (also known as CCA or Tanalith) was invented, and for decades, this treatment was the most extensive industrial use of arsenic. An increased appreciation of the toxicity of arsenic led to a ban of CCA in consumer products in 2004, initiated by the European Union and United States. However, CCA remains in heavy use in other countries (such as on Malaysian rubber plantations).

Arsenic was also used in various agricultural insecticides and poisons. For example, lead hydrogen arsenate was a common insecticide on fruit trees, but contact with the compound sometimes resulted in brain damage among those working the sprayers. In the second half of the 20th century, monosodium methyl arsenate (MSMA) and disodium methyl arsenate (DSMA) – less toxic organic forms of arsenic – replaced lead arsenate in agriculture. These organic arsenicals were in turn phased out by 2013 in all agricultural activities except cotton farming.

The biogeochemistry of arsenic is complex and includes various adsorption and desorption processes. The toxicity of arsenic is connected to its solubility and is affected by pH. Arsenite () is more soluble than arsenate () and is more toxic; however, at a lower pH, arsenate becomes more mobile and toxic. It was found that addition of sulfur, phosphorus, and iron oxides to high-arsenite soils greatly reduces arsenic phytotoxicity.

Arsenic is used as a feed additive in poultry and swine production, in particular in the U.S. to increase weight gain, improve feed efficiency, and to prevent disease. An example is roxarsone, which had been used as a broiler starter by about 70% of U.S. broiler growers. Alpharma, a subsidiary of Pfizer Inc., which produces roxarsone, voluntarily suspended sales of the drug in response to studies showing elevated levels of inorganic arsenic, a carcinogen, in treated chickens. A successor to Alpharma, Zoetis, continues to sell nitarsone, primarily for use in turkeys.

Arsenic is intentionally added to the feed of chickens raised for human consumption. Organic arsenic compounds are less toxic than pure arsenic, and promote the growth of chickens. Under some conditions, the arsenic in chicken feed is converted to the toxic inorganic form.

A 2006 study of the remains of the Australian racehorse, Phar Lap, determined that the 1932 death of the famous champion was caused by a massive overdose of arsenic. Sydney veterinarian Percy Sykes stated, "In those days, arsenic was quite a common tonic, usually given in the form of a solution (Fowler's Solution) ... It was so common that I'd reckon 90 per cent of the horses had arsenic in their system."

During the 18th, 19th, and 20th centuries, a number of arsenic compounds were used as medicines, including arsphenamine (by Paul Ehrlich) and arsenic trioxide (by Thomas Fowler). Arsphenamine, as well as neosalvarsan, was indicated for syphilis, but has been superseded by modern antibiotics. However, arsenicals such as melarsoprol are still used for the treatment of trypanosomiasis, since although these drugs suffer from severe toxicity, the disease is almost uniformly fatal if untreated.

Arsenic trioxide has been used in a variety of ways over the past 500 years, most commonly in the treatment of cancer, but also in medications as diverse as Fowler's solution in psoriasis. The US Food and Drug Administration in the year 2000 approved this compound for the treatment of patients with acute promyelocytic leukemia that is resistant to all-trans retinoic acid.

Recently, researchers have been locating tumors using arsenic-74 (a positron emitter). This isotope produces clearer PET scan images than the previous radioactive agent, iodine-124, because the body tends to transport iodine to the thyroid gland producing signal noise.Nanoparticles of arsenic has shown ability to kill cancer cells with lesser cytotoxicity than other arsenic formulations.

In subtoxic doses, soluble arsenic compounds act as stimulants, and were once popular in small doses as medicine by people in the mid-18th to 19th centuries.

The main use of arsenic is in alloying with lead. Lead components in car batteries are strengthened by the presence of a very small percentage of arsenic. Dezincification of brass (a copper-zinc alloy) is greatly reduced by the addition of arsenic. "Phosphorus Deoxidized Arsenical Copper" with an arsenic content of 0.3% has an increased corrosion stability in certain environments. Gallium arsenide is an important semiconductor material, used in integrated circuits. Circuits made from GaAs are much faster (but also much more expensive) than those made from silicon. Unlike silicon, GaAs has a direct bandgap, and can be used in laser diodes and LEDs to convert electrical energy directly into light.

After World War I, the United States built a stockpile of 20,000 tons of weaponized lewisite (ClCH=CHAsCl), an organoarsenic vesicant (blister agent) and lung irritant. The stockpile was neutralized with bleach and dumped into the Gulf of Mexico in the 1950s. During the Vietnam War, the United States used Agent Blue, a mixture of sodium cacodylate and its acid form, as one of the rainbow herbicides to deprive North Vietnamese soldiers of foliage cover and rice.


Some species of bacteria obtain their energy in the absence of oxygen by oxidizing various fuels while reducing arsenate to arsenite. Under oxidative environmental conditions some bacteria use arsenite as fuel, which they oxidize to arsenate. The enzymes involved are known as arsenate reductases (Arr).

In 2008, bacteria were discovered that employ a version of photosynthesis in the absence of oxygen with arsenites as electron donors, producing arsenates (just as ordinary photosynthesis uses water as electron donor, producing molecular oxygen). Researchers conjecture that, over the course of history, these photosynthesizing organisms produced the arsenates that allowed the arsenate-reducing bacteria to thrive. One strain PHS-1 has been isolated and is related to the gammaproteobacterium "Ectothiorhodospira shaposhnikovii". The mechanism is unknown, but an encoded Arr enzyme may function in reverse to its known homologues.

In 2011, it was postulated that a strain of "Halomonadaceae" was able to be grown in the absence of phosphorus by substituting it with arsenic, exploiting the fact that the arsenate and phosphate anions are similar structurally. The study was widely criticised and subsequently refuted by independent researcher groups.

Some evidence indicates that arsenic is an essential trace mineral in birds (chickens), and in mammals (rats, hamsters, and goats). However, the biological function is not known.

Arsenic has been linked to epigenetic changes, heritable changes in gene expression that occur without changes in DNA sequence. These include DNA methylation, histone modification, and RNA interference. Toxic levels of arsenic cause significant DNA hypermethylation of tumor suppressor genes p16 and p53, thus increasing risk of carcinogenesis. These epigenetic events have been studied "in vitro" using human kidney cells and "in vivo" using rat liver cells and peripheral blood leukocytes in humans. Inductively coupled plasma mass spectrometry (ICP-MS) is used to detect precise levels of intracellular arsenic and other arsenic bases involved in epigenetic modification of DNA. Studies investigating arsenic as an epigenetic factor can be used to develop precise biomarkers of exposure and susceptibility.

The Chinese brake fern ("Pteris vittata") hyperaccumulates arsenic from the soil into its leaves and has a proposed use in phytoremediation.

Inorganic arsenic and its compounds, upon entering the food chain, are progressively metabolized through a process of methylation. For example, the mold "Scopulariopsis brevicaulis" produces significant amounts of trimethylarsine if inorganic arsenic is present. The organic compound arsenobetaine is found in some marine foods such as fish and algae, and also in mushrooms in larger concentrations. The average person's intake is about 10–50 µg/day. Values about 1000 µg are not unusual following consumption of fish or mushrooms, but there is little danger in eating fish because this arsenic compound is nearly non-toxic.

Naturally occurring sources of human exposure include volcanic ash, weathering of minerals and ores, and mineralized groundwater. Arsenic is also found in food, water, soil, and air. Arsenic is absorbed by all plants, but is more concentrated in leafy vegetables, rice, apple and grape juice, and seafood. An additional route of exposure is inhalation of atmospheric gases and dusts.
During the Victorian era, arsenic was widely used in home decor, especially wallpapers.

Extensive arsenic contamination of groundwater has led to widespread arsenic poisoning in Bangladesh and neighboring countries. It is estimated that approximately 57 million people in the Bengal basin are drinking groundwater with arsenic concentrations elevated above the World Health Organization's standard of 10 parts per billion (ppb). However, a study of cancer rates in Taiwan suggested that significant increases in cancer mortality appear only at levels above 150 ppb. The arsenic in the groundwater is of natural origin, and is released from the sediment into the groundwater, caused by the anoxic conditions of the subsurface. This groundwater was used after local and western NGOs and the Bangladeshi government undertook a massive shallow tube well drinking-water program in the late twentieth century. This program was designed to prevent drinking of bacteria-contaminated surface waters, but failed to test for arsenic in the groundwater. Many other countries and districts in Southeast Asia, such as Vietnam and Cambodia, have geological environments that produce groundwater with a high arsenic content. was reported in Nakhon Si Thammarat, Thailand in 1987, and the Chao Phraya River probably contains high levels of naturally occurring dissolved arsenic without being a public health problem because much of the public uses bottled water. In Pakistan, more than 60 million people are exposed to arsenic polluted drinking water indicated by a recent report of Science. Podgorski's team investigated more than 1200 samples and more than 66% samples exceeded the WHO minimum contamination level.

In the United States, arsenic is most commonly found in the ground waters of the southwest. Parts of New England, Michigan, Wisconsin, Minnesota and the Dakotas are also known to have significant concentrations of arsenic in ground water. Increased levels of skin cancer have been associated with arsenic exposure in Wisconsin, even at levels below the 10 part per billion drinking water standard. According to a recent film funded by the US Superfund, millions of private wells have unknown arsenic levels, and in some areas of the US, more than 20% of the wells may contain levels that exceed established limits.

Low-level exposure to arsenic at concentrations of 100 parts per billion (i.e., above the 10 parts per billion drinking water standard) compromises the initial immune response to H1N1 or swine flu infection according to NIEHS-supported scientists. The study, conducted in laboratory mice, suggests that people exposed to arsenic in their drinking water may be at increased risk for more serious illness or death from the virus.

Some Canadians are drinking water that contains inorganic arsenic. Private-dug–well waters are most at risk for containing inorganic arsenic. Preliminary well water analysis typically does not test for arsenic. Researchers at the Geological Survey of Canada have modeled relative variation in natural arsenic hazard potential for the province of New Brunswick. This study has important implications for potable water and health concerns relating to inorganic arsenic.

Epidemiological evidence from Chile shows a dose-dependent connection between chronic arsenic exposure and various forms of cancer, in particular when other risk factors, such as cigarette smoking, are present. These effects have been demonstrated at contaminations less than 50 ppb. Arsenic is itself a constituent of tobacco smoke.

Analyzing multiple epidemiological studies on inorganic arsenic exposure suggests a small but measurable increase in risk for bladder cancer at 10 ppb. According to Peter Ravenscroft of the Department of Geography at the University of Cambridge, roughly 80 million people worldwide consume between 10 and 50 ppb arsenic in their drinking water. If they all consumed exactly 10 ppb arsenic in their drinking water, the previously cited multiple epidemiological study analysis would predict an additional 2,000 cases of bladder cancer alone. This represents a clear underestimate of the overall impact, since it does not include lung or skin cancer, and explicitly underestimates the exposure. Those exposed to levels of arsenic above the current WHO standard should weigh the costs and benefits of arsenic remediation.
Early (1973) evaluations of the processes for removing dissolved arsenic from drinking water demonstrated the efficacy of co-precipitation with either iron or aluminum oxides. In particular, iron as a coagulant was found to remove arsenic with an efficacy exceeding 90%. Several adsorptive media systems have been approved for use at point-of-service in a study funded by the United States Environmental Protection Agency (US EPA) and the National Science Foundation (NSF). A team of European and Indian scientists and engineers have set up six arsenic treatment plants in West Bengal based on in-situ remediation method (SAR Technology). This technology does not use any chemicals and arsenic is left in an insoluble form (+5 state) in the subterranean zone by recharging aerated water into the aquifer and developing an oxidation zone that supports arsenic oxidizing micro-organisms. This process does not produce any waste stream or sludge and is relatively cheap.

Another effective and inexpensive method to avoid arsenic contamination is to sink wells 500 feet or deeper to reach purer waters. A recent 2011 study funded by the US National Institute of Environmental Health Sciences' Superfund Research Program shows that deep sediments can remove arsenic and take it out of circulation. In this process, called "adsorption", arsenic sticks to the surfaces of deep sediment particles and is naturally removed from the ground water.

Magnetic separations of arsenic at very low magnetic field gradients with high-surface-area and monodisperse magnetite (FeO) nanocrystals have been demonstrated in point-of-use water purification. Using the high specific surface area of FeO nanocrystals, the mass of waste associated with arsenic removal from water has been dramatically reduced.

Epidemiological studies have suggested a correlation between chronic consumption of drinking water contaminated with arsenic and the incidence of all leading causes of mortality. The literature indicates that arsenic exposure is causative in the pathogenesis of diabetes.

Chaff-based filters have recently been shown to reduce the arsenic content of water to 3 µg/L. This may find applications in areas where the potable water is extracted from underground aquifers.

For several centuries, the people of San Pedro de Atacama in Chile have been drinking water that is contaminated with arsenic, and some evidence suggests they have developed some immunity.

Around one-third of the world's population drinks water from groundwater resources. Of this, about 10 percent, approximately 300 million people, obtains water from groundwater resources that are contaminated with unhealthy levels of arsenic or fluoride. These trace elements derive mainly from minerals and ions in the ground.

Arsenic is unique among the trace metalloids and oxyanion-forming trace metals (e.g. As, Se, Sb, Mo, V, Cr, U, Re). It is sensitive to mobilization at pH values typical of natural waters (pH 6.5–8.5) under both oxidizing and reducing conditions. Arsenic can occur in the environment in several oxidation states (−3, 0, +3 and +5), but in natural waters it is mostly found in inorganic forms as oxyanions of trivalent arsenite [As(III)] or pentavalent arsenate [As(V)]. Organic forms of arsenic are produced by biological activity, mostly in surface waters, but are rarely quantitatively important. Organic arsenic compounds may, however, occur where waters are significantly impacted by industrial pollution.

Arsenic may be solubilized by various processes. When pH is high, arsenic may be released from surface binding sites that lose their positive charge. When water level drops and sulfide minerals are exposed to air, arsenic trapped in sulfide minerals can be released into water. When organic carbon is present in water, bacteria are fed by directly reducing As(V) to As(III) or by reducing the element at the binding site, releasing inorganic arsenic.

The aquatic transformations of arsenic are affected by pH, reduction-oxidation potential, organic matter concentration and the concentrations and forms of other elements, especially iron and manganese. The main factors are pH and the redox potential. Generally, the main forms of arsenic under oxic conditions are HAsO, HAsO, HAsO, and AsO at pH 2, 2–7, 7–11 and 11, respectively. Under reducing conditions, HAsO is predominant at pH 2–9.

Oxidation and reduction affects the migration of arsenic in subsurface environments. Arsenite is the most stable soluble form of arsenic in reducing environments and arsenate, which is less mobile than arsenite, is dominant in oxidizing environments at neutral pH. Therefore, arsenic may be more mobile under reducing conditions. The reducing environment is also rich in organic matter which may enhance the solubility of arsenic compounds. As a result, the adsorption of arsenic is reduced and dissolved arsenic accumulates in groundwater. That is why the arsenic content is higher in reducing environments than in oxidizing environments.

The presence of sulfur is another factor that affects the transformation of arsenic in natural water. Arsenic can precipitate when metal sulfides form. In this way, arsenic is removed from the water and its mobility decreases. When oxygen is present, bacteria oxidize reduced sulfur to generate energy, potentially releasing bound arsenic.

Redox reactions involving Fe also appear to be essential factors in the fate of arsenic in aquatic systems. The reduction of iron oxyhydroxides plays a key role in the release of arsenic to water. So arsenic can be enriched in water with elevated Fe concentrations. Under oxidizing conditions, arsenic can be mobilized from pyrite or iron oxides especially at elevated pH. Under reducing conditions, arsenic can be mobilized by reductive desorption or dissolution when associated with iron oxides. The reductive desorption occurs under two circumstances. One is when arsenate is reduced to arsenite which adsorbs to iron oxides less strongly. The other results from a change in the charge on the mineral surface which leads to the desorption of bound arsenic.

Some species of bacteria catalyze redox transformations of arsenic. Dissimilatory arsenate-respiring prokaryotes (DARP) speed up the reduction of As(V) to As(III). DARP use As(V) as the electron acceptor of anaerobic respiration and obtain energy to survive. Other organic and inorganic substances can be oxidized in this process. Chemoautotrophic arsenite oxidizers (CAO) and heterotrophic arsenite oxidizers (HAO) convert As(III) into As(V). CAO combine the oxidation of As(III) with the reduction of oxygen or nitrate. They use obtained energy to fix produce organic carbon from CO. HAO cannot obtain energy from As(III) oxidation. This process may be an arsenic detoxification mechanism for the bacteria.

Equilibrium thermodynamic calculations predict that As(V) concentrations should be greater than As(III) concentrations in all but strongly reducing conditions, i.e. where SO reduction is occurring. However, abiotic redox reactions of arsenic are slow. Oxidation of As(III) by dissolved O is a particularly slow reaction. For example, Johnson and Pilson (1975) gave half-lives for the oxygenation of As(III) in seawater ranging from several months to a year. In other studies, As(V)/As(III) ratios were stable over periods of days or weeks during water sampling when no particular care was taken to prevent oxidation, again suggesting relatively slow oxidation rates. Cherry found from experimental studies that the As(V)/As(III) ratios were stable in anoxic solutions for up to 3 weeks but that gradual changes occurred over longer timescales. Sterile water samples have been observed to be less susceptible to speciation changes than non-sterile samples. Oremland found that the reduction of As(V) to As(III) in Mono Lake was rapidly catalyzed by bacteria with rate constants ranging from 0.02 to 0.3 day.

As of 2002, US-based industries consumed 19,600 metric tons of arsenic. Ninety percent of this was used for treatment of wood with chromated copper arsenate (CCA). In 2007, 50% of the 5,280 metric tons of consumption was still used for this purpose. In the United States, the voluntary phasing-out of arsenic in production of consumer products and residential and general consumer construction products began on 31 December 2003, and alternative chemicals are now used, such as Alkaline Copper Quaternary, borates, copper azole, cyproconazole, and propiconazole.

Although discontinued, this application is also one of the most concerning to the general public. The vast majority of older pressure-treated wood was treated with CCA. CCA lumber is still in widespread use in many countries, and was heavily used during the latter half of the 20th century as a structural and outdoor building material. Although the use of CCA lumber was banned in many areas after studies showed that arsenic could leach out of the wood into the surrounding soil (from playground equipment, for instance), a risk is also presented by the burning of older CCA timber. The direct or indirect ingestion of wood ash from burnt CCA lumber has caused fatalities in animals and serious poisonings in humans; the lethal human dose is approximately 20 grams of ash. Scrap CCA lumber from construction and demolition sites may be inadvertently used in commercial and domestic fires. Protocols for safe disposal of CCA lumber are not consistent throughout the world. Widespread landfill disposal of such timber raises some concern, but other studies have shown no arsenic contamination in the groundwater.

One tool that maps the location (and other information) of arsenic releases in the United State is TOXMAP. TOXMAP is a Geographic Information System (GIS) from the Division of Specialized Information Services of the United States National Library of Medicine (NLM) funded by the US Federal Government. With marked-up maps of the United States, TOXMAP enables users to visually explore data from the United States Environmental Protection Agency's (EPA) Toxics Release Inventory and Superfund Basic Research Programs. TOXMAP's chemical and environmental health information is taken from NLM's Toxicology Data Network (TOXNET), PubMed, and from other authoritative sources.

Physical, chemical, and biological methods have been used to remediate arsenic contaminated water. Bioremediation is said to be cost-effective and environmentally friendly. Bioremediation of ground water contaminated with arsenic aims to convert arsenite, the toxic form of arsenic to humans, to arsenate. Arsenate (+5 oxidation state) is the dominant form of arsenic in surface water, while arsenite (+3 oxidation state) is the dominant form in hypoxic to anoxic environments. Arsenite is more soluble and mobile than arsenate. Many species of bacteria can transform arsenite to arsenate in anoxic conditions by using arsenite as an electron donor. This is a useful method in ground water remediation. Another bioremediation strategy is to use plants that accumulate arsenic in their tissues via phytoremediation but the disposal of contaminated plant material needs to be considered.

Bioremediation requires careful evaluation and design in accordance with existing conditions. Some sites may require the addition of an electron acceptor while others require microbe supplementation (bioaugmentation). Regardless of the method used, only constant monitoring can prevent future contamination.

Arsenic and many of its compounds are especially potent poisons.

Elemental arsenic and arsenic sulfate and trioxide compounds are classified as "toxic" and "dangerous for the environment" in the European Union under directive 67/548/EEC.

The International Agency for Research on Cancer (IARC) recognizes arsenic and inorganic arsenic compounds as group 1 carcinogens, and the EU lists arsenic trioxide, arsenic pentoxide, and arsenate salts as category 1 carcinogens.

Arsenic is known to cause when present in drinking water, "the most common species being arsenate [; As(V)] and arsenite [HAsO; As(III)]".

In the United States since 2006, the maximum concentration in drinking water allowed by the Environmental Protection Agency (EPA) is 10 ppb and the FDA set the same standard in 2005 for bottled water. The Department of Environmental Protection for New Jersey set a drinking water limit of 5 ppb in 2006. The IDLH (immediately dangerous to life and health) value for arsenic metal and inorganic arsenic compounds is 5 mg/m (5 ppb). The Occupational Safety and Health Administration has set the permissible exposure limit (PEL) to a time-weighted average (TWA) of 0.01 mg/m (0.01 ppb), and the National Institute for Occupational Safety and Health (NIOSH) has set the recommended exposure limit (REL) to a 15-minute constant exposure of 0.002 mg/m (0.002 ppb). The PEL for organic arsenic compounds is a TWA of 0.5 mg/m. (0.5 ppb).

In 2008, based on its ongoing testing of a wide variety of American foods for toxic chemicals, the U.S. Food and Drug Administration set the "level of concern" for inorganic arsenic in apple and pear juices at 23 ppb, based on non-carcinogenic effects, and began blocking importation of products in excess of this level; it also required recalls for non-conforming domestic products. In 2011, the national "Dr. Oz" television show broadcast a program highlighting tests performed by an independent lab hired by the producers. Though the methodology was disputed (it did not distinguish between organic and inorganic arsenic) the tests showed levels of arsenic up to 36 ppb. In response, FDA tested the worst brand from the "Dr." "Oz" show and found much lower levels. Ongoing testing found 95% of the apple juice samples were below the level of concern. Later testing by Consumer Reports showed inorganic arsenic at levels slightly above 10 ppb, and the organization urged parents to reduce consumption. In July 2013, on consideration of consumption by children, chronic exposure, and carcinogenic effect, the FDA established an "action level" of 10 ppb for apple juice, the same as the drinking water standard.

Concern about arsenic in rice in Bangladesh was raised in 2002, but at the time only Australia had a legal limit for food (one milligram per kilogram). Concern was raised about people who were eating U.S. rice exceeding WHO standards for personal arsenic intake in 2005. In 2011, the People's Republic of China set a food standard of 150 ppb for arsenic.

In the United States in 2012, testing by separate groups of researchers at the Children's Environmental Health and Disease Prevention Research Center at Dartmouth College (early in the year, focusing on urinary levels in children) and Consumer Reports (in November) found levels of arsenic in rice that resulted in calls for the FDA to set limits. The FDA released some testing results in September 2012, and as of July 2013, is still collecting data in support of a new potential regulation. It has not recommended any changes in consumer behavior.

Consumer Reports recommended: 
A 2014 World Health Organization advisory conference was scheduled to consider limits of 200–300 ppb for rice.

Arsenic is bioaccumulative in many organisms, marine species in particular, but it does not appear to biomagnify significantly in food webs. In polluted areas, plant growth may be affected by root uptake of arsenate, which is a phosphate analog and therefore readily transported in plant tissues and cells. In polluted areas, uptake of the more toxic arsenite ion (found more particularly in reducing conditions) is likely in poorly-drained soils.

Arsenic's toxicity comes from the affinity of arsenic(III) oxides for thiols. Thiols, in the form of cysteine residues and cofactors such as lipoic acid and coenzyme A, are situated at the active sites of many important enzymes.

Arsenic disrupts ATP production through several mechanisms. At the level of the citric acid cycle, arsenic inhibits lipoic acid, which is a cofactor for pyruvate dehydrogenase. By competing with phosphate, arsenate uncouples oxidative phosphorylation, thus inhibiting energy-linked reduction of NAD+, mitochondrial respiration and ATP synthesis. Hydrogen peroxide production is also increased, which, it is speculated, has potential to form reactive oxygen species and oxidative stress. These metabolic interferences lead to death from multi-system organ failure. The organ failure is presumed to be from necrotic cell death, not apoptosis, since energy reserves have been too depleted for apoptosis to occur.

Occupational exposure and arsenic poisoning may occur in persons working in industries involving the use of inorganic arsenic and its compounds, such as wood preservation, glass production, nonferrous metal alloys, and electronic semiconductor manufacturing. Inorganic arsenic is also found in coke oven emissions associated with the smelter industry.

The conversion between As(III) and As(V) is a large factor in arsenic environmental contamination. According to Croal, Gralnick, Malasarn and Newman, "[the] understanding [of] what stimulates As(III) oxidation and/or limits As(V) reduction is relevant for bioremediation of contaminated sites (Croal). The study of chemolithoautotrophic As(III) oxidizers and the heterotrophic As(V) reducers can help the understanding of the oxidation and/or reduction of arsenic.

Treatment of chronic arsenic poisoning is possible. British anti-lewisite (dimercaprol) is prescribed in doses of 5 mg/kg up to 300 mg every 4 hours for the first day, then every 6 hours for the second day, and finally every 8 hours for 8 additional days. However the USA's Agency for Toxic Substances and Disease Registry (ATSDR) states that the long-term effects of arsenic exposure cannot be predicted. Blood, urine, hair, and nails may be tested for arsenic; however, these tests cannot foresee possible health outcomes from the exposure. Long-term exposure and consequent excretion through urine has been linked to bladder and kidney cancer in addition to cancer of the liver, prostate, skin, lungs, and nasal cavity.




</doc>
<doc id="898" url="https://en.wikipedia.org/wiki?curid=898" title="Antimony">
Antimony

Antimony is a chemical element with the symbol Sb (from ) and atomic number 51. A lustrous gray metalloid, it is found in nature mainly as the sulfide mineral stibnite (SbS). Antimony compounds have been known since ancient times and were powdered for use as medicine and cosmetics, often known by the Arabic name kohl. Metallic antimony was also known, but it was erroneously identified as lead upon its discovery. The earliest known description of the metal in the West was written in 1540 by Vannoccio Biringuccio.

For some time, China has been the largest producer of antimony and its compounds, with most production coming from the Xikuangshan Mine in Hunan. The industrial methods for refining antimony are roasting and reduction with carbon or direct reduction of stibnite with iron.

The largest applications for metallic antimony are an alloy with lead and tin and the lead antimony plates in lead–acid batteries. Alloys of lead and tin with antimony have improved properties for solders, bullets, and plain bearings. Antimony compounds are prominent additives for chlorine and bromine-containing fire retardants found in many commercial and domestic products. An emerging application is the use of antimony in microelectronics.

Antimony is a member of group 15 of the periodic table, one of the elements called pnictogens, and has an electronegativity of 2.05. In accordance with periodic trends, it is more electronegative than tin or bismuth, and less electronegative than tellurium or arsenic. Antimony is stable in air at room temperature, but reacts with oxygen if heated to produce antimony trioxide, SbO.

Antimony is a silvery, lustrous gray metalloid with a Mohs scale hardness of 3, which is too soft to make hard objects; coins of antimony were issued in China's Guizhou province in 1931 but the durability was poor and the minting was soon discontinued. Antimony is resistant to attack by acids.

Four allotropes of antimony are known: a stable metallic form and three metastable forms (explosive, black and yellow). Elemental antimony is a brittle, silver-white shiny metalloid. When slowly cooled, molten antimony crystallizes in a trigonal cell, isomorphic with the gray allotrope of arsenic. A rare explosive form of antimony can be formed from the electrolysis of antimony trichloride. When scratched with a sharp implement, an exothermic reaction occurs and white fumes are given off as metallic antimony forms; when rubbed with a pestle in a mortar, a strong detonation occurs. Black antimony is formed upon rapid cooling of antimony vapor. It has the same crystal structure as red phosphorus and black arsenic; it oxidizes in air and may ignite spontaneously. At 100 °C, it gradually transforms into the stable form. The yellow allotrope of antimony is the most unstable. It has only been generated by oxidation of stibine (SbH) at −90 °C. Above this temperature and in ambient light, this metastable allotrope transforms into the more stable black allotrope.

Elemental antimony adopts a layered structure (space group Rm No. 166) in which layers consist of fused, ruffled, six-membered rings. The nearest and next-nearest neighbors form an irregular octahedral complex, with the three atoms in each double layer slightly closer than the three atoms in the next. This relatively close packing leads to a high density of 6.697 g/cm, but the weak bonding between the layers leads to the low hardness and brittleness of antimony.

Antimony has two stable isotopes: Sb with a natural abundance of 57.36% and Sb with a natural abundance of 42.64%. It also has 35 radioisotopes, of which the longest-lived is Sb with a half-life of 2.75 years. In addition, 29 metastable states have been characterized. The most stable of these is Sb with a half-life of 5.76 days. Isotopes that are lighter than the stable Sb tend to decay by β decay, and those that are heavier tend to decay by β decay, with some exceptions.

The abundance of antimony in the Earth's crust is estimated to be 0.2 to 0.5 parts per million, comparable to thallium at 0.5 parts per million and silver at 0.07 ppm. Even though this element is not abundant, it is found in more than 100 mineral species. Antimony is sometimes found natively (e.g. on Antimony Peak), but more frequently it is found in the sulfide stibnite (SbS) which is the predominant ore mineral.

Antimony compounds are often classified according to their oxidation state: Sb(III) and Sb(V). The +5 oxidation state is more stable.

Antimony trioxide is formed when antimony is burnt in air. In the gas phase, the molecule of the compound is , but it polymerizes upon condensing. Antimony pentoxide () can be formed only by oxidation with concentrated nitric acid. Antimony also forms a mixed-valence oxide, antimony tetroxide (), which features both Sb(III) and Sb(V). Unlike oxides of phosphorus and arsenic, these oxides are amphoteric, do not form well-defined oxoacids, and react with acids to form antimony salts.

Antimonous acid is unknown, but the conjugate base sodium antimonite () forms upon fusing sodium oxide and . Transition metal antimonites are also known. Antimonic acid exists only as the hydrate , forming salts as the antimonate anion . When a solution containing this anion is dehydrated, the precipitate contains mixed oxides.

Many antimony ores are sulfides, including stibnite (), pyrargyrite (), zinkenite, jamesonite, and boulangerite. Antimony pentasulfide is non-stoichiometric and features antimony in the +3 oxidation state and S–S bonds. Several thioantimonides are known, such as and .

Antimony forms two series of halides: and . The trihalides , , , and are all molecular compounds having trigonal pyramidal molecular geometry.

The trifluoride is prepared by the reaction of with HF:

It is Lewis acidic and readily accepts fluoride ions to form the complex anions and . Molten is a weak electrical conductor. The trichloride is prepared by dissolving in hydrochloric acid:
The pentahalides and have trigonal bipyramidal molecular geometry in the gas phase, but in the liquid phase, is polymeric, whereas is monomeric. is a powerful Lewis acid used to make the superacid fluoroantimonic acid ("HSbF").

Oxyhalides are more common for antimony than for arsenic and phosphorus. Antimony trioxide dissolves in concentrated acid to form oxoantimonyl compounds such as SbOCl and .

Compounds in this class generally are described as derivatives of Sb. Antimony forms antimonides with metals, such as indium antimonide (InSb) and silver antimonide (). The alkali metal and zinc antimonides, such as NaSb and ZnSb, are more reactive. Treating these antimonides with acid produces the highly unstable gas stibine, :
Stibine can also be produced by treating salts with hydride reagents such as sodium borohydride. Stibine decomposes spontaneously at room temperature. Because stibine has a positive heat of formation, it is thermodynamically unstable and thus antimony does not react with hydrogen directly.

Organoantimony compounds are typically prepared by alkylation of antimony halides with Grignard reagents. A large variety of compounds are known with both Sb(III) and Sb(V) centers, including mixed chloro-organic derivatives, anions, and cations. Examples include Sb(CH) (triphenylstibine), Sb(CH) (with an Sb-Sb bond), and cyclic [Sb(CH)]. Pentacoordinated organoantimony compounds are common, examples being Sb(CH) and several related halides.

Antimony(III) sulfide, SbS, was recognized in predynastic Egypt as an eye cosmetic (kohl) as early as about 3100 BC, when the cosmetic palette was invented.

An artifact, said to be part of a vase, made of antimony dating to about 3000 BC was found at Telloh, Chaldea (part of present-day Iraq), and a copper object plated with antimony dating between 2500 BC and 2200 BC has been found in Egypt. Austen, at a lecture by Herbert Gladstone in 1892, commented that "we only know of antimony at the present day as a highly brittle and crystalline metal, which could hardly be fashioned into a useful vase, and therefore this remarkable 'find' (artifact mentioned above) must represent the lost art of rendering antimony malleable."

The British archaeologist Roger Moorey was unconvinced the artifact was indeed a vase, mentioning that Selimkhanov, after his analysis of the Tello object (published in 1975), "attempted to relate the metal to Transcaucasian natural antimony" (i.e. native metal) and that "the antimony objects from Transcaucasia are all small personal ornaments." This weakens the evidence for a lost art "of rendering antimony malleable."

The Roman scholar Pliny the Elder described several ways of preparing antimony sulfide for medical purposes in his treatise "Natural History". Pliny the Elder also made a distinction between "male" and "female" forms of antimony; the male form is probably the sulfide, while the female form, which is superior, heavier, and less friable, has been suspected to be native metallic antimony.

The Greek naturalist Pedanius Dioscorides mentioned that antimony sulfide could be roasted by heating by a current of air. It is thought that this produced metallic antimony.

The intentional isolation of antimony is described by Jabir ibn Hayyan before 815 AD. A description of a procedure for isolating antimony is later given in the 1540 book "De la pirotechnia" by Vannoccio Biringuccio, predating the more famous 1556 book by Agricola, "De re metallica". In this context Agricola has been often incorrectly credited with the discovery of metallic antimony. The book "Currus Triumphalis Antimonii" (The Triumphal Chariot of Antimony), describing the preparation of metallic antimony, was published in Germany in 1604. It was purported to be written by a Benedictine monk, writing under the name Basilius Valentinus in the 15th century; if it were authentic, which it is not, it would predate Biringuccio.

The metal antimony was known to German chemist Andreas Libavius in 1615 who obtained it by adding iron to a molten mixture of antimony sulfide, salt and potassium tartrate. This procedure produced antimony with a crystalline or starred surface.

With the advent of challenges to phlogiston theory, it was recognized that antimony is an element forming sulfides, oxides, and other compounds, as do other metals.

The first discovery of naturally occurring pure antimony in the Earth's crust was described by the Swedish scientist and local mine district engineer Anton von Swab in 1783; the type-sample was collected from the Sala Silver Mine in the Bergslagen mining district of Sala, Västmanland, Sweden.

The medieval Latin form, from which the modern languages and late Byzantine Greek take their names for antimony, is "antimonium". The origin of this is uncertain; all suggestions have some difficulty either of form or interpretation. The popular etymology, from ἀντίμοναχός "anti-monachos" or French "antimoine", still has adherents; this would mean "monk-killer", and is explained by many early alchemists being monks, and antimony being poisonous. However, the low toxicity of Antimony (see below) makes this unlikely. 

Another popular etymology is the hypothetical Greek word ἀντίμόνος "antimonos", "against aloneness", explained as "not found as metal", or "not found unalloyed". Lippmann conjectured a hypothetical Greek word ανθήμόνιον "anthemonion", which would mean "floret", and cites several examples of related Greek words (but not that one) which describe chemical or biological efflorescence.

The early uses of "antimonium" include the translations, in 1050–1100, by Constantine the African of Arabic medical treatises. Several authorities believe "antimonium" is a scribal corruption of some Arabic form; Meyerhof derives it from "ithmid"; other possibilities include "athimar", the Arabic name of the metalloid, and a hypothetical "as-stimmi", derived from or parallel to the Greek.

The standard chemical symbol for antimony (Sb) is credited to Jöns Jakob Berzelius, who derived the abbreviation from "stibium".

The ancient words for antimony mostly have, as their chief meaning, kohl, the sulfide of antimony.
The Egyptians called antimony "mśdmt"; in hieroglyphs, the vowels are uncertain, but the Coptic form of the word is ⲥⲧⲏⲙ (stēm). The Greek word, στίμμι "stimmi", is probably a loan word from Arabic or from Egyptian "stm" O34:D46-G17-F21:D4 and is used by Attic tragic poets of the 5th century BC. Later Greeks also used στἰβι "stibi", as did Celsus and Pliny, writing in Latin, in the first century AD. Pliny also gives the names "stimi" , "larbaris", alabaster, and the "very common" "platyophthalmos", "wide-eye" (from the effect of the cosmetic). Later Latin authors adapted the word to Latin as "stibium". The Arabic word for the substance, as opposed to the cosmetic, can appear as إثمد "ithmid, athmoud, othmod", or "uthmod". Littré suggests the first form, which is the earliest, derives from "stimmida", an accusative for "stimmi".

The British Geological Survey (BGS) reported that in 2005 China was the top producer of antimony with approximately 84% of the world share, followed at a distance by South Africa, Bolivia and Tajikistan. Xikuangshan Mine in Hunan province has the largest deposits in China with an estimated deposit of 2.1 million metric tons.

In 2016, according to the US Geological Survey, China accounted for 76.9% of total antimony production, followed in second place by Russia with 6.9% and Tajikistan with 6.2%.

Chinese production of antimony is expected to decline in the future as mines and smelters are closed down by the government as part of pollution control. Especially due to a new environmental protection law having gone into effect in January 2015 and revised "Emission Standards of Pollutants for Stanum, Antimony, and Mercury" having gone into effect, hurdles for economic production are higher. According to the National Bureau of Statistics in China, by September 2015 50% of antimony production capacity in the Hunan province (the province with biggest antimony reserves in China) had not been used.

Reported production of antimony in China has fallen and is unlikely to increase in the coming years, according to the Roskill report. No significant antimony deposits in China have been developed for about ten years, and the remaining economic reserves are being rapidly depleted.

The world's largest antimony producers, according to Roskill, are listed below:

According to statistics from the USGS, current global reserves of antimony will be depleted in 13 years. However, the USGS expects more resources will be found.

The extraction of antimony from ores depends on the quality and composition of the ore. Most antimony is mined as the sulfide; lower-grade ores are concentrated by froth flotation, while higher-grade ores are heated to 500–600 °C, the temperature at which stibnite melts and separates from the gangue minerals. Antimony can be isolated from the crude antimony sulfide by reduction with scrap iron:

The sulfide is converted to an oxide; the product is then roasted, sometimes for the purpose of vaporizing the volatile antimony(III) oxide, which is recovered. This material is often used directly for the main applications, impurities being arsenic and sulfide. Antimony is isolated from the oxide by a carbothermal reduction:

The lower-grade ores are reduced in blast furnaces while the higher-grade ores are reduced in reverberatory furnaces.

Antimony has consistently been ranked high in European and US risk lists concerning criticality of the element indicating the relative risk to the supply of chemical elements or element groups required to maintain the current economy and lifestyle.

With most of the antimony imported into Europe and the US coming from China, Chinese production is critical to supply. As China is revising and increasing environmental control standards, antimony production is becoming increasingly restricted. Additionally Chinese export quotas for antimony have been decreasing in the past years. These two factors increase supply risk for both Europe and US.

According to the BGS Risk List 2015, antimony is ranked second highest (after rare earth elements) on the relative supply risk index. This indicates that it has currently the second highest supply risk for chemical elements or element groups which are of economic value to the British economy and lifestyle.
Furthermore, antimony was identified as one of 20 critical raw materials for the EU in a report published in 2014 (which revised the initial report published in 2011). As seen in Figure xxx antimony maintains high supply risk relative to its economic importance. 92% of the antimony is imported from China, which is a significantly high concentration of production.

Much analysis has been conducted in the U.S. toward defining which metals should be called strategic or critical to the nation's security. Exact definitions do not exist, and views as to what constitutes a strategic or critical mineral to U.S. security diverge.

In 2015, no antimony was mined in the U.S. The metal is imported from foreign countries. In the period 2011–2014, 68% of America's antimony came from China, 14% from India, 4% from Mexico, and 14% from other sources. There are no publicly known government stockpiles in place currently.

The U.S. "Subcommittee on Critical and Strategic Mineral Supply Chains" has screened 78 mineral resources from 1996–2008. It found that a small subset of minerals including antimony has fallen into the category of potentially critical minerals consistently. In the future, a second assessment will be made of the found subset of minerals to identify which should be defined of significant risk and critical to U.S. interests.

About 60% of antimony is consumed in flame retardants, and 20% is used in alloys for batteries, plain bearings, and solders.

Antimony is mainly used as the trioxide for flame-proofing compounds, always in combination with halogenated flame retardants except in halogen-containing polymers. The flame retarding effect of antimony trioxide is produced by the formation of halogenated antimony compounds, which react with hydrogen atoms, and probably also with oxygen atoms and OH radicals, thus inhibiting fire. Markets for these flame-retardants include children's clothing, toys, aircraft, and automobile seat covers. They are also added to polyester resins in fiberglass composites for such items as light aircraft engine covers. The resin will burn in the presence of an externally generated flame, but will extinguish when the external flame is removed.

Antimony forms a highly useful alloy with lead, increasing its hardness and mechanical strength. For most applications involving lead, varying amounts of antimony are used as alloying metal. In lead–acid batteries, this addition improves plate strength and charging characteristics. For sailboats, lead keels are used as counterweights, ranging from 600 lbs to over 8000 lbs; to improve hardness and tensile strength of the lead keel, antimony is mixed with lead between 2% and 5% by volume. Antimony is used in antifriction alloys (such as Babbitt metal), in bullets and lead shot, electrical cable sheathing, type metal (for example, for linotype printing machines), solder (some "lead-free" solders contain 5% Sb), in pewter, and in hardening alloys with low tin content in the manufacturing of organ pipes.

Three other applications consume nearly all the rest of the world's supply. One application is as a stabilizer and catalyst for the production of polyethylene terephthalate. Another is as a fining agent to remove microscopic bubbles in glass, mostly for TV screens; antimony ions interact with oxygen, suppressing the tendency of the latter to form bubbles. The third application is pigments.

Antimony is increasingly being used in semiconductors as a dopant in n-type silicon wafers for diodes, infrared detectors, and Hall-effect devices. In the 1950s, the emitters and collectors of n-p-n alloy junction transistors were doped with tiny beads of a lead-antimony alloy. Indium antimonide is used as a material for mid-infrared detectors.

Biology and medicine have few uses for antimony. Treatments containing antimony, known as antimonials, are used as emetics. Antimony compounds are used as antiprotozoan drugs. Potassium antimonyl tartrate, or tartar emetic, was once used as an anti-schistosomal drug from 1919 on. It was subsequently replaced by praziquantel. Antimony and its compounds are used in several veterinary preparations, such as anthiomaline and lithium antimony thiomalate, as a skin conditioner in ruminants. Antimony has a nourishing or conditioning effect on keratinized tissues in animals.

Antimony-based drugs, such as meglumine antimoniate, are also considered the drugs of choice for treatment of leishmaniasis in domestic animals. Besides having low therapeutic indices, the drugs have minimal penetration of the bone marrow, where some of the "Leishmania" amastigotes reside, and curing the disease – especially the visceral form – is very difficult. Elemental antimony as an antimony pill was once used as a medicine. It could be reused by others after ingestion and elimination.

Antimony(III) sulfide is used in the heads of some safety matches. Antimony sulfides help to stabilize the friction coefficient in automotive brake pad materials. Antimony is used in bullets, bullet tracers, paint, glass art, and as an opacifier in enamel. Antimony-124 is used together with beryllium in neutron sources; the gamma rays emitted by antimony-124 initiate the photodisintegration of beryllium. The emitted neutrons have an average energy of 24 keV. Natural antimony is used in startup neutron sources.

Historically, the powder derived from crushed antimony ("kohl") has been applied to the eyes with a metal rod and with one's spittle, thought by the ancients to aid in curing eye infections. The practice is still seen in Yemen and in other Arabian countries.

The effects of antimony and its compounds on human and environmental health differ widely. Elemental antimony metal does not affect human and environmental health. Inhalation of antimony trioxide (and similar poorly soluble Sb(III) dust particles such as antimony dust) is considered harmful and suspected of causing cancer. However, these effects are only observed with female rats and after long-term exposure to high dust concentrations. The effects are hypothesized to be attributed to inhalation of poorly soluble Sb particles leading to impaired lung clearance, lung overload, inflammation and ultimately tumour formation, not to exposure to antimony ions (OECD, 2008). Antimony chlorides are corrosive to skin. The effects of antimony are not comparable to those of arsenic; this might be caused by the significant differences of uptake, metabolism, and excretion between arsenic and antimony.

For oral absorption, ICRP (1994) has recommended values of 10% for tartar emetic and 1% for all other antimony compounds. Dermal absorption for metals is estimated to be at most 1% (HERAG, 2007). Inhalation absorption of antimony trioxide and other poorly soluble Sb(III) substances (such as antimony dust) is estimated at 6.8% (OECD, 2008), whereas a value <1% is derived for Sb(V) substances. Antimony(V) is not quantitatively reduced to antimony(III) in the cell, and both species exist simultaneously.

Antimony is mainly excreted from the human body via urine. Antimony and its compounds do not cause acute human health effects, with the exception of antimony potassium tartrate ("tartar emetic"), a prodrug that is intentionally used to treat leishmaniasis patients.

Prolonged skin contact with antimony dust may cause dermatitis. However, it was agreed at the European Union level that the skin rashes observed are not substance-specific, but most probably due to a physical blocking of sweat ducts (ECHA/PR/09/09, Helsinki, 6 July 2009). Antimony dust may also be explosive when dispersed in the air; when in a bulk solid it is not combustible.

Antimony is incompatible with strong acids, halogenated acids, and oxidizers; when exposed to newly formed hydrogen it may form stibine (SbH).

The 8-hour time-weighted average (TWA) is set at 0.5 mg/m by the American Conference of Governmental Industrial Hygienists and by the Occupational Safety and Health Administration (OSHA) as a legal permissible exposure limit (PEL) in the workplace. The National Institute for Occupational Safety and Health (NIOSH) has set a recommended exposure limit (REL) of 0.5 mg/m as an 8-hour TWA. Antimony compounds are used as catalysts for polyethylene terephthalate (PET) production. Some studies report minor antimony leaching from PET bottles into liquids, but levels are below drinking water guidelines. Antimony concentrations in fruit juice concentrates were somewhat higher (up to 44.7 µg/L of antimony), but juices do not fall under the drinking water regulations. The drinking water guidelines are:

The TDI proposed by WHO is 6 µg antimony per kilogram of body weight. The IDLH (immediately dangerous to life and health) value for antimony is 50 mg/m.

Certain compounds of antimony appear to be toxic, particularly antimony trioxide and antimony potassium tartrate. Effects may be similar to arsenic poisoning. Occupational exposure may cause respiratory irritation, pneumoconiosis, antimony spots on the skin, gastrointestinal symptoms, and cardiac arrhythmias. In addition, antimony trioxide is potentially carcinogenic to humans.

Adverse health effects have been observed in humans and animals following inhalation, oral, or dermal exposure to antimony and antimony compounds. Antimony toxicity typically occurs either due to occupational exposure, during therapy or from accidental ingestion. It is unclear if antimony can enter the body through the skin.





</doc>
<doc id="899" url="https://en.wikipedia.org/wiki?curid=899" title="Actinium">
Actinium

Actinium is a chemical element with the symbol Ac and atomic number 89. It was first isolated by French chemist André-Louis Debierne in 1899. Friedrich Oskar Giesel later independently isolated it in 1902 and, unaware that it was already known, gave it the name emanium. Actinium gave the name to the actinide series, a group of 15 similar elements between actinium and lawrencium in the periodic table. It is also sometimes considered the first of the 7th-period transition metals, although lawrencium is less commonly given that position. Together with polonium, radium, and radon, actinium was one of the first non-primordial radioactive elements to be isolated.

A soft, silvery-white radioactive metal, actinium reacts rapidly with oxygen and moisture in air forming a white coating of actinium oxide that prevents further oxidation. As with most lanthanides and many actinides, actinium assumes oxidation state +3 in nearly all its chemical compounds. Actinium is found only in traces in uranium and thorium ores as the isotope Ac, which decays with a half-life of 21.772 years, predominantly emitting beta and sometimes alpha particles, and Ac, which is beta active with a half-life of 6.15 hours. One tonne of natural uranium in ore contains about 0.2 milligrams of actinium-227, and one tonne of thorium contains about 5 nanograms of actinium-228. The close similarity of physical and chemical properties of actinium and lanthanum makes separation of actinium from the ore impractical. Instead, the element is prepared, in milligram amounts, by the neutron irradiation of in a nuclear reactor. Owing to its scarcity, high price and radioactivity, actinium has no significant industrial use. Its current applications include a neutron source and an agent for radiation therapy.

André-Louis Debierne, a French chemist, announced the discovery of a new element in 1899. He separated it from pitchblende residues left by Marie and Pierre Curie after they had extracted radium. In 1899, Debierne described the substance as similar to titanium and (in 1900) as similar to thorium. Friedrich Oskar Giesel independently discovered actinium in 1902 as a substance being similar to lanthanum and called it "emanium" in 1904. After a comparison of the substances half-lives determined by Debierne, Harriet Brooks in 1904, and Otto Hahn and Otto Sackur in 1905, Debierne's chosen name for the new element was retained because it had seniority, despite the contradicting chemical properties he claimed for the element at different times.

Articles published in the 1970s and later suggest that Debierne's results published in 1904 conflict with those reported in 1899 and 1900. Furthermore, the now-known chemistry of actinium precludes its presence as anything other than a minor constituent of Debierne's 1899 and 1900 results; in fact, the chemical properties he reported make it likely that he had, instead, accidentally identified protactinium, which would not be discovered for another fourteen years, only to have it disappear due to its hydrolysis and adsorption onto his laboratory equipment. This has led some authors to advocate that Giesel alone should be credited with the discovery. A less confrontational vision of scientific discovery is proposed by Adloff. He suggests that hindsight criticism of the early publications should be mitigated by the then nascent state of radiochemistry: highlighting the prudence of Debierne's claims in the original papers, he notes that nobody can contend that Debierne's substance did not contain actinium. Debierne, who is now considered by the vast majority of historians as the discoverer, lost interest in the element and left the topic. Giesel, on the other hand, can rightfully be credited with the first preparation of radiochemically pure actinium and with the identification of its atomic number 89.

The name actinium originates from the Ancient Greek "aktis, aktinos" (ακτίς, ακτίνος), meaning beam or ray. Its symbol Ac is also used in abbreviations of other compounds that have nothing to do with actinium, such as acetyl, acetate and sometimes acetaldehyde.

Actinium is a soft, silvery-white, radioactive, metallic element. Its estimated shear modulus is similar to that of lead. Owing to its strong radioactivity, actinium glows in the dark with a pale blue light, which originates from the surrounding air ionized by the emitted energetic particles. Actinium has similar chemical properties to lanthanum and other lanthanides, and therefore these elements are difficult to separate when extracting from uranium ores. Solvent extraction and ion chromatography are commonly used for the separation.

The first element of the actinides, actinium gave the group its name, much as lanthanum had done for the lanthanides. The group of elements is more diverse than the lanthanides and therefore it was not until 1945 that the most significant change to Dmitri Mendeleev's periodic table since the recognition of the lanthanides, the introduction of the actinides, was generally accepted after Glenn T. Seaborg's research on the transuranium elements (although it had been proposed as early as 1892 by British chemist Henry Bassett).

Actinium reacts rapidly with oxygen and moisture in air forming a white coating of actinium oxide that impedes further oxidation. As with most lanthanides and actinides, actinium exists in the oxidation state +3, and the Ac ions are colorless in solutions. The oxidation state +3 originates from the [Rn]6d7s electronic configuration of actinium, with three valence electrons that are easily donated to give the stable closed-shell structure of the noble gas radon. The rare oxidation state +2 is only known for actinium dihydride (AcH); even this may in reality be an electride compound like its lighter congener LaH and thus have actinium(III). Ac is the largest of all known tripositive ions and its first coordination sphere contains approximately 10.9 ± 0.5 water molecules.

Due to actinium's intense radioactivity, only a limited number of actinium compounds are known. These include: AcF, AcCl, AcBr, AcOF, AcOCl, AcOBr, AcS, AcO and AcPO. Except for AcPO, they are all similar to the corresponding lanthanum compounds. They all contain actinium in the oxidation state +3. In particular, the lattice constants of the analogous lanthanum and actinium compounds differ by only a few percent.

Here "a", "b" and "c" are lattice constants, No is space group number and "Z" is the number of formula units per unit cell. Density was not measured directly but calculated from the lattice parameters.

Actinium oxide (AcO) can be obtained by heating the hydroxide at 500 °C or the oxalate at 1100 °C, in vacuum. Its crystal lattice is isotypic with the oxides of most trivalent rare-earth metals.

Actinium trifluoride can be produced either in solution or in solid reaction. The former reaction is carried out at room temperature, by adding hydrofluoric acid to a solution containing actinium ions. In the latter method, actinium metal is treated with hydrogen fluoride vapors at 700 °C in an all-platinum setup. Treating actinium trifluoride with ammonium hydroxide at 900–1000 °C yields oxyfluoride AcOF. Whereas lanthanum oxyfluoride can be easily obtained by burning lanthanum trifluoride in air at 800 °C for an hour, similar treatment of actinium trifluoride yields no AcOF and only results in melting of the initial product.

Actinium trichloride is obtained by reacting actinium hydroxide or oxalate with carbon tetrachloride vapors at temperatures above 960 °C. Similar to oxyfluoride, actinium oxychloride can be prepared by hydrolyzing actinium trichloride with ammonium hydroxide at 1000 °C. However, in contrast to the oxyfluoride, the oxychloride could well be synthesized by igniting a solution of actinium trichloride in hydrochloric acid with ammonia.

Reaction of aluminium bromide and actinium oxide yields actinium tribromide:

and treating it with ammonium hydroxide at 500 °C results in the oxybromide AcOBr.

Actinium hydride was obtained by reduction of actinium trichloride with potassium at 300 °C, and its structure was deduced by analogy with the corresponding LaH hydride. The source of hydrogen in the reaction was uncertain.

Mixing monosodium phosphate (NaHPO) with a solution of actinium in hydrochloric acid yields white-colored actinium phosphate hemihydrate (AcPO·0.5HO), and heating actinium oxalate with hydrogen sulfide vapors at 1400 °C for a few minutes results in a black actinium sulfide AcS. It may possibly be produced by acting with a mixture of hydrogen sulfide and carbon disulfide on actinium oxide at 1000 °C.

Naturally occurring actinium is composed of two radioactive isotopes; (from the radioactive family of ) and (a granddaughter of ). decays mainly as a beta emitter with a very small energy, but in 1.38% of cases it emits an alpha particle, so it can readily be identified through alpha spectrometry. Thirty-six radioisotopes have been identified, the most stable being with a half-life of 21.772 years, with a half-life of 10.0 days and with a half-life of 29.37 hours. All remaining radioactive isotopes have half-lives that are less than 10 hours and the majority of them have half-lives shorter than one minute. The shortest-lived known isotope of actinium is (half-life of 69 nanoseconds) which decays through alpha decay. Actinium also has two known meta states. The most significant isotopes for chemistry are Ac, Ac, and Ac.

Purified comes into equilibrium with its decay products after about a half of year. It decays according to its 21.772-year half-life emitting mostly beta (98.62%) and some alpha particles (1.38%); the successive decay products are part of the actinium series. Owing to the low available amounts, low energy of its beta particles (maximum 44.8 keV) and low intensity of alpha radiation, is difficult to detect directly by its emission and it is therefore traced via its decay products. The isotopes of actinium range in atomic weight from 205 u () to 236 u ().

Actinium is found only in traces in uranium ores – one tonne of uranium in ore contains about 0.2 milligrams of Ac – and in thorium ores, which contain about 5 nanograms of Ac per one tonne of thorium. The actinium isotope Ac is a transient member of the uranium-actinium series decay chain, which begins with the parent isotope U (or Pu) and ends with the stable lead isotope Pb. The isotope Ac is a transient member of the thorium series decay chain, which begins with the parent isotope Th and ends with the stable lead isotope Pb. Another actinium isotope (Ac) is transiently present in the neptunium series decay chain, beginning with Np (or U) and ending with thallium (Tl) and near-stable bismuth (Bi); even though all primordial Np has decayed away, it is continuously produced by neutron knock-out reactions on natural U.

The low natural concentration, and the close similarity of physical and chemical properties to those of lanthanum and other lanthanides, which are always abundant in actinium-bearing ores, render separation of actinium from the ore impractical, and complete separation was never achieved. Instead, actinium is prepared, in milligram amounts, by the neutron irradiation of in a nuclear reactor.
The reaction yield is about 2% of the radium weight. Ac can further capture neutrons resulting in small amounts of Ac. After the synthesis, actinium is separated from radium and from the products of decay and nuclear fusion, such as thorium, polonium, lead and bismuth. The extraction can be performed with thenoyltrifluoroacetone-benzene solution from an aqueous solution of the radiation products, and the selectivity to a certain element is achieved by adjusting the pH (to about 6.0 for actinium). An alternative procedure is anion exchange with an appropriate resin in nitric acid, which can result in a separation factor of 1,000,000 for radium and actinium vs. thorium in a two-stage process. Actinium can then be separated from radium, with a ratio of about 100, using a low cross-linking cation exchange resin and nitric acid as eluant.

Ac was first produced artificially at the Institute for Transuranium Elements (ITU) in Germany using a cyclotron and at St George Hospital in Sydney using a linac in 2000. This rare isotope has potential applications in radiation therapy and is most efficiently produced by bombarding a radium-226 target with 20–30 MeV deuterium ions. This reaction also yields Ac which however decays with a half-life of 29 hours and thus does not contaminate Ac.

Actinium metal has been prepared by the reduction of actinium fluoride with lithium vapor in vacuum at a temperature between 1100 and 1300 °C. Higher temperatures resulted in evaporation of the product and lower ones lead to an incomplete transformation. Lithium was chosen among other alkali metals because its fluoride is most volatile.

Owing to its scarcity, high price and radioactivity, Ac currently has no significant industrial use, but Ac is currently being studied for use in cancer treatments such as targeted alpha therapies.
Ac is highly radioactive and was therefore studied for use as an active element of radioisotope thermoelectric generators, for example in spacecraft. The oxide of Ac pressed with beryllium is also an efficient neutron source with the activity exceeding that of the standard americium-beryllium and radium-beryllium pairs. In all those applications, Ac (a beta source) is merely a progenitor which generates alpha-emitting isotopes upon its decay. Beryllium captures alpha particles and emits neutrons owing to its large cross-section for the (α,n) nuclear reaction:

The AcBe neutron sources can be applied in a neutron probe – a standard device for measuring the quantity of water present in soil, as well as moisture/density for quality control in highway construction. Such probes are also used in well logging applications, in neutron radiography, tomography and other radiochemical investigations.

Ac is applied in medicine to produce in a reusable generator or can be used alone as an agent for radiation therapy, in particular targeted alpha therapy (TAT). This isotope has a half-life of 10 days, making it much more suitable for radiation therapy than Bi (half-life 46 minutes). Additionally, Ac decays to nontoxic Bi rather than stable but toxic lead, which is the final product in the decay chains of several other candidate isotopes, namely Th, Th, and U. Not only Ac itself, but also its daughters, emit alpha particles which kill cancer cells in the body. The major difficulty with application of Ac was that intravenous injection of simple actinium complexes resulted in their accumulation in the bones and liver for a period of tens of years. As a result, after the cancer cells were quickly killed by alpha particles from Ac, the radiation from the actinium and its daughters might induce new mutations. To solve this problem, Ac was bound to a chelating agent, such as citrate, ethylenediaminetetraacetic acid (EDTA) or diethylene triamine pentaacetic acid (DTPA). This reduced actinium accumulation in the bones, but the excretion from the body remained slow. Much better results were obtained with such chelating agents as HEHA () or DOTA () coupled to trastuzumab, a monoclonal antibody that interferes with the HER2/neu receptor. The latter delivery combination was tested on mice and proved to be effective against leukemia, lymphoma, breast, ovarian, neuroblastoma and prostate cancers.

The medium half-life of Ac (21.77 years) makes it very convenient radioactive isotope in modeling the slow vertical mixing of oceanic waters. The associated processes cannot be studied with the required accuracy by direct measurements of current velocities (of the order 50 meters per year). However, evaluation of the concentration depth-profiles for different isotopes allows estimating the mixing rates. The physics behind this method is as follows: oceanic waters contain homogeneously dispersed U. Its decay product, Pa, gradually precipitates to the bottom, so that its concentration first increases with depth and then stays nearly constant. Pa decays to Ac; however, the concentration of the latter isotope does not follow the Pa depth profile, but instead increases toward the sea bottom. This occurs because of the mixing processes which raise some additional Ac from the sea bottom. Thus analysis of both Pa and Ac depth profiles allows researchers to model the mixing behavior.

There are theoretical predictions that AcH hydrides (in this case with very high pressure) are a candidate for a near room-temperature superconductor as they have T significantly higher than H3S, possibly near 250 K.

Ac is highly radioactive and experiments with it are carried out in a specially designed laboratory equipped with a tight glove box. When actinium trichloride is administered intravenously to rats, about 33% of actinium is deposited into the bones and 50% into the liver. Its toxicity is comparable to, but slightly lower than that of americium and plutonium. For trace quantities, fume hoods with good aeration suffice; for gram amounts, hot cells with shielding from the intense gamma radiation emitted by Ac are necessary.




</doc>
<doc id="900" url="https://en.wikipedia.org/wiki?curid=900" title="Americium">
Americium

Americium is a synthetic radioactive chemical element with the symbol Am and atomic number 95. It is a transuranic member of the actinide series, in the periodic table located under the lanthanide element europium, and thus by analogy was named after the Americas.

Americium was first produced in 1944 by the group of Glenn T. Seaborg from Berkeley, California, at the Metallurgical Laboratory of the University of Chicago, a part of the Manhattan Project. Although it is the third element in the transuranic series, it was discovered fourth, after the heavier curium. The discovery was kept secret and only released to the public in November 1945. Most americium is produced by uranium or plutonium being bombarded with neutrons in nuclear reactors – one tonne of spent nuclear fuel contains about 100 grams of americium. It is widely used in commercial ionization chamber smoke detectors, as well as in neutron sources and industrial gauges. Several unusual applications, such as nuclear batteries or fuel for space ships with nuclear propulsion, have been proposed for the isotope Am, but they are as yet hindered by the scarcity and high price of this nuclear isomer.

Americium is a relatively soft radioactive metal with silvery appearance. Its common isotopes are Am and Am. In chemical compounds, americium usually assumes the oxidation state +3, especially in solutions. Several other oxidation states are known, ranging from +2 to +7, and can be identified by their characteristic optical absorption spectra. The crystal lattice of solid americium and its compounds contain small intrinsic radiogenic defects, due to metamictization induced by self-irradiation with alpha particles, which accumulates with time; this can cause a drift of some material properties over time, more noticeable in older samples.

Although americium was likely produced in previous nuclear experiments, it was first intentionally synthesized, isolated and identified in late autumn 1944, at the University of California, Berkeley, by Glenn T. Seaborg, Leon O. Morgan, Ralph A. James, and Albert Ghiorso. They used a 60-inch cyclotron at the University of California, Berkeley. The element was chemically identified at the Metallurgical Laboratory (now Argonne National Laboratory) of the University of Chicago. Following the lighter neptunium, plutonium, and heavier curium, americium was the fourth transuranium element to be discovered. At the time, the periodic table had been restructured by Seaborg to its present layout, containing the actinide row below the lanthanide one. This led to americium being located right below its twin lanthanide element europium; it was thus by analogy named after the Americas: "The name americium (after the Americas) and the symbol Am are suggested for the element on the basis of its position as the sixth member of the actinide rare-earth series, analogous to europium, Eu, of the lanthanide series."

The new element was isolated from its oxides in a complex, multi-step process. First plutonium-239 nitrate (PuNO) solution was coated on a platinum foil of about 0.5 cm area, the solution was evaporated and the residue was converted into plutonium dioxide (PuO) by calcining. After cyclotron irradiation, the coating was dissolved with nitric acid, and then precipitated as the hydroxide using concentrated aqueous ammonia solution. The residue was dissolved in perchloric acid. Further separation was carried out by ion exchange, yielding a certain isotope of curium. The separation of curium and americium was so painstaking that those elements were initially called by the Berkeley group as "pandemonium" (from Greek for "all demons" or "hell") and "delirium" (from Latin for "madness").

Initial experiments yielded four americium isotopes: Am, Am, Am and Am. Americium-241 was directly obtained from plutonium upon absorption of two neutrons. It decays by emission of a α-particle to Np; the half-life of this decay was first determined as years but then corrected to 432.2 years.

The second isotope Am was produced upon neutron bombardment of the already-created Am. Upon rapid β-decay, Am converts into the isotope of curium Cm (which had been discovered previously). The half-life of this decay was initially determined at 17 hours, which was close to the presently accepted value of 16.02 h.

The discovery of americium and curium in 1944 was closely related to the Manhattan Project; the results were confidential and declassified only in 1945. Seaborg leaked the synthesis of the elements 95 and 96 on the U.S. radio show for children "Quiz Kids" five days before the official presentation at an American Chemical Society meeting on 11 November 1945, when one of the listeners asked whether any new transuranium element beside plutonium and neptunium had been discovered during the war. After the discovery of americium isotopes Am and Am, their production and compounds were patented listing only Seaborg as the inventor. The initial americium samples weighed a few micrograms; they were barely visible and were identified by their radioactivity. The first substantial amounts of metallic americium weighing 40–200 micrograms were not prepared until 1951 by reduction of americium(III) fluoride with barium metal in high vacuum at 1100 °C.

The longest-lived and most common isotopes of americium, Am and Am, have half-lives of 432.2 and 7,370 years, respectively. Therefore, any primordial americium (americium that was present on Earth during its formation) should have decayed by now. Trace amounts of americium probably occur naturally in uranium minerals as a result of nuclear reactions, though this has not been confirmed.

Existing americium is concentrated in the areas used for the atmospheric nuclear weapons tests conducted between 1945 and 1980, as well as at the sites of nuclear incidents, such as the Chernobyl disaster. For example, the analysis of the debris at the testing site of the first U.S. hydrogen bomb, Ivy Mike, (1 November 1952, Enewetak Atoll), revealed high concentrations of various actinides including americium; but due to military secrecy, this result was not published until later, in 1956. Trinitite, the glassy residue left on the desert floor near Alamogordo, New Mexico, after the plutonium-based Trinity nuclear bomb test on 16 July 1945, contains traces of americium-241. Elevated levels of americium were also detected at the crash site of a US Boeing B-52 bomber aircraft, which carried four hydrogen bombs, in 1968 in Greenland.

In other regions, the average radioactivity of surface soil due to residual americium is only about 0.01 picocuries/g (0.37 mBq/g). Atmospheric americium compounds are poorly soluble in common solvents and mostly adhere to soil particles. Soil analysis revealed about 1,900 times higher concentration of americium inside sandy soil particles than in the water present in the soil pores; an even higher ratio was measured in loam soils.

Americium is produced mostly artificially in small quantities, for research purposes. A tonne of spent nuclear fuel contains about 100 grams of various americium isotopes, mostly Am and Am. Their prolonged radioactivity is undesirable for the disposal, and therefore americium, together with other long-lived actinides, must be neutralized. The associated procedure may involve several steps, where americium is first separated and then converted by neutron bombardment in special reactors to short-lived nuclides. This procedure is well known as nuclear transmutation, but it is still being developed for americium. The transuranic elements from americium to fermium occurred naturally in the natural nuclear fission reactor at Oklo, but no longer do so.

Americium has been produced in small quantities in nuclear reactors for decades, and kilograms of its Am and Am isotopes have been accumulated by now. Nevertheless, since it was first offered for sale in 1962, its price, about US$1,500 per gram of Am, remains almost unchanged owing to the very complex separation procedure. The heavier isotope Am is produced in much smaller amounts; it is thus more difficult to separate, resulting in a higher cost of the order 100,000–160,000 USD/g.

Americium is not synthesized directly from uranium – the most common reactor material – but from the plutonium isotope Pu. The latter needs to be produced first, according to the following nuclear process:

The capture of two neutrons by Pu (a so-called (n,γ) reaction), followed by a β-decay, results in Am:

The plutonium present in spent nuclear fuel contains about 12% of Pu. Because it spontaneously converts to Am, Pu can be extracted and may be used to generate further Am. However, this process is rather slow: half of the original amount of Pu decays to Am after about 15 years, and the Am amount reaches a maximum after 70 years.

The obtained Am can be used for generating heavier americium isotopes by further neutron capture inside a nuclear reactor. In a light water reactor (LWR), 79% of Am converts to Am and 10% to its nuclear isomer Am:
Americium-242 has a half-life of only 16 hours, which makes its further conversion to Am extremely inefficient. The latter isotope is produced instead in a process where Pu captures four neutrons under high neutron flux:

Most synthesis routines yield a mixture of different actinide isotopes in oxide forms, from which isotopes of americium can be separated. In a typical procedure, the spent reactor fuel (e.g. MOX fuel) is dissolved in nitric acid, and the bulk of uranium and plutonium is removed using a PUREX-type extraction (Plutonium–URanium EXtraction) with tributyl phosphate in a hydrocarbon. The lanthanides and remaining actinides are then separated from the aqueous residue (raffinate) by a diamide-based extraction, to give, after stripping, a mixture of trivalent actinides and lanthanides. Americium compounds are then selectively extracted using multi-step chromatographic and centrifugation techniques with an appropriate reagent. A large amount of work has been done on the solvent extraction of americium. For example, a 2003 EU-funded project codenamed "EUROPART" studied triazines and other compounds as potential extraction agents. A "bis"-triazinyl bipyridine complex was proposed in 2009 as such a reagent is highly selective to americium (and curium). Separation of americium from the highly similar curium can be achieved by treating a slurry of their hydroxides in aqueous sodium bicarbonate with ozone, at elevated temperatures. Both Am and Cm are mostly present in solutions in the +3 valence state; whereas curium remains unchanged, americium oxidizes to soluble Am(IV) complexes which can be washed away.

Metallic americium is obtained by reduction from its compounds. Americium(III) fluoride was first used for this purpose. The reaction was conducted using elemental barium as reducing agent in a water- and oxygen-free environment inside an apparatus made of tantalum and tungsten.

An alternative is the reduction of americium dioxide by metallic lanthanum or thorium:

In the periodic table, americium is located to the right of plutonium, to the left of curium, and below the lanthanide europium, with which it shares many similarities in physical and chemical properties. Americium is a highly radioactive element. When freshly prepared, it has a silvery-white metallic lustre, but then slowly tarnishes in air. With a density of 12 g/cm, americium is less dense than both curium (13.52 g/cm) and plutonium (19.8 g/cm); but has a higher density than europium (5.264 g/cm)—mostly because of its higher atomic mass. Americium is relatively soft and easily deformable and has a significantly lower bulk modulus than the actinides before it: Th, Pa, U, Np and Pu. Its melting point of 1173 °C is significantly higher than that of plutonium (639 °C) and europium (826 °C), but lower than for curium (1340 °C).

At ambient conditions, americium is present in its most stable α form which has a hexagonal crystal symmetry, and a space group P6/mmc with cell parameters "a" = 346.8 pm and "c" = 1124 pm, and four atoms per unit cell. The crystal consists of a double-hexagonal close packing with the layer sequence ABAC and so is isotypic with α-lanthanum and several actinides such as α-curium. The crystal structure of americium changes with pressure and temperature. When compressed at room temperature to 5 GPa, α-Am transforms to the β modification, which has a face-centered cubic ("fcc") symmetry, space group Fmm and lattice constant "a" = 489 pm. This "fcc" structure is equivalent to the closest packing with the sequence ABC. Upon further compression to 23 GPa, americium transforms to an orthorhombic γ-Am structure similar to that of α-uranium. There are no further transitions observed up to 52 GPa, except for an appearance of a monoclinic phase at pressures between 10 and 15 GPa. There is no consistency on the status of this phase in the literature, which also sometimes lists the α, β and γ phases as I, II and III. The β-γ transition is accompanied by a 6% decrease in the crystal volume; although theory also predicts a significant volume change for the α-β transition, it is not observed experimentally. The pressure of the α-β transition decreases with increasing temperature, and when α-americium is heated at ambient pressure, at 770 °C it changes into an "fcc" phase which is different from β-Am, and at 1075 °C it converts to a body-centered cubic structure. The pressure-temperature phase diagram of americium is thus rather similar to those of lanthanum, praseodymium and neodymium.

As with many other actinides, self-damage of the crystal structure due to alpha-particle irradiation is intrinsic to americium. It is especially noticeable at low temperatures, where the mobility of the produced structure defects is relatively low, by broadening of X-ray diffraction peaks. This effect makes somewhat uncertain the temperature of americium and some of its properties, such as electrical resistivity. So for americium-241, the resistivity at 4.2 K increases with time from about 2 µOhm·cm to 10 µOhm·cm after 40 hours, and saturates at about 16 µOhm·cm after 140 hours. This effect is less pronounced at room temperature, due to annihilation of radiation defects; also heating to room temperature the sample which was kept for hours at low temperatures restores its resistivity. In fresh samples, the resistivity gradually increases with temperature from about 2 µOhm·cm at liquid helium to 69 µOhm·cm at room temperature; this behavior is similar to that of neptunium, uranium, thorium and protactinium, but is different from plutonium and curium which show a rapid rise up to 60 K followed by saturation. The room temperature value for americium is lower than that of neptunium, plutonium and curium, but higher than for uranium, thorium and protactinium.

Americium is paramagnetic in a wide temperature range, from that of liquid helium, to room temperature and above. This behavior is markedly different from that of its neighbor curium which exhibits antiferromagnetic transition at 52 K. The thermal expansion coefficient of americium is slightly anisotropic and amounts to along the shorter "a" axis and for the longer "c" hexagonal axis. The enthalpy of dissolution of americium metal in hydrochloric acid at standard conditions is , from which the standard enthalpy change of formation (Δ"H"°) of aqueous Am ion is . The standard potential Am/Am is .

Americium metal readily reacts with oxygen and dissolves in aqueous acids. The most stable oxidation state for americium is +3. The chemistry of americium(III) has many similarities to the chemistry of lanthanide(III) compounds. For example, trivalent americium forms insoluble fluoride, oxalate, iodate, hydroxide, phosphate and other salts. Compounds of americium in oxidation states 2, 4, 5 and 6 have also been studied. This is the widest range that has been observed with actinide elements. The color of americium compounds in aqueous solution is as follows: Am (yellow-reddish), Am (yellow-reddish), Am; (yellow), Am (brown) and Am (dark green). The absorption spectra have sharp peaks, due to "f"-"f" transitions' in the visible and near-infrared regions. Typically, Am(III) has absorption maxima at ca. 504 and 811 nm, Am(V) at ca. 514 and 715 nm, and Am(VI) at ca. 666 and 992 nm.

Americium compounds with oxidation state +4 and higher are strong oxidizing agents, comparable in strength to the permanganate ion () in acidic solutions. Whereas the Am ions are unstable in solutions and readily convert to Am, compounds such as americium dioxide (AmO) and americium(IV) fluoride (AmF) are stable in the solid state.

The pentavalent oxidation state of americium was first observed in 1951. In acidic aqueous solution the ion is unstable with respect to disproportionation. The reaction

is typical. The chemistry of Am(V) and Am(VI) is comparable to the chemistry of uranium in those oxidation states. In particular, compounds like LiAmO and LiAmO are comparable to uranates and the ion AmO is comparable to the uranyl ion, UO. Such compounds can be prepared by oxidation of Am(III) in dilute nitric acid with ammonium persulfate. Other oxidising agents that have been used include silver(I) oxide, ozone and sodium persulfate.

Three americium oxides are known, with the oxidation states +2 (AmO), +3 (AmO) and +4 (AmO). Americium(II) oxide was prepared in minute amounts and has not been characterized in detail. Americium(III) oxide is a red-brown solid with a melting point of 2205 °C. Americium(IV) oxide is the main form of solid americium which is used in nearly all its applications. As most other actinide dioxides, it is a black solid with a cubic (fluorite) crystal structure.

The oxalate of americium(III), vacuum dried at room temperature, has the chemical formula Am(CO)·7HO. Upon heating in vacuum, it loses water at 240 °C and starts decomposing into AmO at 300 °C, the decomposition completes at about 470 °C. The initial oxalate dissolves in nitric acid with the maximum solubility of 0.25 g/L.

Halides of americium are known for the oxidation states +2, +3 and +4, where the +3 is most stable, especially in solutions.

Reduction of Am(III) compounds with sodium amalgam yields Am(II) salts – the black halides AmCl, AmBr and AmI. They are very sensitive to oxygen and oxidize in water, releasing hydrogen and converting back to the Am(III) state. Specific lattice constants are:

Americium(III) fluoride (AmF) is poorly soluble and precipitates upon reaction of Am and fluoride ions in weak acidic solutions:

The tetravalent americium(IV) fluoride (AmF) is obtained by reacting solid americium(III) fluoride with molecular fluorine:

Another known form of solid tetravalent americium chloride is KAmF. Tetravalent americium has also been observed in the aqueous phase. For this purpose, black Am(OH) was dissolved in 15-M NHF with the americium concentration of 0.01 M. The resulting reddish solution had a characteristic optical absorption spectrum which is similar to that of AmF but differed from other oxidation states of americium. Heating the Am(IV) solution to 90 °C did not result in its disproportionation or reduction, however a slow reduction was observed to Am(III) and assigned to self-irradiation of americium by alpha particles.

Most americium(III) halides form hexagonal crystals with slight variation of the color and exact structure between the halogens. So, chloride (AmCl) is reddish and has a structure isotypic to uranium(III) chloride (space group P6/m) and the melting point of 715 °C. The fluoride is isotypic to LaF (space group P6/mmc) and the iodide to BiI (space group R). The bromide is an exception with the orthorhombic PuBr-type structure and space group Cmcm. Crystals of americium hexahydrate (AmCl·6HO) can be prepared by dissolving americium dioxide in hydrochloric acid and evaporating the liquid. Those crystals are hygroscopic and have yellow-reddish color and a monoclinic crystal structure.

Oxyhalides of americium in the form AmOX, AmOX, AmOX and AmOX can be obtained by reacting the corresponding americium halide with oxygen or SbO, and AmOCl can also be produced by vapor phase hydrolysis:

The known chalcogenides of americium include the sulfide AmS, selenides AmSe and AmSe, and tellurides AmTe and AmTe. The pnictides of americium (Am) of the AmX type are known for the elements phosphorus, arsenic, antimony and bismuth. They crystallize in the rock-salt lattice.

Americium monosilicide (AmSi) and "disilicide" (nominally AmSi with: 1.87 < x < 2.0) were obtained by reduction of americium(III) fluoride with elementary silicon in vacuum at 1050 °C (AmSi) and 1150−1200 °C (AmSi). AmSi is a black solid isomorphic with LaSi, it has an orthorhombic crystal symmetry. AmSi has a bright silvery lustre and a tetragonal crystal lattice (space group "I"4/amd), it is isomorphic with PuSi and ThSi. Borides of americium include AmB and AmB. The tetraboride can be obtained by heating an oxide or halide of americium with magnesium diboride in vacuum or inert atmosphere.

Analogous to uranocene, americium forms the organometallic compound amerocene with two cyclooctatetraene ligands, with the chemical formula (η-CH)Am. A cyclopentadienyl complex is also known that is likely to be stoichiometrically AmCp.

Formation of the complexes of the type Am(n-CH-BTP), where BTP stands for 2,6-di(1,2,4-triazin-3-yl)pyridine, in solutions containing n-CH-BTP and Am ions has been confirmed by EXAFS. Some of these BTP-type complexes selectively interact with americium and therefore are useful in its selective separation from lanthanides and another actinides.

Americium is an artificial element of recent origin, and thus does not have a biological requirement. It is harmful to life. It has been proposed to use bacteria for removal of americium and other heavy metals from rivers and streams. Thus, Enterobacteriaceae of the genus "Citrobacter" precipitate americium ions from aqueous solutions, binding them into a metal-phosphate complex at their cell walls. Several studies have been reported on the biosorption and bioaccumulation of americium by bacteria and fungi.

The isotope Am (half-life 141 years) has the largest cross sections for absorption of thermal neutrons (5,700 barns), that results in a small critical mass for a sustained nuclear chain reaction. The critical mass for a bare Am sphere is about 9–14 kg (the uncertainty results from insufficient knowledge of its material properties). It can be lowered to 3–5 kg with a metal reflector and should become even smaller with a water reflector. Such small critical mass is favorable for portable nuclear weapons, but those based on Am are not known yet, probably because of its scarcity and high price. The critical masses of two other readily available isotopes, Am and Am, are relatively high – 57.6 to 75.6 kg for Am and 209 kg for Am. Scarcity and high price yet hinder application of americium as a nuclear fuel in nuclear reactors.

There are proposals of very compact 10-kW high-flux reactors using as little as 20 grams of Am. Such low-power reactors would be relatively safe to use as neutron sources for radiation therapy in hospitals.

About 19 isotopes and 8 nuclear isomers are known for americium. There are two long-lived alpha-emitters; Am has a half-life of 7,370 years and is the most stable isotope, and Am has a half-life of 432.2 years. The most stable nuclear isomer is Am; it has a long half-life of 141 years. The half-lives of other isotopes and isomers range from 0.64 microseconds for Am to 50.8 hours for Am. As with most other actinides, the isotopes of americium with odd number of neutrons have relatively high rate of nuclear fission and low critical mass.

Americium-241 decays to Np emitting alpha particles of 5 different energies, mostly at 5.486 MeV (85.2%) and 5.443 MeV (12.8%). Because many of the resulting states are metastable, they also emit gamma rays with the discrete energies between 26.3 and 158.5 keV.

Americium-242 is a short-lived isotope with a half-life of 16.02 h. It mostly (82.7%) converts by β-decay to Cm, but also by electron capture to Pu (17.3%). Both Cm and Pu transform via nearly the same decay chain through Pu down to U.

Nearly all (99.541%) of Am decays by internal conversion to Am and the remaining 0.459% by α-decay to Np. The latter subsequently decays to Pu and then to U.

Americium-243 transforms by α-emission into Np, which converts by β-decay to Pu, and the Pu changes into U by emitting an α-particle.

Americium is used in the most common type of household smoke detector, which uses Am in the form of americium dioxide as its source of ionizing radiation. This isotope is preferred over Ra because it emits 5 times more alpha particles and relatively little harmful gamma radiation.

The amount of americium in a typical new smoke detector is 1 microcurie (37 kBq) or 0.29 microgram. This amount declines slowly as the americium decays into neptunium-237, a different transuranic element with a much longer half-life (about 2.14 million years). With its half-life of 432.2 years, the americium in a smoke detector includes about 3% neptunium after 19 years, and about 5% after 32 years. The radiation passes through an ionization chamber, an air-filled space between two electrodes, and permits a small, constant current between the electrodes. Any smoke that enters the chamber absorbs the alpha particles, which reduces the ionization and affects this current, triggering the alarm. Compared to the alternative optical smoke detector, the ionization smoke detector is cheaper and can detect particles which are too small to produce significant light scattering; however, it is more prone to false alarms.

As Am has a roughly similar half-life to Pu (432.2 years vs. 87 years), it has been proposed as an active element of radioisotope thermoelectric generators, for example in spacecraft. Although americium produces less heat and electricity – the power yield is 114.7 mW/g for Am and 6.31 mW/g for Am (cf. 390 mW/g for Pu) – and its radiation poses more threat to humans owing to neutron emission, the European Space Agency is considering using americium for its space probes.

Another proposed space-related application of americium is a fuel for space ships with nuclear propulsion. It relies on the very high rate of nuclear fission of Am, which can be maintained even in a micrometer-thick foil. Small thickness avoids the problem of self-absorption of emitted radiation. This problem is pertinent to uranium or plutonium rods, in which only surface layers provide alpha-particles. The fission products of Am can either directly propel the spaceship or they can heat a thrusting gas. They can also transfer their energy to a fluid and generate electricity through a magnetohydrodynamic generator.

One more proposal which utilizes the high nuclear fission rate of Am is a nuclear battery. Its design relies not on the energy of the emitted by americium alpha particles, but on their charge, that is the americium acts as the self-sustaining "cathode". A single 3.2 kg Am charge of such battery could provide about 140 kW of power over a period of 80 days. Even with all the potential benefits, the current applications of Am are as yet hindered by the scarcity and high price of this particular nuclear isomer.

In 2019, researchers at the UK National Nuclear Laboratory and the University of Leicester demonstrated the use of heat generated by americium to illuminate a small light bulb. This technology could lead to systems to power missions with durations up to 400 years into interstellar space, where solar panels do not function.

The oxide of Am pressed with beryllium is an efficient neutron source. Here americium acts as the alpha source, and beryllium produces neutrons owing to its large cross-section for the (α,n) nuclear reaction:

The most widespread use of AmBe neutron sources is a neutron probe – a device used to measure the quantity of water present in soil, as well as moisture/density for quality control in highway construction. Am neutron sources are also used in well logging applications, as well as in neutron radiography, tomography and other radiochemical investigations.

Americium is a starting material for the production of other transuranic elements and transactinides – for example, 82.7% of Am decays to Cm and 17.3% to Pu. In the nuclear reactor, Am is also up-converted by neutron capture to Am and Am, which transforms by β-decay to Cm:

Irradiation of Am by C or Ne ions yields the isotopes Es (einsteinium) or Db (dubnium), respectively. Furthermore, the element berkelium (Bk isotope) had been first intentionally produced and identified by bombarding Am with alpha particles, in 1949, by the same Berkeley group, using the same 60-inch cyclotron. Similarly, nobelium was produced at the Joint Institute for Nuclear Research, Dubna, Russia, in 1965 in several reactions, one of which included irradiation of Am with N ions. Besides, one of the synthesis reactions for lawrencium, discovered by scientists at Berkeley and Dubna, included bombardment of Am with O.

Americium-241 has been used as a portable source of both gamma rays and alpha particles for a number of medical and industrial uses. The 59.5409 keV gamma ray emissions from Am in such sources can be used for indirect analysis of materials in radiography and X-ray fluorescence spectroscopy, as well as for quality control in fixed nuclear density gauges and nuclear densometers. For example, the element has been employed to gauge glass thickness to help create flat glass. Americium-241 is also suitable for calibration of gamma-ray spectrometers in the low-energy range, since its spectrum consists of nearly a single peak and negligible Compton continuum (at least three orders of magnitude lower intensity). Americium-241 gamma rays were also used to provide passive diagnosis of thyroid function. This medical application is however obsolete.

As a highly radioactive element, americium and its compounds must be handled only in an appropriate laboratory under special arrangements. Although most americium isotopes predominantly emit alpha particles which can be blocked by thin layers of common materials, many of the daughter products emit gamma-rays and neutrons which have a long penetration depth.

If consumed, most of the americium is excreted within a few days, with only 0.05% absorbed in the blood, of which roughly 45% goes to the liver and 45% to the bones, and the remaining 10% is excreted. The uptake to the liver depends on the individual and increases with age. In the bones, americium is first deposited over cortical and trabecular surfaces and slowly redistributes over the bone with time. The biological half-life of Am is 50 years in the bones and 20 years in the liver, whereas in the gonads (testicles and ovaries) it remains permanently; in all these organs, americium promotes formation of cancer cells as a result of its radioactivity.

Americium often enters landfills from discarded smoke detectors. The rules associated with the disposal of smoke detectors are relaxed in most jurisdictions. In 1994, 17-year-old David Hahn extracted the americium from about 100 smoke detectors in an attempt to build a breeder nuclear reactor. There have been a few cases of exposure to americium, the worst case being that of chemical operations technician Harold McCluskey, who at the age of 64 was exposed to 500 times the occupational standard for americium-241 as a result of an explosion in his lab. McCluskey died at the age of 75 of unrelated pre-existing disease.






</doc>
<doc id="901" url="https://en.wikipedia.org/wiki?curid=901" title="Astatine">
Astatine

Astatine is a chemical element with the symbol At and atomic number 85. It is the rarest naturally occurring element in the Earth's crust, occurring only as the decay product of various heavier elements. All of astatine's isotopes are short-lived; the most stable is astatine-210, with a half-life of 8.1 hours. A sample of the pure element has never been assembled, because any macroscopic specimen would be immediately vaporized by the heat of its own radioactivity.

The bulk properties of astatine are not known with certainty. Many of them have been estimated based on the element's position on the periodic table as a heavier analog of iodine, and a member of the halogens (the group of elements including fluorine, chlorine, bromine, and iodine). Astatine is likely to have a dark or lustrous appearance and may be a semiconductor or possibly a metal; it probably has a higher melting point than that of iodine. Chemically, several anionic species of astatine are known and most of its compounds resemble those of iodine. It also shows some metallic behavior, including being able to form a stable monatomic cation in aqueous solution (unlike the lighter halogens).

The first synthesis of the element was in 1940 by Dale R. Corson, Kenneth Ross MacKenzie, and Emilio G. Segrè at the University of California, Berkeley, who named it from the Greek "astatos" (ἄστατος), meaning "unstable". Four isotopes of astatine were subsequently found to be naturally occurring, although much less than one gram is present at any given time in the Earth's crust. Neither the most stable isotope astatine-210, nor the medically useful astatine-211, occur naturally; they can only be produced synthetically, usually by bombarding bismuth-209 with alpha particles.

Astatine is an extremely radioactive element; all its isotopes have half-lives of 8.1 hours or less, decaying into other astatine isotopes, bismuth, polonium, or radon. Most of its isotopes are very unstable, with half-lives of one second or less. Of the first 101 elements in the periodic table, only francium is less stable, and all the astatine isotopes more stable than francium are in any case synthetic and do not occur in nature.

The bulk properties of astatine are not known with any certainty. Research is limited by its short half-life, which prevents the creation of weighable quantities. A visible piece of astatine would immediately vaporize itself because of the heat generated by its intense radioactivity. It remains to be seen if, with sufficient cooling, a macroscopic quantity of astatine could be deposited as a thin film. Astatine is usually classified as either a nonmetal or a metalloid; metal formation has also been predicted.

Most of the physical properties of astatine have been estimated (by interpolation or extrapolation), using theoretically or empirically derived methods. For example, halogens get darker with increasing atomic weight – fluorine is nearly colorless, chlorine is yellow-green, bromine is red-brown, and iodine is dark gray/violet. Astatine is sometimes described as probably being a black solid (assuming it follows this trend), or as having a metallic appearance (if it is a metalloid or a metal). The melting and boiling points of astatine are also expected to follow the trend seen in the halogen series, increasing with atomic number. On this basis they are estimated to be , respectively. Some experimental evidence suggests astatine may have lower melting and boiling points than those implied by the halogen trend; a chromatographic estimation of the boiling point of elemental astatine in 1982 suggested a boiling point of 503±3 K (about 230±3 °C or 445±5 °F). Astatine sublimes less readily than does iodine, having a lower vapor pressure. Even so, half of a given quantity of astatine will vaporize in approximately an hour if put on a clean glass surface at room temperature. The absorption spectrum of astatine in the middle ultraviolet region has lines at 224.401 and 216.225 nm, suggestive of 6p to 7s transitions.

The structure of solid astatine is unknown. As an analogue of iodine it may have an orthorhombic crystalline structure composed of diatomic astatine molecules, and be a semiconductor (with a band gap of 0.7 eV). Alternatively, if condensed astatine forms a metallic phase, as has been predicted, it may have a monatomic face-centered cubic structure; in this structure it may well be a superconductor, like the similar high-pressure phase of iodine. Evidence for (or against) the existence of diatomic astatine (At) is sparse and inconclusive. Some sources state that it does not exist, or at least has never been observed, while other sources assert or imply its existence. Despite this controversy, many properties of diatomic astatine have been predicted; for example, its bond length would be , dissociation energy , and heat of vaporization (∆H) 54.39 kJ/mol. The latter figure means that astatine may (at least) be metallic in the liquid state on the basis that elements with a heat of vaporization greater than ~42 kJ/mol are metallic when liquid; diatomic iodine, with a value of 41.71 kJ/mol, falls just short of the threshold figure.

The chemistry of astatine is "clouded by the extremely low concentrations at which astatine experiments have been conducted, and the possibility of reactions with impurities, walls and filters, or radioactivity by-products, and other unwanted nano-scale interactions". Many of its apparent chemical properties have been observed using tracer studies on extremely dilute astatine solutions, typically less than 10 mol·L. Some properties, such as anion formation, align with other halogens. Astatine has some metallic characteristics as well, such as plating onto a cathode, coprecipitating with metal sulfides in hydrochloric acid, and forming a stable monatomic cation in aqueous solution. It forms complexes with EDTA, a metal chelating agent, and is capable of acting as a metal in antibody radiolabeling; in some respects astatine in the +1 state is akin to silver in the same state. Most of the organic chemistry of astatine is, however, analogous to that of iodine.

Astatine has an electronegativity of 2.2 on the revised Pauling scale – lower than that of iodine (2.66) and the same as hydrogen. In hydrogen astatide (HAt), the negative charge is predicted to be on the hydrogen atom, implying that this compound could be referred to as astatine hydride according to certain nomenclatures. That would be consistent with the electronegativity of astatine on the Allred–Rochow scale (1.9) being less than that of hydrogen (2.2). However, official IUPAC stoichiometric nomenclature is based on an idealized convention of determining the relative electronegativities of the elements by the mere virtue of their position within the periodic table. According to this convention, astatine is handled as though it is more electronegative than hydrogen, irrespective of its true electronegativity. The electron affinity of astatine, at 233 kJ mol, is 21% less than that of iodine. In comparison, the value of Cl (349) is 6.4% higher than F (328); Br (325) is 6.9% less than Cl; and I (295) is 9.2% less than Br. The marked reduction for At was predicted as being due to spin–orbit interactions.

Less reactive than iodine, astatine is the least reactive of the halogens, although its compounds have been synthesized in microscopic amounts and studied as intensively as possible before their radioactive disintegration. The reactions involved have been typically tested with dilute solutions of astatine mixed with larger amounts of iodine. Acting as a carrier, the iodine ensures there is sufficient material for laboratory techniques (such as filtration and precipitation) to work. Like iodine, astatine has been shown to adopt odd-numbered oxidation states ranging from −1 to +7.

Only a few compounds with metals have been reported, in the form of astatides of sodium, palladium, silver, thallium, and lead. Some characteristic properties of silver and sodium astatide, and the other hypothetical alkali and alkaline earth astatides, have been estimated by extrapolation from other metal halides.

The formation of an astatine compound with hydrogen – usually referred to as hydrogen astatide – was noted by the pioneers of astatine chemistry. As mentioned, there are grounds for instead referring to this compound as astatine hydride. It is easily oxidized; acidification by dilute nitric acid gives the At or At forms, and the subsequent addition of silver(I) may only partially, at best, precipitate astatine as silver(I) astatide (AgAt). Iodine, in contrast, is not oxidized, and precipitates readily as silver(I) iodide.

Astatine is known to bind to boron, carbon, and nitrogen. Various boron cage compounds have been prepared with At–B bonds, these being more stable than At–C bonds. Astatine can replace a hydrogen atom in benzene to form astatobenzene CHAt; this may be oxidized to CHAtCl by chlorine. By treating this compound with an alkaline solution of hypochlorite, CHAtO can be produced. The dipyridine-astatine(I) cation, [At(CHN)], forms ionic compounds with perchlorate (a non-coordinating anion) and with nitrate, [At(CHN)]NO. This cation exists as a coordination complex in which two dative covalent bonds separately link the astatine(I) centre with each of the pyridine rings via their nitrogen atoms.

With oxygen, there is evidence of the species AtO and AtO in aqueous solution, formed by the reaction of astatine with an oxidant such as elemental bromine or (in the last case) by sodium persulfate in a solution of perchloric acid. The species previously thought to be has since been determined to be , a hydrolysis product of AtO (another such hydrolysis product being AtOOH). The well characterized anion can be obtained by, for example, the oxidation of astatine with potassium hypochlorite in a solution of potassium hydroxide. Preparation of lanthanum triastatate La(AtO), following the oxidation of astatine by a hot NaSO solution, has been reported. Further oxidation of , such as by xenon difluoride (in a hot alkaline solution) or periodate (in a neutral or alkaline solution), yields the perastatate ion ; this is only stable in neutral or alkaline solutions. Astatine is also thought to be capable of forming cations in salts with oxyanions such as iodate or dichromate; this is based on the observation that, in acidic solutions, monovalent or intermediate positive states of astatine coprecipitate with the insoluble salts of metal cations such as silver(I) iodate or thallium(I) dichromate.

Astatine may form bonds to the other chalcogens; these include SAt and with sulfur, a coordination selenourea compound with selenium, and an astatine–tellurium colloid with tellurium.

Astatine is known to react with its lighter homologs iodine, bromine, and chlorine in the vapor state; these reactions produce diatomic interhalogen compounds with formulas AtI, AtBr, and AtCl. The first two compounds may also be produced in water – astatine reacts with iodine/iodide solution to form AtI, whereas AtBr requires (aside from astatine) an iodine/iodine monobromide/bromide solution. The excess of iodides or bromides may lead to and ions, or in a chloride solution, they may produce species like or via equilibrium reactions with the chlorides. Oxidation of the element with dichromate (in nitric acid solution) showed that adding chloride turned the astatine into a molecule likely to be either AtCl or AtOCl. Similarly, or may be produced. The polyhalides PdAtI, CsAtI, TlAtI, and PbAtI are known or presumed to have been precipitated. In a plasma ion source mass spectrometer, the ions [AtI], [AtBr], and [AtCl] have been formed by introducing lighter halogen vapors into a helium-filled cell containing astatine, supporting the existence of stable neutral molecules in the plasma ion state. No astatine fluorides have been discovered yet. Their absence has been speculatively attributed to the extreme reactivity of such compounds, including the reaction of an initially formed fluoride with the walls of the glass container to form a non-volatile product. Thus, although the synthesis of an astatine fluoride is thought to be possible, it may require a liquid halogen fluoride solvent, as has already been used for the characterization of radon fluoride.

In 1869, when Dmitri Mendeleev published his periodic table, the space under iodine was empty; after Niels Bohr established the physical basis of the classification of chemical elements, it was suggested that the fifth halogen belonged there. Before its officially recognized discovery, it was called "eka-iodine" (from Sanskrit "eka" – "one") to imply it was one space under iodine (in the same manner as eka-silicon, eka-boron, and others). Scientists tried to find it in nature; given its extreme rarity, these attempts resulted in several false discoveries.

The first claimed discovery of eka-iodine was made by Fred Allison and his associates at the Alabama Polytechnic Institute (now Auburn University) in 1931. The discoverers named element 85 "alabamine", and assigned it the symbol Ab, designations that were used for a few years. In 1934, H. G. MacPherson of University of California, Berkeley disproved Allison's method and the validity of his discovery. There was another claim in 1937, by the chemist Rajendralal De. Working in Dacca in British India (now Dhaka in Bangladesh), he chose the name "dakin" for element 85, which he claimed to have isolated as the thorium series equivalent of radium F (polonium-210) in the radium series. The properties he reported for dakin do not correspond to those of astatine; moreover, astatine is not found in the thorium series, and the true identity of dakin is not known.

In 1936, the team of Romanian physicist Horia Hulubei and French physicist Yvette Cauchois claimed to have discovered element 85 via X-ray analysis. In 1939, they published another paper which supported and extended previous data. In 1944, Hulubei published a summary of data he had obtained up to that time, claiming it was supported by the work of other researchers. He chose the name "dor", presumably from the Romanian for "longing" [for peace], as World War II had started five years earlier. As Hulubei was writing in French, a language which does not accommodate the "ine" suffix, dor would likely have been rendered in English as "dorine", had it been adopted. In 1947, Hulubei's claim was effectively rejected by the Austrian chemist Friedrich Paneth, who would later chair the IUPAC committee responsible for recognition of new elements. Even though Hulubei's samples did contain astatine, his means to detect it were too weak, by current standards, to enable correct identification. He had also been involved in an earlier false claim as to the discovery of element 87 (francium) and this is thought to have caused other researchers to downplay his work.
In 1940, the Swiss chemist Walter Minder announced the discovery of element 85 as the beta decay product of radium A (polonium-218), choosing the name "helvetium" (from , the Latin name of Switzerland). Karlik and Bernert were unsuccessful in reproducing his experiments, and subsequently attributed Minder's results to contamination of his radon stream (radon-222 is the parent isotope of polonium-218). In 1942, Minder, in collaboration with the English scientist Alice Leigh-Smith, announced the discovery of another isotope of element 85, presumed to be the product of thorium A (polonium-216) beta decay. They named this substance "anglo-helvetium", but Karlik and Bernert were again unable to reproduce these results.

Later in 1940, Dale R. Corson, Kenneth Ross MacKenzie, and Emilio Segrè isolated the element at the University of California, Berkeley. Instead of searching for the element in nature, the scientists created it by bombarding bismuth-209 with alpha particles in a cyclotron (particle accelerator) to produce, after emission of two neutrons, astatine-211. The discoverers, however, did not immediately suggest a name for the element. The reason for this was that at the time, an element created synthetically in "invisible quantities" that had not yet discovered in nature was not seen as a completely valid one; in addition, chemists were reluctant to recognize radioactive isotopes as legitimately as stable ones. In 1943, astatine was found as a product of two naturally occurring decay chains by Berta Karlik and Traude Bernert, first in the so-called uranium series, and then in the actinium series. (Since then, astatine was also found in a third decay chain, the neptunium series.) Friedrich Paneth in 1946 called to finally recognize synthetic elements, quoting, among other reasons, recent confirmation of their natural occurrence, and proposed that the discoverers of the newly discovered unnamed elements name these elements. In early 1947, "Nature" published the discoverers' suggestions; a letter from Corson, MacKenzie, and Segrè suggested the name "astatine" coming from the Greek "astatos" (αστατος) meaning "unstable", because of its propensity for radioactive decay, with the ending "-ine", found in the names of the four previously discovered halogens. The name was also chosen to continue the tradition of the four stable halogens, where the name referred to a property of the element.

Corson and his colleagues classified astatine as a metal on the basis of its analytical chemistry. Subsequent investigators reported iodine-like, cationic, or amphoteric behavior. In a 2003 retrospective, Corson wrote that "some of the properties [of astatine] are similar to iodine … it also exhibits metallic properties, more like its metallic neighbors Po and Bi."

There are 39 known isotopes of astatine, with atomic masses (mass numbers) of 191–229. Theoretical modeling suggests that 37 more isotopes could exist. No stable or long-lived astatine isotope has been observed, nor is one expected to exist.

Astatine's alpha decay energies follow the same trend as for other heavy elements. Lighter astatine isotopes have quite high energies of alpha decay, which become lower as the nuclei become heavier. Astatine-211 has a significantly higher energy than the previous isotope, because it has a nucleus with 126 neutrons, and 126 is a magic number corresponding to a filled neutron shell. Despite having a similar half-life to the previous isotope (8.1 hours for astatine-210 and 7.2 hours for astatine-211), the alpha decay probability is much higher for the latter: 41.81% against only 0.18%. The two following isotopes release even more energy, with astatine-213 releasing the most energy. For this reason, it is the shortest-lived astatine isotope. Even though heavier astatine isotopes release less energy, no long-lived astatine isotope exists, because of the increasing role of beta decay (electron emission). This decay mode is especially important for astatine; as early as 1950 it was postulated that all isotopes of the element undergo beta decay, though nuclear mass measurements indicate that At is in fact beta-stable, as it has the lowest mass of all isobars with "A" = 215. A beta decay mode has been found for all other astatine isotopes except for astatine-213, astatine-214, and astatine-216m. Astatine-210 and lighter isotopes exhibit beta plus decay (positron emission), astatine-216 and heavier isotopes exhibit beta minus decay, and astatine-212 decays via both modes, while astatine-211 undergoes electron capture.

The most stable isotope is astatine-210, which has a half-life of 8.1 hours. The primary decay mode is beta plus, to the relatively long-lived (in comparison to astatine isotopes) alpha emitter polonium-210. In total, only five isotopes have half-lives exceeding one hour (astatine-207 to -211). The least stable ground state isotope is astatine-213, with a half-life of 125 nanoseconds. It undergoes alpha decay to the extremely long-lived bismuth-209.

Astatine has 24 known nuclear isomers, which are nuclei with one or more nucleons (protons or neutrons) in an excited state. A nuclear isomer may also be called a "meta-state", meaning the system has more internal energy than the "ground state" (the state with the lowest possible internal energy), making the former likely to decay into the latter. There may be more than one isomer for each isotope. The most stable of these nuclear isomers is astatine-202m1, which has a half-life of about 3 minutes, longer than those of all the ground states bar those of isotopes 203–211 and 220. The least stable is astatine-214m1; its half-life of 265 nanoseconds is shorter than those of all ground states except that of astatine-213.

Astatine is the rarest naturally occurring element. The total amount of astatine in the Earth's crust (quoted mass 2.36 × 10 grams) is estimated by some to be less than one gram at any given time. Other sources estimate the amount of ephemeral astatine, present on earth at any given moment, to be up to one ounce (about 28 grams).

Any astatine present at the formation of the Earth has long since disappeared; the four naturally occurring isotopes (astatine-215, -217, -218 and -219) are instead continuously produced as a result of the decay of radioactive thorium and uranium ores, and trace quantities of neptunium-237. The landmass of North and South America combined, to a depth of 16 kilometers (10 miles), contains only about one trillion astatine-215 atoms at any given time (around 3.5 × 10 grams). Astatine-217 is produced via the radioactive decay of neptunium-237. Primordial remnants of the latter isotope—due to its relatively short half-life of 2.14 million years—are no longer present on Earth. However, trace amounts occur naturally as a product of transmutation reactions in uranium ores. Astatine-218 was the first astatine isotope discovered in nature. Astatine-219, with a half-life of 56 seconds, is the longest lived of the naturally occurring isotopes.

Isotopes of astatine are sometimes not listed as naturally occurring because of misconceptions that there are no such isotopes, or discrepancies in the literature. Astatine-216 has been counted as a naturally occurring isotope but reports of its observation (which were described as doubtful) have not been confirmed.

Astatine was first produced by bombarding bismuth-209 with energetic alpha particles, and this is still the major route used to create the relatively long-lived isotopes astatine-209 through astatine-211. Astatine is only produced in minuscule quantities, with modern techniques allowing production runs of up to 6.6 giga becquerels (about 86 nanograms or 2.47 × 10 atoms). Synthesis of greater quantities of astatine using this method is constrained by the limited availability of suitable cyclotrons and the prospect of melting the target. Solvent radiolysis due to the cumulative effect of astatine decay is a related problem. With cryogenic technology, microgram quantities of astatine might be able to be generated via proton irradiation of thorium or uranium to yield radon-211, in turn decaying to astatine-211. Contamination with astatine-210 is expected to be a drawback of this method.

The most important isotope is astatine-211, the only one in commercial use. To produce the bismuth target, the metal is sputtered onto a gold, copper, or aluminium surface at 50 to 100 milligrams per square centimeter. Bismuth oxide can be used instead; this is forcibly fused with a copper plate. The target is kept under a chemically neutral nitrogen atmosphere, and is cooled with water to prevent premature astatine vaporization. In a particle accelerator, such as a cyclotron, alpha particles are collided with the bismuth. Even though only one bismuth isotope is used (bismuth-209), the reaction may occur in three possible ways, producing astatine-209, astatine-210, or astatine-211. In order to eliminate undesired nuclides, the maximum energy of the particle accelerator is set to a value (optimally 29.17 MeV) above that for the reaction producing astatine-211 (to produce the desired isotope) and below the one producing astatine-210 (to avoid producing other astatine isotopes).

Since astatine is the main product of the synthesis, after its formation it must only be separated from the target and any significant contaminants. Several methods are available, "but they generally follow one of two approaches—dry distillation or [wet] acid treatment of the target followed by solvent extraction." The methods summarized below are modern adaptations of older procedures, as reviewed by Kugler and Keller. Pre-1985 techniques more often addressed the elimination of co-produced toxic polonium; this requirement is now mitigated by capping the energy of the cyclotron irradiation beam.

The astatine-containing cyclotron target is heated to a temperature of around 650 °C. The astatine volatilizes and is condensed in (typically) a cold trap. Higher temperatures of up to around 850 °C may increase the yield, at the risk of bismuth contamination from concurrent volatilization. Redistilling the condensate may be required to minimize the presence of bismuth (as bismuth can interfere with astatine labeling reactions). The astatine is recovered from the trap using one or more low concentration solvents such as sodium hydroxide, methanol or chloroform. Astatine yields of up to around 80% may be achieved. Dry separation is the method most commonly used to produce a chemically useful form of astatine.

The irradiated bismuth (or sometimes bismuth trioxide) target is first dissolved in, for example, concentrated nitric or perchloric acid. Following this first step, the acid can be distilled away to leave behind a white residue that contains both bismuth and the desired astatine product. This residue is then dissolved in a concentrated acid, such as hydrochloric acid. Astatine is extracted from this acid using an organic solvent such as butyl or isopropyl ether, diisopropylether (DIPE), or thiosemicarbazide. Using liquid-liquid extraction, the astatine product can be repeatedly washed with an acid, such as HCl, and extracted into the organic solvent layer. A separation yield of 93% using nitric acid has been reported, falling to 72% by the time purification procedures were completed (distillation of nitric acid, purging residual nitrogen oxides, and redissolving bismuth nitrate to enable liquid–liquid extraction). Wet methods involve "multiple radioactivity handling steps" and have not been considered well suited for isolating larger quantities of astatine. However, wet extraction methods are being examined for use in production of larger quantities of astatine-211, as it is thought that wet extraction methods can provide more consistency. They can enable the production of astatine in a specific oxidation state and may have greater applicability in experimental radiochemistry.

Newly formed astatine-211 is the subject of ongoing research in nuclear medicine. It must be used quickly as it decays with a half-life of 7.2 hours; this is long enough to permit multistep labeling strategies. Astatine-211 has potential for targeted alpha-particle therapy, since it decays either via emission of an alpha particle (to bismuth-207), or via electron capture (to an extremely short-lived nuclide, polonium-211, which undergoes further alpha decay), very quickly reaching its stable granddaughter lead-207. Polonium X-rays emitted as a result of the electron capture branch, in the range of 77–92 keV, enable the tracking of astatine in animals and patients. Although astatine-210 has a slightly longer half-life, it is wholly unsuitable because it usually undergoes beta plus decay to the extremely toxic polonium-210.

The principal medicinal difference between astatine-211 and iodine-131 (a radioactive iodine isotope also used in medicine) is that iodine-131 emits high-energy beta particles, and astatine does not. Beta particles have much greater penetrating power through tissues than do the much heavier alpha particles. An average alpha particle released by astatine-211 can travel up to 70 µm through surrounding tissues; an average-energy beta particle emitted by iodine-131 can travel nearly 30 times as far, to about 2 mm. The short half-life and limited penetrating power of alpha radiation through tissues offers advantages in situations where the "tumor burden is low and/or malignant cell populations are located in close proximity to essential normal tissues." Significant morbidity in cell culture models of human cancers has been achieved with from one to ten astatine-211 atoms bound per cell.
Several obstacles have been encountered in the development of astatine-based radiopharmaceuticals for cancer treatment. World War II delayed research for close to a decade. Results of early experiments indicated that a cancer-selective carrier would need to be developed and it was not until the 1970s that monoclonal antibodies became available for this purpose. Unlike iodine, astatine shows a tendency to dehalogenate from molecular carriers such as these, particularly at sp carbon sites (less so from sp sites). Given the toxicity of astatine accumulated and retained in the body, this emphasized the need to ensure it remained attached to its host molecule. While astatine carriers that are slowly metabolized can be assessed for their efficacy, more rapidly metabolized carriers remain a significant obstacle to the evaluation of astatine in nuclear medicine. Mitigating the effects of astatine-induced radiolysis of labeling chemistry and carrier molecules is another area requiring further development. A practical application for astatine as a cancer treatment would potentially be suitable for a "staggering" number of patients; production of astatine in the quantities that would be required remains an issue.

Animal studies show that astatine, similarly to iodine – although to a lesser extent, perhaps because of its slightly more metallic nature  – is preferentially (and dangerously) concentrated in the thyroid gland. Unlike iodine, astatine also shows a tendency to be taken up by the lungs and spleen, possibly because of in-body oxidation of At to At. If administered in the form of a radiocolloid it tends to concentrate in the liver. Experiments in rats and monkeys suggest that astatine-211 causes much greater damage to the thyroid gland than does iodine-131, with repetitive injection of the nuclide resulting in necrosis and cell dysplasia within the gland. Early research suggested that injection of astatine into female rodents caused morphological changes in breast tissue; this conclusion remained controversial for many years. General agreement was later reached that this was likely caused by the effect of breast tissue irradiation combined with hormonal changes due to irradiation of the ovaries. Trace amounts of astatine can be handled safely in fume hoods if they are well-aerated; biological uptake of the element must be avoided.




</doc>
<doc id="902" url="https://en.wikipedia.org/wiki?curid=902" title="Atom">
Atom

An atom is the smallest unit of ordinary matter that forms a chemical element. Every solid, liquid, gas, and plasma is composed of neutral or ionized atoms. Atoms are extremely small, typically around 100 picometers across. They are so small that accurately predicting their behavior using classical physics—as if they were billiard balls, for example—is not possible due to quantum effects.

Every atom is composed of a nucleus and one or more electrons bound to the nucleus. The nucleus is made of one or more protons and a number of neutrons. Only the most common variety of hydrogen has no neutrons. More than 99.94% of an atom's mass is in the nucleus. The protons have a positive electric charge, the electrons have a negative electric charge, and the neutrons have no electric charge. If the number of protons and electrons are equal, then the atom is electrically neutral. If an atom has more or fewer electrons than protons, then it has an overall negative or positive charge, respectively — such atoms are called ions.

The electrons of an atom are attracted to the protons in an atomic nucleus by the electromagnetic force. The protons and neutrons in the nucleus are attracted to each other by the nuclear force. This force is usually stronger than the electromagnetic force that repels the positively charged protons from one another. Under certain circumstances, the repelling electromagnetic force becomes stronger than the nuclear force. In this case, the nucleus splits and leaves behind different elements. This is a form of nuclear decay.

The number of protons in the nucleus is the "atomic number" and it defines to which chemical element the atom belongs. For example, any atom that contains 29 protons is copper. The number of neutrons defines the isotope of the element. Atoms can attach to one or more other atoms by chemical bonds to form chemical compounds such as molecules or crystals. The ability of atoms to associate and dissociate is responsible for most of the physical changes observed in nature. Chemistry is the discipline that studies these changes.

The basic idea that matter is made up of tiny indivisible particles is very old, appearing in many ancient cultures such as Greece and India. This ancient idea was based in philosophical reasoning rather than scientific reasoning. The word "atom" is derived from the Greek word "atomos", which means "uncuttable".

In the early 1800s, John Dalton compiled experimental data gathered by himself and other scientists and noticed that chemical elements seemed to combine by weight in ratios of small whole numbers. This pattern is now known as the "law of multiple proportions". For instance, there are two types of tin oxide: one is a black powder that is 88.1% tin and 11.9% oxygen, and the other is a white powder that is 78.7% tin and 21.3% oxygen. Adjusting these figures, for every 100 g of tin there is 13.5 g of oxygen in the black oxide and 27 g of oxygen in the white oxide. 13.5 and 27 form a ratio of 1:2, a ratio of small whole numbers. Similarly, there are two iron oxides in which for every 100 g of iron there is either 28 g or 42 g of oxygen respectively, which gives a ratio of 2:3. As a final example, there are three oxides of nitrogen in which for every 140 g of nitrogen, there is 80 g, 160 g, and 320 g of oxygen respectively, which gives a ratio of 1:2:4. This recurring pattern in the data suggested that each element reacts in multiples of some basic unit of mass, which Dalton concluded were atoms. In the case of the tin oxides, for every one tin atom, there are either one or two oxygen atoms (SnO and SnO). In the case of the iron oxides, for every two iron atoms, there are either two or three oxygen atoms (FeO and FeO). In the case of the nitrogen oxides, their formulas are NO, NO, and NO respectively.

In the late 18th century, a number of scientists found that they could better explain the behavior of gases by describing them as collections of sub-microscopic particles and modelling their behavior using statistics and probability. Unlike Dalton's atomic theory, the kinetic theory of gases describes not how gases react chemically with each other to form compounds, but how they behave physically: diffusion, viscosity, conductivity, pressure, etc.

In 1827, botanist Robert Brown used a microscope to look at dust grains floating in water and discovered that they moved about erratically, a phenomenon that became known as "Brownian motion". This was thought to be caused by water molecules knocking the grains about. In 1905, Albert Einstein proved the reality of these molecules and their motions by producing the first statistical physics analysis of Brownian motion. French physicist Jean Perrin used Einstein's work to experimentally determine the mass and dimensions of molecules, thereby providing physical evidence for the particle nature of matter.

In 1897, J.J. Thomson discovered that cathode rays are not electromagnetic waves but made of particles that are 1,800 times lighter than hydrogen (the lightest atom). Therefore, they were not atoms, but a new particle, the first "subatomic" particle to be discovered. He called these new particles "corpuscles" but they were later renamed "electrons". Thomson also showed that electrons were identical to particles given off by photoelectric and radioactive materials. It was quickly recognized that electrons are the particles that carry electric currents in metal wires. Thomson concluded that these electrons emerged from the very atoms of the cathode in his instruments, which meant that atoms are not indivisible as the name "atomos" suggests.

J. J. Thomson thought that the negatively-charged electrons were distributed throughout the atom in a sea of positive charge that was distributed across the whole volume of the atom. This model is sometimes known as the plum pudding model.

Ernest Rutherford and his colleagues Hans Geiger and Ernest Marsden came to have doubts about the Thomson model after they encountered difficulties when they tried to build an instrument to measure the charge-to-mass ratio of alpha particles (these are positively-charged particles emitted by certain radioactive substances such as radium). The alpha particles were being scattered by the air in the detection chamber, which made the measurements unreliable. Thomson had encountered a similar problem in his work on cathode rays, which he solved by creating a near-perfect vacuum in his instruments. Rutherford didn't think he'd run into this same problem because alpha particles are much heavier than electrons. According to Thomson's model of the atom, the positive charge in the atom is not concentrated enough to produce an electric field strong enough to deflect an alpha particle, and the electrons are so lightweight they should be pushed aside effortlessly by the much heavier alpha particles. Yet there was scattering, so Rutherford and his colleagues decided to investigate this scattering carefully.

Between 1908 and 1913, Rutheford and his colleagues performed a series of experiments in which they bombarded thin foils of metal with alpha particles. They spotted alpha particles being deflected by angles greater than 90°. To explain this, Rutherford proposed that the positive charge of the atom is not distributed throughout the atom's volume as Thomson believed, but is concentrated in a tiny nucleus at the center. Only such an intense concentration of charge could produce an electric field strong enough to deflect the alpha particles as observed.

While experimenting with the products of radioactive decay, in 1913 radiochemist Frederick Soddy discovered that there appeared to be more than one type of atom at each position on the periodic table. The term isotope was coined by Margaret Todd as a suitable name for different atoms that belong to the same element. J.J. Thomson created a technique for isotope separation through his work on ionized gases, which subsequently led to the discovery of stable isotopes.

In 1913 the physicist Niels Bohr proposed a model in which the electrons of an atom were assumed to orbit the nucleus but could only do so in a finite set of orbits, and could jump between these orbits only in discrete changes of energy corresponding to absorption or radiation of a photon. This quantization was used to explain why the electrons' orbits are stable (given that normally, charges in acceleration, including circular motion, lose kinetic energy which is emitted as electromagnetic radiation, see "synchrotron radiation") and why elements absorb and emit electromagnetic radiation in discrete spectra.

Later in the same year Henry Moseley provided additional experimental evidence in favor of Niels Bohr's theory. These results refined Ernest Rutherford's and Antonius Van den Broek's model, which proposed that the atom contains in its nucleus a number of positive nuclear charges that is equal to its (atomic) number in the periodic table. Until these experiments, atomic number was not known to be a physical and experimental quantity. That it is equal to the atomic nuclear charge remains the accepted atomic model today.

Chemical bonds between atoms were explained by Gilbert Newton Lewis in 1916, as the interactions between their constituent electrons. As the chemical properties of the elements were known to largely repeat themselves according to the periodic law, in 1919 the American chemist Irving Langmuir suggested that this could be explained if the electrons in an atom were connected or clustered in some manner. Groups of electrons were thought to occupy a set of electron shells about the nucleus.

The Bohr model of the atom was the first complete physical model of the atom. It described the overall structure of the atom, how atoms bond to each other, and predicted the spectral lines of hydrogen. Bohr's model was not perfect and was soon superseded by the more accurate Schroedinger model (see below), but it was sufficient to evaporate any remaining doubts that matter is composed of atoms. For chemists, the idea of the atom had been a useful heuristic tool, but physicists had doubts as to whether matter really is made up of atoms as nobody had yet developed a complete physical model of the atom.

The Stern-Gerlach experiment of 1922 provided further evidence of the quantum nature of atomic properties. When a beam of silver atoms was passed through a specially shaped magnetic field, the beam was split in a way correlated with the direction of an atom's angular momentum, or spin. As this spin direction is initially random, the beam would be expected to deflect in a random direction. Instead, the beam was split into two directional components, corresponding to the atomic spin being oriented up or down with respect to the magnetic field.

In 1925 Werner Heisenberg published the first consistent mathematical formulation of quantum mechanics (matrix mechanics). One year earlier, Louis de Broglie had proposed the de Broglie hypothesis: that all particles behave like waves to some extent, and in 1926 Erwin Schrödinger used this idea to develop the Schrödinger equation, a mathematical model of the atom (wave mechanics) that described the electrons as three-dimensional waveforms rather than point particles. 

A consequence of using waveforms to describe particles is that it is mathematically impossible to obtain precise values for both the position and momentum of a particle at a given point in time; this became known as the uncertainty principle, formulated by Werner Heisenberg in 1927. In this concept, for a given accuracy in measuring a position one could only obtain a range of probable values for momentum, and vice versa.
This model was able to explain observations of atomic behavior that previous models could not, such as certain structural and spectral patterns of atoms larger than hydrogen. Thus, the planetary model of the atom was discarded in favor of one that described atomic orbital zones around the nucleus where a given electron is most likely to be observed.

The development of the mass spectrometer allowed the mass of atoms to be measured with increased accuracy. The device uses a magnet to bend the trajectory of a beam of ions, and the amount of deflection is determined by the ratio of an atom's mass to its charge. The chemist Francis William Aston used this instrument to show that isotopes had different masses. The atomic mass of these isotopes varied by integer amounts, called the whole number rule. The explanation for these different isotopes awaited the discovery of the neutron, an uncharged particle with a mass similar to the proton, by the physicist James Chadwick in 1932. Isotopes were then explained as elements with the same number of protons, but different numbers of neutrons within the nucleus.

In 1938, the German chemist Otto Hahn, a student of Rutherford, directed neutrons onto uranium atoms expecting to get transuranium elements. Instead, his chemical experiments showed barium as a product. A year later, Lise Meitner and her nephew Otto Frisch verified that Hahn's result were the first experimental "nuclear fission". In 1944, Hahn received the Nobel prize in chemistry. Despite Hahn's efforts, the contributions of Meitner and Frisch were not recognized.

In the 1950s, the development of improved particle accelerators and particle detectors allowed scientists to study the impacts of atoms moving at high energies. Neutrons and protons were found to be hadrons, or composites of smaller particles called quarks. The standard model of particle physics was developed that so far has successfully explained the properties of the nucleus in terms of these sub-atomic particles and the forces that govern their interactions.

Though the word "atom" originally denoted a particle that cannot be cut into smaller particles, in modern scientific usage the atom is composed of various subatomic particles. The constituent particles of an atom are the electron, the proton and the neutron.

The electron is by far the least massive of these particles at , with a negative electrical charge and a size that is too small to be measured using available techniques. It was the lightest particle with a positive rest mass measured, until the discovery of neutrino mass. Under ordinary conditions, electrons are bound to the positively charged nucleus by the attraction created from opposite electric charges. If an atom has more or fewer electrons than its atomic number, then it becomes respectively negatively or positively charged as a whole; a charged atom is called an ion. Electrons have been known since the late 19th century, mostly thanks to J.J. Thomson; see history of subatomic physics for details.

Protons have a positive charge and a mass 1,836 times that of the electron, at . The number of protons in an atom is called its atomic number. Ernest Rutherford (1919) observed that nitrogen under alpha-particle bombardment ejects what appeared to be hydrogen nuclei. By 1920 he had accepted that the hydrogen nucleus is a distinct particle within the atom and named it proton.

Neutrons have no electrical charge and have a free mass of 1,839 times the mass of the electron, or . Neutrons are the heaviest of the three constituent particles, but their mass can be reduced by the nuclear binding energy. Neutrons and protons (collectively known as nucleons) have comparable dimensions—on the order of —although the 'surface' of these particles is not sharply defined. The neutron was discovered in 1932 by the English physicist James Chadwick.

In the Standard Model of physics, electrons are truly elementary particles with no internal structure, whereas protons and neutrons are composite particles composed of elementary particles called quarks. There are two types of quarks in atoms, each having a fractional electric charge. Protons are composed of two up quarks (each with charge +) and one down quark (with a charge of −). Neutrons consist of one up quark and two down quarks. This distinction accounts for the difference in mass and charge between the two particles.

The quarks are held together by the strong interaction (or strong force), which is mediated by gluons. The protons and neutrons, in turn, are held to each other in the nucleus by the nuclear force, which is a residuum of the strong force that has somewhat different range-properties (see the article on the nuclear force for more). The gluon is a member of the family of gauge bosons, which are elementary particles that mediate physical forces.

All the bound protons and neutrons in an atom make up a tiny atomic nucleus, and are collectively called nucleons. The radius of a nucleus is approximately equal to 1.07  fm, where "A" is the total number of nucleons. This is much smaller than the radius of the atom, which is on the order of 10 fm. The nucleons are bound together by a short-ranged attractive potential called the residual strong force. At distances smaller than 2.5 fm this force is much more powerful than the electrostatic force that causes positively charged protons to repel each other.

Atoms of the same element have the same number of protons, called the atomic number. Within a single element, the number of neutrons may vary, determining the isotope of that element. The total number of protons and neutrons determine the nuclide. The number of neutrons relative to the protons determines the stability of the nucleus, with certain isotopes undergoing radioactive decay.

The proton, the electron, and the neutron are classified as fermions. Fermions obey the Pauli exclusion principle which prohibits "identical" fermions, such as multiple protons, from occupying the same quantum state at the same time. Thus, every proton in the nucleus must occupy a quantum state different from all other protons, and the same applies to all neutrons of the nucleus and to all electrons of the electron cloud.

A nucleus that has a different number of protons than neutrons can potentially drop to a lower energy state through a radioactive decay that causes the number of protons and neutrons to more closely match. As a result, atoms with matching numbers of protons and neutrons are more stable against decay, but with increasing atomic number, the mutual repulsion of the protons requires an increasing proportion of neutrons to maintain the stability of the nucleus.

The number of protons and neutrons in the atomic nucleus can be modified, although this can require very high energies because of the strong force. Nuclear fusion occurs when multiple atomic particles join to form a heavier nucleus, such as through the energetic collision of two nuclei. For example, at the core of the Sun protons require energies of 3 to 10 keV to overcome their mutual repulsion—the coulomb barrier—and fuse together into a single nucleus. Nuclear fission is the opposite process, causing a nucleus to split into two smaller nuclei—usually through radioactive decay. The nucleus can also be modified through bombardment by high energy subatomic particles or photons. If this modifies the number of protons in a nucleus, the atom changes to a different chemical element.

If the mass of the nucleus following a fusion reaction is less than the sum of the masses of the separate particles, then the difference between these two values can be emitted as a type of usable energy (such as a gamma ray, or the kinetic energy of a beta particle), as described by Albert Einstein's mass-energy equivalence formula, formula_1, where formula_2 is the mass loss and formula_3 is the speed of light. This deficit is part of the binding energy of the new nucleus, and it is the non-recoverable loss of the energy that causes the fused particles to remain together in a state that requires this energy to separate.

The fusion of two nuclei that create larger nuclei with lower atomic numbers than iron and nickel—a total nucleon number of about 60—is usually an exothermic process that releases more energy than is required to bring them together. It is this energy-releasing process that makes nuclear fusion in stars a self-sustaining reaction. For heavier nuclei, the binding energy per nucleon in the nucleus begins to decrease. That means fusion processes producing nuclei that have atomic numbers higher than about 26, and atomic masses higher than about 60, is an endothermic process. These more massive nuclei can not undergo an energy-producing fusion reaction that can sustain the hydrostatic equilibrium of a star.

The electrons in an atom are attracted to the protons in the nucleus by the electromagnetic force. This force binds the electrons inside an electrostatic potential well surrounding the smaller nucleus, which means that an external source of energy is needed for the electron to escape. The closer an electron is to the nucleus, the greater the attractive force. Hence electrons bound near the center of the potential well require more energy to escape than those at greater separations.

Electrons, like other particles, have properties of both a particle and a wave. The electron cloud is a region inside the potential well where each electron forms a type of three-dimensional standing wave—a wave form that does not move relative to the nucleus. This behavior is defined by an atomic orbital, a mathematical function that characterises the probability that an electron appears to be at a particular location when its position is measured. Only a discrete (or quantized) set of these orbitals exist around the nucleus, as other possible wave patterns rapidly decay into a more stable form. Orbitals can have one or more ring or node structures, and differ from each other in size, shape and orientation.

Each atomic orbital corresponds to a particular energy level of the electron. The electron can change its state to a higher energy level by absorbing a photon with sufficient energy to boost it into the new quantum state. Likewise, through spontaneous emission, an electron in a higher energy state can drop to a lower energy state while radiating the excess energy as a photon. These characteristic energy values, defined by the differences in the energies of the quantum states, are responsible for atomic spectral lines.

The amount of energy needed to remove or add an electron—the electron binding energy—is far less than the binding energy of nucleons. For example, it requires only 13.6 eV to strip a ground-state electron from a hydrogen atom, compared to 2.23 "million" eV for splitting a deuterium nucleus. Atoms are electrically neutral if they have an equal number of protons and electrons. Atoms that have either a deficit or a surplus of electrons are called ions. Electrons that are farthest from the nucleus may be transferred to other nearby atoms or shared between atoms. By this mechanism, atoms are able to bond into molecules and other types of chemical compounds like ionic and covalent network crystals.

By definition, any two atoms with an identical number of "protons" in their nuclei belong to the same chemical element. Atoms with equal numbers of protons but a different number of "neutrons" are different isotopes of the same element. For example, all hydrogen atoms admit exactly one proton, but isotopes exist with no neutrons (hydrogen-1, by far the most common form, also called protium), one neutron (deuterium), two neutrons (tritium) and more than two neutrons. The known elements form a set of atomic numbers, from the single-proton element hydrogen up to the 118-proton element oganesson. All known isotopes of elements with atomic numbers greater than 82 are radioactive, although the radioactivity of element 83 (bismuth) is so slight as to be practically negligible.

About 339 nuclides occur naturally on Earth, of which 252 (about 74%) have not been observed to decay, and are referred to as "stable isotopes". Only 90 nuclides are stable theoretically, while another 162 (bringing the total to 252) have not been observed to decay, even though in theory it is energetically possible. These are also formally classified as "stable". An additional 34 radioactive nuclides have half-lives longer than 100 million years, and are long-lived enough to have been present since the birth of the solar system. This collection of 286 nuclides are known as primordial nuclides. Finally, an additional 53 short-lived nuclides are known to occur naturally, as daughter products of primordial nuclide decay (such as radium from uranium), or as products of natural energetic processes on Earth, such as cosmic ray bombardment (for example, carbon-14).

For 80 of the chemical elements, at least one stable isotope exists. As a rule, there is only a handful of stable isotopes for each of these elements, the average being 3.2 stable isotopes per element. Twenty-six elements have only a single stable isotope, while the largest number of stable isotopes observed for any element is ten, for the element tin. Elements 43, 61, and all elements numbered 83 or higher have no stable isotopes.

Stability of isotopes is affected by the ratio of protons to neutrons, and also by the presence of certain "magic numbers" of neutrons or protons that represent closed and filled quantum shells. These quantum shells correspond to a set of energy levels within the shell model of the nucleus; filled shells, such as the filled shell of 50 protons for tin, confers unusual stability on the nuclide. Of the 252 known stable nuclides, only four have both an odd number of protons "and" odd number of neutrons: hydrogen-2 (deuterium), lithium-6, boron-10 and nitrogen-14. Also, only four naturally occurring, radioactive odd-odd nuclides have a half-life over a billion years: potassium-40, vanadium-50, lanthanum-138 and tantalum-180m. Most odd-odd nuclei are highly unstable with respect to beta decay, because the decay products are even-even, and are therefore more strongly bound, due to nuclear pairing effects.

The large majority of an atom's mass comes from the protons and neutrons that make it up. The total number of these particles (called "nucleons") in a given atom is called the mass number. It is a positive integer and dimensionless (instead of having dimension of mass), because it expresses a count. An example of use of a mass number is "carbon-12," which has 12 nucleons (six protons and six neutrons).

The actual mass of an atom at rest is often expressed in daltons (Da), also called the unified atomic mass unit (u). This unit is defined as a twelfth of the mass of a free neutral atom of carbon-12, which is approximately . Hydrogen-1 (the lightest isotope of hydrogen which is also the nuclide with the lowest mass) has an atomic weight of 1.007825 Da. The value of this number is called the atomic mass. A given atom has an atomic mass approximately equal (within 1%) to its mass number times the atomic mass unit (for example the mass of a nitrogen-14 is roughly 14 Da), but this number will not be exactly an integer except (by definition) in the case of carbon-12. The heaviest stable atom is lead-208, with a mass of .

As even the most massive atoms are far too light to work with directly, chemists instead use the unit of moles. One mole of atoms of any element always has the same number of atoms (about ). This number was chosen so that if an element has an atomic mass of 1 u, a mole of atoms of that element has a mass close to one gram. Because of the definition of the unified atomic mass unit, each carbon-12 atom has an atomic mass of exactly 12 Da, and so a mole of carbon-12 atoms weighs exactly 0.012 kg.

Atoms lack a well-defined outer boundary, so their dimensions are usually described in terms of an atomic radius. This is a measure of the distance out to which the electron cloud extends from the nucleus. This assumes the atom to exhibit a spherical shape, which is only obeyed for atoms in vacuum or free space. Atomic radii may be derived from the distances between two nuclei when the two atoms are joined in a chemical bond. The radius varies with the location of an atom on the atomic chart, the type of chemical bond, the number of neighboring atoms (coordination number) and a quantum mechanical property known as spin. On the periodic table of the elements, atom size tends to increase when moving down columns, but decrease when moving across rows (left to right). Consequently, the smallest atom is helium with a radius of 32 pm, while one of the largest is caesium at 225 pm.

When subjected to external forces, like electrical fields, the shape of an atom may deviate from spherical symmetry. The deformation depends on the field magnitude and the orbital type of outer shell electrons, as shown by group-theoretical considerations. Aspherical deviations might be elicited for instance in crystals, where large crystal-electrical fields may occur at low-symmetry lattice sites. Significant ellipsoidal deformations have been shown to occur for sulfur ions and chalcogen ions in pyrite-type compounds.

Atomic dimensions are thousands of times smaller than the wavelengths of light (400–700 nm) so they cannot be viewed using an optical microscope, although individual atoms can be observed using a scanning tunneling microscope. To visualize the minuteness of the atom, consider that a typical human hair is about 1 million carbon atoms in width. A single drop of water contains about 2 sextillion () atoms of oxygen, and twice the number of hydrogen atoms. A single carat diamond with a mass of contains about 10 sextillion (10) atoms of carbon. If an apple were magnified to the size of the Earth, then the atoms in the apple would be approximately the size of the original apple.

Every element has one or more isotopes that have unstable nuclei that are subject to radioactive decay, causing the nucleus to emit particles or electromagnetic radiation. Radioactivity can occur when the radius of a nucleus is large compared with the radius of the strong force, which only acts over distances on the order of 1 fm.

The most common forms of radioactive decay are:

Other more rare types of radioactive decay include ejection of neutrons or protons or clusters of nucleons from a nucleus, or more than one beta particle. An analog of gamma emission which allows excited nuclei to lose energy in a different way, is internal conversion—a process that produces high-speed electrons that are not beta rays, followed by production of high-energy photons that are not gamma rays. A few large nuclei explode into two or more charged fragments of varying masses plus several neutrons, in a decay called spontaneous nuclear fission.

Each radioactive isotope has a characteristic decay time period—the half-life—that is determined by the amount of time needed for half of a sample to decay. This is an exponential decay process that steadily decreases the proportion of the remaining isotope by 50% every half-life. Hence after two half-lives have passed only 25% of the isotope is present, and so forth.

Elementary particles possess an intrinsic quantum mechanical property known as spin. This is analogous to the angular momentum of an object that is spinning around its center of mass, although strictly speaking these particles are believed to be point-like and cannot be said to be rotating. Spin is measured in units of the reduced Planck constant (ħ), with electrons, protons and neutrons all having spin ½ ħ, or "spin-½". In an atom, electrons in motion around the nucleus possess orbital angular momentum in addition to their spin, while the nucleus itself possesses angular momentum due to its nuclear spin.

The magnetic field produced by an atom—its magnetic moment—is determined by these various forms of angular momentum, just as a rotating charged object classically produces a magnetic field, but the most dominant contribution comes from electron spin. Due to the nature of electrons to obey the Pauli exclusion principle, in which no two electrons may be found in the same quantum state, bound electrons pair up with each other, with one member of each pair in a spin up state and the other in the opposite, spin down state. Thus these spins cancel each other out, reducing the total magnetic dipole moment to zero in some atoms with even number of electrons.

In ferromagnetic elements such as iron, cobalt and nickel, an odd number of electrons leads to an unpaired electron and a net overall magnetic moment. The orbitals of neighboring atoms overlap and a lower energy state is achieved when the spins of unpaired electrons are aligned with each other, a spontaneous process known as an exchange interaction. When the magnetic moments of ferromagnetic atoms are lined up, the material can produce a measurable macroscopic field. Paramagnetic materials have atoms with magnetic moments that line up in random directions when no magnetic field is present, but the magnetic moments of the individual atoms line up in the presence of a field.

The nucleus of an atom will have no spin when it has even numbers of both neutrons and protons, but for other cases of odd numbers, the nucleus may have a spin. Normally nuclei with spin are aligned in random directions because of thermal equilibrium, but for certain elements (such as xenon-129) it is possible to polarize a significant proportion of the nuclear spin states so that they are aligned in the same direction—a condition called hyperpolarization. This has important applications in magnetic resonance imaging.

The potential energy of an electron in an atom is negative relative to when the distance from the nucleus goes to infinity; its dependence on the electron's position reaches the minimum inside the nucleus, roughly in inverse proportion to the distance. In the quantum-mechanical model, a bound electron can occupy only a set of states centered on the nucleus, and each state corresponds to a specific energy level; see time-independent Schrödinger equation for a theoretical explanation. An energy level can be measured by the amount of energy needed to unbind the electron from the atom, and is usually given in units of electronvolts (eV). The lowest energy state of a bound electron is called the ground state, i.e. stationary state, while an electron transition to a higher level results in an excited state. The electron's energy increases along with "n" because the (average) distance to the nucleus increases. Dependence of the energy on is caused not by the electrostatic potential of the nucleus, but by interaction between electrons.

For an electron to transition between two different states, e.g. ground state to first excited state, it must absorb or emit a photon at an energy matching the difference in the potential energy of those levels, according to the Niels Bohr model, what can be precisely calculated by the Schrödinger equation.
Electrons jump between orbitals in a particle-like fashion. For example, if a single photon strikes the electrons, only a single electron changes states in response to the photon; see Electron properties.

The energy of an emitted photon is proportional to its frequency, so these specific energy levels appear as distinct bands in the electromagnetic spectrum. Each element has a characteristic spectrum that can depend on the nuclear charge, subshells filled by electrons, the electromagnetic interactions between the electrons and other factors.

When a continuous spectrum of energy is passed through a gas or plasma, some of the photons are absorbed by atoms, causing electrons to change their energy level. Those excited electrons that remain bound to their atom spontaneously emit this energy as a photon, traveling in a random direction, and so drop back to lower energy levels. Thus the atoms behave like a filter that forms a series of dark absorption bands in the energy output. (An observer viewing the atoms from a view that does not include the continuous spectrum in the background, instead sees a series of emission lines from the photons emitted by the atoms.) Spectroscopic measurements of the strength and width of atomic spectral lines allow the composition and physical properties of a substance to be determined.

Close examination of the spectral lines reveals that some display a fine structure splitting. This occurs because of spin-orbit coupling, which is an interaction between the spin and motion of the outermost electron. When an atom is in an external magnetic field, spectral lines become split into three or more components; a phenomenon called the Zeeman effect. This is caused by the interaction of the magnetic field with the magnetic moment of the atom and its electrons. Some atoms can have multiple electron configurations with the same energy level, which thus appear as a single spectral line. The interaction of the magnetic field with the atom shifts these electron configurations to slightly different energy levels, resulting in multiple spectral lines. The presence of an external electric field can cause a comparable splitting and shifting of spectral lines by modifying the electron energy levels, a phenomenon called the Stark effect.

If a bound electron is in an excited state, an interacting photon with the proper energy can cause stimulated emission of a photon with a matching energy level. For this to occur, the electron must drop to a lower energy state that has an energy difference matching the energy of the interacting photon. The emitted photon and the interacting photon then move off in parallel and with matching phases. That is, the wave patterns of the two photons are synchronized. This physical property is used to make lasers, which can emit a coherent beam of light energy in a narrow frequency band.

Valency is the combining power of an element. It is determined by the number of bonds it can form to other atoms or groups. The outermost electron shell of an atom in its uncombined state is known as the valence shell, and the electrons in
that shell are called valence electrons. The number of valence electrons determines the bonding
behavior with other atoms. Atoms tend to chemically react with each other in a manner that fills (or empties) their outer valence shells. For example, a transfer of a single electron between atoms is a useful approximation for bonds that form between atoms with one-electron more than a filled shell, and others that are one-electron short of a full shell, such as occurs in the compound sodium chloride and other chemical ionic salts. Many elements display multiple valences, or tendencies to share differing numbers of electrons in different compounds. Thus, chemical bonding between these elements takes many forms of electron-sharing that are more than simple electron transfers. Examples include the element carbon and the organic compounds.

The chemical elements are often displayed in a periodic table that is laid out to display recurring chemical properties, and elements with the same number of valence electrons form a group that is aligned in the same column of the table. (The horizontal rows correspond to the filling of a quantum shell of electrons.) The elements at the far right of the table have their outer shell completely filled with electrons, which results in chemically inert elements known as the noble gases.

Quantities of atoms are found in different states of matter that depend on the physical conditions, such as temperature and pressure. By varying the conditions, materials can transition between solids, liquids, gases and plasmas. Within a state, a material can also exist in different allotropes. An example of this is solid carbon, which can exist as graphite or diamond. Gaseous allotropes exist as well, such as dioxygen and ozone.

At temperatures close to absolute zero, atoms can form a Bose-Einstein condensate, at which point quantum mechanical effects, which are normally only observed at the atomic scale, become apparent on a macroscopic scale. This super-cooled collection of atoms
then behaves as a single super atom, which may allow fundamental checks of quantum mechanical behavior.

While atoms are too small to be seen, devices such as the scanning tunneling microscope (STM) enable their visualization at the surfaces of solids. The microscope uses the quantum tunneling phenomenon, which allows particles to pass through a barrier that would be insurmountable in the classical perspective. Electrons tunnel through the vacuum between two biased electrodes, providing a tunneling current that is exponentially dependent on their separation. One electrode is a sharp tip ideally ending with a single atom. At each point of the scan of the surface the tip's height is adjusted so as to keep the tunneling current at a set value. How much the tip moves to and away from the surface is interpreted as the height profile. For low bias, the microscope images the averaged electron orbitals across closely packed energy levels—the local density of the electronic states near the Fermi level. Because of the distances involved, both electrodes need to be extremely stable; only then periodicities can be observed that correspond to individual atoms. The method alone is not chemically specific, and cannot identify the atomic species present at the surface.

Atoms can be easily identified by their mass. If an atom is ionized by removing one of its electrons, its trajectory when it passes through a magnetic field will bend. The radius by which the trajectory of a moving ion is turned by the magnetic field is determined by the mass of the atom. The mass spectrometer uses this principle to measure the mass-to-charge ratio of ions. If a sample contains multiple isotopes, the mass spectrometer can determine the proportion of each isotope in the sample by measuring the intensity of the different beams of ions. Techniques to vaporize atoms include inductively coupled plasma atomic emission spectroscopy and inductively coupled plasma mass spectrometry, both of which use a plasma to vaporize samples for analysis.

The atom-probe tomograph has sub-nanometer resolution in 3-D and can chemically identify individual atoms using time-of-flight mass spectrometry.

Electron emission techniques such as X-ray photoelectron spectroscopy (XPS) and Auger electron spectroscopy (AES), which measure the binding energies of the core electrons, are used to identify the atomic species present in a sample in a non-destructive way. With proper focusing both can be made area-specific. Another such method is electron energy loss spectroscopy (EELS), which measures the energy loss of an electron beam within a transmission electron microscope when it interacts with a portion of a sample.

Spectra of excited states can be used to analyze the atomic composition of distant stars. Specific light wavelengths contained in the observed light from stars can be separated out and related to the quantized transitions in free gas atoms. These colors can be replicated using a gas-discharge lamp containing the same element. Helium was discovered in this way in the spectrum of the Sun 23 years before it was found on Earth.

Baryonic matter forms about 4% of the total energy density of the observable Universe, with an average density of about 0.25 particles/m (mostly protons and electrons). Within a galaxy such as the Milky Way, particles have a much higher concentration, with the density of matter in the interstellar medium (ISM) ranging from 10 to 10 atoms/m. The Sun is believed to be inside the Local Bubble, so the density in the solar neighborhood is only about 10 atoms/m. Stars form from dense clouds in the ISM, and the evolutionary processes of stars result in the steady enrichment of the ISM with elements more massive than hydrogen and helium.

Up to 95% of the Milky Way's baryonic matter are concentrated inside stars, where conditions are unfavorable for atomic matter. The total baryonic mass is about 10% of the mass of the galaxy; the remainder of the mass is an unknown dark matter. High temperature inside stars makes most "atoms" fully ionized, that is, separates "all" electrons from the nuclei. In stellar remnants—with exception of their surface layers—an immense pressure make electron shells impossible.

Electrons are thought to exist in the Universe since early stages of the Big Bang. Atomic nuclei forms in nucleosynthesis reactions. In about three minutes Big Bang nucleosynthesis produced most of the helium, lithium, and deuterium in the Universe, and perhaps some of the beryllium and boron.

Ubiquitousness and stability of atoms relies on their binding energy, which means that an atom has a lower energy than an unbound system of the nucleus and electrons. Where the temperature is much higher than ionization potential, the matter exists in the form of plasma—a gas of positively charged ions (possibly, bare nuclei) and electrons. When the temperature drops below the ionization potential, atoms become statistically favorable. Atoms (complete with bound electrons) became to dominate over charged particles 380,000 years after the Big Bang—an epoch called recombination, when the expanding Universe cooled enough to allow electrons to become attached to nuclei.

Since the Big Bang, which produced no carbon or heavier elements, atomic nuclei have been combined in stars through the process of nuclear fusion to produce more of the element helium, and (via the triple alpha process) the sequence of elements from carbon up to iron; see stellar nucleosynthesis for details.

Isotopes such as lithium-6, as well as some beryllium and boron are generated in space through cosmic ray spallation. This occurs when a high-energy proton strikes an atomic nucleus, causing large numbers of nucleons to be ejected.

Elements heavier than iron were produced in supernovae and colliding neutron stars through the r-process, and in AGB stars through the s-process, both of which involve the capture of neutrons by atomic nuclei. Elements such as lead formed largely through the radioactive decay of heavier elements.

Most of the atoms that make up the Earth and its inhabitants were present in their current form in the nebula that collapsed out of a molecular cloud to form the Solar System. The rest are the result of radioactive decay, and their relative proportion can be used to determine the age of the Earth through radiometric dating. Most of the helium in the crust of the Earth (about 99% of the helium from gas wells, as shown by its lower abundance of helium-3) is a product of alpha decay.

There are a few trace atoms on Earth that were not present at the beginning (i.e., not "primordial"), nor are results of radioactive decay. Carbon-14 is continuously generated by cosmic rays in the atmosphere. Some atoms on Earth have been artificially generated either deliberately or as by-products of nuclear reactors or explosions. Of the transuranic elements—those with atomic numbers greater than 92—only plutonium and neptunium occur naturally on Earth. Transuranic elements have radioactive lifetimes shorter than the current age of the Earth and thus identifiable quantities of these elements have long since decayed, with the exception of traces of plutonium-244 possibly deposited by cosmic dust. Natural deposits of plutonium and neptunium are produced by neutron capture in uranium ore.

The Earth contains approximately atoms. Although small numbers of independent atoms of noble gases exist, such as argon, neon, and helium, 99% of the atmosphere is bound in the form of molecules, including carbon dioxide and diatomic oxygen and nitrogen. At the surface of the Earth, an overwhelming majority of atoms combine to form various compounds, including water, salt, silicates and oxides. Atoms can also combine to create materials that do not consist of discrete molecules, including crystals and liquid or solid metals. This atomic matter forms networked arrangements that lack the particular type of small-scale interrupted order associated with molecular matter.

All nuclides with atomic numbers higher than 82 (lead) are known to be radioactive. No nuclide with an atomic number exceeding 92 (uranium) exists on Earth as a primordial nuclide, and heavier elements generally have shorter half-lives. Nevertheless, an "island of stability" encompassing relatively long-lived isotopes of superheavy elements with atomic numbers 110 to 114 might exist. Predictions for the half-life of the most stable nuclide on the island range from a few minutes to millions of years. In any case, superheavy elements (with "Z" > 104) would not exist due to increasing Coulomb repulsion (which results in spontaneous fission with increasingly short half-lives) in the absence of any stabilizing effects.

Each particle of matter has a corresponding antimatter particle with the opposite electrical charge. Thus, the positron is a positively charged antielectron and the antiproton is a negatively charged equivalent of a proton. When a matter and corresponding antimatter particle meet, they annihilate each other. Because of this, along with an imbalance between the number of matter and antimatter particles, the latter are rare in the universe. The first causes of this imbalance are not yet fully understood, although theories of baryogenesis may offer an explanation. As a result, no antimatter atoms have been discovered in nature. In 1996 the antimatter counterpart of the hydrogen atom (antihydrogen) was synthesized at the CERN laboratory in Geneva.

Other exotic atoms have been created by replacing one of the protons, neutrons or electrons with other particles that have the same charge. For example, an electron can be replaced by a more massive muon, forming a muonic atom. These types of atoms can be used to test fundamental predictions of physics.





</doc>
<doc id="903" url="https://en.wikipedia.org/wiki?curid=903" title="Arable land">
Arable land

Arable land (, "able to be plowed") is any land capable of being ploughed and used to grow crops. Alternatively, for the purposes of agricultural statistics, the term often has a more precise definition: "Arable land is the land under temporary agricultural crops (multiple-cropped areas are counted only once), temporary meadows for mowing or pasture, land under market and kitchen gardens and land temporarily fallow (less than five years). The abandoned land resulting from shifting cultivation is not included in this category. Data for 'Arable land' are not meant to indicate the amount of land that is potentially cultivable." A more concise definition appearing in the Eurostat glossary similarly refers to actual rather than potential uses: "land worked (ploughed or tilled) regularly, generally under a system of crop rotation".

Non-arable land can sometimes be converted to arable land through methods such as loosening and tilling (breaking up) of the soil, though in more extreme cases the degree of modification required to make certain types of land arable can be prohibitively expensive. In Britain, arable land has traditionally been contrasted with pasturable land such as heaths, which could be used for sheep-rearing but not as farmland.

According to the Food and Agriculture Organization of the United Nations, in the year 2013, the world's arable land amounted to 1,407 million hectares, out of a total of 4,924 million hectares of land used for agriculture.

Agricultural land that is not arable according to the FAO definition above includes:

Other non-arable land includes land that is not suitable for any agricultural use. Land that is not arable, in the sense of lacking capability or suitability for cultivation for crop production, has one or more limitations – a lack of sufficient fresh water for irrigation, stoniness, steepness, adverse climate, excessive wetness with impracticality of drainage, and/or excessive salts, among others. Although such limitations may preclude cultivation, and some will in some cases preclude any agricultural use, large areas unsuitable for cultivation may still be agriculturally productive. For example, US NRCS statistics indicate that about 59 percent of US non-federal pasture and unforested rangeland is unsuitable for cultivation, yet such land has value for grazing of livestock. In British Columbia, Canada, 41 percent of the provincial Agricultural Land Reserve area is unsuitable for production of cultivated crops, but is suitable for uncultivated production of forage usable by grazing livestock. Similar examples can be found in many rangeland areas elsewhere.

Land incapable of being cultivated for production of crops can sometimes be converted to arable land. New arable land makes more food, and can reduce starvation. This outcome also makes a country more self-sufficient and politically independent, because food importation is reduced. Making non-arable land arable often involves digging new irrigation canals and new wells, aqueducts, desalination plants, planting trees for shade in the desert, hydroponics, fertilizer, nitrogen fertilizer, pesticides, reverse osmosis water processors, PET film insulation or other insulation against heat and cold, digging ditches and hills for protection against the wind, and installing greenhouses with internal light and heat for protection against the cold outside and to provide light in cloudy areas. Such modifications are often prohibitively expensive. An alternative is the seawater greenhouse, which desalinates water through evaporation and condensation using solar energy as the only energy input. This technology is optimized to grow crops on desert land close to the sea.

Examples of infertile non-arable land being turned into fertile arable land include:

Examples of fertile arable land being turned into infertile land include:




</doc>
<doc id="904" url="https://en.wikipedia.org/wiki?curid=904" title="Aluminium">
Aluminium

Aluminium (aluminum in American and Canadian English) is a chemical element with the symbol Al and atomic number 13. It is a silvery-white, soft, non-magnetic and ductile metal in the boron group. By mass, aluminium makes up about 8% of the Earth's crust, where it is the third most abundant element (after oxygen and silicon) and also the most abundant metal. Occurrence of aluminium decreases in the Earth's mantle below, however. The chief ore of aluminium is bauxite. Aluminium metal is highly reactive, such that native specimens are rare and limited to extreme reducing environments. Instead, it is found combined in over 270 different minerals.

Aluminium is remarkable for its low density and its ability to resist corrosion through the phenomenon of passivation. Aluminium and its alloys are vital to the aerospace industry and important in transportation and building industries, such as building facades and window frames. The oxides and sulfates are the most useful compounds of aluminium.

Despite its prevalence in the environment, no living organism is known to use aluminium salts metabolically, but aluminium is well tolerated by plants and animals. Because of these salts' abundance, the potential for a biological role for them is of continuing interest, and studies continue.

Of aluminium isotopes, only is stable. This is consistent with aluminium having an odd atomic number. It is the only primordial aluminium isotope, i.e. the only one that has existed on Earth in its current form since the creation of the planet. Nearly all aluminium on Earth is present as this isotope, which makes it a mononuclidic element and means that its standard atomic weight is the same as that of the isotope. The standard atomic weight of aluminium is low in comparison with many other metals, which has consequences for the element's properties (see below). This makes aluminium very useful in nuclear magnetic resonance (NMR), as its single stable isotope has a high NMR sensitivity.

All other isotopes of aluminium are radioactive. The most stable of these is Al: while it was present along with stable Al in the interstellar medium from which the Solar System formed, having been produced by stellar nucleosynthesis as well, its half-life is only 717,000 years and therefore it could not have survived since the formation of the planet. However, minute traces of Al are produced from argon in the atmosphere by spallation caused by cosmic ray protons. The ratio of Al to Be has been used for radiodating of geological processes over 10 to 10 year time scales, in particular transport, deposition, sediment storage, burial times, and erosion. Most meteorite scientists believe that the energy released by the decay of Al was responsible for the melting and differentiation of some asteroids after their formation 4.55 billion years ago.

The remaining isotopes of aluminium, with mass numbers ranging from 22 to 43, all have half-lives well under an hour. Three metastable states are known, all with half-lives under a minute.

An aluminium atom has 13 electrons, arranged in an electron configuration of [Ne] 3s 3p, with three electrons beyond a stable noble gas configuration. Accordingly, the combined first three ionization energies of aluminium are far lower than the fourth ionization energy alone. Such an electron configuration is shared with the other well-characterized members of its group, boron, gallium, indium, and thallium; it is also expected for nihonium. Aluminium can relatively easily surrender its three outermost electrons in many chemical reactions (see below). The electronegativity of aluminium is 1.61 (Pauling scale).

A free aluminium atom has a radius of 143 pm. With the three outermost electrons removed, the radius shrinks to 39 pm for a 4-coordinated atom or 53.5 pm for a 6-coordinated atom. At standard temperature and pressure, aluminium atoms (when not affected by atoms of other elements) form a face-centered cubic crystal system bound by metallic bonding provided by atoms' outermost electrons; hence aluminium (at these conditions) is a metal. This crystal system is shared by many other metals, such as lead and copper; the size of a unit cell of aluminium is comparable to that of those other metals. It is however not shared by the other members of its group; boron has ionization energies too high to allow metallization, thallium has a hexagonal close-packed structure, and gallium and indium have unusual structures that are not close-packed like those of aluminium and thallium. Since few electrons are available for metallic bonding, aluminium metal is soft with a low melting point and low electrical resistivity, as is common for post-transition metals.

Aluminium metal has an appearance ranging from silvery white to dull gray, depending on the surface roughness. A fresh film of aluminium serves as a good reflector (approximately 92%) of visible light and an excellent reflector (as much as 98%) of medium and far infrared radiation.

The density of aluminium is 2.70 g/cm, about 1/3 that of steel, much lower than other commonly encountered metals, making aluminium parts easily identifiable through their lightness. Aluminium's low density compared to most other metals arises from the fact that its nuclei are much lighter, while difference in the unit cell size does not compensate for this difference. The only lighter metals are the metals of groups 1 and 2, which apart from beryllium and magnesium are too reactive for structural use (and beryllium is very toxic). Aluminium is not as strong or stiff as steel, but the low density makes up for this in the aerospace industry and for many other applications where light weight and relatively high strength are crucial.

Pure aluminium is quite soft and lacking in strength. In most applications various aluminium alloys are used instead because of their higher strength and hardness. The yield strength of pure aluminium is 7–11 MPa, while aluminium alloys have yield strengths ranging from 200 MPa to 600 MPa. Aluminium is ductile, with a percent elongation of 50-70%, and malleable allowing it to be easily drawn and extruded. It is also easily machined, and the low melting temperature of 660 °C allows for easy casting.

Aluminium is an excellent thermal and electrical conductor, having 59% the conductivity of copper, both thermal and electrical, while having only 30% of copper's density. Aluminium is capable of superconductivity, with a superconducting critical temperature of 1.2 kelvin and a critical magnetic field of about 100 gauss (10 milliteslas). It is paramagnetic and thus essentially unaffected by static magnetic fields. The high electrical conductivity, however, means that it is strongly affected by alternating magnetic fields through the induction of eddy currents.

Aluminium combines characteristics of pre- and post-transition metals. Since it has few available electrons for metallic bonding, like its heavier group 13 congeners, it has the characteristic physical properties of a post-transition metal, with longer-than-expected interatomic distances. Furthermore, as Al is a small and highly charged cation, it is strongly polarizing and aluminium compounds tend towards covalency; this behaviour is similar to that of beryllium (Be), and the two display an example of a diagonal relationship.

The underlying core under aluminium's valence shell is that of the preceding noble gas, whereas those of its heavier congeners gallium and indium, thallium, and nihonium also include a filled d-subshell and in some cases a filled f-subshell. Hence, the inner electrons of aluminium shield the valence electrons almost completely, unlike those of aluminium's heavier congeners. As such, aluminium is the most electropositive metal in its group, and its hydroxide is in fact more basic than that of gallium. In fact, aluminium's electropositive behavior, high affinity for oxygen, and highly negative standard electrode potential are all more similar to those of scandium, yttrium, lanthanum, and actinium, which like aluminium have three valence electrons outside a noble gas core. Aluminium also bears minor similarities to the metalloid boron in the same group: AlX compounds are valence isoelectronic to BX compounds (they have the same valence electronic structure), and both behave as Lewis acids and readily form adducts. Additionally, one of the main motifs of boron chemistry is regular icosahedral structures, and aluminium forms an important part of many icosahedral quasicrystal alloys, including the Al–Zn–Mg class.

Aluminium has a high chemical affinity to oxygen, which renders it suitable for use as a reducing agent in the thermite reaction. A fine powder of aluminium metal reacts explosively on contact with liquid oxygen; under normal conditions, however, aluminium forms a thin oxide layer (~ 5 nm at room temperature) that protects the metal from further corrosion by oxygen, water, or dilute acid, a process termed passivation. Because of its general resistance to corrosion, aluminium is one of the few metals that retains silvery reflectance in finely powdered form, making it an important component of silver-colored paints. Aluminium is not attacked by oxidizing acids because of its passivation. This allows aluminium to be used to store reagents such as nitric acid, concentrated sulfuric acid, and some organic acids.

In hot concentrated hydrochloric acid, aluminium reacts with water with evolution of hydrogen, and in aqueous sodium hydroxide or potassium hydroxide at room temperature to form aluminates—protective passivation under these conditions is negligible. Aqua regia also dissolves aluminium. Aluminium is corroded by dissolved chlorides, such as common sodium chloride, which is why household plumbing is never made from aluminium. The oxide layer on aluminium is also destroyed by contact with mercury due to amalgamation or with salts of some electropositive metals. As such, the strongest aluminium alloys are less corrosion-resistant due to galvanic reactions with alloyed copper, and aluminium's corrosion resistance is greatly reduced by aqueous salts, particularly in the presence of dissimilar metals.

Aluminium reacts with most nonmetals upon heating, forming compounds such as aluminium nitride (AlN), aluminium sulfide (AlS), and the aluminium halides (AlX). It also forms a wide range of intermetallic compounds involving metals from every group on the periodic table.

The vast majority of compounds, including all aluminium-containing minerals and all commercially significant aluminium compounds, feature aluminium in the oxidation state 3+. The coordination number of such compounds varies, but generally Al is either six- or four-coordinate. Almost all compounds of aluminium(III) are colorless.
In aqueous solution, Al exists as the hexaaqua cation [Al(HO)], which has an approximate pK of 10. Such solutions are acidic as this cation can act as a proton donor and progressively hydrolyse until a precipitate of aluminium hydroxide, Al(OH), forms. This is useful for clarification of water, as the precipitate nucleates on suspended particles in the water, hence removing them. Increasing the pH even further leads to the hydroxide dissolving again as aluminate, [Al(HO)(OH)], is formed.

Aluminium hydroxide forms both salts and aluminates and dissolves in acid and alkali, as well as on fusion with acidic and basic oxides. This behaviour of Al(OH) is termed amphoterism, and is characteristic of weakly basic cations that form insoluble hydroxides and whose hydrated species can also donate their protons. One effect of this is that aluminium salts with weak acids are hydrolysed in water to the aquated hydroxide and the corresponding nonmetal hydride: for example, aluminium sulfide yields hydrogen sulfide. However, some salts like aluminium carbonate exist in aqueous solution but are unstable as such; and only incomplete hydrolysis takes place for salts with strong acids, such as the halides, nitrate, and sulfate. For similar reasons, anhydrous aluminium salts cannot be made by heating their "hydrates": hydrated aluminium chloride is in fact not AlCl·6HO but [Al(HO)]Cl, and the Al–O bonds are so strong that heating is not sufficient to break them and form Al–Cl bonds instead:

All four trihalides are well known. Unlike the structures of the three heavier trihalides, aluminium fluoride (AlF) features six-coordinate aluminium, which explains its involatility and insolubility as well as high heat of formation. Each aluminium atom is surrounded by six fluorine atoms in a distorted octahedral arrangement, with each fluorine atom being shared between the corners of two octahedra. Such {AlF} units also exist in complex fluorides such as cryolite, NaAlF. AlF melts at and is made by reaction of aluminium oxide with hydrogen fluoride gas at .

With heavier halides, the coordination numbers are lower. The other trihalides are dimeric or polymeric with tetrahedral four-coordinate aluminium centers. Aluminium trichloride (AlCl) has a layered polymeric structure below its melting point of but transforms on melting to AlCl dimers. At higher temperatures those increasingly dissociate into trigonal planar AlCl monomers similar to the structure of BCl. Aluminium tribromide and aluminium triiodide form AlX dimers in all three phases and hence do not show such significant changes of properties upon phase change. These materials are prepared by treating aluminium metal with the halogen. The aluminium trihalides form many addition compounds or complexes; their Lewis acidic nature makes them useful as catalysts for the Friedel–Crafts reactions. Aluminium trichloride has major industrial uses involving this reaction, such as in the manufacture of anthraquinones and styrene; it is also often used as the precursor for many other aluminium compounds and as a reagent for converting nonmetal fluorides into the corresponding chlorides (a transhalogenation reaction).

Aluminium forms one stable oxide with the chemical formula AlO, commonly called alumina. It can be found in nature in the mineral corundum, α-alumina; there is also a γ-alumina phase. Its crystalline form, corundum, is very hard (Mohs hardness 9), has a high melting point of , has very low volatility, is chemically inert, and a good electrical insulator, it is often used in abrasives (such as toothpaste), as a refractory material, and in ceramics, as well as being the starting material for the electrolytic production of aluminium metal. Sapphire and ruby are impure corundum contaminated with trace amounts of other metals. The two main oxide-hydroxides, AlO(OH), are boehmite and diaspore. There are three main trihydroxides: bayerite, gibbsite, and nordstrandite, which differ in their crystalline structure (polymorphs). Many other intermediate and related structures are also known. Most are produced from ores by a variety of wet processes using acid and base. Heating the hydroxides leads to formation of corundum. These materials are of central importance to the production of aluminium and are themselves extremely useful. Some mixed oxide phases are also very useful, such as spinel (MgAlO), Na-β-alumina (NaAlO), and tricalcium aluminate (CaAlO, an important mineral phase in Portland cement).

The only stable chalcogenides under normal conditions are aluminium sulfide (AlS), selenide (AlSe), and telluride (AlTe). All three are prepared by direct reaction of their elements at about and quickly hydrolyse completely in water to yield aluminium hydroxide and the respective hydrogen chalcogenide. As aluminium is a small atom relative to these chalcogens, these have four-coordinate tetrahedral aluminium with various polymorphs having structures related to wurtzite, with two-thirds of the possible metal sites occupied either in an orderly (α) or random (β) fashion; the sulfide also has a γ form related to γ-alumina, and an unusual high-temperature hexagonal form where half the aluminium atoms have tetrahedral four-coordination and the other half have trigonal bipyramidal five-coordination.

Four pnictides – aluminium nitride (AlN), aluminium phosphide (AlP), aluminium arsenide (AlAs), and aluminium antimonide (AlSb) – are known. They are all III-V semiconductors isoelectronic to silicon and germanium, all of which but AlN have the zinc blende structure. All four can be made by high-temperature (and possibly high-pressure) direct reaction of their component elements.

Although the great majority of aluminium compounds feature Al centers, compounds with lower oxidation states are known and are sometimes of significance as precursors to the Al species.

AlF, AlCl, AlBr, and AlI exist in the gaseous phase when the respective trihalide is heated with aluminium, and at cryogenic temperatures. Their instability in the condensed phase is due to their ready disproportionation to aluminium and the respective trihalide: the reverse reaction is favored at high temperature (although even then they are still short-lived), explaining why AlF is more volatile when heated in the presence of aluminium metal, as is aluminium metal when heated in the presence of AlCl. A stable derivative of aluminium monoiodide is the cyclic adduct formed with triethylamine, . Also of theoretical interest but only of fleeting existence are AlO and AlS. AlO is made by heating the normal oxide, AlO, with silicon at in a vacuum. Such materials quickly disproportionate to the starting materials.

Very simple Al(II) compounds are invoked or observed in the reactions of Al metal with oxidants. For example, aluminium monoxide, AlO, has been detected in the gas phase after explosion and in stellar absorption spectra. More thoroughly investigated are compounds of the formula RAl which contain an Al–Al bond and where R is a large organic ligand.

A variety of compounds of empirical formula AlR and AlRCl exist. The aluminium trialkyls and triaryls are reactive, volatile, and colorless liquids or low-melting solids. They catch fire spontaneously in air and react with water, thus necessitating precautions when handling them. They often form dimers, unlike their boron analogues, but this tendency diminishes for branched-chain alkyls (e.g. Pr, Bu, MeCCH); for example, triisobutylaluminium exists as an equilibrium mixture of the monomer and dimer. These dimers, such as trimethylaluminium (AlMe), usually feature tetrahedral Al centers formed by dimerization with some alkyl group bridging between both aluminium atoms. They are hard acids and react readily with ligands, forming adducts. In industry, they are mostly used in alkene insertion reactions, as discovered by Karl Ziegler, most importantly in "growth reactions" that form long-chain unbranched primary alkenes and alcohols, and in the low-pressure polymerization of ethene and propene. There are also some heterocyclic and cluster organoaluminium compounds involving Al–N bonds.

The industrially most important aluminium hydride is lithium aluminium hydride (LiAlH), which is used in as a reducing agent in organic chemistry. It can be produced from lithium hydride and aluminium trichloride. The simplest hydride, aluminium hydride or alane, is not as important. It is a polymer with the formula (AlH), in contrast to the corresponding boron hydride that is a dimer with the formula (BH).

Aluminium's per-particle abundance in the Solar System is 3.15 ppm (parts per million). It is the twelfth most abundant of all elements and third most abundant among the elements that have odd atomic numbers, after hydrogen and nitrogen. The only stable isotope of aluminium, Al, is the eighteenth most abundant nucleus in the Universe. It is created almost entirely after fusion of carbon in massive stars that will later become Type II supernovae: this fusion creates Mg, which, upon capturing free protons and neutrons becomes aluminium. Some smaller quantities of Al are created in hydrogen burning shells of evolved stars, where Mg can capture free protons. Essentially all aluminium now in existence is Al; Al was present in the early Solar System but is currently extinct. However, the trace quantities of Al that do exist are the most common gamma ray emitter in the interstellar gas.

Overall, the Earth is about 1.59% aluminium by mass (seventh in abundance by mass). Aluminium occurs in greater proportion in the Earth's crust than in the Universe at large, because aluminium easily forms the oxide and becomes bound into rocks and stays in the Earth's crust, while less reactive metals sink to the core. In the Earth's crust, aluminium is the most abundant (8.23% by mass) metallic element and the third most abundant of all elements (after oxygen and silicon). A large number of silicates in the Earth's crust contain aluminium. In contrast, the Earth's mantle is only 2.38% aluminium by mass. Aluminium also occurs in seawater at a concentration of 2 μg/kg.

Because of its strong affinity for oxygen, aluminium is almost never found in the elemental state; instead it is found in oxides or silicates. Feldspars, the most common group of minerals in the Earth's crust, are aluminosilicates. Aluminium also occurs in the minerals beryl, cryolite, garnet, spinel, and turquoise. Impurities in AlO, such as chromium and iron, yield the gemstones ruby and sapphire, respectively. Native aluminium metal can only be found as a minor phase in low oxygen fugacity environments, such as the interiors of certain volcanoes. Native aluminium has been reported in cold seeps in the northeastern continental slope of the South China Sea. It is possible that these deposits resulted from bacterial reduction of tetrahydroxoaluminate Al(OH).

Although aluminium is a common and widespread element, not all aluminium minerals are economically viable sources of the metal. Almost all metallic aluminium is produced from the ore bauxite (AlO(OH)). Bauxite occurs as a weathering product of low iron and silica bedrock in tropical climatic conditions. In 2017, most bauxite was mined in Australia, China, Guinea, and India.

The history of aluminium has been shaped by usage of alum. The first written record of alum, made by Greek historian Herodotus, dates back to the 5th century BCE. The ancients are known to have used alum as a dyeing mordant and for city defense. After the Crusades, alum, an indispensable good in the European fabric industry, was a subject of international commerce; it was imported to Europe from the eastern Mediterranean until the mid-15th century.

The nature of alum remained unknown. Around 1530, Swiss physician Paracelsus suggested alum was a salt of an earth of alum. In 1595, German doctor and chemist Andreas Libavius experimentally confirmed this. In 1722, German chemist Friedrich Hoffmann announced his belief that the base of alum was a distinct earth. In 1754, German chemist Andreas Sigismund Marggraf synthesized alumina by boiling clay in sulfuric acid and subsequently adding potash.

Attempts to produce aluminium metal date back to 1760. The first successful attempt, however, was completed in 1824 by Danish physicist and chemist Hans Christian Ørsted. He reacted anhydrous aluminium chloride with potassium amalgam, yielding a lump of metal looking similar to tin. He presented his results and demonstrated a sample of the new metal in 1825. In 1827, German chemist Friedrich Wöhler repeated Ørsted's experiments but did not identify any aluminium. (The reason for this inconsistency was only discovered in 1921.) He conducted a similar experiment in the same year by mixing anhydrous aluminium chloride with potassium and produced a powder of aluminium. In 1845, he was able to produce small pieces of the metal and described some physical properties of this metal. For many years thereafter, Wöhler was credited as the discoverer of aluminium.

As Wöhler's method could not yield great quantities of aluminium, the metal remained rare; its cost exceeded that of gold. The first industrial production of aluminium was established in 1856 by French chemist Henri Etienne Sainte-Claire Deville and companions. Deville had discovered that aluminium trichloride could be reduced by sodium, which was more convenient and less expensive than potassium, which Wöhler had used. Even then, aluminium was still not of great purity and produced aluminium differed in properties by sample.

The first industrial large-scale production method was independently developed in 1886 by French engineer Paul Héroult and American engineer Charles Martin Hall; it is now known as the Hall–Héroult process. The Hall–Héroult process converts alumina into the metal. Austrian chemist Carl Joseph Bayer discovered a way of purifying bauxite to yield alumina, now known as the Bayer process, in 1889. Modern production of the aluminium metal is based on the Bayer and Hall–Héroult processes.

Prices of aluminium dropped and aluminium became widely used in jewelry, everyday items, eyeglass frames, optical instruments, tableware, and foil in the 1890s and early 20th century. Aluminium's ability to form hard yet light alloys with other metals provided the metal many uses at the time. During World War I, major governments demanded large shipments of aluminium for light strong airframes.

By the mid-20th century, aluminium had become a part of everyday life and an essential component of housewares. During the mid-20th century, aluminium emerged as a civil engineering material, with building applications in both basic construction and interior finish work, and increasingly being used in military engineering, for both airplanes and land armor vehicle engines. Earth's first artificial satellite, launched in 1957, consisted of two separate aluminium semi-spheres joined together and all subsequent space vehicles have used aluminium to some extent. The aluminium can was invented in 1956 and employed as a storage for drinks in 1958.

Throughout the 20th century, the production of aluminium rose rapidly: while the world production of aluminium in 1900 was 6,800 metric tons, the annual production first exceeded 100,000 metric tons in 1916; 1,000,000 tons in 1941; 10,000,000 tons in 1971. In the 1970s, the increased demand for aluminium made it an exchange commodity; it entered the London Metal Exchange, the oldest industrial metal exchange in the world, in 1978. The output continued to grow: the annual production of aluminium exceeded 50,000,000 metric tons in 2013.

The real price for aluminium declined from $14,000 per metric ton in 1900 to $2,340 in 1948 (in 1998 United States dollars). Extraction and processing costs were lowered over technological progress and the scale of the economies. However, the need to exploit lower-grade poorer quality deposits and the use of fast increasing input costs (above all, energy) increased the net cost of aluminium; the real price began to grow in the 1970s with the rise of energy cost. Production moved from the industrialized countries to countries where production was cheaper. Production costs in the late 20th century changed because of advances in technology, lower energy prices, exchange rates of the United States dollar, and alumina prices. The BRIC countries' combined share in primary production and primary consumption grew substantially in the first decade of the 21st century. China is accumulating an especially large share of world's production thanks to abundance of resources, cheap energy, and governmental stimuli; it also increased its consumption share from 2% in 1972 to 40% in 2010. In the United States, Western Europe, and Japan, most aluminium was consumed in transportation, engineering, construction, and packaging.

Aluminium is named after alumina, a naturally occurring oxide of aluminium, and the name "alumina" comes from alum, the mineral from which it was collected. The word "alum" derives from the Latin word "alumen", meaning "bitter salt". The word "alumen" stems from the Proto-Indo-European root "*alu-" meaning "bitter" or "beer".

British chemist Humphry Davy, who performed a number of experiments aimed to isolate the metal, is credited as the person who named the element. The first named proposed for the metal to be isolated from alum was "alumium", which Davy suggested in an 1808 article on his electrochemical research, published in Philosophical Transactions of the Royal Society. This suggestion was criticized by contemporary chemists from France, Germany, and Sweden, who insisted the metal should be named for the oxide, alumina, from which it would be isolated. A January 1811 summary of one of Davy's lectures at the Royal Society proposed the name "aluminium" —this is the earliest known published writing to use either of the modern spellings. However, the following year, Davy published a chemistry textbook in which he settled on the spelling "aluminum". Both spellings have coexisted since; however, their usage has split by region: "aluminum" is in use in the United States and Canada while "aluminium" is in use elsewhere.

Davy's spelling "aluminum" is consistent with the Latin naming of metals, which end in ', e.g. "aurum" (gold), "argentum" (silver), "ferrum" (iron), naming newly discovered elements by replacing a ' or ' suffix in the oxide's name with ': lanthanum was named for its oxide lanthana, magnesium for magnesia, tantalum for tantalite, molybdenum for molybdenite (also known as "molybdena"), cerium for ceria, and thorium for thoria, respectively. As aluminium's oxide is called "alumina", not "aluminia", the ' spelling does not follow this pattern. However, other newly discovered elements of the time had names with a ' suffix, such as potassium, sodium, calcium, and strontium.

In 1812, British scientist Thomas Young wrote an anonymous review of Davy's book, in which he proposed the name "aluminium" instead of "aluminum", which he felt had a "less classical sound". This name did catch on: while the ' spelling was occasionally used in Britain, the American scientific language used ' from the start. Most scientists used ' throughout the world in the 19th century; it still remains the standard in many other Latin-based languages where the name has the same origin. In 1828, American lexicographer Noah Webster used exclusively the "aluminum" spelling in his "American Dictionary of the English Language". In the 1830s, the ' spelling started to gain usage in the United States; by the 1860s, it had become the more common spelling there outside science. In 1892, Hall used the ' spelling in his advertising handbill for his new electrolytic method of producing the metal, despite his constant use of the ' spelling in all the patents he filed between 1886 and 1903. It was subsequently suggested this was a typo rather than intended. By 1890, both spellings had been common in the U.S. overall, the ' spelling being slightly more common; by 1895, the situation had reversed; by 1900, "aluminum" had become twice as common as "aluminium"; during the following decade, the ' spelling dominated American usage. In 1925, the American Chemical Society adopted this spelling.

The International Union of Pure and Applied Chemistry (IUPAC) adopted "aluminium" as the standard international name for the element in 1990. In 1993, they recognized "aluminum" as an acceptable variant; the most recent 2005 edition of the IUPAC nomenclature of inorganic chemistry acknowledges this spelling as well. IUPAC official publications use the "" spelling as primary but list both where appropriate.

Aluminium production is highly energy-consuming, and so the producers tend to locate smelters in places where electric power is both plentiful and inexpensive. As of 2012, the world's largest smelters of aluminium are located in China, Russia, Bahrain, United Arab Emirates, and South Africa.

In 2016, China was the top producer of aluminium with a world share of fifty-five percent; the next largest producing countries were Russia, Canada, India, and the United Arab Emirates.

According to the International Resource Panel's Metal Stocks in Society report, the global per capita stock of aluminium in use in society (i.e. in cars, buildings, electronics, etc.) is . Much of this is in more-developed countries ( per capita) rather than less-developed countries ( per capita).

Bauxite is converted to aluminium oxide by the Bayer process. Bauxite is blended for uniform composition and then is ground. The resulting slurry is mixed with a hot solution of sodium hydroxide; the mixture is then treated in a digester vessel at a pressure well above atmospheric, dissolving the aluminium hydroxide in bauxite while converting impurities into relatively insoluble compounds:

After this reaction, the slurry is at a temperature above its atmospheric boiling point. It is cooled by removing steam as pressure is reduced. The bauxite residue is separated from the solution and discarded. The solution, free of solids, is seeded with small crystals of aluminium hydroxide; this causes decomposition of the [Al(OH)] ions to aluminium hydroxide. After about half of aluminium has precipitated, the mixture is sent to classifiers. Small crystals of aluminium hydroxide are collected to serve as seeding agents; coarse particles are converted to aluminium oxide by heating; excess solution is removed by evaporation, (if needed) purified, and recycled.

The conversion of alumina to aluminium metal is achieved by the Hall–Héroult process. In this energy-intensive process, a solution of alumina in a molten () mixture of cryolite (NaAlF) with calcium fluoride is electrolyzed to produce metallic aluminium. The liquid aluminium metal sinks to the bottom of the solution and is tapped off, and usually cast into large blocks called aluminium billets for further processing.

Anodes of the electrolysis cell are made of carbon—the most resistant material against fluoride corrosion—and either bake at the process or are prebaked. The former, also called Söderberg anodes, are less power-efficient and fumes released during baking are costly to collect, which is why they are being replaced by prebaked anodes even though they save the power, energy, and labor to prebake the cathodes. Carbon for anodes should be preferably pure so that neither aluminium nor the electrolyte is contaminated with ash. Despite carbon's resistivity against corrosion, it is still consumed at a rate of 0.4–0.5 kg per each kilogram of produced aluminium. Cathodes are made of anthracite; high purity for them is not required because impurities leach only very slowly. Cathode is consumed at a rate of 0.02–0.04 kg per each kilogram of produced aluminium. A cell is usually a terminated after 2–6 years following a failure of the cathode.

The Hall–Heroult process produces aluminium with a purity of above 99%. Further purification can be done by the Hoopes process. This process involves the electrolysis of molten aluminium with a sodium, barium, and aluminium fluoride electrolyte. The resulting aluminium has a purity of 99.99%.

Electric power represents about 20 to 40% of the cost of producing aluminium, depending on the location of the smelter. Aluminium production consumes roughly 5% of electricity generated in the United States. Because of this, alternatives to the Hall–Héroult process have been researched, but none has turned out to be economically feasible.

Recovery of the metal through recycling has become an important task of the aluminium industry. Recycling was a low-profile activity until the late 1960s, when the growing use of aluminium beverage cans brought it to public awareness. Recycling involves melting the scrap, a process that requires only 5% of the energy used to produce aluminium from ore, though a significant part (up to 15% of the input material) is lost as dross (ash-like oxide). An aluminium stack melter produces significantly less dross, with values reported below 1%.

White dross from primary aluminium production and from secondary recycling operations still contains useful quantities of aluminium that can be extracted industrially. The process produces aluminium billets, together with a highly complex waste material. This waste is difficult to manage. It reacts with water, releasing a mixture of gases (including, among others, hydrogen, acetylene, and ammonia), which spontaneously ignites on contact with air; contact with damp air results in the release of copious quantities of ammonia gas. Despite these difficulties, the waste is used as a filler in asphalt and concrete.

The global production of aluminium in 2016 was 58.8 million metric tons. It exceeded that of any other metal except iron (1,231 million metric tons).

Aluminium is almost always alloyed, which markedly improves its mechanical properties, especially when tempered. For example, the common aluminium foils and beverage cans are alloys of 92% to 99% aluminium. The main alloying agents are copper, zinc, magnesium, manganese, and silicon (e.g., duralumin) with the levels of other metals in a few percent by weight.

The major uses for aluminium metal are in:

The great majority (about 90%) of aluminium oxide is converted to metallic aluminium. Being a very hard material (Mohs hardness 9), alumina is widely used as an abrasive; being extraordinarily chemically inert, it is useful in highly reactive environments such as high pressure sodium lamps. Aluminium oxide is commonly used as a catalyst for industrial processes; e.g. the Claus process to convert hydrogen sulfide to sulfur in refineries and to alkylate amines. Many industrial catalysts are supported by alumina, meaning that the expensive catalyst material is dispersed over a surface of the inert alumina. Another principal use is as a drying agent or absorbent.
Several sulfates of aluminium have industrial and commercial application. Aluminium sulfate (in its hydrate form) is produced on the annual scale of several millions of metric tons. About two-thirds is consumed in water treatment. The next major application is in the manufacture of paper. It is also used as a mordant in dyeing, in pickling seeds, deodorizing of mineral oils, in leather tanning, and in production of other aluminium compounds. Two kinds of alum, ammonium alum and potassium alum, were formerly used as mordants and in leather tanning, but their use has significantly declined following availability of high-purity aluminium sulfate. Anhydrous aluminium chloride is used as a catalyst in chemical and petrochemical industries, the dyeing industry, and in synthesis of various inorganic and organic compounds. Aluminium hydroxychlorides are used in purifying water, in the paper industry, and as antiperspirants. Sodium aluminate is used in treating water and as an accelerator of solidification of cement.

Many aluminium compounds have niche applications, for example:

Despite its widespread occurrence in the Earth's crust, aluminium has no known function in biology. At pH 6–9 (relevant for most natural waters), aluminium precipitates out of water as the hydroxide and is hence not available; most elements behaving this way have no biological role or are toxic. Aluminium salts are remarkably nontoxic, aluminium sulfate having an LD of 6207 mg/kg (oral, mouse), which corresponds to 435 grams for an person.

In most people, aluminium is not as toxic as heavy metals. Aluminium is classified as a non-carcinogen by the United States Department of Health and Human Services. There is little evidence that normal exposure to aluminium presents a risk to healthy adult, and there is evidence of no toxicity if it is consumed in amounts not greater than 40 mg/day per kg of body mass. Most aluminium consumed will leave the body in feces; most of the small part of it that enters the bloodstream, will be excreted via urine.

Aluminium, although rarely, can cause vitamin D-resistant osteomalacia, erythropoietin-resistant microcytic anemia, and central nervous system alterations. People with kidney insufficiency are especially at a risk. Chronic ingestion of hydrated aluminium silicates (for excess gastric acidity control) may result in aluminium binding to intestinal contents and increased elimination of other metals, such as iron or zinc; sufficiently high doses (>50 g/day) can cause anemia.

During the 1988 Camelford water pollution incident people in Camelford had their drinking water contaminated with aluminium sulfate for several weeks. A final report into the incident in 2013 concluded it was unlikely that this had caused long-term health problems.

Aluminium has been suspected of being a possible cause of Alzheimer's disease, but research into this for over 40 years has found, , no good evidence of causal effect.

Aluminium increases estrogen-related gene expression in human breast cancer cells cultured in the laboratory. In very high doses, aluminium is associated with altered function of the blood–brain barrier. A small percentage of people have contact allergies to aluminium and experience itchy red rashes, headache, muscle pain, joint pain, poor memory, insomnia, depression, asthma, irritable bowel syndrome, or other symptoms upon contact with products containing aluminium.

Exposure to powdered aluminium or aluminium welding fumes can cause pulmonary fibrosis. Fine aluminium powder can ignite or explode, posing another workplace hazard.

Food is the main source of aluminium. Drinking water contains more aluminium than solid food; however, aluminium in food may be absorbed more than aluminium from water. Major sources of human oral exposure to aluminium include food (due to its use in food additives, food and beverage packaging, and cooking utensils), drinking water (due to its use in municipal water treatment), and aluminium-containing medications (particularly antacid/antiulcer and buffered aspirin formulations). Dietary exposure in Europeans averages to 0.2–1.5 mg/kg/week but can be as high as 2.3 mg/kg/week. Higher exposure levels of aluminium are mostly limited to miners, aluminium production workers, and dialysis patients.

Consumption of antacids, antiperspirants, vaccines, and cosmetics provide possible routes of exposure. Consumption of acidic foods or liquids with aluminium enhances aluminium absorption, and maltol has been shown to increase the accumulation of aluminium in nerve and bone tissues.

In case of suspected sudden intake of a large amount of aluminium, the only treatment is deferoxamine mesylate which may be given to help eliminate aluminium from the body by chelation. However, this should be applied with caution as this reduces not only aluminium body levels, but also those of other metals such as copper or iron.

High levels of aluminium occur near mining sites; small amounts of aluminium are released to the environment at the coal-fired power plants or incinerators. Aluminium in the air is washed out by the rain or normally settles down but small particles of aluminium remain in the air for a long time.

Acidic precipitation is the main natural factor to mobilize aluminium from natural sources and the main reason for the environmental effects of aluminium; however, the main factor of presence of aluminium in salt and freshwater are the industrial processes that also release aluminium into air.

In water, aluminium acts as a toxiс agent on gill-breathing animals such as fish by causing loss of plasma- and hemolymph ions leading to osmoregulatory failure. Organic complexes of aluminium may be easily absorbed and interfere with metabolism in mammals and birds, even though this rarely happens in practice.

Aluminium is primary among the factors that reduce plant growth on acidic soils. Although it is generally harmless to plant growth in pH-neutral soils, in acid soils the concentration of toxic Al cations increases and disturbs root growth and function. Wheat has developed a tolerance to aluminium, releasing organic compounds that bind to harmful aluminium cations. Sorghum is believed to have the same tolerance mechanism.

Aluminium production possesses its own challenges to the environment on each step of the production process. The major challenge is the greenhouse gas emissions. These gases result from electrical consumption of the smelters and the byproducts of processing. The most potent of these gases are perfluorocarbons from the smelting process. Released sulfur dioxide is one of the primary precursors of acid rain.

A Spanish scientific report from 2001 claimed that the fungus "Geotrichum candidum" consumes the aluminium in compact discs. Other reports all refer back to that report and there is no supporting original research. Better documented, the bacterium "Pseudomonas aeruginosa" and the fungus "Cladosporium resinae" are commonly detected in aircraft fuel tanks that use kerosene-based fuels (not avgas), and laboratory cultures can degrade aluminium. However, these life forms do not directly attack or consume the aluminium; rather, the metal is corroded by microbe waste products.






</doc>
<doc id="905" url="https://en.wikipedia.org/wiki?curid=905" title="Advanced Chemistry">
Advanced Chemistry

Advanced Chemistry is a German hip hop group from Heidelberg, a scenic city in Baden-Württemberg, South Germany. Advanced Chemistry was founded in 1987 by Toni L, Linguist, Gee-One, DJ Mike MD (Mike Dippon) and MC Torch. Each member of the group holds German citizenship, and Toni L, Linguist, and Torch are of Italian, Ghanaian, and Haitian backgrounds, respectively.

Influenced by North American socially conscious rap and the Native tongues movement, Advanced Chemistry is regarded as one of the main pioneers in German hip hop. They were one of the first groups to rap in German (although their name is in English). Furthermore, their songs tackled controversial social and political issues, distinguishing them from early German hip hop group "Die Fantastischen Vier" (The Fantastic Four), which had a more light-hearted, playful, party image.

One of the first issues that confronts us when we move outside the English-speaking market for recorded music is to establish whether or not the discrete musical genres we know from that market are fully congruent with similar divisions in other pop worlds. This is important in two ways. First, although no single country comes close to matching the amounts spent on recorded music in the United States, these markets are nonetheless economically significant. Germany, for instance, is the largest single market in western Europe, with estimated annual sales of U.S. $3.74 billion in 1996. This represents around 30 percent of reported U.S. sales and makes Germany the third biggest music market in the world.

Advanced Chemistry frequently rapped about their lives and experiences as children of immigrants, exposing the marginalization experienced by most ethnic minorities in Germany, and the feelings of frustration and resentment that being denied a German identity can cause. The song "Fremd im eigenen Land" (Foreign in your own nation) was released by Advanced Chemistry in November 1992. The single became a staple in the German hip hop scene. It made a strong statement about the status of immigrants throughout Germany, as the group was composed of multi-national and multi-racial members. The video shows several members brandishing their German passports as a demonstration of their German citizenship to skeptical and unaccepting 'ethnic' Germans.

This idea of national identity is important, as many rap artists in Germany have been of foreign origin. These so-called "Gastarbeiter" (guest workers) children saw breakdance, graffiti, rap music, and hip hop culture as a means of expressing themselves. Since the release of "Fremd im eigenen Land", many other German-language rappers have also tried to confront anti-immigrant ideas and develop themes of citizenship. However, though many ethnic minority youth in Germany find these German identity themes appealing, others view the desire of immigrants to be seen as German negatively, and they have actively sought to revive and recreate concepts of identity in connection to traditional ethnic origins.

Advanced Chemistry helped to found the German chapter of the Zulu nation.

The rivalry between Advanced Chemistry and Die Fantastischen Vier has served to highlight a dichotomy in the routes that hip hop has taken in becoming a part of the German soundscape. While Die Fantastischen Vier may be said to view hip hop primarily as an aesthetic art form, Advanced Chemistry understand hip hop as being inextricably linked to the social and political circumstances under which it is created. For Advanced Chemistry, hip hop is a “vehicle of general human emancipation,”. In their undertaking of social and political issues, the band introduced the term "Afro-German" into the context of German hip hop, and the theme of race is highlighted in much of their music.

With the release of the single “Fremd im eigenen Land”, Advanced Chemistry separated itself from the rest of the rap being produced in Germany. This single was the first of its kind to go beyond simply imitating US rap and addressed the current issues of the time. Fremd im eigenen Land which translates to “foreign in my own country” dealt with the widespread racism that non-white German citizens faced. This change from simple imitation to political commentary was the start of German identification with rap. The sound of “Fremd im eigenen Land” was influenced by the 'wall of noise' created by Public Enemy's producers, The Bomb Squad.

After the reunification of Germany, an abundance of anti-immigrant sentiment emerged, as well as attacks on the homes of refugees in the early 1990s. Advanced Chemistry came to prominence in the wake of these actions because of their pro-multicultural society stance in their music. Advanced Chemistry's attitudes revolve around their attempts to create a distinct "Germanness" in hip hop, as opposed to imitating American hip hop as other groups had done. Torch has said, "What the Americans do is exotic for us because we don't live like they do. What they do seems to be more interesting and newer. But not for me. For me it's more exciting to experience my fellow Germans in new contexts...For me, it's interesting to see what the kids try to do that's different from what I know." Advanced Chemistry were the first to use the term "Afro-German" in a hip hop context. This was part of the pro-immigrant political message they sent via their music.

While Advanced Chemistry's use of the German language in their rap allows them to make claims to authenticity and true German heritage, bolstering pro-immigration sentiment, their style can also be problematic for immigrant notions of any real ethnic roots. Indeed, part of the Turkish ethnic minority of Frankfurt views Advanced Chemistry's appeal to the German image as a "symbolic betrayal of the right of ethnic minorities to 'roots' or to any expression of cultural heritage." In this sense, their rap represents a complex social discourse internal to the German soundscape in which they attempt to negotiate immigrant assimilation into a xenophobic German culture with the maintenance of their own separate cultural traditions. It is quite possibly the feelings of alienation from the pure-blooded German demographic that drive Advanced Chemistry to attack nationalistic ideologies by asserting their "Germanness" as a group composed primarily of ethnic others. The response to this pseudo-German authenticity can be seen in what Andy Bennett refers to as "alternative forms of local hip hop culture which actively seek to rediscover and, in many cases, reconstruct notions of identity tied to cultural roots." These alternative local hip hop cultures include oriental hip hop, the members of which cling to their Turkish heritage and are confused by Advanced Chemistry's elicitation of a German identity politics to which they technically do not belong. This cultural binary illustrates that rap has taken different routes in Germany and that, even among an already isolated immigrant population, there is still disunity and, especially, disagreement on the relative importance of assimilation versus cultural defiance. According to German hip hop enthusiast 9@home, Advanced Chemistry is part of a "hip-hop movement [which] took a clear stance for the minorities and against the [marginalization] of immigrants who...might be German on paper, but not in real life," which speaks to the group's hope of actually being recognized as German citizens and not foreigners, despite their various other ethnic and cultural ties.

Advanced Chemistry's work was rooted in German history and the country's specific political realities. However, they also drew inspiration from African-American hip-hop acts like A Tribe Called Quest and Public Enemy, who had helped bring a soulful sound and political consciousness to American hip-hop. One member, Torch, later explicitly listed his references on his solo song "Als (When I Was in School):" "My favorite subject, which was quickly discovered poetry in load Poets, awakens the intellect or policy at Chuck D I'll never forget the lyrics by Public Enemy." Torch goes on to list other American rappers like Biz Markie, Big Daddy Kane and Dr. Dre as influences.



El-Tayeb, Fatima “‘If You Cannot Pronounce My Name, You Can Just Call Me 
Pride.’ Afro-German Activism, Gender, and Hip Hop,” "Gender & History"15/3(2003):459-485.

Felbert, Oliver von. “Die Unbestechlichen.” "Spex" (March 1993): 50–53.

Weheliye, Alexander G. "Phonographies:Grooves in Sonic Afro-Modernity", Duke University Press, 2005.


</doc>
<doc id="909" url="https://en.wikipedia.org/wiki?curid=909" title="Anglican Communion">
Anglican Communion

The Anglican Communion is the third largest Christian communion. Founded in 1867 in London, England, the communion currently has over 85 million members within the Church of England and other national and regional churches in full communion. The traditional origins of Anglican doctrines are summarised in the Thirty-nine Articles (1571). The Archbishop of Canterbury (currently Justin Welby) in England acts as a focus of unity, recognised as "primus inter pares" ("first among equals"), but does not exercise authority in Anglican provinces outside of the Church of England. Most, but not all member churches of the communion, are the historic national or regional Anglican churches. 

The Anglican Communion was founded at the Lambeth Conference in 1867 in London, England, under the leadership of Charles Longley, Archbishop of Canterbury. The churches of the Anglican Communion consider themselves to be part of the one, holy, catholic and apostolic church, and to be both catholic and reformed. Although aligned with the Church of England, the communion has a multitude of beliefs, liturgies, and practices, including evangelical, liberal and Anglo-Catholic. Each retains their own legislative process and episcopal polity under the leadership of local primates. For some adherents, Anglicanism represents a non-papal Catholicism, for others a form of Protestantism though without guiding figure such as Luther, Knox, Calvin, Zwingli or Wesley, or for yet others a combination of the two.

Most of its 85 million members live in the Anglosphere of former British territories. Full participation in the sacramental life of each church is available to all communicant members. Due to their historical link to England ("Ecclesia Anglicana" means "English Church"), some of the member churches are known as "Anglican", such as the Anglican Church of Canada. Others, for example the Church of Ireland, the Scottish and American Episcopal churches have official names which do not include "Anglican". Additionally, there are some churches called "Anglican" which are not of the communion.

The Anglican Communion has no official legal existence nor any governing structure which might exercise authority over the member churches. There is an Anglican Communion Office in London, under the aegis of the Archbishop of Canterbury, but it only serves in a supporting and organisational role. The communion is held together by a shared history, expressed in its ecclesiology, polity and ethos and also by participation in international consultative bodies.

Three elements have been important in holding the communion together: first, the shared ecclesial structure of the component churches, manifested in an episcopal polity maintained through the apostolic succession of bishops and synodical government; second, the principle of belief expressed in worship, investing importance in approved prayer books and their rubrics; and third, the historical documents and the writings of early Anglican divines that have influenced the ethos of the communion.

Originally, the Church of England was self-contained and relied for its unity and identity on its own history, its traditional legal and episcopal structure and its status as an established church of the state. As such Anglicanism was, from the outset, a movement with an explicitly episcopal polity, a characteristic which has been vital in maintaining the unity of the communion by conveying the episcopate's role in manifesting visible catholicity and ecumenism.

Early in its development, Anglicanism developed a vernacular prayer book, called the Book of Common Prayer. Unlike other traditions, Anglicanism has never been governed by a magisterium nor by appeal to one founding theologian, nor by an extra-credal summary of doctrine (such as the Westminster Confession of the Presbyterian churches). Instead, Anglicans have typically appealed to the Book of Common Prayer (1662) and its offshoots as a guide to Anglican theology and practise. This had the effect of inculcating the principle of "lex orandi, lex credendi" (Latin loosely translated as "the law of praying [is] the law of believing") as the foundation of Anglican identity and confession.

Protracted conflict through the 17th century with radical Protestants on the one hand and Catholics who recognised the primacy of the Pope on the other, resulted in an association of churches that were both deliberately vague about doctrinal principles, yet bold in developing parameters of acceptable deviation. These parameters were most clearly articulated in the various rubrics of the successive prayer books, as well as the Thirty-Nine Articles of Religion (1563). These articles have historically shaped and continue to direct the ethos of the communion, an ethos reinforced by their interpretation and expansion by such influential early theologians such as Richard Hooker, Lancelot Andrewes and John Cosin.

With the expansion of the British Empire, and hence the growth of Anglicanism outside Great Britain and Ireland, the communion sought to establish new vehicles of unity. The first major expression of this were the Lambeth Conferences of the communion's bishops, first convened in 1867 by Charles Longley, the Archbishop of Canterbury. From the beginning, these were not intended to displace the autonomy of the emerging provinces of the communion, but to "discuss matters of practical interest, and pronounce what we deem expedient in resolutions which may serve as safe guides to future action".

One of the enduringly influential early resolutions of the conference was the so-called Chicago-Lambeth Quadrilateral of 1888. Its intent was to provide the basis for discussions of reunion with the Roman Catholic and Orthodox churches, but it had the ancillary effect of establishing parameters of Anglican identity. It establishes four principles with these words:

As mentioned above, the Anglican Communion has no international juridical organisation. The Archbishop of Canterbury's role is strictly symbolic and unifying and the communion's three international bodies are consultative and collaborative, their resolutions having no legal effect on the autonomous provinces of the communion. Taken together, however, the four do function as "instruments of communion", since all churches of the communion participate in them. In order of antiquity, they are:


Since there is no binding authority in the Anglican Communion, these international bodies are a vehicle for consultation and persuasion. In recent times, persuasion has tipped over into debates over conformity in certain areas of doctrine, discipline, worship and ethics. The most notable example has been the objection of many provinces of the communion (particularly in Africa and Asia) to the changing acceptance of LGBTQ+ individuals in the North American churches (e.g., by blessing same-sex unions and ordaining and consecrating same-sex relationships) and to the process by which changes were undertaken. (See Anglican realignment)

Those who objected condemned these actions as unscriptural, unilateral, and without the agreement of the communion prior to these steps being taken. In response, the American Episcopal Church and the Anglican Church of Canada answered that the actions had been undertaken after lengthy scriptural and theological reflection, legally in accordance with their own canons and constitutions and after extensive consultation with the provinces of the communion.

The Primates' Meeting voted to request the two churches to withdraw their delegates from the 2005 meeting of the Anglican Consultative Council. Canada and the United States decided to attend the meeting but without exercising their right to vote. They have not been expelled or suspended, since there is no mechanism in this voluntary association to suspend or expel an independent province of the communion. Since membership is based on a province's communion with Canterbury, expulsion would require the Archbishop of Canterbury's refusal to be in communion with the affected jurisdictions. In line with the suggestion of the Windsor Report, Rowan Williams (the then Archbishop of Canterbury) established a working group to examine the feasibility of an Anglican covenant which would articulate the conditions for communion in some fashion.

The Anglican communion consists of forty-one autonomous provinces each with its own primate and governing structure. These provinces may take the form of national churches (such as in Canada, Uganda, or Japan) or a collection of nations (such as the West Indies, Central Africa, or Southeast Asia).

In addition to the forty-one provinces, there are five extraprovincial churches under the metropolitical authority of the Archbishop of Canterbury.

In addition to other member churches, the churches of the Anglican Communion are in full communion with the Old Catholic churches of the Union of Utrecht and the Scandinavian Lutheran churches of the Porvoo Communion in Europe, the India-based Malankara Mar Thoma Syrian and Malabar Independent Syrian churches and the Philippine Independent Church, also known as the Aglipayan Church.

The Anglican Communion traces much of its growth to the older mission organisations of the Church of England such as the Society for Promoting Christian Knowledge (founded 1698), the Society for the Propagation of the Gospel in Foreign Parts (founded 1701) and the Church Missionary Society (founded 1799). The Church of England (which until the 20th century included the Church in Wales) initially separated from the Roman Catholic Church in 1534 in the reign of Henry VIII, reunited in 1555 under Mary I and then separated again in 1570 under Elizabeth I (the Roman Catholic Church excommunicated Elizabeth I in 1570 in response to the Act of Supremacy 1559).

The Church of England has always thought of itself not as a new foundation but rather as a reformed continuation of the ancient "English Church" ("Ecclesia Anglicana") and a reassertion of that church's rights. As such it was a distinctly national phenomenon. The Church of Scotland was formed as a separate church from the Roman Catholic Church as a result of the Scottish Reformation in 1560 and the later formation of the Scottish Episcopal Church began in 1582 in the reign of James VI over disagreements about the role of bishops.

The oldest-surviving Anglican church building outside the British Isles (Britain and Ireland) is St Peter's Church in St. George's, Bermuda, established in 1612 (though the actual building had to be rebuilt several times over the following century). This is also the oldest surviving non-Roman Catholic church in the New World. It remained part of the Church of England until 1978 when the Anglican Church of Bermuda separated. The Church of England was the established church not only in England, but in its trans-Oceanic colonies.

Thus the only member churches of the present Anglican Communion existing by the mid-18th century were the Church of England, its closely linked sister church the Church of Ireland (which also separated from Roman Catholicism under Henry VIII) and the Scottish Episcopal Church which for parts of the 17th and 18th centuries was partially underground (it was suspected of Jacobite sympathies).

The enormous expansion in the 18th and 19th centuries of the British Empire brought Anglicanism along with it. At first all these colonial churches were under the jurisdiction of the bishop of London. After the American Revolution, the parishes in the newly independent country found it necessary to break formally from a church whose supreme governor was (and remains) the British monarch. Thus they formed their own dioceses and national church, the Episcopal Church in the United States of America, in a mostly amicable separation.

At about the same time, in the colonies which remained linked to the crown, the Church of England began to appoint colonial bishops. In 1787 a bishop of Nova Scotia was appointed with a jurisdiction over all of British North America; in time several more colleagues were appointed to other cities in present-day Canada. In 1814 a bishop of Calcutta was made; in 1824 the first bishop was sent to the West Indies and in 1836 to Australia. By 1840 there were still only ten colonial bishops for the Church of England; but even this small beginning greatly facilitated the growth of Anglicanism around the world. In 1841 a "Colonial Bishoprics Council" was set up and soon many more dioceses were created.

In time, it became natural to group these into provinces and a metropolitan bishop was appointed for each province. Although it had at first been somewhat established in many colonies, in 1861 it was ruled that, except where specifically established, the Church of England had just the same legal position as any other church. Thus a colonial bishop and colonial diocese was by nature quite a different thing from their counterparts back home. In time bishops came to be appointed locally rather than from England and eventually national synods began to pass ecclesiastical legislation independent of England.

A crucial step in the development of the modern communion was the idea of the Lambeth Conferences (discussed above). These conferences demonstrated that the bishops of disparate churches could manifest the unity of the church in their episcopal collegiality despite the absence of universal legal ties. Some bishops were initially reluctant to attend, fearing that the meeting would declare itself a council with power to legislate for the church; but it agreed to pass only advisory resolutions. These Lambeth Conferences have been held roughly every 10 years since 1878 (the second such conference) and remain the most visible coming-together of the whole Communion.

The Lambeth Conference of 1998 included what has been seen by Philip Jenkins and others as a "watershed in global Christianity". The 1998 Lambeth Conference considered the issue of the theology of same-sex attraction in relation to human sexuality. At this 1998 conference for the first time in centuries the Christians of developing regions, especially, Africa, Asia, and Latin America, prevailed over the bishops of more prosperous countries (many from the US, Canada, and the UK) who supported a redefinition of Anglican doctrine. Seen in this light 1998 is a date that marked the shift from a West-dominated Christianity to one wherein the growing churches of the two-thirds world are predominant, but the gay bishop controversy in subsequent years led to the reassertion of Western dominance, this time of the liberal variety.

The churches of the Anglican Communion have traditionally held that ordination in the historic episcopate is a core element in the validity of clerical ordinations. The Roman Catholic Church, however, does not recognise Anglican orders (see "Apostolicae curae"). Some Eastern Orthodox churches have issued statements to the effect that Anglican orders could be accepted, yet have still reordained former Anglican clergy; other Eastern Orthodox churches have rejected Anglican orders altogether. Orthodox bishop Kallistos Ware explains this apparent discrepancy as follows:

One effect of the Communion's dispersed authority has been the conflicts arising over divergent practices and doctrines in parts of the Communion. Disputes that had been confined to the Church of England could be dealt with legislatively in that realm, but as the Communion spread out into new nations and disparate cultures, such controversies multiplied and intensified. These controversies have generally been of two types: liturgical and social.

The first such controversy of note concerned that of the growing influence of the Catholic Revival manifested in the tractarian and so-called ritualism controversies of the late nineteenth and early twentieth centuries. This controversy produced the Free Church of England and, in the United States and Canada, the Reformed Episcopal Church.

Later, rapid social change and the dissipation of British cultural hegemony over its former colonies contributed to disputes over the role of women, the parameters of marriage and divorce, and the practices of contraception and abortion. In the late 1970s, the Continuing Anglican movement produced a number of new church bodies in opposition to women's ordination, prayer book changes, and the new understandings concerning marriage.

More recently, disagreements over homosexuality have strained the unity of the communion as well as its relationships with other Christian denominations, leading to another round of withdrawals from the Anglican Communion. Some churches were founded outside the Anglican Communion in the late 20th and early 21st centuries, largely in opposition to the ordination of openly homosexual bishops and other clergy and are usually referred to as belonging to the Anglican realignment movement, or else as "orthodox" Anglicans. These disagreements were especially noted when the Episcopal Church (US) consecrated an openly gay bishop in a same-sex relationship, Gene Robinson, in 2003, which led some Episcopalians to defect and found the Anglican Church in North America (ACNA); then, the debate re-ignited when the Church of England agreed to allow clergy to enter into same-sex civil partnerships in 2005. The Church of Nigeria opposed the Episcopal Church's decision as well as the Church of England's approval for civil partnerships.

"The more liberal provinces that are open to changing Church doctrine on marriage in order to allow for same-sex unions include Brazil, Canada, New Zealand, Scotland, South India, South Africa, the US and Wales". The Church of England does not allow same-gender marriages or blessing rites, but does permit special prayer services for same-sex couples following a civil marriage or partnership. The Church of England also permits clergy to enter into same-sex civil partnerships. The Church of Ireland has no official position on civil unions, and one senior cleric has entered into a same-sex civil partnership. The Church of Ireland recognised that it will "treat civil partners the same as spouses". The Anglican Church of Australia does not have an official position on homosexuality.

The conservative Anglican churches, encouraging the realignment movement, are more concentrated in the Global South. For example, the Anglican Church of Kenya, the Church of Nigeria and the Church of Uganda have opposed homosexuality. GAFCON, a fellowship of conservative Anglican churches, has appointed "missionary bishops" in response to the disagreements with the perceived liberalisation in the Anglican churches in North America and Europe.

Debates about social theology and ethics have occurred at the same time as debates on prayer book revision and the acceptable grounds for achieving full communion with non-Anglican churches.



</doc>
<doc id="910" url="https://en.wikipedia.org/wiki?curid=910" title="Arne Kaijser">
Arne Kaijser

Arne Kaijser (born 1950) is a professor of History of Technology at the Royal Institute of Technology in Stockholm, and former head of the university's department of History of science and technology.

Kaijser has published two books in Swedish: "Stadens ljus. Etableringen av de första svenska gasverken" and "I fädrens spår. Den svenska infrastrukturens historiska utveckling och framtida utmaningar", and has co-edited several anthologies. Kaijser is a member of the Royal Swedish Academy of Engineering Sciences since 2007 and also a member of the editorial board of two scientific journals: "Journal of Urban Technology" and "Centaurus". Lately, he has been occupied with the history of Large Technical Systems.



</doc>
<doc id="911" url="https://en.wikipedia.org/wiki?curid=911" title="Archipelago">
Archipelago

An archipelago ( ), sometimes called an island group or island chain, is a chain, cluster or collection of islands, or sometimes a sea containing a small number of scattered islands.
Indonesia, Andaman and Nicobar Islands, the Galápagos Islands, Japan, the Philippines, Maldives, the Balearic Isles, the Bahamas, the Aegean Islands, Hawaii, the Canary Islands, Malta, the Azores, Canadian Arctic Archipelago, Archipelago Sea (Finland) and the Shetland Islands are all examples of well-known archipelagos. They are sometimes defined by political boundaries. The Gulf archipelago off the pacific coast forms part of a larger archipelago that geographically includes Washington state's San Juan islands. While the Gulf archipelago and San Juan islands are geographically related, they are not technically included in the same archipelago due to manmade geopolitical borders. 

The word "archipelago" is derived from the Ancient Greek ἄρχι-("arkhi-", "chief") and πέλαγος ("pélagos", "sea") through the Italian "arcipelago". In antiquity, "Archipelago" (from medieval Greek *ἀρχιπέλαγος and Latin "archipelagus") was the proper name for the Aegean Sea. Later, usage shifted to refer to the Aegean Islands (since the sea is remarkable for its large number of islands).

Archipelagos may be found isolated in large amounts of water or neighbouring a large land mass. For example, Scotland has more than 700 islands surrounding its mainland which form an archipelago.

Archipelagos are often volcanic, forming along island arcs generated by subduction zones or hotspots, but may also be the result of erosion, deposition, and land elevation. Depending on their geological origin, islands forming archipelagos can be referred to as "oceanic islands", "continental fragments", and "continental islands".

Oceanic islands are mainly of volcanic origin, and widely separated from any adjacent continent. The Hawaiian Islands and Easter Island in the Pacific, and Île Amsterdam in the south Indian Ocean are examples.

Continental fragments correspond to land masses that have separated from a continental mass due to tectonic displacement. The Farallon Islands off the coast of California are an example.

Sets of islands formed close to the coast of a continent are considered continental archipelagos when they form part of the same continental shelf, when those islands are above-water extensions of the shelf. The islands of the Inside Passage off the coast of British Columbia and the Canadian Arctic Archipelago are examples.
Artificial archipelagos have been created in various countries for different purposes. Palm Islands and the World Islands off Dubai were or are being created for leisure and tourism purposes. Marker Wadden in the Netherlands is being built as a conservation area for birds and other wildlife.

The largest archipelagic state in the world by area, and by population, is Indonesia.




</doc>
<doc id="914" url="https://en.wikipedia.org/wiki?curid=914" title="Author">
Author

An author is the creator or originator of any written work such as a book or play, and is also considered a writer. More broadly defined, an author is "the person who originated or gave existence to anything" and whose authorship determines responsibility for what was created.

Typically, the first owner of a copyright is the person who created the work, i.e. the author. If more than one person created the work, then a case of joint authorship can be made provided some criteria are met. In the copyright laws of various jurisdictions, there is a necessity for little flexibility regarding what constitutes authorship. The United States Copyright Office, for example, defines copyright as "a form of protection provided by the laws of the United States (title 17, U.S. Code) to authors of 'original works of authorship'".

Holding the title of "author" over any "literary, dramatic, musical, artistic, [or] certain other intellectual works" gives rights to this person, the owner of the copyright, especially the exclusive right to engage in or authorize any production or distribution of their work. Any person or entity wishing to use intellectual property held under copyright must receive permission from the copyright holder to use this work, and often will be asked to pay for the use of copyrighted material. After a fixed amount of time, the copyright expires on intellectual work and it enters the public domain, where it can be used without limit. Copyright laws in many jurisdictions – mostly following the lead of the United States, in which the entertainment and publishing industries have very strong lobbying power – have been amended repeatedly since their inception, to extend the length of this fixed period where the work is exclusively controlled by the copyright holder. However, copyright is merely the legal reassurance that one owns his/her work. Technically, someone owns their work from the time it's created. A notable aspect of authorship emerges with copyright in that, in many jurisdictions, it can be passed down to another upon one's death. The person who inherits the copyright is not the author, but enjoys the same legal benefits.

Questions arise as to the application of copyright law. How does it, for example, apply to the complex issue of fan fiction? If the media agency responsible for the authorized production allows material from fans, what is the limit before legal constraints from actors, music, and other considerations, come into play? Additionally, how does copyright apply to fan-generated stories for books? What powers do the original authors, as well as the publishers, have in regulating or even stopping the fan fiction? This particular sort of case also illustrates how complex intellectual property law can be, since such fiction may also involved trademark law (e.g. for names of characters in media franchises), likeness rights (such as for actors, or even entirely fictional entities), fair use rights held by the public (including the right to parody or satirize), and many other interacting complications.

Authors may portion out different rights they hold to different parties, at different times, and for different purposes or uses, such as the right to adapt a plot into a film, but only with different character names, because the characters have already been optioned by another company for a television series or a video game. An author may also not have rights when working under contract that they would otherwise have, such as when creating a work for hire (e.g., hired to write a city tour guide by a municipal government that totally owns the copyright to the finished work), or when writing material using intellectual property owned by others (such as when writing a novel or screenplay that is a new installment in an already established media franchise).

In literary theory, critics find complications in the term "author" beyond what constitutes authorship in a legal setting. In the wake of postmodern literature, critics such as Roland Barthes and Michel Foucault have examined the role and relevance of authorship to the meaning or interpretation of a text.

Barthes challenges the idea that a text can be attributed to any single author. He writes, in his essay "Death of the Author" (1968), that "it is language which speaks, not the author". The words and language of a text itself determine and expose meaning for Barthes, and not someone possessing legal responsibility for the process of its production. Every line of written text is a mere reflection of references from any of a multitude of traditions, or, as Barthes puts it, "the text is a tissue of quotations drawn from the innumerable centres of culture"; it is never original. With this, the perspective of the author is removed from the text, and the limits formerly imposed by the idea of one authorial voice, one ultimate and universal meaning, are destroyed. The explanation and meaning of a work does not have to be sought in the one who produced it, "as if it were always in the end, through the more or less transparent allegory of the fiction, the voice of a single person, the author 'confiding' in us". The psyche, culture, fanaticism of an author can be disregarded when interpreting a text, because the words are rich enough themselves with all of the traditions of language. To expose meanings in a written work without appealing to the celebrity of an author, their tastes, passions, vices, is, to Barthes, to allow language to speak, rather than author.

Michel Foucault argues in his essay "What is an author?" (1969) that all authors are writers, but not all writers are authors. He states that "a private letter may have a signatory—it does not have an author". For a reader to assign the title of author upon any written work is to attribute certain standards upon the text which, for Foucault, are working in conjunction with the idea of "the author function". Foucault's author function is the idea that an author exists only as a function of a written work, a part of its structure, but not necessarily part of the interpretive process. The author's name "indicates the status of the discourse within a society and culture", and at one time was used as an anchor for interpreting a text, a practice which Barthes would argue is not a particularly relevant or valid endeavor.

Expanding upon Foucault's position, Alexander Nehamas writes that Foucault suggests "an author [...] is whoever can be understood to have produced a particular text as we interpret it", not necessarily who penned the text. It is this distinction between producing a written work and producing the interpretation or meaning in a written work that both Barthes and Foucault are interested in. Foucault warns of the risks of keeping the author's name in mind during interpretation, because it could affect the value and meaning with which one handles an interpretation.

Literary critics Barthes and Foucault suggest that readers should not rely on or look for the notion of one overarching voice when interpreting a written work, because of the complications inherent with a writer's title of "author". They warn of the dangers interpretations could suffer from when associating the subject of inherently meaningful words and language with the personality of one authorial voice. Instead, readers should allow a text to be interpreted in terms of the language as "author".

Self-publishing, self-publishing, independent publishing, or artisanal publishing is the "publication of any book, album or other media by its author without the involvement of a traditional publisher. It is the modern equivalent to traditional publishing".

Unless a book is to be sold directly from the author to the public, an ISBN is required to uniquely identify the title. ISBN is a global standard used for all titles worldwide. Most self-publishing companies either provide their own ISBN to a title or can provide direction; it may be in the best interest of the self-published author to retain ownership of ISBN and copyright instead of using a number owned by a vanity press. A separate ISBN is needed for each edition of the book.

There are a variety of e-book formats and tools that can be used to create them. Because it is possible to create e-books with no up-front or per-book costs, this is a popular option for self-publishers. E-book publishing platforms include Pronoun, Smashwords, Blurb, Amazon Kindle Direct Publishing, CinnamonTeal Publishing, Papyrus Editor, ebook leap, Bookbaby, Pubit, Lulu, Llumina Press, and CreateSpace. E-book formats include e-pub, mobi, and PDF, among others.

Print-on-demand (POD) publishing refers to the ability to print high-quality books as needed. For self-published books, this is often a more economical option than conducting a print run of hundreds or thousands of books. Many companies, such as Createspace (owned by Amazon.com), Outskirts Press, Blurb, Lulu, Llumina Press, ReadersMagnet, and iUniverse, allow printing single books at per-book costs not much higher than those paid by publishing companies for large print runs.

With commissioned publishing, the publisher makes all the publication arrangements and the author covers all expenses.

The more specific phrase published author refers to an author (especially but not necessarily of books) whose work has been independently accepted for publication by a reputable publisher , versus a self-publishing author or an unpublished one.

The author of a work may receive a percentage calculated on a wholesale or a specific price or a fixed amount on each book sold. Publishers, at times, reduced the risk of this type of arrangement, by agreeing only to pay this after a certain number of copies had sold. In Canada, this practice occurred during the 1890s, but was not commonplace until the 1920s. Established and successful authors may receive advance payments, set against future royalties, but this is no longer common practice. Most independent publishers pay royalties as a percentage of net receipts – how net receipts are calculated varies from publisher to publisher. Under this arrangement, the author does not pay anything towards the expense of publication. The costs and financial risk are all carried by the publisher, who will then take the greatest percentage of the receipts. See Compensation for more.

This type of publisher normally charges a flat fee for arranging publication, offers a platform for selling, and then takes a percentage of the sale of every copy of a book. The author receives the rest of the money made.

The relationship between the author and the editor, often the author's only liaison to the publishing company, is often characterized as the site of tension. For the author to reach their audience, often through publication, the work usually must attract the attention of the editor. The idea of the author as the sole meaning-maker of necessity changes to include the influences of the editor and the publisher in order to engage the audience in writing as a social act. There are three principal areas covered by editors – Proofing (checking the Grammar and spelling, looking for typing errors), Story (potentially an area of deep angst for both author and publisher), and Layout (the setting of the final proof ready for publishing often requires minor text changes so a layout editor is required to ensure that these do not alter the sense of the text).

Pierre Bourdieu's essay "The Field of Cultural Production" depicts the publishing industry as a "space of literary or artistic position-takings", also called the "field of struggles", which is defined by the tension and movement inherent among the various positions in the field. Bourdieu claims that the "field of position-takings [...] is not the product of coherence-seeking intention or objective consensus", meaning that an industry characterized by position-takings is not one of harmony and neutrality. In particular for the writer, their authorship in their work makes their work part of their identity, and there is much at stake personally over the negotiation of authority over that identity. However, it is the editor who has "the power to impose the dominant definition of the writer and therefore to delimit the population of those entitled to take part in the struggle to define the writer". As "cultural investors," publishers rely on the editor position to identify a good investment in "cultural capital" which may grow to yield economic capital across all positions.

According to the studies of James Curran, the system of shared values among editors in Britain has generated a pressure among authors to write to fit the editors' expectations, removing the focus from the reader-audience and putting a strain on the relationship between authors and editors and on writing as a social act. Even the book review by the editors has more significance than the readership's reception.

A standard contract for an author will usually include provision for payment in the form of an advance and royalties. An advance is a lump sum paid in advance of publication. An advance must be earned out before royalties are payable. An advance may be paid in two lump sums: the first payment on contract signing, and the second on delivery of the completed manuscript or on publication.

An author's contract may specify, for example, that they will earn 10% of the retail price of each book sold. Some contracts specify a scale of royalties payable (for example, where royalties start at 10% for the first 10,000 sales, but then increase to a higher percentage rate at higher sale thresholds).

An author's book must earn the advance before any further royalties are paid. For example, if an author is paid a modest advance of $2000, and their royalty rate is 10% of a book priced at $20 – that is, $2 per book – the book will need to sell 1000 copies before any further payment will be made. Publishers typically withhold payment of a percentage of royalties earned against returns.

In some countries, authors also earn income from a government scheme such as the ELR (educational lending right) and PLR (public lending right) schemes in Australia. Under these schemes, authors are paid a fee for the number of copies of their books in educational and/or public libraries.

These days, many authors supplement their income from book sales with public speaking engagements, school visits, residencies, grants, and teaching positions.

Ghostwriters, technical writers, and textbooks writers are typically paid in a different way: usually a set fee or a per word rate rather than on a percentage of sales.



</doc>
<doc id="915" url="https://en.wikipedia.org/wiki?curid=915" title="Andrey Markov">
Andrey Markov

Andrey Andreyevich Markov (1856–1922) was a Russian mathematician best known for his work on stochastic processes. A primary subject of his research later became known as Markov chains and Markov processes.

Markov and his younger brother Vladimir Andreevich Markov (1871–1897) proved the Markov brothers' inequality.
His son, another Andrei Andreyevich Markov (1903–1979), was also a notable mathematician, making contributions to constructive mathematics and recursive function theory.

Andrey Markov was born on 14 June 1856 in Russia. He attended Petersburg Grammar, where he was seen as a rebellious student by a select few teachers. In his academics he performed poorly in most subjects other than mathematics. Later in life he attended Petersburg University; among his teachers were Yulian Sokhotski (differential calculus, higher algebra), Konstantin Posse (analytic geometry), Yegor Zolotarev (integral calculus), Pafnuty Chebyshev (number theory and probability theory), Aleksandr Korkin (ordinary and partial differential equations), Mikhail Okatov (mechanism theory), Osip Somov (mechanics), and Nikolai Budajev (descriptive and higher geometry). He completed his studies at the University and was later asked if he would like to stay and have a career as a Mathematician. He later taught at high schools and continued his own mathematical studies. In this time he found a practical use for his mathematical skills. He figured out that he could use chains to model the alliteration of vowels and consonants in Russian literature. He also contributed to many other mathematical aspects in his time. He died at age 66 on 20 July 1922.

In 1877, Markov was awarded a gold medal for his outstanding solution of the problem

"About Integration of Differential Equations by Continued Fractions with an Application to the Equation" formula_1.

During the following year, he passed the candidate's examinations, and he remained at the university to prepare for a lecturer's position.

In April 1880, Markov defended his master's thesis "On the Binary Square Forms with Positive Determinant", which was encouraged by Aleksandr Korkin and Yegor Zolotarev. Four years later in 1884, he defended his doctoral thesis titled "On Certain Applications of the Algebraic Continuous Fractions".

His pedagogical work began after the defense of his master's thesis in autumn 1880. As a privatdozent he lectured on differential and integral calculus. Later he lectured alternately on "introduction to analysis", probability theory (succeeding Chebyshev, who had left the university in 1882) and the calculus of differences. From 1895 through 1905 he also lectured in differential calculus.
One year after the defense of his doctoral thesis, Markov was appointed extraordinary professor (1886) and in the same year he was elected adjunct to the Academy of Sciences. In 1890, after the death of Viktor Bunyakovsky, Markov became an extraordinary member of the academy. His promotion to an ordinary professor of St. Petersburg University followed in the fall of 1894.

In 1896, Markov was elected an ordinary member of the academy as the successor of Chebyshev. In 1905, he was appointed merited professor and was granted the right to retire, which he did immediately. Until 1910, however, he continued to lecture in the calculus of differences.

In connection with student riots in 1908, professors and lecturers of St. Petersburg University were ordered to monitor their students. Markov refused to accept this decree, and he wrote an explanation in which he declined to be an "agent of the governance". Markov was removed from further teaching duties at St. Petersburg University, and hence he decided to retire from the university.

Markov was an atheist. In 1912 he protested Leo Tolstoy's excommunication from the Russian Orthodox Church by requesting his own excommunication. The Church complied with his request. 
In 1913, the council of St. Petersburg elected nine scientists honorary members of the university. Markov was among them, but his election was not affirmed by the minister of education. The affirmation only occurred four years later, after the February Revolution in 1917. Markov then resumed his teaching activities and lectured on probability theory and the calculus of differences until his death in 1922.




</doc>
<doc id="921" url="https://en.wikipedia.org/wiki?curid=921" title="Angst">
Angst

Angst means fear or anxiety ("anguish" is its Latinate equivalent, and "anxious," "anxiety" are of similar origin). The dictionary definition for angst is a feeling of anxiety, apprehension, or insecurity. 

The word "angst" was introduced into English from the Danish, Norwegian, and Dutch word and the German word . It is attested since the 19th century in English translations of the works of Kierkegaard and Freud. It is used in English to describe an intense feeling of apprehension, anxiety, or inner turmoil.

In other languages, having the meaning of the Latin word for "fear", the derived words differ in meaning; for example, as in the French and . The word "angst" has existed since the 8th century, from the Proto-Indo-European root "", "restraint" from which Old High German developed. It is pre-cognate with the Latin , "tensity, tightness" and , "choking, clogging"; compare to the Ancient Greek () "strangle".

In Existentialist philosophy, the term "angst" carries a specific conceptual meaning. The use of the term was first attributed to Danish philosopher Søren Kierkegaard (1813–1855). In "The Concept of Anxiety" (also known as "The Concept of Dread", depending on the translation), Kierkegaard used the word "Angest" (in common Danish, "angst", meaning "dread" or "anxiety") to describe a profound and deep-seated condition. Where non-human animals are guided solely by instinct, said Kierkegaard, human beings enjoy a freedom of choice that we find both appealing and terrifying. It is the anxiety of understanding of being free when considering undefined possibilities of one's life and one's power of choice over them. Kierkegaard's concept of angst reappeared in the works of existentialist philosophers who followed, such as Friedrich Nietzsche, Jean-Paul Sartre, and Martin Heidegger, each of whom developed the idea further in individual ways. While Kierkegaard's angst referred mainly to ambiguous feelings about moral freedom within a religious personal belief system, later existentialists discussed conflicts of personal principles, cultural norms, and existential despair.
Existential angst makes its appearance in classical musical composition in the early twentieth century as a result of both philosophical developments and as a reflection of the war-torn times. Notable composers whose works are often linked with the concept include Gustav Mahler, Richard Strauss (operas "Elektra" and "Salome"), Claude-Achille Debussy (opera "Pelleas et Melisande", ballet "Jeux", other works), Jean Sibelius (especially the Fourth Symphony), Arnold Schoenberg "(A Survivor from Warsaw", other works), Alban Berg, Francis Poulenc (opera "Dialogues of the Carmelites"), Dmitri Shostakovich (opera "Lady Macbeth of the Mtsensk District", symphonies and chamber music), Béla Bartók (opera "Bluebeard's Castle", other works), and Krzysztof Penderecki (especially "Threnody to the Victims of Hiroshima").
Angst began to be discussed in reference to popular music in the mid- to late 1950s amid widespread concerns over international tensions and nuclear proliferation. Jeff Nuttall's book "Bomb Culture" (1968) traced angst in popular culture to Hiroshima. Dread was expressed in works of folk rock such as Bob Dylan's "Masters of War" (1963) and "A Hard Rain's a-Gonna Fall". The term often makes an appearance in reference to punk rock, grunge, nu metal, and works of emo where expressions of melancholy, existential despair, or nihilism predominate.


</doc>
<doc id="922" url="https://en.wikipedia.org/wiki?curid=922" title="Anxiety">
Anxiety

Anxiety is an emotion characterized by an unpleasant state of inner turmoil, often accompanied by nervous behavior such as pacing back and forth, somatic complaints, and rumination. It is the subjectively unpleasant feelings of dread over anticipated events.

Anxiety is a feeling of uneasiness and worry, usually generalized and unfocused as an overreaction to a situation that is only subjectively seen as menacing. It is often accompanied by muscular tension, restlessness, fatigue and problems in concentration. Anxiety is closely related to fear, which is a response to a real or perceived immediate threat; anxiety involves the expectation of future threat. People facing anxiety may withdraw from situations which have provoked anxiety in the past.

Anxiety disorders differ from developmentally normative fear or anxiety by being excessive or persisting beyond developmentally appropriate periods. They differ from transient fear or anxiety, often stress-induced, by being persistent (e.g., typically lasting 6 months or more), although the criterion for duration is intended as a general guide with allowance for some degree of flexibility and is sometimes of shorter duration in children.

Anxiety is distinguished from fear, which is an appropriate cognitive and emotional response to a perceived threat. Anxiety is related to the specific behaviors of fight-or-flight responses, defensive behavior or escape. It occurs in situations only perceived as uncontrollable or unavoidable, but not realistically so. David Barlow defines anxiety as "a future-oriented mood state in which one is not ready or prepared to attempt to cope with upcoming negative events," and that it is a distinction between future and present dangers which divides anxiety and fear. Another description of anxiety is agony, dread, terror, or even apprehension. In positive psychology, anxiety is described as the mental state that results from a difficult challenge for which the subject has insufficient coping skills.

Fear and anxiety can be differentiated in four domains: (1) duration of emotional experience, (2) temporal focus, (3) specificity of the threat, and (4) motivated direction. Fear is short-lived, present-focused, geared towards a specific threat, and facilitating escape from threat; anxiety, on the other hand, is long-acting, future-focused, broadly focused towards a diffuse threat, and promoting excessive caution while approaching a potential threat and interferes with constructive coping.

Joseph E. LeDoux and Lisa Feldman Barrett have both sought to separate automatic threat responses from additional associated cognitive activity within anxiety.

Anxiety can be experienced with long, drawn-out daily symptoms that reduce quality of life, known as chronic (or generalized) anxiety, or it can be experienced in short spurts with sporadic, stressful panic attacks, known as acute anxiety. Symptoms of anxiety can range in number, intensity, and frequency, depending on the person. While almost everyone has experienced anxiety at some point in their lives, most do not develop long-term problems with anxiety.

Anxiety may cause psychiatric and physiological symptoms.

The risk of anxiety leading to depression could possibly even lead to an individual harming themselves, which is why there are many 24-hour suicide prevention hotlines.

The behavioral effects of anxiety may include withdrawal from situations which have provoked anxiety or negative feelings in the past. Other effects may include changes in sleeping patterns, changes in habits, increase or decrease in food intake, and increased motor tension (such as foot tapping).

The emotional effects of anxiety may include "feelings of apprehension or dread, trouble concentrating, feeling tense or jumpy, anticipating the worst, irritability, restlessness, watching (and waiting) for signs (and occurrences) of danger, and, feeling like your mind's gone blank" as well as "nightmares/bad dreams, obsessions about sensations, déjà vu, a trapped-in-your-mind feeling, and feeling like everything is scary."

The cognitive effects of anxiety may include thoughts about suspected dangers, such as fear of dying. "You may ... fear that the chest pains are a deadly heart attack or that the shooting pains in your head are the result of a tumor or an aneurysm. You feel an intense fear when you think of dying, or you may think of it more often than normal, or can't get it out of your mind."

The physiological symptoms of anxiety may include:


There are various types of anxiety. Existential anxiety can occur when a person faces angst, an existential crisis, or nihilistic feelings. People can also face mathematical anxiety, somatic anxiety, stage fright, or test anxiety. Social anxiety refers to a fear of rejection and negative evaluation by other people.

The philosopher Søren Kierkegaard, in "The Concept of Anxiety" (1844), described anxiety or dread associated with the "dizziness of freedom" and suggested the possibility for positive resolution of anxiety through the self-conscious exercise of responsibility and choosing. In "Art and Artist" (1932), the psychologist Otto Rank wrote that the psychological trauma of birth was the pre-eminent human symbol of existential anxiety and encompasses the creative person's simultaneous fear of – and desire for – separation, individuation, and differentiation.

The theologian Paul Tillich characterized existential anxiety as "the state in which a being is aware of its possible nonbeing" and he listed three categories for the nonbeing and resulting anxiety: ontic (fate and death), moral (guilt and condemnation), and spiritual (emptiness and meaninglessness). According to Tillich, the last of these three types of existential anxiety, i.e. spiritual anxiety, is predominant in modern times while the others were predominant in earlier periods. Tillich argues that this anxiety can be accepted as part of the human condition or it can be resisted but with negative consequences. In its pathological form, spiritual anxiety may tend to "drive the person toward the creation of certitude in systems of meaning which are supported by tradition and authority" even though such "undoubted certitude is not built on the rock of reality".

According to Viktor Frankl, the author of "Man's Search for Meaning", when a person is faced with extreme mortal dangers, the most basic of all human wishes is to find a meaning of life to combat the "trauma of nonbeing" as death is near.

Depending on the source of the threat, psychoanalytic theory distinguishes the following types of anxiety:


According to Yerkes-Dodson law, an optimal level of arousal is necessary to best complete a task such as an exam, performance, or competitive event. However, when the anxiety or level of arousal exceeds that optimum, the result is a decline in performance.

Test anxiety is the uneasiness, apprehension, or nervousness felt by students who have a fear of failing an exam. Students who have test anxiety may experience any of the following: the association of grades with personal worth; fear of embarrassment by a teacher; fear of alienation from parents or friends; time pressures; or feeling a loss of control. Sweating, dizziness, headaches, racing heartbeats, nausea, fidgeting, uncontrollable crying or laughing and drumming on a desk are all common. Because test anxiety hinges on fear of negative evaluation, debate exists as to whether test anxiety is itself a unique anxiety disorder or whether it is a specific type of social phobia. The DSM-IV classifies test anxiety as a type of social phobia.

While the term "test anxiety" refers specifically to students, many workers share the same experience with regard to their career or profession. The fear of failing at a task and being negatively evaluated for failure can have a similarly negative effect on the adult. Management of test anxiety focuses on achieving relaxation and developing mechanisms to manage anxiety.

Humans generally require social acceptance and thus sometimes dread the disapproval of others. Apprehension of being judged by others may cause anxiety in social environments.

Anxiety during social interactions, particularly between strangers, is common among young people. It may persist into adulthood and become social anxiety or social phobia. "Stranger anxiety" in small children is not considered a phobia. In adults, an excessive fear of other people is not a developmentally common stage; it is called social anxiety. According to Cutting, social phobics do not fear the crowd but the fact that they may be judged negatively.

Social anxiety varies in degree and severity. For some people, it is characterized by experiencing discomfort or awkwardness during physical social contact (e.g. embracing, shaking hands, etc.), while in other cases it can lead to a fear of interacting with unfamiliar people altogether. Those suffering from this condition may restrict their lifestyles to accommodate the anxiety, minimizing social interaction whenever possible. Social anxiety also forms a core aspect of certain personality disorders, including avoidant personality disorder.

To the extent that a person is fearful of social encounters with unfamiliar others, some people may experience anxiety particularly during interactions with outgroup members, or people who share different group memberships (i.e., by race, ethnicity, class, gender, etc.). Depending on the nature of the antecedent relations, cognitions, and situational factors, intergroup contact may be stressful and lead to feelings of anxiety. This apprehension or fear of contact with outgroup members is often called interracial or intergroup anxiety.

As is the case the more generalized forms of social anxiety, intergroup anxiety has behavioral, cognitive, and affective effects. For instance, increases in schematic processing and simplified information processing can occur when anxiety is high. Indeed, such is consistent with related work on attentional bias in implicit memory. Additionally recent research has found that implicit racial evaluations (i.e. automatic prejudiced attitudes) can be amplified during intergroup interaction. Negative experiences have been illustrated in producing not only negative expectations, but also avoidant, or antagonistic, behavior such as hostility. Furthermore, when compared to anxiety levels and cognitive effort (e.g., impression management and self-presentation) in intragroup contexts, levels and depletion of resources may be exacerbated in the intergroup situation.

Anxiety can be either a short-term 'state' or a long-term personality "trait". Trait anxiety reflects a stable tendency across the lifespan of responding with acute, state anxiety in the anticipation of threatening situations (whether they are actually deemed threatening or not). A meta-analysis showed that a high level of neuroticism is a risk factor for development of anxiety symptoms and disorders. Such anxiety may be conscious or unconscious.

Personality can also be a trait leading to anxiety and depression. Through experience, many find it difficult to collect themselves due to their own personal nature.

Anxiety induced by the need to choose between similar options is increasingly being recognized as a problem for individuals and for organizations. In 2004, Capgemini wrote: "Today we're all faced with greater choice, more competition and less time to consider our options or seek out the right advice."

In a decision context, unpredictability or uncertainty may trigger emotional responses in anxious individuals that systematically alter decision-making. There are primarily two forms of this anxiety type. The first form refers to a choice in which there are multiple potential outcomes with known or calculable probabilities. The second form refers to the uncertainty and ambiguity related to a decision context in which there are multiple possible outcomes with unknown probabilities.

Panic disorder may share symptoms of stress and anxiety, but it is actually very different. Panic disorder is an anxiety disorder that occurs without any triggers. According to the U.S Department of Health and Human Services, this disorder can be distinguished by unexpected and repeated episodes of intense fear. Someone who suffers from panic disorder will eventually develop constant fear of another attack and as this progresses it will begin to affect daily functioning and an individual's general quality of life. It is reported by the Cleveland Clinic that panic disorder affects 2 to 3 percent of adult Americans and can begin around the time of the teenage and early adult years. Some symptoms include: difficulty breathing, chest pain, dizziness, trembling or shaking, feeling faint, nausea, fear that you are losing control or are about to die. Even though they suffer from these symptoms during an attack, the main symptom is the persistent fear of having future panic attacks.

Anxiety disorders are a group of mental disorders characterized by exaggerated feelings of anxiety and fear responses. Anxiety is a worry about future events and fear is a reaction to current events. These feelings may cause physical symptoms, such as a fast heart rate and shakiness. There are a number of anxiety disorders: including generalized anxiety disorder, specific phobia, social anxiety disorder, separation anxiety disorder, agoraphobia, panic disorder, and selective mutism. The disorder differs by what results in the symptoms. People often have more than one anxiety disorder.
Anxiety disorders are caused by a complex combination of genetic and environmental factors. To be diagnosed, symptoms typically need to be present for at least six months, be more than would be expected for the situation, and decrease a person's ability to function in their daily lives. Other problems that may result in similar symptoms include hyperthyroidism, heart disease, caffeine, alcohol, or cannabis use, and withdrawal from certain drugs, among others.
Without treatment, anxiety disorders tend to remain. Treatment may include lifestyle changes, counselling, and medications. Counselling is typically with a type of cognitive behavioural therapy. Medications, such as antidepressants or beta blockers, may improve symptoms.
About 12% of people are affected by an anxiety disorder in a given year and between 5–30% are affected at some point in their life. They occur about twice as often in women than they do in men, and generally begin before the age of 25. The most common are specific phobia which affects nearly 12% and social anxiety disorder which affects 10% at some point in their life. They affect those between the ages of 15 and 35 the most and become less common after the age of 55. Rates appear to be higher in the United States and Europe.

Anxiety can be either a short-term "state" or a long-term "trait". Whereas trait anxiety represents worrying about future events, anxiety disorders are a group of mental disorders characterized by feelings of anxiety and fear.

Anxiety disorders often occur with other mental health disorders, particularly major depressive disorder, bipolar disorder, eating disorders, or certain personality disorders. It also commonly occurs with personality traits such as neuroticism. This observed co-occurrence is partly due to genetic and environmental influences shared between these traits and anxiety.

Anxiety is often experienced by those with obsessive–compulsive disorder and is an acute presence in panic disorder.

Anxiety disorders are partly genetic, with twin studies suggesting 30-40% genetic influence on individual differences in anxiety. Environmental factors are also important. Twin studies show that individual-specific environments have a large influence on anxiety, whereas shared environmental influences (environments that affect twins in the same way) operate during childhood but decline through adolescence. Specific measured ‘environments’ that have been associated with anxiety include child abuse, family history of mental health disorders, and poverty. Anxiety is also associated with drug use, including alcohol, caffeine, and benzodiazepines (which are often prescribed to treat anxiety).

Neural circuitry involving the amygdala (which regulates emotions like anxiety and fear, stimulating the HPA Axis and sympathetic nervous system) and hippocampus (which is implicated in emotional memory along with the amygdala) is thought to underlie anxiety. People who have anxiety tend to show high activity in response to emotional stimuli in the amygdala. Some writers believe that excessive anxiety can lead to an overpotentiation of the limbic system (which includes the amygdala and nucleus accumbens), giving increased future anxiety, but this does not appear to have been proven.

Research upon adolescents who as infants had been highly apprehensive, vigilant, and fearful finds that their nucleus accumbens is more sensitive than that in other people when deciding to make an action that determined whether they received a reward. This suggests a link between circuits responsible for fear and also reward in anxious people. As researchers note, "a sense of 'responsibility', or self-agency, in a context of uncertainty (probabilistic outcomes) drives the neural system underlying appetitive motivation (i.e., nucleus accumbens) more strongly in temperamentally inhibited than noninhibited adolescents".

The microbes of the gut can connect with the brain to affect anxiety. There are various pathways along which this communication can take place. One is through the major neurotransmitters. The gut microbes such as "Bifidobacterium" and "Bacillus" produce the neurotransmitters GABA and dopamine, respectively. The neurotransmitters signal to the nervous system of the gastrointestinal tract, and those signals will be carried to the brain through the vagus nerve or the spinal system. This is demonstrated by the fact that altering the microbiome has shown anxiety- and depression-reducing effects in mice, but not in subjects without vagus nerves.

Another key pathway is the HPA axis, as mentioned above. The microbes can control the levels of cytokines in the body, and altering cytokine levels creates direct effects on areas of the brain such as the hypothalmus, the area that triggers HPA axis activity. The HPA axis regulates production of cortisol, a hormone that takes part in the body's stress response. When HPA activity spikes, cortisol levels increase, processing and reducing anxiety in stressful situations. These pathways, as well as the specific effects of individual taxa of microbes, are not yet completely clear, but the communication between the gut microbiome and the brain is undeniable, as is the ability of these pathways to alter anxiety levels.

With this communication comes the potential to treat anxiety. Prebiotics and probiotics have been shown to reduced anxiety. For example, experiments in which mice were given fructo- and galacto-oligosaccharide prebiotics and "Lactobacillus" probiotics have both demonstrated a capability to reduce anxiety. In humans, results are not as concrete, but promising.

Genetics and family history (e.g. parental anxiety) may put an individual at increased risk of an anxiety disorder, but generally external stimuli will trigger its onset or exacerbation. Estimates of genetic influence on anxiety, based on studies of twins, range from 25–40% depending on the specific type and age-group under study. For example, genetic differences account for about 43% of variance in panic disorder and 28% in generalized anxiety disorder. Longitudinal twin studies have shown the moderate stability of anxiety from childhood through to adulthood is mainly influenced by stability in genetic influence. When investigating how anxiety is passed on from parents to children, it is important to account for sharing of genes as well as environments, for example using the intergenerational children-of-twins design.

Many studies in the past used a candidate gene approach to test whether single genes were associated with anxiety. These investigations were based on hypotheses about how certain known genes influence neurotransmitters (such as serotonin and norepinephrine) and hormones (such as cortisol) that are implicated in anxiety. None of these findings are well replicated, with the possible exception of TMEM132D, COMT and MAO-A. The epigenetic signature of "BDNF", a gene that codes for a protein called "brain derived neurotrophic factor" that is found in the brain, has also been associated with anxiety and specific patterns of neural activity. and a receptor gene for "BDNF" called "NTRK2" was associated with anxiety in a large genome-wide investigation. The reason that most candidate gene findings have not replicated is that anxiety is a complex trait that is influenced by many genomic variants, each of which has a small effect on its own. Increasingly, studies of anxiety are using a hypothesis-free approach to look for parts of the genome that are implicated in anxiety using big enough samples to find associations with variants that have small effects. The largest explorations of the common genetic architecture of anxiety have been facilitated by the UK Biobank, the ANGST consortium and the CRC Fear, Anxiety and Anxiety Disorders.

Many medical conditions can cause anxiety. This includes conditions that affect the ability to breathe, like COPD and asthma, and the difficulty in breathing that often occurs near death. Conditions that cause abdominal pain or chest pain can cause anxiety and may in some cases be a somatization of anxiety; the same is true for some sexual dysfunctions. Conditions that affect the face or the skin can cause social anxiety especially among adolescents, and developmental disabilities often lead to social anxiety for children as well. Life-threatening conditions like cancer also cause anxiety.

Furthermore, certain organic diseases may present with anxiety or symptoms that mimic anxiety. These disorders include certain endocrine diseases (hypo- and hyperthyroidism, hyperprolactinemia), metabolic disorders (diabetes), deficiency states (low levels of vitamin D, B2, B12, folic acid), gastrointestinal diseases (celiac disease, non-celiac gluten sensitivity, inflammatory bowel disease), heart diseases, blood diseases (anemia), cerebral vascular accidents (transient ischemic attack, stroke), and brain degenerative diseases (Parkinson's disease, dementia, multiple sclerosis, Huntington's disease), among others.

Several drugs can cause or worsen anxiety, whether in intoxication, withdrawal or as side effect. These include alcohol, tobacco, cannabis, sedatives (including prescription benzodiazepines), opioids (including prescription pain killers and illicit drugs like heroin), stimulants (such as caffeine, cocaine and amphetamines), hallucinogens, and inhalants. While many often report self-medicating anxiety with these substances, improvements in anxiety from drugs are usually short-lived (with worsening of anxiety in the long term, sometimes with acute anxiety as soon as the drug effects wear off) and tend to be exaggerated. Acute exposure to toxic levels of benzene may cause euphoria, anxiety, and irritability lasting up to 2 weeks after the exposure.

Poor coping skills (e.g., rigidity/inflexible problem solving, denial, avoidance, impulsivity, extreme self-expectation, negative thoughts, affective instability, and inability to focus on problems) are associated with anxiety. Anxiety is also linked and perpetuated by the person's own pessimistic outcome expectancy and how they cope with feedback negativity. Temperament (e.g., neuroticism) and attitudes (e.g. pessimism) have been found to be risk factors for anxiety.

Cognitive distortions such as overgeneralizing, catastrophizing, mind reading, emotional reasoning, binocular trick, and mental filter can result in anxiety. For example, an overgeneralized belief that something bad "always" happens may lead someone to have excessive fears of even minimally risky situations and to avoid benign social situations due to anticipatory anxiety of embarrassment. In addition, those who have high anxiety can also create future stressful life events. Together, these findings suggest that anxious thoughts can lead to anticipatory anxiety as well as stressful events, which in turn cause more anxiety. Such unhealthy thoughts can be targets for successful treatment with cognitive therapy.

Psychodynamic theory posits that anxiety is often the result of opposing unconscious wishes or fears that manifest via maladaptive defense mechanisms (such as suppression, repression, anticipation, regression, somatization, passive aggression, dissociation) that develop to adapt to problems with early objects (e.g., caregivers) and empathic failures in childhood. For example, persistent parental discouragement of anger may result in repression/suppression of angry feelings which manifests as gastrointestinal distress (somatization) when provoked by another while the anger remains unconscious and outside the individual's awareness. Such conflicts can be targets for successful treatment with psychodynamic therapy. While psychodynamic therapy tends to explore the underlying roots of anxiety, cognitive behavioral therapy has also been shown to be a successful treatment for anxiety by altering irrational thoughts and unwanted behaviors.

An evolutionary psychology explanation is that increased anxiety serves the purpose of increased vigilance regarding potential threats in the environment as well as increased tendency to take proactive actions regarding such possible threats. This may cause false positive reactions but an individual suffering from anxiety may also avoid real threats. This may explain why anxious people are less likely to die due to accidents.

When people are confronted with unpleasant and potentially harmful stimuli such as foul odors or tastes, PET-scans show increased blood flow in the amygdala. In these studies, the participants also reported moderate anxiety. This might indicate that anxiety is a protective mechanism designed to prevent the organism from engaging in potentially harmful behaviors.

Social risk factors for anxiety include a history of trauma (e.g., physical, sexual or emotional abuse or assault), bullying, early life experiences and parenting factors (e.g., rejection, lack of warmth, high hostility, harsh discipline, high parental negative affect, anxious childrearing, modelling of dysfunctional and drug-abusing behaviour, discouragement of emotions, poor socialization, poor attachment, and child abuse and neglect), cultural factors (e.g., stoic families/cultures, persecuted minorities including the disabled), and socioeconomics (e.g., uneducated, unemployed, impoverished although developed countries have higher rates of anxiety disorders than developing countries). 
A 2019 comprehensive systematic review of over 50 studies showed that food insecurity in the United States is strongly associated with depression, anxiety, and sleep disorders. Food-insecure individuals had an almost 3 fold risk increase of testing positive for anxiety when compared to food-secure individuals.

Contextual factors that are thought to contribute to anxiety include gender socialization and learning experiences. In particular, learning mastery (the degree to which people perceive their lives to be under their own control) and instrumentality, which includes such traits as self-confidence, self-efficacy, independence, and competitiveness fully mediate the relation between gender and anxiety. That is, though gender differences in anxiety exist, with higher levels of anxiety in women compared to men, gender socialization and learning mastery explain these gender differences.

The first step in the management of a person with anxiety symptoms involves evaluating the possible presence of an underlying medical cause, whose recognition is essential in order to decide the correct treatment. Anxiety symptoms may mask an organic disease, or appear associated with or as a result of a medical disorder.

Cognitive behavioral therapy (CBT) is effective for anxiety disorders and is a first line treatment. CBT appears to be equally effective when carried out via the internet. While evidence for mental health apps is promising, it is preliminary.

Psychopharmacological treatment can be used in parallel to CBT or can be used alone. As a general rule, most anxiety disorders respond well to first-line agents. First-line drugs are the selective serotonin reuptake inhibitors and serotonin-norepinephrine reuptake inhibitors. Benzodiazepines are not recommended for routine use. Other treatment options include pregabalin, tricyclic antidepressants, buspirone, moclobemide, and others.

The above risk factors give natural avenues for prevention. A 2017 review found that psychological or educational interventions have a small yet statistically significant benefit for the prevention of anxiety in varied population types.

Anxiety disorder appears to be a genetically inherited neurochemical dysfunction that may involve autonomic imbalance; decreased GABA-ergic tone; allelic polymorphism of the catechol-O-methyltransferase (COMT) gene; increased adenosine receptor function; increased cortisol.

In the central nervous system (CNS), the major mediators of the symptoms of anxiety disorders appear to be norepinephrine, serotonin, dopamine, and gamma-aminobutyric acid (GABA). Other neurotransmitters and peptides, such as corticotropin-releasing factor, may be involved. Peripherally, the autonomic nervous system, especially the sympathetic nervous system, mediates many of the symptoms. Increased flow in the right parahippocampal region and reduced serotonin type 1A receptor binding in the anterior and posterior cingulate and raphe of patients are the diagnostic factors for prevalence of anxiety disorder.

The amygdala is central to the processing of fear and anxiety, and its function may be disrupted in anxiety disorders. Anxiety processing in the basolateral amygdala has been implicated with dendritic arborization of the amygdaloid neurons. SK2 potassium channels mediate inhibitory influence on action potentials and reduce arborization.




</doc>
<doc id="924" url="https://en.wikipedia.org/wiki?curid=924" title="A. A. Milne">
A. A. Milne

Alan Alexander Milne (; 18 January 1882 – 31 January 1956) was an English author, best known for his books about the teddy bear Winnie-the-Pooh and for various poems. Milne was a noted writer, primarily as a playwright, before the huge success of Pooh overshadowed all his previous work. Milne served in both World Wars, joining the British Army in World War I, and as a captain of the British Home Guard in World War II.

He is the father of bookseller Christopher Robin Milne, upon whom the character Christopher Robin is based.

Alan Alexander Milne was born in Kilburn, London to parents John Vine Milne, who was born in England, and Sarah Marie Milne (née Heginbotham) and grew up at Henley House School, 6/7 Mortimer Road (now Crescent), Kilburn, a small independent school run by his father. One of his teachers was H. G. Wells, who taught there in 1889–90. Milne attended Westminster School and Trinity College, Cambridge where he studied on a mathematics scholarship, graduating with a B.A. in Mathematics in 1903. He edited and wrote for "Granta", a student magazine. He collaborated with his brother Kenneth and their articles appeared over the initials AKM. Milne's work came to the attention of the leading British humour magazine "Punch", where Milne was to become a contributor and later an assistant editor. Considered a talented cricket fielder, Milne played for two amateur teams that were largely composed of British writers: the Allahakbarries and the Authors XI. His teammates included fellow writers J. M. Barrie, Arthur Conan Doyle and P. G. Wodehouse.

Milne joined the British Army in World War I and served as an officer in the Royal Warwickshire Regiment and later, after a debilitating illness, the Royal Corps of Signals. He was commissioned into the 4th Battalion, Royal Warwickshire Regiment on 1 February 1915 as a second lieutenant (on probation). His commission was confirmed on 20 December 1915. On 7 July 1916, he was injured in the Battle of the Somme and invalided back to England. Having recuperated, he was recruited into Military Intelligence to write propaganda articles for MI7 (b) between 1916 and 1918. He was discharged on 14 February 1919, and settled in Mallord Street, Chelsea. He relinquished his commission on 19 February 1920, retaining the rank of lieutenant.

After the war, he wrote a denunciation of war titled "Peace with Honour" (1934), which he retracted somewhat with 1940's "War with Honour". During World War II, Milne was one of the most prominent critics of fellow English writer (and Authors XI cricket teammate) P. G. Wodehouse, who was captured at his country home in France by the Nazis and imprisoned for a year. Wodehouse made radio broadcasts about his internment, which were broadcast from Berlin. Although the light-hearted broadcasts made fun of the Germans, Milne accused Wodehouse of committing an act of near treason by cooperating with his country's enemy. Wodehouse got some revenge on his former friend (e.g. in "The Mating Season") by creating fatuous parodies of the Christopher Robin poems in some of his later stories, and claiming that Milne "was probably jealous of all other writers... But I loved his stuff."

Milne married Dorothy "Daphne" de Sélincourt (1890–1971) in 1913 and their son Christopher Robin Milne was born in 1920. In 1925, Milne bought a country home, Cotchford Farm, in Hartfield, East Sussex.

During World War II, Milne was a captain in the British Home Guard in Hartfield & Forest Row, insisting on being plain "Mr. Milne" to the members of his platoon. He retired to the farm after a stroke and brain surgery in 1952 left him an invalid, and by August 1953, "he seemed very old and disenchanted." Milne died in January 1956, aged 74.

After graduating from Cambridge University in 1903, A. A. Milne contributed humorous verse and whimsical essays to "Punch", joining the staff in 1906 and becoming an assistant editor.

During this period he published 18 plays and three novels, including the murder mystery "The Red House Mystery" (1922). His son was born in August 1920 and in 1924 Milne produced a collection of children's poems, "When We Were Very Young", which were illustrated by "Punch" staff cartoonist E. H. Shepard. A collection of short stories for children "A Gallery of Children", and other stories that became part of the Winnie-the-Pooh books, were first published in 1925.

Milne was an early screenwriter for the nascent British film industry, writing four stories filmed in 1920 for the company Minerva Films (founded in 1920 by the actor Leslie Howard and his friend and story editor Adrian Brunel). These were "The Bump", starring Aubrey Smith; "Twice Two"; "Five Pound Reward"; and "Bookworms". Some of these films survive in the archives of the British Film Institute. Milne had met Howard when the actor starred in Milne's play "Mr Pim Passes By" in London.

Looking back on this period (in 1926), Milne observed that when he told his agent that he was going to write a detective story, he was told that what the country wanted from a ""Punch" humorist" was a humorous story; when two years later he said he was writing nursery rhymes, his agent and publisher were convinced he should write another detective story; and after another two years, he was being told that writing a detective story would be in the worst of taste given the demand for children's books. He concluded that "the only excuse which I have yet discovered for writing anything is that I want to write it; and I should be as proud to be delivered of a Telephone Directory "con amore" as I should be ashamed to create a Blank Verse Tragedy at the bidding of others."

Milne is most famous for his two "Pooh" books about a boy named Christopher Robin after his son, Christopher Robin Milne (1920–1996), and various characters inspired by his son's stuffed animals, most notably the bear named Winnie-the-Pooh. Christopher Robin Milne's stuffed bear, originally named Edward, was renamed Winnie after a Canadian black bear named Winnie (after Winnipeg), which was used as a military mascot in World War I, and left to London Zoo during the war. "The Pooh" comes from a swan the young Milne named "Pooh". E. H. Shepard illustrated the original Pooh books, using his own son's teddy Growler ("a magnificent bear") as the model. The rest of Christopher Robin Milne's toys, Piglet, Eeyore, Kanga, Roo and Tigger, were incorporated into A. A. Milne's stories, and two more characters – Rabbit and Owl – were created by Milne's imagination. Christopher Robin Milne's own toys are now on display in New York where 750,000 people visit them every year.

The fictional Hundred Acre Wood of the Pooh stories derives from Five Hundred Acre Wood in Ashdown Forest in East Sussex, South East England, where the Pooh stories were set. Milne lived on the northern edge of the forest at Cotchford Farm, , and took his son walking there. E. H. Shepard drew on the landscapes of Ashdown Forest as inspiration for many of the illustrations he provided for the Pooh books. The adult Christopher Robin commented: "Pooh's Forest and Ashdown Forest are identical." Popular tourist locations at Ashdown Forest include: "Galleon's Lap", "The Enchanted Place", the "Heffalump Trap" and "Lone Pine", "Eeyore’s Sad and Gloomy Place", and the wooden "Pooh Bridge" where Pooh and Piglet invented Poohsticks.

Not yet known as Pooh, he made his first appearance in a poem, "Teddy Bear", published in "Punch" magazine in February 1924 and republished in "When We Were Very Young". Pooh first appeared in the "London Evening News" on Christmas Eve, 1925, in a story called "The Wrong Sort Of Bees". "Winnie-the-Pooh" was published in 1926, followed by "The House at Pooh Corner" in 1928. A second collection of nursery rhymes, "Now We Are Six", was published in 1927. All four books were illustrated by E. H. Shepard. Milne also published four plays in this period. He also "gallantly stepped forward" to contribute a quarter of the costs of dramatising P. G. Wodehouse's "A Damsel in Distress". "The World of Pooh" won the Lewis Carroll Shelf Award in 1958.

The success of his children's books was to become a source of considerable annoyance to Milne, whose self-avowed aim was to write whatever he pleased and who had, until then, found a ready audience for each change of direction: he had freed pre-war "Punch" from its ponderous facetiousness; he had made a considerable reputation as a playwright (like his idol J. M. Barrie) on both sides of the Atlantic; he had produced a witty piece of detective writing in "The Red House Mystery" (although this was severely criticised by Raymond Chandler in the essay "The Simple Art of Murder" is the euphonious-named story story collection (1950) for the implausibility of its plot). But once Milne had, in his own words, "said goodbye to all that in 70,000 words" (the approximate length of his four principal children's books), he had no intention of producing any reworkings lacking in originality, given that one of the sources of inspiration, his son, was growing older.

Another reason Milne stopped writing children's books, and especially about Winnie-the-Pooh, was that he felt "amazement and disgust" over the fame his son was exposed to, and said that "I feel that the legal Christopher Robin has already had more publicity than I want for him. I do not want CR Milne to ever wish that his name were Charles Robert."

In his literary home, "Punch", where the "When We Were Very Young" verses had first appeared, Methuen continued to publish whatever Milne wrote, including the long poem "The Norman Church" and an assembly of articles entitled "Year In, Year Out" (which Milne likened to a benefit night for the author).

In 1930, Milne adapted Kenneth Grahame's novel "The Wind in the Willows" for the stage as "Toad of Toad Hall". The title was an implicit admission that such chapters as Chapter 7, "The Piper at the Gates of Dawn," could not survive translation to the theatre. A special introduction written by Milne is included in some editions of Grahame's novel.

Milne and his wife became estranged from their son, who came to resent what he saw as his father's exploitation of his childhood and came to hate the books that had thrust him into the public eye. Christopher's marriage to his first cousin, Lesley de Sélincourt, distanced him still further from his parents – Lesley's father and Christopher's mother had not spoken to each other for 30 years.

The rights to A. A. Milne's Pooh books were left to four beneficiaries: his family, the Royal Literary Fund, Westminster School and the Garrick Club. After Milne's death in 1956, one week and six days after his 74th birthday, his widow sold her rights to the Pooh characters to Stephen Slesinger, whose widow sold the rights after Slesinger's death to the Walt Disney Company, which has made many Pooh cartoon movies, a Disney Channel television show, as well as Pooh-related merchandise. In 2001, the other beneficiaries sold their interest in the estate to the Disney Corporation for $350m. Previously Disney had been paying twice-yearly royalties to these beneficiaries. The estate of E. H. Shepard also received a sum in the deal. The UK copyright on the text of the original Winnie the Pooh books expires on 1 January 2027; at the beginning of the year after the 70th anniversary of the author's death (PMA-70), and has already expired in those countries with a PMA-50 rule. This applies to all of Milne's works except those first published posthumously. The illustrations in the Pooh books will remain under copyright until the same amount of time has passed, after the illustrator's death; in the UK, this will be on 1 January 2047. In the United States, copyright will not expire until 95 years after publication for each of Milne's books first published before 1978, but this includes the illustrations.

In 2008, a collection of original illustrations featuring Winnie-the-Pooh and his animal friends sold for more than £1.2 million at auction in Sotheby's, London. "Forbes" magazine ranked Winnie the Pooh the most valuable fictional character in 2002; Winnie the Pooh merchandising products alone had annual sales of more than $5.9 billion. In 2005, Winnie the Pooh generated $6 billion, a figure surpassed by only Mickey Mouse.

A memorial plaque in Ashdown Forest, unveiled by Christopher Robin in 1979, commemorates the work of A. A. Milne and Shepard in creating the world of Pooh. Milne once wrote of Ashdown Forest: "In that enchanted place on the top of the forest a little boy and his bear will always be playing."

In 2003, "Winnie the Pooh" was listed at number 7 on the BBC's poll The Big Read which determined the UK's "best-loved novels" of all time. In 2006, Winnie the Pooh received a star on the Hollywood Walk of Fame, marking the 80th birthday of Milne's creation. That same year a UK poll saw Winnie the Pooh voted onto the list of icons of England.

Marking the 90th anniversary of Milne's creation of the character, and the 90th birthday of Elizabeth II, in 2016 a new story sees Winnie the Pooh meet the Queen at Buckingham Palace. The illustrated and audio adventure is titled "Winnie-the-Pooh Meets the Queen", and has been narrated by actor Jim Broadbent. Also in 2016, a new character, a Penguin, was unveiled in "The Best Bear in All the World", which was inspired by a long-lost photograph of Milne and his son Christopher with a toy penguin.

Several of Milne's children's poems were set to music by the composer Harold Fraser-Simson. His poems have been parodied many times, including with the books "When We Were Rather Older" and "Now We Are Sixty". The 1963 film "The King's Breakfast" was based on Milne's poem of the same name.
The Pooh books were used as the basis for two academic satires by Frederick C Crews: 'The Pooh Perplex'(1963/4) and 'Postmodern Pooh'(2002).

An exhibition entitled """" appeared at the "V & A" from 9 December 2017 to 8 April 2018.

An elementary school in Houston, Texas, United States, operated by the Houston Independent School District (HISD), is named after Milne. The school, A. A. Milne Elementary School in Brays Oaks, opened in 1991.

The bulk of A. A. Milne's papers are housed at the Harry Ransom Center at the University of Texas at Austin. The collection, established at the center in 1964, consists of manuscript drafts and fragments for over 150 of Milne's works, as well as correspondence, legal documents, genealogical records, and some personal effects. The library division holds several books formerly belonging to Milne and his wife Dorothy. The Harry Ransom Center also has small collections of correspondence from Christopher Robin Milne and Milne's frequent illustrator Ernest Shepard.

The original manuscripts for "Winnie the Pooh" and "The House at Pooh Corner" are archived separately at Trinity College Library, Cambridge.

Milne did not speak out much on the subject of religion, although he used religious terms to explain his decision, while remaining a pacifist, to join the British Home Guard: "In fighting Hitler," he wrote, "we are truly fighting the Devil, the Anti-Christ ... Hitler was a crusader against God."

His best known comment on the subject was recalled on his death:
He wrote in the poem "Explained":

He also wrote in the poem "Vespers":










Milne is portrayed by Domhnall Gleeson in "Goodbye Christopher Robin", a 2017 film.

In the 2018 fantasy film "Christopher Robin", an extension of the Disney Winnie the Pooh franchise, Tristan Sturrock plays A.A. Milne.




</doc>
<doc id="925" url="https://en.wikipedia.org/wiki?curid=925" title="Asociación Alumni">
Asociación Alumni

Asociación Alumni, usually just Alumni, is an Argentine rugby union club located in Tortuguitas, Greater Buenos Aires. The senior squad currently competes at Top 12, the first division of the Unión de Rugby de Buenos Aires league system.

The club has ties with former football club Alumni because both were established by Buenos Aires English High School students.

The first club with the name "Alumni" played association football, having been found in 1898 by students of Buenos Aires English High School (BAEHS) along with director Alexander Watson Hutton. Originally under the name "English High School A.C.", the team would be later obliged by the Association to change its name, therefore "Alumni" was chosen, following a proposal by Carlos Bowers, a former student of the school.

Alumni was the most successful team during the first years of Argentine football, winning 10 of 14 league championships contested. Alumni is still considered the first great football team in the country. Alumni was reorganised in 1908, "in order to encourage people to practise all kind of sports, specially football". This was the last try to develop itself as a sports club rather than just a football team, such as Lomas, Belgrano and Quilmes had successfully done in the past, but the efforts were not enough. Alumni played its last game in 1911 and was definitely dissolved on April 24, 1913.

In 1951, two guards of the BAEHS, Daniel Ginhson (also a former player of Buenos Aires F.C.) and Guillermo Cubelli, supported by the school's alumni and fathers of the students, they decided to establish a club focused on rugby union exclusively. Former players still alive of Alumni football club and descendants of other players already dead gave their permission to use the name "Alumni".

On December 13, in a meeting presided by Carlos Bowers himself (who had proposed the name "Alumni" to the original football team 50 years before), the club was officially established under the name "Asociación Juvenil Alumni", also adopting the same colors as its predecessor.

The team achieved good results and in 1960 the club presented a team that won the third division of the Buenos Aires league, reaching the second division. Since then, Alumni has played at the highest level of Argentine rugby and its rivalry with Belgrano Athletic Club is one of the fiercest local derbies in Buenos Aires. Alumni would later climb up to first division winning 5 titles: 4 consecutive between 1989 and 1992, and the other in 2001.

In 2002, Alumni won its first Nacional de Clubes title, defeating Jockey Club de Rosario 23–21 in the final.

As of January 2018:




</doc>
<doc id="928" url="https://en.wikipedia.org/wiki?curid=928" title="Axiom">
Axiom

An axiom or postulate is a statement that is taken to be true, to serve as a premise or starting point for further reasoning and arguments. The word comes from the Greek "axíōma" () 'that which is thought worthy or fit' or 'that which commends itself as evident.'

The term has subtle differences in definition when used in the context of different fields of study. As defined in classic philosophy, an axiom is a statement that is so evident or well-established, that it is accepted without controversy or question. As used in modern logic, an axiom is a premise or starting point for reasoning. 
As used in mathematics, the term "axiom" is used in two related but distinguishable senses: "logical axioms" and "non-logical axioms". Logical axioms are usually statements that are taken to be true within the system of logic they define and are often shown in symbolic form (e.g., ("A" and "B") implies "A"), while non-logical axioms (e.g., ) are actually substantive assertions about the elements of the domain of a specific mathematical theory (such as arithmetic). 

When used in the latter sense, "axiom", "postulate", and "assumption" may be used interchangeably. In most cases, a non-logical axiom is simply a formal logical expression used in deduction to build a mathematical theory, and might or might not be self-evident in nature (e.g., parallel postulate in Euclidean geometry). To axiomatize a system of knowledge is to show that its claims can be derived from a small, well-understood set of sentences (the axioms), and there may be multiple ways to axiomatize a given mathematical domain.

Any axiom is a statement that serves as a starting point from which other statements are logically derived. Whether it is meaningful (and, if so, what it means) for an axiom to be "true" is a subject of debate in the philosophy of mathematics.

The word "axiom" comes from the Greek word ("axíōma"), a verbal noun from the verb ("axioein"), meaning "to deem worthy", but also "to require", which in turn comes from ("áxios"), meaning "being in balance", and hence "having (the same) value (as)", "worthy", "proper". Among the ancient Greek philosophers an axiom was a claim which could be seen to be self-evidently true without any need for proof.

The root meaning of the word "postulate" is to "demand"; for instance, Euclid demands that one agree that some things can be done (e.g., any two points can be joined by a straight line).

Ancient geometers maintained some distinction between axioms and postulates. While commenting on Euclid's books, Proclus remarks that, "Geminus held that this [4th] Postulate should not be classed as a postulate but as an axiom, since it does not, like the first three Postulates, assert the possibility of some construction but expresses an essential property." Boethius translated 'postulate' as "petitio" and called the axioms "notiones communes" but in later manuscripts this usage was not always strictly kept.

The logico-deductive method whereby conclusions (new knowledge) follow from premises (old knowledge) through the application of sound arguments (syllogisms, rules of inference) was developed by the ancient Greeks, and has become the core principle of modern mathematics. Tautologies excluded, nothing can be deduced if nothing is assumed. Axioms and postulates are thus the basic assumptions underlying a given body of deductive knowledge. They are accepted without demonstration. All other assertions (theorems, in the case of mathematics) must be proven with the aid of these basic assumptions. However, the interpretation of mathematical knowledge has changed from ancient times to the modern, and consequently the terms "axiom" and "postulate" hold a slightly different meaning for the present day mathematician, than they did for Aristotle and Euclid.

The ancient Greeks considered geometry as just one of several sciences, and held the theorems of geometry on par with scientific facts. As such, they developed and used the logico-deductive method as a means of avoiding error, and for structuring and communicating knowledge. Aristotle's posterior analytics is a definitive exposition of the classical view.

An "axiom", in classical terminology, referred to a self-evident assumption common to many branches of science. A good example would be the assertion that "When an equal amount is taken from equals, an equal amount results."

At the foundation of the various sciences lay certain additional hypotheses which were accepted without proof. Such a hypothesis was termed a "postulate". While the axioms were common to many sciences, the postulates of each particular science were different. Their validity had to be established by means of real-world experience. Indeed, Aristotle warns that the content of a science cannot be successfully communicated, if the learner is in doubt about the truth of the postulates.

The classical approach is well-illustrated by Euclid's Elements, where a list of postulates is given (common-sensical geometric facts drawn from our experience), followed by a list of "common notions" (very basic, self-evident assertions).

A lesson learned by mathematics in the last 150 years is that it is useful to strip the meaning away from the mathematical assertions (axioms, postulates, propositions, theorems) and definitions. One must concede the need for primitive notions, or undefined terms or concepts, in any study. Such abstraction or formalization makes mathematical knowledge more general, capable of multiple different meanings, and therefore useful in multiple contexts. Alessandro Padoa, Mario Pieri, and Giuseppe Peano were pioneers in this movement.

Structuralist mathematics goes further, and develops theories and axioms (e.g. field theory, group theory, topology, vector spaces) without "any" particular application in mind. The distinction between an "axiom" and a "postulate" disappears. The postulates of Euclid are profitably motivated by saying that they lead to a great wealth of geometric facts. The truth of these complicated facts rests on the acceptance of the basic hypotheses. However, by throwing out Euclid's fifth postulate, one can get theories that have meaning in wider contexts (e.g., hyperbolic geometry). As such, one must simply be prepared to use labels such as "line" and "parallel" with greater flexibility. The development of hyperbolic geometry taught mathematicians that it is useful to regard postulates as purely formal statements, and not as facts based on experience.

When mathematicians employ the field axioms, the intentions are even more abstract. The propositions of field theory do not concern any one particular application; the mathematician now works in complete abstraction. There are many examples of fields; field theory gives correct knowledge about them all.

It is not correct to say that the axioms of field theory are "propositions that are regarded as true without proof." Rather, the field axioms are a set of constraints. If any given system of addition and multiplication satisfies these constraints, then one is in a position to instantly know a great deal of extra information about this system.

Modern mathematics formalizes its foundations to such an extent that mathematical theories can be regarded as mathematical objects, and mathematics itself can be regarded as a branch of logic. Frege, Russell, Poincaré, Hilbert, and Gödel are some of the key figures in this development.

Another lesson learned in modern mathematics is to examine purported proofs carefully for hidden assumptions.

In the modern understanding, a set of axioms is any collection of formally stated assertions from which other formally stated assertions follow — by the application of certain well-defined rules. In this view, logic becomes just another formal system. A set of axioms should be consistent; it should be impossible to derive a contradiction from the axiom. A set of axioms should also be non-redundant; an assertion that can be deduced from other axioms need not be regarded as an axiom.

It was the early hope of modern logicians that various branches of mathematics, perhaps all of mathematics, could be derived from a consistent collection of basic axioms. An early success of the formalist program was Hilbert's formalization of Euclidean geometry, and the related demonstration of the consistency of those axioms.

In a wider context, there was an attempt to base all of mathematics on Cantor's set theory. Here, the emergence of Russell's paradox and similar antinomies of naïve set theory raised the possibility that any such system could turn out to be inconsistent.

The formalist project suffered a decisive setback, when in 1931 Gödel showed that it is possible, for any sufficiently large set of axioms (Peano's axioms, for example) to construct a statement whose truth is independent of that set of axioms. As a corollary, Gödel proved that the consistency of a theory like Peano arithmetic is an unprovable assertion within the scope of that theory.

It is reasonable to believe in the consistency of Peano arithmetic because it is satisfied by the system of natural numbers, an infinite but intuitively accessible formal system. However, at present, there is no known way of demonstrating the consistency of the modern Zermelo–Fraenkel axioms for set theory. Furthermore, using techniques of forcing (Cohen) one can show that the continuum hypothesis (Cantor) is independent of the Zermelo–Fraenkel axioms. Thus, even this very general set of axioms cannot be regarded as the definitive foundation for mathematics.

Axioms play a key role not only in mathematics, but also in other sciences, notably in theoretical physics. In particular, the monumental work of Isaac Newton is essentially based on Euclid's axioms, augmented by a postulate on the non-relation of spacetime and the physics taking place in it at any moment.

In 1905, Newton's axioms were replaced by those of Albert Einstein's special relativity, and later on by those of general relativity.

Another paper of Albert Einstein and coworkers (see EPR paradox), almost immediately contradicted by Niels Bohr, concerned the interpretation of quantum mechanics. This was in 1935. According to Bohr, this new theory should be probabilistic, whereas according to Einstein it should be deterministic. Notably, the underlying quantum mechanical theory, i.e. the set of "theorems" derived by it, seemed to be identical. Einstein even assumed that it would be sufficient to add to quantum mechanics "hidden variables" to enforce determinism. However, thirty years later, in 1964, John Bell found a theorem, involving complicated optical correlations (see Bell inequalities), which yielded measurably different results using Einstein's axioms compared to using Bohr's axioms. And it took roughly another twenty years until an experiment of Alain Aspect got results in favour of Bohr's axioms, not Einstein's. (Bohr's axioms are simply: The theory should be probabilistic in the sense of the Copenhagen interpretation.)

As a consequence, it is not necessary to explicitly cite Einstein's axioms, the more so since they concern subtle points on the "reality" and "locality" of experiments.

Regardless, the role of axioms in mathematics and in the above-mentioned sciences is different. In mathematics one neither "proves" nor "disproves" an axiom for a set of theorems; the point is simply that in the conceptual realm identified by the axioms, the theorems logically follow. In contrast, in physics a comparison with experiments always makes sense, since a falsified physical theory needs modification.

In the field of mathematical logic, a clear distinction is made between two notions of axioms: "logical" and "non-logical" (somewhat similar to the ancient distinction between "axioms" and "postulates" respectively).

These are certain formulas in a formal language that are universally valid, that is, formulas that are satisfied by every assignment of values. Usually one takes as logical axioms "at least" some minimal set of tautologies that is sufficient for proving all tautologies in the language; in the case of predicate logic more logical axioms than that are required, in order to prove logical truths that are not tautologies in the strict sense.

In propositional logic it is common to take as logical axioms all formulae of the following forms, where formula_1, formula_2, and formula_3 can be any formulae of the language and where the included primitive connectives are only "formula_4" for negation of the immediately following proposition and "formula_5" for implication from antecedent to consequent propositions:


Each of these patterns is an "axiom schema", a rule for generating an infinite number of axioms. For example, if formula_9, formula_10, and formula_11 are propositional variables, then formula_12 and formula_13 are both instances of axiom schema 1, and hence are axioms. It can be shown that with only these three axiom schemata and "modus ponens", one can prove all tautologies of the propositional calculus. It can also be shown that no pair of these schemata is sufficient for proving all tautologies with "modus ponens".

Other axiom schemata involving the same or different sets of primitive connectives can be alternatively constructed.

These axiom schemata are also used in the predicate calculus, but additional logical axioms are needed to include a quantifier in the calculus.

This means that, for any variable symbol formula_14 the formula formula_15 can be regarded as an axiom. Also, in this example, for this not to fall into vagueness and a never-ending series of "primitive notions", either a precise notion of what we mean by formula_15 (or, for that matter, "to be equal") has to be well established first, or a purely formal and syntactical usage of the symbol formula_17 has to be enforced, only regarding it as a string and only a string of symbols, and mathematical logic does indeed do that.

Another, more interesting example axiom scheme, is that which provides us with what is known as Universal Instantiation:

Where the symbol formula_18 stands for the formula formula_1 with the term formula_20 substituted for formula_21. (See Substitution of variables.) In informal terms, this example allows us to state that, if we know that a certain property formula_22 holds for every formula_21 and that formula_20 stands for a particular object in our structure, then we should be able to claim formula_25. Again, "we are claiming that the formula" formula_26 "is valid", that is, we must be able to give a "proof" of this fact, or more properly speaking, a "metaproof". Actually, these examples are "metatheorems" of our theory of mathematical logic since we are dealing with the very concept of "proof" itself. Aside from this, we can also have Existential Generalization:

Non-logical axioms are formulas that play the role of theory-specific assumptions. Reasoning about two different structures, for example the natural numbers and the integers, may involve the same logical axioms; the non-logical axioms aim to capture what is special about a particular structure (or set of structures, such as groups). Thus non-logical axioms, unlike logical axioms, are not "tautologies". Another name for a non-logical axiom is "postulate".

Almost every modern mathematical theory starts from a given set of non-logical axioms, and it was thought that in principle every theory could be axiomatized in this way and formalized down to the bare language of logical formulas.

Non-logical axioms are often simply referred to as "axioms" in mathematical discourse. This does not mean that it is claimed that they are true in some absolute sense. For example, in some groups, the group operation is commutative, and this can be asserted with the introduction of an additional axiom, but without this axiom we can do quite well developing (the more general) group theory, and we can even take its negation as an axiom for the study of non-commutative groups.

Thus, an "axiom" is an elementary basis for a formal logic system that together with the rules of inference define a deductive system.

This section gives examples of mathematical theories that are developed entirely from a set of non-logical axioms (axioms, henceforth). A rigorous treatment of any of these topics begins with a specification of these axioms.

Basic theories, such as arithmetic, real analysis and complex analysis are often introduced non-axiomatically, but implicitly or explicitly there is generally an assumption that the axioms being used are the axioms of Zermelo–Fraenkel set theory with choice, abbreviated ZFC, or some very similar system of axiomatic set theory like Von Neumann–Bernays–Gödel set theory, a conservative extension of ZFC. Sometimes slightly stronger theories such as Morse–Kelley set theory or set theory with a strongly inaccessible cardinal allowing the use of a Grothendieck universe are used, but in fact most mathematicians can actually prove all they need in systems weaker than ZFC, such as second-order arithmetic.

The study of topology in mathematics extends all over through point set topology, algebraic topology, differential topology, and all the related paraphernalia, such as homology theory, homotopy theory. The development of "abstract algebra" brought with itself group theory, rings, fields, and Galois theory.

This list could be expanded to include most fields of mathematics, including measure theory, ergodic theory, probability, representation theory, and differential geometry.

The Peano axioms are the most widely used "axiomatization" of first-order arithmetic. They are a set of axioms strong enough to prove many important facts about number theory and they allowed Gödel to establish his famous second incompleteness theorem.

We have a language formula_27 where formula_28 is a constant symbol and formula_29 is a unary function and the following axioms:


The standard structure is formula_35 where formula_36 is the set of natural numbers, formula_29 is the successor function and formula_28 is naturally interpreted as the number 0.

Probably the oldest, and most famous, list of axioms are the 4 + 1 Euclid's postulates of plane geometry. The axioms are referred to as "4 + 1" because for nearly two millennia the fifth (parallel) postulate ("through a point outside a line there is exactly one parallel") was suspected of being derivable from the first four. Ultimately, the fifth postulate was found to be independent of the first four. Indeed, one can assume that exactly one parallel through a point outside a line exists, or that infinitely many exist. This choice gives us two alternative forms of geometry in which the interior angles of a triangle add up to exactly 180 degrees or less, respectively, and are known as Euclidean and hyperbolic geometries. If one also removes the second postulate ("a line can be extended indefinitely") then elliptic geometry arises, where there is no parallel through a point outside a line, and in which the interior angles of a triangle add up to more than 180 degrees.

The objectives of study are within the domain of real numbers. The real numbers are uniquely picked out (up to isomorphism) by the properties of a "Dedekind complete ordered field", meaning that any nonempty set of real numbers with an upper bound has a least upper bound. However, expressing these properties as axioms requires use of second-order logic. The Löwenheim–Skolem theorems tell us that if we restrict ourselves to first-order logic, any axiom system for the reals admits other models, including both models that are smaller than the reals and models that are larger. Some of the latter are studied in non-standard analysis.

A deductive system consists of a set formula_39 of logical axioms, a set formula_40 of non-logical axioms, and a set formula_41 of "rules of inference". A desirable property of a deductive system is that it be complete. A system is said to be complete if, for all formulas formula_1,

formula_43
that is, for any statement that is a "logical consequence" of formula_40 there actually exists a "deduction" of the statement from formula_40. This is sometimes expressed as "everything that is true is provable", but it must be understood that "true" here means "made true by the set of axioms", and not, for example, "true in the intended interpretation". Gödel's completeness theorem establishes the completeness of a certain commonly used type of deductive system.

Note that "completeness" has a different meaning here than it does in the context of Gödel's first incompleteness theorem, which states that no "recursive", "consistent" set of non-logical axioms formula_40 of the Theory of Arithmetic is "complete", in the sense that there will always exist an arithmetic statement formula_1 such that neither formula_1 nor formula_49 can be proved from the given set of axioms.

There is thus, on the one hand, the notion of "completeness of a deductive system" and on the other hand that of "completeness of a set of non-logical axioms". The completeness theorem and the incompleteness theorem, despite their names, do not contradict one another.

Early mathematicians regarded axiomatic geometry as a model of physical space, and obviously there could only be one such model. The idea that alternative mathematical systems might exist was very troubling to mathematicians of the 19th century and the developers of systems such as Boolean algebra made elaborate efforts to derive them from traditional arithmetic. Galois showed just before his untimely death that these efforts were largely wasted. Ultimately, the abstract parallels between algebraic systems were seen to be more important than the details and modern algebra was born. In the modern view axioms may be any set of formulas, as long as they are not known to be inconsistent.





</doc>
<doc id="929" url="https://en.wikipedia.org/wiki?curid=929" title="Alpha">
Alpha

Alpha (uppercase , lowercase ; , "álpha", "álfa") is the first letter of the Greek alphabet. In the system of Greek numerals, it has a value of 1.

It was derived from the Phoenician and Hebrew letter aleph - an ox or leader.

Letters that arose from alpha include the Latin A and the Cyrillic letter А.

In English, the noun "alpha" is used as a synonym for "beginning", or "first" (in a series), reflecting its Greek roots.

In Ancient Greek, alpha was pronounced and could be either phonemically long ([aː]) or short ([a]). Where there is ambiguity, long and short alpha are sometimes written with a macron and breve today: Ᾱᾱ, Ᾰᾰ.

In Modern Greek, vowel length has been lost, and all instances of alpha simply represent .

In the polytonic orthography of Greek, alpha, like other vowel letters, can occur with several diacritic marks: any of three accent symbols (), and either of two breathing marks (), as well as combinations of these. It can also combine with the iota subscript ().

In the Attic–Ionic dialect of Ancient Greek, long alpha fronted to (eta). In Ionic, the shift took place in all positions. In Attic, the shift did not take place after epsilon, iota, and rho (ε, ι, ρ; "e", "i", "r"). In Doric and Aeolic, long alpha is preserved in all positions.

Privative a is the Ancient Greek prefix ἀ- or ἀν- "a-", "an-", added to words to negate them. It originates from the Proto-Indo-European *"" (syllabic nasal) and is cognate with English "un-".

Copulative a is the Greek prefix ἁ- or ἀ- "ha-", "a-". It comes from Proto-Indo-European *"".

The letter alpha represents various concepts in physics and chemistry, including alpha radiation, angular acceleration, alpha particles, alpha carbon and strength of electromagnetic interaction (as Fine-structure constant). Alpha also stands for thermal expansion coefficient of a compound in physical chemistry. It is also commonly used in mathematics in algebraic solutions representing quantities such as angles. Furthermore, in mathematics, the letter alpha is used to denote the area underneath a normal curve in statistics to denote significance level when proving null and alternative hypotheses. In zoology, it is used to name the dominant individual in a wolf or dog pack. In aerodynamics, the letter is used as a symbol for the angle of attack of an aircraft and the word "alpha" is used as a synonym for this property.

The proportionality operator "∝" (in Unicode: U+221D) is sometimes mistaken for alpha.

The uppercase letter alpha is not generally used as a symbol because it tends to be rendered identically to the uppercase Latin A.

In the International Phonetic Alphabet, the letter ɑ, which looks similar to the lower-case alpha, represents the open back unrounded vowel.

Alpha was derived from "aleph", which in Phoenician means "ox".

Plutarch, in "Moralia", presents a discussion on why the letter alpha stands first in the alphabet. Ammonius asks Plutarch what he, being a Boeotian, has to say for Cadmus, the Phoenician who reputedly settled in Thebes and introduced the alphabet to Greece, placing "alpha" first because it is the Phoenician name for ox—which, unlike Hesiod, the Phoenicians considered not the second or third, but the first of all necessities. "Nothing at all," Plutarch replied. He then added that he would rather be assisted by Lamprias, his own grandfather, than by Dionysus' grandfather, i.e. Cadmus. For Lamprias had said that the first articulate sound made is "alpha", because it is very plain and simple—the air coming off the mouth does not require any motion of the tongue—and therefore this is the first sound that children make.

According to Plutarch's natural order of attribution of the vowels to the planets, alpha was connected with the Moon.

Alpha, both as a symbol and term, is used to refer to or describe a variety of things, including the first or most significant occurrence of something. The New Testament has God declaring himself to be the "Alpha and Omega, the beginning and the end, the first and the last." (Revelation 22:13, KJV, and see also 1:8). Because of this symbolism, the characters codice_1 and codice_2 denote the left and right arguments in the APL programming language.

The term "alpha" has been used to denote position in social hierarchy, examples being "alpha males" or pack leaders.


For accented Greek characters, see Greek diacritics: Computer encoding.




</doc>
<doc id="930" url="https://en.wikipedia.org/wiki?curid=930" title="Alvin Toffler">
Alvin Toffler

Alvin Toffler (October 4, 1928 – June 27, 2016) was an American writer, futurist, and businessman known for his works discussing modern technologies, including the digital revolution and the communication revolution, with emphasis on their effects on cultures worldwide. He is regarded as one of the world's outstanding futurists.

Toffler was an associate editor of "Fortune" magazine. In his early works he focused on technology and its impact, which he termed "information overload." In 1970 his first major book about the future, "Future Shock", became a worldwide best-seller and has sold over 6 million copies.

He and his wife Heidi Toffler, who collaborated with him for most of his writings, moved on to examining the reaction to changes in society with another best-selling book, "The Third Wave" in 1980. In it, he foresaw such technological advances as cloning, personal computers, the Internet, cable television and mobile communication. His later focus, via their other best-seller, "Powershift", (1990), was on the increasing power of 21st-century military hardware and the proliferation of new technologies.

He founded Toffler Associates, a management consulting company, and was a visiting scholar at the Russell Sage Foundation, visiting professor at Cornell University, faculty member of the New School for Social Research, a White House correspondent, and a business consultant. Toffler's ideas and writings were a significant influence on the thinking of business and government leaders worldwide, including China's Zhao Ziyang, and AOL founder Steve Case.

Alvin Toffler was born on October 4, 1928, in New York City, and raised in Brooklyn. He was the son of Rose (Albaum) and Sam Toffler, a furrier, both Jewish immigrants from Poland. He had one younger sister. He was inspired to become a writer at the age of 7 by his aunt and uncle, who lived with the Tofflers. "They were Depression-era literary intellectuals," Toffler said, "and they always talked about exciting ideas."

Toffler graduated from New York University in 1950 as an English major, though by his own account he was more focused on political activism than grades. He met his future wife, Adelaide Elizabeth Farrell (nicknamed "Heidi"), when she was starting a graduate course in linguistics. Being radical students, they decided against further graduate work and moved to the Midwest, where they married on April 29, 1950.

Seeking experiences to write about, Alvin and Heidi Toffler spent the next five years as blue collar workers on assembly lines while studying industrial mass production in their daily work. He compared his own desire for experience to other writers, such as Jack London, who in his quest for subjects to write about sailed the seas, and John Steinbeck, who went to pick grapes with migrant workers. In their first factory jobs, Heidi became a union shop steward in the aluminum foundry where she worked. Alvin became a millwright and welder. In the evenings Alvin would write poetry and fiction, but discovered he was proficient at neither.

His hands-on practical labor experience helped Alvin Toffler land a position at a union-backed newspaper, a transfer to its Washington bureau in 1957, then three years as a White House correspondent, covering Congress and the White House for a Pennsylvania daily newspaper.

They returned to New York City in 1959 when "Fortune" magazine invited Alvin to become its labor columnist, later having him write about business and management. After leaving "Fortune" magazine in 1962, Toffler began a freelance career, writing long form articles for scholarly journals and magazines. His 1964 "Playboy interviews" with Russian novelist Vladimir Nabokov and Ayn Rand were considered among the magazine's best. His interview with Rand was the first time the magazine had given such a platform to a female intellectual, which as one commentator said, "the real bird of paradise Toffler captured for Playboy in 1964 was Ayn Rand."

Toffler was hired by IBM to conduct research and write a paper on the social and organizational impact of computers, leading to his contact with the earliest computer "gurus" and artificial intelligence researchers and proponents. Xerox invited him to write about its research laboratory and AT&T consulted him for strategic advice. This AT&T work led to a study of telecommunications, which advised the company's top management to break up the company more than a decade before the government forced AT&T to break up.

In the mid-1960s, the Tofflers began five years of research on what would become "Future Shock", published in 1970. It has sold over 6 million copies worldwide, according to the "New York Times," or over 15 million copies according to the Tofflers' Web site. Toffler coined the term "future shock" to refer to what happens to a society when change happens too fast, which results in social confusion and normal decision-making processes breaking down. The book has never been out of print and has been translated into dozens of languages.

He continued the theme in "The Third Wave" in 1980. While he describes the first and second waves as the agricultural and industrial revolutions, the "third wave," a phrase he coined, represents the current information, computer-based revolution. He forecast the spread of the Internet and email, interactive media, cable television, cloning, and other digital advancements. He claimed that one of the side effects of the digital age has been "information overload," another term he coined. In 1990 he wrote "Powershift", also with the help of his wife, Heidi.

In 1996, with American business consultant Tom Johnson, they co-founded Toffler Associates, an advisory firm designed to implement many of the ideas the Tofflers had written on. The firm worked with businesses, NGOs, and governments in the United States, South Korea, Mexico, Brazil, Singapore, Australia, and other countries. During this period in his career, Toffler lectured worldwide, taught at several schools and met world leaders, such as Mikhail Gorbachev, along with key executives and military officials.

Toffler stated many of his ideas during an interview with the Australian Broadcasting Corporation in 1998. "Society needs people who take care of the elderly and who know how to be compassionate and honest," he said. "Society needs people who work in hospitals. Society needs all kinds of skills that are not just cognitive; they're emotional, they're affectional. You can't run the society on data and computers alone."

His opinions about the future of education, many of which were in "Future Shock", have often been quoted. An often misattributed quote, however, is that of psychologist Herbert Gerjuoy: "Tomorrow's illiterate will not be the man who can't read; he will be the man who has not learned how to learn."

Early in his career, after traveling to other countries, he became aware of the new and myriad inputs that visitors received from these other cultures. He explained during an interview that some visitors would become "truly disoriented and upset" by the strange environment, which he described as a reaction to culture shock. From that issue, he foresaw another problem for the future, when a culturally "new environment comes to you ... and comes to you rapidly." That kind of sudden cultural change within one's own country, which he felt many would not understand, would lead to a similar reaction, one of "future shock", which he wrote about in his book by that title. Toffler writes:
In "The Third Wave", Toffler describes three types of societies, based on the concept of "waves"—each wave pushes the older societies and cultures aside. He describes the "First Wave" as the society after agrarian revolution and replaced the first hunter-gatherer cultures. The "Second Wave," he labels society during the Industrial Revolution (ca. late 17th century through the mid-20th century). That period saw the increase of urban industrial populations which had undermined the traditional nuclear family, and initiated a factory-like education system, and the growth of the corporation. Toffler said:

The "Third Wave" was a term he coined to describe the post-industrial society, which began in the late 1950s. His description of this period dovetails with other futurist writers, who also wrote about the Information Age, Space Age, Electronic Era, Global Village, terms which highlighted a scientific-technological revolution. The Tofflers claimed to have predicted a number of geopolitical events, such as the collapse of the Soviet Union, the fall of the Berlin Wall and the future economic growth in the Asia-Pacific region.

Toffler often visited with dignitaries in Asia, including China's Zhao Ziyang, Singapore's Lee Kuan Yew and South Korea's Kim Dae Jung, all of whom were influenced by his views as Asia's emerging markets increased in global significance during the 1980s and 1990s. Although they had originally censored some of his books and ideas, China's government cited him along with Franklin Roosevelt and Bill Gates as being among the Westerners who had most influenced their country. "The Third Wave" along with a video documentary based on it became best-sellers in China and were widely distributed to schools. Toffler's influence on Asian thinkers was summed up in an article in "Daedalus", published by the American Academy of Arts & Sciences:
U.S. House Speaker Newt Gingrich publicly lauded his ideas about the future, and urged members of Congress to read Toffler's book, "Creating a New Civilization" (1995). Others, such as AOL founder Steve Case, cited Toffler's "The Third Wave" as a formative influence on his thinking, which inspired him to write "The Third Wave: An Entrepreneur's Vision of the Future" in 2016. Case said that Toffler was a "real pioneer in helping people, companies and even countries lean into the future."

In 1980 Ted Turner founded CNN, which he said was inspired by Toffler's forecasting the end of the dominance of the three main television networks. Turner's company, Turner Broadcasting, published Toffler's "Creating a New Civilization" in 1995. Shortly after the book was released, the former Soviet president Mikhail Gorbachev hosted the Global Governance Conference in San Francisco with the theme, "Toward a New Civilization", which was attended by dozens of world figures, including the Tofflers, George H. W. Bush, Margaret Thatcher, Carl Sagan, Abba Eban and Turner with his then-wife, actress Jane Fonda.

Mexican billionaire Carlos Slim was influenced by his works, and became a friend of the writer. Global marketer J.D. Power also said he was inspired by Toffler's works.

Since the 1960s, people had tried to make sense out of the effect of new technologies and social change, a problem which made Toffler's writings widely influential beyond the confines of scientific, economic, and public policy. His works and ideas have been subject to various criticisms, usually with the same argumentation used against futurology: that foreseeing the future is nigh impossible.

Techno music pioneer Juan Atkins cites Toffler's phrase "techno rebels" in "The Third Wave" as inspiring him to use the word "techno" to describe the musical style he helped to create

Musician Curtis Mayfield released a disco song called "Future Shock," later covered in an electro version by Herbie Hancock. Science fiction author John Brunner wrote "The Shockwave Rider," from the concept of "future shock."

The nightclub Toffler, in Rotterdam, is named after him.

Accenture, the management consultancy firm, identified Toffler in 2002 as being among the most influential voices in business leaders, along with Bill Gates and Peter Drucker. Toffler has also been described in a "Financial Times" interview as the "world's most famous futurologist". In 2006 the "People's Daily" classed him among the 50 foreigners who shaped modern China, which one U.S. newspaper notes made him a "guru of sorts to world statesmen." Chinese Premier and General Secretary Zhao Ziyang was greatly influenced by Toffler. He convened conferences to discuss "The Third Wave" in the early 1980s, and in 1985 the book was the No. 2 best seller in China.

Author Mark Satin characterizes Toffler as an important early influence on radical centrist political thought.

Newt Gingrich became close to the Tofflers in the 1970s and said "The Third Wave" had immensely influenced his own thinking and was "one of the great seminal works of our time."

Toffler has received several prestigious prizes and awards, including the McKinsey Foundation Book Award for Contributions to Management Literature, Officier de L'Ordre des Arts et Lettres, and appointments, including Fellow of the American Association for the Advancement of Science and the International Institute for Strategic Studies.

In 2006, Alvin and Heidi Toffler were recipients of Brown University's Independent Award.

Toffler was married to Heidi Toffler, also a writer and futurist. They lived in the Bel Air section of Los Angeles, California, and previously lived in Redding, Connecticut.

The couple's only child, Karen Toffler (1954–2000), died at age 46 after more than a decade suffering from Guillain–Barré syndrome.

Alvin Toffler died in his sleep on June 27, 2016, at his home in Los Angeles. No cause of death was given. He is buried at Westwood Memorial Park.

Alvin Toffler co-wrote his books with his wife Heidi.




</doc>
<doc id="931" url="https://en.wikipedia.org/wiki?curid=931" title="The Amazing Spider-Man">
The Amazing Spider-Man

The Amazing Spider-Man is an American comic book series published by Marvel Comics, featuring the fictional superhero Spider-Man as its main protagonist. Being in the mainstream continuity of the franchise, it began publication in 1963 as a monthly periodical and was published continuously, with a brief interruption in 1995, until its relaunch with a new numbering order in 1999. In 2003 the series reverted to the numbering order of the first volume. The title has occasionally been published biweekly, and was published three times a month from 2008 to 2010. A video game based on the comic book series was released in 2000 and a film named after and inspired by the comic book series was released July 3, 2012.

After DC Comics' relaunch of "Action Comics" and "Detective Comics" with new No. 1 issues in 2011, it had been the highest-numbered American comic still in circulation until it was cancelled. The title ended its 50-year run as a continuously published comic with issue#700 in December 2012. It was replaced by "The Superior Spider-Man" as part of the Marvel NOW! relaunch of Marvel's comic lines.

The title was relaunched in April 2014, starting fresh from issue No. 1, after the "Goblin Nation" story arc published in "The Superior Spider-Man" and "Superior Spider-Man Team-Up". In late 2015, "The Amazing Spider-Man" was relaunched again with a new volume with issue No. 1 following the 2015 "Secret Wars" event. The series was relaunched again in 2018. 

The character was created by writer-editor Stan Lee and artist and co-plotter Steve Ditko, and the pair produced 38 issues from March 1963 to July 1966. Ditko left after the 38th issue, while Lee remained as writer until issue 100. Since then, many writers and artists have taken over the monthly comic through the years, chronicling the adventures of Marvel's most identifiable hero.

"The Amazing Spider-Man" has been the character's flagship series for his first fifty years in publication, and was the only monthly series to star Spider-Man until "Peter Parker, The Spectacular Spider-Man", in 1976, although 1972 saw the debut of "Marvel Team-Up", with the vast majority of issues featuring Spider-Man along with a rotating cast of other Marvel characters. Most of the major characters and villains of the Spider-Man saga have been introduced in "Amazing", and with few exceptions, it is where most key events in the character's history have occurred. The title was published continuously until No. 441 (Nov. 1998) when Marvel Comics relaunched it as vol. 2 No. 1 (Jan. 1999), but on Spider-Man's 40th anniversary, this new title reverted to using the numbering of the original series, beginning again with issue No. 500 (Dec. 2003) and lasting until the final issue, No. 700 (Feb. 2013).
Due to strong sales on the character's first appearance in "Amazing Fantasy" No. 15, Spider-Man was given his own ongoing series in March 1963. The initial years of the series, under Lee and Ditko, chronicled Spider-Man's nascent career as a masked super-human vigilante with his civilian life as hard-luck yet perpetually good-humored and well-meaning teenager Peter Parker. Peter balanced his career as Spider-Man with his job as a freelance photographer for "The Daily Bugle" under the bombastic editor-publisher J. Jonah Jameson to support himself and his frail Aunt May. At the same time, Peter dealt with public hostility towards Spider-Man and the antagonism of his classmates Flash Thompson and Liz Allan at Midtown High School, while embarking on a tentative, ill-fated romance with Jameson's secretary, Betty Brant.

By focusing on Parker's everyday problems, Lee and Ditko created a groundbreakingly flawed, self-doubting superhero, and the first major teenaged superhero to be a protagonist and not a sidekick. Ditko's quirky art provided a stark contrast to the more cleanly dynamic stylings of Marvel's most prominent artist, Jack Kirby, and combined with the humor and pathos of Lee's writing to lay the foundation for what became an enduring mythos.

Most of Spider-Man's key villains and supporting characters were introduced during this time. Issue No. 1 (March 1963) featured the first appearances of J. Jonah Jameson and his astronaut son John Jameson, and the supervillain the Chameleon. It included the hero's first encounter with the superhero team the Fantastic Four. Issue No. 2 (May 1963) featured the first appearance of the Vulture and the Tinkerer as well as the beginning of Parker's freelance photography career at the newspaper "The Daily Bugle".

The Lee-Ditko era continued to usher in a significant number of villains and supporting characters, including Doctor Octopus in No. 3 (July 1963); the Sandman and Betty Brant in No. 4 (Sept. 1963); the Lizard in No. 6 (Nov. 1963); Living Brain in (#8, January 1964); Electro in No. 9 (March 1964); Mysterio in No. 13 (June 1964); the Green Goblin in No. 14 (July 1964); Kraven The Hunter in No. 15 (Aug. 1964); reporter Ned Leeds in No. 18 (Nov. 1964); and the Scorpion in No. 20 (Jan. 1965). The Molten Man was introduced in No. 28 (Sept. 1965) which also featured Parker's graduation from high school. Peter began attending Empire State University in No. 31 (Dec. 1965), the issue which featured the first appearances of friends and classmates Gwen Stacy and Harry Osborn. Harry's father, Norman Osborn first appeared in No. 23 (April 1965) as a member of Jameson's country club but is not named nor revealed as Harry's father until No. 37 (June 1966). One of the most celebrated issues of the Lee-Ditko run is No. 33 (Feb. 1966), the third part of the story arc "If This Be My Destiny...!", which features the dramatic scene of Spider-Man, through force of will and thoughts of family, escaping from being pinned by heavy machinery. Comics historian Les Daniels noted that "Steve Ditko squeezes every ounce of anguish out of Spider-Man's predicament, complete with visions of the uncle he failed and the aunt he has sworn to save." Peter David observed that "After his origin, this two-page sequence from "Amazing Spider-Man" No. 33 is perhaps the best-loved sequence from the Stan Lee/Steve Ditko era." Steve Saffel stated the "full page Ditko image from "The Amazing Spider-Man" No. 33 is one of the most powerful ever to appear in the series and influenced writers and artists for many years to come." and Matthew K. Manning wrote that "Ditko's illustrations for the first few pages of this Lee story included what would become one of the most iconic scenes in Spider-Man's history." The story was chosen as No. 15 in the 100 Greatest Marvels of All Time poll of Marvel's readers in 2001. Editor Robert Greenberger wrote in his introduction to the story that "These first five pages are a modern-day equivalent to Shakespeare as Parker's soliloquy sets the stage for his next action. And with dramatic pacing and storytelling, Ditko delivers one of the great sequences in all comics."

Although credited only as artist for most of his run, Ditko would eventually plot the stories as well as draw them, leaving Lee to script the dialogue. A rift between Ditko and Lee developed, and the two men were not on speaking terms long before Ditko completed his last issue, "The Amazing Spider-Man" No. 38 (July 1966). The exact reasons for the Ditko-Lee split have never been fully explained. Spider-Man successor artist John Romita Sr., in a 2010 deposition, recalled that Lee and Ditko "ended up not being able to work together because they disagreed on almost everything, cultural, social, historically, everything, they disagreed on characters..."

In successor penciler Romita Sr.'s first issue, No. 39 (Aug. 1966), nemesis the Green Goblin discovers Spider-Man's secret identity and reveals his own to the captive hero. Romita's Spider-Man – more polished and heroic-looking than Ditko's – became the model for two decades. The Lee-Romita era saw the introduction of such characters as "Daily Bugle" managing editor Robbie Robertson in No. 52 (Sept. 1967) and NYPD Captain George Stacy, father of Parker's girlfriend Gwen Stacy, in No. 56 (Jan. 1968). The most important supporting character to be introduced during the Romita era was Mary Jane Watson, who made her first full appearance in No. 42, (Nov. 1966), although she first appeared in No. 25 (June 1965) with her face obscured and had been mentioned since No. 15 (Aug. 1964). Peter David wrote in 2010 that Romita "made the definitive statement of his arrival by pulling Mary Jane out from behind the oversized potted plant [that blocked the readers' view of her face in issue #25] and placing her on panel in what would instantly become an iconic moment." Romita has stated that in designing Mary Jane, he "used Ann-Margret from the movie "Bye Bye Birdie" as a guide, using her coloring, the shape of her face, her red hair and her form-fitting short skirts."

Lee and Romita toned down the prevalent sense of antagonism in Parker's world by improving Parker's relationship with the supporting characters and having stories focused as much on the social and college lives of the characters as they did on Spider-Man's adventures. The stories became more topical, addressing issues such as civil rights, racism, prisoners' rights, the Vietnam War, and political elections.

Issue No. 50 (June 1967) introduced the highly enduring criminal mastermind the Kingpin, who would become a major force as well in the superhero series "Daredevil". Other notable first appearances in the Lee-Romita era include the Rhino in No. 41 (Oct. 1966), the Shocker in No. 46 (March 1967), the Prowler in No. 78 (Nov. 1969), and the Kingpin's son, Richard Fisk, in No. 83 (April 1970).

Several spin-off series debuted in the 1970s: "Marvel Team-Up" in 1972, and "The Spectacular Spider-Man" in 1976. A short-lived series titled "Giant-Size Spider-Man" began in July 1974 and ran six issues through 1975. "Spidey Super Stories", a series aimed at children ages 6–10, ran for 57 issues from October 1974 through 1982.
The flagship title's second decade took a grim turn with a story in #89-90 (Oct.-Nov. 1970) featuring the death of Captain George Stacy. This was the first Spider-Man story to be penciled by Gil Kane, who would alternate drawing duties with Romita for the next year-and-a-half and would draw several landmark issues.

One such story took place in the controversial issues #96–98 (May–July 1971). Writer-editor Lee defied the Comics Code Authority with this story, in which Parker's friend Harry Osborn, was hospitalized after over-dosing on pills. Lee wrote this story upon a request from the U. S. Department of Health, Education, and Welfare for a story about the dangers of drugs. Citing its dictum against depicting drug use, even in an anti-drug context, the CCA refused to put its seal on these issues. With the approval of Marvel publisher Martin Goodman, Lee had the comics published without the seal. The comics sold well and Marvel won praise for its socially conscious efforts. The CCA subsequently loosened the Code to permit negative depictions of drugs, among other new freedoms.

"The Six Arms Saga" of #100–102 (Sept.–Nov. 1971) introduced Morbius, the Living Vampire. The second installment was the first "Amazing Spider-Man" story not written by co-creator Lee, with Roy Thomas taking over writing the book for several months before Lee returned to write #105–110 (Feb.-July 1972). Lee, who was going on to become Marvel Comics' publisher, with Thomas becoming editor-in-chief, then turned writing duties over to 19-year-old Gerry Conway, who scripted the series through 1975. Romita penciled Conway's first half-dozen issues, which introduced the gangster Hammerhead in No. 113 (Oct. 1972). Kane then succeeded Romita as penciler, although Romita would continue inking Kane for a time.

Issues 121–122 (June–July 1973, by Conway-Kane-Romita), which featured the death of Gwen Stacy at the hands of the Green Goblin in "The Night Gwen Stacy Died" in issue No. 121. Her demise and the Goblin's apparent death one issue later formed a story arc widely considered as the most defining in the history of Spider-Man. The aftermath of the story deepened both the characterization of Mary Jane Watson and her relationship with Parker.

In 1973, Gil Kane was succeeded by Ross Andru, whose run lasted from issue No. 125 (October 1973) to No. 185 (October 1978). Issue#129 (Feb. 1974) introduced the Punisher, who would become one of Marvel Comics' most popular characters. The Conway-Andru era featured the first appearances of the Man-Wolf in #124–125 (Sept.-Oct. 1973); the near-marriage of Doctor Octopus and Aunt May in No. 131 (April 1974); Harry Osborn stepping into his father's role as the Green Goblin in #135–137 (Aug.-Oct.1974); and the original "Clone Saga", containing the introduction of Spider-Man's clone, in #147–149 (Aug.-Oct. 1975).
Archie Goodwin and Gil Kane produced the title's 150th issue (Nov. 1975) before Len Wein became writer with issue No. 151. During Wein's tenure, Harry Osborn and Liz Allen dated and became engaged; J. Jonah Jameson was introduced to his eventual second wife, Marla Madison; and Aunt May suffered a heart attack. Wein's last story on "Amazing" was a five-issue arc in #176–180 (Jan.-May 1978) featuring a third Green Goblin (Harry Osborn's psychiatrist, Bart Hamilton). Marv Wolfman, Marvel's editor-in-chief from 1975 to 1976, succeeded Wein as writer, and in his first issue, No. 182 (July 1978), had Parker propose marriage to Watson who refused, in the following issue. Keith Pollard succeeded Ross Andru as artist shortly afterward, and with Wolfman introduced the likable rogue the Black Cat (Felicia Hardy) in No. 194 (July 1979). As a love interest for Spider-Man, the Black Cat would go on to be an important supporting character for the better part of the next decade, and remain a friend and occasional lover into the 2010s.

"The Amazing Spider-Man" No. 200 (Jan. 1980) featured the return and death of the burglar who killed Spider-Man's Uncle Ben. Writer Marv Wolfman and penciler Keith Pollard both left the title by mid-year, succeeded by Dennis O'Neil, a writer known for groundbreaking 1970s work at rival DC Comics, and penciler John Romita Jr.. O'Neil wrote two issues of "The Amazing Spider-Man Annual" which were both drawn by Frank Miller. The 1980 "Annual" featured a team-up with Doctor Strange while the 1981 "Annual" showcased a meeting with the Punisher. Roger Stern, who had written nearly 20 issues of sister title "The Spectacular Spider-Man", took over "Amazing" with issue No. 224 (January 1982). During his two years on the title, Stern augmented the backgrounds of long-established Spider-Man villains, and with Romita Jr. created the mysterious supervillain the Hobgoblin in #238–239 (March–April 1983). Fans engaged with the mystery of the Hobgoblin's secret identity, which continued throughout #244–245 and 249–251 (Sept.-Oct. 1983 and Feb.-April 1984). One lasting change was the reintroduction of Mary Jane Watson as a more serious, mature woman who becomes Peter's confidante after she reveals that she knows his secret identity. Stern also wrote "The Kid Who Collects Spider-Man" in "The Amazing Spider-Man" No. 248 (January 1984), a story which ranks among his most popular.

By mid-1984, Tom DeFalco and Ron Frenz took over scripting and penciling. DeFalco helped establish Parker and Watson's mature relationship, laying the foundation for the characters' wedding in 1987. Notably, in No. 257 (Oct. 1984), Watson tells Parker that she knows he is Spider-Man, and in No. 259 (Dec. 1984), she reveals to Parker the extent of her troubled childhood. Other notable issues of the DeFalco-Frenz era include No. 252 (May 1984), with the first appearance of Spider-Man's black costume, which the hero would wear almost exclusively for the next four years' worth of comics; the debut of criminal mastermind the Rose, in No. 253 (June 1984); the revelation in No. 258 (Nov. 1984) that the black costume is a living being, a symbiote; and the introduction of the female mercenary Silver Sable in No. 265 (June 1985).

Tom DeFalco and Ron Frenz were both removed from "The Amazing Spider-Man" in 1986 by editor Jim Owsley under acrimonious circumstances. A succession of artists including Alan Kupperberg, John Romita Jr., and Alex Saviuk penciled the series from 1987 to 1988; Owsley wrote the book for the first half of 1987, scripting the five-part "Gang War" story (#284–288) that DeFalco plotted. Former "Spectacular Spider-Man" writer Peter David scripted No. 289 (June 1987), which revealed Ned Leeds as being the Hobgoblin although this was retconned in 1996 by Roger Stern into Leeds not being the original Hobgoblin after all.

David Michelinie took over as writer in the next issue, for a story arc in #290–292 (July–Sept. 1987) that led to the marriage of Peter Parker and Mary Jane Watson in "Amazing Spider-Man Annual" No. 21. The "Kraven's Last Hunt" storyline by writer J.M. DeMatteis and artists Mike Zeck and Bob McLeod crossed over into "The Amazing Spider-Man" No. 293 and 294. Issue No. 298 (March 1988) was the first Spider-Man comic to be drawn by future industry star Todd McFarlane, the first regular artist on "The Amazing Spider-Man" since Frenz's departure. McFarlane revolutionized Spider-Man's look. His depiction – large-eyed, with wiry, contorted limbs, and messy, knotted, convoluted webbing – influenced the way virtually all subsequent artists would draw the character. McFarlane's other significant contribution to the Spider-Man canon was the design for what would become one of Spider-Man's most wildly popular antagonists, the supervillain Venom. Issue No. 299 (April 1988) featured Venom's first appearance (a last-page cameo) before his first full appearance in No. 300 (May 1988). The latter issue featured Spider-Man reverting to his original red-and-blue costume.

Other notable issues of the Michelinie-McFarlane era include No. 312 (Feb. 1989), featuring the Green Goblin vs. the Hobgoblin; and #315–317 (May–July 1989), with the return of Venom. In July 2012, Todd McFarlane's original cover art for "The Amazing Spider-Man" No. 328 sold for a bid of $657,250, making it the most expensive American comic book art ever sold at auction.

With a civilian life as a married man, the Spider-Man of the 1990s was different from the superhero of the previous three decades. McFarlane left the title in 1990 to write and draw a new series titled simply "". His successor, Erik Larsen, penciled the book from early 1990 to mid-1991. After issue No. 350, Larsen was succeeded by Mark Bagley, who had won the 1986 Marvel Tryout Contest and was assigned a number of low-profile penciling jobs followed by a run on "New Warriors" in 1990. Bagley penciled the flagship Spider-Man title from 1991 to 1996. During that time, Bagley's rendition of Spider-Man was used extensively for licensed material and merchandise.

Issues #361–363 (April–June 1992) introduced Carnage, a second symbiote nemesis for Spider-Man. The series' 30th-anniversary issue, No. 365 (Aug. 1992), was a double-sized, hologram-cover issue with the cliffhanger ending of Peter Parker's parents, long thought dead, reappearing alive. It would be close to two years before they were revealed to be impostors, who are killed in No. 388 (April 1994), scripter Michelinie's last issue. His 1987–1994 stint gave him the second-longest run as writer on the title, behind Stan Lee.

Issue No. 375 was released with a gold foil cover. There was an error affecting some issues and which are missing the majority of the foil.

With No. 389, writer J. M. DeMatteis, whose Spider-Man credits included the 1987 "Kraven's Last Hunt" story arc and a 1991–1993 run on "The Spectacular Spider-Man", took over the title. From October 1994 to June 1996, "Amazing" stopped running stories exclusive to it, and ran installments of multi-part stories that crossed over into all the Spider-Man books. One of the few self-contained stories during this period was in No. 400 (April 1995), which featured the death of Aunt May – later revealed to have been faked (although the death still stands in the MC2 continuity). The "Clone Saga" culminated with the revelation that the Spider-Man who had appeared in the previous 20 years of comics was a clone of the real Spider-Man. This plot twist was massively unpopular with many readers, and was later reversed in the "Revelations" story arc that crossed over the Spider-Man books in late 1996.

The Clone Saga tied into a publishing gap after No. 406 (Oct. 1995), when the title was temporarily replaced by "The Amazing Scarlet Spider" #1–2 (Nov.-Dec. 1995), featuring Ben Reilly. The series picked up again with No. 407 (Jan. 1996), with Tom DeFalco returning as writer. Bagley completed his 5½-year run by September 1996. A succession of artists, including Ron Garney, Steve Skroce, Joe Bennett, Rafael Kayanan and John Byrne penciled the book until the final issue, No. 441 (Nov. 1998), after which Marvel rebooted the title with vol. 2, No. 1 (Jan. 1999).

Marvel began "The Amazing Spider-Man" anew with (vol. 2) #1 (Jan. 1999). Howard Mackie wrote the first 29 issues. The relaunch included the Sandman being regressed to his criminal ways and the "death" of Mary Jane, which was ultimately reversed. Other elements included the introduction of a new Spider-Woman (who was spun off into her own short-lived series) and references to John Byrne's miniseries "", which was launched at the same time as the reboot. Byrne also penciled issues #1–18 (from 1999 to 2000) and wrote #13–14, John Romita Jr. took his place soon after in October 2000. Mackie's run ended with "The Amazing Spider-Man Annual 2001", which saw the return of Mary Jane, who then left Parker upon reuniting with him.

With issue #30 (June 2001), J. Michael Straczynski took over as writer and oversaw additional storylines – most notably his lengthy "Spider-Totem" arc, which raised the issue of whether Spider-Man's powers were magic-based, rather than as the result of a radioactive spider's bite. Additionally, Straczynski resurrected the plot point of Aunt May discovering her nephew was Spider-Man, and returned Mary Jane, with the couple reuniting in "The Amazing Spider-Man" (vol. 2) #50. Straczynski gave Spider-Man a new profession, having Parker teach at his former high school.

Issue #30 began a dual numbering system, with the original series numbering (#471) returned and placed alongside the volume two number on the cover. Other longtime, rebooted Marvel Comics titles, including "Fantastic Four", likewise were given the dual numbering around this time. After (vol. 2) #58 (Nov. 2003), the title reverted completely to its original numbering for issue #500 (Dec. 2003). Mike Deodato, Jr. penciled the series from mid-2004 until 2006.

That year Peter Parker revealed his Spider-Man identity on live television in the company-crossover storyline "Civil War", in which the superhero community is split over whether to conform to the federal government's new Superhuman Registration Act. This knowledge was erased from the world with the event of the four-part, crossover story arc, "", written partially by J. Michael Straczynski and illustrated by Joe Quesada, running through "The Amazing Spider-Man" #544–545 (Nov.-Dec. 2007), "Friendly Neighborhood Spider-Man" No. 24 (Nov. 2007) and "The Sensational Spider-Man" No. 41 (Dec. 2007), the final issues of those two titles. Here, the demon Mephisto makes a Faustian bargain with Parker and Mary Jane, offering to save Parker's dying Aunt May if the couple will allow their marriage to have never existed, rewriting that portion of their pasts. This story arc marked the end of Straczynski's work on the title.

Following this, Marvel made "The Amazing Spider-Man" the company's sole Spider-Man title, increasing its frequency of publication to three issues monthly, and inaugurating the series with a sequence of "back to basics" story arcs under the banner of "". Parker now exists in a changed world where he and Mary Jane had never married, and Parker has no memory of being married to her, with domino effect differences in their immediate world. The most notable of these revisions to Spider-Man continuity are the return of Harry Osborn, whose death in "The Spectacular Spider-Man" No. 200 (May 1993) is erased; and the reestablishment of Spider-Man's secret identity, with no one except Mary Jane able to recall that Parker is Spider-Man (although he soon reveals his secret identity to the New Avengers and the Fantastic Four). Under the banner of "Brand New Day", Marvel tried to only use newly created villains instead of relying on older ones. Characters like Mister Negative and Overdrive both in Free Comic Book Day 2007 Spider-Man (July 2007), Menace in No. 549 (March 2008), Ana and Sasha Kravinoff in No. 565 (September 2008) and No. 567 (October 2008) respectively, and several more were introduced. The alternating regular writers were initially Dan Slott, Bob Gale, Marc Guggenheim, Fred Van Lente, and Zeb Wells, joined by a rotation of artists that included Chris Bachalo, Phil Jimenez, Mike McKone, John Romita Jr. and Marcos Martín. Joe Kelly, Mark Waid and Roger Stern later joined the writing team and Barry Kitson the artist roster. Waid's work on the series included a meeting between Spider-Man and Stephen Colbert in "The Amazing Spider-Man" No. 573 (Dec. 2008).
Issue No. 583 (March 2009) included a back-up story in which Spider-Man meets President Barack Obama.

Mark Waid scripted the opening of "The Gauntlet" storyline in issue No. 612 (Jan. 2010). The "Gauntlet" story was concluded by "Grim Hunt" (No. 634-637) which saw the resurrection of long-dead Spider-Man villain, Kraven the Hunter. The series became a twice-monthly title with Dan Slott as sole writer at issue No. 648 (Jan. 2011), launching the "" storyline. Eight additional pages were added per issue. "Big Time" saw major changes in Spider-Man/Peter Parker's life, Peter would start working at Horizon Labs and begin a relationship with Carlie Cooper (his first serious relationship since his marriage to Mary Jane), Mac Gargan returned as Scorpion after spending the past few years as Venom, Phil Urich would take up the mantle of Hobgoblin, and the death of J. Jonah Jameson's wife, Marla Jameson. Issues 654 and 654.1 saw the birth of Agent Venom, Flash Thompson bonded with the Venom symbiote, which would lead to Venom getting his own series "Venom (volume 2)". Starting in No. 659 and going to No. 655, the series built-up to the "Spider-Island" event which officially started in No. 666 and ended in No. 673. "Ends of the Earth" was the next event that ran from No. 682 through No. 687. This publishing format lasted until issue No. 700, which concluded the "Dying Wish" storyline, in which Parker and Doctor Octopus swapped bodies, and the latter taking on the mantle of Spider-Man when Parker apparently died in Doctor Octopus' body. "The Amazing Spider-Man" ended with this issue, with the story continuing in the new series "The Superior Spider-Man". Despite "The Superior Spider-Man" being considered a different series to "The Amazing Spider-Man", the first 33 issue run goes towards the legacy numbering of "The Amazing Spider-Man" acting as issues 701-733. In December 2013, the series returned for five issues, numbered 700.1 through 700.5, with the first two written by David Morrell and drawn by Klaus Janson.

In January 2014, Marvel confirmed that "The Amazing Spider-Man" would be relaunched on April 30, 2014, starting from issue No. 1, with Peter Parker as Spider-Man once again.
The first issue of this new version of "The Amazing Spider-Man" was, according to Diamond Comics Distributors, the "best-selling comic book... in over a decade."
Issues #1–6 were a story arc called "Lucky to be Alive", taking place immediately after "Goblin Nation", with issues No. 4 and No. 5 being a crossover with the "Original Sin" storyline. Issue No. 4 introduced Silk, a new heroine who was bitten by the same spider as Peter Parker. Issues #7–8 featured a team-up between Ms. Marvel and Spider-Man, and had backup stories that tied into "Edge of Spider-Verse". The next major plot arc, titled "Spider-Verse", began in Issue No. 9 and ended in No. 15, features every Spider-Man from across the dimensions being hunted by Morlun, and a team-up to stop him, with Peter Parker of Earth-616 in command of the Spider-Men's Alliance. "The Amazing Spider-Man Annual" No. 1 of the relaunched series was released in December 2014, featuring stories unrelated to "Spider-Verse".

In 2015, Marvel started the universe wide Secret Wars event where the core and several other Marvel universes were combined into one big planet called Battleworld. Battleworld was divided into sections with most of them being self-contained universes. Marvel announced that several of these self-contained universes would get their own tie in series and one of them was "", an alternate universe where Peter Parker and Mary Jane are still married and give birth to their child Annie May Parker, written by Dan Slott. Despite the series being considered separate from the main "Amazing Spider-Man" series, the original 5 issue run is counted towards its legacy numbering acting as No. 752-756.

Following the 2015 "Secret Wars" event, a number of Spider-Man-related titles were either relaunched or created as part of the "All-New, All-Different Marvel" event. Among them, "The Amazing Spider-Man" was relaunched as well and primarily focuses on Peter Parker continuing to run Parker Industries, and becoming a successful businessman who is operating worldwide. It also tied with "Civil War II" (involving an Inhuman who can predict possible future named Ulysses Cain), "Dead No More" (where Ben Reilly [the original Scarlet Spider] revealed to be revived and as one of the antagonists instead), and "Secret Empire" (during Hydra's reign led by a Hydra influenced Captain America/Steve Rogers, and the dismissal of Parker Industries by Peter Parker to stop Otto Octavius). Starting in September 2017, Marvel started the Marvel Legacy event which renumbered several Marvel series to their original numbering, "The Amazing Spider-Man" was put back to its original numbering for issue 789. Issues 789 through 791 focused on the aftermath of Peter destroying Parker Industries and his fall from grace. Issues 792 and 793 were part of the "Venom Inc." story. "Threat Level: Red" was the story for the next three issues which saw Norman Osborn obtain and bond with the Carnage symbiote. "Go Down Swinging" saw the results of the combination of Osborn's goblin serum and Carnage symbiote creating the Red Goblin. Issue 801 was Dan Slott's goodbye issue.

In March 2018, it was announced that writer Nick Spencer would be writing the main bi-monthly "The Amazing Spider-Man" series beginning with a new No. 1, replacing long-time writer Dan Slott, as part of the Fresh Start relaunch that July.
The first five-issue story arc was titled 'Back to Basics.' During the "Back to Basics" story, Kindred, a mysterious villain with some relation to Peter's past, was revealed. The first major story under Spencer was "Hunted" which ran through issues 16 through 23, the story also included four ".HU" issues for issues 16, 18, 19, and 20. The end of the story saw the death of long-running Spider-Man villain Kraven the Hunter. Issues 30 and 31 were tie-ins to the "Absolute Carnage" event. The main bulk of Marvel's "2099 event" took place from issues 32 through 36. Issues 45 through 49 (which will be issue 850 for the series) will all focus on the "Sins Rising" story.




</doc>
<doc id="933" url="https://en.wikipedia.org/wiki?curid=933" title="AM">
AM

AM may refer to:













</doc>
<doc id="951" url="https://en.wikipedia.org/wiki?curid=951" title="Antigua and Barbuda">
Antigua and Barbuda

Antigua and Barbuda (; ) is an island sovereign state in the West Indies in the Americas, lying between the Caribbean Sea and the Atlantic Ocean. It consists of two major islands, Antigua and Barbuda (separated by 39 miles), and a number of smaller islands (including Great Bird, Green, Guiana, Long, Maiden, Prickly Pear, York Islands and further south, the island of Redonda). The permanent population numbers about 95,900 (2018 est.), with 97% being resident on Antigua. The capital and largest port and city is St. John's on Antigua, with Codrington being the largest town on Barbuda. Lying near each other (the main Barbuda airport is less than 0.5° of latitude, or , north of the main Antigua airport), Antigua and Barbuda are in the middle of the Leeward Islands, part of the Lesser Antilles, roughly at 17°N of the equator.

The island of Antigua was explored by Christopher Columbus in 1493 and named for the Church of Santa María La Antigua. Antigua was colonized by Britain in 1632; Barbuda island was first colonised in 1678. Antigua and Barbuda joined the West Indies Federation in 1958. With the breakup of the federation, it became one of the West Indies Associated States in 1967. Following by self-governing on its internal affairs, independence was granted from United Kingdom on 1 November 1981.

Antigua and Barbuda is a member of the Commonwealth and Elizabeth II is the country's queen and head of state.

In September 2017, Hurricane Irma damaged or destroyed 95% of Barbuda's buildings and infrastructure and as a result, all the island's inhabitants were evacuated to Antigua, leaving Barbuda empty for the first time in modern history.

' is Spanish for "ancient" and ' is Spanish for "bearded". The island of Antigua was originally called ' by Arawaks and is locally known by that name today; Caribs possibly called Barbuda '. Christopher Columbus, while sailing by in 1493 may have named it Santa Maria la Antigua, after an icon in the Spanish Seville Cathedral. The "bearded" of Barbuda is thought to refer either to the male inhabitants of the island, or the bearded fig trees present there.

Antigua was first settled by archaic age hunter-gatherer Amerindians called the Ciboney. Carbon dating has established the earliest settlements started around 3100 BC. They were succeeded by the ceramic age pre-Columbian Arawak-speaking Saladoid people who migrated from the lower Orinoco River. They introduced agriculture, raising, among other crops, the famous Antigua black pineapple ("Ananas comosus"), corn, sweet potatoes, chiles, guava, tobacco, and cotton. Later on the more bellicose Caribs also settled the island, possibly by force.

Christopher Columbus was the first European to sight the islands in 1493. The Spanish did not colonise Antigua until after a combination of European and African diseases, malnutrition, and slavery eventually extirpated most of the native population; smallpox was probably the greatest killer.

The English settled on Antigua in 1632; Christopher Codrington settled on Barbuda in 1685. Tobacco and then sugar was grown, worked by a large population of slaves from West Africa who soon came to vastly outnumber the European settlers.

The English maintained control of the islands, repulsing an attempted French attack in 1666. The brutal conditions endured by the slaves led to revolts in 1701 and 1729 and a planned revolt in 1736, the latter led by Prince Klaas, though it was discovered before it began and the ringleaders were executed. Slavery was abolished in the British Empire in 1834, affecting the economy. This was exacerbated by natural disasters such as the 1843 earthquake and the 1847 hurricane. Mining occurred on the isle of Redonda, however this ceased in 1929 and the island has since remained uninhabited.

Part of the Leeward Islands colony, Antigua and Barbuda became part of the short-lived West Indies Federation from 1958 to 1962. Antigua and Barbuda subsequently became an associated state of the United Kingdom with full internal autonomy on 27 February 1967. The 1970s were dominated by discussions as to the islands' future and the rivalry between Vere Bird of the Antigua Labour Party (ALP) (Premier from 1967 to 1971 and 1976 to 1981) and the Progressive Labour Movement (PLM) of George Walter (Premier 1971–1976). Eventually Antigua and Barbuda gained full independence on 1 November 1981; Vere Bird became Prime Minister of the new country. The country opted to remain within the Commonwealth, retaining Queen Elizabeth as head of state, with the last Governor, Sir Wilfred Jacobs, as Governor-General.

The first two decades of Antigua's independence were dominated politically by the Bird family and the ALP, with Vere Bird ruling from 1981 to 1994, followed by his son Lester Bird from 1994 to 2004. Though providing a degree of political stability, and boosting tourism to the country, the Bird governments were frequently accused of corruption, cronyism and financial malfeasance. Vere Bird Jr., the elder son, was forced to leave the cabinet in 1990 following a scandal in which he was accused of smuggling Israeli weapons to Colombian drug-traffickers. Another son, Ivor Bird, was convicted of selling cocaine in 1995.

In 1995 Hurricane Luis caused severe damage on Barbuda.

The ALP's dominance of Antiguan politics ended with the 2004 Antiguan general election, which was won by Baldwin Spencer's United Progressive Party (UPP). However the UPP lost the 2014 Antiguan general election, with the ALP returning to power under Gaston Browne.

Most of Barbuda was devastated in early September 2017 by Hurricane Irma, which brought winds with speeds reaching 295 km/h (185 mph). The storm damaged or destroyed 95% of the island's buildings and infrastructure, leaving Barbuda "barely habitable" according to Prime Minister Gaston Browne. Nearly everyone on the island was evacuated to Antigua.
Amidst the following rebuilding efforts on Barbuda that were estimated to cost at least $100 million, the government announced plans to revoke a century old law of communal land ownership by allowing residents to buy land; a move that has been criticised as promoting "disaster capitalism".

Antigua and Barbuda both are generally low-lying islands whose terrain has been influenced more by limestone formations than volcanic activity. The highest point on Antigua is Boggy Peak, the remnant of a volcanic crater rising .

The shorelines of both islands are greatly indented with beaches, lagoons, and natural harbors. The islands are rimmed by reefs and shoals. There are few streams as rainfall is slight. Both islands lack adequate amounts of fresh groundwater.

About south-west of Antigua lies the small, rocky island of Redonda, which is uninhabited.

Rainfall averages per year, with the amount varying widely from season to season. In general the wettest period is between September and November. The islands generally experience low humidity and recurrent droughts. Temperatures average , with a range from to in the winter to from to in the summer and autumn. The coolest period is between December and February.

Hurricanes strike on an average of once a year, including the powerful Category 5 Hurricane Irma, on 6 September 2017, which damaged 95% of the structures on Barbuda. Some 1,800 people were evacuated to Antigua.

An estimate published by "Time" indicated that over $100 million would be required to rebuild homes and infrastructure. Philmore Mullin, Director of Barbuda's National Office of Disaster Services, said that "all critical infrastructure and utilities are non-existent – food supply, medicine, shelter, electricity, water, communications, waste management". He summarised the situation as follows: "Public utilities need to be rebuilt in their entirety... It is optimistic to think anything can be rebuilt in six months ... In my 25 years in disaster management, I have never seen something like this."

Antigua has a population of , mostly made up of people of West African, British, and Madeiran descent. The ethnic distribution consists of 91% Black & Mulatto, 4.4% mixed race, 1.7% White, and 2.9% other (primarily East Indian and other Asian). Most Whites are of British descent. Christian Levantine Arabs, and a small number of Asians and Sephardic Jews make up the remainder of the population.

An increasingly large percentage of the population lives abroad, most notably in the United Kingdom (Antiguan Britons), United States and Canada. A minority of Antiguan residents are immigrants from other countries, particularly from Dominica, Guyana and Jamaica, and, increasingly, from the Dominican Republic, St. Vincent and the Grenadines and Nigeria. An estimated 4,500 American citizens also make their home in Antigua and Barbuda, making their numbers one of the largest American populations in the English-speaking Eastern Caribbean.

English is the official language. The Barbudan accent is slightly different from the Antiguan.

In the years before Antigua and Barbuda's independence, Standard English was widely spoken in preference to Antiguan Creole. Generally, the upper and middle classes shun Antiguan Creole. The educational system dissuades the use of Antiguan Creole and instruction is done in Standard (British) English.

Many of the words used in the Antiguan dialect are derived from British as well as African languages. This can be easily seen in phrases such as: "Ent it?" meaning "Ain't it?" which is itself dialectal and means "Isn't it?". Common island proverbs can often be traced to Africa.

Spanish is spoken by around 10,000 inhabitants.

A majority (77%) of Antiguans are Christians, with the Anglicans (17.6%) being the largest single denomination. Other Christian denominations present are Seventh-day Adventist Church (12.4%), Pentecostalism (12.2%), Moravian Church (8.3%), Roman Catholics
(8.2%), Methodist Church (5.6%), Wesleyan Holiness Church (4.5%), Church of God (4.1%), Baptists (3.6%), Mormonism (<1.0%), as well as Jehovah's Witnesses.

Non-Christian religions practiced in the islands include the Rastafari, Islam, and Bahá'í Faith.

The politics of Antigua and Barbuda take place within a framework of a unitary, parliamentary, representative democratic monarchy, in which the head of State is the monarch who appoints the Governor General as vice-regal representative. Elizabeth II is the present Queen of Antigua and Barbuda, having served in that position since the islands' independence from the United Kingdom in 1981. The Queen is currently represented by Governor General Sir Rodney Williams. A council of ministers is appointed by the governor general on the advice of the prime minister, currently Gaston Browne (2014–). The prime minister is the head of government.

Executive power is exercised by the government while legislative power is vested in both the government and the two Chambers of Parliament. The bicameral Parliament consists of the Senate (17 members appointed by members of the government and the opposition party, and approved by the Governor-General), and the House of Representatives (17 members elected by first past the post) to serve five-year terms.

The current Leader of Her Majesty's Loyal Opposition is the United Progressive Party Member of Parliament (MP), the Honourable Baldwin Spencer.

The last elections held were on 12 June 2014, during which the Antigua Labour Party won 14 seats, and the United Progressive Party 3 seats.

Since 1949, elections have been won by the populist Antigua Labour Party. However, in the Antigua and Barbuda legislative election of 2004 saw the defeat of the longest-serving elected government in the Caribbean. Prime Minister Lester Bryant Bird, who had succeeded his father Vere Cornwall Bird Sr., and Deputy Robin Yearwood had been in office since 1976.

The elder Bird was Prime Minister from 1981 to 1994 and Chief Minister of Antigua from 1960 to 1981, except for the 1971–1976 period when the Progressive Labour Movement (PLM) defeated his party. Vere Cornwall Bird, the nation's first Prime Minister, is credited with having brought Antigua and Barbuda and the Caribbean into a new era of independence.

Gaston Browne defeated his predecessor Lester Bryant Bird at the Antigua Labour Party's biennial convention in November 2012 held to elect a political leader and other officers. The party then altered its name from the Antigua Labour Party (ALP) to the Antigua and Barbuda Labour Party (ABLP). This was done to officially include the party's presence on the sister island of Barbuda in its organisation, the only political party on the mainland to have a physical branch in Barbuda.

The Judicial branch is the Eastern Caribbean Supreme Court (based in Saint Lucia; one judge of the Supreme Court is a resident of the islands and presides over the High Court of Justice). Antigua is also a member of the Caribbean Court of Justice. The Judicial Committee of the Privy Council serves as its Supreme Court of Appeal.

Antigua and Barbuda is a member of the United Nations, the Bolivarian Alliance for the Americas, the Commonwealth of Nations, the Caribbean Community, the Organization of Eastern Caribbean States, the Organization of American States, the World Trade Organization and the Eastern Caribbean's Regional Security System.

Antigua and Barbuda is also a member of the International Criminal Court (with a Bilateral Immunity Agreement of Protection for the US military as covered under Article 98 of the Rome Statute).

In 2013, Antigua and Barbuda called for reparations for slavery at the United Nations. Prime Minister Baldwin Spencer said "We have recently seen a number of leaders apologising", and that they should now "match their words with concrete and material benefits."

The Royal Antigua and Barbuda Defence Force has around 260 members dispersed between the line infantry regiment, service and support unit and coast guard. There is also the Antigua and Barbuda Cadet Corps made up of 200 teenagers between the ages of 12 to 18.

In 2018, Antigua and Barbuda signed the UN treaty on the Prohibition of Nuclear Weapons.

Antigua and Barbuda is divided into six parishes and two dependencies:

Note: Though Barbuda and Redonda are called dependencies they are integral parts of the state, making them essentially administrative divisions. Dependency is simply a title.

Although it has not been enforced or a case brought to trial in many years, like other Caribbean islands, Same-sex sexual activity is illegal in Antigua and Barbuda and punishable by prison time. There are several current movements under way to repeal the buggery laws.

Tourism dominates the economy, accounting for more than half of the gross domestic product (GDP). Antigua is famous for its many luxury resorts as an ultra-high end travel destination. Weakened tourist activity in the lower and middle market segments since early 2000 has slowed the economy, however, and squeezed the government into a tight fiscal corner.

Investment banking and financial services also make up an important part of the economy. Major world banks with offices in Antigua include the Royal Bank of Canada (RBC) and Scotiabank. Financial-services corporations with offices in Antigua include PriceWaterhouseCoopers. The US Securities and Exchange Commission has accused the Antigua-based Stanford International Bank, owned by Texas billionaire Allen Stanford, of orchestrating a huge fraud which may have bilked investors of some $8 billion.

The twin-island nation's agricultural production is focused on its domestic market and constrained by a limited water supply and a labour shortage stemming from the lure of higher wages in tourism and construction work.

Manufacturing is made up of enclave-type assembly for export, the major products being bedding, handicrafts and electronic components. Prospects for economic growth in the medium term will continue to depend on income growth in the industrialised world, especially in the United States, from which about one-third of all tourists come.

Access to biocapacity is lower than world average. In 2016, Antigua and Barbuda had 0.8 global hectares of biocapacity per person within its territory, much less than the world average of 1.6 global hectares per person. In 2016 Antigua and Barbuda used 4.3 global hectares of biocapacity per person - their ecological footprint of consumption. This means they use more biocapacity than Antigua and Barbuda contains. As a result, Antigua and Barbuda are running a biocapacity deficit.

Following the opening of the American University of Antigua College of Medicine by investor and attorney Neil Simon in 2003, a new source of revenue was established. The university employs many local Antiguans and the approximate 1000 students consume a large amount of the goods and services.

Antigua and Barbuda also uses an economic citizenship program to spur investment into the country.

Antigua and Barbuda has a greater than 90% literacy rate. In 1998, Antigua and Barbuda adopted a national mandate to become the pre-eminent provider of medical services in the Caribbean. As part of this mission, Antigua and Barbuda built the most technologically advanced hospital in the Caribbean, the Mt. St. John Medical Centre. The island of Antigua currently has three foreign-owned for-profit offshore medical school. The island's medical schools cater mostly to foreign students but contribute to the local economy and health care. The three schools are:

There is also the government-run Antigua State College as well as the Antigua and Barbuda Institute of Information Technology (ABIIT) and the Antigua and Barbuda Hospitality Training Institute (ABHTI). In 2019, the University of the West Indies opened its fifth campus overall (and fourth physical campus) in Five Islands. The country was previously served solely by the University of the West Indies Open Campus.

Island Academy International, which offers the International Baccalaureate, is the only international school in the country. There are also many other private schools but these institutions tend to follow the same local curriculum (CXCs) as government schools.

The culture is predominantly a mixture of West African and British cultural influences.

Cricket is the national sport. Other popular sports include football, boat racing and surfing. (Antigua Sailing Week attracts locals and visitors from all over the world).

Calypso and soca music, both originating primarily out of Trinidad, are important in Antigua and Barbuda.

The national Carnival held each August commemorates the abolition of slavery in the British West Indies, although on some islands, Carnival may celebrate the coming of Lent. Its festive pageants, shows, contests and other activities are a major tourist attraction.

Corn and sweet potatoes play an important role in Antiguan cuisine. For example, a popular Antiguan dish, Dukuna is a sweet, steamed dumpling made from grated sweet potatoes, flour and spices. One of the Antiguan staple foods, fungi , is a cooked paste made of cornmeal and water.

There are three newspapers: the "Antigua Daily Observer, Antigua New Room and The Antiguan Times." The Antigua Observer is the only daily printed newspaper.

The local television channel ABS TV 10 is available (it is the only station that shows exclusively local programs). There are also several local and regional radio stations, such as V2C-AM 620, ZDK-AM 1100, VYBZ-FM 92.9, ZDK-FM 97.1, Observer Radio 91.1 FM, DNECA Radio 90.1 FM, Second Advent Radio 101.5 FM, Abundant Life Radio 103.9 FM, Crusader Radio 107.3 FM, Nice FM 104.3.

Antiguan author Jamaica Kincaid has published over 20 works of literature.

The Antigua and Barbuda national cricket team represented the country at the 1998 Commonwealth Games, but Antiguan cricketers otherwise play for the Leeward Islands cricket team in domestic matches and the West Indies cricket team internationally. The 2007 Cricket World Cup was hosted in the West Indies from 11 March to 28 April 2007.

Antigua hosted eight matches at the Sir Vivian Richards Stadium, which was completed on 11 February 2007 and can hold up to 20,000 people.
Antigua is a Host of Stanford Twenty20 – Twenty20 Cricket, a version started by Allen Stanford in 2006 as a regional cricket game with almost all Caribbean islands taking part. 
Rugby and netball are popular as well.

Association football, or soccer, is also a very popular sport. Antigua has a national football team which entered World Cup qualification for the 1974 tournament and for 1986 and onwards. A professional team was formed in 2011, Antigua Barracuda FC, which played in the USL Pro, a lower professional league in the USA. The nation's team had a major achievement in 2012, getting out of its preliminary group for the 2014 World Cup, notably due to a victory over powerful Haiti. In its first game in the next CONCACAF group play on 8 June 2012 in Tampa, FL, Antigua and Barbuda, comprising 17 Barracuda players and 7 from the lower English professional leagues, scored a goal against the United States. However, the team lost 3:1 to the US.


The national bird is the frigate bird, and the national tree is the Bucida buceras (Whitewood tree).

Clare Waight Keller included agave karatto to represent Antigua and Barbuda in Meghan Markle's wedding veil, which included the distinctive flora of each Commonwealth country.




</doc>
<doc id="953" url="https://en.wikipedia.org/wiki?curid=953" title="Azincourt">
Azincourt

Azincourt (; historically, Agincourt in English) is a commune in the Pas-de-Calais department in northern France.

The Battle of Agincourt (1415) took place nearby.

Situated north-west of Saint-Pol-sur-Ternoise on the D71 road between Hesdin and Fruges

The toponym is attested as "Aisincurt" in 1175, derived from a Germanic masculine name Aizo, Aizino and the early Northern French word "curt" 'farm with a courtyard' (Late Latin "cortem"). It has no etymological connection in French with Agincourt, Meurthe-et-Moselle (attested as "Egincourt" 875), which is derived from another Germanic male name "*Ingin-".
The battle was named after a nearby castle called Azincourt. The modern settlement has in turn been named after the battle in the 17th century.

Azincourt is famous as being near the site of the battle fought on 25 October 1415 in which the army led by King Henry V of England defeated the forces led by Charles d'Albret on behalf of Charles VI of France, which has gone down in history as the Battle of Agincourt. According to M. Forrest, the French knights were so encumbered by their armour that they were exhausted even before the start of the battle.

Later on, when he became king in 1509, Henry VIII is supposed to have commissioned an English translation of a Life of Henry V so that he could emulate him, on the grounds that he thought that launching a campaign against France would help him to impose himself on the European stage. In 1513, Henry VIII crossed the English Channel, stopping by at Azincourt.

The battle, as was the tradition, was named after a nearby castle called Azincourt. The castle has since disappeared and the settlement now known as Azincourt adopted the name in the seventeenth century.

John Cassell wrote in 1857 that "the village of Azincourt itself is now a group of dirty farmhouses and wretched cottages, but where the hottest of the battle raged, between that village and the commune of Tramecourt, there still remains a wood precisely corresponding with the one in which Henry placed his ambush; and there are yet existing the foundations of the castle of Azincourt, from which the king named the field."

The original battlefield museum in the village featured model knights made out of Action Man figures. This has now been replaced by the Centre historique médiéval d'Azincourt (CHM)a more professional museum, conference centre and exhibition space incorporating laser, video, slide shows, audio commentaries, and some interactive elements. The museum building is shaped like a longbow similar to those used at the battle by archers under King Henry.

Since 2004 a large medieval festival organised by the local community, the CHM, The Azincourt Alliance, and various other UK societies commemorating the battle, local history and medieval life, arts and crafts has been held in the village. Prior to this date the festival was held in October, but due to the inclement weather and local heavy clay soil (like the battle) making the festival difficult, it was moved to the last Sunday in July. 
Azincourt is twinned with:





</doc>
<doc id="954" url="https://en.wikipedia.org/wiki?curid=954" title="Albert Speer">
Albert Speer

Albert Speer (; ; March 19, 1905 – September 1, 1981) served as the Minister of Armaments and War Production in Nazi Germany during most of World War II. A close ally of Adolf Hitler, he was convicted at the Nuremberg trials and sentenced to 20 years in prison.

An architect by training, Speer joined the Nazi Party in 1931. His architectural skills made him increasingly prominent within the Party, and he became a member of Hitler's inner circle. Hitler instructed him to design and construct structures including the Reich Chancellery and the Nazi party rally grounds in Nuremberg. In 1937, Hitler appointed Speer as General Building Inspector for Berlin. In this capacity he was responsible for the Central Department for Resettlement that evicted Jewish tenants from their homes in Berlin. In February 1942, Speer was appointed as Reich Minister of Armaments and War Production. Using doctored statistics, he promoted himself as having performed an "armaments miracle" that was widely credited with keeping Germany in the war. In 1944, Speer established a task force to increase production of fighter aircraft. It became instrumental in the exploitation of slave labor for the benefit of the German war effort.

After the war, Speer was among the 24 "major war criminals" arrested and charged with the crimes of the Nazi regime at the Nuremberg trials. He was found guilty of war crimes and crimes against humanity, principally for the use of slave labor, narrowly avoiding a death sentence. Having served his full term, Speer was released in 1966. He used his writings from the time of imprisonment as the basis for two autobiographical books, "Inside the Third Reich" and "". Speer's books were a success; the public was fascinated by an inside view of the Third Reich. Speer died of a stroke in 1981. Little remains of his personal architectural work.

Through his autobiographies and interviews, Speer carefully constructed an image of himself as a man who deeply regretted having failed to discover the monstrous crimes of the Third Reich. He continued to deny explicit knowledge of, and responsibility for, the Holocaust. This image dominated his historiography in the decades following the war, giving rise to the "Speer Myth": the perception of him as an apolitical technocrat responsible for revolutionizing the German war machine. The myth began to fall apart in the 1980s, when the armaments miracle was attributed to Nazi propaganda. Adam Tooze wrote in "The Wages of Destruction" that the idea that Speer was an apolitical technocrat was "absurd". Martin Kitchen, writing in "", stated that much of the increase in Germany's arms production was actually due to systems instituted by Speer's predecessor (Fritz Todt) and furthermore that Speer was intimately involved in the "Final Solution".

Speer was born in Mannheim, into an upper-middle-class family. He was the second of three sons of Luise Máthilde Wilhelmine (Hommel) and Albert Friedrich Speer. In 1918, the family leased their Mannheim residence and moved to a home they had in Heidelberg. Henry T. King, deputy prosecutor at the Nuremberg trials who later wrote a book about Speer said, "Love and warmth were lacking in the household of Speer's youth." His brothers, Ernst and Hermann, bullied him throughout his childhood. Speer was active in sports, taking up skiing and mountaineering. He followed in the footsteps of his father and grandfather and studied architecture.

Speer began his architectural studies at the University of Karlsruhe instead of a more highly acclaimed institution because the hyperinflation crisis of 1923 limited his parents' income. In 1924 when the crisis had abated, he transferred to the "much more reputable" Technical University of Munich. In 1925 he transferred again, this time to the Technical University of Berlin where he studied under Heinrich Tessenow, whom Speer greatly admired. After passing his exams in 1927, Speer became Tessenow's assistant, a high honor for a man of 22. As such, Speer taught some of his classes while continuing his own postgraduate studies. In Munich Speer began a close friendship, ultimately spanning over 50 years, with Rudolf Wolters, who also studied under Tessenow.

In mid-1922, Speer began courting Margarete (Margret) Weber (1905–1987), the daughter of a successful craftsman who employed 50 workers. The relationship was frowned upon by Speer's class-conscious mother, who felt the Webers were socially inferior. Despite this opposition, the two married in Berlin on August 28, 1928; seven years elapsed before Margarete was invited to stay at her in-laws' home. The couple would have six children together, but Albert Speer grew increasingly distant from his family after 1933. He remained so even after his release from imprisonment in 1966, despite their efforts to forge closer bonds.
In January 1931, Speer applied for Nazi Party membership, and on March 1, 1931, he became member number 474,481. The same year, with stipends shrinking amid the Depression, Speer surrendered his position as Tessenow's assistant and moved to Mannheim, hoping to make a living as an architect. After he failed to do so, his father gave him a part-time job as manager of his properties. In July 1932, the Speers visited Berlin to help out the Party before the "Reichstag" elections. While they were there his friend, Nazi Party official Karl Hanke recommended the young architect to Joseph Goebbels to help renovate the Party's Berlin headquarters. When the commission was completed, Speer returned to Mannheim and remained there as Hitler took office in January 1933.

The organizers of the 1933 Nuremberg Rally asked Speer to submit designs for the rally, bringing him into contact with Hitler for the first time. Neither the organizers nor Rudolf Hess were willing to decide whether to approve the plans, and Hess sent Speer to Hitler's Munich apartment to seek his approval. This work won Speer his first national post, as Nazi Party "Commissioner for the Artistic and Technical Presentation of Party Rallies and Demonstrations".

Shortly after Hitler came into power, he began to make plans to rebuild the chancellery. At the end of 1933, he contracted Paul Troost to renovate the entire building. Hitler appointed Speer, whose work for Goebbels had impressed him, to manage the building site for Troost. As Chancellor, Hitler had a residence in the building and came by every day to be briefed by Speer and the building supervisor on the progress of the renovations. After one of these briefings, Hitler invited Speer to lunch, to the architect's great excitement. Speer quickly became part of Hitler's inner circle; he was expected to call on him in the morning for a walk or chat, to provide consultation on architectural matters, and to discuss Hitler's ideas. Most days he was invited to dinner.

In the English version of his memoirs, Speer says that his political commitment merely consisted of paying his "monthly dues". He assumed his German readers would not be so gullible and told them the Nazi Party offered a "new mission." He was more forthright in an interview with William Hamsher in which he said he joined the party in order to save "Germany from Communism." After the war, he claimed to have had little interest in politics at all and had joined almost by chance. Like many of those in power in the Third Reich, he was not an ideologue, "nor was he anything more than an instinctive anti-Semite." The historian Magnus Brechtken, discussing Speer, said he did not give anti-Jewish public speeches and that his anti-Semitism can best be understood through his actions—which were anti-Semitic. Brechtken added that, throughout Speer's life, his central motives were to gain power, rule, and acquire wealth.

When Troost died on January 21, 1934, Speer effectively replaced him as the Party's chief architect. Hitler appointed Speer as head of the Chief Office for Construction, which placed him nominally on Hess's staff.

One of Speer's first commissions after Troost's death was the "Zeppelinfeld" stadium in Nuremberg. It was used for Nazi propaganda rallies and can be seen in Leni Riefenstahl's propaganda film "Triumph of the Will". The building was able to hold 340,000 people. Speer insisted that as many events as possible be held at night, both to give greater prominence to his lighting effects and to hide the overweight Nazis. Nuremberg was the site of many official Nazi buildings. Many more buildings were planned. If built, the German Stadium would have accommodated 400,000 spectators. Speer modified Werner March's design for the being built for the 1936 Summer Olympics. He added a stone exterior that pleased Hitler. Speer designed the German Pavilion for the 1937 international exposition in Paris.

In 1937, Hitler appointed Speer as General Building Inspector for the Reich Capital. This carried with it the rank of undersecretary of state in the Reich government and gave him extraordinary powers over the Berlin city government. It also made Speer a member of the "Reichstag", though the body by then had little effective power. Hitler ordered Speer to develop plans to rebuild Berlin. These centered on a three-mile-long grand boulevard running from north to south, which Speer called the "Prachtstrasse", or Street of Magnificence; he also referred to it as the "North-South Axis". At the northern end of the boulevard, Speer planned to build the "Volkshalle", a huge domed assembly hall over high, with floor space for 180,000 people. At the southern end of the avenue, a great triumphal arch, almost high and able to fit the Arc de Triomphe inside its opening, was planned. The existing Berlin railroad termini were to be dismantled, and two large new stations built. Speer hired Wolters as part of his design team, with special responsibility for the "Prachtstrasse". The outbreak of World War II in 1939 led to the postponement, and later the abandonment, of these plans.

Plans to build a new Reich chancellery had been underway since 1934. Land had been purchased by the end of 1934 and starting in March 1936 the first buildings were demolished to create space at Voßstraße. Speer was involved virtually from the beginning. In the aftermath of the Night of the Long Knives, he had been commissioned to renovate the Borsig Palace on the corner of Voßstraße and Wilhelmstraße as headquarters of the "Sturmabteilung" (SA). He completed the preliminary work for the new chancellery by May 1936. In June 1936 he charged a personal honorarium of 30,000 Reichsmark and estimated the chancellery would be completed within three to four years. Detailed plans were completed in July 1937 and the first shell of the new chancellery was complete on January 1, 1938. On January 27, 1938, Speer received plenipotentiary powers from Hitler to finish the new chancellery by January 1, 1939. For propaganda Hitler claimed during the topping-out ceremony on August 2, 1938, that he had ordered Speer to complete the new chancellery that year. Shortages of labor meant the construction workers had to work in ten-to-twelve-hour shifts. The "Schutzstaffel" (SS) built two concentration camps in 1938 and used the inmates to quarry stone for its construction. A brick factory was built near the Oranienburg concentration camp at Speer's behest; when someone commented on the poor conditions there, Speer stated, "The Yids got used to making bricks while in Egyptian captivity". The chancellery was completed in early January 1939. The building itself was hailed by Hitler as the "crowning glory of the greater German political empire".

During the Chancellery project, the pogrom of "Kristallnacht" took place. Speer made no mention of it in the first draft of "Inside the Third Reich". It was only on the urgent advice of his publisher that he added a mention of seeing the ruins of the Central Synagogue in Berlin from his car. "Kristallnacht" accelerated Speer's ongoing efforts to dispossess Berlin's Jews from their homes. From 1939 on, Speer's Department used the Nuremberg Laws to evict Jewish tenants of non-Jewish landlords in Berlin, to make way for non-Jewish tenants displaced by redevelopment or bombing. Eventually, 75,000 Jews were displaced by these measures. Speer denied he knew they were being put on Holocaust trains and claimed that those displaced were, "Completely free and their families were still in their apartments". He also said: " ... en route to my ministry on the city highway, I could see ... crowds of people on the platform of nearby Nikolassee Railroad Station. I knew that these must be Berlin Jews who were being evacuated. I am sure that an oppressive feeling struck me as I drove past. I presumably had a sense of somber events." Matthias Schmidt said Speer had personally inspected concentration camps and described his comments as an "outright farce". Martin Kitchen described Speer's often repeated line that he knew nothing of the "dreadful things" as hollow—because not only was he fully aware of the fate of the Jews he was actively participating in their persecution.

As Germany started World War II in Europe, Speer instituted quick-reaction squads to construct roads or clear away debris; before long, these units would be used to clear bomb sites. Speer used forced Jewish labor on these projects, in addition to regular German workers. Construction stopped on the Berlin and Nüremberg plans at the outbreak of war. Though stockpiling of materials and other work continued, this slowed to a halt as more resources were needed for the armament industry. Speer's offices undertook building work for each branch of the military, and for the SS, using slave labor. Speer's building work made him among the wealthiest of the Nazi elite.

On February 8, 1942, Minister of Armaments Fritz Todt died in a plane crash shortly after taking off from Hitler's eastern headquarters at Rastenburg. Speer arrived there the previous evening and accepted Todt's offer to fly with him to Berlin. Speer cancelled some hours before take-off because the previous night he had been up late in a meeting with Hitler. Hitler appointed Speer in Todt's place. Martin Kitchen, a British historian, says that the choice was not surprising. Speer was loyal to Hitler, and his experience building prisoner of war camps and other structures for the military qualified him for the job. Hitler also appointed Speer as head of the Organisation Todt, a massive, government-controlled construction company. Characteristically Hitler did not give Speer any clear remit; he was left to fight his contemporaries in the regime for power and control. He proved to be ambitious, unrelenting and ruthless. Speer set out to gain control not just of armaments production in the army, but in the whole armed forces. It did not immediately dawn on his political rivals that his calls for rationalization and reorganization were hiding his desire to sideline them and take control.

Speer was fêted at the time, and in the post-war era, for performing an "armaments miracle" in which German war production dramatically increased. This "miracle" was brought to a halt in the summer of 1943 by, among other factors, the first sustained Allied bombing. Other factors probably contributed to the increase more than Speer himself. Germany's armaments production had already begun to result in increases under his predecessor, Todt. Naval armaments were not under Speer's supervision until October 1943, nor the Luftwaffe's armaments until June of the following year. Yet each showed comparable increases in production despite not being under Speer's control. Another factor that produced the boom in ammunition was the policy of allocating more coal to the steel industry. Production of every type of weapon peaked in June and July 1944, but there was now a severe shortage of fuel. After August 1944, oil from the Romanian fields was no longer available. Oil production became so low that any possibility of offensive action became impossible and weaponry lay idle.

As Minister of Armaments, Speer was responsible for supplying weapons to the army. With Hitler's full agreement, he decided to prioritize tank production, and he was given unrivalled power to ensure success. Hitler was closely involved with the design of the tanks, but kept changing his mind about the specifications. This delayed the program, and Speer was unable to remedy the situation. In consequence, despite tank production having the highest priority, relatively little of the armaments budget was spent on it. This led to a significant German Army failure at the Battle of Prokhorovka, a major turning point on the Eastern Front against the Soviet Red Army.

As head of Organisation Todt, Speer was directly involved in the construction and alteration of concentration camps. He agreed to expand Auschwitz and some other camps, allocating 13.7 million Reichsmarks for the work to be carried out. This allowed an extra 300 huts to be built at Auschwitz, increasing the total human capacity to 132,000. Included in the building works was material to build gas chambers, crematoria and morgues. The SS called this "Professor Speer's Special Programme".

Speer realized that with six million workers drafted into the armed forces, there was a labor shortage in the war economy, and not enough workers for his factories. In response, Hitler appointed Fritz Sauckel as a "manpower dictator" to obtain new workers. Speer and Sauckel cooperated closely to meet Speer's labor demands. Hitler gave Sauckel a free hand to obtain labor, something that delighted Speer, who had requested 1,000,000 "voluntary" laborers to meet the need for armament workers. Sauckel had whole villages in France, Holland and Belgium forcibly rounded up and shipped to Speer's factories. Sauckel obtained new workers often using the most brutal methods. In occupied areas of the Soviet Union, that had been subject to partisan action, civilian men and women were rounded up en masse and sent to work forcibly in Germany. By April 1943, Sauckel had supplied 1,568,801 "voluntary" laborers, forced laborers, prisoners of war and concentration camp prisoners to Speer for use in his armaments factories. It was for the maltreatment of these people, that Speer was principally convicted at the Nuremberg Trials.

Following his appointment as Minister of Armaments, Speer was in control of armaments production solely for the Army. He coveted control of the production of armaments for the "Luftwaffe" and "Kriegsmarine" as well. He set about extending his power and influence with unexpected ambition. His close relationship with Hitler provided him with political protection, and he was able to outwit and outmanoeuver his rivals in the regime. Hitler's cabinet was dismayed at his tactics, but, regardless, he was able to accumulate new responsibilities and more power. By July 1943, he had gained control of armaments production for the "Luftwaffe" and "Kriegsmarine". In August 1943, he took control of most of the Ministry of Economics, to become, in Admiral Dönitz's words, "Europe's economic dictator". His formal title was changed to "Reich Minister for Armaments and War Production". He had become one of the most powerful people in Nazi Germany.

Speer and his hand-picked director of submarine construction Otto Merker believed that the shipbuilding industry was being held back by outdated methods, and revolutionary new approaches imposed by outsiders would dramatically improve output. This belief proved incorrect, and Speer and Merker's attempt to build the "Kriegsmarine"s new generation of submarines, the Type XXI and Type XXIII, as prefabricated sections at different facilities rather than at single dockyards contributed to the failure of this strategically important program. The designs were rushed into production, and the completed submarines were crippled by flaws which resulted from the way they had been constructed. While dozens of submarines were built, few ever entered service.

In December 1943, Speer visited Organisation Todt workers in Lapland, while there he seriously damaged his knee and was incapacitated for several months. He was under the dubious care of Professor Karl Gebhardt at a medical clinic called Hohenlychen where patients "mysteriously failed to survive". In mid-January 1944, Speer had a lung embolism and fell seriously ill. Concerned about retaining power, he did not appoint a deputy and continued to direct work of the Armaments Ministry from his bedside. Speer's illness coincided with the Allied "Big Week", a series of bombing raids on the German aircraft factories that were a devastating blow to aircraft production. His political rivals used the opportunity to undermine his authority and damage his reputation with Hitler. He lost Hitler's unconditional support and began to lose power.

In response to the Allied Big Week, Adolf Hitler authorized the creation of a Fighter Staff committee. Its aim was to ensure the preservation and growth of fighter aircraft production. The task force was established by the March 1, 1944, orders of Speer, with support from Erhard Milch of the Reich Aviation Ministry. Production of German fighter aircraft more than doubled between 1943 and 1944. The growth, however, consisted in large part of models that were becoming obsolescent and proved easy prey for Allied aircraft. On August 1, 1944, Speer merged the Fighter Staff into a newly formed Armament Staff committee.

The Fighter Staff committee was instrumental in bringing about the increased exploitation of slave labor in the war economy. The SS provided 64,000 prisoners for 20 separate projects from various concentration camps including Mittelbau-Dora. Prisoners worked for Junkers, Messerschmitt, Henschel and , among others. To increase production, Speer introduced a system of punishments for his workforce. Those who feigned illness, slacked off, sabotaged production or tried to escape were denied food or sent to concentration camps. In 1944 this became endemic; over half a million workers were arrested. By this time, 140,000 people were working in Speer's underground factories. These factories were death-traps; discipline was brutal, with regular executions. There were so many corpses at the Dora underground factory, for example, that the crematorium was overwhelmed. Speer's own staff described the conditions there as "hell".

The largest technological advance under Speer's command came through the rocket program. It began in 1932 but had not supplied any weaponry. Speer enthusiastically supported the program and in March 1942 made an order for A4 rockets, the predecessor of the world's first ballistic missile, the V-2 rocket. The rockets were researched at a facility in Peenemünde along with the V-1 flying bomb. The V-2's first target was Paris on September 8, 1944. The program while advanced proved to be an impediment to the war economy. The large capital investment was not repaid in military effectiveness. The rockets were built at an underground factory at Mittelwerk. Labor to build the A4 rockets came from the Mittelbau-Dora concentration camp. Of the 60,000 people who ended up at the camp 20,000 died, due to the appalling conditions.

By the summer of 1944 Speer had lost control of Organisation Todt and armaments. He opposed the assassination attempt against Hitler on July 20, 1944. He was not involved in the plot, and played a minor role in the regime's efforts to regain control over Berlin after Hitler survived. After the plot Speer's rivals attacked some of his closest allies and his management system fell out of favor with radicals in the party. He lost yet more authority.

Losses of territory and a dramatic expansion of the Allied strategic bombing campaign caused the collapse of the German economy from late 1944. Air attacks on the transport network were particularly effective, as they cut the main centres of production off from essential coal supplies. In January 1945, Speer told Goebbels that armaments production could be sustained for at least a year. However, he concluded that the war was lost after Soviet forces captured the important Silesian industrial region later that month. Nevertheless, Speer believed that Germany should continue the war for as long as possible with the goal of winning better conditions from the Allies than the unconditional surrender they insisted upon. During January and February, Speer claimed that his ministry would deliver "decisive weapons" and a large increase in armaments production which would "bring about a dramatic change on the battlefield". Speer gained control over the railways in February, and asked Himmler to supply concentration camp prisoners to work on their repair.

By mid-March, Speer had accepted that Germany's economy would collapse within the next eight weeks. While he sought to frustrate directives to destroy industrial facilities in areas at risk of capture, so that they could be used after the war, he still supported the war's continuation. Speer provided Hitler with a memorandum on March 15, which detailed Germany's dire economic situation and sought approval to cease demolitions of infrastructure. Three days later, he also proposed to Hitler that Germany's remaining military resources be concentrated along the Rhine and Vistula rivers in an attempt to prolong the fighting. This ignored military realities, as the German armed forces were unable to match the Allies' firepower and were facing total defeat. Hitler rejected Speer's proposal to cease demolitions. Instead, he issued the "Nero Decree" on March 19, which called for the destruction of all infrastructure as the army retreated. Speer was appalled by this order, and persuaded several key military and political leaders to ignore it. During a meeting with Speer on March 28/29, Hitler rescinded the decree and gave him authority over demolitions. Speer ended them, though the army continued to blow up bridges.

By April, little was left of the armaments industry, and Speer had few official duties. Speer visited the "Führerbunker" on April 22 for the last time. He met Hitler and toured the damaged Chancellery before leaving Berlin to return to Hamburg. On April 29, the day before committing suicide, Hitler dictated a final political testament which dropped Speer from the successor government. Speer was to be replaced by his subordinate, Karl-Otto Saur. Speer was disappointed that Hitler had not selected him as his successor. After Hitler's death, Speer offered his services to the so-called Flensburg Government, headed by Hitler's successor, Karl Dönitz. He took a role in that short-lived regime as Minister of Industry and Production. Speer provided information to the Allies, regarding the effects of the air war, and on a broad range of subjects, beginning on May 10. On May 23, two weeks after the surrender of German forces, British troops arrested the members of the Flensburg Government and brought Nazi Germany to a formal end.

Speer was taken to several internment centres for Nazi officials and interrogated. In September 1945, he was told that he would be tried for war crimes, and several days later, he was moved to Nuremberg and incarcerated there. Speer was indicted on four counts: participating in a common plan or conspiracy for the accomplishment of crime against peace; planning, initiating and waging wars of aggression and other crimes against peace; war crimes; and crimes against humanity.

The chief United States prosecutor, Robert H. Jackson, of the U.S. Supreme Court said, "Speer joined in planning and executing the program to dragoon prisoners of war and foreign workers into German war industries, which waxed in output while the workers waned in starvation." Speer's attorney, Hans Flächsner, presented Speer as an artist thrust into political life who had always remained a non-ideologue.

Speer was found guilty of war crimes and crimes against humanity, principally for the use of slave labor and forced labor. He was acquitted on the other two counts. He had claimed that he was unaware of Nazi extermination plans, and this probably saved him from hanging. His claim was revealed to be false in a private correspondence written in 1971 and publicly disclosed in 2007. On October 1, 1946, he was sentenced to 20 years' imprisonment. While three of the eight judges (two Soviet and American Francis Biddle) advocated the death penalty for Speer, the other judges did not, and a compromise sentence was reached after two days of discussions.

On July 18, 1947, Speer was transferred to Spandau Prison in Berlin to serve his prison term. There he was known as Prisoner Number Five. Speer's parents died while he was incarcerated. His father, who died in 1947, despised the Nazis and was silent upon meeting Hitler. His mother died in 1952. A Nazi, she had greatly enjoyed dining with Hitler. Wolters and longtime Speer secretary Annemarie Kempf, while not permitted direct communication with Speer in Spandau did what they could to help his family and carry out the requests Speer put in letters to his wife—the only written communication he was officially allowed. Beginning in 1948, Speer had the services of Toni Proost, a sympathetic Dutch orderly to smuggle mail and his writings.
In 1949, Wolters opened a bank account for Speer and began fundraising among those architects and industrialists who had benefited from Speer's activities during the war. Initially, the funds were used only to support Speer's family, but increasingly the money was used for other purposes. They paid for Toni Proost to go on holiday, and for bribes to those who might be able to secure Speer's release. Once Speer became aware of the existence of the fund, he sent detailed instructions about what to do with the money. Wolters raised a total of DM158,000 for Speer over the final seventeen years of his sentence.

The prisoners were forbidden to write memoirs. Speer was able to have his writings sent to Wolters, however, and they eventually amounted to 20,000 pages. He had completed his memoirs by November 1953, which became the basis of "Inside the Third Reich". In "Spandau Diaries", Speer aimed to present himself as a tragic hero who had made a Faustian bargain for which he endured a harsh prison sentence.

Much of Speer's energy was dedicated to keeping fit, both physically and mentally, during his long confinement. Spandau had a large enclosed yard where inmates were allocated plots of land for gardening. Speer created an elaborate garden complete with lawns, flower beds, shrubbery, and fruit trees. To make his daily walks around the garden more engaging Speer embarked on an imaginary trip around the globe. Carefully measuring distance travelled each day, he mapped distances to the real-world geography. He had walked more than , ending his sentence near Guadalajara, Mexico. Speer also read, studied architectural journals, and brushed up on English and French. In his writings, Speer claimed to have finished five thousand books while in prison, a gross exaggeration. His sentence amounted to 7,300 days, which only allotted one and a half days per book.

Speer's supporters maintained calls for his release. Among those who pledged support for his sentence to be commuted were Charles de Gaulle and US diplomat George Wildman Ball. Willy Brandt was an advocate of his release, putting an end to the de-Nazification proceedings against him, which could have caused his property to be confiscated. Speer's efforts for an early release came to naught. The Soviet Union, having demanded a death sentence at trial, was unwilling to entertain a reduced sentence. Speer served a full term and was released at midnight on October 1, 1966.

Speer's release from prison was a worldwide media event. Reporters and photographers crowded both the street outside Spandau and the lobby of the Berlin hotel where Speer spent the night. He said little, reserving most comments for a major interview published in "Der Spiegel" in November 1966. Although he stated he hoped to resume an architectural career, his sole project, a collaboration for a brewery, was unsuccessful. Instead, he revised his Spandau writings into two autobiographical books, and later published a work about Himmler and the SS. His books included "Inside the Third Reich" (in German, "Erinnerungen", or "Reminiscences") and "". Speer was aided in shaping the works by Joachim Fest and Wolf Jobst Siedler from the publishing house Ullstein. He found himself unable to re-establish a relationship with his children, even with his son Albert who had also become an architect. According to Speer's daughter Hilde Schramm, "One by one my sister and brothers gave up. There was no communication." He supported Hermann, his brother, financially after the war. However, his other brother Ernst had died in the Battle of Stalingrad, despite repeated requests from his parents for Speer to repatriate him.

Following his release from Spandau, Speer donated the "Chronicle", his personal diary, to the German Federal Archives. It had been edited by Wolters and made no mention of the Jews. David Irving discovered discrepancies between the deceptively edited "Chronicle" and independent documents. Speer asked Wolters to destroy the material he had omitted from his donation but Wolters refused and retained an original copy. Wolters' friendship with Speer deteriorated and one year before Speer's death Wolters gave Matthias Schmidt access to the unedited "Chronicle". Schmidt authored the first book that was highly critical of Speer.

Speer's memoirs were a phenomenal success. The public was fascinated by an inside view of the Third Reich and a major war criminal became a popular figure almost overnight. Importantly, he provided an alibi to older Germans who had been Nazis. If Speer, who had been so close to Hitler, had not known the full extent of the crimes of the Nazi regime and had just been "following orders", then they could tell themselves and others they too had done the same. Speer provided a whitewash for an entire generation of older Germans. So great was the need to believe this "Speer Myth" that Fest and Siedler were able to strengthen it—even in the face of mounting historical evidence to the contrary.

Speer made himself widely available to historians and other enquirers. In October 1973, he made his first trip to Britain, flying to London to be interviewed on the BBC "Midweek" program. In the same year, he appeared on the television program "The World at War". Speer returned to London in 1981 to participate in the BBC "Newsnight" program. He suffered a stroke and died in London on September 1.

He had remained married to his wife, but he had formed a relationship with a German woman living in London and was with her at the time of his death. His daughter, Margret Nissen, wrote in her 2005 memoirs that after his release from Spandau he spent all of his time constructing the "Speer Myth".

After his release from Spandau, Speer portrayed himself as the "good Nazi". He was well-educated, middle class, bourgeois and could contrast himself with the psychopaths and murderers who, in the popular mind, typified "bad Nazis". In his memoirs and interviews, he had distorted the truth and made so many major omissions that his lies became known as "myths". Speer took his myth-making to a mass media level and his "cunning apologies" were reproduced countless times in post-war Germany. Isabell Trommer writes in her biography of Speer that Fest and Siedler were co-authors of Speer's memoirs and co-creators of his myths. In return they were paid handsomely in royalties and other financial inducements. Speer, Siedler and Fest had constructed a masterpiece; the image of the "good Nazi" remained in place for decades, despite historical evidence indicating that it was false.
Speer had carefully constructed an image of himself as an apolitical technocrat who deeply regretted having failed to discover the monstrous crimes of the Third Reich. After Speer's death, Matthias Schmidt published a book that demonstrated that he had ordered the eviction of Jews from their Berlin homes. By 1999 historians had amply demonstrated that he had lied extensively. Even so, public perceptions of Speer did not change substantially until Heinrich Breloer aired a biographical film on TV in 2004. The film began a process of demystification and critical reappraisal. Adam Tooze in his book "The Wages of Destruction" said Speer had manoeuvred himself through the ranks of the regime skillfully and ruthlessly and that the idea he was a technocrat blindly carrying out orders was "absurd". Trommer said he was not an apolitical technocrat; instead, he was one of the most powerful and unscrupulous leaders in the Nazi regime. Kitchen said he had deceived the Nuremberg Tribunal and post-war Germany. Brechtken said that if his extensive involvement in the Holocaust had been known at the time of his trial he would have been sentenced to death.

The image of the good Nazi was supported by numerous Speer myths. In addition to the myth that he was an apolitical technocrat, he claimed he did not have full knowledge of the Holocaust or the persecution of the Jews. Another myth posits that Speer revolutionized the German war machine after his appointment as Minister of Armaments. He was credited with a dramatic increase in the shipment of arms that was widely reported as keeping Germany in the war. Another myth centered around a faked plan to assassinate Hitler with poisonous gas. The idea for this myth came to him after he recalled the panic when car fumes came through an air ventilation system. He fabricated the additional details. Brechtken wrote that his most brazen lie was fabricated during an interview with a French journalist in 1952. The journalist described an invented scenario in which Speer had refused Hitler's orders and Hitler had left with tears in his eyes. Speer liked the scenario so much that he wrote it into his memoirs. The journalist had unwittingly collaborated in one of his myths.

Speer also sought to portray himself as an opponent of Hitler's leadership. Despite his opposition to the 20 July plot, he falsely claimed in his memoirs to have been sympathetic to the plotters. He maintained Hitler was cool towards him for the remainder of his life after learning they had included him on a list of potential ministers. This formed a key element of the myths Speer encouraged. Speer also falsely claimed that he had realised the war was lost at an early stage, and thereafter worked to preserve the resources needed for the civilian population's survival. In reality, he had sought to prolong the war until further resistance was impossible, thus contributing to the large number of deaths and the extensive destruction Germany suffered in the conflict's final months.

Speer maintained at the Nuremberg trials and in his memoirs that he had no direct knowledge of the Holocaust. He admitted only to being uncomfortable around Jews in the published version of the "Spandau Diaries". More broadly, Speer accepted responsibility for the Nazi regime's actions. Historian Martin Kitchen states that Speer was actually "fully aware of what had happened to the Jews" and was "intimately involved in the 'Final Solution'". Brechtken said Speer only admitted to a generalized responsibility for the Holocaust to hide his direct and actual responsibility. Speer was photographed with slave laborers at Mauthausen concentration camp during a visit on 31 March 1943; he also visited Gusen concentration camp. Although survivor Francisco Boix testified at the Nuremberg trials about Speer's visit, Taylor writes that, had the photo been available, he would have been hanged. In 2005, "The Daily Telegraph" reported that documents had surfaced indicating that Speer had approved the allocation of materials for the expansion of Auschwitz concentration camp after two of his assistants inspected the facility on a day when almost a thousand Jews were massacred. Heinrich Breloer, discussing the construction of Auschwitz, said Speer was not just a cog in the work—he was the "terror itself".

Speer denied being present at the Posen speeches to Nazi leaders at a conference in Posen (Poznań) on October 6, 1943. Himmler said during his speech, "The grave decision had to be taken to cause this people to vanish from the earth"., and later, "The Jews must be exterminated". Speer is mentioned several times in the speech, and Himmler addresses him directly. In 2007, "The Guardian" reported that a letter from Speer dated December 23, 1971, had been found in a collection of his correspondence with Hélène Jeanty, the widow of a Belgian resistance fighter. In the letter, Speer says, "There is no doubt—I was present as Himmler announced on October 6, 1943, that all Jews would be killed."

Speer was credited with an "armaments miracle". During the winter of 1941–42, in the light of Germany's disastrous defeat in the Battle of Moscow, the German leadership including Fromm, Thomas and Todt had come to the conclusion that the war could not be won. The rational position to adopt was to seek a political solution that would end the war without defeat. Speer in response used his propaganda expertise to display a new dynamism of the war economy. He produced spectacular statistics, claiming a sixfold increase in munitions production, a fourfold increase in artillery production, and he sent further propaganda to the newsreels of the country. He was able to curtail the discussion that the war should be ended.

The armaments "miracle" was a myth; Speer had used statistical manipulation to support his claims. The production of armaments did go up; however, this was due to the normal causes of reorganization before Speer came to office, the relentless mobilization of slave labor and a deliberate reduction in the quality of output to favor quantity. By July 1943 Speer's armaments propaganda became irrelevant because a catalogue of dramatic defeats on the battlefield meant the prospect of losing the war could no longer be hidden from the German public. Brechtken writes that Speer knew Germany was going to lose the war and deliberately extended its length, thus causing the deaths of millions of people in the death camps and on the battlefield who would have otherwise lived. Kitchen said "There can be no doubt that Speer did indeed help to prolong the war longer than many thought possible, as a result of which millions were killed and Germany reduced to a pile of rubble".

Little remains of Speer's personal architectural works, other than the plans and photographs. No buildings designed by Speer during the Nazi era are extant in Berlin, other than the "Schwerbelastungskörper", a heavy load bearing body built around 1941. The concrete cylinder, high, was used to measure ground subsidence as part of feasibility studies for a massive triumphal arch and other large structures proposed as part of "Welthauptstadt Germania", Hitler's planned post-war renewal project for the city. The cylinder is now a protected landmark and is open to the public. The tribune of the "Zeppelinfeld" stadium in Nuremberg, though partly demolished, can also be seen.

During the war, the Speer-designed Reich Chancellery was largely destroyed by air raids and in the Battle of Berlin. The exterior walls survived, but they were eventually dismantled by the Soviets. Unsubstantiated rumors have claimed that the remains were used for other building projects such as the Humboldt University, Mohrenstraße metro station and Soviet war memorials in Berlin.





</doc>
<doc id="956" url="https://en.wikipedia.org/wiki?curid=956" title="Asteraceae">
Asteraceae

Asteraceae or Compositae (commonly referred to as the aster, daisy, composite, or sunflower family), is a very large and widespread family of flowering plants (Angiospermae).

The family includes over 32,000 currently accepted species, in over 1,900 genera (list) in 13 subfamilies. In terms of numbers of species, the Asteraceae are rivaled only by the Orchidaceae. Which is the larger family is unclear, because of the uncertainty about how many extant species each family includes.

Nearly all Asteraceae bear their flowers in dense heads (capitula or pseudanthia) surrounded by involucral bracts. When viewed from a distance, each capitulum may appear to be a single flower. Enlarged outer (peripheral) flowers in the capitula may resemble petals, and the involucral bracts may look like a calyx. The name Asteraceae comes from the type genus "Aster", from the Ancient Greek , meaning star, and refers to the star-like form of the inflorescence. The alternative name Compositae is still valid under the International Code of Nomenclature for algae, fungi, and plants.) It refers to the "composite" nature of the capitula, which consist of a few or many individual flowers.

Most members of Asteraceae are annual or perennial herbs, but a significant number are also shrubs, vines, or trees. The family has a cosmopolitan distribution, with species ranging from to tropical regions, colonizing a wide variety of habitats. The largest proportion of the species occur in the arid and semiarid regions of subtropical and lower temperate latitudes. The Asteraceae may represent as much as 10% of autochthonous flora in many regions of the world.

Asteraceae is an economically important family, providing products such as cooking oils, leaf vegetables like lettuce, sunflower seeds, artichokes, sweetening agents, coffee substitutes and herbal teas. Several genera are of horticultural importance, including pot marigold ("Calendula officinalis"), "Echinacea" (coneflowers), various daisies, fleabane, chrysanthemums, dahlias, zinnias, and heleniums. Asteraceae are important in herbal medicine, including "Grindelia", yarrow, and many others.

On the other hand, many Asteraceae are considered weeds in various circumstances. Of these, many are invasive species in particular regions, often having been introduced by human agency. Examples include various tumbleweeds, "Bidens", ragweeds, thistles, and dandelion. Dandelion was introduced into North America by European settlers who used the young leaves as a salad green.

The study of this family is known as synantherology.

The name Asteraceae () comes to international scientific vocabulary from New Latin, from "Aster", the type genus, + "-aceae", a standardized suffix for plant family names in modern taxonomy. The genus name comes from the Classical Latin word , "star", which came from Ancient Greek (), "star".

The earlier name, Compositae (now recognized as an alternative name) means "composite" and refers to the characteristic inflorescence, a special type of pseudanthium found in only a few other angiosperm families.

The vernacular name "daisy", widely applied to members of this family, is derived from the Old English name of the daisy ("Bellis perennis"): , meaning "day's eye". This is because the petals open at dawn and close at dusk.

Asteraceae species have a cosmopolitan distribution, and are found everywhere except Antarctica and the extreme Arctic. They are especially numerous in tropical and subtropical regions (notably Central America, eastern Brazil, the Mediterranean, the Levant, southern Africa, central Asia, and southwestern China).

Compositae, the original name for Asteraceae, were first described in 1792 by the German botanist Paul Dietrich Giseke. Traditionally, two subfamilies were recognised: Asteroideae (or Tubuliflorae) and Cichorioideae (or Liguliflorae). The latter has been shown to be extensively paraphyletic, and has now been divided into 12 subfamilies, but the former still stands. The phylogenetic tree presented below is based on Panero & Funk (2002) updated in 2014, and now also includes the monotypic Famatinanthoideae.
The diamond denotes a very poorly supported node (<50% bootstrap support), the dot a poorly supported node (<80%).

The four subfamilies Asteroideae, Cichorioideae, Carduoideae and Mutisioideae contain 99% of the species diversity of the whole family (approximately 70%, 14%, 11% and 3% respectively).

Because of the morphological complexity exhibited by this family, agreeing on generic circumscriptions has often been difficult for taxonomists. As a result, several of these genera have required multiple revisions.

Members of the Asteraceae are mostly herbaceous plants, but some shrubs, climbers and trees (such as "Lachanodes arborea") do exist. They are generally easy to distinguish from other plants, mainly because of their characteristic inflorescence and other shared characteristics. However, determining genera and species of some groups such as "Hieracium" is notoriously difficult (see "damned yellow composite" for example).

Members of the Asteraceae generally produce taproots, but sometimes they possess fibrous root systems. Stems are herbaceous aerial branched cylindrical with glandular hairs generally erect but can be prostrate to ascending. Some species have underground stems in the form of caudices or rhizomes. These can be fleshy or woody depending on the species.

The leaves and the stems very often contain secretory canals with resin or latex (particularly common among the Cichorioideae). The leaves can be alternate, opposite, or whorled. They may be simple, but are often deeply lobed or otherwise incised, often conduplicate or revolute. The margins can be entire or lobed or toothed.

In plants of the family Asteraceae, what appears to be a single flower is actually a cluster of much smaller flowers. The overall appearance of the cluster, as a single flower, functions in attracting pollinators in the same way as the structure of an individual flower in some other plant families. The older family name, Compositae, comes from the fact that what appears to be a single flower is actually a "composite" of smaller flowers. The "petals" or "sunrays" in a sunflower head are actually individual strap-shaped flowers called ray flowers, and the "sun disk" is made of smaller circular shaped individual flowers called disc flowers. The word "aster" means "star" in Greek, referring to the appearance of some family members, as a "star" surrounded by "rays". The cluster of flowers that may appear to be a single flower, is called a "head". The entire head may move tracking the sun, like a "smart" solar panel, which maximizes reflectivity of the whole unit and can thereby attract more pollinators.
At the base of the head, and surrounding the flowers before opening, is a bundle of sepal-like bracts or scales called phyllaries, which together form the "involucre" that protects the individual flowers in the head before opening. The individual heads have the smaller individual flowers arranged on a round or dome-like structure called the "receptacle". The flowers mature first at the outside, moving toward the center, with the youngest in the middle.

The individual flowers in a head have 5 fused petals (rarely 4), but instead of sepals, have threadlike, hairy, or bristly structures called "pappus", which surround the fruit and can stick to animal fur or be lifted by wind, aiding in seed dispersal. The whitish fluffy head of a dandelion, commonly blown on by children, is made of the pappus, with tiny seeds attached at the ends, whereby the pappus provides a parachute like structure to help the seed be carried away in the wind.

A "ray flower" is a 3-tipped (3-lobed), strap-shaped, individual flower in the head of some members of the family Asteraceae. Sometimes a ray flower is 2-tipped (2-lobed). The corolla of the ray flower may have 2 tiny teeth opposite the 3-lobed strap, or tongue, indicating evolution by fusion from an originally 5-part corolla. Sometimes, the 3:2 arrangement is reversed, with 2 tips on the tongue, and 0 or 3 tiny teeth opposite the tongue. A "ligulate flower" is a 5-tipped, strap-shaped, individual flower in the heads of other members. A "ligule" is the strap-shaped tongue of the corolla of either a ray flower or of a ligulate flower. A "disk flower" (or "disc flower") is a radially symmetric (i.e., with identical shaped petals arranged in circle around the center) individual flower in the head, which is ringed by ray flowers when both are present. Sometimes ray flowers may be slightly off from radial symmetry, or weakly bilaterally symmetric, as in the case of desert pincushions "Chaenactis fremontii".

A "radiate head" has disc flowers surrounded by ray flowers. A "ligulate head" has all ligulate flowers. When a sunflower family flower head has only disc flowers that are sterile, male, or have both male and female parts, it is a "discoid head". "Disciform heads" have only disc flowers, but may have two kinds (male flowers and female flowers) in one head, or may have different heads of two kinds (all male, or all female). "Pistillate heads" have all female flowers. "Staminate heads" have all male flowers.

Sometimes, but rarely, the head contains only a single flower, or has a single flowered pistillate (female) head, and a multi-flowered male staminate (male) head.

The distinguishing characteristic of Asteraceae is their inflorescence, a type of specialised, composite flower head or "pseudanthium", technically called a calathium or "capitulum", that may look superficially like a single flower. The "capitulum" is a contracted raceme composed of numerous individual sessile flowers, called "florets", all sharing the same receptacle.

A set of bracts forms an involucre surrounding the base of the capitulum. These are called "phyllaries", or "involucral bracts". They may simulate the sepals of the pseudanthium. These are mostly herbaceous but can also be brightly coloured (e.g. "Helichrysum") or have a scarious (dry and membranous) texture. The phyllaries can be free or fused, and arranged in one to many rows, overlapping like the tiles of a roof ("imbricate") or not (this variation is important in identification of tribes and genera).

Each floret may be subtended by a bract, called a "palea" or "receptacular bract". These bracts are often called "chaff". The presence or absence of these bracts, their distribution on the receptacle, and their size and shape are all important diagnostic characteristics for genera and tribes.

The florets have five petals fused at the base to form a corolla tube and they may be either actinomorphic or zygomorphic. "Disc florets" are usually actinomorphic, with five petal lips on the rim of the corolla tube. The petal lips may be either very short, or long, in which case they form deeply lobed petals. The latter is the only kind of floret in the Carduoideae, while the first kind is more widespread. "Ray florets" are always highly zygomorphic and are characterised by the presence of a "ligule", a strap-shaped structure on the edge of the corolla tube consisting of fused petals. In the Asteroideae and other minor subfamilies these are usually borne only on florets at the circumference of the capitulum and have a 3+2 scheme – above the fused corolla tube, three very long fused petals form the ligule, with the other two petals being inconspicuously small. The Cichorioideae has only ray florets, with a 5+0 scheme – all five petals form the ligule. A 4+1 scheme is found in the Barnadesioideae. The tip of the ligule is often divided into teeth, each one representing a petal. Some marginal florets may have no petals at all (filiform floret).

The calyx of the florets may be absent, but when present is always modified into a pappus of two or more teeth, scales or bristles and this is often involved in the dispersion of the seeds. As with the bracts, the nature of the pappus is an important diagnostic feature.

There are usually five stamens. The filaments are fused to the corolla, while the anthers are generally connate ("syngenesious" anthers), thus forming a sort of tube around the style ("theca"). They commonly have basal and/or apical appendages. Pollen is released inside the tube and is collected around the growing style, and then, as the style elongates, is pushed out of the tube ("nüdelspritze").

The pistil consists of two connate carpels. The style has two lobes. Stigmatic tissue may be located in the interior surface or form two lateral lines. The ovary is inferior and has only one ovule, with basal placentation.

In members of the Asteraceae the fruit is achene-like, and is called a "cypsela" (plural "cypselae"). Although there are two fused carpels, there is only one locule, and only one seed per fruit is formed. It may sometimes be winged or spiny because the pappus, which is derived from calyx tissue often remains on the fruit (for example in dandelion). In some species, however, the pappus falls off (for example in "Helianthus"). Cypsela morphology is often used to help determine plant relationships at the genus and species level. The mature seeds usually have little endosperm or none.

The pollen of composites is typically echinolophate, a morphological term meaning "with elaborate systems of ridges and spines dispersed around and between the apertures."

In Asteraceae, the energy store is generally in the form of inulin rather than starch. They produce iso/chlorogenic acid, sesquiterpene lactones, pentacyclic triterpene alcohols, various alkaloids, acetylenes (cyclic, aromatic, with vinyl end groups), tannins. They have terpenoid essential oils which never contain iridoids.

Asteraceae produce secondary metabolites, such as flavonoids and terpenoids. Some of these molecules can inhibit protozoan parasites such as "Plasmodium", "Trypanosoma", "Leishmania" and parasitic intestinal worms, and thus have potential in medicine.

The oldest known fossils of members of Asteraceae are pollen grains from the Late Cretaceous of Antarctica, dated to ∼76–66 Mya (Campanian to Maastrichtian) and assigned to the extant genus "Dasyphyllum". Barreda, "et al." (2015) estimated that the crown group of Asteraceae evolved at least 85.9 Mya (Late Cretaceous, Santonian) with a stem node age of 88–89 Mya (Late Cretaceous, Coniacian).

It is still unknown whether the precise cause of their great success was the development of the highly specialised capitulum, their ability to store energy as fructans (mainly inulin), which is an advantage in relatively dry zones, or some combination of these and possibly other factors.

Asteraceans are especially common in open and dry environments.

Many members of Asteraceae are pollinated by insects, which explains their value in attracting beneficial insects, but anemophily is also present (e.g. "Ambrosia", "Artemisia"). There are many apomictic species in the family.

Seeds are ordinarily dispersed intact with the fruiting body, the cypsela. "Anemochory" (wind dispersal) is common, assisted by a hairy pappus. "Epizoochory" is another common method, in which the dispersal unit, a single cypsela (e.g. "Bidens") or entire capitulum (e.g. "Arctium") has hooks, spines or some structure to attach to the fur or plumage (or even clothes, as in the photo) of an animal just to fall off later far from its mother plant.

Commercially important plants in Asteraceae include the food crops "Lactuca sativa" (lettuce), "Cichorium" (chicory), "Cynara scolymus" (globe artichoke), "Helianthus annuus" (sunflower), "Smallanthus sonchifolius" (yacón), "Carthamus tinctorius" (safflower) and "Helianthus tuberosus" (Jerusalem artichoke).

Plants are used as herbs and in herbal teas and other beverages. Chamomile, for example, comes from two different species: the annual "Matricaria chamomilla" (German chamomile) and the perennial "Chamaemelum nobile" (Roman chamomile). "Calendula" (known as pot marigold) is grown commercially for herbal teas and potpourri. "Echinacea" is used as a medicinal tea. The wormwood genus "Artemisia" includes absinthe ("A. absinthium") and tarragon ("A. dracunculus"). Winter tarragon ("Tagetes lucida"), is commonly grown and used as a tarragon substitute in climates where tarragon will not survive.

Many members of the family are grown as ornamental plants for their flowers, and some are important ornamental crops for the cut flower industry. Some examples are "Chrysanthemum", "Gerbera", "Calendula", "Dendranthema", "Argyranthemum", "Dahlia", "Tagetes", "Zinnia", and many others.

Many species of this family possess medicinal properties and are used as traditional antiparasitic medicine.

Members of the family are also commonly featured in medical and phytochemical journals because the sesquiterpene lactone compounds contained within them are an important cause of allergic contact dermatitis. Allergy to these compounds is the leading cause of allergic contact dermatitis in florists in the US. Pollen from ragweed "Ambrosia" is among the main causes of so-called hay fever in the United States.

Asteraceae are also used for some industrial purposes. French Marigold ("Tagetes patula") is common in commercial poultry feeds and its oil is extracted for uses in cola and the cigarette industry.

Several members of the family are copious nectar producers and are useful for evaluating pollinator populations during their bloom. "Centaurea" (knapweed), "Helianthus annuus" (domestic sunflower), and some species of "Solidago" (goldenrod) are major "honey plants" for beekeepers. "Solidago" produces relatively high protein pollen, which helps honey bees over winter.

Some members of Asteraceae are economically important as weeds. Notable in the United States are "Senecio jacobaea" (ragwort), "Senecio vulgaris" (groundsel), and "Taraxacum" (dandelion).

The genera "Chrysanthemum", "Pulicaria", "Tagetes", and "Tanacetum" contain species with useful insecticidal properties.

"Parthenium argentatum" (guayule) is a source of hypoallergenic latex.



</doc>
<doc id="957" url="https://en.wikipedia.org/wiki?curid=957" title="Apiaceae">
Apiaceae

Apiaceae or Umbelliferae is a family of mostly aromatic flowering plants named after the type genus "Apium" and commonly known as the celery, carrot or parsley family, or simply as umbellifers. It is the 16th-largest family of flowering plants, with more than 3,700 species in 434 genera including such well-known and economically important plants such as ajwain, angelica, anise, asafoetida, caraway, carrot, celery, chervil, coriander, cumin, dill, fennel, lovage, cow parsley, parsley, parsnip and sea holly, as well as silphium, a plant whose identity is unclear and which may be extinct.

The family Apiaceae includes a significant number of phototoxic species, such as giant hogweed, and a smaller number of highly poisonous species, such as water hemlock, poison hemlock, water dropwort and spotted cowbane.

Most Apiaceae are annual, biennial or perennial herbs (frequently with the leaves aggregated toward the base), though a minority are woody shrubs or small trees such as "Bupleurum fruticosum". Their leaves are of variable size and alternately arranged, or with the upper leaves becoming nearly opposite. The leaves may be petiolate or sessile. There are no stipules but the petioles are frequently sheathing and the leaves may be perfoliate. The leaf blade is usually dissected, ternate or pinnatifid, but simple and entire in some genera, e.g. "Bupleurum". Commonly, their leaves emit a marked smell when crushed, aromatic to foetid, but absent in some species.

The defining characteristic of this family is the inflorescence, the flowers nearly always aggregated in terminal umbels, that may be simple or more commonly compound, often umbelliform cymes. The flowers are usually perfect (hermaphroditic) and actinomorphic, but there may be zygomorphic flowers at the edge of the umbel, as in carrot ("Daucus carota") and coriander, with petals of unequal size, the ones pointing outward from the umbel larger than the ones pointing inward. Some are andromonoecious, polygamomonoecious, or even dioecious (as in "Acronema"), with a distinct calyx and corolla, but the calyx is often highly reduced, to the point of being undetectable in many species, while the corolla can be white, yellow, pink or purple. The flowers are nearly perfectly pentamerous, with five petals, sepals, and stamens.
The androecium consists of five stamens, but there is often variation in the functionality of the stamens even within a single inflorescence. Some flowers are functionally staminate (where a pistil may be present but has no ovules capable of being fertilized) while others are functionally pistillate (where stamens are present but their anthers do not produce viable pollen). Pollination of one flower by the pollen of a different flower of the same plant (geitonogamy) is common. The gynoecium consists of two carpels fused into a single, bicarpellate pistil with an inferior ovary. Stylopodia support two styles and secrete nectar, attracting pollinators like flies, mosquitoes, gnats, beetles, moths, and bees. The fruit is a schizocarp consisting of two fused carpels that separate at maturity into two mericarps, each containing a single seed. The fruits of many species are dispersed by wind but others such as those of "Daucus" spp., are covered in bristles, which may be hooked in sanicle "Sanicula europaea" and thus catch in the fur of animals. The seeds have an oily endosperm and often contain essential oils, containing aromatic compounds that are responsible for the flavour of commercially important umbelliferous seed such as anise, cumin and coriander. The shape and details of the ornamentation of the ripe fruits are important for identification to species level.

Apiaceae was first described by John Lindley in 1836. The name is derived from the type genus "Apium", which was originally used by Pliny the Elder circa 50 AD for a celery-like plant. The alternative name for the family, Umbelliferae, derives from the inflorescence being generally in the form of a compound umbel. The family was one of the first to be recognized as a distinct group in Jacques Daleschamps' 1586 "Historia generalis plantarum". With Robert Morison's 1672 "Plantarum umbelliferarum distribution nova" it became the first group of plants for which a systematic study was published.

The family is solidly placed within the Apiales order in the APG III system. It is closely related to Araliaceae and the boundaries between these families remain unclear. Traditionally groups within the family have been delimited largely based on fruit morphology, and the results from this have not been congruent with the more recent molecular phylogenetic analyses. The subfamilial and tribal classification for the family is currently in a state of flux, with many of the groups being found to be grossly paraphyletic or polyphyletic.

According to the Angiosperm Phylogeny Website , 434 genera are in the family Apiaceae.

The black swallowtail butterfly, "Papilio polyxenes", uses the family Apiaceae for food and host plants for oviposition. The 22-spot ladybird is also commonly found eating mildew on these shrubs.

Many members of this family are cultivated for various purposes. Parsnip ("Pastinaca sativa"), carrot ("Daucus carota") and Hamburg parsley ("Petroselinum crispum") produce tap roots that are large enough to be useful as food. Many species produce essential oils in their leaves or fruits and as a result are flavourful aromatic herbs. Examples are parsley ("Petroselinum crispum"), coriander ("Coriandrum sativum"), culantro, and dill ("Anethum graveolens"). The seeds may be used in cuisine, as with coriander ("Coriandrum sativum"), fennel ("Foeniculum vulgare"), cumin ("Cuminum cyminum"), and caraway ("Carum carvi").

Other notable cultivated Apiaceae include chervil ("Anthriscus cerefolium"), angelica ("Angelica" spp.), celery ("Apium graveolens"), arracacha ("Arracacia xanthorrhiza"), sea holly ("Eryngium" spp.), asafoetida ("Ferula asafoetida"), galbanum ("Ferula gummosa"), cicely ("Myrrhis odorata"), anise ("Pimpinella anisum"), lovage ("Levisticum officinale"), and hacquetia ("Hacquetia epipactis").

Generally, all members of this family are best cultivated in the cool-season garden; indeed, they may not grow at all if the soils are too warm. Almost every widely cultivated plant of this group is a considered useful as a companion plant. One reason is because the tiny flowers clustered into umbels, are well suited for ladybugs, parasitic wasps, and predatory flies, which actually drink nectar when not reproducing. They then prey upon insect pests on nearby plants. Some of the members of this family considered "herbs" produce scents that are believed to ...mask the odours of nearby plants, thus making them harder for insect pests to find.

The poisonous members of the Apiaceae have been used for a variety of purposes globally. The poisonous "Oenanthe crocata" has been used to stupefy fish, "Cicuta douglasii" has been used as an aid in suicides, and arrow poisons have been made from various other family species.

"Daucus carota" has been used as coloring for butter.

"Dorema ammoniacum", "Ferula galbaniflua", and "Ferula sumbul" are sources of incense.

The woody "Azorella compacta" Phil. has been used in South America for fuel.

Many species in the family Apiaceae produce phototoxic substances (called furanocoumarins) that sensitize human skin to sunlight. Contact with plant parts that contain furanocoumarins, followed by exposure to sunlight, may cause phytophotodermatitis, a serious skin inflammation. Of all the plant species that have been reported to induce phytophotodermatitis, approximately half belong to the family Apiaceae.

Phototoxic species include "Ammi majus", the parsnip ("Pastinaca sativa") and numerous species of the genus "Heracleum", especially the giant hogweed ("Heracleum mantegazzianum"). The family Apiaceae also includes a smaller number of poisonous species, including poison hemlock, water hemlock, and fool's parsley.

Some members of the family Apiaceae, including carrot, celery, fennel, parsley and parsnip, contain polyynes, an unusual class of organic compounds that exhibit cytotoxic effects.




</doc>
<doc id="958" url="https://en.wikipedia.org/wiki?curid=958" title="Axon">
Axon

An axon (from Greek ἄξων "áxōn", axis), or nerve fiber (or nerve fibre: see spelling differences), is a long, slender projection of a nerve cell, or neuron, in vertebrates, that typically conducts electrical impulses known as action potentials away from the nerve cell body. The function of the axon is to transmit information to different neurons, muscles, and glands. In certain sensory neurons (pseudounipolar neurons), such as those for touch and warmth, the axons are called afferent nerve fibers and the electrical impulse travels along these from the periphery to the cell body, and from the cell body to the spinal cord along another branch of the same axon. Axon dysfunction has caused many inherited and acquired neurological disorders which can affect both the peripheral and central neurons. Nerve fibers are classed into three types – group A nerve fibers, group B nerve fibers, and group C nerve fibers. Groups A and B are myelinated, and group C are unmyelinated. These groups include both sensory fibers and motor fibers. Another classification groups only the sensory fibers as Type I, Type II, Type III, and Type IV.

An axon is one of two types of cytoplasmic protrusions from the cell body of a neuron; the other type is a dendrite. Axons are distinguished from dendrites by several features, including shape (dendrites often taper while axons usually maintain a constant radius), length (dendrites are restricted to a small region around the cell body while axons can be much longer), and function (dendrites receive signals whereas axons transmit them). Some types of neurons have no axon and transmit signals from their dendrites. In some species, axons can emanate from dendrites known as axon-carrying dendrites. No neuron ever has more than one axon; however in invertebrates such as insects or leeches the axon sometimes consists of several regions that function more or less independently of each other.

Axons are covered by a membrane known as an axolemma; the cytoplasm of an axon is called axoplasm. Most axons branch, in some cases very profusely. The end branches of an axon are called telodendria. The swollen end of a telodendron is known as the axon terminal which joins the dendron or cell body of another neuron forming a synaptic connection. Axons make contact with other cells—usually other neurons but sometimes muscle or gland cells—at junctions called synapses. In some circumstances, the axon of one neuron may form a synapse with the dendrites of the same neuron, resulting in an autapse. At a synapse, the membrane of the axon closely adjoins the membrane of the target cell, and special molecular structures serve to transmit electrical or electrochemical signals across the gap. Some synaptic junctions appear along the length of an axon as it extends—these are called "en passant" ("in passing") synapses and can be in the hundreds or even the thousands along one axon. Other synapses appear as terminals at the ends of axonal branches.

A single axon, with all its branches taken together, can innervate multiple parts of the brain and generate thousands of synaptic terminals. A bundle of axons make a nerve tract in the central nervous system, and a fascicle in the peripheral nervous system. In placental mammals the largest white matter tract in the brain is the corpus callosum, formed of some 200 million axons in the human brain.

Axons are the primary transmission lines of the nervous system, and as bundles they form nerves. Some axons can extend up to one meter or more while others extend as little as one millimeter. The longest axons in the human body are those of the sciatic nerve, which run from the base of the spinal cord to the big toe of each foot. The diameter of axons is also variable. Most individual axons are microscopic in diameter (typically about one micrometer (µm) across). The largest mammalian axons can reach a diameter of up to 20 µm. The squid giant axon, which is specialized to conduct signals very rapidly, is close to 1 millimetre in diameter, the size of a small pencil lead. The numbers of axonal telodendria (the branching structures at the end of the axon) can also differ from one nerve fiber to the next. Axons in the central nervous system (CNS) typically show multiple telodendria, with many synaptic end points. In comparison, the cerebellar granule cell axon is characterized by a single T-shaped branch node from which two parallel fibers extend. Elaborate branching allows for the simultaneous transmission of messages to a large number of target neurons within a single region of the brain.

There are two types of axons in the nervous system: myelinated and unmyelinated axons. Myelin is a layer of a fatty insulating substance, which is formed by two types of glial cells Schwann cells and oligodendrocytes. In the peripheral nervous system Schwann cells form the myelin sheath of a myelinated axon. In the central nervous system oligodendrocytes form the insulating myelin. Along myelinated nerve fibers, gaps in the myelin sheath known as nodes of Ranvier occur at evenly spaced intervals. The myelination enables an especially rapid mode of electrical impulse propagation called saltatory conduction.

The myelinated axons from the cortical neurons form the bulk of the neural tissue called white matter in the brain. The myelin gives the white appearance to the tissue in contrast to the grey matter of the cerebral cortex which contains the neuronal cell bodies. A similar arrangement is seen in the cerebellum. Bundles of myelinated axons make up the nerve tracts in the CNS. Where these tracts cross the midline of the brain to connect opposite regions they are called "commissures". The largest of these is the corpus callosum that connects the two cerebral hemispheres, and this has around 20 million axons.

The structure of a neuron is seen to consist of two separate functional regions, or compartments – the cell body together with the dendrites as one region, and the axonal region as the other.

The axonal region or compartment, includes the axon hillock, the initial segment, the rest of the axon, and the axon telodendria, and axon terminals. It also includes the myelin sheath. The Nissl bodies that produce the neuronal proteins are absent in the axonal region. Proteins needed for the growth of the axon, and the removal of waste materials, need a framework for transport. This axonal transport is provided for in the axoplasm by arrangements of microtubules and intermediate filaments known as neurofilaments.

The axon hillock is the area formed from the cell body of the neuron as it extends to become the axon. It precedes the initial segment. The received action potentials that are summed in the neuron are transmitted to the axon hillock for the generation of an action potential from the initial segment.

The axonal initial segment (AIS) is a structurally and functionally separate microdomain of the axon. One function of the initial segment is to separate the main part of an axon from the rest of the neuron; another function is to help initiate action potentials. Both of these functions support neuron cell polarity, in which dendrites (and, in some cases the soma) of a neuron receive input signals at the basal region, and at the apical region the neuron's axon provides output signals.

The axon initial segment is unmyelinated and contains a specialized complex of proteins. It is between approximately 20 and 60 µm in length and functions as the site of action potential initiation. Both the position on the axon and the length of the AIS can change showing a degree of plasticity that can fine-tune the neuronal output. A longer AIS is associated with a greater excitability. Plasticity is also seen in the ability of the AIS to change its distribution and to maintain the activity of neural circuitry at a constant level.

The AIS is highly specialized for the fast conduction of nerve impulses. This is achieved by a high concentration of voltage-gated sodium channels in the initial segment where the action potential is initiated. The ion channels are accompanied by a high number of cell adhesion molecules and scaffolding proteins that anchor them to the cytoskeleton. Interactions with ankyrin G are important as it is the major organizer in the AIS.

The axoplasm is the equivalent of cytoplasm in the cell. Microtubules form in the axoplasm at the axon hillock. They are arranged along the length of the axon, in overlapping sections, and all point in the same direction – towards the axon terminals. This is noted by the positive endings of the microtubules. This overlapping arrangement provides the routes for the transport of different materials from the cell body. Studies on the axoplasm has shown the movement of numerous vesicles of all sizes to be seen along cytoskeletal filaments – the microtubules, and neurofilaments, in both directions between the axon and its terminals and the cell body.

Outgoing anterograde transport from the cell body along the axon, carries mitochondria and membrane proteins needed for growth to the axon terminal. Ingoing retrograde transport carries cell waste materials from the axon terminal to the cell body. Outgoing and ingoing tracks use different sets of motor proteins. Outgoing transport is provided by kinesin, and ingoing return traffic is provided by dynein. Dynein is minus-end directed. There are many forms of kinesin and dynein motor proteins, and each is thought to carry a different cargo. The studies on transport in the axon led to the naming of kinesin.

In the nervous system, axons may be myelinated, or unmyelinated. This is the provision of an insulating layer, called a myelin sheath. The myelin membrane is unique in its relatively high lipid to protein ratio.

In the peripheral nervous system axons are myelinated by glial cells known as Schwann cells. In the central nervous system the myelin sheath is provided by another type of glial cell, the oligodendrocyte. Schwann cells myelinate a single axon. An oligodendrocyte can myelinate up to 50 axons.

The composition of myelin is different in the two types. In the CNS the major myelin protein is proteolipid protein, and in the PNS it is myelin basic protein.

Nodes of Ranvier (also known as "myelin sheath gaps") are short unmyelinated segments of a myelinated axon, which are found periodically interspersed between segments of the myelin sheath. Therefore, at the point of the node of Ranvier, the axon is reduced in diameter. These nodes are areas where action potentials can be generated. In saltatory conduction, electrical currents produced at each node of Ranvier are conducted with little attenuation to the next node in line, where they remain strong enough to generate another action potential. Thus in a myelinated axon, action potentials effectively "jump" from node to node, bypassing the myelinated stretches in between, resulting in a propagation speed much faster than even the fastest unmyelinated axon can sustain.

An axon can divide into many branches called telodendria (Greek–end of tree). At the end of each telodendron is an axon terminal (also called a synaptic bouton, or terminal bouton). Axon terminals contain synaptic vesicles that store the neurotransmitter for release at the synapse. This makes multiple synaptic connections with other neurons possible. Sometimes the axon of a neuron may synapse onto dendrites of the same neuron, when it is known as an autapse.

Most axons carry signals in the form of action potentials, which are discrete electrochemical impulses that travel rapidly along an axon, starting at the cell body and terminating at points where the axon makes synaptic contact with target cells. The defining characteristic of an action potential is that it is "all-or-nothing" — every action potential that an axon generates has essentially the same size and shape. This all-or-nothing characteristic allows action potentials to be transmitted from one end of a long axon to the other without any reduction in size. There are, however, some types of neurons with short axons that carry graded electrochemical signals, of variable amplitude.

When an action potential reaches a presynaptic terminal, it activates the synaptic transmission process. The first step is rapid opening of calcium ion channels in the membrane of the axon, allowing calcium ions to flow inward across the membrane. The resulting increase in intracellular calcium concentration causes synaptic vesicles (tiny containers enclosed by a lipid membrane) filled with a neurotransmitter chemical to fuse with the axon's membrane and empty their contents into the extracellular space. The neurotransmitter is released from the presynaptic nerve through exocytosis. The neurotransmitter chemical then diffuses across to receptors located on the membrane of the target cell. The neurotransmitter binds to these receptors and activates them. Depending on the type of receptors that are activated, the effect on the target cell can be to excite the target cell, inhibit it, or alter its metabolism in some way. This entire sequence of events often takes place in less than a thousandth of a second. Afterward, inside the presynaptic terminal, a new set of vesicles is moved into position next to the membrane, ready to be released when the next action potential arrives. The action potential is the final electrical step in the integration of synaptic messages at the scale of the neuron.

Extracellular recordings of action potential propagation in axons has been demonstrated in freely moving animals. While extracellular somatic action potentials have been used to study cellular activity in freely moving animals such as place cells, axonal activity in both white and gray matter can also be recorded. Extracellular recordings of axon action potential propagation is distinct from somatic action potentials in three ways: 1. The signal has a shorter peak-trough duration (~150μs) than of pyramidal cells (~500μs) or interneurons (~250μs). 2. The voltage change is triphasic. 3. Activity recorded on a tetrode is seen on only one of the four recording wires. In recordings from freely moving rats, axonal signals have been isolated in white matter tracts including the alveus and the corpus callosum as well hippocampal gray matter.

In fact, the generation of action potentials in vivo is sequential in nature, and these sequential spikes constitute the digital codes in the neurons. Although previous studies indicate an axonal origin of a single spike evoked by short-term pulses, physiological signals in vivo trigger the initiation of sequential spikes at the cell bodies of the neurons.

In addition to propagating action potentials to axonal terminals, the axon is able to amplify the action potentials, which makes sure a secure propagation of sequential action potentials toward the axonal terminal. In terms of molecular mechanisms, voltage-gated sodium channels in the axons possess lower threshold and shorter refractory period in response to short-term pulses.

The development of the axon to its target, is one of the six major stages in the overall development of the nervous system. Studies done on cultured hippocampal neurons suggest that neurons initially produce multiple neurites that are equivalent, yet only one of these neurites is destined to become the axon. It is unclear whether axon specification precedes axon elongation or vice versa, although recent evidence points to the latter. If an axon that is not fully developed is cut, the polarity can change and other neurites can potentially become the axon. This alteration of polarity only occurs when the axon is cut at least 10 μm shorter than the other neurites. After the incision is made, the longest neurite will become the future axon and all the other neurites, including the original axon, will turn into dendrites. Imposing an external force on a neurite, causing it to elongate, will make it become an axon. Nonetheless, axonal development is achieved through a complex interplay between extracellular signaling, intracellular signaling and cytoskeletal dynamics.

The extracellular signals that propagate through the extracellular matrix surrounding neurons play a prominent role in axonal development. These signaling molecules include proteins, neurotrophic factors, and extracellular matrix and adhesion molecules. 
Netrin (also known as UNC-6) a secreted protein, functions in axon formation. When the UNC-5 netrin receptor is mutated, several neurites are irregularly projected out of neurons and finally a single axon is extended anteriorly. The neurotrophic factors – nerve growth factor (NGF), brain-derived neurotrophic factor (BDNF) and neurotrophin-3 (NTF3) are also involved in axon development and bind to Trk receptors.

The ganglioside-converting enzyme plasma membrane ganglioside sialidase (PMGS), which is involved in the activation of TrkA at the tip of neutrites, is required for the elongation of axons. PMGS asymmetrically distributes to the tip of the neurite that is destined to become the future axon.

During axonal development, the activity of PI3K is increased at the tip of destined axon. Disrupting the activity of PI3K inhibits axonal development. Activation of PI3K results in the production of phosphatidylinositol (3,4,5)-trisphosphate (PtdIns) which can cause significant elongation of a neurite, converting it into an axon. As such, the overexpression of phosphatases that dephosphorylate PtdIns leads into the failure of polarization.

The neurite with the lowest actin filament content will become the axon. PGMS concentration and f-actin content are inversely correlated; when PGMS becomes enriched at the tip of a neurite, its f-actin content is substantially decreased. In addition, exposure to actin-depolimerizing drugs and toxin B (which inactivates Rho-signaling) causes the formation of multiple axons. Consequently, the interruption of the actin network in a growth cone will promote its neurite to become the axon.

Growing axons move through their environment via the growth cone, which is at the tip of the axon. The growth cone has a broad sheet-like extension called a lamellipodium which contain protrusions called filopodia. The filopodia are the mechanism by which the entire process adheres to surfaces and explores the surrounding environment. Actin plays a major role in the mobility of this system. Environments with high levels of cell adhesion molecules (CAMs) create an ideal environment for axonal growth. This seems to provide a "sticky" surface for axons to grow along. Examples of CAM's specific to neural systems include N-CAM, TAG-1—an axonal glycoprotein——and MAG, all of which are part of the immunoglobulin superfamily. Another set of molecules called extracellular matrix-adhesion molecules also provide a sticky substrate for axons to grow along. Examples of these molecules include laminin, fibronectin, tenascin, and perlecan. Some of these are surface bound to cells and thus act as short range attractants or repellents. Others are difusible ligands and thus can have long range effects.

Cells called guidepost cells assist in the guidance of neuronal axon growth. These cells that help axon guidance, are typically other neurons that are sometimes immature. When the axon has completed its growth at its connection to the target, the diameter of the axon can increase by up to five times, depending on the speed of conduction required.

It has also been discovered through research that if the axons of a neuron were damaged, as long as the soma (the cell body of a neuron) is not damaged, the axons would regenerate and remake the synaptic connections with neurons with the help of guidepost cells. This is also referred to as neuroregeneration.

Nogo-A is a type of neurite outgrowth inhibitory component that is present in the central nervous system myelin membranes (found in an axon). It has a crucial role in restricting axonal regeneration in adult mammalian central nervous system. In recent studies, if Nogo-A is blocked and neutralized, it is possible to induce long-distance axonal regeneration which leads to enhancement of functional recovery in rats and mouse spinal cord. This has yet to be done on humans. A recent study has also found that macrophages activated through a specific inflammatory pathway activated by the Dectin-1 receptor are capable of promoting axon recovery, also however causing neurotoxicity in the neuron.

Axons vary largely in length from a few micrometers up to meters in some animals. This emphasizes that there must be a cellular length regulation mechanism allowing the neurons both to sense the length of their axons and to control their growth accordingly. It was discovered that motor proteins play an important role in regulating the length of axons. Based on this observation, researchers developed an explicit model for axonal growth describing how motor proteins could affect the axon length on the molecular level. These studies suggest that motor proteins carry signaling molecules from the soma to the growth cone and vice versa whose concentration oscillates in time with a length-dependent frequency.

The axons of neurons in the human peripheral nervous system can be classified based on their physical features and signal conduction properties. Axons were known to have different thicknesses (from 0.1 to 20 µm) and these differences were thought to relate to the speed at which an action potential could travel along the axon – its "conductance velocity". Erlanger and Gasser proved this hypothesis, and identified several types of nerve fiber, establishing a relationship between the diameter of an axon and its nerve conduction velocity. They published their findings in 1941 giving the first classification of axons.

Axons are classified in two systems. The first one introduced by Erlanger and Gasser, grouped the fibers into three main groups using the letters A, B, and C. These groups, group A, group B, and group C include both the sensory fibers (afferents) and the motor fibres (efferents). The first group A, was subdivided into alpha, beta, gamma, and delta fibers — Aα, Aβ, Aγ, and Aδ. The motor neurons of the different motor fibers, were the lower motor neurons – alpha motor neuron, beta motor neuron, and gamma motor neuron having the Aα, Aβ, and Aγ nerve fibers respectively.

Later findings by other researchers identified two groups of Aa fibers that were sensory fibers. These were then introduced into a system that only included sensory fibers (though some of these were mixed nerves and were also motor fibers). This system refers to the sensory groups as Types and uses Roman numerals: Type Ia, Type Ib, Type II, Type III, and Type IV.
Lower motor neurons have two kind of fibers:

Different sensory receptors innervate different types of nerve fibers. Proprioceptors are innervated by type Ia, Ib and II sensory fibers, mechanoreceptors by type II and III sensory fibers and nociceptors and thermoreceptors by type III and IV sensory fibers.

The autonomic nervous system has two kinds of peripheral fibers:

In order of degree of severity, injury to a nerve can be described as neurapraxia, axonotmesis, or neurotmesis.
Concussion is considered a mild form of diffuse axonal injury. Axonal injury can also cause central chromatolysis. The dysfunction of axons in the nervous system is one of the major causes of many inherited neurological disorders that affect both peripheral and central neurons.

When an axon is crushed, an active process of axonal degeneration takes place at the part of the axon furthest from the cell body. This degeneration takes place quickly following the injury, with the part of the axon being sealed off at the membranes and broken down by macrophages. This is known as Wallerian degeneration. Dying back of an axon can also take place in many neurodegenerative diseases, particularly when axonal transport is impaired, this is known as Wallerian-like degeneration. Studies suggest that the degeneration happens as
a result of the axonal protein NMNAT2, being prevented from reaching all of the axon.
Demyelination of axons causes the multitude of neurological symptoms found in the disease multiple sclerosis.

Dysmyelination is the abnormal formation of the myelin sheath. This is implicated in several leukodystrophies, and also in schizophrenia.

A severe traumatic brain injury can result in widespread lesions to nerve tracts damaging the axons in a condition known as diffuse axonal injury. This can lead to a persistent vegetative state. It has been shown in studies on the rat that axonal damage from a single mild traumatic brain injury, can leave a susceptibility to further damage, after repeated mild traumatic brain injuries.

A nerve guidance conduit is an artificial means of guiding axon growth to enable neuroregeneration, and is one of the many treatments used for different kinds of nerve injury.

German anatomist Otto Friedrich Karl Deiters is generally credited with the discovery of the axon by distinguishing it from the dendrites. Swiss Rüdolf Albert von Kölliker and German Robert Remak were the first to identify and characterize the axon initial segment. Kölliker named the axon in 1896. Louis-Antoine Ranvier was the first to describe the gaps or nodes found on axons and for this contribution these axonal features are now commonly referred to as the nodes of Ranvier. Santiago Ramón y Cajal, a Spanish anatomist, proposed that axons were the output components of neurons, describing their functionality. Joseph Erlanger and Herbert Gasser earlier developed the classification system for peripheral nerve fibers, based on axonal conduction velocity, myelination, fiber size etc. Alan Hodgkin and Andrew Huxley also employed the squid giant axon (1939) and by 1952 they had obtained a full quantitative description of the ionic basis of the action potential, leading to the formulation of the Hodgkin–Huxley model. Hodgkin and Huxley were awarded jointly the Nobel Prize for this work in 1963. The formulae detailing axonal conductance were extended to vertebrates in the Frankenhaeuser–Huxley equations. The understanding of the biochemical basis for action potential propagation has advanced further, and includes many details about individual ion channels.

The axons in invertebrates have been extensively studied. The longfin inshore squid, often used as a model organism has the longest known axon. The giant squid has the largest axon known. Its size ranges from a half (typically) to one millimetre in diameter and is used in the control of its jet propulsion system. The fastest recorded conduction speed of 210 m/s, is found in the ensheathed axons of some pelagic Penaeid shrimps and the usual range is between 90 and 200 m/s (cf 100–120 m/s for the fastest myelinated vertebrate axon.)

In other cases as seen in rat studies an axon originates from a dendrite; such axons are said to have "dendritic origin". Some axons with dendritic origin similarly have a "proximal" initial segment that starts directly at the axon origin, while others have a "distal" initial segment, discernibly separated from the axon origin. In many species some of the neurons have axons that emanate from the dendrite and not from the cell body, and these are known as axon-carrying dendrites. In many cases, an axon originates at an axon hillock on the soma; such axons are said to have "somatic origin". Some axons with somatic origin have a "proximal" initial segment adjacent the axon hillock, while others have a "distal" initial segment, separated from the soma by an extended axon hillock.




</doc>
<doc id="960" url="https://en.wikipedia.org/wiki?curid=960" title="Aramaic alphabet">
Aramaic alphabet

The ancient Aramaic alphabet was adapted by Arameans from the Phoenician alphabet and became a distinct script by the 8th century BC. It was used to write the Aramaic language and had displaced the Paleo-Hebrew alphabet, itself a derivative of the Phoenician alphabet, for the writing of Hebrew. The letters all represent consonants, some of which are also used as "matres lectionis" to indicate long vowels.

The Aramaic alphabet is historically significant since virtually all modern Middle Eastern writing systems can be traced back to it as well as numerous non-Chinese writing systems of Central and East Asia. That is primarily from the widespread usage of the Aramaic language as both a "lingua franca" and the official language of the Neo-Assyrian and Neo-Babylonian Empires, and their successor, the Achaemenid Empire. Among the scripts in modern use, the Hebrew alphabet bears the closest relation to the Imperial Aramaic script of the 5th century BC, with an identical letter inventory and, for the most part, nearly identical letter shapes. The Aramaic alphabet was an ancestor to the Nabataean alphabet and the later Arabic alphabet.

Writing systems (like the Aramaic one) that indicate consonants but do not indicate most vowels other than by means of "matres lectionis" or added diacritical signs, have been called abjads by Peter T. Daniels to distinguish them from alphabets, such as the Greek alphabet, which represent vowels more systematically. The term was coined to avoid the notion that a writing system that represents sounds must be either a syllabary or an alphabet, which would imply that a system like Aramaic must be either a syllabary (as argued by Ignace Gelb) or an incomplete or deficient alphabet (as most other writers have said). Rather, it is a different type.

The earliest inscriptions in the Aramaic language use the Phoenician alphabet. Over time, the alphabet developed into the form shown below. Aramaic gradually became the "lingua franca" throughout the Middle East, with the script at first complementing and then displacing Assyrian cuneiform, as the predominant writing system.

Around 500 BC, following the Achaemenid conquest of Mesopotamia under Darius I, Old Aramaic was adopted by the Persians as the "vehicle for written communication between the different regions of the vast Persian empire with its different peoples and languages. The use of a single official language, which modern scholarship has dubbed as Official Aramaic, Imperial Aramaic or Achaemenid Aramaic, can be assumed to have greatly contributed to the astonishing success of the Achaemenid Persians in holding their far-flung empire together for as long as they did."

Imperial Aramaic was highly standardised; its orthography was based more on historical roots than any spoken dialect and was inevitably influenced by Old Persian. The Aramaic glyph forms of the period are often divided into two main styles, the "lapidary" form, usually inscribed on hard surfaces like stone monuments, and a cursive form whose lapidary form tended to be more conservative by remaining more visually similar to Phoenician and early Aramaic. Both were in use through the Achaemenid Persian period, but the cursive form steadily gained ground over the lapidary, which had largely disappeared by the 3rd century BC.
For centuries after the fall of the Achaemenid Empire in 331 BC, Imperial Aramaic, or something near enough to it to be recognisable, would remain an influence on the various native Iranian languages. The Aramaic script would survive as the essential characteristics of the Iranian Pahlavi writing system.

30 Aramaic documents from Bactria have been recently discovered, an analysis of which was published in November 2006. The texts, which were rendered on leather, reflect the use of Aramaic in the 4th century BC in the Persian Achaemenid administration of Bactria and Sogdiana.

The widespread usage of Achaemenid Aramaic in the Middle East led to the gradual adoption of the Aramaic alphabet for writing Hebrew. Formerly, Hebrew had been written using an alphabet closer in form to that of Phoenician, the Paleo-Hebrew alphabet.

Since the evolution of the Aramaic alphabet out of the Phoenician one was a gradual process, the division of the world's alphabets into the ones derived from the Phoenician one directly and the ones derived from Phoenician via Aramaic is somewhat artificial. In general, the alphabets of the Mediterranean region (Anatolia, Greece, Italy) are classified as Phoenician-derived, adapted from around the 8th century BC, and those of the East (the Levant, Persia, Central Asia and India) are considered Aramaic-derived, adapted from around the 6th century BC from the Imperial Aramaic script of the Achaemenid Empire.

After the fall of the Achaemenid Empire, the unity of the Imperial Aramaic script was lost, diversifying into a number of descendant cursives.

The Hebrew and Nabataean alphabets, as they stood by the Roman era, were little changed in style from the Imperial Aramaic alphabet. Ibn Khaldun (1332–1406) alleges that not only the old Nabataean writing was influenced by the "Syrian script" (i.e. Aramaic), but also the old Chaldean script.

A cursive Hebrew variant developed from the early centuries AD, but it remained restricted to the status of a variant used alongside the noncursive. By contrast, the cursive developed out of the Nabataean alphabet in the same period soon became the standard for writing Arabic, evolving into the Arabic alphabet as it stood by the time of the early spread of Islam.

The development of cursive versions of Aramaic also led to the creation of the Syriac, Palmyrene and Mandaic alphabets, which formed the basis of the historical scripts of Central Asia, such as the Sogdian and Mongolian alphabets.

The Old Turkic script is generally considered to have its ultimate origins in Aramaic, in particular via the Pahlavi or Sogdian alphabets, as suggested by V. Thomsen, or possibly via Kharosthi ("cf"., Issyk inscription).

Aramaic is also considered to be the most likely source of the Brahmi script, ancestor of the Brahmic family of scripts, which includes Devanagari.

Today, Biblical Aramaic, Jewish Neo-Aramaic dialects and the Aramaic language of the Talmud are written in the modern-Hebrew alphabet (distinguished from the Old Hebrew script). In classical Jewish literature, the name given to the modern-Hebrew script was "Ashurit" (the ancient Assyrian script), a script now known widely as the Aramaic script. It is believed that during the period of Assyrian dominion that Aramaic script and language received official status. Syriac and Christian Neo-Aramaic dialects are today written in the Syriac alphabet, which script has superseded the more ancient Assyrian script and now bears its name. Mandaic is written in the Mandaic alphabet. The near-identity of the Aramaic and the classical Hebrew alphabets caused Aramaic text to be typeset mostly in the standard Hebrew script in scholarly literature.

In Maaloula, one of few surviving communities in which a Western Aramaic dialect is still spoken, an Aramaic institute was established in 2007 by Damascus University that teaches courses to keep the language alive. The institute's activities were suspended in 2010 amidst fears that the square Aramaic alphabet used in the program too closely resembled the square script of the Hebrew alphabet and all the signs with the square Aramaic script were taken down. The program stated that they would instead use the more distinct Syriac alphabet, although use of the Aramaic alphabet has continued to some degree. Al Jazeera Arabic also broadcast a program about Western Neo-Aramaic and the villages in which it is spoken with the square script still in use.

In Aramaic writing, Waw and Yodh serve a double function. Originally, they represented only the consonants "w" and "y", but they were later adopted to indicate the long vowels "ū" and "ī" respectively as well (often also "ō" and "ē" respectively). In the latter role, they are known as " or "mothers of reading".

Ālap, likewise, has some of the characteristics of a " because in initial positions, it indicates a glottal stop (followed by a vowel), but otherwise, it often also stands for the long vowels "ā" or "ē". Among Jews, the influence of Hebrew often led to the use of Hē instead, at the end of a word.

The practice of using certain letters to hold vowel values spread to Aramaic-derived writing systems, such as in Arabic and Hebrew, which still follow the practice.

The Syriac Aramaic alphabet was added to the Unicode Standard in September 1999, with the release of version 3.0.

The Syriac Abbreviation (a type of overline) can be represented with a special control character called the Syriac Abbreviation Mark (U+070F). The Unicode block for Syriac Aramaic is U+0700–U+074F:

The Imperial Aramaic alphabet was added to the Unicode Standard in October 2009, with the release of version 5.2.

The Unicode block for Imperial Aramaic is U+10840–U+1085F:





</doc>
<doc id="966" url="https://en.wikipedia.org/wiki?curid=966" title="American shot">
American shot

"American shot" is a translation of a phrase from French film criticism, "plan américain," and refers to a medium-long ("knee") film shot of a group of characters, who are arranged so that all are visible to the camera. The usual arrangement is for the actors to stand in an irregular line from one side of the screen to the other, with the actors at the end coming forward a little and standing more in profile than the others. The purpose of the composition is to allow complex dialogue scenes to be played out without changes in camera position. In some literature, this is simply referred to as a 3/4 shot.

One of the other main reasons why French critics called it "American shot" was its frequent use in westerns. This was because a shot that started at knee level would reveal the weapon of a cowboy, usually holstered at his waist. It's actually the closest you can get to an actor while keeping both his face and his holstered gun in frame.

The French critics thought it was characteristic of American films of the 1930s or 1940s; however, it was mostly characteristic of "cheaper" American movies, such as Charlie Chan mysteries where people collected in front of a fireplace or at the foot of the stairs in order to explain what happened a few minutes ago.

Howard Hawks legitimized this style in his films, allowing characters to act, even when not talking, when most of the audience would not be paying attention. It became his trademark style.


</doc>
<doc id="967" url="https://en.wikipedia.org/wiki?curid=967" title="Acute disseminated encephalomyelitis">
Acute disseminated encephalomyelitis

Acute disseminated encephalomyelitis (ADEM), or acute demyelinating encephalomyelitis, is a rare autoimmune disease marked by a sudden, widespread attack of inflammation in the brain and spinal cord. As well as causing the brain and spinal cord to become inflamed, ADEM also attacks the nerves of the central nervous system and damages their myelin insulation, which, as a result, destroys the white matter. It is often triggered by a viral infection or (very rarely) specific non-routine vaccinations.

ADEM's symptoms resemble the symptoms of multiple sclerosis (MS), so the disease itself is sorted into the classification of the multiple sclerosis borderline diseases. However, ADEM has several features that distinguish it from MS. Unlike MS, ADEM occurs usually in children and is marked with rapid fever, although adolescents and adults can get the disease too. ADEM consists of a single flare-up whereas MS is marked with several flare-ups (or relapses), over a long period of time. Relapses following ADEM are reported in up to a quarter of patients, but the majority of these 'multiphasic' presentations following ADEM likely represent MS. ADEM is also distinguished by a loss of consciousness, coma and death, which is very rare in MS, except in severe cases.

It affects about 8 per 1,000,000 people per year. Although it occurs in all ages, most reported cases are in children and adolescents, with the average age around 5 to 8 years old. The disease affects males and females almost equally. ADEM shows seasonal variation with higher incidence in winter and spring months which may coincide with higher viral infections during these months. The mortality rate may be as high as 5%; however, full recovery is seen in 50 to 75% of cases with increase in survival rates up to 70 to 90% with figures including minor residual disability as well. The average time to recover from ADEM flare-ups is one to six months.

ADEM produces multiple inflammatory lesions in the brain and spinal cord, particularly in the white matter. Usually these are found in the subcortical and central white matter and cortical gray-white junction of both cerebral hemispheres, cerebellum, brainstem, and spinal cord, but periventricular white matter and gray matter of the cortex, thalami and basal ganglia may also be involved.

When a person has more than one demyelinating episode of ADEM, the disease is then called recurrent disseminated encephalomyelitis or multiphasic disseminated encephalomyelitis (MDEM). Also, a fulminant course in adults has been described.

ADEM has an abrupt onset and a monophasic course. Symptoms usually begin 1–3 weeks after infection. Major symptoms include fever, headache, nausea and vomiting, confusion, vision impairment, drowsiness, seizures and coma. Although initially the symptoms are usually mild, they worsen rapidly over the course of hours to days, with the average time to maximum severity being about four and a half days. Additional symptoms include hemiparesis, paraparesis, and cranial nerve palsies.

Since the discovery of the anti-MOG specificity against multiple sclerosis diagnosis it is considered that ADEM is one of the possible clinical causes of anti-MOG associated encephalomyelitis

About how the anti-MOG antibodies appear in the patients serum there are several theories:

ADEM term has been inconsistently used at different times Currently, the commonly accepted international standard for the clinical case definition is the one published by the International Pediatric MS Study Group, revision 2007.

Given that the definition is clinical, it is currently unknown if all the cases with ADEM are positive for anti-MOG autoantibody, but in any case it seems strongly related to ADEM diagnosis.

While ADEM and MS both involve autoimmune demyelination, they differ in many clinical, genetic, imaging, and histopathological aspects. Some authors consider MS and its borderline forms to constitute a spectrum, differing only in chronicity, severity, and clinical course, while others consider them discretely different diseases.

Typically, ADEM appears in children following an antigenic challenge and remains monophasic. Nevertheless, ADEM does occur in adults, and can also be clinically multiphasic.

Problems for differential diagnosis increase due to the lack of agreement for a definition of multiple sclerosis. If MS were defined just by the separation in time and space of the demyelinating lesions as McDonald did, it would not be enough to make a difference, as some cases of ADEM satisfy these conditions. Therefore, some authors propose to establish the separation line in the shape of the lesions around the veins, being therefore "perivenous vs. confluent demyelination".

The pathology of ADEM is very similar to that of MS with some differences. The pathological hallmark of ADEM is perivenular inflammation with limited "sleeves of demyelination". Nevertheless, MS-like plaques (confluent demyelination) can appear

Plaques in the white matter in MS are sharply delineated, while the glial scar in ADEM is smooth. Axons are better preserved in ADEM lesions. Inflammation in ADEM is widely disseminated and ill-defined, and finally, lesions are strictly perivenous, while in MS they are disposed around veins, but not so sharply.

Nevertheless, the co-occurrence of perivenous and confluent demyelination in some individuals suggests pathogenic overlap between acute disseminated encephalomyelitis and multiple sclerosis and misclassification even with biopsy or even postmortem ADEM in adults can progress to MS

When the person has more than one demyelinating episode of ADEM, the disease is then called recurrent disseminated encephalomyelitis or multiphasic disseminated encephalomyelitis (MDEM).

It has been found that anti-MOG auto-antibodies are related to this kind of ADEM

Another variant of ADEM in adults has been described, also related to anti-MOG auto-antibodies, has been named fulminant disseminated encephalomyelitis, and it has been reported to be clinically ADEM, but showing MS-like lesions on autopsy. It has been classified inside the anti-MOG associated inflammatory demyelinating diseases.

Acute hemorrhagic leukoencephalitis (AHL, or AHLE), acute hemorrhagic encephalomyelitis (AHEM), acute necrotizing hemorrhagic leukoencephalitis (ANHLE), Weston-Hurst syndrome, or Hurst's disease, is a hyperacute and frequently fatal form of ADEM. AHL is relatively rare (less than 100 cases have been reported in the medical literature ), it is seen in about 2% of ADEM cases, and is characterized by necrotizing vasculitis of venules and hemorrhage, and edema. Death is common in the first week and overall mortality is about 70%, but increasing evidence points to favorable outcomes after aggressive treatment with corticosteroids, immunoglobulins, cyclophosphamide, and plasma exchange. About 70% of survivors show residual neurological deficits, but some survivors have shown surprisingly little deficit considering the magnitude of the white matter affected.

This disease has been occasionally associated with ulcerative colitis and Crohn's disease, malaria, sepsis associated with immune complex deposition, methanol poisoning, and other underlying conditions. Also anecdotal association with MS has been reported

Laboratory studies that support diagnosis of AHL are: peripheral leukocytosis, cerebrospinal fluid (CSF) pleocytosis associated with normal glucose and increased protein. On magnetic resonance imaging (MRI), lesions of AHL typically show extensive T2-weighted and fluid-attenuated inversion recovery (FLAIR) white matter hyperintensities with areas of hemorrhages, significant edema, and mass effect.

No controlled clinical trials have been conducted on ADEM treatment, but aggressive treatment aimed at rapidly reducing inflammation of the CNS is standard. The widely accepted first-line treatment is high doses of intravenous corticosteroids, such as methylprednisolone or dexamethasone, followed by 3–6 weeks of gradually lower oral doses of prednisolone. Patients treated with methylprednisolone have shown better outcomes than those treated with dexamethasone. Oral tapers of less than three weeks duration show a higher chance of relapsing, and tend to show poorer outcomes. Other anti-inflammatory and immunosuppressive therapies have been reported to show beneficial effect, such as plasmapheresis, high doses of intravenous immunoglobulin (IVIg), mitoxantrone and cyclophosphamide. These are considered alternative therapies, used when corticosteroids cannot be used or fail to show an effect.

There is some evidence to suggest that patients may respond to a combination of methylprednisolone and immunoglobulins if they fail to respond to either separately
In a study of 16 children with ADEM, 10 recovered completely after high-dose methylprednisolone, one severe case that failed to respond to steroids recovered completely after IV Ig; the five most severe cases -with ADAM and severe peripheral neuropathy- were treated with combined high-dose methylprednisolone and immunoglobulin, two remained paraplegic, one had motor and cognitive handicaps, and two recovered. A recent review of IVIg treatment of ADEM (of which the previous study formed the bulk of the cases) found that 70% of children showed complete recovery after treatment with IVIg, or IVIg plus corticosteroids. A study of IVIg treatment in adults with ADEM showed that IVIg seems more effective in treating sensory and motor disturbances, while steroids seem more effective in treating impairments of cognition, consciousness and rigor. This same study found one subject, a 71-year-old man who had not responded to steroids, that responded to an IVIg treatment 58 days after disease onset.

Full recovery is seen in 50 to 70% of cases, ranging to 70 to 90% recovery with some minor residual disability (typically assessed using measures such as mRS or EDSS), average time to recover is one to six months. The mortality rate may be as high as 5%-10%. Poorer outcomes are associated with unresponsiveness to steroid therapy, unusually severe neurological symptoms, or sudden onset. Children tend to have more favorable outcomes than adults, and cases presenting without fevers tend to have poorer outcomes. The latter effect may be due to either protective effects of fever, or that diagnosis and treatment is sought more rapidly when fever is present.

ADEM can progress to MS. It will be considered MS if some lesions appear in different times and brain areas

Residual motor deficits are estimated to remain in about 8 to 30% of cases, the range in severity from mild clumsiness to ataxia and hemiparesis.

Patients with demyelinating illnesses, such as MS, have shown cognitive deficits even when there is minimal physical disability. Research suggests that similar effects are seen after ADEM, but that the deficits are less severe than those seen in MS. A study of six children with ADEM (mean age at presentation 7.7 years) were tested for a range of neurocognitive tests after an average of 3.5 years of recovery. All six children performed in the normal range on most tests, including verbal IQ and performance IQ, but performed at least one standard deviation below age norms in at least one cognitive domain, such as complex attention (one child), short-term memory (one child) and internalizing behaviour/affect (two children). Group means for each cognitive domain were all within one standard deviation of age norms, demonstrating that, as a group, they were normal. These deficits were less severe than those seen in similar aged children with a diagnosis of MS.

Another study compared nineteen children with a history of ADEM, of which 10 were five years of age or younger at the time (average age 3.8 years old, tested an average of 3.9 years later) and nine were older (mean age 7.7y at time of ADEM, tested an average of 2.2 years later) to nineteen matched controls. Scores on IQ tests and educational achievement were lower for the young onset ADEM group (average IQ 90) compared to the late onset (average IQ 100) and control groups (average IQ 106), while the late onset ADEM children scored lower on verbal processing speed. Again, all groups means were within one standard deviation of the controls, meaning that while effects were statistically reliable, the children were as a whole, still within the normal range. There were also more behavioural problems in the early onset group, although there is some suggestion that this may be due, at least in part, to the stress of hospitalization at a young age.

The relationship between ADEM and anti-MOG associated encephalomyelitis is currently under research. A new entity called MOGDEM has been proposed.

About animal models, the main animal model for MS, experimental autoimmune encephalomyelitis (EAE) is also an animal model for ADEM. Being an acute monophasic illness, EAE is far more similar to ADEM than MS.




</doc>
<doc id="969" url="https://en.wikipedia.org/wiki?curid=969" title="Ataxia">
Ataxia

Ataxia is a neurological sign consisting of lack of voluntary coordination of muscle movements that can include gait abnormality, speech changes, and abnormalities in eye movements. Ataxia is a clinical manifestation indicating dysfunction of the parts of the nervous system that coordinate movement, such as the cerebellum. Ataxia can be limited to one side of the body, which is referred to as hemiataxia. Several possible causes exist for these patterns of neurological dysfunction. Dystaxia is a mild degree of ataxia. Friedreich's ataxia has gait abnormality as the most commonly presented symptom. The word is from Greek "α-" [a negative prefix] + "-τάξις" [order] = "lack of order".

The term cerebellar ataxia is used to indicate ataxia that is due to dysfunction of the cerebellum. The cerebellum is responsible for integrating a significant amount of neural information that is used to coordinate smoothly ongoing movements and to participate in motor planning. Although ataxia is not present with all cerebellar lesions, many conditions affecting the cerebellum do produce ataxia. People with cerebellar ataxia may have trouble regulating the force, range, direction, velocity and rhythm of muscle contractions. This results in a characteristic type of irregular, uncoordinated movement that can manifest itself in many possible ways, such as asthenia, asynergy, delayed reaction time, and dyschronometria. Individuals with cerebellar ataxia could also display instability of gait, difficulty with eye movements, dysarthria, dysphagia, hypotonia, dysmetria and dysdiadochokinesia. These deficits can vary depending on which cerebellar structures have been damaged, and whether the lesion is bilateral or unilateral.

People with cerebellar ataxia may initially present with poor balance, which could be demonstrated as an inability to stand on one leg or perform tandem gait. As the condition progresses, walking is characterized by a widened base and high stepping, as well as staggering and lurching from side to side. Turning is also problematic and could result in falls. As cerebellar ataxia becomes severe, great assistance and effort are needed to stand and walk. Dysarthria, an impairment with articulation, may also be present and is characterized by "scanning" speech that consists of slower rate, irregular rhythm and variable volume. There may also be slurring of speech, tremor of the voice and ataxic respiration. Cerebellar ataxia could result with incoordination of movement, particularly in the extremities. There is overshooting (or hypermetria) with finger to nose testing, and heel to shin testing; thus, dysmetria is evident. Impairments with alternating movements (dysdiadochokinesia), as well as dysrhythmia, may also be displayed. There may also be tremor of the head and trunk (titubation) in individuals with cerebellar ataxia.

It is thought that dysmetria is caused by a deficit in the control of interaction torques in multijoint motion. Interaction torques are created at an associated joint when the primary joint is moved. For example, if a movement required reaching to touch a target in front of the body, flexion at the shoulder would create a torque at the elbow, while extension of the elbow would create a torque at the wrist. These torques increase as the speed of movement increases and must be compensated and adjusted for to create coordinated movement. This may, therefore, explain decreased coordination at higher movement velocities and accelerations.

The term sensory ataxia is employed to indicate ataxia due to loss of proprioception, the loss of sensitivity to the positions of joint and body parts. This is generally caused by dysfunction of the dorsal columns of the spinal cord, because they carry proprioceptive information up to the brain. In some cases, the cause of sensory ataxia may instead be dysfunction of the various parts of the brain which receive positional information, including the cerebellum, thalamus, and parietal lobes.

Sensory ataxia presents itself with an unsteady "stomping" gait with heavy heel strikes, as well as a postural instability that is usually worsened when the lack of proprioceptive input cannot be compensated for by visual input, such as in poorly lit environments.

Physicians can find evidence of sensory ataxia during physical examination by having the patient stand with his/her feet together and eyes shut. In affected patients, this will cause the instability to worsen markedly, producing wide oscillations and possibly a fall. This is called a positive Romberg's test. Worsening of the finger-pointing test with the eyes closed is another feature of sensory ataxia. Also, when the patient is standing with arms and hands extended toward the physician, if the eyes are closed, the patient's finger will tend to "fall down" and then be restored to the horizontal extended position by sudden muscular contractions (the "ataxic hand").

The term "vestibular ataxia" is employed to indicate ataxia due to dysfunction of the vestibular system, which in acute and unilateral cases is associated with prominent vertigo, nausea and vomiting. In slow-onset, chronic bilateral cases of vestibular dysfunction, these characteristic manifestations may be absent, and dysequilibrium may be the sole presentation.

The three types of ataxia have overlapping causes, and therefore can either coexist or occur in isolation. Cerebellar ataxia can have many causes despite normal neuroimaging.

Any type of focal lesion of the central nervous system (such as stroke, brain tumor, multiple sclerosis, inflammatory [such as sarcoidosis], and “chronic lymphocytyc inflammation with pontine perivascular enhancement responsive to steroids syndrome” [CLIPPERS]) will cause the type of ataxia corresponding to the site of the lesion: cerebellar if in the cerebellum; sensory if in the dorsal spinal cord...to include cord compression by thickened ligamentum flavum or stenosis of the boney spinal canal...(and rarely in the thalamus or parietal lobe); or vestibular if in the vestibular system (including the vestibular areas of the cerebral cortex).

Exogenous substances that cause ataxia mainly do so because they have a depressant effect on central nervous system function. The most common example is ethanol (alcohol), which is capable of causing reversible cerebellar and vestibular ataxia. Other examples include various prescription drugs (e.g. most antiepileptic drugs have cerebellar ataxia as a possible adverse effect), Lithium level over 1.5mEq/L, synthetic cannabinoid HU-211 ingestion and various other medical and recreational drugs (e.g. ketamine, PCP or dextromethorphan, all of which are NMDA receptor antagonists that produce a dissociative state at high doses). A further class of pharmaceuticals which can cause short term ataxia, especially in high doses, are benzodiazepines. Exposure to high levels of methylmercury, through consumption of fish with high mercury concentrations, is also a known cause of ataxia and other neurological disorders.

Ataxia can be induced as a result of severe acute radiation poisoning with an absorbed dose of more than 30 grays.

Vitamin B deficiency may cause, among several neurological abnormalities, overlapping cerebellar and sensory ataxia.

Symptoms of neurological dysfunction may be the presenting feature in some patients with hypothyroidism. These include reversible cerebellar ataxia, dementia, peripheral neuropathy, psychosis and coma. Most of the neurological complications improve completely after thyroid hormone replacement therapy.

Peripheral neuropathies may cause generalised or localised sensory ataxia (e.g. a limb only) depending on the extent of the neuropathic involvement. Spinal disorders of various types may cause sensory ataxia from the lesioned level below, when they involve the dorsal columns.

Non-hereditary causes of cerebellar degeneration include chronic alcohol abuse, head injury, paraneoplastic and non-paraneoplastic autoimmune ataxia, high altitude cerebral oedema, coeliac disease, normal pressure hydrocephalus and infectious or post-infectious cerebellitis.

Ataxia may depend on hereditary disorders consisting of degeneration of the cerebellum or of the spine; most cases feature both to some extent, and therefore present with overlapping cerebellar and sensory ataxia, even though one is often more evident than the other. Hereditary disorders causing ataxia include autosomal dominant ones such as spinocerebellar ataxia, episodic ataxia, and dentatorubropallidoluysian atrophy, as well as autosomal recessive disorders such as Friedreich's ataxia (sensory and cerebellar, with the former predominating) and Niemann Pick disease, ataxia-telangiectasia (sensory and cerebellar, with the latter predominating), and abetalipoproteinaemia. An example of X-linked ataxic condition is the rare fragile X-associated tremor/ataxia syndrome or FXTAS.

Arnold–Chiari malformation is a malformation of the brain. It consists of a downward displacement of the cerebellar tonsils and the medulla through the foramen magnum, sometimes causing hydrocephalus as a result of obstruction of cerebrospinal fluid outflow.

Succinic semialdehyde dehydrogenase deficiency is an autosomal-recessive gene disorder where mutations in the ALDH5A1 gene results in the accumulation of gamma-Hydroxybutyric acid (GHB) in the body. GHB accumulates in the nervous system and can cause ataxia as well as other neurological dysfunction.

Wilson's disease is an autosomal-recessive gene disorder whereby an alteration of the ATP7B gene results in an inability to properly excrete copper from the body. Copper accumulates in the nervous system and liver and can cause ataxia as well as other neurological and organ impairments.

Gluten ataxia is an autoimmune disease triggered by the ingestion of gluten. Early diagnosis and treatment with a gluten-free diet can improve ataxia and prevent its progression. The effectiveness of the treatment depends on the elapsed time from the onset of the ataxia until diagnosis, because the death of neurons in the cerebellum as a result of gluten exposure is irreversible. It accounts for 40% of ataxias of unknown origin and 15% of all ataxias. Less than 10% of people with gluten ataxia present any gastrointestinal symptom, yet about 40% have intestinal damage. In some cases, the immune ataxia remains of unknown origin and lacks biomarkers. This entity is called primary auto-immune ataxia (PACA).

Malfunction of the sodium-potassium pump may be a factor in some ataxias. The - pump has been shown to control and set the intrinsic activity mode of cerebellar Purkinje neurons. This suggests that the pump might not simply be a homeostatic, "housekeeping" molecule for ionic gradients; but could be a computational element in the cerebellum and the brain. Indeed, an ouabain block of - pumps in the cerebellum of a live mouse results in it displaying ataxia and dystonia. Ataxia is observed for lower ouabain concentrations, dystonia is observed at higher ouabain concentrations.

Antibodies against the enzyme glutamic acid decarboxylase (GAD: enzyme changing glutamate into GABA) cause cerebellar deficits. The antibodies impair motor learning and cause behavioral deficits.
GAD antibodies related ataxia is part of the group called immune-mediated cerebellar ataxias. The antibodies induce a synaptopathy.

The treatment of ataxia and its effectiveness depend on the underlying cause. Treatment may limit or reduce the effects of ataxia, but it is unlikely to eliminate them entirely. Recovery tends to be better in individuals with a single focal injury (such as stroke or a benign tumour), compared to those who have a neurological degenerative condition. A review of the management of degenerative ataxia was published in 2009. A small number of rare conditions presenting with prominent cerebellar ataxia are amenable to specific treatment and recognition of these disorders is critical. Diseases include vitamin E deficiency, abetalipoproteinemia, cerebrotendinous xanthomatosis, Niemann–Pick type C disease, Refsum's disease, glucose transporter type 1 deficiency, episodic ataxia type 2, gluten ataxia, glutamic acid decarboxylase ataxia. Novel therapies target the RNA defects associated with cerebellar disorders, using in particular anti-sense oligonucleotides.

The movement disorders associated with ataxia can be managed by pharmacological treatments and through physical therapy and occupational therapy to reduce disability. Some drug treatments that have been used to control ataxia include: 5-hydroxytryptophan (5-HTP), idebenone, amantadine, physostigmine, L-carnitine or derivatives, trimethoprim/sulfamethoxazole, vigabatrin, phosphatidylcholine, acetazolamide, 4-aminopyridine, buspirone, and a combination of coenzyme Q10 and vitamin E.

Physical therapy requires a focus on adapting activity and facilitating motor learning for retraining specific functional motor patterns. A recent systematic review suggested that physical therapy is effective, but there is only moderate evidence to support this conclusion. The most commonly used physical therapy interventions for cerebellar ataxia are vestibular habituation, Frenkel exercises, proprioceptive neuromuscular facilitation (PNF), and balance training; however, therapy is often highly individualized and gait and coordination training are large components of therapy.

Current research suggests that, if a person is able to walk with or without a mobility aid, physical therapy should include an exercise program addressing five components: static balance, dynamic balance, trunk-limb coordination, stairs, and contracture prevention. Once the physical therapist determines that the individual is able to safely perform parts of the program independently, it is important that the individual be prescribed and regularly engage in a supplementary home exercise program that incorporates these components to further improve long term outcomes. These outcomes include balance tasks, gait, and individual activities of daily living. While the improvements are attributed primarily to changes in the brain and not just the hip or ankle joints, it is still unknown whether the improvements are due to adaptations in the cerebellum or compensation by other areas of the brain.

Decomposition, simplification, or slowing of multijoint movement may also be an effective strategy that therapists may use to improve function in patients with ataxia. Training likely needs to be intense and focused—as indicated by one study performed with stroke patients experiencing limb ataxia who underwent intensive upper limb retraining. Their therapy consisted of constraint-induced movement therapy which resulted in improvements of their arm function. Treatment should likely include strategies to manage difficulties with everyday activities such as walking. Gait aids (such as a cane or walker) can be provided to decrease the risk of falls associated with impairment of balance or poor coordination. Severe ataxia may eventually lead to the need for a wheelchair. To obtain better results, possible coexisting motor deficits need to be addressed in addition to those induced by ataxia. For example, muscle weakness and decreased endurance could lead to increasing fatigue and poorer movement patterns.

There are several assessment tools available to therapists and health care professionals working with patients with ataxia. The International Cooperative Ataxia Rating Scale (ICARS) is one of the most widely used and has been proven to have very high reliability and validity. Other tools that assess motor function, balance and coordination are also highly valuable to help the therapist track the progress of their patient, as well as to quantify the patient's functionality. These tests include, but are not limited to:

The term "ataxia" is sometimes used in a broader sense to indicate lack of coordination in some physiological process. Examples include optic ataxia (lack of coordination between visual inputs and hand movements, resulting in inability to reach and grab objects) and ataxic respiration (lack of coordination in respiratory movements, usually due to dysfunction of the respiratory centres in the medulla oblongata). Optic ataxia may be caused by lesions to the posterior parietal cortex, which is responsible for combining and expressing positional information and relating it to movement. Outputs of the posterior parietal cortex include the spinal cord, brain stem motor pathways, pre-motor and pre-frontal cortex, basal ganglia and the cerebellum. Some neurons in the posterior parietal cortex are modulated by intention. Optic ataxia is usually part of Balint's syndrome, but can be seen in isolation with injuries to the superior parietal lobule, as it represents a disconnection between visual-association cortex and the frontal premotor and motor cortex.




</doc>
<doc id="974" url="https://en.wikipedia.org/wiki?curid=974" title="Ada Lovelace">
Ada Lovelace

Augusta Ada King, Countess of Lovelace ("née" Byron; 10 December 1815 – 27 November 1852) was an English mathematician and writer, chiefly known for her work on Charles Babbage's proposed mechanical general-purpose computer, the Analytical Engine. She is believed by some to be the first to recognise that the machine had applications beyond pure calculation, and is believed by some to have published the first algorithm intended to be carried out by such a machine. As a result, she is often regarded as the first to recognise the full potential of computers and one of the first computer programmers.

Augusta Byron was the only legitimate child of poet Lord Byron and his wife Lady Byron. All of Byron's other children were born out of wedlock to other women. Byron separated from his wife a month after Ada was born and left England forever four months later. He commemorated the parting in a poem that begins, "Is thy face like thy mother's my fair child! ADA! sole daughter of my house and heart?". He died of disease in the Greek War of Independence when Ada was eight years old. Her mother remained bitter and promoted Ada's interest in mathematics and logic in an effort to prevent her from developing her father's perceived insanity. Despite this, Ada remained interested in Byron, naming her two sons Byron and Gordon. Upon her eventual death, she was buried next to him at her request. Although often ill in her childhood, Ada pursued her studies assiduously. She married William King in 1835. King was made Earl of Lovelace in 1838, Ada thereby becoming Countess of Lovelace.

Her educational and social exploits brought her into contact with scientists such as Andrew Crosse, Charles Babbage, Sir David Brewster, Charles Wheatstone, Michael Faraday and the author Charles Dickens, contacts which she used to further her education. Ada described her approach as "poetical science" and herself as an "Analyst (& Metaphysician)".

When she was a teenager, her mathematical talents led her to a long working relationship and friendship with fellow British mathematician Charles Babbage, who is known as "the father of computers". She was in particular interested in Babbage's work on the Analytical Engine. Lovelace first met him in June 1833, through their mutual friend, and her private tutor, Mary Somerville.

Between 1842 and 1843, Ada translated an article by Italian military engineer Luigi Menabrea on the calculating engine, supplementing it with an elaborate set of notes, simply called "Notes". Lovelace's notes are important in the early history of computers, containing what many consider to be the first computer program—that is, an algorithm designed to be carried out by a machine. Other historians reject this perspective and point out that Babbage's personal notes from the years 1836/1837 contain the first programs for the engine. She also developed a vision of the capability of computers to go beyond mere calculating or number-crunching, while many others, including Babbage himself, focused only on those capabilities. Her mindset of "poetical science" led her to ask questions about the Analytical Engine (as shown in her notes) examining how individuals and society relate to technology as a collaborative tool.

She died of uterine cancer in 1852 at the age of 36.

Lord Byron expected his child to be a "glorious boy" and was disappointed when Lady Byron gave birth to a girl. The child was named after Byron's half-sister, Augusta Leigh, and was called "Ada" by Byron himself. On 16 January 1816, at Lord Byron's command, Lady Byron left for her parents' home at Kirkby Mallory, taking their five-week-old daughter with her. Although English law at the time granted full custody of children to the father in cases of separation, Lord Byron made no attempt to claim his parental rights, but did request that his sister keep him informed of Ada's welfare.
On 21 April, Lord Byron signed the deed of separation, although very reluctantly, and left England for good a few days later. Aside from an acrimonious separation, Lady Byron continued throughout her life to make allegations about her husband's immoral behaviour. This set of events made Lovelace infamous in Victorian society. She did not have a relationship with her father. He died in 1824 when she was eight years old. Her mother was the only significant parental figure in her life. Lovelace was not shown the family portrait of her father until her 20th birthday.

Lovelace did not have a close relationship with her mother. She was often left in the care of her maternal grandmother Judith, Hon. Lady Milbanke, who doted on her. However, because of societal attitudes of the time—which favoured the husband in any separation, with the welfare of any child acting as mitigation—Lady Byron had to present herself as a loving mother to the rest of society. This included writing anxious letters to Lady Milbanke about her daughter's welfare, with a cover note saying to retain the letters in case she had to use them to show maternal concern. In one letter to Lady Milbanke, she referred to her daughter as "it": "I talk to it for your satisfaction, not my own, and shall be very glad when you have it under your own." Lady Byron had her teenage daughter watched by close friends for any sign of moral deviation. Lovelace dubbed these observers the "Furies" and later complained they exaggerated and invented stories about her.
Lovelace was often ill, beginning in early childhood. At the age of eight, she experienced headaches that obscured her vision. In June 1829, she was paralyzed after a bout of measles. She was subjected to continuous bed rest for nearly a year, something which may have extended her period of disability. By 1831, she was able to walk with crutches. Despite the illnesses, she developed her mathematical and technological skills.

Ada Byron had an affair with a tutor in early 1833. She tried to elope with him after she was caught, but the tutor's relatives recognised her and contacted her mother. Lady Byron and her friends covered the incident up to prevent a public scandal. Lovelace never met her younger half-sister, Allegra, the daughter of Lord Byron and Claire Clairmont. Allegra died in 1822 at the age of five. Lovelace did have some contact with Elizabeth Medora Leigh, the daughter of Byron's half-sister Augusta Leigh, who purposely avoided Lovelace as much as possible when introduced at court.

Lovelace became close friends with her tutor Mary Somerville, who introduced her to Charles Babbage in 1833. She had a strong respect and affection for Somerville, and they corresponded for many years. Other acquaintances included the scientists Andrew Crosse, Sir David Brewster, Charles Wheatstone, Michael Faraday and the author Charles Dickens. She was presented at Court at the age of seventeen "and became a popular belle of the season" in part because of her "brilliant mind." By 1834 Ada was a regular at Court and started attending various events. She danced often and was able to charm many people, and was described by most people as being dainty, although John Hobhouse, Byron's friend, described her as "a large, coarse-skinned young woman but with something of my friend's features, particularly the mouth". This description followed their meeting on 24 February 1834 in which Ada made it clear to Hobhouse that she did not like him, probably due to her mother's influence, which led her to dislike all of her father's friends. This first impression was not to last, and they later became friends.

On 8 July 1835, she married William, 8th Baron King, becoming Lady King. They had three homes: Ockham Park, Surrey; a Scottish estate on Loch Torridon in Ross-shire; and a house in London. They spent their honeymoon at Worthy Manor in Ashley Combe near Porlock Weir, Somerset. The Manor had been built as a hunting lodge in 1799 and was improved by King in preparation for their honeymoon. It later became their summer retreat and was further improved during this time. From 1845, the family's main house was Horsley Towers, built in the Tudorbethan fashion by the architect of the Houses of Parliament, Charles Barry, and later greatly enlarged to Lovelace’s own designs.

They had three children: Byron (born 12 May 1836); Anne Isabella (called Annabella, born 22 September 1837); and Ralph Gordon (born 2 July 1839). Immediately after the birth of Annabella, Lady King experienced "a tedious and suffering illness, which took months to cure." Ada was a descendant of the extinct Barons Lovelace and in 1838, her husband was made Earl of Lovelace and Viscount Ockham, meaning Ada became the Countess of Lovelace. In 1843–44, Ada's mother assigned William Benjamin Carpenter to teach Ada's children and to act as a "moral" instructor for Ada. He quickly fell for her and encouraged her to express any frustrated affections, claiming that his marriage meant he would never act in an "unbecoming" manner. When it became clear that Carpenter was trying to start an affair, Ada cut it off.

In 1841, Lovelace and Medora Leigh (the daughter of Lord Byron's half-sister Augusta Leigh) were told by Ada's mother that Ada's father was also Medora's father. On 27 February 1841, Ada wrote to her mother: "I am not in the least "astonished". In fact, you merely "confirm" what I have for "years and years" felt scarcely a doubt about, but should have considered it most improper in me to hint to you that I in any way suspected." She did not blame the incestuous relationship on Byron, but instead blamed Augusta Leigh: "I fear she is more inherently wicked than he ever was." In the 1840s, Ada flirted with scandals: firstly, from a relaxed approach to extra-marital relationships with men, leading to rumours of affairs; and secondly, from her love of gambling. She apparently lost more than £3,000 on the horses during the later 1840s. The gambling led to her forming a syndicate with male friends, and an ambitious attempt in 1851 to create a mathematical model for successful large bets. This went disastrously wrong, leaving her thousands of pounds in debt to the syndicate, forcing her to admit it all to her husband. She had a shadowy relationship with Andrew Crosse's son John from 1844 onwards. John Crosse destroyed most of their correspondence after her death as part of a legal agreement. She bequeathed him the only heirlooms her father had personally left to her. During her final illness, she would panic at the idea of the younger Crosse being kept from visiting her.

Throughout her illnesses, she continued her education. Her mother's obsession with rooting out any of the insanity of which she accused Byron was one of the reasons that Ada was taught mathematics from an early age. She was privately educated in mathematics and science by William Frend, William King, and Mary Somerville, the noted 19th-century researcher and scientific author. One of her later tutors was the mathematician and logician Augustus De Morgan. From 1832, when she was seventeen, her mathematical abilities began to emerge, and her interest in mathematics dominated the majority of her adult life. In a letter to Lady Byron, De Morgan suggested that her daughter's skill in mathematics could lead her to become "an original mathematical investigator, perhaps of first-rate eminence."

Lovelace often questioned basic assumptions by integrating poetry and science. Whilst studying differential calculus, she wrote to De Morgan:
I may remark that the curious transformations many formulae can undergo, the unsuspected and to a beginner apparently impossible identity of forms exceedingly dissimilar at first sight, is I think one of the chief difficulties in the early part of mathematical studies. I am often reminded of certain sprites and fairies one reads of, who are at one's elbows in "one" shape now, and the next minute in a form most dissimilar
Lovelace believed that intuition and imagination were critical to effectively applying mathematical and scientific concepts. She valued metaphysics as much as mathematics, viewing both as tools for exploring "the unseen worlds around us."

Lovelace died at the age of 36 on 27 November 1852, from uterine cancer probably exacerbated by bloodletting by her physicians. The illness lasted several months, in which time Annabella took command over whom Ada saw, and excluded all of her friends and confidants. Under her mother's influence, Ada had a religious transformation and was coaxed into repenting of her previous conduct and making Annabella her executor. She lost contact with her husband after confessing something to him on 30 August which caused him to abandon her bedside. It is not known what she told him. She was buried, at her request, next to her father at the Church of St. Mary Magdalene in Hucknall, Nottinghamshire. A memorial plaque, written in Latin, to her and her father is in the chapel attached to Horsley Towers.

Throughout her life, Lovelace was strongly interested in scientific developments and fads of the day, including phrenology and mesmerism. After her work with Babbage, Lovelace continued to work on other projects. In 1844 she commented to a friend Woronzow Greig about her desire to create a mathematical model for how the brain gives rise to thoughts and nerves to feelings ("a calculus of the nervous system"). She never achieved this, however. In part, her interest in the brain came from a long-running pre-occupation, inherited from her mother, about her "potential" madness. As part of her research into this project, she visited the electrical engineer Andrew Crosse in 1844 to learn how to carry out electrical experiments. In the same year, she wrote a review of a paper by Baron Karl von Reichenbach, "Researches on Magnetism", but this was not published and does not appear to have progressed past the first draft. In 1851, the year before her cancer struck, she wrote to her mother mentioning "certain productions" she was working on regarding the relation of maths and music.
Lovelace first met Charles Babbage in June 1833, through their mutual friend Mary Somerville. Later that month Babbage invited Lovelace to see the prototype for his difference engine. She became fascinated with the machine and used her relationship with Somerville to visit Babbage as often as she could. Babbage was impressed by Lovelace's intellect and analytic skills. He called her "The Enchantress of Number." In 1843 he wrote to her:

During a nine-month period in 1842–43, Lovelace translated the Italian mathematician Luigi Menabrea's article on Babbage's newest proposed machine, the Analytical Engine. With the article, she appended a set of notes. Explaining the Analytical Engine's function was a difficult task, as many other scientists did not really grasp the concept and the British establishment had shown little interest in it. Lovelace's notes even had to explain how the Analytical Engine differed from the original Difference Engine. Her work was well received at the time; the scientist Michael Faraday described himself as a supporter of her writing.

The notes are around three times longer than the article itself and include (in "Note G"), in complete detail, a method for calculating a sequence of Bernoulli numbers using the Analytical Engine, which might have run correctly had it ever been built (only Babbage's Difference Engine has been built, completed in London in 2002). Based on this work, Lovelace is now widely considered to be the first computer programmer and her method is recognised as the world's first computer program.

"Note G" also contains Lovelace's dismissal of artificial intelligence. She wrote that "The Analytical Engine has no pretensions whatever to "originate" anything. It can do "whatever we know how to order it" to perform. It can follow analysis; but it has no power of anticipating any analytical relations or truths." This objection has been the subject of much debate and rebuttal, for example by Alan Turing in his paper "Computing Machinery and Intelligence."

Lovelace and Babbage had a minor falling out when the papers were published, when he tried to leave his own statement (criticising the government's treatment of his Engine) as an unsigned preface, which could have been mistakenly interpreted as a joint declaration. When Taylor's "Scientific Memoirs" ruled that the statement should be signed, Babbage wrote to Lovelace asking her to withdraw the paper. This was the first that she knew he was leaving it unsigned, and she wrote back refusing to withdraw the paper. The historian Benjamin Woolley theorised that "His actions suggested he had so enthusiastically sought Ada's involvement, and so happily indulged her ... because of her 'celebrated name'." Their friendship recovered, and they continued to correspond. On 12 August 1851, when she was dying of cancer, Lovelace wrote to him asking him to be her executor, though this letter did not give him the necessary legal authority. Part of the terrace at Worthy Manor was known as "Philosopher's Walk", as it was there that Lovelace and Babbage were reputed to have walked while discussing mathematical principles.

In 1840, Babbage was invited to give a seminar at the University of Turin about his Analytical Engine. Luigi Menabrea, a young Italian engineer and the future Prime Minister of Italy, transcribed Babbage's lecture into French, and this transcript was subsequently published in the Bibliothèque universelle de Genève in October 1842.
Babbage's friend Charles Wheatstone commissioned Ada Lovelace to translate Menabrea's paper into English. She then augmented the paper with notes, which were added to the translation. Ada Lovelace spent the better part of a year doing this, assisted with input from Babbage. These notes, which are more extensive than Menabrea's paper, were then published in the September 1843 edition of Taylor's "Scientific Memoirs" under the initialism "AAL".

Ada Lovelace's notes were labelled alphabetically from A to G. In note G, she describes an algorithm for the Analytical Engine to compute Bernoulli numbers. It is considered to be the first published algorithm ever specifically tailored for implementation on a computer, and Ada Lovelace has often been cited as the first computer programmer for this reason. The engine was never completed so her program was never tested.

In 1953, more than a century after her death, Ada Lovelace's notes on Babbage's Analytical Engine were republished as an appendix to B.V. Bowden's "Faster than Thought: A Symposium on Digital Computing Machines". The engine has now been recognised as an early model for a computer and her notes as a description of a computer and software.

In her notes, Ada Lovelace emphasised the difference between the Analytical Engine and previous calculating machines, particularly its ability to be programmed to solve problems of any complexity. She realised the potential of the device extended far beyond mere number crunching. In her notes, she wrote:
This analysis was an important development from previous ideas about the capabilities of computing devices and anticipated the implications of modern computing one hundred years before they were realised. Walter Isaacson ascribes Ada's's insight regarding the application of computing to "any" process based on logical symbols to an observation about textiles: "When she saw some mechanical looms that used punchcards to direct the weaving of beautiful patterns, it reminded her of how Babbage's engine used punched cards to make calculations." This insight is seen as significant by writers such as Betty Toole and Benjamin Woolley, as well as the programmer John Graham-Cumming, whose project Plan 28 has the aim of constructing the first complete Analytical Engine.

According to the historian of computing and Babbage specialist Doron Swade: Ada saw something that Babbage in some sense failed to see. In Babbage's world his engines were bound by number...What Lovelace saw...was that number could represent entities other than quantity. So once you had a machine for manipulating numbers, if those numbers represented other things, letters, musical notes, then the machine could manipulate symbols of which number was one instance, according to rules. It is this fundamental transition from a machine which is a number cruncher to a machine for manipulating symbols according to rules that is the fundamental transition from calculation to computation—to general-purpose computation—and looking back from the present high ground of modern computing, if we are looking and sifting history for that transition, then that transition was made explicitly by Ada in that 1843 paper.

Though Lovelace is often referred to as the first computer programmer, some biographers, computer scientists and historians of computing claim otherwise.

Allan G. Bromley, in the 1990 article "Difference and Analytical Engines":
Bruce Collier, who later wrote a biography of Babbage, wrote in his 1970 Harvard University PhD thesis that Lovelace "made a considerable contribution to publicizing the Analytical Engine, but there is no evidence that she advanced the design or theory of it in any way".

Eugene Eric Kim and Betty Alexandra Toole consider it "incorrect" to regard Lovelace as the first computer programmer, as Babbage wrote the initial programs for his Analytical Engine, although the majority were never published. Bromley notes several dozen sample programs prepared by Babbage between 1837 and 1840, all substantially predating Lovelace's notes. Dorothy K. Stein regards Lovelace's notes as "more a reflection of the mathematical uncertainty of the author, the political purposes of the inventor, and, above all, of the social and cultural context in which it was written, than a blueprint for a scientific development."

In his book, "Idea Makers", Stephen Wolfram defends Lovelace's contributions. While acknowledging that Babbage wrote several unpublished algorithms for the Analytical Engine prior to Lovelace's notes, Wolfram argues that "there's nothing as sophisticated—or as clean—as Ada's computation of the Bernoulli numbers. Babbage certainly helped and commented on Ada's work, but she was definitely the driver of it." Wolfram then suggests that Lovelace's main achievement was to distill from Babbage's correspondence "a clear exposition of the abstract operation of the machine—something which Babbage never did."

Doron Swade, a specialist on history of computing known for his work on Babbage, analysed four claims about Lovelace during a lecture on Babbage's analytical engine:

According to him, only the fourth claim had "any substance at all." He explained that Ada was only a "promising beginner" instead of genius in mathematics, that she began studying basic concepts of mathematics five years after Babbage conceived the analytical engine so she could not have made important contributions to it, and that she only published the first computer program instead of actually writing it. But he agrees that Ada was the only person to see the potential of the analytical engine as a machine capable of expressing entities other than quantities.

Lord Byron wrote in 1816 the poem "Fare Thee Well" to his wife Lady Byron after their separation after the birth of Ada Lovelace. In the poem he writes:
<poem>
And when thou would'st solace gather—
Wilt thou teach her to say "Father!"
When her little hands shall press thee—
Think of him whose prayer shall bless thee—
Should her lineaments resemble
Then thy heart will softly tremble
</poem>

Lovelace is portrayed in Romulus Linney's 1977 play "Childe Byron".

In the 1990 steampunk novel "The Difference Engine" by William Gibson and Bruce Sterling, Ada Lovelace delivers a lecture on the "punched cards" programme that proves two theorems, the discovery that, in reality, would not be made until 1931 by Kurt Gödel.

In the 1997 film "Conceiving Ada," a computer scientist obsessed with Countess Ada Lovelace finds a way of communicating with Lovelace in the past by means of "undying information waves."

In Tom Stoppard's 1993 play "Arcadia", the precocious teenage genius Thomasina Coverly (a character "apparently based" on Ada Lovelace—the play also involves Lord Byron) comes to understand chaos theory, and theorises the second law of thermodynamics, before either is officially recognised.

In John Crowley's 2005 novel "Lord Byron's Novel: The Evening Land", where she is featured as an unseen character whose personality is forcefully depicted in her annotations and anti-heroic efforts to archive her father's lost novel.

The 2015 play "Ada and the Memory Engine" by Lauren Gunderson portrays Lovelace and Charles Babbage in unrequited love, and it imagines a post-death meeting between Lovelace and her father.

Lovelace and Babbage are the main characters in Sydney Padua's webcomic and graphic novel "The Thrilling Adventures of Lovelace and Babbage". The comic features extensive footnotes on the history of Ada Lovelace, and many lines of dialogue are drawn from actual correspondence.

Lovelace and Mary Shelley as teenagers are the central characters in Jordan Stratford's steampunk series, "The Wollstonecraft Detective Agency".

Lovelace, identified as Ada Augusta Byron, is portrayed by Lily Lesser in the second season of "The Frankenstein Chronicles". She is employed as an "analyst" to provide the workings of a life-sized humanoid automaton. The brass workings of the machine are reminiscent of Babbage's analytical engine. Her employment is described as keeping her occupied until she returns to her studies in advanced mathematics.

Lovelace and Babbage appear as characters in the second season of the ITV series "Victoria" (2017). Emerald Fennell portrays Lovelace in the episode, "The Green-Eyed Monster."

"Spyfall, Part 2," the second episode of "Doctor Who", series 12: On 5 January 2020, actress Sylvie Briggs played 1834 Lovelace (the year before her marriage), alongside characterisations of Charles Babbage and 1943 Noor Inayat Khan (the year before her murder).

The computer language Ada, created on behalf of the United States Department of Defense, was named after Lovelace. The reference manual for the language was approved on 10 December 1980 and the Department of Defense Military Standard for the language, "MIL-STD-1815", was given the number of the year of her birth.

In 1981, the Association for Women in Computing inaugurated its Ada Lovelace Award. Since 1998, the British Computer Society (BCS) has awarded the Lovelace Medal, and in 2008 initiated an annual competition for women students. BCSWomen sponsors the Lovelace Colloquium, an annual conference for women undergraduates. Ada College is a further-education college in Tottenham Hale, London, focused on digital skills.

Ada Lovelace Day is an annual event celebrated on the second Tuesday of October, which began in 2009. Its goal is to "... raise the profile of women in science, technology, engineering, and maths," and to "create new role models for girls and women" in these fields. Events have included Wikipedia edit-a-thons with the aim of improving the representation of women on Wikipedia in terms of articles and editors to reduce unintended gender bias on Wikipedia. The Ada Initiative was a non-profit organisation dedicated to increasing the involvement of women in the free culture and open source movements.

The Engineering in Computer Science and Telecommunications College building in Zaragoza University is called the Ada Byron Building. The computer centre in the village of Porlock, near where Lovelace lived, is named after her. Ada Lovelace House is a council-owned building in Kirkby-in-Ashfield, Nottinghamshire, near where Lovelace spent her infancy.

She is also the inspiration and influence for the Ada Developers Academy in Seattle, Washington. The academy is a non-profit that seeks to increase diversity in tech by training women, trans and non-binary people to be software engineers.

As of November 2015, all new British passports have included an illustration of Lovelace and Babbage on pages 46 and 47.

In 2017, a Google Doodle honoured her on International Women's Day.

On 2 February 2018, Satellogic, a high-resolution Earth observation imaging and analytics company, launched a ÑuSat type micro-satellite named in honour of Ada Lovelace.

In March 2018, "The New York Times" published a belated obituary for Ada Lovelace.

On 27 July 2018, Senator Ron Wyden submitted, in the United States Senate, the designation of 9 October 2018 as National Ada Lovelace Day: "To honor the life and contributions of Ada Lovelace as a leading woman in science and mathematics". The resolution (S.Res.592) was considered, and agreed to without amendment and with a preamble by unanimous consent.

The Cardano platform uses ADA as the name for their cryptocurrency and Lovelace as the smallest sub-unit of an ADA.

The bicentenary of Ada Lovelace's birth was celebrated with a number of events, including:


Special exhibitions were displayed by the Science Museum in London, England and the Weston Library (part of the Bodleian Library) in Oxford, England.


Six copies of the 1843 first edition of "Sketch of the Analytical Engine" with Ada Lovelace's "Notes" have been located. Three are held at Harvard University, one at the University of Oklahoma, and one at the United States Air Force Academy. On 20 July 2018, the sixth copy was sold at auction to an anonymous buyer for £95,000. A digital facsimile of one of the copies in the Harvard University Library is available online.

In December 2016, a letter written by Ada Lovelace was forfeited by Martin Shkreli to the New York State Department of Taxation and Finance for unpaid taxes owed by Shkreli.





</doc>
<doc id="980" url="https://en.wikipedia.org/wiki?curid=980" title="August Derleth">
August Derleth

August William Derleth (February 24, 1909 – July 4, 1971) was an American writer and anthologist. Though best remembered as the first book publisher of the writings of H. P. Lovecraft, and for his own contributions to the Cthulhu Mythos and the Cosmic Horror genre, as well as his founding of the publisher Arkham House (which did much to bring supernatural fiction into print in hardcover in the US that had only been readily available in the UK), Derleth was a leading American regional writer of his day, as well as prolific in several other genres, including historical fiction, poetry, detective fiction, science fiction, and biography.

A 1938 Guggenheim Fellow, Derleth considered his most serious work to be the ambitious "Sac Prairie Saga", a series of fiction, historical fiction, poetry, and non-fiction naturalist works designed to memorialize life in the Wisconsin he knew. Derleth can also be considered a pioneering naturalist and conservationist in his writing.

The son of William Julius Derleth and Rose Louise Volk, Derleth grew up in Sauk City, Wisconsin. He was educated in local parochial and public high school. Derleth wrote his first fiction at age 13. He was interested most in reading, and he made three trips to the library a week. He would save his money to buy books (his personal library exceeded 12,000 later on in life). Some of his biggest influences were Ralph Waldo Emerson's essays, Walt Whitman, H. L. Mencken's "The American Mercury", Samuel Johnson's "The History of Rasselas, Prince of Abissinia", Alexandre Dumas, Edgar Allan Poe, Walter Scott, and Henry David Thoreau's Walden.

Forty rejected stories and three years later, according to anthologist Jim Stephens, he sold his first story, "Bat's Belfry", to "Weird Tales" magazine. Derleth wrote throughout his four years at the University of Wisconsin, where he received a B.A. in 1930. During this time he also served briefly as associate editor of Minneapolis-based Fawcett Publications "Mystic Magazine".

Returning to Sauk City in the summer of 1931, Derleth worked in a local canning factory and collaborated with childhood friend Mark Schorer (later Chairman of the University of California, Berkeley English Department). They rented a cabin, writing Gothic and other horror stories and selling them
to "Weird Tales" magazine. Derleth won a place on the O'Brien Roll of Honor for "Five Alone", published in "Place of Hawks", but was first found in "Pagany" magazine.

As a result of his early work on the "Sac Prairie Saga", Derleth was awarded the prestigious Guggenheim Fellowship; his sponsors were Helen C. White, Nobel Prize-winning novelist Sinclair Lewis and poet Edgar Lee Masters of "Spoon River Anthology" fame.

In the mid-1930s, Derleth organized a Ranger's Club for young people, served as clerk and president of the local school board, served as a parole officer, organized a local men's club and a parent-teacher association. He also lectured in American regional literature at the University of Wisconsin and was a contributing editor of "Outdoors Magazine".

With longtime friend Donald Wandrei, Derleth in 1939 founded Arkham House. Its initial objective was to publish the works of H. P. Lovecraft, with whom Derleth had corresponded since his teenage years. At the same time, he began teaching a course in American Regional Literature at the University of Wisconsin.

In 1941, he became literary editor of "The Capital Times" newspaper in Madison, a post he held until his resignation in 1960. His hobbies included fencing, swimming, chess, philately and comic-strips (Derleth reportedly used the funding from his Guggenheim Fellowship to bind his comic book collection, most recently valued in the millions of dollars, rather than to travel abroad as the award intended.). Derleth's true avocation, however, was hiking the terrain of his native Wisconsin lands, and observing and recording nature with an expert eye.

Derleth once wrote of his writing methods, "I write very swiftly, from 750,000 to a million words yearly, very little of it pulp material."

In 1948, he was elected president of the Associated Fantasy Publishers at the 6th World Science Fiction Convention in Toronto.

He was married April 6, 1953, to Sandra Evelyn Winters. They divorced six years later. Derleth retained custody of the couple's two children, April Rose and Walden William. April earned a Bachelor of Arts degree in English from the University of Wisconsin-Madison in 1977. She became majority stockholder, President, and CEO of Arkham House in 1994. She remained in that capacity until her death. She was known in the community as a naturalist and humanitarian. April died on March 21, 2011.

In 1960, Derleth began editing and publishing a magazine called "Hawk and Whippoorwill", dedicated to poems of man and nature.

Derleth died of a heart attack on July 4, 1971, and is buried in St. Aloysius Cemetery in Sauk City. The U.S. 12 bridge over the Wisconsin River is named in his honor. Derleth was Roman Catholic.

Derleth wrote more than 150 short stories and more than 100 books during his lifetime.

Derleth wrote an expansive series of novels, short stories, journals, poems, and other works about Sac Prairie (whose prototype is Sauk City). Derleth intended this series to comprise up to 50 novels telling the projected life-story of the region from the 19th century onwards, with analogies to Balzac's "Human Comedy" and Proust's "Remembrance of Things Past".

This, and other early work by Derleth, made him a well-known figure among the regional literary figures of his time: early Pulitzer Prize winners Hamlin Garland and Zona Gale, as well as Sinclair Lewis, the last both an admirer and critic of Derleth.

As Edward Wagenknecht wrote in "Cavalcade of the American Novel", "What Mr. Derleth has that is lacking...in modern novelists generally, is a country. He belongs. He writes of a land and a people that are bone of his bone and flesh of his flesh. In his fictional world, there is a unity much deeper and more fundamental than anything that can be conferred by an ideology. It is clear, too, that he did not get the best, and most fictionally useful, part of his background material from research in the library; like Scott, in his Border novels, he gives, rather, the impression of having drunk it in with his mother's milk."

Jim Stephens, editor of "An August Derleth Reader", (1992), argues: "what Derleth accomplished...was to gather a Wisconsin mythos which gave respect to the ancient fundament of our contemporary life."

The author inaugurated the "Sac Prairie Saga" with four novellas comprising "Place of Hawks", published by Loring & Mussey in 1935. At publication, "The Detroit News" wrote: "Certainly with this book Mr. Derleth may be added to the American writers of distinction."

Derleth's first novel, "Still is the Summer Night", was published two years later by the famous Charles Scribners' editor Maxwell Perkins, and was the second in his Sac Prairie Saga.

"Village Year", the first in a series of journals–meditations on nature, Midwestern village American life, and more–was published in 1941 to praise from "The New York Times Book Review": "A book of instant sensitive responsiveness...recreates its scene with acuteness and beauty, and makes an unusual contribution to the Americana of the present day." The "New York Herald Tribune" observed that "Derleth...deepens the value of his village setting by presenting in full the enduring natural background; with the people projected against this, the writing comes to have the quality of an old Flemish picture, humanity lively and amusing and loveable in the foreground and nature magnificent beyond." James Grey, writing in the "St. Louis Dispatch" concluded, "Derleth has achieved a kind of prose equivalent of the "Spoon River Anthology"."

In the same year, "Evening in Spring" was published by Charles Scribners & Sons. This work Derleth considered among his finest. What "The Milwaukee Journal" called "this beautiful little love story", is an autobiographical novel of first love beset by small-town religious bigotry. The work received critical praise: "The New Yorker" considered it a story told "with tenderness and charm", while the "Chicago Tribune" concluded: "It's as though he turned back the pages of an old diary and told, with rekindled emotion, of the pangs of pain and the sharp, clear sweetness of a boy's first love." Helen Constance White, wrote in "The Capital Times" that it was "...the best articulated, the most fully disciplined of his stories."

These were followed in 1943 with "Shadow of Night", a Scribners' novel of which "The Chicago Sun" wrote: "Structurally it has the perfection of a carved jewel...A psychological novel of the first order, and an adventure tale that is unique and inspiriting."

In November 1945, however, Derleth's work was attacked by his one-time admirer and mentor, Sinclair Lewis. Writing in "Esquire", Lewis observed, "It is a proof of Mr. Derleth's merit that he makes one want to make the journey and see his particular Avalon: The Wisconsin River shining among its islands, and the castles of Baron Pierneau and Hercules Dousman. He is a champion and a justification of regionalism. Yet he is also a burly, bounding, bustling, self-confident, opinionated, and highly-sweatered young man with faults so grievous that a melancholy perusal of them may be of more value to apprentices than a study of his serious virtues. If he could ever be persuaded that he isn't half as good as he thinks he is, if he would learn the art of sitting still and using a blue pencil, he might become twice as good as he thinks he is–which would about rank him with Homer." Derleth good-humoredly reprinted the criticism along with a photograph of himself sans sweater, on the back cover of his 1948 country journal: "Village Daybook".

A lighter side to the "Sac Prairie Saga" is a series of quasi-autobiographical short stories known as the "Gus Elker Stories", amusing tales of country life that Peter Ruber, Derleth's last editor, said were "...models of construction and...fused with some of the most memorable characters in American literature." Most were written between 1934 and the late 1940s, though the last, "Tail of the Dog", was published in 1959 and won the "Scholastic Magazine" short story award for the year. The series was collected and republished in "Country Matters" in 1996.

"Walden West", published in 1961, is considered by many Derleth's finest work. This prose meditation is built out of the same fundamental material as the series of Sac Prairie journals, but is organized around three themes: "the persistence of memory...the sounds and odors of the country...and Thoreau's observation that the 'mass of men lead lives of quiet desperation.'" A blend of nature writing, philosophic musings, and careful observation of the people and place of "Sac Prairie." Of this work, George Vukelich, author of "North Country Notebook", writes: "Derleth's "Walden West" is...the equal of Sherwood Anderson's "Winesburg,Ohio", Thornton Wilder's "Our Town", and Edgar Lee Masters' "Spoon River Anthology"." This was followed eight years later by "Return to Walden West", a work of similar quality, but with a more noticeable environmentalist edge to the writing, notes critic Norbert Blei.

A close literary relative of the "Sac Prairie Saga" was Derleth's "Wisconsin Saga", which comprises several historical novels.

Detective fiction represented another substantial body of Derleth's work. Most notable among this work was a series of 70 stories in affectionate pastiche of Sherlock Holmes, whose creator, Sir Arthur Conan Doyle, he admired greatly. These included one published novel as well ("Mr. Fairlie's Final Journey"). The series features a (Sherlock Holmes-styled) British detective named Solar Pons, of Praed Street in London. The series was greatly admired by such notable writers and critics of mystery and detective fiction as Ellery Queen (Frederic Dannay), Anthony Boucher, Vincent Starrett and Howard Haycraft.

In his 1944 volume "The Misadventures of Sherlock Holmes", Ellery Queen wrote of Derleth's "The Norcross Riddle", an early Pons story: "How many budding authors, not even old enough to vote, could have captured the spirit and atmosphere with as much fidelity?" Queen adds, "...and his choice of the euphonic Solar Pons is an appealing addition to the fascinating lore of Sherlockian nomenclature." Vincent Starrett, in his foreword to the 1964 edition of "The Casebook of Solar Pons", wrote that the series is "...as sparkling a galaxy of Sherlockian pastiches as we have had since the canonical entertainments came to an end."

Despite close similarities to Doyle's creation, Pons lived in the post-World War I era, in the decade of the 1920s. Though Derleth never wrote a Pons novel to equal "The Hound of the Baskervilles", editor Peter Ruber wrote: "...Derleth produced more than a few Solar Pons stories almost as good as Sir Arthur's, and many that had better plot construction."

Although these stories were a form of diversion for Derleth, Ruber, who edited "The Original Text Solar Pons Omnibus Edition" (2000), argued: "Because the stories were generally of such high quality, they ought to be assessed on their own merits as a unique contribution in the annals of mystery fiction, rather than suffering comparison as one of the endless imitators of Sherlock Holmes."

Some of the stories were self-published, through a new imprint called "Mycroft & Moran", an appellation of humorous significance to Holmesian scholars. For approximately a decade, an active supporting group was the Praed Street Irregulars, patterned after the Baker Street Irregulars.

In 1946, Conan Doyle's two sons made some attempts to force Derleth to cease publishing the Solar Pons series, but the efforts were unsuccessful and eventually withdrawn.

Derleth's mystery and detective fiction also included a series of works set in Sac Prairie and featuring Judge Peck as the central character.

Derleth wrote many and varied children's works, including biographies meant to introduce younger readers to explorer Fr. Marquette, as well as Ralph Waldo Emerson and Henry David Thoreau. Arguably most important among his works for younger readers, however, is the Steve and Sim Mystery Series, also known as the Mill Creek Irregulars series. The ten-volume series, published between 1958 and 1970, is set in Sac Prairie of the 1920s and can thus be considered in its own right a part of the "Sac Prairie Saga", as well as an extension of Derleth's body of mystery fiction. Robert Hood, writing in the "New York Times" said: "Steve and Sim, the major characters, are twentieth-century cousins of Huck Finn and Tom Sawyer; Derleth's minor characters, little gems of comic drawing." The first novel in the series, "The Moon Tenders", does, in fact, involve a rafting adventure down the Wisconsin River, which led regional writer Jesse Stuart to suggest the novel was one that "older people might read to recapture the spirit and dream of youth." The connection to the "Sac Prairie Saga" was noted by the "Chicago Tribune": "Once again a small midwest community in 1920s is depicted with perception, skill, and dry humor."

Derleth was a correspondent and friend of H. P. Lovecraft – when Lovecraft wrote about "le Comte d'Erlette" in his fiction, it was in homage to Derleth. Derleth invented the term "Cthulhu Mythos" to describe the fictional universe described in the series of stories shared by Lovecraft and other writers in his circle.

When Lovecraft died in 1937, Derleth and Donald Wandrei assembled a collection of Lovecraft's stories and tried to get them published. Existing publishers showed little interest, so Derleth and Wandrei founded Arkham House in 1939 for that purpose. The name of the company derived from Lovecraft's fictional town of Arkham, Massachusetts, which features in many of his stories. In 1939 Arkham House published "The Outsider and Others", a huge collection that contained most of Lovecraft's known short stories. Derleth and Wandrei soon expanded Arkham House and began a regular publishing schedule after its second book, "Someone in the Dark", a collection of some of Derleth's own horror stories, was published in 1941.

Following Lovecraft's death, Derleth wrote a number of stories based on fragments and notes left by Lovecraft. These were published in "Weird Tales" and later in book form, under the byline "H. P. Lovecraft and August Derleth", with Derleth calling himself a "posthumous collaborator." This practice has raised objections in some quarters that Derleth simply used Lovecraft's name to market what was essentially his own fiction; S. T. Joshi refers to the "posthumous collaborations" as marking the beginning of "perhaps the most disreputable phase of Derleth's activities".

Dirk W. Mosig, S. T. Joshi, and Richard L. Tierney were dissatisfied with Derleth's invention of the term "Cthulhu Mythos" (Lovecraft himself used "Yog-Sothothery") and his presentation of Lovecraft's fiction as having an overall pattern reflecting Derleth's own Christian world view, which they contrast with Lovecraft's depiction of an amoral universe. However Robert M. Price points out that while Derleth's tales are distinct from Lovecraft's in their use of hope and his depiction of a struggle between good and evil, nevertheless the basis of Derlerth's systemization are found in Lovecraft. He also suggests that the differences can be overstated:

Derleth "was" more optimistic than Lovecraft in his conception of the Mythos, but we are dealing with a difference more of degree than kind. There are indeed tales wherein Derleth's protagonists get off scot-free (like "The Shadow in the Attic", "Witches' Hollow", or "The Shuttered Room"), but often the hero is doomed (e.g., "The House in the Valley", "The Peabody Heritage", "Something in Wood"), as in Lovecraft. And it must be remembered that an occasional Lovecraftian hero does manage to overcome the odds, e.g., in "The Horror in the Museum", "The Shunned House", and 'The Case of Charles Dexter Ward'. 

Derleth also treated Lovecraft's Old Ones as representatives of elemental forces, creating new fictional entities to flesh out this framework.

Such debates aside, Derleth's founding of Arkham House and his successful effort to rescue Lovecraft from literary obscurity are widely acknowledged by practitioners in the horror field as seminal events in the field. For instance, Ramsey Campbell has acknowledged Derleth's encouragement and guidance during the early part of his own writing career, and Kirby McCauley has cited Derleth and Arkham House as an inspiration for his own anthology, "Dark Forces". Arkham House and Derleth published "Dark Carnival", the first book by Ray Bradbury, as well. Brian Lumley cites the importance of Derleth to his own Lovecraftian work, and contends in a 2009 introduction to Derleth's work that he was "...one of the first, finest, and most discerning editors and publishers of macabre fiction."

Important as was Derleth's work to rescue H.P. Lovecraft from literary obscurity at the time of Lovecraft's death, Derleth also built a body of horror and spectral fiction of his own; still frequently anthologized. The best of this work, recently reprinted in four volumes of short stories–most of which were originally published in "Weird Tales", illustrates Derleth's original abilities in the genre. While Derleth considered his work in this genre less important than his most serious literary efforts, the compilers of these four anthologies, including Ramsey Campbell, note that the stories still resonate after more than fifty years.

In 2009, The Library of America selected Derleth's story "The Panelled Room" for inclusion in its two-century retrospective of American Fantastic Tales.

Derleth also wrote many historical novels, as part of both the "Sac Prairie Saga" and the "Wisconsin Saga". He also wrote history; arguably most notable among these was "The Wisconsin: River of a Thousand Isles", published in 1942. The work was one in a series entitled "The Rivers of America", conceived by writer Constance Lindsay Skinner in the Great Depression as a series that would connect Americans to their heritage through the history of the great rivers of the nation. Skinner wanted the series to be written by artists, not academicians. Derleth, while not a trained historian, was, according to former Wisconsin state historian William F. Thompson, "...a very competent regional historian who based his historical writing upon research in the primary documents and who regularly sought the help of professionals... ." In the foreword to the 1985 reissue of the work by The University of Wisconsin Press, Thompson concluded: "No other writer, of whatever background or training, knew and understood his particular 'corner of the earth' better than August Derleth."

Derleth wrote several volumes of poems, as well as biographies of Zona Gale, Ralph Waldo Emerson and Henry David Thoreau.

He also wrote introductions to several collections of classic early 20th century comics, such as "Buster Brown", "Little Nemo in Slumberland", and "Katzenjammer Kids", as well as a book of children's poetry entitled "A Boy's Way", and the foreword to "Tales from an Indian Lodge" by "Phebe Jewell Nichols." Derleth also wrote under the noms de plume Stephen Grendon, Kenyon Holmes and Tally Mason.

Derleth's papers were donated to the Wisconsin Historical Society in Madison.







</doc>
<doc id="981" url="https://en.wikipedia.org/wiki?curid=981" title="Alps">
Alps

The Alps are the highest and most extensive mountain range system that lies entirely in Europe, and stretch approximately across eight Alpine countries (from west to east): France, Switzerland, Monaco, Italy, Liechtenstein, Austria, Germany, and Slovenia. The Alpine arch generally extends from Nice on the western Mediterranean to Trieste on the Adriatic and Vienna at the beginning of the Pannonian basin. The mountains were formed over tens of millions of years as the African and Eurasian tectonic plates collided. Extreme shortening caused by the event resulted in marine sedimentary rocks rising by thrusting and folding into high mountain peaks such as Mont Blanc and the Matterhorn. Mont Blanc spans the French–Italian border, and at is the highest mountain in the Alps. The Alpine region area contains about a hundred peaks higher than .

The altitude and size of the range affects the climate in Europe; in the mountains precipitation levels vary greatly and climatic conditions consist of distinct zones. Wildlife such as ibex live in the higher peaks to elevations of , and plants such as Edelweiss grow in rocky areas in lower elevations as well as in higher elevations. Evidence of human habitation in the Alps goes back to the Palaeolithic era. A mummified man, determined to be 5,000 years old, was discovered on a glacier at the Austrian–Italian border in 1991.

By the 6th century BC, the Celtic La Tène culture was well established. Hannibal famously crossed the Alps with a herd of elephants, and the Romans had settlements in the region. In 1800, Napoleon crossed one of the mountain passes with an army of 40,000. The 18th and 19th centuries saw an influx of naturalists, writers, and artists, in particular, the Romantics, followed by the golden age of alpinism as mountaineers began to ascend the peaks.

The Alpine region has a strong cultural identity. The traditional culture of farming, cheesemaking, and woodworking still exists in Alpine villages, although the tourist industry began to grow early in the 20th century and expanded greatly after World War II to become the dominant industry by the end of the century. The Winter Olympic Games have been hosted in the Swiss, French, Italian, Austrian and German Alps. At present, the region is home to 14 million people and has 120 million annual visitors.

The English word "Alps" derives from the Latin "Alpes" (through French).

The Latin word "Alpes" could possibly come from the adjective "albus" ("white"), which could possibly come from the Greek goddess Alphito, whose name is related to "alphita", the "white flour"; "alphos", a dull white leprosy; and finally the Proto-Indo-European word alphos. Similarly, the river god Alpheus (deity) also derives from the Greek "alphos" and means whitish.

In his commentary on the "Aeneid" of Vergil, the late fourth-century grammarian Maurus Servius Honoratus says that all high mountains are called "Alpes" by Celts. The term may be common to Italo-Celtic, because the Celtic languages have terms for high mountains derived from "alp"..

This may be consistent with the theory that in Greek "Alpes" is a name of non-Indo-European origin (which is common for prominent mountains and mountain ranges in the Mediterranean region). According to the Oxford English Dictionary, the Latin "Alpes" might possibly derive from a pre-Indo-European word *"alb" "hill"; "Albania" is a related derivation. Albania, a name not native to the region known as the country of Albania, has been used as a name for a number of mountainous areas across Europe. In Roman times, "Albania" was a name for the eastern Caucasus, while in the English languages "Albania" (or "Albany") was occasionally used as a name for Scotland, although it is more likely derived from the Latin word "albus", the color white.

In modern languages the term "alp", "alm", "albe" or "alpe" refers to a grazing pastures in the alpine regions below the glaciers, not the peaks. An "alp" refers to a high mountain pasture where cows are taken to be grazed during the summer months and where hay barns can be found, and the term "the Alps", referring to the mountains, is a misnomer. The term for the mountain peaks varies by nation and language: words such as "Horn", "Kogel", "Kopf", "Gipfel", "Spitze", "Stock", and "Berg" are used in German speaking regions; "Mont", "Pic", "Tête", "Pointe", "Dent", "Roche", and "Aiguille" in French speaking regions; and "Monte", "Picco", "Corno", "Punta", "Pizzo", or "Cima" in Italian speaking regions.

The Alps are a crescent shaped geographic feature of central Europe that ranges in an arc (curved line) from east to west and is in width. The mean height of the mountain peaks is . The range stretches from the Mediterranean Sea north above the Po basin, extending through France from Grenoble, and stretching eastward through mid and southern Switzerland. The range continues onward toward Vienna, Austria, and east to the Adriatic Sea and Slovenia. To the south it dips into northern Italy and to the north extends to the southern border of Bavaria in Germany. In areas like Chiasso, Switzerland, and Allgäu, Bavaria, the demarcation between the mountain range and the flatlands are clear; in other places such as Geneva, the demarcation is less clear. The countries with the greatest alpine territory are Austria (28.7% of the total area), Italy (27.2%), France (21.4%) and Switzerland (13.2%).

The highest portion of the range is divided by the glacial trough of the Rhône valley, from Mont Blanc to the Matterhorn and Monte Rosa on the southern side, and the Bernese Alps on the northern. The peaks in the easterly portion of the range, in Austria and Slovenia, are smaller than those in the central and western portions. 

The variances in nomenclature in the region spanned by the Alps makes classification of the mountains and subregions difficult, but a general classification is that of the Eastern Alps and Western Alps with the divide between the two occurring in eastern Switzerland according to geologist Stefan Schmid, near the Splügen Pass.
The highest peaks of the Western Alps and Eastern Alps, respectively, are Mont Blanc, at and Piz Bernina at . The second-highest major peaks are Monte Rosa at and Ortler at , respectively

Series of lower mountain ranges run parallel to the main chain of the Alps, including the French Prealps in France and the Jura Mountains in Switzerland and France. The secondary chain of the Alps follows the watershed from the Mediterranean Sea to the Wienerwald, passing over many of the highest and most well-known peaks in the Alps. From the Colle di Cadibona to Col de Tende it runs westwards, before turning to the northwest and then, near the Colle della Maddalena, to the north. Upon reaching the Swiss border, the line of the main chain heads approximately east-northeast, a heading it follows until its end near Vienna.

The northeast end of the Alpine arc directly on the Danube, which flows into the Black Sea, is the Leopoldsberg near Vienna. In contrast, the southeastern part of the Alps ends on the Adriatic Sea in the area around Trieste towards Duino and Barcola.

The Alps have been crossed for war and commerce, and by pilgrims, students and tourists. Crossing routes by road, train or foot are known as "passes", and usually consist of depressions in the mountains in which a valley leads from the plains and hilly pre-mountainous zones. In the medieval period hospices were established by religious orders at the summits of many of the main passes. The most important passes are the Col de l'Iseran (the highest), the Col Agnel, the Brenner Pass, the Mont-Cenis, the Great St. Bernard Pass, the Col de Tende, the Gotthard Pass, the Semmering Pass, the Simplon Pass, and the Stelvio Pass.
Crossing the Italian-Austrian border, the Brenner Pass separates the Ötztal Alps and Zillertal Alps and has been in use as a trading route since the 14th century. The lowest of the Alpine passes at , the Semmering crosses from Lower Austria to Styria; since the 12th century when a hospice was built there, it has seen continuous use. A railroad with a tunnel long was built along the route of the pass in the mid-19th century. With a summit of , the Great St. Bernard Pass is one of the highest in the Alps, crossing the Italian-Swiss border east of the Pennine Alps along the flanks of Mont Blanc. The pass was used by Napoleon Bonaparte to cross 40,000 troops in 1800.

The Mont Cenis pass has been a major commercial and military road between Western Europe and Italy. The pass was crossed by many troops on their way to the Italian peninsula. From Constantine I, Pepin the Short and Charlemagne to Henry IV, Napoléon and more recently the German Gebirgsjägers during World War II.
Now the pass has been supplanted by the Fréjus Highway Tunnel (opened 1980) and Rail Tunnel (opened 1871).

The Saint Gotthard Pass crosses from Central Switzerland to Ticino; in 1882 the Saint Gotthard Railway Tunnel was opened connecting Lucerne in Switzerland, with Milan in Italy. 98 years later followed Gotthard Road Tunnel ( long) connecting the A2 motorway in Göschenen on the German-Swiss side with Airolo on the Italian-Swiss side, exactly like the railway tunnel. On June 1, 2016 the world's longest railway tunnel, the Gotthard Base Tunnel was opened, which connects Erstfeld in canton of Uri with Bodio in canton of Ticino by two single tubes of . It is the first tunnel that traverses the Alps on a flat route. From December 11, 2016 it is part of the regular railway timetable and be used hourly as standard way to ride between Basel/Lucerne/Zurich and Bellinzona/Lugano/Milano.

The highest pass in the alps is the col de l'Iseran in Savoy (France) at , followed by the Stelvio Pass in northern Italy at ; the road was built in the 1820s.

Important geological concepts were established as naturalists began studying the rock formations of the Alps in the 18th century. In the mid-19th century the now defunct theory of geosynclines was used to explain the presence of "folded" mountain chains but by the mid-20th century the theory of plate tectonics became widely accepted.

The formation of the Alps (the Alpine orogeny) was an episodic process that began about 300 million years ago. In the Paleozoic Era the Pangaean supercontinent consisted of a single tectonic plate; it broke into separate plates during the Mesozoic Era and the Tethys sea developed between Laurasia and Gondwana during the Jurassic Period. The Tethys was later squeezed between colliding plates causing the formation of mountain ranges called the Alpide belt, from Gibraltar through the Himalayas to Indonesia—a process that began at the end of the Mesozoic and continues into the present. The formation of the Alps was a segment of this orogenic process, caused by the collision between the African and the Eurasian plates that began in the late Cretaceous Period.

Under extreme compressive stresses and pressure, marine sedimentary rocks were uplifted, creating characteristic recumbent folds, or "nappes", and thrust faults. As the rising peaks underwent erosion, a layer of marine flysch sediments was deposited in the foreland basin, and the sediments became involved in younger nappes (folds) as the orogeny progressed. Coarse sediments from the continual uplift and erosion were later deposited in foreland areas as molasse. The molasse regions in Switzerland and Bavaria were well-developed and saw further upthrusting of flysch.
The Alpine orogeny occurred in ongoing cycles through to the Paleogene causing differences in nappe structures, with a late-stage orogeny causing the development of the Jura Mountains. A series of tectonic events in the Triassic, Jurassic and Cretaceous periods caused different paleogeographic regions. The Alps are subdivided by different lithology (rock composition) and nappe structure according to the orogenic events that affected them. The geological subdivision differentiates the Western, Eastern Alps and Southern Alps: the Helveticum in the north, the Penninicum and Austroalpine system in the centre and, south of the Periadriatic Seam, the Southern Alpine system.

According to geologist Stefan Schmid, because the Western Alps underwent a metamorphic event in the Cenozoic Era while the Austroalpine peaks underwent an event in the Cretaceous Period, the two areas show distinct differences in nappe formations. Flysch deposits in the Southern Alps of Lombardy probably occurred in the Cretaceous or later.

Peaks in France, Italy and Switzerland lie in the "Houillière zone", which consists of basement with sediments from the Mesozoic Era. High "massifs" with external sedimentary cover are more common in the Western Alps and were affected by Neogene Period thin-skinned thrusting whereas the Eastern Alps have comparatively few high peaked massifs. Similarly the peaks in eastern Switzerland extending to western Austria (Helvetic nappes) consist of thin-skinned sedimentary folding that detached from former basement rock.

In simple terms the structure of the Alps consists of layers of rock of European, African and oceanic (Tethyan) origin. The bottom nappe structure is of continental European origin, above which are stacked marine sediment nappes, topped off by nappes derived from the African plate. The Matterhorn is an example of the ongoing orogeny and shows evidence of great folding. The tip of the mountain consists of gneisses from the African plate; the base of the peak, below the glaciated area, consists of European basement rock. The sequence of Tethyan marine sediments and their oceanic basement is sandwiched between rock derived from the African and European plates.

The core regions of the Alpine orogenic belt have been folded and fractured in such a manner that erosion created the characteristic steep vertical peaks of the Swiss Alps that rise seemingly straight out of the foreland areas. Peaks such as Mont Blanc, the Matterhorn, and high peaks in the Pennine Alps, the Briançonnais, and Hohe Tauern consist of layers of rock from the various orogenies including exposures of basement rock.

Due to the ever-present geologic instability, earthquakes continue in the Alps to this day. Typically, the largest earthquakes in the alps have been between magnitude 6 and 7 on the Richter scale.

The Union Internationale des Associations d'Alpinisme (UIAA) has defined a list of 82 "official" Alpine summits that reach at least . The list includes not only mountains, but also subpeaks with little prominence that are considered important mountaineering objectives. Below are listed the 22 "four-thousanders" with at least of prominence.

While Mont Blanc was first climbed in 1786, most of the Alpine four-thousanders were climbed during the second half of the 19th century; the ascent of the Matterhorn in 1865 marked the end of the golden age of alpinism. Karl Blodig (1859–1956) was among the first to successfully climb all the major 4,000 m peaks. He completed his series of ascents in 1911.

The first British Mont Blanc ascent was in 1788; the first female ascent in 1819. By the mid-1850s Swiss mountaineers had ascended most of the peaks and were eagerly sought as mountain guides. Edward Whymper reached the top of the Matterhorn in 1865 (after seven attempts), and in 1938 the last of the six great north faces of the Alps was climbed with the first ascent of the Eiger "Nordwand" (north face of the Eiger).

The Alps are a source of minerals that have been mined for thousands of years. In the 8th to 6th centuries BC during the Hallstatt culture, Celtic tribes mined copper; later the Romans mined gold for coins in the Bad Gastein area. Erzberg in Styria furnishes high-quality iron ore for the steel industry. Crystals, such as cinnabar, amethyst, and quartz, are found throughout much of the Alpine region. The cinnabar deposits in Slovenia are a notable source of cinnabar pigments.

Alpine crystals have been studied and collected for hundreds of years, and began to be classified in the 18th century. Leonhard Euler studied the shapes of crystals, and by the 19th century crystal hunting was common in Alpine regions. David Friedrich Wiser amassed a collection of 8000 crystals that he studied and documented. In the 20th century Robert Parker wrote a well-known work about the rock crystals of the Swiss Alps; at the same period a commission was established to control and standardize the naming of Alpine minerals.

In the Miocene Epoch the mountains underwent severe erosion because of glaciation, which was noted in the mid-19th century by naturalist Louis Agassiz who presented a paper proclaiming the Alps were covered in ice at various intervals—a theory he formed when studying rocks near his Neuchâtel home which he believed originated to the west in the Bernese Oberland. Because of his work he came to be known as the "father of the ice-age concept" although other naturalists before him put forth similar ideas.

Agassiz studied glacier movement in the 1840s at the Unteraar Glacier where he found the glacier moved per year, more rapidly in the middle than at the edges. His work was continued by other scientists and now a permanent laboratory exists inside a glacier under the Jungfraujoch, devoted exclusively to the study of Alpine glaciers.

Glaciers pick up rocks and sediment with them as they flow. This causes erosion and the formation of valleys over time. The Inn valley is an example of a valley carved by glaciers during the ice ages with a typical terraced structure caused by erosion. Eroded rocks from the most recent ice age lie at the bottom of the valley while the top of the valley consists of erosion from earlier ice ages. Glacial valleys have characteristically steep walls (reliefs); valleys with lower reliefs and talus slopes are remnants of glacial troughs or previously infilled valleys. Moraines, piles of rock picked up during the movement of the glacier, accumulate at edges, centre and the terminus of glaciers.

Alpine glaciers can be straight rivers of ice, long sweeping rivers, spread in a fan-like shape (Piedmont glaciers), and curtains of ice that hang from vertical slopes of the mountain peaks. The stress of the movement causes the ice to break and crack loudly, perhaps explaining why the mountains were believed to be home to dragons in the medieval period. The cracking creates unpredictable and dangerous crevasses, often invisible under new snowfall, which cause the greatest danger to mountaineers.

Glaciers end in ice caves (the Rhône Glacier), by trailing into a lake or river, or by shedding snowmelt on a meadow. Sometimes a piece of glacier will detach or break resulting in flooding, property damage and loss of life.

High levels of precipitation cause the glaciers to descend to permafrost levels in some areas whereas in other, more arid regions, glaciers remain above about the level. The of the Alps covered by glaciers in 1876 had shrunk to by 1973, resulting in decreased river run-off levels. Forty percent of the glaciation in Austria has disappeared since 1850, and 30% of that in Switzerland.

The Alps provide lowland Europe with drinking water, irrigation, and hydroelectric power. Although the area is only about 11 percent of the surface area of Europe, the Alps provide up to 90 percent of water to lowland Europe, particularly to arid areas and during the summer months. Cities such as Milan depend on 80 percent of water from Alpine runoff. Water from the rivers is used in over 500 hydroelectricity power plants, generating as much as 2900 GWh of electricity.

Major European rivers flow from the Alps, such as the Rhine, the Rhône, the Inn, and the Po, all of which have headwaters in the Alps and flow into neighbouring countries, finally emptying into the North Sea, the Mediterranean Sea, the Adriatic Sea and the Black Sea. Other rivers such as the Danube have major tributaries flowing into them that originate in the Alps. The Rhône is second to the Nile as a freshwater source to the Mediterranean Sea; the river begins as glacial meltwater, flows into Lake Geneva, and from there to France where one of its uses is to cool nuclear power plants. The Rhine originates in a area in Switzerland and represents almost 60 percent of water exported from the country. Tributary valleys, some of which are complicated, channel water to the main valleys which can experience flooding during the snow melt season when rapid runoff causes debris torrents and swollen rivers.

The rivers form lakes, such as Lake Geneva, a crescent shaped lake crossing the Swiss border with Lausanne on the Swiss side and the town of Evian-les-Bains on the French side. In Germany, the medieval St. Bartholomew's chapel was built on the south side of the Königssee, accessible only by boat or by climbing over the abutting peaks.
Additionally, the Alps have led to the creation of large lakes in Italy. For instance, the Sarca, the primary inflow of Lake Garda, originates in the Italian Alps.

Scientists have been studying the impact of climate change and water use. For example, each year more water is diverted from rivers for snowmaking in the ski resorts, the effect of which is yet unknown. Furthermore, the decrease of glaciated areas combined with a succession of winters with lower-than-expected precipitation may have a future impact on the rivers in the Alps as well as an effect on the water availability to the lowlands.

The Alps are a classic example of what happens when a temperate area at lower altitude gives way to higher-elevation terrain. Elevations around the world that have cold climates similar to those of the polar regions have been called Alpine. A rise from sea level into the upper regions of the atmosphere causes the temperature to decrease (see adiabatic lapse rate). The effect of mountain chains on prevailing winds is to carry warm air belonging to the lower region into an upper zone, where it expands in volume at the cost of a proportionate loss of temperature, often accompanied by precipitation in the form of snow or rain. The height of the Alps is sufficient to divide the weather patterns in Europe into a wet north and a dry south because moisture is sucked from the air as it flows over the high peaks.

The severe weather in the Alps has been studied since the 18th century; particularly the weather patterns such as the seasonal foehn wind. Numerous weather stations were placed in the mountains early in the early 20th century, providing continuous data for climatologists. Some of the valleys are quite arid such as the Aosta valley in Italy, the Maurienne in France, the Valais in Switzerland, and northern Tyrol.

The areas that are not arid and receive high precipitation experience periodic flooding from rapid snowmelt and runoff. The mean precipitation in the Alps ranges from a low of per year to per year, with the higher levels occurring at high altitudes. At altitudes between , snowfall begins in November and accumulates through to April or May when the melt begins. Snow lines vary from , above which the snow is permanent and the temperatures hover around the freezing point even during July and August. High-water levels in streams and rivers peak in June and July when the snow is still melting at the higher altitudes.

The Alps are split into five climatic zones, each with different vegetation. The climate, plant life and animal life vary among the different sections or zones of the mountains. The lowest zone is the colline zone, which exists between , depending on the location. The montane zone extends from , followed by the sub-Alpine zone from . The Alpine zone, extending from tree line to snow line, is followed by the glacial zone, which covers the glaciated areas of the mountain. Climatic conditions show variances within the same zones; for example, weather conditions at the head of a mountain valley, extending directly from the peaks, are colder and more severe than those at the mouth of a valley which tend to be less severe and receive less snowfall.

Various models of climate change have been projected into the 22nd century for the Alps, with an expectation that a trend toward increased temperatures will have an effect on snowfall, snowpack, glaciation, and river runoff. Significant changes, of both natural and anthropogenic origins, have already been diagnosed from observations.

Thirteen thousand species of plants have been identified in the Alpine regions. Alpine plants are grouped by habitat and soil type which can be limestone or non-calcareous. The habitats range from meadows, bogs, woodland (deciduous and coniferous) areas to soil-less scree and moraines, and rock faces and ridges. A natural vegetation limit with altitude is given by the presence of the chief deciduous trees—oak, beech, ash and sycamore maple. These do not reach exactly to the same elevation, nor are they often found growing together; but their upper limit corresponds accurately enough to the change from a temperate to a colder climate that is further proved by a change in the presence of wild herbaceous vegetation. This limit usually lies about above the sea on the north side of the Alps, but on the southern slopes it often rises to , sometimes even to .

Above the forestry, there is often a band of short pine trees ("Pinus mugo"), which is in turn superseded by "Alpenrosen", dwarf shrubs, typically "Rhododendron ferrugineum" (on acid soils) or "Rhododendron hirsutum" (on alkaline soils). Although the Alpenrose prefers acidic soil, the plants are found throughout the region. Above the tree line is the area defined as "alpine" where in the alpine meadow plants are found that have adapted well to harsh conditions of cold temperatures, aridity, and high altitudes. The alpine area fluctuates greatly because of regional fluctuations in tree lines.

Alpine plants such as the Alpine gentian grow in abundance in areas such as the meadows above the Lauterbrunnental. Gentians are named after the Illyrian king Gentius, and 40 species of the early-spring blooming flower grow in the Alps, in a range of . Writing about the gentians in Switzerland D. H. Lawrence described them as "darkening the day-time, torch-like with the smoking blueness of Pluto's gloom." Gentians tend to "appear" repeatedly as the spring blooming takes place at progressively later dates, moving from the lower altitude to the higher altitude meadows where the snow melts much later than in the valleys. On the highest rocky ledges the spring flowers bloom in the summer.

At these higher altitudes, the plants tend to form isolated cushions. In the Alps, several species of flowering plants have been recorded above , including "Ranunculus glacialis", "Androsace alpina" and "Saxifraga biflora". "Eritrichium nanum", commonly known as the King of the Alps, is the most elusive of the alpine flowers, growing on rocky ridges at . Perhaps the best known of the alpine plants is Edelweiss which grows in rocky areas and can be found at altitudes as low as and as high as . The plants that grow at the highest altitudes have adapted to conditions by specialization such as growing in rock screes that give protection from winds.

The extreme and stressful climatic conditions give way to the growth of plant species with secondary metabolites important for medicinal purposes. Origanum vulgare, Prunella vulgaris, Solanum nigrum and Urtica dioica are some of the more useful medicinal species found in the Alps.

Human interference has nearly exterminated the trees in many areas, and, except for the beech forests of the Austrian Alps, forests of deciduous trees are rarely found after the extreme deforestation between the 17th and 19th centuries. The vegetation has changed since the second half of the 20th century, as the high alpine meadows cease to be harvested for hay or used for grazing which eventually might result in a regrowth of forest. In some areas the modern practice of building ski runs by mechanical means has destroyed the underlying tundra from which the plant life cannot recover during the non-skiing months, whereas areas that still practice a natural "piste" type of ski slope building preserve the fragile underlayers.

The Alps are a habitat for 30,000 species of wildlife, ranging from the tiniest snow fleas to brown bears, many of which have made adaptations to the harsh cold conditions and high altitudes to the point that some only survive in specific micro-climates either directly above or below the snow line.

The largest mammal to live in the highest altitudes are the alpine ibex, which have been sighted as high as . The ibex live in caves and descend to eat the succulent alpine grasses. Classified as antelopes, chamois are smaller than ibex and found throughout the Alps, living above the tree line and are common in the entire alpine range. Areas of the eastern Alps are still home to brown bears. In Switzerland the canton of Bern was named for the bears but the last bear is recorded as having been killed in 1792 above Kleine Scheidegg by three hunters from Grindelwald.

Many rodents such as voles live underground. Marmots live almost exclusively above the tree line as high as . They hibernate in large groups to provide warmth, and can be found in all areas of the Alps, in large colonies they build beneath the alpine pastures. Golden eagles and bearded vultures are the largest birds to be found in the Alps; they nest high on rocky ledges and can be found at altitudes of . The most common bird is the alpine chough which can be found scavenging at climber's huts or at the Jungfraujoch, a high altitude tourist destination.

Reptiles such as adders and vipers live up to the snow line; because they cannot bear the cold temperatures they hibernate underground and soak up the warmth on rocky ledges. The high-altitude Alpine salamanders have adapted to living above the snow line by giving birth to fully developed young rather than laying eggs. Brown trout can be found in the streams up to the snow line. Molluscs such as the wood snail live up the snow line. Popularly gathered as food, the snails are now protected.

A number of species of moths live in the Alps, some of which are believed to have evolved in the same habitat up to 120 million years ago, long before the Alps were created. Blue butterflies can commonly be seen drinking from the snow melt; some species of blues fly as high as . The butterflies tend to be large, such as those from the swallowtail Parnassius family, with a habitat that ranges to . Twelve species of beetles have habitats up to the snow line; the most beautiful and formerly collected for its colours but now protected is "Rosalia alpina". Spiders, such as the large wolf spider, live above the snow line and can be seen as high as . Scorpions can be found in the Italian Alps.

Some of the species of moths and insects show evidence of having been indigenous to the area from as long ago as the Alpine orogeny. In Emosson in Valais, Switzerland, dinosaur tracks were found in the 1970s, dating probably from the Triassic Period.

About 10,000 years ago, when the ice melted after the Würm glaciation, late Palaeolithic communities were established along the lake shores and in cave systems. Evidence of human habitation has been found in caves near Vercors, close to Grenoble; in Austria the Mondsee culture shows evidence of houses built on piles to keep them dry. Standing stones have been found in Alpine areas of France and Italy. The Rock Drawings in Valcamonica are more than 5000 years old; more than 200,000 drawings and etchings have been identified at the site.

In 1991 a mummy of a neolithic body, known as Ötzi the Iceman, was discovered by hikers on the Similaun glacier. His clothing and gear indicate that he lived in an alpine farming community, while the location and manner of his death – an arrowhead was discovered in his shoulder – suggests he was travelling from one place to another. Analysis of the mitochondrial DNA of Ötzi, has shown that he belongs to the K1 subclade which cannot be categorized into any of the three modern branches of that subclade. The new subclade has provisionally been named "K1ö" for "Ötzi".

Celtic tribes settled in Switzerland between 1500 and 1000 BC. The Raetians lived in the eastern regions, while the west was occupied by the Helvetii and the Allobrogi settled in the Rhône valley and in Savoy. The Ligurians and Adriatic Veneti lived in north-west Italy and Triveneto respectively. Among the many substances Celtic tribes mined was salt in areas such as Salzburg in Austria where evidence of the Hallstatt culture was found by a mine manager in the 19th century. By the 6th century BC the La Tène culture was well established in the region, and became known for high quality decorated weapons and jewellery. The Celts were the most widespread of the mountain tribes—they had warriors that were strong, tall and fair skinned, and skilled with iron weapons, which gave them an advantage in warfare.

During the Second Punic War in 218 BC, the Carthaginian general Hannibal probably crossed the Alps with an army numbering 38,000 infantry, 8,000 cavalry, and 37 war elephants. This was one of the most celebrated achievements of any military force in ancient warfare, although no evidence exists of the actual crossing or the place of crossing. The Romans, however, had built roads along the mountain passes, which continued to be used through the medieval period to cross the mountains and Roman road markers can still be found on the mountain passes.

The Roman expansion brought the defeat of the Allobrogi in 121 BC and during the Gallic Wars in 58 BC Julius Caesar overcame the Helvetii. The Rhaetians continued to resist but were eventually conquered when the Romans turned northward to the Danube valley in Austria and defeated the Brigantes. The Romans built settlements in the Alps; towns such as Aosta (named for Augustus) in Italy, Martigny and Lausanne in Switzerland, and Partenkirchen in Bavaria show remains of Roman baths, villas, arenas and temples. Much of the Alpine region was gradually settled by Germanic tribes, (Lombards, Alemanni, Bavarii, and Franks) from the 6th to the 13th centuries mixing with the local Celtic tribes.

Christianity was established in the region by the Romans, and saw the establishment of monasteries and churches in the high regions. The Frankish expansion of the Carolingian Empire and the Bavarian expansion in the eastern Alps introduced feudalism and the building of castles to support the growing number of dukedoms and kingdoms. Castello del Buonconsiglio in Trento, Italy, still has intricate frescoes, excellent examples of Gothic art, in a tower room. In Switzerland, Château de Chillon is preserved as an example of medieval architecture.

Much of the medieval period was a time of power struggles between competing dynasties such as the House of Savoy, the Visconti in northern Italy and the House of Habsburg in Austria and Slovenia. In 1291 to protect themselves from incursions by the Habsburgs, four cantons in the middle of Switzerland drew up a charter that is considered to be a declaration of independence from neighbouring kingdoms. After a series of battles fought in the 13th, 14th and 15th centuries, more cantons joined the confederacy and by the 16th century Switzerland was well-established as a separate state.
During the Napoleonic Wars in the late 18th century and early 19th century, Napoleon annexed territory formerly controlled by the Habsburgs and Savoys. In 1798 he established the Helvetic Republic in Switzerland; two years later he led an army across the St. Bernard pass and conquered almost all of the Alpine regions.
After the fall of Napoléon, many alpine countries developed heavy protections to prevent any new invasion. Thus, Savoy built a series of fortifications in the Maurienne valley in order to protect the major alpine passes, such as the col du Mont-Cenis that was even crossed by Charlemagne and his father to defeat the Lombards. The later indeed became very popular after the construction of a paved road ordered by Napoléon Bonaparte.
The Barrière de l'Esseillon is a serie of forts with heavy batteries, built on a cliff with a perfect view on the valley, a gorge on one side and steep mountains on the other side.

In the 19th century, the monasteries built in the high Alps during the medieval period to shelter travellers and as places of pilgrimage, became tourist destinations. The Benedictines had built monasteries in Lucerne, Switzerland, and Oberammergau; the Cistercians in the Tyrol and at Lake Constance; and the Augustinians had abbeys in the Savoy and one in the centre of Interlaken, Switzerland. The Great St Bernard Hospice, built in the 9th or 10th centuries, at the summit of the Great Saint Bernard Pass was shelter for travellers and place for pilgrims since its inception; by the 19th century it became a tourist attraction with notable visitors such as author Charles Dickens and mountaineer Edward Whymper.

Radiocarbon-dated charcoal placed around 50,000 years ago was found in the "Drachloch" (Dragon's Hole) cave above the village of Vattis in the canton of St. Gallen, proving that the high peaks were visited by prehistoric people. Seven bear skulls from the cave may have been buried by the same prehistoric people. The peaks, however, were mostly ignored except for a few notable examples, and long left to the exclusive attention of the people of the adjoining valleys. The mountain peaks were seen as terrifying, the abode of dragons and demons, to the point that people blindfolded themselves to cross the Alpine passes. The glaciers remained a mystery and many still believed the highest areas to be inhabited by dragons.

Charles VII of France ordered his chamberlain to climb Mont Aiguille in 1356. The knight reached the summit of Rocciamelone where he left a bronze triptych of three crosses, a feat which he conducted with the use of ladders to traverse the ice. In 1492 Antoine de Ville climbed Mont Aiguille, without reaching the summit, an experience he described as "horrifying and terrifying." Leonardo da Vinci was fascinated by variations of light in the higher altitudes, and climbed a mountain—scholars are uncertain which one; some believe it may have been Monte Rosa. From his description of a "blue like that of a gentian" sky it is thought that he reached a significantly high altitude. In the 18th century four Chamonix men almost made the summit of Mont Blanc but were overcome by altitude sickness and snowblindness.

Conrad Gessner was the first naturalist to ascend the mountains in the 16th century, to study them, writing that in the mountains he found the "theatre of the Lord". By the 19th century more naturalists began to arrive to explore, study and conquer the high peaks. Two men who first explored the regions of ice and snow were Horace-Bénédict de Saussure (1740–1799) in the Pennine Alps, and the Benedictine monk of Disentis Placidus a Spescha (1752–1833). Born in Geneva, Saussure was enamoured with the mountains from an early age; he left a law career to become a naturalist and spent many years trekking through the Bernese Oberland, the Savoy, the Piedmont and Valais, studying the glaciers and the geology, as he became an early proponent of the theory of rock upheaval. Saussure, in 1787, was a member of the third ascent of Mont Blanc—today the summits of all the peaks have been climbed.

Albrecht von Haller's poem "Die Alpen" (1732) described the mountains as an area of mythical purity. Jean-Jacques Rousseau was another writer who presented the Alps as a place of allure and beauty, in his novel "Julie, or the New Heloise" (1761), Later the first wave of Romantics such as Goethe and Turner came to admire the scenery; Wordsworth visited the area in 1790, writing of his experiences in "The Prelude" (1799). Schiller later wrote the play "William Tell" (1804), which tells the story the legendary Swiss marksman William Tell as part of the greater Swiss struggle for independence from the Habsburg Empire in the early 14th century. At the end of the Napoleonic Wars, the Alpine countries began to see an influx of poets, artists, and musicians, as visitors came to experience the sublime effects of monumental nature.

In 1816 Byron, Percy Bysshe Shelley and his wife Mary Shelley visited Geneva and all three were inspired by the scenery in their writings. During these visits Shelley wrote the poem "Mont Blanc", Byron wrote "The Prisoner of Chillon" and the dramatic poem "Manfred", and Mary Shelley, who found the scenery overwhelming, conceived the idea for the novel "Frankenstein" in her villa on the shores of Lake Geneva in the midst of a thunderstorm. When Coleridge travelled to Chamonix, he declaimed, in defiance of Shelley, who had signed himself "Atheos" in the guestbook of the Hotel de Londres near Montenvers, "Who would be, who could be an atheist in this valley of wonders".

By the mid-19th century scientists began to arrive en masse to study the geology and ecology of the region.

Austrian-born Adolf Hitler had a lifelong romantic fascination with the Alps and by the 1930s established a home at Berghof, in the Obersalzberg region outside of Berchtesgaden. His first visit to the area was in 1923 and he maintained a strong tie there until the end of his life. At the end of World War II the US Army occupied Obersalzberg, to prevent Hitler from retreating with the Wehrmacht into the mountains.

By 1940 many of the Alpine countries were under the control of the Axis powers. Austria underwent a political coup that made it part of the Third Reich; France had been invaded and Italy was a fascist regime. Switzerland and Liechtenstein were the only countries to avoid Axis takeover. The Swiss Confederation mobilized its troops—the country follows the doctrine of "armed neutrality" with all males required to have military training—a number that General Eisenhower estimated to be about 850,000. The Swiss commanders wired the infrastructure leading into the country with explosives, and threatened to destroy bridges, railway tunnels and roads across passes in the event of a Nazi invasion; and if there was an invasion the Swiss army would then have retreated to the heart of the mountain peaks, where conditions were harsher, and a military invasion would involve difficult and protracted battles.

German Ski troops were trained for the war, and battles were waged in mountainous areas such as the battle at Riva Ridge in Italy, where the American 10th Mountain Division encountered heavy resistance in February 1945. At the end of the war, a substantial amount of Nazi plunder was found stored in Austria, where Hitler had hoped to retreat as the war drew to a close. The salt mines surrounding the Altaussee area, where American troops found of gold coins stored in a single mine, were used to store looted art, jewels, and currency; vast quantities of looted art were found and returned to the owners.
The largest city within the Alps is the city of Grenoble in France. Other larger and important cities within the Alps with over 100,000 inhabitants are in Tyrol with Bolzano (Italy), Trento (Italy) and Innsbruck (Austria). Larger cities outside the Alps are Milan, Verona, Turin (Italy), Munich (Germany), Vienna, Salzburg (Austria), Zurich, Geneva (Switzerland) and Lyon (France).

Cities with over 100,000 inhabitants in the Alps are:
The population of the region is 14 million spread across eight countries. On the rim of the mountains, on the plateaus and the plains the economy consists of manufacturing and service jobs whereas in the higher altitudes and in the mountains farming is still essential to the economy. Farming and forestry continue to be mainstays of Alpine culture, industries that provide for export to the cities and maintain the mountain ecology.
Much of the Alpine culture is unchanged since the medieval period when skills that guaranteed survival in the mountain valleys and in the highest villages became mainstays, leading to strong traditions of carpentry, woodcarving, baking and pastry-making, and cheesemaking.

Farming had been a traditional occupation for centuries, although it became less dominant in the 20th century with the advent of tourism. Grazing and pasture land are limited because of the steep and rocky topography of the Alps. In mid-June cows are moved to the highest pastures close to the snowline, where they are watched by herdsmen who stay in the high altitudes often living in stone huts or wooden barns during the summers. Villagers celebrate the day the cows are herded up to the pastures and again when they return in mid-September. The Almabtrieb, Alpabzug, Alpabfahrt, Désalpes ("coming down from the alps") is celebrated by decorating the cows with garlands and enormous cowbells while the farmers dress in traditional costumes.

Cheesemaking is an ancient tradition in most Alpine countries. A wheel of cheese from the Emmental in Switzerland can weigh up to , and the Beaufort in Savoy can weigh up to . Owners of the cows traditionally receive from the cheesemakers a portion in relation to the proportion of the cows' milk from the summer months in the high alps. Haymaking is an important farming activity in mountain villages which has become somewhat mechanized in recent years, although the slopes are so steep that usually scythes are necessary to cut the grass. Hay is normally brought in twice a year, often also on festival days. Alpine festivals vary from country to country and often include the display of local costumes such as dirndl and trachten, the playing of Alpenhorns, wrestling matches, some pagan traditions such as Walpurgis Night and, in many areas, Carnival is celebrated before Lent.

In the high villages people live in homes built according to medieval designs that withstand cold winters. The kitchen is separated from the living area (called the "stube", the area of the home heated by a stove), and second-floor bedrooms benefit from rising heat. The typical Swiss chalet originated in the Bernese Oberland. Chalets often face south or downhill, and are built of solid wood, with a steeply gabled roof to allow accumulated snow to slide off easily. Stairs leading to upper levels are sometimes built on the outside, and balconies are sometimes enclosed.
Food is passed from the kitchen to the stube, where the dining room table is placed. Some meals are communal, such as fondue, where a pot is set in the middle of the table for each person to dip into. Other meals are still served in a traditional manner on carved wooden plates. Furniture has been traditionally elaborately carved and in many Alpine countries carpentry skills are passed from generation to generation.

Roofs are traditionally constructed from Alpine rocks such as pieces of schist, gneiss or slate. Such chalets are typically found in the higher parts of the valleys, as in the Maurienne valley in Savoy, where the amount of snow during the cold months is important. The inclination of the roof cannot exceed 40%, allowing the snow to stay on top, thereby functioning as insulation from the cold. In the lower areas where the forests are widespread, wooden tiles are traditionally used. Commonly made of Norway spruce, they are called "tavaillon".
The Alpine regions are multicultural and linguistically diverse. Dialects are common, and vary from valley to valley and region to region. In the Slavic Alps alone 19 dialects have been identified. Some of the French dialects spoken in the French, Swiss and Italian alps of Aosta Valley derive from Arpitan, while the southern part of the western range is related to Old Provençal; the German dialects derive from Germanic tribal languages. Romansh, spoken by two percent of the population in southeast Switzerland, is an ancient Rhaeto-Romanic language derived from Latin, remnants of ancient Celtic languages and perhaps Etruscan.

The Alps are one of the more popular tourist destinations in the world with many resorts such Oberstdorf, in Bavaria, Saalbach in Austria, Davos in Switzerland, Chamonix in France, and Cortina d'Ampezzo in Italy recording more than a million annual visitors. With over 120 million visitors a year, tourism is integral to the Alpine economy with much it coming from winter sports, although summer visitors are also an important component.

The tourism industry began in the early 19th century when foreigners visited the Alps, travelled to the bases of the mountains to enjoy the scenery, and stayed at the spa-resorts. Large hotels were built during the Belle Époque; cog-railways, built early in the 20th century, brought tourists to ever higher elevations, with the Jungfraubahn terminating at the Jungfraujoch, well above the eternal snow-line, after going through a tunnel in Eiger. During this period winter sports were slowly introduced: in 1882 the first figure skating championship was held in St. Moritz, and downhill skiing became a popular sport with English visitors early in the 20th century, as the first ski-lift was installed in 1908 above Grindelwald.

In the first half of the 20th century the Olympic Winter Games were held three times in Alpine venues: the 1924 Winter Olympics in Chamonix, France; the 1928 Winter Olympics in St. Moritz, Switzerland; and the 1936 Winter Olympics in Garmisch-Partenkirchen, Germany. During World War II the winter games were cancelled but after that time the Winter Games have been held in St. Moritz (1948), Cortina d'Ampezzo (1956), Innsbruck, Austria (1964 and 1976), Grenoble, France, (1968), Albertville, France, (1992), and Torino (2006). In 1930 the "Lauberhorn Rennen" (Lauberhorn Race), was run for the first time on the Lauberhorn above Wengen; the equally demanding Hahnenkamm was first run in the same year in Kitzbühl, Austria. Both races continue to be held each January on successive weekends. The Lauberhorn is the more strenuous downhill race at and poses danger to racers who reach within seconds of leaving the start gate.

During the post-World War I period ski-lifts were built in Swiss and Austrian towns to accommodate winter visitors, but summer tourism continued to be important; by the mid-20th century the popularity of downhill skiing increased greatly as it became more accessible and in the 1970s several new villages were built in France devoted almost exclusively to skiing, such as Les Menuires. Until this point Austria and Switzerland had been the traditional and more popular destinations for winter sports, but by the end of the 20th century and into the early 21st century, France, Italy and the Tyrol began to see increases in winter visitors. From 1980 to the present, ski-lifts have been modernized and snow-making machines installed at many resorts, leading to concerns regarding the loss of traditional Alpine culture and questions regarding sustainable development as the winter ski industry continues to develop quickly and the number of summer tourists decline.

In the 17th century about 2500 people were killed by an avalanche in a village on the French-Italian border; in the 19th century 120 homes in a village near Zermatt were destroyed by an avalanche.

The region is serviced by of roads used by six million vehicles per year. Train travel is well established in the Alps, with, for instance of track for every in a country such as Switzerland. Most of Europe's highest railways are located there. In 2007 the new Lötschberg Base Tunnel was opened, which circumvents the 100 years older Lötschberg Tunnel. With the opening of the Gotthard Base Tunnel on June 1, 2016 it bypasses the Gotthard Tunnel built in the 19th century and realizes the first flat route through the Alps.

Some high mountain villages are car free either because of inaccessibility or by choice. Wengen, and Zermatt (in Switzerland) are accessible only by cable car or cog-rail trains. Avoriaz (in France), is car free, with other Alpine villages considering becoming car free zones or limiting the number of cars for reasons of sustainability of the fragile Alpine terrain.

The lower regions and larger towns of the Alps are well-served by motorways and main roads, but higher mountain passes and byroads, which are amongst the highest in Europe, can be treacherous even in summer due to steep slopes. Many passes are closed in winter. A number of airports around the Alps (and some within), as well as long-distance rail links from all neighbouring countries, afford large numbers of travellers easy access.



</doc>
<doc id="983" url="https://en.wikipedia.org/wiki?curid=983" title="Albert Camus">
Albert Camus

Albert Camus ( , , ; 7 November 1913 – 4 January 1960) was a French philosopher, author, and journalist. He won the Nobel Prize in Literature at the age of 44 in 1957, the second-youngest recipient in history.

Camus was born in Algeria (a French colony at the time) to French "Pieds Noirs" parents. His citizenship was French. He spent his childhood in a poor neighbourhood and later studied philosophy at the University of Algiers. He was in Paris when the Germans invaded France during World War II in 1940. Camus tried to flee but finally joined the French Resistance where he served as editor-in-chief at "Combat", an outlawed newspaper. After the war, he was a celebrity figure and gave many lectures around the world. He married twice but had many extramarital affairs. Camus was politically active; he was part of the Left that opposed the Soviet Union because of its totalitarianism. Camus was a moralist and leaned towards anarcho-syndicalism. He was part of many organisations seeking European integration. During the Algerian War (1954 –1962), he kept a neutral stance, advocating for a multicultural and pluralistic Algeria, a position that caused controversy and was rejected by most parties.

Philosophically, Camus's views contributed to the rise of the philosophy known as absurdism. He is also considered to be an existentialist, even though he firmly rejected the term throughout his lifetime.

Albert Camus was born on 7 November 1913 in a working-class neighbourhood in Mondovi (present-day Dréan), in French Algeria. His mother, Catherine Hélène Camus (née Sintès), was French with Spanish-Balearic ancestry. His father, Lucien Camus, a poor French agricultural worker, died in the Battle of the Marne in 1914 during World War I. Camus never knew him. Camus, his mother and other relatives lived without many basic material possessions during his childhood in the Belcourt section of Algiers. He was a second-generation French in Algeria, a French territory from 1830 until 1962. His paternal grandfather, along with many others of his generation, had moved to Algeria for a better life during the first decades of the 19th century. Hence, he was called —a slang term for French who were born in Algeria—and his identity and his poor background had a substantial effect on his later life. Nevertheless, Camus was a French citizen, in contrast to the Arab or Berber inhabitants of Algeria who were kept under an inferior legal status. During his childhood, Camus developed a love for football and swimming.

Under the influence of his teacher Louis Germain, Camus gained a scholarship in 1924 to continue his studies at a prestigious lyceum (secondary school) near Algiers. In 1930, he was diagnosed with tuberculosis. Because it is a transmitted disease, he moved out of his home and stayed with his uncle Gustave Acault, a butcher, who influenced the young Camus. It was at that time that Camus turned to philosophy, with the mentoring of his philosophy teacher Jean Grenier. He was impressed by ancient Greek philosophers and Friedrich Nietzsche. During that time, he was only able to study part-time. To earn money, he took odd jobs: as a private tutor, car parts clerk, and assistant at the Meteorological Institute.

In 1933, Camus enrolled at the University of Algiers and completed his "licence de philosophie" (BA) in 1936; after presenting his thesis on Plotinus. Camus developed an interest in early Christian philosophers, but Nietzsche and Arthur Schopenhauer had paved the way towards pessimism and atheism. Camus also studied novelist-philosophers such as Stendhal, Herman Melville, Fyodor Dostoyevsky, and Franz Kafka. In 1933, he also met Simone Hié, then a partner of a friend of Camus, who would become his first wife.

Camus played goalkeeper for the Racing Universitaire d'Alger junior team from 1928 to 1930. The sense of team spirit, fraternity, and common purpose appealed to Camus enormously. In match reports, he was often praised for playing with passion and courage. Any football ambitions disappeared when he contracted tuberculosis at the age of 17. Camus drew parallels among football, human existence, morality, and personal identity. For him, the simplistic morality of football contradicted the complicated morality imposed by authorities such as the state and Church.

In 1934, aged 20, Camus was in a relationship with Simone Hié. Simone suffered from an addiction to morphine, a drug she used to ease her menstrual pains. His uncle Gustave did not approve of the relationship, but Camus married Hié to help her fight her addiction. He subsequently discovered she was in a relationship with her doctor at the same time and the couple later divorced. Camus was a womaniser throughout his life.

Camus joined the French Communist Party (PCF) in early 1935. He saw it as a way to "fight inequalities between Europeans and 'natives' in Algeria," even though he was not a Marxist. He explained: "We might see communism as a springboard and asceticism that prepares the ground for more spiritual activities." Camus left the PCF a year later. In 1936, the independence-minded Algerian Communist Party (PCA) was founded, and Camus joined it after his mentor Grenier advised him to do so. Camus's main role within the PCA was to organise the "Théâtre du Travail" ("Workers' Theatre"). Camus was also close to the Parti du Peuple Algérien (Algerian People's Party (PPA)), which was a moderate anti-colonialist/nationalist party. As tensions in the interwar period escalated, the Stalinist PCA and PPA broke ties. Camus was expelled from the PCA for refusing to toe the party line. This series of events sharpened his belief in human dignity. Camus's mistrust of bureaucracies that aimed for efficiency instead of justice grew. He continued his involvement with theatre and renamed his group Théâtre de l'Equipe ("Theatre of the Team"). Some of his scripts were the basis for his later novels.

In 1938, Camus began working for the leftist newspaper "Alger républicain" (founded by Pascal Pia) as he had strong anti-fascist feelings, and the rise of fascist regimes in Europe was worrying him. By then, Camus had developed strong feelings against authoritative colonialism as he witnessed the harsh treatment of the Arabs and Berbers by French authorities. "Alger républicain" was banned in 1940 and Camus flew to Paris to take a new job at "Paris-Soir" as editor-in-chief. In Paris, he almost completed his "first cycle" of works dealing with the absurd and the meaningless—the novel "L'Étranger" ("The Outsider" (UK), or "The Stranger" (US)), the philosophical essay "Le Mythe de Sisyphe" ("The Myth of Sisyphus") and the play "Caligula". Each cycle consisted of a novel, an essay and a theatrical play.

Soon after Camus moved to Paris, the outbreak of World War II began to affect France. Camus volunteered to join the army but was not accepted because he had suffered from tuberculosis. As the Germans were marching towards Paris, Camus fled. He was laid off from "Paris-Soir" and ended up in Lyon, where he married pianist and mathematician Francine Faure on 3 December 1940. Camus and Faure moved back to Algeria (Oran) where he taught in primary schools. Because of his tuberculosis, he moved to the French Alps on medical advice. There he began writing his second cycle of works, this time dealing with revolt—a novel "La Peste" ("The Plague") and a play "Le Malentendu" ("The Misunderstanding"). By 1943 he was known because of his earlier work. He returned to Paris where he met and became friends with Jean-Paul Sartre. He also became part of a circle of intellectuals including Simone de Beauvoir, André Breton, and others. Among them was the actress María Casares, who would later have an affair with Camus.

Camus took an active role in the underground resistance movement against the Germans during the French Occupation. Upon his arrival in Paris, he started working as a journalist and editor of the banned newspaper "Combat". He continued writing for the paper after the liberation of France. Camus used a pseudonym for his "Combat" articles and used false ID cards to avoid being captured. During that period he composed four "Lettres à un Ami Allemand" ("Letters to a German Friend"), explaining why resistance was necessary.

After the War, Camus lived in Paris with Faure, who gave birth to twins, Catherine and Jean in 1945. Camus was now a celebrated writer known for his role in the Resistance. He gave lectures at various universities in the United States and Latin America during two separate trips. He also visited Algeria once more, only to leave disappointed by the continued oppressive colonial policies, which he had warned about many times. During this period he completed the second cycle of his work, with the novel "L'Homme révolté" ("The Rebel"). Camus attacked totalitarian communism while advocating libertarian socialism and anarcho-syndicalism. Upsetting many of his colleagues and contemporaries in France with his rejection of communism, the book brought about the final split with Sartre. His relations with the Marxist Left deteriorated further during the Algerian War.

Camus was a strong supporter of European integration in various marginal organisations working towards that end. In 1944, he founded the "Comité français pour la féderation européenne"—(CFFE (French Committee for the European Federation))—declaring that Europe "can only evolve along the path of economic progress, democracy, and peace if the nation states become a federation." In 1947–48, he founded the Groupes de liaison internationale (GLI) a trade union movement in the context of revolutionary syndicalism ("syndicalisme révolutionnaire"). His main aim was to express the positive side of surrealism and existentialism, rejecting the negativity and the nihilism of André Breton. Camus also raised his voice against the Soviet intervention in Hungary and the totalitarian tendencies of Franco's regime in Spain.

Camus had numerous affairs, particularly an irregular and eventually public affair with the Spanish-born actress María Casares, with whom he had an extensive correspondence. Faure did not take this affair lightly. She had a mental breakdown and needed hospitalisation in the early 1950s. Camus, who felt guilty, withdrew from public life and was slightly depressed for some time.

In 1957, Camus received the news that he was to be awarded the Nobel Prize in Literature. This came as a shock to him. He was anticipating André Malraux would win the prestigious award. At age 44, he was the second-youngest recipient of the prize, after Rudyard Kipling, who was 42. After this he began working on his autobiography "Le Premier Homme" ("The First Man") in an attempt to examine "moral learning". He also turned to the theatre once more. Financed by the money he received with his Nobel Prize, he adapted and directed for the stage Dostoyevsky's novel "Demons". The play opened in January 1959 at the Antoine Theatre in Paris and was a critical success.

During these years, he published posthumously the works of the philosopher Simone Weil, in the series ""Espoir"" ("Hope") which he had founded for Éditions Gallimard. Weil had great influence on his philosophy, since he saw her writings as an "antidote" to nihilism. Camus described her as "the only great spirit of our times".

Camus died on 4 January 1960 at the age of 46, in a car accident near Sens, in Le Grand Fossard in the small town of Villeblevin. He had spent the New Year's holiday of 1960 at his house in Lourmarin, Vaucluse with his family, and his publisher Michel Gallimard of Éditions Gallimard, along with Gallimard's wife, Janine, and daughter. Camus's wife and children went back to Paris by train on 2 January, but Camus decided to return in Gallimard's luxurious Facel Vega HK500. The car crashed into a plane tree on a long straight stretch of the Route nationale 5 (now the RN 6). Camus, who was in the passenger seat and not wearing a safety belt, died instantly. Gallimard died a few days later, although his wife and daughter were unharmed. There has been speculation that Camus was assassinated by the KGB because of his criticism of Soviet abuses.

144 pages of a handwritten manuscript entitled "Le premier Homme" ("The First Man") were found in the wreckage. Camus had predicted that this unfinished novel based on his childhood in Algeria would be his finest work. Camus was buried in the Lourmarin Cemetery, Vaucluse, France, where he had lived. His friend Sartre read a eulogy, paying tribute to Camus's heroic "stubborn humanism".

Camus's first publication was a play called "Révolte dans les Asturies" ("Revolt in the Asturias") written with three friends in May 1936. The subject was the 1934 revolt by Spanish miners that was brutally suppressed by the Spanish government resulting in 1,500 to 2,000 deaths. In May 1937 he wrote his first book, "L'Envers et l'Endroit" ("Betwixt and Between", also translated as "The Wrong Side and the Right Side"). Both were published by Edmond Charlot's small publishing house.
Camus separated his work into three cycles. Each cycle consisted of a novel, an essay, and a play. The first was the cycle of the absurd consisting of "L'Étranger", "Le Mythe de Sysiphe", and "Caligula". The second was the cycle of the revolt which included "La Peste" ("The Plague"), "L'Homme révolté" ("The Rebel"), and "Les Justes" ("The Just Assassins"). The third, the cycle of the love, consisted of "Nemesis". Each cycle was an examination of a theme with the use of a pagan myth and including biblical motifs.

The books in the first cycle were published between 1942 and 1944, but the theme was conceived earlier, at least as far back as 1936. With this cycle, Camus aims to pose a question on the human condition, discuss the world as an absurd place, and warn humanity of the consequences of totalitarianism.

Camus began his work on the second cycle while he was in Algeria, in the last months of 1942, just as the Germans were reaching North Africa. In the second cycle, Camus used Prometheus, who is depicted as a revolutionary humanist, to highlight the nuances between revolution and rebellion. He analyses various aspects of rebellion, its metaphysics, its connection to politics, and examines it under the lens of modernity, of historicity and the absence of a God.

After receiving the Nobel Prize, Camus gathered, clarified, and published his pacifist leaning views at "Actuelles III: Chronique algérienne 1939–1958" ("Algerian Chronicles"). He then decided to distance himself from the Algerian War as he found the mental burden too heavy. He turned to theatre and the third cycle which was about love and the goddess Nemesis.

Two of Camus's works were published posthumously. The first entitled "La mort heureuse" ("A Happy Death") (1970), features a character named Patrice Mersault, comparable to "The Stranger"s Meursault. There is scholarly debate about the relationship between the two books. The second was an unfinished novel, "Le Premier homme" ("The First Man") (1995), which Camus was writing before he died. It was an autobiographical work about his childhood in Algeria and its publication in 1994 sparked a widespread reconsideration of Camus's allegedly unrepentant colonialism.

Camus was a moralist; he claimed morality should guide politics. While he did not deny that morals change over time, he rejected the classical Marxist doctrine that history defines morality.

Camus was also strongly critical of authoritarian communism, especially in the case of the Soviet regime, which he considered totalitarian. Camus rebuked Soviet apologists and their "decision to call total servitude freedom". As a proponent of libertarian socialism, he claimed the USSR was not socialist, and the United States was not liberal. His fierce critique of the USSR caused him to clash with others on the political left, most notably with his friend Jean-Paul Sartre.

Active in the French Resistance to the German occupation of France during World War II, Camus wrote for and edited the famous Resistance journal "Combat". Of the French collaboration with the German occupiers, he wrote: "Now the only moral value is courage, which is useful here for judging the puppets and chatterboxes who pretend to speak in the name of the people." After France's liberation, Camus remarked, "This country does not need a Talleyrand, but a Saint-Just." The reality of the bloody postwar tribunals soon changed his mind: Camus publicly reversed himself and became a lifelong opponent of capital punishment.

Camus leaned towards anarchism, a tendency that intensified in the 1950s, when he came to believe that the Soviet model was morally bankrupt. Camus was firmly against any kind of exploitation, authority and property, bosses, the State and centralization. Philosophy professor David Sherman considers Camus an anarcho-syndicalist. Graeme Nicholson considers Camus an existentialist anarchist.

The anarchist André Prudhommeaux first introduced him at a meeting of the Cercle des Étudiants Anarchistes ("Anarchist Student Circle") in 1948 as a sympathiser familiar with anarchist thought. Camus wrote for anarchist publications such as "Le Libertaire", "La Révolution prolétarienne", and "Solidaridad Obrera" ("Workers' Solidarity"), the organ of the anarcho-syndicalist Confederación Nacional del Trabajo (CNT) ("National Confederation of Labor").

Camus kept a neutral stance during the Algerian Revolution (1954–62). While he was against the violence of the National Liberation Front (FLN) he acknowledged the injustice and brutalities imposed by colonialist France. He was supportive of Pierre Mendès' Unified Socialist Party (PSU) and its approach to the crisis;· Mendes advocated reconciliation. Camus also supported a like-minded Algerian militant, Aziz Kessous. Camus traveled to Algeria to negotiate a truce between the two belligerents but was met with distrust by all parties. His confrontation with an Algerian nationalist during his acceptance speech for the Nobel Prize caused a sensation. When confronted with the dilemma of choosing between his mother and justice, his response was: "I have always condemned terrorism, and I must condemn a terrorism that works blindly in the streets of Algiers and one day might strike at my mother and family. I believe in justice, but I will defend my mother before justice." Camus' critics called it reactionary and a result of a colonialist attitude. According to David Sherman, though, Camus tried to highlight the false dichotomy of the two choices as the use of terrorism and indiscriminate violence could not bring justice under any circumstances.

He was sharply critical of the proliferation of nuclear weapons and the bombings of Hiroshima and Nagasaki. In the 1950s, Camus devoted his efforts to human rights. In 1952, he resigned from his work for UNESCO when the UN accepted Spain, under the leadership of the caudillo General Francisco Franco, as a member. Camus maintained his pacifism and resisted capital punishment anywhere in the world. He wrote an essay against capital punishment in collaboration with Arthur Koestler, the writer, intellectual, and founder of the League Against Capital Punishment entitled "Réflexions sur la peine capitale", published by Calmann-Levy in 1957.

Born in Algeria to French parents, Camus was familiar with the institutional racism of France against Arabs and Berbers, but he was not part of a rich elite. He lived in very poor conditions as a child but was a citizen of France and as such was entitled to citizens' rights; the Arab and Berbers majority of the country were not.

Camus was a vocal advocate of the "new Mediterranean Culture". This was a term he used to describe his vision of embracing the multi-ethnicity of the Algerian people, in opposition to "Latiny", a popular pro-fascist and antisemitic ideology among other "Pieds-Noirs"—or French or Europeans born in Algeria. For Camus, this vision encapsulated the Hellenic humanism which survived among ordinary people around the Mediterranean Sea. His 1938 address on "The New Mediterranean Culture" represents Camus's most systematic statement of his views at this time. Camus also supported the Blum–Viollette proposal to grant Algerians full French citizenship in a manifesto with arguments defending this assimilative proposal on radical egalitarian grounds. In 1939, Camus wrote a stinging series of articles for the "Alger républicain" on the atrocious living conditions of the inhabitants of the Kabylie highlands. He advocated for economic, educational and political reforms as a matter of emergency.

In 1945, following the Sétif and Guelma massacre after Arab revolts against French mistreatment, Camus was one of only a few mainland journalists to visit the colony. He wrote a series of articles reporting on conditions, and advocating for French reforms and concessions to the demands of the Algerian people.

When the Algerian War began in 1954, Camus was confronted with a moral dilemma. He identified with the "Pieds-Noirs" such as his own parents and defended the French government's actions against the revolt. He argued the Algerian uprising was an integral part of the "new Arab imperialism" led by Egypt, and an "anti-Western" offensive orchestrated by Russia to "encircle Europe" and "isolate the United States". Although favoring greater Algerian autonomy or even federation, though not full-scale independence, he believed the "Pieds-Noirs" and Arabs could co-exist. During the war, he advocated a civil truce that would spare the civilians. It was rejected by both sides who regarded it as foolish. Behind the scenes, he began working for imprisoned Algerians who faced the death penalty. His position drew much criticism from the left who considered colonialism unacceptable. In their eyes, Camus was no longer the defender of the oppressed.

Camus once confided that the troubles in Algeria "affected him as others feel pain in their lungs."

Even though Camus is mostly connected to Absurdism, he is routinely categorized as an Existentialist, a term he rejected on several occasions.

Camus himself said his philosophical origins lay in ancient Greek philosophy, Nietzsche, and 17th-century moralists whereas existentialism arises from 19th- and early 20th-century philosophy such as Kierkegaard, Karl Jaspers, and Heidegger. He also said his work, "The Myth of Sisyphus", was a criticism of various aspects of existentialism. Camus was rejecting existentialism as a philosophy, but his critique was mostly focused on Sartrean existentialism, and to a lesser extent on religious existentialism. He thought that the importance of history held by Marx and Sartre was incompatible with his belief in human freedom. David Sherman and others also suggest the rivalry between Sartre and Camus also played a part in his rejection of existentialism. David Simpson argues further that his humanism and belief in human nature set him apart from the existentialist doctrine that existence precedes essence.

On the other hand, Camus focused most of his philosophy around existential questions. The absurdity of life, the inevitable ending (death) is highlighted in his acts. His belief was that the absurd—life being void of meaning, or man's inability to know that meaning if it were to exist—was something that man should embrace. His anti-Christianity, his commitment to individual moral freedom and responsibility are only a few of the similarities with other existential writers. More importantly, Camus addressed one of the fundamental questions of existentialism: the problem of suicide. He wrote: "There is only one really serious philosophical question, and that is suicide." Camus viewed the question of suicide as arising naturally as a solution to the absurdity of life.

Many existentialist writers have addressed the Absurd, each with their own interpretation of what it is and what makes it important. Kierkegaard explains that the absurdity of religious truths prevents us from reaching God rationally. Sartre recognizes the absurdity of individual experience. Camus's thoughts on the Absurd begins with his first cycle of books and the literary essay "The Myth of Sisyphus", ("Le Mythe de Sisyphe"), his major work on the subject. In 1942 he published the story of a man living an absurd life in "L'Étranger". He also wrote a play about the Roman emperor Caligula, pursuing an absurd logic, which was not performed until 1945. His early thoughts appeared in his first collection of essays, "L'Envers et l'endroit" ("Betwixt and Between") in 1937. Absurd themes were expressed with more sophistication in his second collection of essays, "Noces" ("Nuptials"), in 1938 and "Betwixt and Between". In these essays, Camus reflects on the experience of the Absurd. Aspects of the notion of the Absurd can be found in "The Plague".

Camus follows Sartre's definition of the Absurd: "That which is meaningless. Thus man's existence is absurd because his contingency finds no external justification". The Absurd is created because man, who is placed in an unintelligent universe, realises that human values are not founded on a solid external component; or as Camus himself explains, the Absurd is the result of the "confrontation between human need and the unreasonable silence of the world." Even though absurdity is inescapable, Camus does not drift towards nihilism. But the realization of absurdity leads to the question: Why should someone continue to live? Suicide is an option that Camus firmly dismisses as the renunciation of human values and freedom. Rather, he proposes we accept that absurdity is a part of our lives and live with it.

The turning point in Camus's attitude to the Absurd occurs in a collection of four letters to an anonymous German friend, written between July 1943 and July 1944. The first was published in the "Revue Libre" in 1943, the second in the "Cahiers de Libération" in 1944, and the third in the newspaper "Libertés", in 1945. The four letters were published as "Lettres à un ami allemand" ("Letters to a German Friend") in 1945, and were included in the collection "Resistance, Rebellion, and Death".

Camus regretted the continued reference to himself as a "philosopher of the absurd". He showed less interest in the Absurd shortly after publishing "Le Mythe de Sisyphe". To distinguish his ideas, scholars sometimes refer to the Paradox of the Absurd, when referring to "Camus's Absurd".

Camus is known for articulating the case for revolting against any kind of oppression, injustice, or whatever disrespects the human condition. He is cautious enough, however, to set the limits on the rebellion. "L'Homme révolté" ("The Rebel") explains in detail his thoughts on the issue. There, he builds upon the absurd (described in "The Myth of Sisyphus") but goes further. In the introduction, where he examines the metaphysics of rebellion, he concludes with the phrase "I revolt, therefore we exist" implying the recognition of a common human condition. Camus also delineates the difference between revolution and rebellion and notices that history has shown that the rebel's revolution might easily end up as an oppressive regime; he therefore places importance on the morals accompanying the revolution. Camus poses a crucial question: Is it possible for humans to act in an ethical and meaningful manner, in a silent universe? According to him the answer is yes, as the experience and awareness of the Absurd creates the moral values and also sets the limits of our actions. Camus separates the modern form of rebellion into two modes. First, there is the metaphysical rebellion, which is "the movement by which man protests against his condition and against the whole of creation." The other mode, historical rebellion, is the attempt to materialize the abstract spirit of metaphysical rebellion and change the world. In this attempt, the rebel must balance between the evil of the world and the intrinsic evil which every revolt carries, and not cause any unjustifiable suffering.

Camus's novels and philosophical essays are still influential. After his death, interest in Camus followed the rise (and diminution) of the New Left. Following the collapse of Soviet Union, interest in his alternative road to communism resurfaced. He is remembered for his skeptical humanism and his support for political tolerance, dialogue, and civil rights.

Although Camus has been linked to anti-Soviet communism, reaching as far as anarcho-syndicalism, some neo-liberals have tried to associate him with their policies; for instance, the French President Nicolas Sarkozy suggested that his remains be moved to the Panthéon, an idea that angered many on the Left.

The works of Albert Camus include:











</doc>
