<doc id="24403" url="https://en.wikipedia.org/wiki?curid=24403" title="Presbyterianism">
Presbyterianism

Presbyterianism is a part of the Reformed tradition within Protestantism, which traces its origins to Great Britain, particularly Scotland.

Presbyterian churches derive their name from the presbyterian form of church government, which is governed by representative assemblies of elders. A great number of Reformed churches are organized this way, but the word "Presbyterian", when capitalized, is often applied uniquely to churches that trace their roots to the Church of Scotland, as well as several English dissenter groups that formed during the English Civil War. Presbyterian theology typically emphasizes the sovereignty of God, the authority of the Scriptures, and the necessity of grace through faith in Christ. Presbyterian church government was ensured in Scotland by the Acts of Union in 1707, which created the Kingdom of Great Britain. In fact, most Presbyterians found in England can trace a Scottish connection, and the Presbyterian denomination was also taken to North America, mostly by Scots and Scots-Irish immigrants. The Presbyterian denominations in Scotland hold to the Reformed theology of John Calvin and his immediate successors, although there is a range of theological views within contemporary Presbyterianism. Local congregations of churches which use presbyterian polity are governed by sessions made up of representatives of the congregation (elders); a conciliar approach which is found at other levels of decision-making (presbytery, synod and general assembly).

The roots of Presbyterianism lie in the Reformation of the 16th century, the example of John Calvin's Republic of Geneva being particularly influential. Most Reformed churches that trace their history back to Scotland are either presbyterian or congregationalist in government. In the twentieth century, some Presbyterians played an important role in the ecumenical movement, including the World Council of Churches. Many Presbyterian denominations have found ways of working together with other Reformed denominations and Christians of other traditions, especially in the World Communion of Reformed Churches. Some Presbyterian churches have entered into unions with other churches, such as Congregationalists, Lutherans, Anglicans, and Methodists. Presbyterians in the United States came largely from Scottish immigrants, Scots-Irish immigrants, and also from New England Yankee communities that had originally been Congregational but changed because of an agreed-upon Plan of Union of 1801 for frontier areas. Historically, along with Lutherans and Episcopalians, Presbyterians tend to be considerably wealthier and better educated (having more graduate and post-graduate degrees per capita) than most other religious groups in United States, and are disproportionately represented in the upper reaches of American business, law and politics.

Presbyterian tradition, particularly that of the Church of Scotland, traces its early roots to the Church founded by Saint Columba, through the 6th century Hiberno-Scottish mission. Tracing their apostolic origin to Saint John, the Culdees practiced Christian monasticism, a key feature of Celtic Christianity in the region, with a presbyter exercising "authority within the institution, while the different monastic institutions were independent of one another." The Church in Scotland kept the Christian feast of Easter at a date different from the See of Rome and its monks used a unique style of tonsure. The Synod of Whitby in 664, however, ended these distinctives as it ruled "that Easter would be celebrated according to the Roman date, not the Celtic date." Although Roman influence came to dominate the Church in Scotland, certain Celtic influences remained in the Scottish Church, such as "the singing of metrical psalms, many of them set to old Celtic Christianity Scottish traditional and folk tunes", which later became a "distinctive part of Scottish Presbyterian worship".

Presbyterian history is part of the history of Christianity, but the beginning of Presbyterianism as a distinct movement occurred during the 16th-century Protestant Reformation. As the Catholic Church resisted the reformers, several different theological movements splintered from the Church and bore different denominations. Presbyterianism was especially influenced by the French theologian John Calvin, who is credited with the development of Reformed theology, and the work of John Knox, a Scotsman and a Roman Catholic Priest, who studied with Calvin in Geneva, Switzerland. He brought back Reformed teachings to Scotland. The Presbyterian church traces its ancestry back primarily to England and Scotland. In August 1560 the Parliament of Scotland adopted the "Scots Confession" as the creed of the Scottish Kingdom. In December 1560, the "First Book of Discipline" was published, outlining important doctrinal issues but also establishing regulations for church government, including the creation of ten ecclesiastical districts with appointed superintendents which later became known as presbyteries.

In time, the Scots Confession would be supplanted by the Westminster Confession of Faith, and the Larger and Shorter Catechisms, which were formulated by the Westminster Assembly between 1643 and 1649.

Presbyterians distinguish themselves from other denominations by doctrine, institutional organization (or "church order") and worship; often using a "Book of Order" to regulate common practice and order. The origins of the Presbyterian churches are in Calvinism. Many branches of Presbyterianism are remnants of previous splits from larger groups. Some of the splits have been due to doctrinal controversy, while some have been caused by disagreement concerning the degree to which those ordained to church office should be required to agree with the Westminster Confession of Faith, which historically serves as an important confessional document – second only to the Bible, yet directing particularities in the standardization and translation of the Bible – in Presbyterian churches.

Presbyterians place great importance upon education and lifelong learning, tempered with the knowledge that no human action can affect salvation.

Continuous study of the scriptures, theological writings, and understanding and interpretation of church doctrine are embodied in several statements of faith and catechisms formally adopted by various branches of the church, often referred to as "subordinate standards".

Presbyterian government is by councils (properly known as "courts") of elders. Teaching and ruling elders are ordained and convene in the lowest council known as a "session" or "consistory" responsible for the discipline, nurture, and mission of the local . Teaching elders (pastors or ministers) have responsibility for teaching, worship, and performing sacraments. Pastors or ministers are called by individual congregations. A congregation issues a call for the pastor or minister's service, but this call must be ratified by the local presbytery. The pastor or minister is a teaching elder, and Moderator of the Session, but is not usually a member of the congregation.

Ruling elders are men and women who are elected by the congregation and ordained to serve with the teaching elders, assuming responsibility for nurture and leadership of the congregation. Often, especially in larger congregations, the elders delegate the practicalities of buildings, finance, and temporal ministry to the needy in the congregation to a distinct group of officers (sometimes called deacons, which are ordained in some denominations). This group may variously be known as a "Deacon Board", "Board of Deacons" "Diaconate", or "Deacons' Court". These are sometimes known as "presbyters" to the full congregation.

Above the sessions exist presbyteries, which have area responsibilities. These are composed of teaching elders and ruling elders from each of the constituent congregations. The presbytery sends representatives to a broader regional or national assembly, generally known as the General Assembly, although an intermediate level of a "synod" sometimes exists. This congregation / presbytery / synod / general assembly schema is based on the historical structure of the larger Presbyterian churches, such as the Church of Scotland or the Presbyterian Church (U.S.A.); some bodies, such as the Presbyterian Church in America and the Presbyterian Church in Ireland, skip one of the steps between congregation and General Assembly, and usually the step skipped is the Synod. The Church of Scotland abolished the Synod in 1993.

Presbyterian governance is practised by Presbyterian denominations and also by many other Reformed churches.

Presbyterianism is historically a confessional tradition. This has two implications. The obvious one is that confessional churches express their faith in the form of "confessions of faith," which have some level of authoritative status. However this is based on a more subtle point: In confessional churches, theology is not solely an individual matter. While individuals are encouraged to understand Scripture, and may challenge the current institutional understanding, theology is carried out by the community as a whole. It is this community understanding of theology that is expressed in confessions.

However, there has arisen a spectrum of approaches to confessionalism. The manner of subscription, or the degree to which the official standards establish the actual doctrine of the church, turns out to be a practical matter. That is, the decisions rendered in ordination and in the courts of the church largely determine what the church means, representing the whole, by its adherence to the doctrinal standard.

Some Presbyterian traditions adopt only the Westminster Confession of Faith as the doctrinal standard to which teaching elders are required to subscribe, in contrast to the Larger and Shorter catechisms, which are approved for use in instruction. Many Presbyterian denominations, especially in North America, have adopted all of the Westminster Standards as their standard of doctrine which is subordinate to the Bible. These documents are Calvinistic in their doctrinal orientation. The Presbyterian Church in Canada retains the Westminster Confession of Faith in its original form, while admitting the historical period in which it was written should be understood when it is read.

The Westminster Confession is "The principal subordinate standard of the Church of Scotland" but "with due regard to liberty of opinion in points which do not enter into the substance of the Faith" (V). This formulation represents many years of struggle over the extent to which the confession reflects the Word of God and the struggle of conscience of those who came to believe it did not fully do so (e.g. William Robertson Smith). Some Presbyterian Churches, such as the Free Church of Scotland, have no such "conscience clause".

The Presbyterian Church (U.S.A.) has adopted the "Book of Confessions", which reflects the inclusion of other Reformed confessions in addition to the Westminster Standards. These other documents include ancient creedal statements (the Nicene Creed, the Apostles' Creed), 16th-century Reformed confessions (the Scots Confession, the Heidelberg Catechism, the Second Helvetic Confession), and 20th century documents (The Theological Declaration of Barmen, Confession of 1967 and A Brief Statement of Faith).

The Presbyterian Church in Canada developed the confessional document "Living Faith" (1984) and retains it as a subordinate standard of the denomination. It is confessional in format, yet like the Westminster Confession, draws attention back to original Bible text.

Presbyterians in Ireland who rejected Calvinism and the Westminster Confessions formed the Non-subscribing Presbyterian Church of Ireland.

Presbyterian denominations that trace their heritage to the British Isles usually organise their church services inspired by the principles in the Directory of Public Worship, developed by the Westminster Assembly in the 1640s. This directory documented Reformed worship practices and theology adopted and developed over the preceding century by British Puritans, initially guided by John Calvin and John Knox. It was enacted as law by the Scottish Parliament, and became one of the foundational documents of Presbyterian church legislation elsewhere.
Historically, the driving principle in the development of the standards of Presbyterian worship is the Regulative principle of worship, which specifies that (in worship), what is not commanded is forbidden.

Over subsequent centuries, many Presbyterian churches modified these prescriptions by introducing hymnody, instrumental accompaniment, and ceremonial vestments into worship. However, there is not one fixed "Presbyterian" worship style. Although there are set services for the Lord's Day in keeping with first-day Sabbatarianism, one can find a service to be evangelical and even revivalist in tone (especially in some conservative denominations), or strongly liturgical, approximating the practices of Lutheranism or Anglicanism (especially where Scottish tradition is esteemed), or semi-formal, allowing for a balance of hymns, preaching, and congregational participation (favored by probably most American Presbyterians). Most Presbyterian churches follow the traditional liturgical year and observe the traditional holidays, holy seasons, such as Advent, Christmas, Ash Wednesday, Holy Week, Easter, Pentecost, etc. They also make use of the appropriate seasonal liturgical colors, etc. Many incorporate ancient liturgical prayers and responses into the communion services and follow a daily, seasonal, and festival lectionary. Other Presbyterians, however, such as the Reformed Presbyterians, would practice a cappella exclusive psalmody, as well as eschew the celebration of holy days.

Among the paleo-orthodox and emerging church movements in Protestant and evangelical churches, in which some Presbyterians are involved, clergy are moving away from the traditional black Geneva gown to such vestments as the alb and chasuble, but also cassock and surplice (typically a full length Old English style surplice which resembles the Celtic alb, an ungirdled liturgical tunic of the old Gallican Rite), which some, particularly those identifying with the Liturgical Renewal Movement, hold to be more ancient and representative of a more ecumenical past.

Presbyterians traditionally have held the Worship position that there are only two sacraments:

Early Presbyterians were careful to distinguish between the "church," which referred to the "members", and the "meeting house," which was the building in which the church met. Until the late 19th century, very few Presbyterians ever referred to their buildings as "churches." Presbyterians believed that meeting-houses (now called churches) are buildings to support the worship of God. The decor in some instances was austere so as not to detract from worship. Early Presbyterian meeting-houses were extremely plain. No stained glass, no elaborate furnishings, and no images were to be found in the meeting-house. The pulpit, often raised so as only to be accessible by a staircase, was the centerpiece of the building. But these were not the standard characteristics of the mainline Presbyterians. These were more of the wave of Presbyterians that were influenced by the Puritans and simplicity.

In the late 19th century a gradual shift began to occur. Prosperous congregations built imposing churches, such as Fourth Presbyterian Church of Chicago and Brick Presbyterian Church in New York City, Shadyside Presbyterian Church in PA, St Stephen Presbyterian in Fort Worth Texas and many many others.

Usually a Presbyterian church will not have statues of saints, nor the ornate altar more typical of a Roman Catholic church. Instead, one will find a "communion table," usually on the same level as the congregation. There may be a rail between the communion table and the "Chancel" behind it, which may contain a more decorative altar-type table, choir loft, or choir stalls, lectern and clergy area. The altar is called the communion table and the altar area is called the Chancel by Presbyterians. In a Presbyterian (Reformed Church) there may be an altar cross, either on the communion table or on a table in the chancel. By using the "empty" cross, or cross of the resurrection, Presbyterians emphasize the resurrection and that Christ is not continually dying, but died once and is alive for all eternity. Some Presbyterian church buildings are often decorated with a cross that has a circle around the center, or Celtic cross. This not only emphasized the resurrection, but also acknowledges historical aspects of Presbyterianism. A baptismal font will be located either at the entrance or near the chancel area. Presbyterian architecture generally makes significant use of symbolism. You may also find decorative and ornate stained glass windows depicting scenes from the bible. Some Presbyterian churches will also have ornate statues of Christ or Graven Scenes from the Last Supper located behind the Chancel. St. Giles Cathedral ( Church of Scotland- The Mother Church of Presbyterians) does have a Crucifix next to one of the Pulpits that hangs alongside. The image of Christ is more of faint image and more modern design. 
John Knox (1505–1572), a Scot who had spent time studying under Calvin in Geneva, returned to Scotland and urged his countrymen to reform the Church in line with Calvinist doctrines. After a period of religious convulsion and political conflict culminating in a victory for the Protestant party at the Siege of Leith the authority of the Catholic Church was abolished in favour of Reformation by the legislation of the Scottish Reformation Parliament in 1560. The Church was eventually organised by Andrew Melville along Presbyterian lines to become the national Church of Scotland. King James VI and I moved the Church of Scotland towards an episcopal form of government, and in 1637, James' successor, Charles I and William Laud, the Archbishop of Canterbury, attempted to force the Church of Scotland to use the Book of Common Prayer. What resulted was an armed insurrection, with many Scots signing the "Solemn League and Covenant". The Covenanters would serve as the government of Scotland for nearly a decade, and would also send military support to the Parliamentarians during the English Civil War. Following the restoration of the monarchy in 1660, Charles II, despite the initial support that he received from the Covenanters, reinstated an episcopal form of government on the church.
However, with the Glorious Revolution of 1688 the Church of Scotland was finally unequivocally recognised as a Presbyterian institution by the monarch due to Scottish Presbyterian support for the aforementioned revolution and the Acts of Union 1707 between Scotland and England guaranteed the Church of Scotland's form of government. However, legislation by the United Kingdom parliament allowing patronage led to splits in the Church. In 1733, a group of ministers seceded from the Church of Scotland to form the Associate Presbytery, another group seceded in 1761 to form the Relief Church and the Disruption of 1843 led to the formation of the Free Church of Scotland. Further splits took place, especially over theological issues, but most Presbyterians in Scotland were reunited by 1929 union of the established Church of Scotland and the United Free Church of Scotland.

There are now eight Presbyterian denominations in Scotland today. These are, in order of size: the Church of Scotland, the Free Church of Scotland, the United Free Church of Scotland, the Free Presbyterian Church of Scotland, the Free Church of Scotland (Continuing), the Associated Presbyterian Church, the Reformed Presbyterian Church of Scotland, and the International Presbyterian Church. Combined, they have over 1500 congregations in Scotland.

Within Scotland the term kirk is usually used to refer to a local Presbyterian church. Informally, the term 'The Kirk' refers to the Church of Scotland. Some of the values and ideals espoused in Scottish Presbyterian denominations can be reflected in this reference in a book from Norman Drummond, chaplain to the Queen in Scotland.

In England, Presbyterianism was established in secret in 1592. Thomas Cartwright is thought to be the first Presbyterian in England. Cartwright's controversial lectures at Cambridge University condemning the episcopal hierarchy of the Elizabethan Church led to his deprivation of his post by Archbishop John Whitgift and his emigration abroad. Between 1645 and 1648, a series of ordinances of the Long Parliament established Presbyterianism as the polity of the Church of England. Presbyterian government was established in London and Lancashire and in a few other places in England, although Presbyterian hostility to the execution of Charles I and the establishment of the republican Commonwealth of England meant that Parliament never enforced the Presbyterian system in England. The re-establishment of the monarchy in 1660 brought the return of Episcopal church government in England (and in Scotland for a short time); but the Presbyterian church in England continued in Non-Conformity, outside of the established church. In 1719 a major split, the Salter's Hall controversy, occurred; with the majority siding with nontrinitarian views. Thomas Bradbury published several sermons bearing on the controversy, and in 1719, "An answer to the reproaches cast on the dissenting ministers who subscribed their belief of the Eternal Trinity.". By the 18th century many English Presbyterian congregations had become Unitarian in doctrine.

A number of new Presbyterian Churches were founded by Scottish immigrants to England in the 19th century and later. Following the 'Disruption' in 1843 many of those linked to the Church of Scotland eventually joined what became the Presbyterian Church of England in 1876. Some, that is Crown Court (Covent Garden, London), St Andrew's (Stepney, London) and Swallow Street (London), did not join the English denomination, which is why there are Church of Scotland congregations in England such as those at Crown Court, and St Columba's, Pont Street (Knightsbridge) in London. There is also a congregation in the heart of London's financial district called London City Presbyterian Church that is also affiliated with Free Church of Scotland.

In 1972, the Presbyterian Church of England (PCofE) united with the Congregational Church in England and Wales to form the United Reformed Church (URC). Among the congregations the PCofE brought to the URC were Tunley (Lancashire), Aston Tirrold (Oxfordshire) and John Knox Presbyterian Church, Stepney, London (now part of Stepney Meeting House URC) – these are among the sole survivors today of the English Presbyterian churches of the 17th century. The URC also has a presence in Scotland, mostly of former Congregationalist Churches. Two former Presbyterian congregations, St Columba's, Cambridge (founded in 1879), and St Columba's, Oxford (founded as a chaplaincy by the PCofE and the Church of Scotland in 1908 and as a congregation of the PCofE in 1929), continue as congregations of the URC and university chaplaincies of the Church of Scotland.

In recent years a number of smaller denominations adopting Presbyterian forms of church government have organised in England, including the International Presbyterian Church planted by evangelical theologian Francis Schaeffer of L'Abri Fellowship in the 1970s, and the Evangelical Presbyterian Church in England and Wales founded in the North of England in the late 1980s.

In Wales, Presbyterianism is represented by the Presbyterian Church of Wales, which was originally composed largely of Calvinistic Methodists who accepted Calvinist theology rather than the Arminianism of the Wesleyan Methodists. They broke off from the Church of England in 1811, ordaining their own ministers. They were originally known as the Calvinist Methodist connexion and in the 1920s it became alternatively known as the Presbyterian Church of Wales.

Presbyterianism is the largest Protestant denomination in Northern Ireland and the second largest on the island of Ireland (after the Anglican Church of Ireland), and was brought by Scottish plantation settlers to Ulster who had been strongly encouraged to emigrate by James VI of Scotland, also James I of Ireland and England. An estimated 100,000 Scottish Presbyterians moved to the northern counties of Ireland between 1607 and the Battle of the Boyne in 1690. The Presbytery of Ulster was formed in 1642 separately from the established Anglican Church. Presbyterians, along with Roman Catholics in Ulster and the rest of Ireland, suffered under the discriminatory Penal Laws until they were revoked in the early 19th century. Presbyterianism is represented in Ireland by the Presbyterian Church in Ireland, the Non-subscribing Presbyterian Church of Ireland, the Free Presbyterian Church of Ulster, the Reformed Presbyterian Church of Ireland and the Evangelical Presbyterian Church.

There is a Church of Scotland (Presbyterian) in central Paris: The Scots Kirk, which is English-speaking, and is attended by many nationalities. It maintains close links with the Church of Scotland in Scotland itself, as well as with the Reformed Church of France.

The Waldensian Evangelical Church (Chiesa Evangelica Valdese, CEV) is an Italian Protestant denomination.
The church was founded in the 12th century, and centuries later, after the Protestant Reformation, it adhered to Calvinist theology and became the Italian branch of the Presbyterian churches. As such, the church is a member of the World Communion of Reformed Churches.

Even before Presbyterianism spread with immigrants abroad from Scotland, there were divisions in the larger Presbyterian family. Some later rejoined only to separate again. In what some interpret as rueful self-reproach, some Presbyterians refer to the divided Presbyterian churches as the "Split Ps".

Presbyterianism first officially arrived in Colonial America in 1644 with the establishment of Christ's First Presbyterian Church in Hempstead, NY. The Church was organized by the Rev. Richard Denton.

Another notable church was established in 1703 the first Presbytery in Philadelphia. In time, the presbytery would be joined by two more to form a synod (1717) and would eventually evolve into the Presbyterian Church in the United States of America in 1789. The nation's largest Presbyterian denomination, the Presbyterian Church (U.S.A.) – PC (USA) – can trace their heritage back to the original PCUSA, as can the Presbyterian Church in America (PCA), the Orthodox Presbyterian Church (OPC), the Bible Presbyterian Church (BPC), the Cumberland Presbyterian Church (CPC), the Cumberland Presbyterian Church in America, the Evangelical Presbyterian Church (EPC), and the Evangelical Covenant Order of Presbyterians (ECO).

Other Presbyterian bodies in the United States include the Reformed Presbyterian Church of North America (RPCNA), the Associate Reformed Presbyterian Church (ARP), the Reformed Presbyterian Church in the United States (RPCUS), the Reformed Presbyterian Church General Assembly, the Reformed Presbyterian Church – Hanover Presbytery, the Covenant Presbyterian Church, the Presbyterian Reformed Church, the Westminster Presbyterian Church in the United States, the Korean American Presbyterian Church, and the Free Presbyterian Church of North America.
The territory within about a radius of Charlotte, North Carolina, is historically the greatest concentration of Presbyterianism in the Southern United States, while an almost identical geographic area around Pittsburgh, Pennsylvania, contains probably the largest number of Presbyterians in the entire nation.

The PC(USA), beginning with its predecessor bodies, has, in common with other so-called "mainline" Protestant denominations, experienced a significant decline in members in recent years. Some estimates have placed that loss at nearly half in the last forty years.

Presbyterian influence, especially through Princeton theology, can be traced in modern Evangelicalism. Balmer says that:
In the late 1800s, Presbyterian missionaries established a presence in what is now northern New Mexico. This provided an alternative to the Catholicism, which was brought to the area by the Spanish Conquistadors and had remained unchanged. The area experienced a "mini" reformation, in that many converts were made to Presbyterianism, prompting persecution. In some cases, the converts left towns and villages to establish their own neighboring villages. The arrival of the United States to the area prompted the Catholic church to modernize and make efforts at winning the converts back, many of which did return. However, there are still stalwart Presbyterians and Presbyterian churches in the area.

In Canada, the largest Presbyterian denomination – and indeed the largest Protestant denomination – was the Presbyterian Church in Canada, formed in 1875 with the merger of four regional groups. In 1925, the United Church of Canada was formed by the majority of Presbyterians combining with the Methodist Church, Canada, and the Congregational Union of Canada. A sizable minority of Canadian Presbyterians, primarily in southern Ontario but also throughout the entire nation, withdrew, and reconstituted themselves as a non-concurring continuing Presbyterian body. They regained use of the original name in 1939.

Presbyterianism arrived in Latin America in the 19th century.

The biggest Presbyterian church is the National Presbyterian Church in Mexico ("Iglesia Nacional Presbiteriana de México"), which has around 2,500,000 members and associates and 3000 congregations, but there are other small denominations like the Associate Reformed Presbyterian Church in Mexico which was founded in 1875 by the Associate Reformed Church in North America. The Independent Presbyterian Church and the Presbyterian Reformed Church in Mexico, the National Conservative Presbyterian Church in Mexico are existing churches in the Reformed tradition.

In Brazil, the Presbyterian Church of Brazil ("Igreja Presbiteriana do Brasil") totals approximately 1,011,300 members; other Presbyterian churches (Independents, United, Conservatives, Renovated, etc.) in this nation have around 350,000 members. The Renewed Presbyterian Church in Brazil was influenced by the charismatic movement and has about 131 000 members as of 2011. The Conservative Presbyterian Church was founded in 1940 and has eight presbyteries. The Fundamentalist Presbyterian church in Brazil was influenced by Karl McIntire and the Bible Presbyterian church USA and has around 1 800 members. The Independent Presbyterian Church in Brasil was founded in 1903 by pastor Pereira, has 500 congregations and 75 000 members. The United Presbyterian Church in Brazil has around 4 000 members. There are also ethnic Korean Presbyterian churches in the country. The Evangelical Reformed Church in Brazil has Dutch origin. The Reformed Churches in Brazil were recently founded by the Canadian Reformed Churches with the Reformed Church in the Netherlands (liberated).

Congregational churches present in the country are also part of the Calvinistic tradition in Latin America.

There are probably more than four million members of Presbyterian churches in all of Latin America. Presbyterian churches are also present in Peru, Bolivia, Cuba, Trinidad and Tobago, Venezuela, Colombia, Chile, Paraguay, Costa Rica, Nicaragua, Argentina, Honduras and others, but with few members. The Presbyterian Church in Belize has 14 churches and church plants and there is a Reformed Seminary founded in 2004. Some Latin Americans in North America are active in the Presbyterian Cursillo Movement.

Presbyterianism arrived in Africa in the 19th century through the work of Scottish missionaries and founded churches such as St Michael and All Angels Church, Blantyre, Malawi. The church has grown extensively and now has a presence in at least 23 countries in the region.

African Presbyterian churches often incorporate diaconal ministries, including social services, emergency relief, and the operation of mission hospitals. A number of partnerships exist between presbyteries in Africa and the PC(USA), including specific connections with Lesotho, Cameroon, Malawi, South Africa, Ghana and Zambia. For example, the Lackawanna Presbytery, located in Northeastern Pennsylvania, has a partnership with a presbytery in Ghana. Also the Southminster Presbyterian Church, located near Pittsburgh, has partnerships with churches in Malawi and Kenya. The Presbyterian Church of Nigeria, western Africa is also healthy and strong in mostly the southern states of this nation, strong density in the south-eastern states of this country. Beginning from Cross River state, the nearby coastal states, Rivers state, Lagos state to Ebonyi and Abia States. The missionary expedition of Mary Slessor and Hope Waddel and their group in the mid 18th century in this coastal regions of the ten British colony has brought about the beginning and the flourishing of this church in these areas.

The Presbyterian Church of East Africa, based in Kenya, is particularly strong, with 500 clergy and 4 million members.

The Reformed Presbyterian Church in Malawi has 150 congregations and 17 000–20 000 members. It was a mission of the Free Presbyterian church of Scotland. The Restored Reformed Church works with RPCM. Evangelical Presbyterian Church in Malawi is an existing small church. Part of the Presbyterian Church in Malawi and Zambia is known as CCAP, Church of Central Africa-Presbyterian. Often the churches there have one main congregation and a number of Prayer Houses develop. Education, health ministries as well as worship and spiritual development are important.

Southern Africa is a major base of Reformed and Presbyterian Churches.

In addition, there are a number of Presbyterian Churches in north Africa, the most known is the Nile Synod in Egypt and a recently founded synod for Sudan.

Cumberland Presbyterian Church Yao Dao Secondary School is a Presbyterian school in Yuen Long, New Territories. The Cumberland Presbyterian Church also have a church on the island of Cheung Chau. There are also Korean Christians resident in Hong Kong who are Presbyterians.

Presbyterian Churches are the biggest and by far the most influential Protestant denominations in South Korea, with close to 20,000 churches affiliated with the two largest Presbyterian denominations in the country. In South Korea there are 9 million Presbyterians, forming the majority of the 15 million Korean Protestants. In South Korea there are 100 different Presbyterian denominations.

Most of the Korean Presbyterian denominations share the same name in Korean, 대한예수교장로회 (literally means the Presbyterian Church of Korea or PCK), tracing its roots to the United Presbyterian Assembly before its long history of disputes and schisms. The Presbyterian schism began with the controversy in relation to the Japanese shrine worship enforced during the Japanese colonial period and the establishment of a minor division (Koryu-pa, 고려파, later The Koshin Presbyterian Church in Korea, Koshin 고신) in 1952. And in 1953 the second schism happened when the theological orientation of the Chosun Seminary (later Hanshin University) founded in 1947 could not be tolerated in the PCK and another minor group (The Presbyterian Church in the Republic of Korea, Kijang, 기장) was separated. The last major schism had to do with the issue of whether the PCK should join the WCC. The controversy divided the PCK into two denominations, The Presbyterian Church of Korea (Tonghap, 통합) and The General Assembly of Presbyterian Church in Korea (Hapdong, 합동) in 1959. All major seminaries associated with each denomination claim heritage from the Pyung Yang Theological Seminary, therefore, not only Presbyterian University and Theological Seminary and Chongsin University which are related to PCK but also Hanshin University of PROK all celebrated the 100th class in 2007, 100 years from the first graduates of Pyung Yang Theological Seminary.

Korean Presbyterian denominations are active in evangelism and many of its missionaries are being sent overseas, being the second biggest missionary sender in the world after the United States. GMS, the missionary body of the "Hapdong" General Assembly of Presbyterian Churches of Korea, is the single largest Presbyterian missionary organization in Korea.
In addition there are many Korean-American Presbyterians in the United States, either with their own church sites or sharing space in pre-existing churches as is the case in Australia, New Zealand and even Muslim countries such as Saudi Arabia with Korean immigration.

The Korean Presbyterian Church started through the mission of the Presbyterian Church (USA) and the Australian Presbyterian theological tradition is central to the United States. But after independence, the 'Presbyterian Church in Korea (KoRyuPa)' advocated a Dutch Reformed position. In the 21st century, a new General Assembly of the Orthodox Presbyterian Church of Korea (Founder. Ha Seung-moo) in 2012 declared itself an authentic historical succession of Scottish Presbyterian John Knox.

The Presbyterian Church in Taiwan (PCT) is by far the largest Protestant denomination in Taiwan, with some 238,372 members as of 2009 (including a majority of the island's aborigines). English Presbyterian Missionary James Laidlaw Maxwell established the first Presbyterian church in Tainan in 1865. His colleague George Leslie Mackay, of the Canadian Presbyterian Mission, was active in Tamsui and north Taiwan from 1872 to 1901; he founded the island's first university and hospital, and created a written script for Taiwanese Minnan. The English and Canadian missions joined together as the PCT in 1912. One of the few churches permitted to operate in Taiwan through the era of Japanese rule (1895–1945), the PCT experienced rapid growth during the era of Kuomintang-imposed martial law (1949–1987), in part due to its support for democracy, human rights, and Taiwan independence. Former ROC president Lee Teng-hui (in office 1988–2000) is a Presbyterian.

In the mainly Christian Indian state of Mizoram, Presbyterianism is the largest of all Christian denominations. It was brought there by missionaries from Wales in 1894. Prior to Mizoram, Welsh Presbyterians started venturing into the northeast India through the Khasi Hills (now in the state of Meghalaya in India) and established Presbyterian churches all over the Khasi Hills from the 1840s onwards. Hence, there is a strong presence of Presbyterians in Shillong (the present capital of Meghalaya) and the areas adjoining it. The Welsh missionaries built their first church in Sohra (aka Cherrapunji) in 1846. The Presbyterian church in India was integrated in 1970 into the United Church of Northern India (originally formed in 1924). It is the largest Presbyterian denomination in India.

In Australia, Presbyterianism is the fourth largest denomination of Christianity, with nearly 600,000 Australians claiming to be Presbyterian in the 2006 Commonwealth Census. Presbyterian churches were founded in each colony, some with links to the Church of Scotland and others to the Free Church. There were also congregations originating from United Presbyterian Church of Scotland as well as a number founded by John Dunmore Lang. Most of these bodies merged between 1859 and 1870, and in 1901 formed a federal union called the Presbyterian Church of Australia but retaining their state assemblies. The Presbyterian Church of Eastern Australia representing the Free Church of Scotland tradition, and congregations in Victoria of the Reformed Presbyterian Church, originally from Ireland, are the other existing denominations dating from colonial times.

In 1977, two-thirds of the Presbyterian Church of Australia, along with most of the Congregational Union of Australia and all the Methodist Church of Australasia, combined to form the Uniting Church in Australia. The third who did not unite had various reasons for so acting, often cultural attachment but often conservative theological or social views. The permission for the ordination of women given in 1974 was rescinded in 1991 without affecting the two or three existing woman ministers. The approval of women elders given in the 1960s has been rescinded in all states except New South Wales, which has the largest membership. The theology of the church is now generally conservative and Reformed. A number of small Presbyterian denominations have arisen since the 1950s through migration or schism.

In New Zealand, Presbyterian is the dominant denomination in Otago and Southland due largely to the rich Scottish and to a lesser extent Ulster-Scots heritage in the region. The area around Christchurch, Canterbury, is dominated philosophically by the Anglican denomination.

Originally there were two branches of Presbyterianism in New Zealand, the northern Presbyterian church which existed in the North Island and the parts of the South Island north of the Waitaki River, and the Synod of Otago and Southland, founded by Free Church settlers in southern South Island. The two churches merged in 1901, forming what is now the Presbyterian Church of Aotearoa New Zealand.

In addition to the Presbyterian Church of Aotearoa New Zealand, there is also a more conservative Presbyterian church called Grace Presbyterian Church of New Zealand. Many of its members left the largely liberal PCANZ because they were seeking a more Biblical church. It has 17 churches throughout New Zealand.

The Presbyterian Church in Vanuatu is the largest denomination in the country, with approximately one-third of the population of Vanuatu members of the church. The PCV was taken to Vanuatu by missionaries from Scotland. The PCV (Presbyterian Church of Vanuatu) is headed by a moderator with offices in Port Vila. The PCV is particularly strong in the provinces of Tafea, Shefa, and Malampa. The Province of Sanma is mainly Presbyterian with a strong Roman Catholic minority in the Francophone areas of the province. There are some Presbyterian people, but no organised Presbyterian churches in Penama and Torba, both of which are traditionally Anglican. Vanuatu is the only country in the South Pacific with a significant Presbyterian heritage and membership. The PCV is a founding member of the Vanuatu Christian Council (VCC). The PCV runs many primary schools and Onesua secondary school. The church is strong in the rural villages.






</doc>
<doc id="24406" url="https://en.wikipedia.org/wiki?curid=24406" title="Parliament">
Parliament

In modern politics and history, a parliament is a legislative body of government. Generally, a modern parliament has three functions: representing the electorate, making laws, and overseeing the government via hearings and inquiries. The term is similar to the idea of a senate, synod or congress, and is commonly used in countries that are current or former monarchies, a form of government with a monarch as the head. Some contexts restrict the use of the word "parliament" to parliamentary systems, although it is also used to describe the legislature in some presidential systems (e.g. the Parliament of Ghana), even where it is not in the official name.

Historically, parliaments included various kinds of deliberative, consultative, and judicial assemblies, e.g. medieval parliaments.

The English term is derived from Anglo-Norman and dates to the 14th century, coming from the 11th century Old French , from , meaning "to talk". The meaning evolved over time, originally referring to any discussion, conversation, or negotiation through various kinds of deliberative or judicial groups, often summoned by a monarch. By the 15th century, in Britain, it had come to specifically mean the legislature.

Since ancient times, when societies were tribal, there were councils or a headman whose decisions were assessed by village elders. This is called tribalism. Some scholars suggest that in ancient Mesopotamia there was a primitive democratic government where the kings were assessed by council. The same has been said about ancient India, where some form of deliberative assemblies existed, and therefore there was some form of democracy. However, these claims are not accepted by most scholars, who see these forms of government as oligarchies.

Ancient Athens was the cradle of democracy. The Athenian assembly (, "ekklesia") was the most important institution, and every free male citizen could take part in the discussions. Slaves and women could not. However, Athenian democracy was not representative, but rather direct, and therefore the "ekklesia" was different from the parliamentary system.

The Roman Republic had legislative assemblies, who had the final say regarding the election of magistrates, the enactment of new statutes, the carrying out of capital punishment, the declaration of war and peace, and the creation (or dissolution) of alliances. The Roman Senate controlled money, administration, and the details of foreign policy.

Some Muslim scholars argue that the Islamic shura (a method of taking decisions in Islamic societies) is analogous to the parliament. However, others highlight what they consider fundamental differences between the shura system and the parliamentary system.

The first recorded signs of a council to decide on different issues in ancient Iran dates back to 247 BC while the Parthian empire was in power. The Parthians established the first Iranian empire since the conquest of Persia by Alexander. In the early years of their rule, an assembly of the nobles called “Mehestan” was formed that made the final decision on serious issues of state.

The word "Mehestan" consists of two parts. "Meh", a word of the old Persian origin, which literally means "The Great" and "-Stan", a suffix in the Persian language, which describes an especial place. Altogether Mehestan means a place where the greats come together.

The Mehestan Assembly, which consisted of Zoroastrian religious leaders and clan elders exerted great influence over the administration of the kingdom.

One of the most important decisions of the council took place in 208 AD, when a civil war broke out and the Mehestan decided that the empire would be ruled by two brothers simultaneously, Ardavan V and Blash V.
In 224 AD, following the dissolution of the Parthian empire, after over 470 years, the Mahestan council came to an end.

Although there are documented councils held in 873, 1020, 1050 and 1063, there was no representation of commoners. What is considered to be the first parliament (with the presence of commoners), the Cortes of León, was held in the Kingdom of León in 1188. According to the UNESCO, the Decreta of Leon of 1188 is the oldest documentary manifestation of the European parliamentary system. In addition, UNESCO granted the 1188 Cortes of Alfonso IX the title of "Memory of the World" and the city of Leon has been recognized as the "Cradle of Parliamentarism".

After coming to power, King Alfonso IX, facing an attack by his two neighbors, Castile and Portugal, decided to summon the "Royal Curia". This was a medieval organization composed of aristocrats and bishops but because of the seriousness of the situation and the need to maximize political support, Alfonso IX took the decision to also call the representatives of the urban middle class from the most important cities of the kingdom to the assembly. León's Cortes dealt with matters like the right to private property, the inviolability of domicile, the right to appeal to justice opposite the King and the obligation of the King to consult the Cortes before entering a war. Prelates, nobles and commoners met separately in the three estates of the Cortes. In this meeting, new laws were approved to protect commoners against the arbitrarities of nobles, prelates and the king. This important set of laws is known as the "Carta Magna Leonesa".

Following this event, new Cortes would appear in the other different territories that would make up Spain: Principality of Catalonia in 1192, the Kingdom of Castile in 1250, Kingdom of Aragon in 1274, Kingdom of Valencia in 1283 and Kingdom of Navarre in 1300.

After the union of the Kingdoms of Leon and Castile under the Crown of Castile, their Cortes were united as well in 1258. The Castilian Cortes had representatives from Burgos, Toledo, León, Seville, Córdoba, Murcia, Jaén, Zamora, Segovia, Ávila, Salamanca, Cuenca, Toro, Valladolid, Soria, Madrid, Guadalajara and Granada (after 1492). The Cortes' assent was required to pass new taxes, and could also advise the king on other matters. The comunero rebels intended a stronger role for the Cortes, but were defeated by the forces of Habsburg Emperor Charles V in 1521. The Cortes maintained some power, however, though it became more of a consultative entity. However, by the time of King Philip II, Charles's son, the Castilian Cortes had come under functionally complete royal control, with its delegates dependent on the Crown for their income.

The Cortes of the Crown of Aragon kingdoms retained their power to control the king's spending with regard to the finances of those kingdoms. But after the War of the Spanish Succession and the victory of another royal house – the Bourbons – and King Philip V, their Cortes were suppressed (those of Aragon and Valencia in 1707, and those of Catalonia and the Balearic islands in 1714).

The very first Cortes representing the whole of Spain (and the Spanish empire of the day) assembled in 1812, in Cadiz, where it operated as a government in exile as at that time most of the rest of Spain was in the hands of Napoleon's army.

After its self-proclamation as an independent kingdom in 1139 by Afonso I of Portugal (followed by the recognition by the Kingdom of León in the Treaty of Zamora of 1143), the first historically established Cortes of the Kingdom of Portugal occurred in 1211 in Coimbra by initiative of Afonso II of Portugal. These established the first general laws of the kingdom ("Leis Gerais do Reino"): protection of the king's property, stipulation of measures for the administration of justice and the rights of his subjects to be protected from abuses by royal officials, and confirming the clerical donations of the previous king Sancho I of Portugal. These Cortes also affirmed the validity of canon law for the Church in Portugal, while introducing the prohibition of the purchase of lands by churches or monasteries (although they can be acquired by donations and legacies).

After the conquest of Algarve in 1249, the Kingdom of Portugal completed its Reconquista. In 1254 King Afonso III of Portugal summoned Portuguese Cortes in Leiria, with the inclusion of burghers from old and newly incorporated municipalities. This inclusion establishes the Cortes of Leiria of 1254 as the second sample of modern parliamentarism in the history of Europe (after the Cortes of León in 1188). In these Cortes the monetagio was introduced: a fixed sum was to be paid by the burghers to the Crown as a substitute for the septennium (the traditional revision of the face value of coinage by the Crown every seven years). These Cortes also introduced staple laws on the Douro River, favoring the new royal city of Vila Nova de Gaia at the expense of the old episcopal city of Porto. 

The Portuguese Cortes met again under King Afonso III of Portugal in 1256, 1261 and 1273, always by royal summon. Medieval Kings of Portugal continued to rely on small assemblies of notables, and only summoned the full Cortes on extraordinary occasions. A Cortes would be called if the king wanted to introduce new taxes, change some fundamental laws, announce significant shifts in foreign policy (e.g. ratify treaties), or settle matters of royal succession, issues where the cooperation and assent of the towns was thought necessary. Changing taxation (especially requesting war subsidies), was probably the most frequent reason for convening the Cortes. As the nobles and clergy were largely tax-exempt, setting taxation involved intensive negotiations between the royal council and the burgher delegates at the Cortes.

Delegates ("procuradores") not only considered the king's proposals, but, in turn, also used the Cortes to submit petitions of their own to the royal council on a myriad of matters, e.g. extending and confirming town privileges, punishing abuses of officials, introducing new price controls, constraints on Jews, pledges on coinage, etc. The royal response to these petitions became enshrined as ordinances and statutes, thus giving the Cortes the aspect of a legislature. These petitions were originally referred to as "aggravamentos" (grievances) then "artigos" (articles) and eventually "capitulos" (chapters). In a Cortes-Gerais, petitions were discussed and voted upon separately by each estate and required the approval of at least two of the three estates before being passed up to the royal council. The proposal was then subject to royal veto (either accepted or rejected by the king in its entirety) before becoming law.

Nonetheless, the exact extent of Cortes power was ambiguous. Kings insisted on their ancient prerogative to promulgate laws independently of the Cortes. The compromise, in theory, was that ordinances enacted in Cortes could only be modified or repealed by Cortes. But even that principle was often circumvented or ignored in practice.

The Cortes probably had their heyday in the 14th and 15th centuries, reaching their apex when John I of Portugal relied almost wholly upon the bourgeoisie for his power. For a period after the 1383–1385 Crisis, the Cortes were convened almost annually. But as time went on, they became less important. Portuguese monarchs, tapping into the riches of the Portuguese empire overseas, grew less dependent on Cortes subsidies and convened them less frequently. John II (r.1481-1495) used them to break the high nobility, but dispensed with them otherwise. Manuel I (r.1495-1521) convened them only four times in his long reign. By the time of Sebastian (r.1554–1578), the Cortes was practically an irrelevance.

Curiously, the Cortes gained a new importance with the Iberian Union of 1581, finding a role as the representative of Portuguese interests to the new Habsburg monarch. The Cortes played a critical role in the 1640 Restoration, and enjoyed a brief period of resurgence during the reign of John IV of Portugal (r.1640-1656). But by the end of the 17th century, it found itself sidelined once again. The last Cortes met in 1698, for the mere formality of confirming the appointment of Infante John (future John V of Portugal) as the successor of Peter II of Portugal. Thereafter, Portuguese kings ruled as absolute monarchs and no Cortes were assembled for over a century. This state of affairs came to an end with the Liberal Revolution of 1820, which set in motion the introduction of a new constitution, and a permanent and proper parliament, that however inherited the name of Cortes Gerais.

England has long had a tradition of a body of men who would assist and advise the king on important matters. Under the Anglo-Saxon kings, there was an advisory council, the Witenagemot. The name derives from the Old English ƿitena ȝemōt, or witena gemōt, for "meeting of wise men". The first recorded act of a witenagemot was the law code issued by King Æthelberht of Kent ca. 600, the earliest document which survives in sustained Old English prose; however, the witan was certainly in existence long before this time. The Witan, along with the folkmoots (local assemblies), is an important ancestor of the modern English parliament.

As part of the Norman Conquest of England, the new king, William I, did away with the Witenagemot, replacing it with a Curia Regis ("King's Council"). Membership of the Curia was largely restricted to the tenants in chief, the few nobles who "rented" great estates directly from the king, along with ecclesiastics. William brought to England the feudal system of his native Normandy, and sought the advice of the curia regis before making laws. This is the original body from which the Parliament, the higher courts of law, and the Privy Council and Cabinet descend. Of these, the legislature is formally the High Court of Parliament; judges sit in the Supreme Court of Judicature. Only the executive government is no longer conducted in a royal court.

Most historians date the emergence of a parliament with some degree of power to which the throne had to defer no later than the rule of Edward I. Like previous kings, Edward called leading nobles and church leaders to discuss government matters, especially finance and taxation. A meeting in 1295 became known as the Model Parliament because it set the pattern for later Parliaments. The significant difference between the Model Parliament and the earlier Curia Regis was the addition of the Commons; that is, the inclusion of elected representatives of rural landowners and of townsmen. In 1307, Edward I agreed not to collect certain taxes without the "consent of the realm" through parliament. He also enlarged the court system.

The tenants-in-chief often struggled with their spiritual counterparts and with the king for power. In 1215, they secured from King John of England "Magna Carta", which established that the king may not levy or collect any taxes (except the feudal taxes to which they were hitherto accustomed), save with the consent of a council. It was also established that the most important tenants-in-chief and ecclesiastics be summoned to the council by personal writs from the sovereign, and that all others be summoned to the council by general writs from the sheriffs of their counties. Modern government has its origins in the Curia Regis; parliament descends from the Great Council later known as the "parliamentum" established by "Magna Carta".

During the reign of King Henry III, 13th-Century English Parliaments incorporated elected representatives from shires and towns. These parliaments are, as such, considered forerunners of the modern parliament.

In 1265, Simon de Montfort, then in rebellion against Henry III, summoned a parliament of his supporters without royal authorization. The archbishops, bishops, abbots, earls, and barons were summoned, as were two knights from each shire and two burgesses from each borough. Knights had been summoned to previous councils, but it was unprecedented for the boroughs to receive any representation. Come 1295, Edward I later adopted de Montfort's ideas for representation and election in the so-called "Model Parliament". At first, each estate debated independently; by the reign of Edward III, however, Parliament recognisably assumed its modern form, with authorities dividing the legislative body into two separate chambers.

The purpose and structure of Parliament in Tudor England underwent a significant transformation under the reign of Henry VIII. Originally its methods were primarily medieval, and the monarch still possessed a form of inarguable dominion over its decisions. According to Elton, it was Thomas Cromwell, 1st Earl of Essex, then chief minister to Henry VIII, who initiated still other changes within parliament.

The Reformation Acts supplied Parliament with unlimited power over the country. This included authority over virtually every matter, whether social, economic, political, or religious ; it legalised the Reformation, officially and indisputably. The king had to rule through the council, not over it, and all sides needed to reach a mutual agreement when creating or passing laws, adjusting or implementing taxes, or changing religious doctrines. This was significant: the monarch no longer had sole control over the country. For instance, during the later years of Mary, Parliament exercised its authority in originally rejecting Mary's bid to revive Catholicism in the realm. Later on, the legislative body even denied Elizabeth her request to marry . If Parliament had possessed this power before Cromwell, such as when Wolsey served as secretary, the Reformation may never have happened, as the king would have had to gain the consent of all parliament members before so drastically changing the country's religious laws and fundamental identity .

The power of Parliament increased considerably after Cromwell's adjustments. It also provided the country with unprecedented stability. More stability, in turn, helped assure more effective management, organisation, and efficiency. Parliament printed statutes and devised a more coherent parliamentary procedure.

The rise of Parliament proved especially important in the sense that it limited the repercussions of dynastic complications that had so often plunged England into civil war. Parliament still ran the country even in the absence of suitable heirs to the throne, and its legitimacy as a decision-making body reduced the royal prerogatives of kings like Henry VIII and the importance of their whims. For example, Henry VIII could not simply establish supremacy by proclamation; he required Parliament to enforce statutes and add felonies and treasons. An important liberty for Parliament was its freedom of speech; Henry allowed anything to be spoken openly within Parliament and speakers could not face arrest – a fact which they exploited incessantly. Nevertheless, Parliament in Henry VIII's time offered up very little objection to the monarch's desires. Under his and Edward's reign, the legislative body complied willingly with the majority of the kings' decisions.

Much of this compliance stemmed from how the English viewed and traditionally understood authority. As Williams described it, "King and parliament were not separate entities, but a single body, of which the monarch was the senior partner and the Lords and the Commons the lesser, but still essential, members.".

Although its role in government expanded significantly during the reigns of Henry VIII and Edward VI, the Parliament of England saw some of its most important gains in the 17th century. A series of conflicts between the Crown and Parliament culminated in the execution of King Charles I in 1649. Afterward, England became a commonwealth, with Oliver Cromwell, its lord protector, the de facto ruler. Frustrated with its decisions, Cromwell purged and suspended Parliament on several occasions.

A controversial figure accused of despotism, war crimes, and even genocide, Cromwell is nonetheless regarded as essential to the growth of democracy in England. The years of the Commonwealth, coupled with the restoration of the monarchy in 1660 and the subsequent Glorious Revolution of 1688, helped reinforce and strengthen Parliament as an institution separate from the Crown.

The Parliament of England met until it merged with the Parliament of Scotland under the Acts of Union. This union created the new Parliament of Great Britain in 1707.

From the 10th century the Kingdom of Alba was ruled by chiefs ("toisechs") and subkings ("mormaers") under the suzerainty, real or nominal, of a High King. Popular assemblies, as in Ireland, were involved in law-making, and sometimes in king-making, although the introduction of tanistry—naming a successor in the lifetime of a king—made the second less than common. These early assemblies cannot be considered "parliaments" in the later sense of the word, and were entirely separate from the later, Norman-influenced, institution.

The Parliament of Scotland evolved during the Middle Ages from the King's Council of Bishops and Earls. The unicameral parliament is first found on record, referred to as a "colloquium", in 1235 at Kirkliston (a village now in Edinburgh).

By the early fourteenth century the attendance of knights and freeholders had become important, and from 1326 burgh commissioners attended. Consisting of the Three Estates; of clerics, lay tenants-in-chief and burgh commissioners sitting in a single chamber, the Scottish parliament acquired significant powers over particular issues. Most obviously it was needed for consent for taxation (although taxation was only raised irregularly in Scotland in the medieval period), but it also had a strong influence over justice, foreign policy, war, and all manner of other legislation, whether political, ecclesiastical, social or economic. Parliamentary business was also carried out by "sister" institutions, before c. 1500 by General Council and thereafter by the Convention of Estates. These could carry out much business also dealt with by Parliament – taxation, legislation and policy-making – but lacked the ultimate authority of a full parliament.

The parliament, which is also referred to as the Estates of Scotland, the Three Estates, the Scots Parliament or the auld Scots Parliament (Eng: "old"), met until the Acts of Union merged the Parliament of Scotland and the Parliament of England, creating the new Parliament of Great Britain in 1707.

Following the 1997 Scottish devolution referendum, and the passing of the Scotland Act 1998 by the Parliament of the United Kingdom, the Scottish Parliament was reconvened on 1 July 1999, although with much more limited powers than its 18th-century predecessor. The parliament has sat since 2004 at its newly constructed Scottish Parliament Building in Edinburgh, situated at the foot of the Royal Mile, next to the royal palace of Holyroodhouse.

A "thing" or "ting" (Old Norse and ; other modern Scandinavian: "ting", "ding" in Dutch) was the governing assembly in Germanic societies, made up of the free men of the community and presided by lawspeakers.

The thing was the assembly of the free men of a country, province or a hundred "(hundare/härad/herred)". There were consequently, hierarchies of things, so that the local things were represented at the thing for a larger area, for a province or land. At the thing, disputes were solved and political decisions were made. The place for the thing was often also the place for public religious rites and for commerce.

The thing met at regular intervals, legislated, elected chieftains and kings, and judged according to the law, which was memorised and recited by the "law speaker" (the judge).

The Icelandic, Faroese and Manx parliaments trace their origins back to the Viking expansion originating from the Petty kingdoms of Norway as well as Denmark, replicating Viking government systems in the conquered territories, such as those represented by the Gulating near Bergen in western Norway.

Later national diets with chambers for different estates developed, e.g. in Sweden and in Finland (which was part of Sweden until 1809), each with a House of Knights for the nobility. In both these countries, the national parliaments are now called riksdag (in Finland also "eduskunta"), a word used since the Middle Ages and equivalent of the German word Reichstag.

Today the term lives on in the official names of national legislatures, political and judicial institutions in the North-Germanic countries. In the Yorkshire and former Danelaw areas of England, which were subject to much Norse invasion and settlement, the wapentake was another name for the same institution.

The Sicilian Parliament, dating to 1097, evolved as the legislature of the Kingdom of Sicily.

The Federal Diet of Switzerland was one of the longest-lived representative bodies in history, continuing from the 13th century to 1848.

Originally, there was only the Parliament of Paris, born out of the Curia Regis in 1307, and located inside the medieval royal palace, now the Paris Hall of Justice. The jurisdiction of the "Parliament" of Paris covered the entire kingdom. In the thirteenth century, judicial functions were added. In 1443, following the turmoil of the Hundred Years' War, King Charles VII of France granted Languedoc its own "parliament" by establishing the "Parliament" of Toulouse, the first "parliament" outside of Paris, whose jurisdiction extended over the most part of southern France. From 1443 until the French Revolution several other "parliaments" were created in some provinces of France (Grenoble, Bordeaux).

All the "parliaments" could issue regulatory decrees for the application of royal edicts or of customary practices; they could also refuse to register laws that they judged contrary to fundamental law or simply as being untimely. Parliamentary power in France was suppressed more so than in England as a result of absolutism, and parliaments were eventually overshadowed by the larger Estates General, up until the French Revolution, when the National Assembly became the lower house of France's bicameral legislature.

According to the "Chronicles" of Gallus Anonymus, the first legendary Polish ruler, Siemowit, who began the Piast Dynasty, was chosen by a "wiec". The "veche" (, ) was a popular assembly in medieval Slavic countries, and in late medieval period, a parliament. The idea of the "wiec" led in 1182 to the development of the Polish parliament, the "Sejm".

The term "sejm" comes from an old Polish expression denoting a meeting of the populace. The power of early sejms grew between 1146–1295, when the power of individual rulers waned and various councils and wiece grew stronger. The history of the national Sejm dates back to 1182. Since the 14th century irregular sejms (described in various Latin sources as "contentio generalis, conventio magna, conventio solemna, parlamentum, parlamentum generale, dieta" or Polish "sejm walny") have been called by Polish kings. From 1374, the king had to receive sejm permission to raise taxes. The General Sejm (Polish "Sejm Generalny" or "Sejm Walny"), first convoked by the king John I Olbracht in 1493 near Piotrków, evolved from earlier regional and provincial meetings ("sejmiks"). It followed most closely the "sejmik generally", which arose from the 1454 Nieszawa Statutes, granted to the szlachta (nobles) by King Casimir IV the Jagiellonian. From 1493 forward, indirect elections were repeated every two years. With the development of the unique Polish Golden Liberty the Sejm's powers increased.

The Commonwealth's general parliament consisted of three estates: the King of Poland (who also acted as the Grand Duke of Lithuania, Russia/Ruthenia, Prussia, Mazovia, etc.), the Senat (consisting of Ministers, Palatines, Castellans and Bishops) and the Chamber of Envoys—circa 170 nobles (szlachta) acting on behalf of their Lands and sent by Land Parliaments. Also representatives of selected cities but without any voting powers. Since 1573 at a royal election all peers of the Commonwealth could participate in the Parliament and become the King's electors.

Cossack Rada was the legislative body of a military republic of the Ukrainian Cossacks that grew rapidly in the 15th century from serfs fleeing the more controlled parts of the Polish Lithuanian Commonwealth. The republic did not regard social origin/nobility and accepted all people who declared to be Orthodox Christians.

Originally established at the Zaporizhian Sich, the rada (council) was an institution of Cossack administration in Ukraine from the 16th to the 18th century. With the establishment of the Hetman state in 1648, it was officially known as the General Military Council until 1750.

The zemsky sobor (Russian: зе́мский собо́р) was the first Russian parliament of the feudal Estates type, in the 16th and 17th centuries. The term roughly means assembly of the land.

It could be summoned either by tsar, or patriarch, or the Boyar Duma. Three categories of population, comparable to the Estates-General of France but with the numbering of the first two Estates reversed, participated in the assembly:

Nobility and high bureaucracy, including the Boyar Duma

The Holy Sobor of high Orthodox clergy

Representatives of merchants and townspeople (third estate)

The name of the parliament of nowadays Russian Federation is the Federal Assembly of Russia. The term for its lower house, State Duma (which is better known than the Federal Assembly itself, and is often mistaken for the entirety of the parliament) comes from the Russian word "думать" ("dumat"), "to think". The Boyar Duma was an advisory council to the grand princes and tsars of Muscovy. The Duma was discontinued by Peter the Great, who transferred its functions to the Governing Senate in 1711.

The "veche" was the highest legislature and judicial authority in the republic of Novgorod until 1478. In its sister state, Pskov, a separate veche operated until 1510.

Since the Novgorod revolution of 1137 ousted the ruling grand prince, the veche became the supreme state authority. After the reforms of 1410, the veche was restructured on a model similar to that of Venice, becoming the Commons chamber of the parliament. An upper Senate-like Council of Lords was also created, with title membership for all former city magistrates. Some sources indicate that veche membership may have become full-time, and parliament deputies were now called "vechniks". It is recounted that the Novgorod assembly could be summoned by anyone who rung the veche bell, although it is more likely that the common procedure was more complex. This bell was a symbol of republican sovereignty and independence. The whole population of the city—boyars, merchants, and common citizens—then gathered at Yaroslav's Court. Separate assemblies could be held in the districts of Novgorod. In Pskov the veche assembled in the court of the Trinity cathedral.

"Conciliarism" or the "conciliar movement", was a reform movement in the 14th and 15th century Roman Catholic Church which held that final authority in spiritual matters resided with the Roman Church as corporation of Christians, embodied by a general church council, not with the pope. In effect, the movement sought – ultimately, in vain – to create an All-Catholic Parliament. Its struggle with the Papacy had many points in common with the struggle of parliaments in specific countries against the authority of Kings and other secular rulers.

The development of the modern concept of parliamentary government dates back to the Kingdom of Great Britain (1707–1800) and the parliamentary system in Sweden during the Age of Liberty (1718–1772).

The British Parliament is often referred to as the "Mother of Parliaments" (in fact a misquotation of John Bright, who remarked in 1865 that "England is the Mother of Parliaments") because the British Parliament has been the model for most other parliamentary systems, and its Acts have created many other parliaments. Many nations with parliaments have to some degree emulated the British "three-tier" model. Most countries in Europe and the Commonwealth have similarly organised parliaments with a largely ceremonial head of state who formally opens and closes parliament, a large elected lower house and a smaller, upper house.

The Parliament of Great Britain was formed in 1707 by the Acts of Union that replaced the former parliaments of England and Scotland. A further union in 1801 united the Parliament of Great Britain and the Parliament of Ireland into a Parliament of the United Kingdom.

In the United Kingdom, Parliament consists of the House of Commons, the House of Lords, and the Monarch. The House of Commons is composed of 650 (soon to be 600) members who are directly elected by British citizens to represent single-member constituencies. The leader of a Party that wins more than half the seats, or less than half but is able to gain the support of smaller parties to achieve a majority in the house is invited by the Monarch to form a government. The House of Lords is a body of long-serving, unelected members: Lords Temporal – 92 of whom inherit their titles (and of whom 90 are elected internally by members of the House to lifetime seats), 588 of whom have been appointed to lifetime seats, and Lords Spiritual – 26 bishops, who are part of the house while they remain in office.

Legislation can originate from either the Lords or the Commons. It is voted on in several distinct stages, called readings, in each house. First reading is merely a formality. Second reading is where the bill as a whole is considered. Third reading is detailed consideration of clauses of the bill.

In addition to the three readings a bill also goes through a committee stage where it is considered in great detail. Once the bill has been passed by one house it goes to the other and essentially repeats the process. If after the two sets of readings there are disagreements between the versions that the two houses passed it is returned to the first house for consideration of the amendments made by the second. If it passes through the amendment stage Royal Assent is granted and the bill becomes law as an Act of Parliament.

The House of Lords is the less powerful of the two houses as a result of the Parliament Acts 1911 and 1949. These Acts removed the veto power of the Lords over a great deal of legislation. If a bill is certified by the Speaker of the House of Commons as a money bill (i.e. acts raising taxes and similar) then the Lords can only block it for a month. If an ordinary bill originates in the Commons the Lords can only block it for a maximum of one session of Parliament. The exceptions to this rule are things like bills to prolong the life of a Parliament beyond five years.

In addition to functioning as the second chamber of Parliament, the House of Lords was also the final court of appeal for much of the law of the United Kingdom—a combination of judicial and legislative function that recalls its origin in the Curia Regis. This changed in October 2009 when the Supreme Court of the United Kingdom opened and acquired the former jurisdiction of the House of Lords.

Since 1999, there has been a Scottish Parliament in Edinburgh, and, since 2020, a Welsh Parliament—or Senedd—in Cardiff. However, these national, unicameral legislatures do not have complete power over their respective countries of the United Kingdom, holding only those powers devolved to them by Westminster from 1997. They cannot legislate on defence issues, currency, or national taxation (e.g. VAT, or Income Tax). Additionally, the bodies can be dissolved, at any given time, by the British Parliament without the consent of the devolved government.

In Sweden, the half-century period of parliamentary government beginning with Charles XII's death in 1718 and ending with Gustav III's self-coup in 1772 is known as the Age of Liberty. During this period, civil rights were expanded and power shifted from the monarch to parliament.

While suffrage did not become universal, the taxed peasantry was represented in Parliament, although with little influence and commoners without taxed property had no suffrage at all.

Many parliaments are part of a parliamentary system of government, in which the executive is constitutionally answerable to the parliament. Some restrict the use of the word "parliament" to parliamentary systems, while others use the word for any elected legislative body. Parliaments usually consist of "chambers" or "houses", and are usually either bicameral or unicameral although more complex models exist, or have existed ("see Tricameralism).

In some parliamentary systems, the prime minister is a member of the parliament (e.g. in the United Kingdom), whereas in others they are not (e.g. in the Netherlands). They are commonly the leader of the majority party in the lower house of parliament, but only hold the office as long as the "confidence of the house" is maintained. If members of the lower house lose faith in the leader for whatever reason, they can call a vote of no confidence and force the prime minister to resign.

This can be particularly dangerous to a government when the distribution of seats among different parties is relatively even, in which case a new election is often called shortly thereafter. However, in case of general discontent with the head of government, their replacement can be made very smoothly without all the complications that it represents in the case of a presidential system.

The parliamentary system can be contrasted with a presidential system, such as the American congressional system, which operates under a stricter separation of powers, whereby the executive does not form part of, nor is it appointed by, the parliamentary or legislative body. In such a system, congresses do not select or dismiss heads of governments, and governments cannot request an early dissolution as may be the case for parliaments. Some states, such as France, have a semi-presidential system which falls between parliamentary and congressional systems, combining a powerful head of state (president) with a head of government, the prime minister, who is responsible to parliament.



Australia's States and territories:

In the federal (bicameral) kingdom of Belgium, there is a curious asymmetrical constellation serving as directly elected legislatures for three "territorial" "regions"—Flanders (Dutch), Brussels (bilingual, certain peculiarities of competence, also the only region not comprising any of the 10 provinces) and Wallonia (French)—and three cultural "communities"—Flemish (Dutch, competent in Flanders and for the Dutch-speaking inhabitants of Brussels), Francophone (French, for Wallonia and for Francophones in Brussels) and German (for speakers of that language in a few designated municipalities in the east of the Walloon Region, living alongside Francophones but under two different regimes):

Canada's provinces and territories:




<maplink latitude="21" longitude="78" zoom="5" width="500" height="500" text="Map" >
</maplink>
Indian States and Territories Legislative assemblies:


Indian States Legislative councils









</doc>
<doc id="24408" url="https://en.wikipedia.org/wiki?curid=24408" title="Polar bear">
Polar bear

The polar bear ("Ursus maritimus") is a hypercarnivorous bear whose native range lies largely within the Arctic Circle, encompassing the Arctic Ocean, its surrounding seas and surrounding land masses. It is the largest extant bear species, as well as the largest extant predatory carnivore. A boar (adult male) weighs around , while a sow (adult female) is about half that size. Although it is the sister species of the brown bear, it has evolved to occupy a narrower ecological niche, with many body characteristics adapted for cold temperatures, for moving across snow, ice and open water, and for hunting seals, which make up most of its diet. Although most polar bears are born on land, they spend most of their time on the sea ice. Their scientific name means "maritime bear" and derives from this fact. Polar bears hunt their preferred food of seals from the edge of sea ice, often living off fat reserves when no sea ice is present. Because of their dependence on the sea ice, polar bears are classified as marine mammals.

Because of expected habitat loss caused by climate change, the polar bear is classified as a vulnerable species. For decades, large-scale hunting raised international concern for the future of the species, but populations rebounded after controls and quotas began to take effect. For thousands of years, the polar bear has been a key figure in the material, spiritual, and cultural life of circumpolar peoples, and polar bears remain important in their cultures. Historically, the polar bear has also been known as the "white bear". It is sometimes referred to as the "nanook", based on the Inuit term "nanuq".

Constantine John Phipps was the first to describe the polar bear as a distinct species in 1774. He chose the scientific name "Ursus maritimus", the Latin for 'maritime bear', due to the animal's native habitat. The Inuit refer to the animal as "nanook" (transliterated as "nanuq" in the Inupiat language). The Yupik also refer to the bear as "nanuuk" in Siberian Yupik. The bear is "umka" in the Chukchi language. In Russian, it is usually called бе́лый медве́дь ("bélyj medvédj", the white bear), though an older word still in use is ошку́й ("Oshkúj", which comes from the Komi "oski", "bear"). In Quebec, the polar bear is referred to as "ours blanc" ("white bear") or "ours polaire" ("polar bear"). In the Norwegian-administered Svalbard archipelago, the polar bear is referred to as "Isbjørn" ("ice bear").

The polar bear was previously considered to be in its own genus, "Thalarctos". However, evidence of hybrids between polar bears and brown bears, and of the recent evolutionary divergence of the two species, does not support the establishment of this separate genus, and the accepted scientific name is now therefore "Ursus maritimus", as Phipps originally proposed.

The bear family, Ursidae, is thought to have split from other carnivorans about 38 million years ago. The subfamily Ursinae originated approximately 4.2 million years ago. The oldest known polar bear fossil is a 130,000 to 110,000-year-old jaw bone, found on Prince Charles Foreland in 2004. Fossils show that between 10,000 and 20,000 years ago, the polar bear's molar teeth changed significantly from those of the brown bear. Polar bears are thought to have diverged from a population of brown bears that became isolated during a period of glaciation in the Pleistocene from the eastern part of Siberia (from Kamchatka and the Kolym Peninsula).

The evidence from DNA analysis is more complex. The mitochondrial DNA (mtDNA) of the polar bear diverged from the brown bear, "Ursus arctos", roughly 150,000 years ago. Further, some clades of brown bear, as assessed by their mtDNA, were thought to be more closely related to polar bears than to other brown bears, meaning that the brown bear might not be considered a species under some species concepts, but paraphyletic. The mtDNA of extinct Irish brown bears is particularly close to polar bears. A comparison of the nuclear genome of polar bears with that of brown bears revealed a different pattern, the two forming genetically distinct clades that diverged approximately 603,000 years ago, although the latest research is based on analysis of the complete genomes (rather than just the mitochondria or partial nuclear genomes) of polar and brown bears, and establishes the divergence of polar and brown bears at 400,000 years ago.

However, the two species have mated intermittently for all that time, most likely coming into contact with each other during warming periods, when polar bears were driven onto land and brown bears migrated northward. Most brown bears have about 2 percent genetic material from polar bears, but one population, the ABC Islands bears, has between 5 percent and 10 percent polar bear genes, indicating more frequent and recent mating. Polar bears can breed with brown bears to produce fertile grizzly–polar bear hybrids; rather than indicating that they have only recently diverged, the new evidence suggests more frequent mating has continued over a longer period of time, and thus the two bears remain genetically similar. However, because neither species can survive long in the other's ecological niche, and because they have different morphology, metabolism, social and feeding behaviours, and other phenotypic characteristics, the two bears are generally classified as separate species.

When the polar bear was originally documented, two subspecies were identified: the American polar bear ("Ursus maritimus maritimus") by Constantine J. Phipps in 1774, and the Siberian polar bear ("Ursus maritimus marinus") by Peter Simon Pallas in 1776. This distinction has since been invalidated. One alleged fossil subspecies has been identified: "Ursus maritimus tyrannus", which became extinct during the Pleistocene. "U.m. tyrannus" was significantly larger than the living subspecies. However, recent reanalysis of the fossil suggests that it was actually a brown bear.

The polar bear is found in the Arctic Circle and adjacent land masses as far south as Newfoundland. Due to the absence of human development in its remote habitat, it retains more of its original range than any other extant carnivore. While they are rare north of 88°, there is evidence that they range all the way across the Arctic, and as far south as James Bay in Canada. Their southernmost range is near the boundary between the subarctic and humid continental climate zones. They can occasionally drift widely with the sea ice, and there have been anecdotal sightings as far south as Berlevåg on the Norwegian mainland and the Kuril Islands in the Sea of Okhotsk. It is difficult to estimate a global population of polar bears as much of the range has been poorly studied; however, biologists use a working estimate of about 20–25,000 or 22–31,000 polar bears worldwide.

There are 19 generally recognized, discrete subpopulations, though polar bears are thought to exist only in low densities in the area of the Arctic Basin. The subpopulations display seasonal fidelity to particular areas, but DNA studies show that they are not reproductively isolated. The 13 North American subpopulations range from the Beaufort Sea south to Hudson Bay and east to Baffin Bay in western Greenland and account for about 54% of the global population.
The range includes the territory of five nations: Denmark (Greenland), Norway (Svalbard), Russia, the United States (Alaska) and Canada. These five nations are the signatories of the International Agreement on the Conservation of Polar Bears, which mandates cooperation on research and conservation efforts throughout the polar bear's range. Bears sometimes swim to Iceland from Greenland—about 600 sightings since the country's settlement in the 9th century AD, and five in the 21st century —and are always killed because of their danger, and the cost and difficulty of repatriation.

Modern methods of tracking polar bear populations have been implemented only since the mid-1980s, and are expensive to perform consistently over a large area. The most accurate counts require flying a helicopter in the Arctic climate to find polar bears, shooting a tranquilizer dart at the bear to sedate it, and then tagging the bear. In Nunavut, some Inuit have reported increases in bear sightings around human settlements in recent years, leading to a belief that populations are increasing. Scientists have responded by noting that hungry bears may be congregating around human settlements, leading to the illusion that populations are higher than they actually are. The Polar Bear Specialist Group of the IUCN Species Survival Commission takes the position that "estimates of subpopulation size or sustainable harvest levels should not be made solely on the basis of traditional ecological knowledge without supporting scientific studies."

Of the 19 recognized polar bear subpopulations, one is in decline, two are increasing, seven are stable, and nine have insufficient data, as of 2017.

The polar bear is a marine mammal because it spends many months of the year at sea. However, it is the only living marine mammal with powerful, large limbs and feet that allow them to cover kilometres on foot and run on land. Its preferred habitat is the annual sea ice covering the waters over the continental shelf and the Arctic inter-island archipelagos. These areas, known as the "Arctic ring of life", have high biological productivity in comparison to the deep waters of the high Arctic. The polar bear tends to frequent areas where sea ice meets water, such as polynyas and leads (temporary stretches of open water in Arctic ice), to hunt the seals that make up most of its diet. Freshwater is limited in these environments because it is either locked up in snow or saline. Polar bears are able to produce water through the metabolism of fats found in seal blubber, and are therefore found primarily along the perimeter of the polar ice pack, rather than in the Polar Basin close to the North Pole where the density of seals is low.

Annual ice contains areas of water that appear and disappear throughout the year as the weather changes. Seals migrate in response to these changes, and polar bears must follow their prey. In Hudson Bay, James Bay, and some other areas, the ice melts completely each summer (an event often referred to as "ice-floe breakup"), forcing polar bears to go onto land and wait through the months until the next freeze-up. In the Chukchi and Beaufort seas, polar bears retreat each summer to the ice further north that remains frozen year-round.

The only other bear similar in size to the polar bear is the Kodiak bear, which is a subspecies of brown bear. Adult male polar bears weigh and measure in total length. Around the Beaufort Sea, however, mature males reportedly average . Adult females are roughly half the size of males and normally weigh , measuring in length. Elsewhere, a slightly larger estimated average weight of was claimed for adult females. When pregnant, however, females can weigh as much as . The polar bear is among the most sexually dimorphic of mammals, surpassed only by the pinnipeds such as elephant seals. The largest polar bear on record, reportedly weighing , was a male shot at Kotzebue Sound in northwestern Alaska in 1960. This specimen, when mounted, stood tall on its hindlegs. The shoulder height of an adult polar bear is . While all bears are short-tailed, the polar bear's tail is relatively the shortest amongst living bears, ranging from in length.

Compared with its closest relative, the brown bear, the polar bear has a more elongated body build and a longer skull and nose. As predicted by Allen's rule for a northerly animal, the legs are stocky and the ears and tail are small. However, the feet are very large to distribute load when walking on snow or thin ice and to provide propulsion when swimming; they may measure across in an adult. The pads of the paws are covered with small, soft papillae (dermal bumps), which provide traction on the ice. The polar bear's claws are short and stocky compared to those of the brown bear, perhaps to serve the former's need to grip heavy prey and ice. The claws are deeply scooped on the underside to assist in digging in the ice of the natural habitat. Research of injury patterns in polar bear forelimbs found injuries to the right forelimb to be more frequent than those to the left, suggesting, perhaps, right-handedness. Unlike the brown bear, polar bears in captivity are rarely overweight or particularly large, possibly as a reaction to the warm conditions of most zoos.

The 42 teeth of a polar bear reflect its highly carnivorous diet. The cheek teeth are smaller and more jagged than in the brown bear, and the canines are larger and sharper. The dental formula is .

Polar bears are superbly insulated by up to of adipose tissue, their hide and their fur; they overheat at temperatures above , and are nearly invisible under infrared photography. Polar bear fur consists of a layer of dense underfur and an outer layer of guard hairs, which appear white to tan but are actually transparent. Two genes that are known to influence melanin production, LYST and AIM1, are both mutated in polar bears, possibly leading to the absence on this pigment in their fur. The guard hair is over most of the body. Polar bears gradually moult from May to August, but, unlike other Arctic mammals, they do not shed their coat for a darker shade to provide camouflage in summer conditions. The hollow guard hairs of a polar bear coat were once thought to act as fiber-optic tubes to conduct light to its black skin, where it could be absorbed; however, this hypothesis was disproved by a study in 1998.

The white coat usually yellows with age. When kept in captivity in warm, humid conditions, the fur may turn a pale shade of green due to algae growing inside the guard hairs. Males have significantly longer hairs on their forelegs, which increase in length until the bear reaches 14 years of age. The male's ornamental foreleg hair is thought to attract females, serving a similar function to the lion's mane.

The polar bear has an extremely well developed sense of smell, being able to detect seals nearly away and buried under of snow. Its hearing is about as acute as that of a human, and its vision is also good at long distances.

The polar bear is an excellent swimmer and often will swim for days. One bear swam continuously for 9 days in the frigid Bering Sea for to reach ice far from land. She then travelled another . During the swim, the bear lost 22% of her body mass and her yearling cub died. With its body fat providing buoyancy, the bear swims in a dog paddle fashion using its large forepaws for propulsion. Polar bears can swim . When walking, the polar bear tends to have a lumbering gait and maintains an average speed of around . When sprinting, they can reach up to .

Unlike brown bears, polar bears are not territorial. Although stereotyped as being voraciously aggressive, they are normally cautious in confrontations, and often choose to escape rather than fight. Satiated polar bears rarely attack humans unless severely provoked. However, due to their lack of prior human interaction, hungry polar bears are extremely unpredictable, fearless towards people and are known to kill and sometimes eat humans. Many attacks by brown bears are the result of surprising the animal, which is not the case with the polar bear. Polar bears are stealth hunters, and the victim is often unaware of the bear's presence until the attack is underway. Whereas brown bears often maul a person and then leave, polar bear attacks are more likely to be predatory and are almost always fatal. However, due to the very small human population around the Arctic, such attacks are rare. Michio Hoshino, a Japanese wildlife photographer, was once pursued briefly by a hungry male polar bear in northern Alaska. According to Hoshino, the bear started running but Hoshino made it to his truck. The bear was able to reach the truck and tore one of the doors off the truck before Hoshino was able to drive off.

In general, adult polar bears live solitary lives. Yet, they have often been seen playing together for hours at a time and even sleeping in an embrace, and polar bear zoologist Nikita Ovsianikov has described adult males as having "well-developed friendships." Cubs are especially playful as well. Among young males in particular, play-fighting may be a means of practicing for serious competition during mating seasons later in life. Polar bears are usually quiet but do communicate with various sounds and vocalizations. Females communicate with their young with moans and chuffs, and the distress calls of both cubs and subadults consists of bleats. Cubs may hum while nursing. When nervous, bears produce huffs, chuffs and snorts while hisses, growls and roars are signs of aggression. Chemical communication can also be important: bears leave behind their scent in their tracks which allow individuals to keep track of one another in the vast Arctic wilderness.

In 1992, a photographer near Churchill took a now widely circulated set of photographs of a polar bear playing with a Canadian Eskimo Dog ("Canis lupus familiaris") a tenth of its size. The pair wrestled harmlessly together each afternoon for 10 days in a row for no apparent reason, although the bear may have been trying to demonstrate its friendliness in the hope of sharing the kennel's food. This kind of social interaction is uncommon; it is far more typical for polar bears to behave aggressively towards dogs.

The polar bear is the most carnivorous member of the bear family, and throughout most of its range, its diet primarily consists of ringed ("Pusa hispida") and bearded seals ("Erignathus barbatus"). The Arctic is home to millions of seals, which become prey when they surface in holes in the ice in order to breathe, or when they haul out on the ice to rest. Polar bears hunt primarily at the interface between ice, water, and air; they only rarely catch seals on land or in open water.

The polar bear's most common hunting method is called "still-hunting": the bear uses its excellent sense of smell to locate a seal breathing hole, and crouches nearby in silence for a seal to appear. The bear may lie in wait for several hours. When the seal exhales, the bear smells its breath, reaches into the hole with a forepaw, and drags it out onto the ice. The polar bear kills the seal by biting its head to crush its skull. The polar bear also hunts by stalking seals resting on the ice: upon spotting a seal, it walks to within , and then crouches. If the seal does not notice, the bear creeps to within of the seal and then suddenly rushes forth to attack. A third hunting method is to raid the birth lairs that female seals create in the snow.

A widespread legend tells that polar bears cover their black noses with their paws when hunting. This behaviour, if it happens, is rare – although the story exists in the oral history of northern peoples and in accounts by early Arctic explorers, there is no record of an eyewitness account of the behaviour in recent decades.
Mature bears tend to eat only the calorie-rich skin and blubber of the seal, which are highly digestible, whereas younger bears consume the protein-rich red meat. Studies have also photographed polar bears scaling near-vertical cliffs, to eat birds' chicks and eggs. For subadult bears, which are independent of their mother but have not yet gained enough experience and body size to successfully hunt seals, scavenging the carcasses from other bears' kills is an important source of nutrition. Subadults may also be forced to accept a half-eaten carcass if they kill a seal but cannot defend it from larger polar bears. After feeding, polar bears wash themselves with water or snow.

Although polar bears are extraordinarily powerful, its primary prey species, the ringed seal, is much smaller than itself, and many of the seals hunted are pups rather than adults. Ringed seals are born weighing and grown to an estimated average weight of only . They also in places prey heavily upon the harp seal ("Pagophilus groenlandicus") or the harbour seal. The bearded seal, on the other hand, can be nearly the same size as the bear itself, averaging . Adult male bearded seals, at are too large for a female bear to overtake, and so are potential prey only for mature male bears. Large males also occasionally attempt to hunt and kill even larger prey items. It can kill an adult walrus ("Odobenus rosmarus"), although this is rarely attempted. At up to and a typical adult mass range of , a walrus can be more than twice the bear's weight, has extremely thick skin and has up to -long ivory tusks that can be used as formidable weapons. A polar bear may charge a group of walruses, with the goal of separating a young, infirm, or injured walrus from the pod. They will even attack adult walruses when their diving holes have frozen over or intercept them before they can get back to the diving hole in the ice. Yet, polar bears will very seldom attack full-grown adult walruses, with the largest male walrus probably invulnerable unless otherwise injured or incapacitated. Since an attack on a walrus tends to be an extremely protracted and exhausting venture, bears have been known to back down from the attack after making the initial injury to the walrus. Polar bears have also been seen to prey on beluga whales ("Delphinapterus leucas") and narwhals ("Monodon monoceros"), by swiping at them at breathing holes. The whales are of similar size to the walrus and nearly as difficult for the bear to subdue. Most terrestrial animals in the Arctic can outrun the polar bear on land as polar bears overheat quickly, and most marine animals the bear encounters can outswim it. In some areas, the polar bear's diet is supplemented by walrus calves and by the carcasses of dead adult walruses or whales, whose blubber is readily devoured even when rotten. Polar bears sometimes swim underwater to catch fish like the Arctic charr or the fourhorn sculpin.
With the exception of pregnant females, polar bears are active year-round, although they have a vestigial hibernation induction trigger in their blood. Unlike brown and black bears, polar bears are capable of fasting for up to several months during late summer and early fall, when they cannot hunt for seals because the sea is unfrozen. When sea ice is unavailable during summer and early autumn, some populations live off fat reserves for months at a time, as polar bears do not 'hibernate' any time of the year.

Being both curious animals and scavengers, polar bears investigate and consume garbage where they come into contact with humans. Polar bears may attempt to consume almost anything they can find, including hazardous substances such as styrofoam, plastic, car batteries, ethylene glycol, hydraulic fluid, and motor oil. The dump in Churchill, Manitoba was closed in 2006 to protect bears, and waste is now recycled or transported to Thompson, Manitoba.

Although seal predation is the primary and an indispensable way of life for most polar bears, when alternatives are present they are quite flexible. Polar bears consume a wide variety of other wild foods, including muskox ("Ovibos moschatus"), reindeer ("Rangifer tarandus"), birds, eggs, rodents, crabs, other crustaceans and other polar bears. They may also eat plants, including berries, roots, and kelp; however, none of these have been a significant part of their diet, except for beachcast marine mammal carcasses. Given the change in climate, with ice breaking up in areas such as the Hudson Bay earlier than it used to, polar bears are exploiting food resources such as snow geese and eggs, and plants such as lyme grass in increased quantities.

When stalking land animals, such as muskox, reindeer, and even willow ptarmigan ("Lagopus lagopus"), polar bears appear to make use of vegetative cover and wind direction to bring them as close to their prey as possible before attacking. Polar bears have been observed to hunt the small Svalbard reindeer ("R. t. platyrhynchus"), which weigh only as adults, as well as the barren-ground caribou ("R. t. groenlandicus"), which is about twice as heavy as the former. Adult muskox, which can weigh or more, are a more formidable quarry. Although ungulates are not typical prey, the killing of one during the summer months can greatly increase the odds of survival during that lean period. Like the brown bear, most ungulate prey of polar bears is likely to be young, sickly or injured specimens rather than healthy adults. The polar bear's metabolism is specialized to require large amounts of fat from marine mammals, and it cannot derive sufficient caloric intake from terrestrial food.

In their southern range, especially near Hudson Bay and James Bay, Canadian polar bears endure all summer without sea ice to hunt from. Here, their food ecology shows their dietary flexibility. They still manage to consume some seals, but they are food-deprived in summer as only marine mammal carcasses are an important alternative without sea ice, especially carcasses of the beluga whale. These alternatives may reduce the rate of weight loss of bears when on land. One scientist found that 71% of the Hudson Bay bears had fed on seaweed (marine algae) and that about half were feeding on birds such as the dovekie and sea ducks, especially the long-tailed duck (53%) and common eider, by swimming underwater to catch them. They were also diving to feed on blue mussels and other underwater food sources like the green sea urchin. 24% had eaten moss recently, 19% had consumed grass, 34% had eaten black crowberry and about half had consumed willows. This study illustrates the polar bear's dietary flexibility but it does not represent its life history elsewhere. Most polar bears elsewhere will never have access to these alternatives, except for the marine mammal carcasses that are important wherever they occur.

In Svalbard, polar bears were observed to kill white-beaked dolphins during spring, when the dolphins were trapped in the sea ice. The bears then proceeded to cache the carcasses, which remained and were eaten during the ice-free summer and autumn.

Courtship and mating take place on the sea ice in April and May, when polar bears congregate in the best seal hunting areas. A male may follow the tracks of a breeding female for or more, and after finding her engage in intense fighting with other males over mating rights, fights that often result in scars and broken teeth. Polar bears have a generally polygynous mating system; recent genetic testing of mothers and cubs, however, has uncovered cases of litters in which cubs have different fathers. Partners stay together and mate repeatedly for an entire week; the mating ritual induces ovulation in the female.

After mating, the fertilized egg remains in a suspended state until August or September. During these four months, the pregnant female eats prodigious amounts of food, gaining at least and often more than doubling her body weight.

When the ice floes are at their minimum in the fall, ending the possibility of hunting, each pregnant female digs a "maternity den" consisting of a narrow entrance tunnel leading to one to three chambers. Most maternity dens are in snowdrifts, but may also be made underground in permafrost if it is not sufficiently cold yet for snow. In most subpopulations, maternity dens are situated on land a few kilometres from the coast, and the individuals in a subpopulation tend to reuse the same denning areas each year. The polar bears that do not den on land make their dens on the sea ice. In the den, she enters a dormant state similar to hibernation. This hibernation-like state does not consist of continuous sleeping; however, the bear's heart rate slows from 46 to 27 beats per minute. Her body temperature does not decrease during this period as it would for a typical mammal in hibernation.

Between November and February, cubs are born blind, covered with a light down fur, and weighing less than , but in captivity they might be delivered in the earlier months. The earliest recorded birth of polar bears in captivity was on 11 October 2011 in the Toronto Zoo. On average, each litter has two cubs. The family remains in the den until mid-February to mid-April, with the mother maintaining her fast while nursing her cubs on a fat-rich milk. By the time the mother breaks open the entrance to the den, her cubs weigh about . For about 12 to 15 days, the family spends time outside the den while remaining in its vicinity, the mother grazing on vegetation while the cubs become used to walking and playing. Then they begin the long walk from the denning area to the sea ice, where the mother can once again catch seals. Depending on the timing of ice-floe breakup in the fall, she may have fasted for up to eight months. During this time, cubs playfully imitate the mother's hunting methods in preparation for later life.

Female polar bears have been known to adopt other cubs. Multiple cases of adoption of wild cubs have been confirmed by genetic testing. Adult bears of either gender occasionally kill and eat polar bear cubs. As of 2006, in Alaska, 42% of cubs were reaching 12 months of age, down from 65% in 1991. In most areas, cubs are weaned at two and a half years of age, when the mother chases them away or abandons them. The Western Hudson Bay subpopulation is unusual in that its female polar bears sometimes wean their cubs at only one and a half years. This was the case for 40% of cubs there in the early 1980s; however by the 1990s, fewer than 20% of cubs were weaned this young. After the mother leaves, sibling cubs sometimes travel and share food together for weeks or months.

Females begin to breed at the age of four years in most areas, and five years in the area of the Beaufort Sea. Males usually reach sexual maturity at six years; however, as competition for females is fierce, many do not breed until the age of eight or ten. A study in Hudson Bay indicated that both the reproductive success and the maternal weight of females peaked in their mid-teens.Maternal success appeared to decline after this point, possibly because of an age-related impairment in the ability to store the fat necessary to rear cubs.

Polar bears appear to be less affected by infectious diseases and parasites than most terrestrial mammals. Polar bears are especially susceptible to "Trichinella", a parasitic roundworm they contract through cannibalism, although infections are usually not fatal. Only one case of a polar bear with rabies has been documented, even though polar bears frequently interact with Arctic foxes, which often carry rabies. Bacterial leptospirosis and "Morbillivirus" have been recorded. Polar bears sometimes have problems with various skin diseases that may be caused by mites or other parasites.

Polar bears rarely live beyond 25 years. The oldest wild bears on record died at age 32, whereas the oldest captive was a female who died in 1991, age 43. The causes of death in wild adult polar bears are poorly understood, as carcasses are rarely found in the species's frigid habitat. In the wild, old polar bears eventually become too weak to catch food, and gradually starve to death. Polar bears injured in fights or accidents may either die from their injuries, or become unable to hunt effectively, leading to starvation.

The polar bear is the apex predator within its range, and is a keystone species for the Arctic. Several animal species, particularly Arctic foxes ("Vulpes lagopus") and glaucous gulls ("Larus hyperboreus"), routinely scavenge polar bear kills.

The relationship between ringed seals and polar bears is so close that the abundance of ringed seals in some areas appears to regulate the density of polar bears, while polar bear predation in turn regulates density and reproductive success of ringed seals. The evolutionary pressure of polar bear predation on seals probably accounts for some significant differences between Arctic and Antarctic seals. Compared to the Antarctic, where there is no major surface predator, Arctic seals use more breathing holes per individual, appear more restless when hauled out on the ice, and rarely defecate on the ice. The baby fur of most Arctic seal species is white, presumably to provide camouflage from predators, whereas Antarctic seals all have dark fur at birth.

Brown bears tend to dominate polar bears in disputes over carcasses, and dead polar bear cubs have been found in brown bear dens. Wolves are rarely encountered by polar bears, though there are two records of Arctic wolf ("Canis lupus arctos") packs killing polar bear cubs. Adult polar bears are occasionally vulnerable to predation by orcas ("Orcinus orca") while swimming, but they are rarely reported as taken and bears are likely to avoid entering the water if possible if they detect an orca pod in the area. The melting sea ice in the Arctic may be causing an increase of orcas in the Arctic sea, which may increase the risk of predation on polar bears but also may benefit the bears by providing more whale carcasses that they can scavenge. The remains of polar bears have found in the stomachs of large Greenland sharks ("Somniosus microcephalus"), although it certainly cannot be ruled out that the bears were merely scavenged by this slow-moving, unusual shark. A rather unlikely killer of a grown polar bear has reportedly included a wolverine ("Gulo gulo"), anecedotely reported to have suffocated a bear in a zoo with a bite to the throat during a conflict. This report may well be dubious, however. Polar bears are sometimes the host of arctic mites such as "Alaskozetes antarcticus".

Researchers tracked 52 sows in the southern Beaufort Sea off Alaska with GPS system collars; no boars were involved in the study due to males' necks being too thick for the GPS-equipped collars. Fifty long-distance swims were recorded; the longest at , with an average of . The length of these swims ranged from most of a day to ten days. Ten of the sows had a cub swim with them and after a year, six cubs survived. The study did not determine if the others lost their cubs before, during, or some time after their long swims. Researchers do not know whether or not this is a new behaviour; before polar ice shrinkage, they opined that there was probably neither the need nor opportunity to swim such long distances.

The polar bear may swim underwater for up to three minutes to approach seals on shore or on ice floes.

Polar bears have long provided important raw materials for Arctic peoples, including the Inuit, Yupik, Chukchi, Nenets, Russian Pomors and others. Hunters commonly used teams of dogs to distract the bear, allowing the hunter to spear the bear or shoot it with arrows at closer range. Almost all parts of captured animals had a use. The fur was used in particular to make trousers and, by the Nenets, to make galoshes-like outer footwear called "tobok"; the meat is edible, despite some risk of trichinosis; the fat was used in food and as a fuel for lighting homes, alongside seal and whale blubber; sinews were used as thread for sewing clothes; the gallbladder and sometimes heart were dried and powdered for medicinal purposes; the large canine teeth were highly valued as talismans. Only the liver was not used, as its high concentration of vitamin A is poisonous. As a carnivore, which feeds largely upon fish-eating carnivores, the polar bear ingests large amounts of vitamin A that is stored in their livers. The resulting high concentrations cause Hypervitaminosis A, Hunters make sure to either toss the liver into the sea or bury it in order to spare their dogs from potential poisoning. Traditional subsistence hunting was on a small enough scale to not significantly affect polar bear populations, mostly because of the sparseness of the human population in polar bear habitat.

In Russia, polar bear furs were already being commercially traded in the 14th century, though it was of low value compared to Arctic fox or even reindeer fur. The growth of the human population in the Eurasian Arctic in the 16th and 17th century, together with the advent of firearms and increasing trade, dramatically increased the harvest of polar bears. However, since polar bear fur has always played a marginal commercial role, data on the historical harvest is fragmentary. It is known, for example, that already in the winter of 1784/1785 Russian Pomors on Spitsbergen harvested 150 polar bears in Magdalenefjorden. In the early 20th century, Norwegian hunters were harvesting 300 bears per year at the same location. Estimates of total historical harvest suggest that from the beginning of the 18th century, roughly 400 to 500 animals were being harvested annually in northern Eurasia, reaching a peak of 1,300 to 1,500 animals in the early 20th century, and falling off as the numbers began dwindling.

In the first half of the 20th century, mechanized and overpoweringly efficient methods of hunting and trapping came into use in North America as well. Polar bears were chased from snowmobiles, icebreakers, and airplanes, the latter practice described in a 1965 "New York Times" editorial as being "about as sporting as machine gunning a cow." Norwegians used "self-killing guns", comprising a loaded rifle in a baited box that was placed at the level of a bear's head, and which fired when the string attached to the bait was pulled. The numbers taken grew rapidly in the 1960s, peaking around 1968 with a global total of 1,250 bears that year.

Concerns over the future survival of the species led to the development of national regulations on polar bear hunting, beginning in the mid-1950s. The Soviet Union banned all hunting in 1956. Canada began imposing hunting quotas in 1968. Norway passed a series of increasingly strict regulations from 1965 to 1973, and has completely banned hunting since then. The United States began regulating hunting in 1971 and adopted the Marine Mammal Protection Act in 1972. In 1973, the International Agreement on the Conservation of Polar Bears was signed by all five nations whose territory is inhabited by polar bears: Canada, Denmark, Norway, the Soviet Union, and the United States. Member countries agreed to place restrictions on recreational and commercial hunting, ban hunting from aircraft and icebreakers, and conduct further research. The treaty allows hunting "by local people using traditional methods". Norway is the only country of the five in which all harvest of polar bears is banned. The agreement was a rare case of international cooperation during the Cold War. Biologist Ian Stirling commented, "For many years, the conservation of polar bears was the only subject in the entire Arctic that nations from both sides of the Iron Curtain could agree upon sufficiently to sign an agreement. Such was the intensity of human fascination with this magnificent predator, the only marine bear."

Agreements have been made between countries to co-manage their shared polar bear subpopulations. After several years of negotiations, Russia and the United States signed an agreement in October 2000 to jointly set quotas for indigenous subsistence hunting in Alaska and Chukotka. The treaty was ratified in October 2007. In September 2015, the polar bear range states agreed upon a "circumpolar action plan" describing their conservation strategy for polar bears.

Although the United States government has proposed that polar bears be transferred to Appendix I of CITES, which would ban all international trade in polar bear parts, polar bears currently remain listed under Appendix II. This decision was approved of by members of the IUCN and TRAFFIC, who determined that such an uplisting was unlikely to confer a conservation benefit.

Polar bears were designated "Not at Risk" in April 1986 and uplisted to "Special Concern" in April 1991. This status was re-evaluated and confirmed in April 1999, November 2002, and April 2008. Polar bears continue to be listed as a species of special concern in Canada because of their sensitivity to overharvest and because of an expected range contraction caused by loss of Arctic sea ice.

More than 600 bears are killed per year by humans across Canada, a rate calculated by scientists to be unsustainable for some areas, notably Baffin Bay. Canada has allowed sport hunters accompanied by local guides and dog-sled teams since 1970, but the practice was not common until the 1980s. The guiding of sport hunters provides meaningful employment and an important source of income for northern communities in which economic opportunities are few. Sport hunting can bring CDN$20,000 to $35,000 per bear into northern communities, which until recently has been mostly from American hunters.

The territory of Nunavut accounts for the location 80% of annual kills in Canada. In 2005, the government of Nunavut increased the quota from 400 to 518 bears, despite protests from the IUCN Polar Bear Specialist Group. In two areas where harvest levels have been increased based on increased sightings, science-based studies have indicated declining populations, and a third area is considered data-deficient. While most of that quota is hunted by the indigenous Inuit people, a growing share is sold to recreational hunters. (0.8% in the 1970s, 7.1% in the 1980s, and 14.6% in the 1990s) Nunavut polar bear biologist, Mitchell Taylor, who was formerly responsible for polar bear conservation in the territory, has insisted that bear numbers are being sustained under current hunting limits. In 2010, the 2005 increase was partially reversed. Government of Nunavut officials announced that the polar bear quota for the Baffin Bay region would be gradually reduced from 105 per year to 65 by the year 2013. The Government of the Northwest Territories maintain their own quota of 72 to 103 bears within the Inuvialuit communities of which some are set aside for sports hunters. Environment Canada also banned the export from Canada of fur, claws, skulls and other products from polar bears harvested in Baffin Bay as of 1 January 2010.

Because of the way polar bear hunting quotas are managed in Canada, attempts to discourage sport hunting would actually increase the number of bears killed in the short term. Canada allocates a certain number of permits each year to sport and subsistence hunting, and those that are not used for sport hunting are re-allocated to indigenous subsistence hunting. Whereas northern communities kill all the polar bears they are permitted to take each year, only half of sport hunters with permits actually manage to kill a polar bear. If a sport hunter does not kill a polar bear before his or her permit expires, the permit cannot be transferred to another hunter.

In August 2011, Environment Canada published a national polar bear conservation strategy.

In Greenland, hunting restrictions were first introduced in 1994 and expanded by executive order in 2005. Until 2005 Greenland placed no limit on hunting by indigenous people. However, in 2006 it imposed a limit of 150, while also allowed recreational hunting for the first time. Other provisions included year-round protection of cubs and mothers, restrictions on weapons used and various administrative requirements to catalogue kills.

Polar bears were hunted heavily in Svalbard, Norway throughout the 19th century and to as recently as 1973, when the conservation treaty was signed. 900 bears a year were harvested in the 1920s and after World War II, there were as many as 400–500 harvested annually. Some regulations of hunting did exist. In 1927, poisoning was outlawed while in 1939, certain denning sights were declared off limits. The killing of females and cubs was made illegal in 1965. Killing of polar bears decreased somewhat 25–30 years before the treaty. Despite this, the polar bear population continued to decline and by 1973, only around 1000 bears were left in Svalbard. Only with the passage of the treaty did they begin to recover.

The Soviet Union banned the harvest of polar bears in 1956; however, poaching continued, and is estimated to pose a serious threat to the polar bear population. In recent years, polar bears have approached coastal villages in Chukotka more frequently due to the shrinking of the sea ice, endangering humans and raising concerns that illegal hunting would become even more prevalent. In 2007, the Russian government made subsistence hunting legal for indigenous Chukotkan peoples only, a move supported by Russia's most prominent bear researchers and the World Wide Fund for Nature as a means to curb poaching.

Polar bears are currently listed as "Rare", of "Uncertain Status", or "Rehabilitated and rehabilitating" in the Red Data Book of Russia, depending on population. In 2010, the Ministry of Natural Resources and Environment published a strategy for polar bear conservation in Russia.

The Marine Mammal Protection Act of 1972 afforded polar bears some protection in the United States. It banned hunting (except by indigenous subsistence hunters), banned importing of polar bear parts (except polar bear pelts taken legally in Canada), and banned the harassment of polar bears. On 15 May 2008, the United States Department of the Interior listed the polar bear as a threatened species under the Endangered Species Act, citing the melting of Arctic sea ice as the primary threat to the polar bear. It banned all importing of polar bear trophies. Importing products made from polar bears had been prohibited from 1972 to 1994 under the Marine Mammal Protection Act, and restricted between 1994 and 2008. Under those restrictions, permits from the United States Fish and Wildlife Service were required to import sport-hunted polar bear trophies taken in hunting expeditions in Canada. The permit process required that the bear be taken from an area with quotas based on sound management principles. Since 1994, hundreds of sport-hunted polar bear trophies have been imported into the U.S. In 2015, the U.S. Fish and Wildlife Service published a draft conservation management plan for polar bears to improve their status under the Endangered Species Act and the Marine Mammal Protection Act.

Polar bear population sizes and trends are difficult to estimate accurately because they occupy remote home ranges and exist at low population densities. Polar bear fieldwork can also be hazardous to researchers. As of 2015, the International Union for Conservation of Nature (IUCN) reports that the global population of polar bears is 22,000 to 31,000, and the current population trend is unknown. Nevertheless, polar bears are listed as "Vulnerable" under criterion A3c, which indicates an expected population decrease of ≥30% over the next three generations (~34.5 years) due to "decline in area of occupancy, extent of occurrence and/or quality of habitat". Risks to the polar bear include climate change, pollution in the form of toxic contaminants, conflicts with shipping, oil and gas exploration and development, and human-bear interactions including harvesting and possible stresses from recreational polar-bear watching.

According to the World Wildlife Fund, the polar bear is important as an indicator of Arctic ecosystem health. Polar bears are studied to gain understanding of what is happening throughout the Arctic, because at-risk polar bears are often a sign of something wrong with the Arctic marine ecosystem.

The International Union for Conservation of Nature, Arctic Climate Impact Assessment, United States Geological Survey and many leading polar bear biologists have expressed grave concerns about the impact of climate change, including the belief that the current warming trend imperils the survival of the polar bear.

The key danger posed by climate change is malnutrition or starvation due to habitat loss. Polar bears hunt seals from a platform of sea ice. Rising temperatures cause the sea ice to melt earlier in the year, driving the bears to shore before they have built sufficient fat reserves to survive the period of scarce food in the late summer and early fall. Reduction in sea-ice cover also forces bears to swim longer distances, which further depletes their energy stores and occasionally leads to drowning. Thinner sea ice tends to deform more easily, which appears to make it more difficult for polar bears to access seals. Insufficient nourishment leads to lower reproductive rates in adult females and lower survival rates in cubs and juvenile bears, in addition to poorer body condition in bears of all ages.
In addition to creating nutritional stress, a warming climate is expected to affect various other aspects of polar bear life: Changes in sea ice affect the ability of pregnant females to build suitable maternity dens. As the distance increases between the pack ice and the coast, females must swim longer distances to reach favoured denning areas on land. Thawing of permafrost would affect the bears who traditionally den underground, and warm winters could result in den roofs collapsing or having reduced insulative value. For the polar bears that currently den on multi-year ice, increased ice mobility may result in longer distances for mothers and young cubs to walk when they return to seal-hunting areas in the spring. Disease-causing bacteria and parasites would flourish more readily in a warmer climate.

Problematic interactions between polar bears and humans, such as foraging by bears in garbage dumps, have historically been more prevalent in years when ice-floe breakup occurred early and local polar bears were relatively thin. Increased human-bear interactions, including fatal attacks on humans, are likely to increase as the sea ice shrinks and hungry bears try to find food on land.
The effects of climate change are most profound in the southern part of the polar bear's range, and this is indeed where significant degradation of local populations has been observed. The Western Hudson Bay subpopulation, in a southern part of the range, also happens to be one of the best-studied polar bear subpopulations. This subpopulation feeds heavily on ringed seals in late spring, when newly weaned and easily hunted seal pups are abundant. The late spring hunting season ends for polar bears when the ice begins to melt and break up, and they fast or eat little during the summer until the sea freezes again.

Due to warming air temperatures, ice-floe breakup in western Hudson Bay is currently occurring three weeks earlier than it did 30 years ago, reducing the duration of the polar bear feeding season. The body condition of polar bears has declined during this period; the average weight of lone (and likely pregnant) female polar bears was approximately in 1980 and in 2004. Between 1987 and 2004, the Western Hudson Bay population declined by 22%, although the population was listed as "stable" as of 2017. As the climate change melts sea ice, the U.S. Geological Survey projects that two-thirds of polar bears will disappear by 2050.

In Alaska, the effects of sea ice shrinkage have contributed to higher mortality rates in polar bear cubs, and have led to changes in the denning locations of pregnant females. The proportion of maternity dens on sea ice has changed from 62% between the years 1985 through 1994, to 37% over the years 1998 through 2004. Thus, now the Alaskan population more resembles the world population in that it is more likely to den on land. In recent years, polar bears in the Arctic have undertaken longer than usual swims to find prey, possibly resulting in four recorded drownings in the unusually large ice pack regression of 2005.

A new development is that polar bears have begun ranging to new territory. While not unheard of but still uncommon, polar bears have been sighted increasingly in larger numbers ashore, staying on the mainland for longer periods of time during the summer months, particularly in North Canada, traveling farther inland. This may cause an increased reliance on terrestrial diets, such as goose eggs, waterfowl and caribou, as well as increased human–bear conflict.

Polar bears accumulate high levels of persistent organic pollutants such as polychlorinated biphenyl (PCBs) and chlorinated pesticides. Due to their position at the top of the ecological pyramid, with a diet heavy in blubber in which halocarbons concentrate, their bodies are among the most contaminated of Arctic mammals. Halocarbons (also known as organohalogens) are known to be toxic to other animals, because they mimic hormone chemistry, and biomarkers such as immunoglobulin G and retinol suggest similar effects on polar bears. PCBs have received the most study, and they have been associated with birth defects and immune system deficiency.

Many chemicals, such as PCBs and DDT, have been internationally banned due to the recognition of their harm on the environment. Their concentrations in polar bear tissues continued to rise for decades after being banned, as these chemicals spread through the food chain. Since then, the trend seems to have abated, with tissue concentrations of PCBs declining between studies performed from 1989 to 1993 and studies performed from 1996 to 2002. During the same time periods, DDT was found to be notably lower in the Western Hudson Bay population only.

Oil and gas development in polar bear habitat can affect the bears in a variety of ways. An oil spill in the Arctic would most likely concentrate in the areas where polar bears and their prey are also concentrated, such as sea ice leads. Because polar bears rely partly on their fur for insulation and soiling of the fur by oil reduces its insulative value, oil spills put bears at risk of dying from hypothermia. Polar bears exposed to oil spill conditions have been observed to lick the oil from their fur, leading to fatal kidney failure. Maternity dens, used by pregnant females and by females with infants, can also be disturbed by nearby oil exploration and development. Disturbance of these sensitive sites may trigger the mother to abandon her den prematurely, or abandon her litter altogether.

Steven Amstrup and other U.S. Geological Survey scientists have predicted two-thirds of the world's polar bears may disappear by 2050, based on moderate projections for the shrinking of summer sea ice caused by climate change, though the validity of this study has been debated. The bears could disappear from Europe, Asia, and Alaska, and be depleted from the Canadian Arctic Archipelago and areas off the northern Greenland coast. By 2080, they could disappear from Greenland entirely and from the northern Canadian coast, leaving only dwindling numbers in the interior Arctic Archipelago. However, in the short term, some polar bear populations in historically colder regions of the Arctic may temporarily benefit from a milder climate, as multiyear ice that is too thick for seals to create breathing holes is replaced by thinner annual ice.

Polar bears diverged from brown bears 400,000–600,000 years ago and have survived past periods of climate fluctuation. It has been claimed that polar bears will be able to adapt to terrestrial food sources as the sea ice they use to hunt seals disappears. However, most polar bear biologists think that polar bears will be unable to completely offset the loss of calorie-rich seal blubber with terrestrial foods, and that they will be outcompeted by brown bears in this terrestrial niche, ultimately leading to a population decline.

Warnings about the future of the polar bear are often contrasted with the fact that worldwide population estimates have increased over the past 50 years and are relatively stable today. Some estimates of the global population are around 5,000 to 10,000 in the early 1970s; other estimates were 20,000 to 40,000 during the 1980s. Current estimates put the global population at between 20,000 and 25,000 or 22,000 and 31,000.

There are several reasons for the apparent discordance between past and projected population trends: estimates from the 1950s and 1960s were based on stories from explorers and hunters rather than on scientific surveys. Second, controls of harvesting were introduced that allowed this previously overhunted species to recover. Third, the recent effects of climate change have affected sea ice abundance in different areas to varying degrees.

Debate over the listing of the polar bear under endangered species legislation has put conservation groups and Canada's Inuit at opposing positions; the Nunavut government and many northern residents have condemned the U.S. initiative to list the polar bear under the Endangered Species Act. Many Inuit believe the polar bear population is increasing, and restrictions on commercial sport-hunting are likely to lead to a loss of income to their communities.

For the indigenous peoples of the Arctic, polar bears have long played an important cultural and material role. Polar bear remains have been found at hunting sites dating to 2,500 to 3,000 years ago and 1,500-year-old cave paintings of polar bears have been found in the Chukchi Peninsula. Indeed, it has been suggested that Arctic peoples' skills in seal hunting and igloo construction has been in part acquired from the polar bears themselves.

The Inuit and Alaska Natives have many folk tales featuring the bears including legends in which bears are humans when inside their own houses and put on bear hides when going outside, and stories of how the constellation that is said to resemble a great bear surrounded by dogs came into being. These legends reveal a deep respect for the polar bear, which is portrayed as both spiritually powerful and closely akin to humans. The human-like posture of bears when standing and sitting, and the resemblance of a skinned bear carcass to the human body, have probably contributed to the belief that the spirits of humans and bears were interchangeable.

Among the Chukchi and Yupik of eastern Siberia, there was a longstanding shamanistic ritual of "thanksgiving" to the hunted polar bear. After killing the animal, its head and skin were removed and cleaned and brought into the home, and a feast was held in the hunting camp in its honor. To appease the spirit of the bear, traditional song and drum music was played, and the skull was ceremonially fed and offered a pipe. Only once the spirit was appeased was the skull be separated from the skin, taken beyond the bounds of the homestead, and placed in the ground, facing north.

The Nenets of north-central Siberia placed particular value on the talismanic power of the prominent canine teeth. These were traded in the villages of the lower Yenisei and Khatanga rivers to the forest-dwelling peoples further south, who would sew them into their hats as protection against brown bears. It was believed that the "little nephew" (the brown bear) would not dare to attack a man wearing the tooth of its powerful "big uncle", the polar bear. The skulls of killed polar bears were buried at sacred sites, and altars, called "sedyangi", were constructed out of the skulls. Several such sites have been preserved on the Yamal Peninsula.

Their distinctive appearance and their association with the Arctic have made polar bears popular icons, especially in those areas where they are native. The Canadian two-dollar coin carries an image of a lone polar bear on its reverse side, while a special millennium edition featured three. Vehicle licence plates in the Northwest Territories in Canada are in the shape of a polar bear, as was the case in Nunavut until 2012; these now display polar bear artwork instead. The polar bear is the mascot of Bowdoin College, Maine; the University of Alaska Fairbanks; and the 1988 Winter Olympics held in Calgary. The Eisbären Berlin hockey team uses a roaring polar bear as their logo, and the Charlotte, North Carolina hockey team the Charlotte Checkers uses a polar bear named Chubby Checker as their mascot.

Coca-Cola has used images of the polar bear in its advertising, and Polar Beverages, Nelvana, Bundaberg Rum, Klondike bars, and Fox's Glacier Mints all feature polar bears in their logos.

Polar bears are popular in fiction, particularly in books for children or teenagers. For example, "The Polar Bear Son" is adapted from a traditional Inuit tale. The animated television series "Noah's Island" features a polar bear named Noah as the protagonist. Polar bears feature prominently in "East" ("North Child" in the UK) by Edith Pattou, "The Bear" by Raymond Briggs (adapted into an animated short in 1998), and Chris d'Lacey's "The Fire Within" series. The "panserbjørne" of Philip Pullman's fantasy trilogy "His Dark Materials" are sapient, dignified polar bears who exhibit anthropomorphic qualities, and feature prominently in the 2007 film adaptation of "The Golden Compass". The television series "Lost" features polar bears living on the tropical island setting.





</doc>
<doc id="24411" url="https://en.wikipedia.org/wiki?curid=24411" title="Pagan (disambiguation)">
Pagan (disambiguation)

A pagan is an adherent of paganism.

Pagan or Pagans may also refer to:









</doc>
<doc id="24412" url="https://en.wikipedia.org/wiki?curid=24412" title="Phalanx (disambiguation)">
Phalanx (disambiguation)

The phalanx is a rectangular mass military formation.

Phalanx may also refer to:









</doc>
<doc id="24413" url="https://en.wikipedia.org/wiki?curid=24413" title="Penguin Island">
Penguin Island

Penguin Island may refer to:



</doc>
<doc id="24416" url="https://en.wikipedia.org/wiki?curid=24416" title="Pommern (disambiguation)">
Pommern (disambiguation)

Pommern is the German language name for Pomerania, a historical region divided between Germany and Poland.

Pommern may also refer to:




</doc>
<doc id="24417" url="https://en.wikipedia.org/wiki?curid=24417" title="Punic Wars">
Punic Wars

The Punic Wars were a series of three wars between 264 and 146BC fought by the states of Rome and Carthage. The First Punic War broke out in Sicily in 264BC as a result of Rome's expansionary attitude combined with Carthage's proprietary approach to the island. At the start of the war Carthage was the dominant power of the western Mediterranean, with an extensive maritime empire; while Rome was a rapidly expanding power in Italy, with a strong army but a weak maritime arm. The fighting took place primarily on the Mediterranean island of Sicily and its surrounding waters, and also in North Africa, Corsica and Sardinia. It lasted 23 years, until 241BC, when after immense materiel and human losses on both sides the Carthaginians were defeated. By the terms of the peace treaty agreed Carthage paid large reparations and Sicily was annexed as a Roman province. The end of the war sparked a major but unsuccessful revolt within the Carthaginian Empire.
The Second Punic War began in 218BC and witnessed Hannibal's crossing of the Alps and invasion of mainland Italy in 217BC. This expedition enjoyed considerable early success, but after 14 years the survivors withdrew. There was also extensive fighting in Iberia (modern Spain and Portugal); on Sicily; on Sardinia; and in North Africa. The successful Roman invasion of the Carthaginian homeland in Africa in 204BC led to Hannibal's recall. He was defeated in the Battle of Zama and Carthage sued for peace. A treaty was agreed in 201BC which stripped Carthage of its overseas territories, and some of their African ones; imposed a large indemnity, to be paid over 50 years; severely restricted the size of its armed forces; and prohibited Carthage from waging war without Rome's express permission. Carthage ceased to be a military threat.

Rome contrived a justification to declare war on Carthage again in 149BC in the Third Punic War. This conflict was fought entirely on Carthage's territories in what is now Tunisia and largely centred around the Siege of Carthage. In 146BC the Romans stormed the city of Carthage, sacked it, slaughtered most of its population and completely demolished it. The previously Carthaginian territories were taken over as the Roman province of Africa. The ruins of the city lie east of modern Tunis on the North African coast.
The main source for almost every aspect of the Punic Wars is the historian Polybius ( – ), a Greek sent to Rome in 167BC as a hostage. His works include a now-lost manual on military tactics, but he is now known for "The Histories", written sometime after 146BC. Polybius's work is considered broadly objective and largely neutral as between Carthaginian and Roman points of view. Polybius was an analytical historian and wherever possible personally interviewed participants, from both sides, in the events he wrote about. He accompanied the Roman general Scipio Aemilianus during his campaign in North Africa which resulted in the Roman victory in the Third Punic War.

The accuracy of Polybius's account has been much debated over the past 150 years, but the modern consensus is to accept it largely at face value, and the details of the war in modern sources are largely based on interpretations of Polybius's account. The modern historian Andrew Curry sees Polybius as being "fairly reliable"; while Craige Champion describes him as "a remarkably well-informed, industrious, and insightful historian".

Other, later, ancient histories of the war exist, although often in fragmentary or summary form. Modern historians usually take into account the writings of various Roman annalists, some contemporary; the Sicilian Greek Diodorus Siculus; the later Roman historians, Livy (who relied heavily on Polybius), Plutarch, Appian (whose account of the Third Punic War is especially valuable) and Dio Cassius. The classicist Adrian Goldsworthy states "Polybius' account is usually to be preferred when it differs with any of our other accounts". Other sources include coins, inscriptions, archaeological evidence and empirical evidence from reconstructions such as the trireme "Olympias".

The Roman Republic had been aggressively expanding in the southern Italian mainland for a century before the First Punic War. It had conquered peninsular Italy south of the River Arno by 272BC, when the Greek cities of southern Italy (Magna Graecia) submitted at the conclusion of the Pyrrhic War. During this period Carthage, with its capital in what is now Tunisia, had come to dominate southern Spain, much of the coastal regions of North Africa, the Balearic Islands, Corsica, Sardinia, and the western half of Sicily in a military and commercial empire.

Beginning in 480BC, Carthage had fought a series of inconclusive wars against the Greek city states of Sicily, led by Syracuse. By 264BC Carthage and Rome were the preeminent powers in the western Mediterranean. The two states had several times asserted their mutual friendship via formal alliances: in 509BC, 348BC and around 279BC. Relationships were good, with strong commercial links. During the Pyrrhic War of 280–275BC, against a king of Epirus who alternately fought Rome in Italy and Carthage on Sicily, Carthage provided materiel to the Romans and on at least one occasion used its navy to ferry a Roman force. According to the classicist Richard Miles, Rome's expansionary attitude after southern Italy came under its control combined with Carthage's proprietary approach to Sicily caused the two powers to stumble into war more by accident than design. The immediate cause of the war was the issue of control of the independent Sicilian city state of Messana (modern Messina). In 264BC Carthage and Rome went to war, starting the First Punic War.

Most male Roman citizens were eligible for military service and would serve as infantry, a better-off minority providing a cavalry component. Traditionally, when at war the Romans would raise two legions, each of 4,200 infantry and 300 cavalry. Approximately 1,200 of the infantry, poorer or younger men unable to afford the armour and equipment of a standard legionary, served as javelin-armed skirmishers, known as velites. They carried several javelins, which would be thrown from a distance, a short sword, and a shield. The balance were equipped as heavy infantry, with body armour, a large shield and short thrusting swords. They were divided into three ranks, of which the front rank also carried two javelins, while the second and third ranks had a thrusting spear instead. Both legionary sub-units and individual legionaries fought in relatively open order. It was the long-standing Roman procedure to elect two men each year, known as consuls, to each lead an army. An army was usually formed by combining a Roman legion with a similarly sized and equipped legion provided by their Latin allies; allied legions usually had a larger attached complement of cavalry than Roman ones.

Carthaginian citizens only served in their army if there was a direct threat to the city. When they did they fought as well-armoured heavy infantry armed with long thrusting spears, although they were notoriously ill-trained and ill-disciplined. In most circumstances Carthage recruited foreigners to make up its army. Many would be from North Africa which provided several types of fighters including: close-order infantry equipped with large shields, helmets, short swords and long thrusting spears; javelin-armed light infantry skirmishers; close-order shock cavalry carrying spears; and light cavalry skirmishers who threw javelins from a distance and avoided close combat. Both Iberia and Gaul provided large numbers of experienced infantryunarmoured troops who would charge ferociously, but had a reputation for breaking off if a combat was protractedand unarmoured close-order cavalry referred to by Livy as "steady", meaning that they were accustomed to sustained hand-to-hand combat rather than hit and run tactics. The close-order Libyan infantry and the citizen-militia would fight in a tightly packed formation known as a phalanx. On occasion some of the infantry would wear captured Roman armour, especially among Hannibal's troops. Slingers were frequently recruited from the Balearic Islands. The Carthaginians also employed war elephants; North Africa had indigenous African forest elephants at the time.
Garrison duty and land blockades were the most common operations. When armies were campaigning, surprise attacks, ambushes and strategems were common. More formal battles were usually preceded by the two armies camping one to seven miles apart (2–12 km) for days or weeks; sometimes forming up in battle order each day. If neither commander could see an advantage, both sides might march off without engaging. In such circumstances it was difficult to force an engagement if the other commander was unwilling to fight. Forming up in battle order was a complicated and premeditated affair, which took several hours. Infantry were usually positioned in the centre of the battle line, with light infantry skirmishers to their front and cavalry on each flank. Many battles were decided when one side's infantry force was attacked in the flank or rear and they were partially or wholly enveloped.

Quinqueremes, meaning "five-oarsmen", provided the workhorses of the Roman and Carthaginian fleets throughout the Punic Wars. So ubiquitous was the type that Polybius uses it as a shorthand for "warship" in general. A quinquereme carried a crew of 300: 280 oarsmen and 20 deck crew and officers. It would also normally carry a complement of 40 marines, if battle was thought to be imminent this would be increased to as many as 120. In 260BC Romans set out to construct a fleet and used a shipwrecked Carthaginian quinquereme as a blueprint for their own.
As novice shipwrights, the Romans built copies that were heavier than the Carthaginian vessels, and so slower and less manoeuvrable. Getting the oarsmen to row as a unit, let alone to execute more complex battle manoeuvres, required long and arduous training. At least half of the oarsmen would need to have had some experience if the ship was to be handled effectively. As a result, the Romans were initially at a disadvantage against the more experienced Carthaginians. To counter this, the Romans introduced the "corvus", a bridge wide and long, with a heavy spike on the underside, which was designed to pierce and anchor into an enemy ship's deck. This allowed Roman legionaries acting as marines to board enemy ships and capture them, rather than employing the previously traditional tactic of ramming.

All warships were equipped with rams, a triple set of bronze blades weighing up to positioned at the waterline. In the century prior to the Punic Wars, boarding had become increasingly common and ramming had declined, as the larger and heavier vessels adopted in this period lacked the speed and manoeuvrability necessary to ram, while their sturdier construction reduced the ram's effect even in case of a successful attack. The Roman adaptation of the was a continuation of this trend and compensated for their initial disadvantage in ship-manoeuvring skills. The added weight in the prow compromised both the ship's manoeuvrability and its seaworthiness, and in rough sea conditions the became useless; part way through the First Punic War the Romans ceased using it.

Much of the First Punic War was fought on, or in the waters near, Sicily. Away from the coasts its hilly and rugged terrain made manoeuvring large forces difficult and favoured the defence over the offence. Land operations were largely confined to raids, sieges and interdiction; in 23 years of war on Sicily there were only two full-scale pitched battles.

The war began with the Romans gaining a foothold on Sicily at Messana (modern Messina). The Romans then pressed Syracuse, the only significant independent power on the island, into allying with them and laid siege to Carthage's main base at Akragas on the south coast. A Carthaginian army of 50,000 infantry, 6,000 cavalry and 60 elephants attempted to lift the siege in 262BC, but was heavily defeated at the Battle of Akragas. That night the Carthaginian garrison escaped and the Romans seized the city and its inhabitants, selling 25,000 of them into slavery. 

The land war on Sicily reached a stalemate as the Carthaginians focused on defending their well-fortified towns and cities; these were mostly on the coast and so could be supplied and reinforced without the Romans being able to use their superior army to interfere. The focus of the war shifted to the sea, where the Romans had little experience; on the few occasions they had previously felt the need for a naval presence they had usually relied on small squadrons provided by their Latin or Greek allies. The Romans built a navy to challenge Carthage's, and using the inflicted a major defeat at the Battle of Mylae in 260BC. A Carthaginian base on Corsica was seized, but an attack on Sardinia was repulsed; the base on Corsica was then lost. In 258BC a Roman fleet heavily defeated a smaller Carthaginian fleet at the Battle of Sulci off the western coast of Sardinia.

Taking advantage of their naval victories the Romans launched an invasion of North Africa in 256BC, which the Carthaginians intercepted. At the Battle of Cape Ecnomus off the south coast of Sicily the Carthaginians were again beaten; this was possibly the largest naval battle in history by the number of combatants involved. The invasion initially went well and in 255BC the Carthaginians sued for peace; the proposed terms were so harsh they fought on. At the Battle of Tunis in spring 255BC a combined force of infantry, cavalry and war elephants under the command of the Spartan mercenary Xanthippus crushed the Romans. The Romans sent a fleet to evacuate their survivors and the Carthaginians opposed it at the Battle of Cape Hermaeum (modern Cape Bon); the Carthaginians were again heavily defeated. The Roman fleet, in turn, was devastated by a storm while returning to Italy, losing most of its ships and more than 100,000 men.

The war continued, with neither side able to gain a decisive advantage. The Carthaginians attacked and recaptured Akragas in 255BC, but not believing they could hold the city, they razed and abandoned it. The Romans rapidly rebuilt their fleet, adding 220 new ships, and captured Panormus (modern Palermo) in 254BC. The next year they lost another 150 ships to a storm. On Sicily the Romans avoided battle in 252 and 251BC, according to Polybius because they feared the war elephants which the Carthaginians had shipped to the island. In 250BC the Carthaginians advanced on Panormus, but in a battle outside the walls the Romans drove off the Carthaginian elephants with javelin fire. The elephants routed through the Carthaginian infantry, who were then charged by the Roman infantry to complete their defeat.

Slowly the Romans had occupied most of Sicily; in 249BC they besieged the last two Carthaginian strongholds Lilybaeum and Drepana in the extreme west. Repeated attempts to storm Lilybaeum's strong walls failed, as did attempts to block access to its harbour, and the Romans settled down to a siege which was to last nine years. They launched a surprise attack on the Carthaginian fleet, but were defeated at the Battle of Drepana; Carthage's greatest naval victory of the war. Carthage turned to the maritime offensive, inflicting another heavy naval defeat at the Battle of Phintias and all but swept the Romans from the sea. It was to be seven years before Rome again attempted to field a substantial fleet, while Carthage put most of its ships into reserve to save money and free up manpower.

After more than 20 years of war, both states were financially and demographically exhausted. Evidence of Carthage's financial situation includes their request for a 2,000 talent loan from Ptolemaic Egypt, which was refused. Rome was also close to bankruptcy and the number of adult male citizens, who provided the manpower for the navy and the legions, had declined by 17 per cent since the start of the war. Goldsworthy describes Roman manpower losses as "appalling". 

The Romans rebuilt their fleet again in 243BC after the Senate approached Rome's wealthiest citizens for loans to finance the construction of one ship each, repayable from the reparations to be imposed on Carthage once the war was won. This new fleet effectively blockaded the Carthaginian garrisons. Carthage assembled a fleet which attempted to relieve them, but it was destroyed at the Battle of the Aegates Islands in 241BC, forcing the cut-off Carthaginian troops on Sicily to negotiate for peace.

A treaty was agreed. By its terms Carthage paid 3,200 talents of silver in reparations and Sicily was annexed as a Roman province. Henceforth Rome considered itself the leading military power in the western Mediterranean, and increasingly the Mediterranean region as a whole. The immense effort of repeatedly building large fleets of galleys during the war laid the foundation for Rome's maritime dominance for 600 years.

The Mercenary, or Truceless, War began in 241BC as a dispute over the payment of wages owed to 20,000 foreign soldiers who had fought for Carthage on Sicily during the First Punic War. This erupted into full-scale mutiny under the leadership of Spendius and Matho and 70,000 Africans from Carthage's oppressed dependant territories flocked to join them, bringing supplies and finance. War-weary Carthage fared poorly in the initial engagements, especially under the generalship of Hanno. Hamilcar Barca, a veteran of the campaigns in Sicily, was given joint command of the army in 240BC, and supreme command in 239BC. He campaigned successfully, initially demonstrating leniency in an attempt to woo the rebels over. To prevent this, in 240BC Spendius tortured 700 Carthaginian prisoners to death, and henceforth the war was pursued with great brutality.

By early 237BC, after numerous setbacks, the rebels were defeated and their cities brought back under Carthaginian rule. An expedition was prepared to reoccupy Sardinia, where mutinous soldiers had slaughtered all Carthaginians. The Roman Senate stated they considered the preparation of this force an act of war, and demanded Carthage cede Sardinia and Corsica, and pay an additional 1,200-talent indemnity. Weakened by 30 years of war, Carthage agreed rather than again enter into conflict with Rome. Polybius considered this "contrary to all justice" and modern historians have variously described the Romans' behaviour as "unprovoked aggression and treaty-breaking", "shamelessly opportunistic" and an "unscrupulous act". These events fuelled resentment in Carthage, which was not reconciled to Rome's perception of its situation. This breach of the recently signed treaty has been considered to be the single greatest cause of war with Carthage breaking out again in 218BC in the Second Punic War.

With the suppression of the rebellion, Hamilcar understood that Carthage needed to strengthen its economic and military base if it were to again confront Rome. After the First Punic War, Carthaginian possessions in Iberia (modern Spain and Portugal) were limited to a handful of prosperous coastal cities in the south. Hamilcar took the army which he had led to victory in the Mercenary War to Iberia in 237BC and carved out a quasi-monarchial, autonomous state in its south east. This gave Carthage the silver mines, agricultural wealth, manpower, military facilities such as shipyards and territorial depth to stand up to future Roman demands with confidence. Hamilcar ruled as a viceroy and was succeeded by his son-in-law, Hasdrubal, in the early 220sBC and then his son, Hannibal, in 221BC. In 226BC the Ebro Treaty was agreed with Rome, specifying the Ebro River as the northern boundary of the Carthaginian sphere of influence. A little later Rome made a separate treaty with the city of Saguntum, which was situated well south of the Ebro.

In 219BC a Carthaginian army under Hannibal besieged, captured and sacked Saguntum and in spring 218BC Rome declared war on Carthage. There were three main military theatres in the war: Italy, where Hannibal defeated the Roman legions repeatedly, with occasional subsidiary campaigns in Sicily, Sardinia and Greece; Iberia, where Hasdrubal, a younger brother of Hannibal, defended the Carthaginian colonial cities with mixed success until moving into Italy; and Africa, where the war was decided.

In 218BC there was some naval skirmishing in the waters around Sicily. The Romans beat off a Carthaginian attack and captured the island of Malta. In Cisalpine Gaul (modern northern Italy), the major Gallic tribes attacked the Roman colonies there, causing the Romans to flee to Mutina (modern Modena), which they besieged. A Roman relief army broke through the siege, but was then ambushed and besieged itself. An army had been created to campaign in Iberia, but the Roman Senate detached one Roman and one allied legion from it to send to north Italy. Raising fresh troops to replace these delayed the army's departure for Iberia until September.

Meanwhile, Hannibal assembled a Carthaginian army in New Carthage (modern Cartagena) and led it northwards along the coast in May or June. It entered Gaul and took an inland route, to avoid the Roman allies along the coast. At the Battle of Rhone Crossing, Hannibal defeated a force of local Allobroges which sought to bar his way. A Roman fleet carrying the Iberian-bound army landed at Rome's ally Massalia (modern Marseille) at the mouth of the Rhone, but Hannibal evaded the Romans and they continued to Iberia. The Carthaginians reached the foot of the Alps by late autumn and crossed them, surmounting the difficulties of climate, terrain and the guerrilla tactics of the native tribes. Hannibal arrived with 20,000 infantry, 6,000 cavalry, and an unknown number of elephantsthe survivors of the 37 with which he left Iberiain what is now Piedmont, northern Italy. The Romans were still in their winter quarters. His surprise entry into the Italian peninsula led to the cancellation of Rome's planned campaign for the year: an invasion of Africa.

Hannibal captured the chief city of the hostile Taurini (in the area of modern Turin) and his army routed the cavalry and light infantry of the Romans at the Battle of Ticinus. As a result, most of the Gallic tribes declared for the Carthaginian cause, and Hannibal's army grew to more than 40,000 men. A large Roman army was lured into combat by Hannibal at the Battle of the Trebia, encircled and destroyed. Only 10,000 Romans out of 42,000 were able to cut their way to safety. Gauls now joined Hannibal's army in large numbers, bringing it up to 60,000 men. The Romans stationed an army at Arretium and one on the Adriatic coast to block Hannibal's advance into central Italy.

In early spring 217BC, the Carthaginians crossed the Apennines unopposed, taking a difficult but unguarded route. Hannibal attempted without success to draw the main Roman army under Gaius Flaminius into a pitched battle by devastating the area they had been sent to protect. Hannibal then cut off the Roman army from Rome, which provoked Flaminius into a hasty pursuit without proper reconnaissance. Hannibal set an ambush and in the Battle of Lake Trasimene completely defeated the Roman army, killing Flaminius and other 15,000 Romans and taking 15,000 prisoner. A cavalry force of 4,000 from the other Roman army were also engaged and wiped out. The prisoners were badly treated if they were Romans, but released if they were from one of Rome's Latin allies. Hannibal hoped some of these allies could be persuaded to defect, and marched south in the hope of winning over Roman allies among the ethnic Greek and Italic city states.

The Romans, panicked by these heavy defeats, appointed Quintus Fabius Maximus as dictator. Fabius introduced the Fabian strategy of avoiding open battle with his opponent, but constantly skirmishing with small detachments of the enemy. This was not popular among the soldiers, the Roman public nor the Roman elite, since he avoided battle while Italy was being devastated by the enemy. Hannibal marched through the richest and most fertile provinces of Italy, hoping the devastation would draw Fabius into battle, but Fabius refused.

At the elections of 216BC Gaius Terentius Varro and Lucius Aemilius Paullus were elected as consuls; both were more aggressive-minded than Fabius. The Roman Senate authorised the raising of a force of 86,000 men, the largest in Roman history to that point. Paullus and Varro marched southward to confront Hannibal, who accepted battle on the open plain near Cannae. In the Battle of Cannae the Roman legions forced their way through Hannibal's deliberately weak centre, but Libyan heavy infantry on the wings swung around their advance, menacing their flanks. Hasdrubal led Carthaginian cavalry on the left wing and routed the Roman cavalry opposite, then swept around the rear of the Romans to attack the cavalry on the other wing. He then charged into the legions from behind. As a result, the Roman infantry was surrounded with no means of escape. At least 67,500 Romans were killed or captured.

Within a few weeks of Cannae a Roman army of 25,000 was ambushed by Boii Gauls at the Battle of Silva Litana and annihilated.

Little has survived of Polybius's account of Hannibal's army in Italy after Cannae. Livy gives a fuller record, but according to Goldsworthy "his reliability is often suspect", especially with regard to his descriptions of battles; nevertheless his is the best surviving source for this part of the war. Several of the city states in southern Italy allied themselves with Hannibal, or were captured when pro-Carthaginian factions betrayed their defences. These included the large city of Capua and the major port city of Tarentum (modern Taranto). Two of the major Samnite tribes also joined the Carthaginian cause. By 214BC the bulk of southern Italy had turned against Rome.

However, the majority of Rome's allies remained loyal, including many in southern Italy. All except the smallest towns were too well fortified for Hannibal to take by assault, and blockade could be a long-drawn-out affair, or if the target was a port, impossible. Carthage's new allies felt little sense of community with Carthage, or even with each other. The new allies increased the number of fixed points which Hannibal's army was expected to defend from Roman retribution, but provided relatively few fresh troops to assist him in doing so. Such Italian forces as were raised resisted operating away from their home cities and performed badly when they did.
When the port city of Locri defected to Carthage in the summer of 215BC it was immediately used to reinforce the Carthaginian forces in Italy with soldiers, supplies and war elephants. It was the only time during the war that Carthage reinforced Hannibal. A second force, under Hannibal's youngest brother Mago, was meant to land in Italy in 215BC but was diverted to Iberia after the Carthaginian defeat in Iberia at the Battle of Dertosa.

Meanwhile the Romans took drastic steps to raise new legions: enrolling slaves, criminals and those who did not meet the usual property qualification. By early 215BC they were fielding at least 12 legions; by 214BC, 18; and by 213BC, 22. By 212 BC the full complement of the legions deployed would have been in excess of 100,000 men, plus, as always, a similar number of allied troops. The majority were deployed in southern Italy in field armies of approximately 20,000 men each. This was insufficient to challenge Hannibal's army in open battle, but sufficient to force him to concentrate his forces and to hamper his movements.

For 11 years after Cannae the war surged around southern Italy as cities went over to the Carthaginians or were taken by subterfuge, and the Romans recaptured them by siege or by suborning pro-Roman factions. Hannibal repeatedly defeated Roman armies, but wherever his main army was not active the Romans threatened Carthaginian-supporting towns or sought battle with Carthaginian or Carthaginian-allied detachments; frequently with success. By 207BC Hannibal had been confined to the extreme south of Italy and many of the cities and territories which had joined the Carthaginian cause had returned to their Roman allegiance.

During 216BC the Macedonian king, Philip V, pledged his support to Hannibal – thus initiating the First Macedonian War against Rome in 215BC. In 211BC, Rome contained the threat of Macedonia by allying with the Aetolian League, an anti-Macedonian coalition of Greek city states. In 205BC this war ended with a negotiated peace.

A rebellion in support of the Carthaginians broke out on Sardinia in 213BC, but it was quickly put down by the Romans.

Sicily remained firmly in Roman hands, blocking the ready seaborne reinforcement and resupply of Hannibal from Carthage. Hiero II, the old tyrant of Syracuse and a staunch Roman ally, died in 215BC and his successor Hieronymus was discontented with his situation. Hannibal negotiated a treaty whereby Syracuse came over to Carthage, at the price of making the whole of Sicily a Syracusan possession. The Syracusan army proved no match for the Romans, and by spring 213BC Syracuse was besieged. The siege was marked by the ingenuity of Archimedes in inventing war machines to counteract the traditional siege warfare methods of the Romans.

A large Carthaginian army led by Himilco was sent to relieve the city in 213BC. It captured several Roman-garrisoned towns on Sicily; many Roman garrisons were either expelled or massacred by Carthaginian partisans. In the spring of 212BC the Romans stormed Syracuse in a surprise night assault and captured several districts of the city. Meanwhile, the Carthaginian army was crippled by plague. After the Carthaginians failed to resupply the city, Syracuse fell in the autumn of 212BC; Archimedes was killed by a Roman soldier.

Carthage sent more reinforcements to Sicily in 211BC and went on the offensive. A fresh Roman army attacked the main Carthaginian stronghold on the island, Agrigentum, in 210BC and the city was betrayed to the Romans by a discontented Carthaginian officer. The remaining Carthaginian-controlled towns then surrendered or were taken through force or treachery and the Sicilian grain supply to Rome and its armies was resumed.

In the spring of 207 BC, Hasdrubal Barca marched across the Alps and invaded Italy with an army of 30,000 men. His aim was to join his forces with those of Hannibal, but Hannibal was unaware of his presence. The Romans facing Hannibal in southern Italy tricked him into believing the whole Roman army was still in camp, while a large portion marched north and reinforced the Romans facing Hasdrubal. The combined Roman force attacked Hasdrubal at the Battle of the Metaurus and destroyed his army, killing Hasdrubal. This battle confirmed Roman dominance in Italy.

In 205BC, Mago landed in Genua in north-west Italy with the remnants of his Spanish army ("see below"). It soon received Gallic and Ligurian reinforcements. Mago's arrival in the north of the Italian peninsula was followed by Hannibal's inconclusive Battle of Crotona in 204BC in the far south of the peninsula. Mago marched his reinforced army towards the lands of Carthage's main Gallic allies in the Po Valley, but was checked by a large Roman army and defeated at the Battle of Insubria in 203BC.

After Publius Cornelius Scipio invaded the Carthaginian homeland in 204BC, defeating the Carthaginians in two major battles and winning the allegiance of the Numidian kingdoms of North Africa, Hannibal and the remnants of his army were recalled. They sailed from Croton and landed at Carthage with 15,000–20,000 experienced veterans. Mago was also recalled; he died of wounds on the voyage and some of his ships were intercepted by the Romans, but 12,000 of his troops reached Carthage.

The Roman fleet continued on from Massala in the autumn of 218BC, landing the army it was transporting in north-east Iberia, where it won support among the local tribes. A rushed Carthaginian attack in late 218BC was beaten off at the Battle of Cissa. In 217BC 40 Carthaginian and Iberian warships were beaten by 55 Roman and Massalian vessels at the Battle of Ebro River, with 29 Carthaginian ships lost. The Romans' lodgement between the Ebro and Pyrenees blocked the route from Iberia to Italy and prevented the despatch of reinforcements from Iberia to Hannibal. The Carthaginian commander in Iberia, Hannibal's brother Hasdrubal, marched into this area in 215BC, offered battle and was defeated at Dertosa, although both sides suffered heavy casualties.

The Carthaginians suffered a wave of defections of local Celtiberian tribes to Rome. The Roman commanders captured Saguntum in 212BC and in 211BC hired 20,000 Celtiberian mercenaries to reinforce their army. Observing that the three Carthaginian armies were deployed apart from each other, the Scipios split their forces. This strategy resulted in the Battle of Castulo and the Battle of Ilorca, usually combined as the Battle of the Upper Baetis. Both battles ended in complete defeat for the Romans, as Hasdrubal had bribed the Romans' mercenaries to desert. The Romans retreated to their coastal stronghold north of the Ebro, from which the Carthaginians again failed to expel them. Claudius Nero brought over reinforcements in 210BC and stabilised the situation.
In 210 BC Publius Cornelius Scipio, arrived in Iberia with further reinforcements. In a carefully planned assault in 209BC, he captured the lightly-defended centre of Carthaginian power in Iberia, Cartago Nova, seizing a vast booty of gold, silver and siege artillery. He released the captured population and liberated the Iberian hostages held there by the Carthaginians to ensure the loyalty of their tribes, although many of them were subsequently to fight against the Romans.

In the spring of 208BC, Hasdrubal moved to engage Scipio at the Battle of Baecula. The Carthaginians were defeated, but Hasdrubal was able to withdraw the majority of his army in good order. Most of his losses were among his Iberian allies. Scipio was not able to prevent Hasdrubal from leading his depleted army over the western passes of the Pyrenees into Gaul. In 297BC, after recruiting heavily in Gaul, Hasdrubal crossed the Alps into Italy in an attempt to join his brother, Hannibal.

In 206 BC, at the Battle of Ilipa, Scipio with 48,000 men, half Italian and half Iberian, defeated a Carthaginian army of 54,500 men and 32 elephants. This sealed the fate of the Carthaginians in Iberia. It was followed by the Roman capture of Gades after the city rebelled against Carthaginian rule.

Later the same year a mutiny broke out among Roman troops, which initially attracted support from Iberian leaders, disappointed that Roman forces had remained in the peninsula after the expulsion of the Carthaginians, but it was effectively put down by Scipio. In 205BC a last attempt was made by Mago to recapture New Carthage when the Roman occupiers were shaken by another mutiny and an Iberian uprising, but he was repulsed. Mago left Iberia for northern Italy with his remaining forces. In 203BC Carthage succeeded in recruiting at least 4,000 mercenaries from Iberia, despite Rome's nominal control.

In 213BC Syphax, a powerful Numidian king in North Africa, declared for Rome. In response, Roman advisers were sent to train his soldiers and he waged war against the Carthaginian ally Gala. In 206BC the Carthaginians ended this drain on their resources by dividing several Numidian kingdoms with him. One of those disinherited was the Numidian prince Masinissa, who was thus driven into the arms of Rome.

In 205BC Scipio was given command of the legions in Sicily and allowed to enrol volunteers for his plan to end the war by an invasion of Africa. After landing in Africa in 204BC, he was joined by Masinissa and a force of Numidian cavalry. Scipio gave battle to and destroyed two large Carthaginian armies. After the second of these Syphax was pursued and taken prisoner at the Battle of Cirta and Masinissa seized most of his kingdom with Roman help.

Rome and Carthage entered into peace negotiations, and Carthage recalled Hannibal from Italy. The Roman Senate ratified a draft treaty, but due to mistrust and a surge in confidence when Hannibal arrived from Italy Carthage repudiated it. Hannibal was placed in command of another army, formed from his veterans from Italy and newly raised troops from Africa, but with few cavalry. The decisive Battle of Zama followed in October 202BC. Unlike most battles of the Second Punic War, the Romans had superiority in cavalry and the Carthaginians in infantry. Hannibal attempted to use 80 elephants to break into the Roman infantry formation, but the Romans countered them effectively and they routed back through the Carthaginian ranks. The Roman and allied Numidian cavalry drove the Carthaginian cavalry from the field. The two sides' infantry fought inconclusively until the Roman cavalry returned and attacked his rear. The Carthaginian formation collapsed; Hannibal was one of the few to escape the field.

The peace treaty imposed on the Carthaginians stripped them of all of their overseas territories, and some of their African ones. An indemnity of 10,000 silver talents was to be paid over 50 years. Hostages were taken. Carthage was forbidden to possess war elephants and its fleet was restricted to 10 warships. It was prohibited from waging war outside Africa, and in Africa only with Rome's express permission. Many senior Carthaginians wanted to reject it, but Hannibal spoke strongly in its favour and it was accepted in spring 201BC. Henceforth it was clear that Carthage was politically subordinate to Rome. Scipio was awarded a triumph and received the "agnomen" "Africanus".

At the end of the war, Masinissa emerged as by far the most powerful ruler among the Numidians. Over the following 48 years he repeatedly took advantage of Carthage's inability to protect its possessions. Whenever Carthage petitioned Rome for redress, or permission to take military action, Rome backed its ally, Masinissa, and refused. Masinissa's seizures of and raids into Carthaginian territory became increasingly flagrant. In 151BC Carthage raised a large army, the treaty notwithstanding, and counter attacked the Numidians. The campaign ended in disaster and the army surrendered. Carthage had paid off its indemnity and was prospering economically, but was no military threat to Rome. Elements in the Roman Senate had long wished to destroy Carthage, and with the breach of the treaty as a "casus belli", war was declared in 149BC.

In 149BC a Roman army of approximately 50,000 men, jointly commanded by both consuls, landed near Utica, north of Carthage. Rome demanded that if war were to be avoided, the Carthaginians must hand over all of their armaments. Vast amounts of materiel delivered, including 200,000 sets of armour, 2,000 catapults and a large number of warships. This done, the Romans demanded the Carthaginians burn their city and relocate at least from the sea; the Carthaginians broke off negotiations and set to recreating their armoury.

As well as manning the walls of Carthage, the Carthaginians formed a field army under Hasdrubal, which was based to the south. The Roman army moved to lay siege to Carthage, but its walls were so strong and its citizen-militia so determined it was unable to make any impact, while the Carthaginians struck back effectively. Their army raided the Roman lines of communication, and in 148BC Carthaginian fire ships destroyed many Roman vessels. The main Roman camp was in a swamp, which caused an outbreak of disease during the summer. The Romans moved their camp, and their ships, further awayso they were now more blockading than closely besieging the city. The war dragged on into 147BC.

In early 147BC Scipio Aemilianus, an adopted grandson of Scipio Africanus who had distinguished himself during the previous two years' fighting, was elected consul and took control of the war. The Carthaginians continued to resist vigorously: they constructed warships and during the summer twice gave battle to the Roman fleet, losing both times. The Romans launched an assault on the walls; after confused fighting they broke into the city, but lost in the dark, withdrew. Hasdrubal and his army retreated into the city to reinforce the garrison. Hasdrubal had Roman prisoners tortured to death on the walls, in view of the Roman army. He was reinforcing the will to resist in the Carthaginian citizens; from this point there could be no possibility of negotiations. Some members of the city council denounced his actions and Hasdrubal had them too put to death and took control of the city. With no Carthaginian army in the field those cities which had remained loyal went over to the Romans or were captured.

Scipio moved back to a close blockade of the city, and built a mole which cut off supply from the sea. In the spring of 146BC the Roman army managed to secure a foothold on the fortifications near the harbour. When the main assault began it quickly captured the city's main square, where the legions camped overnight. The next morning the Romans systematically worked their way through the residential part of the city, killing everyone they encountered and firing the buildings behind them. At times the Romans progressed from rooftop to rooftop, to prevent missiles being hurled down on them. It took six days to clear the city of resistance, and on the last day Scipio agreed to accept prisoners. The last holdouts, including Roman deserters in Carthaginian service, fought on from the Temple of Eshmoun and burnt it down around themselves when all hope was gone. There were 50,000 Carthaginian prisoners, a small proportion of the pre-war population, who were sold into slavery. The notion that Roman forces then sowed the city with salt is a 19th-century invention.

The remaining Carthaginian territories were annexed by Rome and reconstituted to become the Roman province of Africa with Utica as its capital. The province became a major source of grain and other foodstuffs. Numerous large Punic cities, such as those in Mauretania, were taken over by the Romans, although they were permitted to retain their Punic system of government. A century later, the site of Carthage was rebuilt as a Roman city by Julius Caesar, and would become one of the main cities of Roman Africa by the time of the Empire. Rome still exists as the capital of Italy; the ruins of Carthage lie east of modern Tunis on the North African coast.


</doc>
<doc id="24419" url="https://en.wikipedia.org/wiki?curid=24419" title="Peter Carey (novelist)">
Peter Carey (novelist)

Peter Philip Carey AO (born 7 May 1943) is an Australian novelist. Carey has won the Miles Franklin Award three times and is frequently named as Australia's next contender for the Nobel Prize in Literature. Carey is one of only five writers to have won the Booker Prize twice—the others being J. G. Farrell, J. M. Coetzee, Hilary Mantel and Margaret Atwood. Carey won his first Booker Prize in 1988 for "Oscar and Lucinda", and won for the second time in 2001 with "True History of the Kelly Gang". In May 2008 he was nominated for the Best of the Booker Prize.

In addition to writing fiction, he collaborated on the screenplay of the film "Until the End of the World" with Wim Wenders and is executive director of the Master of Fine Arts in Creative Writing program at Hunter College, part of the City University of New York.

Peter Carey was born in Bacchus Marsh, Victoria, in 1943. His parents ran a General Motors dealership, Carey Motors. He attended Bacchus Marsh State School from 1948 to 1953, then boarded at Geelong Grammar School between 1954 and 1960. In 1961, Carey enrolled in a science degree at the new Monash University in Melbourne, majoring in chemistry and zoology, but cut his studies short because of a car accident and a lack of interest. It was at university that he met his first wife, Leigh Weetman, who was studying German and philosophy, and who also dropped out.

In 1962, he began to work in advertising. He was employed by various Melbourne agencies between 1962 and 1967, including on campaigns for Volkswagen and Lindeman's Wine. His advertising work brought him into contact with older writers who introduced him to recent European and American fiction: "I didn't really start getting an education until I worked in advertising with people like Barry Oakley and Morris Lurie—and Bruce Petty had an office next door."

During this time, he read widely, particularly the works of Samuel Beckett, William Faulkner, James Joyce, Franz Kafka, and Gabriel García Márquez, and began writing on his own, receiving his first rejection slip in 1964, the same year he married Weetman. Over the next few years he wrote five novels—"Contacts" (1964–1965), "Starts Here, Ends Here" (1965–1967), "The Futility Machine" (1966–1967), "Wog" (1969), and "Adventures on Board the Marie" [sic] "Celeste" (1971). None of them were published. Sun Books accepted "The Futility Machine" but did not proceed with publication, and "Adventures on Board the Marie Celeste" was accepted by Outback Press before being withdrawn by Carey himself. These and other unpublished manuscripts from the period—including twenty-one short stories—are now held by the Fryer Library at the University of Queensland.

Carey's only publications during the 1960s were "Contacts" (a short extract from the unpublished novel of the same name, in "Under Twenty-Five: An Anthology", 1966) and "She Wakes" (a short story, in "Australian Letters", 1967). Towards the end of the decade, Carey and Weetman abandoned Australia with "a certain degree of self-hatred", travelling through Europe and Iran before settling in London in 1968, where Carey continued to write highly regarded advertising copy and unpublished fiction.

Returning to Australia in 1970, Carey once again did advertising work in Melbourne and Sydney. He also kept writing, and gradually broke through with editors, publishing short stories in magazines and newspapers such as "Meanjin" and "Nation Review". Most of these were collected in his first book, "The Fat Man in History", which appeared in 1974. In the same year Carey moved to Balmain in Sydney to work for Grey Advertising.

In 1976, Carey moved to Queensland and joined an alternative community named Starlight in Yandina, north of Brisbane, with his new partner, the painter Margot Hutcheson, with whom he lived in the 1970s and 1980s. He remained with Grey, writing in Yandina for three weeks, then spending the fourth week at the agency in Sydney. It was during this time that he produced most of the stories collected in "War Crimes" (1979), as well as "Bliss" (1981), his first published novel.

Carey started his own advertising agency in 1980, the Sydney-based McSpedden Carey Advertising Consultants, in partnership with Bani McSpedden. After many years of separation, Leigh Weetman asked for a divorce in 1980 so that she could remarry and Peter agreed. In 1981, he moved to Bellingen in northern New South Wales. There he wrote "Illywhacker", published in 1985. In the same year he married theatre director Alison Summers. "Illusion", a stage musical Carey wrote with Mike Mullins and composer Martin Armiger, was performed at the 1986 Adelaide Festival of the Arts and a studio cast recording of the musical was nominated for a 1987 ARIA Award (for which Carey as lyricist was nominated).

The decade—and the Australian phase of Carey's career—culminated with the publication of "Oscar and Lucinda" (1988), which won the Booker McConnell Prize (as it was then known) and brought the author international recognition. Carey explained that the novel was inspired, in part, by his time in Bellingen:

Carey sold his share of McSpedden Carey and in 1990 moved with Alison Summers and their son to New York, where he took a job teaching creative writing at New York University. He later said that New York would not have been his first choice of place to live, and that moving there was his wife's idea. Carey and Summers divorced in 2005 after a four-year separation. Carey is now married to the British-born publisher Frances Coady.

"The Tax Inspector" (1991), begun in Australia, was the first book he completed in the United States. It was followed by "The Unusual Life of Tristan Smith" (1994), a fable in which he explored the relationship between Australia and America, disguised in the novel as "Efica" and "Voorstand". This is a relationship that has preoccupied him throughout his career, going back to "Bliss" (1981), "Illywhacker" (1985), and the early short stories. Nevertheless, Carey continued to set his fiction primarily in Australia and remained diffident about writing explicitly on American themes. In a piece on "True History of the Kelly Gang" (2001), Mel Gussow reported that:

It was only after nearly two decades in the United States that he embarked on "Parrot and Olivier in America" (2010), loosely based on events in the life of Alexis de Tocqueville. Carey says "Tocqueville opened a door I could enter. I saw the present in the past. It was accessible, imaginable." Carey continues to extend his canvas; in his novel, "The Chemistry of Tears" (2012), "contemporary London is brought intimately in touch with ... a 19th-century Germany redolent of the Brothers Grimm".

In 1998, Carey was accused of snubbing Queen Elizabeth II by declining an invitation to meet her after winning the Commonwealth Writers Prize for "Jack Maggs" (1997). While Carey is a republican, in the Australian sense, he insists that no offence was intended:

The meeting did eventually take place, with the Queen remarking, according to Carey, "I believe you had a little trouble getting here."

The unhappy circumstances of Carey's break-up with Alison Summers received publicity (largely in Australia) in 2006 when "" appeared, depicting the toxic relationship between its protagonist, Butcher Bones, and his ex-wife, known only as "the Plaintiff".

In April 2015 he, alongside Michael Ondaatje, Francine Prose, Teju Cole, Rachel Kushner and Taiye Selasi, withdrew from the PEN American Center gala honouring the French satirical magazine "Charlie Hebdo" with its "Freedom of Expression Courage" award. He stated that one of his reasons for doing so was "PEN’s seeming blindness to the cultural arrogance of the French nation, which does not recognise its moral obligation to a large and disempowered segment of their population.". In addition, 204 PEN members, including Teju Cole and Deborah Eisenberg, wrote to PEN, objecting to its decision to give the award to Charlie Hebdo.

Carey has been awarded three honorary degrees. He has been elected a Fellow of the Royal Society of Literature (1989), an Honorary Fellow of the Australian Academy of the Humanities (2001), a Member of the American Academy of Arts and Sciences (2003), and a Member of the American Academy of Arts and Letters (2016), which has also awarded him its Harold D Vursell Memorial Award (2012). In 2010, he appeared on two Australian postage stamps in a series dedicated to "Australian Legends". On 11 June 2012, Carey was named an Officer of the Order of Australia for "distinguished service to literature as a novelist, through international promotion of the Australian identity, as a mentor to emerging writers." And in 2014, Carey was awarded an honorary Doctor of Letters (honoris causa) by Sydney University.

Carey has won numerous literary awards, including:


Stories from Carey's first two collections have been repackaged in "The Fat Man in History and Other Stories" (1980), "Exotic Pleasures" (1990), and "Collected Stories" (1994); the last also includes three previously uncollected stories: "Joe" ("Australian New Writing", 1973), "A Million Dollars Worth of Amphetamines" ("Nation Review", 1975), and "Concerning the Greek Tyrant" ("The Tabloid Story Pocket Book", 1978).










</doc>
<doc id="24420" url="https://en.wikipedia.org/wiki?curid=24420" title="Punched card">
Punched card

A punched card or punch card is a piece of stiff paper that can be used to contain digital data represented by the presence or absence of holes in predefined positions. Digital data can be used for data processing applications or used to directly control automated machinery.

Punched cards were widely used through much of the 20th century in the data processing industry, where specialized and increasingly complex unit record machines, organized into semiautomatic data processing systems, used punched cards for data input, output, and storage. The IBM 12-row/80-column punched card format came to dominate the industry. Many early digital computers used punched cards as the primary medium for input of both computer programs and data.

While punched cards are now obsolete as a storage medium, as of 2012, some voting machines still use punched cards to record votes.

The idea of control and data storage via punched holes was developed over a long period of time. In most cases there is no evidence that each of the inventors was aware of the earlier work.

Basile Bouchon developed the control of a loom by punched holes in paper tape in 1725. The design was improved by his assistant Jean-Baptiste Falcon and by Jacques Vaucanson. Although these improvements controlled the patterns woven, they still required an assistant to operate the mechanism. 

In 1804 Joseph Marie Jacquard demonstrated a mechanism to automate loom operation. A number of punched cards were linked into a chain of any length. Each card held the instructions for shedding (raising and lowering the warp) and selecting the shuttle for a single pass.
Semyon Korsakov was reputedly the first to propose punched cards in informatics for information store and search. Korsakov announced his new method and machines in September 1832.

Charles Babbage proposed the use of "Number Cards", "pierced with certain holes and stand[ing] opposite levers connected with a set of figure wheels ... advanced they push in those levers opposite to which there are no holes on the cards and thus transfer that number together with its sign" in his description of the Calculating Engine's Store. There is no evidence that he built a practical example.

In 1881 Jules Carpentier developed a method of recording and playing back performances on a harmonium using punched cards. The system was called the "Mélographe Répétiteur" and “writes down ordinary music played on the keyboard dans la langage de Jacquard”, that is as holes punched in a series of cards. By 1887 Carpentier had separated the mechanism into the "Melograph" which recorded the player's key presses and the "Melotrope" which played the music.

At the end of the 1800s Herman Hollerith invented the recording of data on a medium that could then be read by a machine. claim 2 of which reads: "After some initial trials with paper tape, he settled on punched cards...", developing punched card data processing technology for the 1890 U.S. census. His tabulating machines read and summarized data stored on punched cards and they began use for government and commercial data processing. 

Initially, these electromechanical machines only counted holes, but by the 1920s they had units for carrying out basic arithmetic operations.
Hollerith founded the "Tabulating Machine Company" (1896) which was one of four companies that were amalgamated via stock acquisition to form a fifth company, Computing-Tabulating-Recording Company (CTR) (1911), later renamed International Business Machines Corporation (IBM) (1924). Other companies entering the punched card business included The Tabulator Limited (1902), Deutsche Hollerith-Maschinen Gesellschaft mbH (Dehomag) (1911), Powers Accounting Machine Company (1911), Remington Rand (1927), and H.W. Egli Bull (1931). These companies, and others, manufactured and marketed a variety of punched cards and unit record machines for creating, sorting, and tabulating punched cards, even after the development of electronic computers in the 1950s.

Both IBM and Remington Rand tied punched card purchases to machine leases, a violation of the 1914 Clayton Antitrust Act. In 1932, the US government took both to court on this issue. Remington Rand settled quickly. IBM viewed its business as providing a service and that the cards were part of the machine. IBM fought all the way to the Supreme Court and lost in 1936; the court ruled that IBM could only set card specifications.

"By 1937... IBM had 32 presses at work in Endicott, N.Y., printing, cutting and stacking five to 10 million punched cards every day." Punched cards were even used as legal documents, such as U.S. Government checks and savings bonds.

During World War II punched card equipment was used by the Allies in some of their efforts to decrypt Axis communications. See, for example, Central Bureau in Australia. At Bletchley Park in England, "some 2 million punched cards a week were being produced, indicating the sheer scale of this part of the operation".

Punched card technology developed into a powerful tool for business data-processing. By 1950 punched cards had become ubiquitous in industry and government. "Do not fold, spindle or mutilate," a warning that appeared on some punched cards distributed as documents such as checks and utility bills to be returned for processing, became a motto for the post-World War II era.

In 1955 IBM signed a consent decree requiring, amongst other things, that IBM would by 1962 have no more than one-half of the punched card manufacturing capacity in the United States. Tom Watson Jr.'s decision to sign this decree, where IBM saw the punched card provisions as the most significant point, completed the transfer of power to him from Thomas Watson, Sr.

The UNITYPER introduced magnetic tape for data entry in the 1950s. During the 1960s, the punched card was gradually replaced as the primary means for data storage by magnetic tape, as better, more capable computers became available. Mohawk Data Sciences introduced a magnetic tape encoder in 1965, a system marketed as a keypunch replacement which was somewhat successful. Punched cards were still commonly used for entering both data and computer programs until the mid-1980s when the combination of lower cost magnetic disk storage, and affordable interactive terminals on less expensive minicomputers made punched cards obsolete for these roles as well. However, their influence lives on through many standard conventions and file formats. The terminals that replaced the punched cards, the IBM 3270 for example, displayed 80 columns of text in text mode, for compatibility with existing software. Some programs still operate on the convention of 80 text columns, although fewer and fewer do as newer systems employ graphical user interfaces with variable-width type fonts.

The terms "punched card", "punch card", and "punchcard" were all commonly used, as were "IBM card" and "Hollerith card" (after Herman Hollerith). IBM used "IBM card" or, later, "punched card" at first mention in its documentation and thereafter simply "card" or "cards". Specific formats were often indicated by the number of character positions available, e.g. "80-column card". A sequence of cards that is input to or output from some step in an application's processing is called a "card deck" or simply "deck". The rectangular, round, or oval bits of paper punched out were called chad ("chads") or "chips" (in IBM usage). Sequential card columns allocated for a specific use, such as names, addresses, multi-digit numbers, etc., are known as a "field". The first card of a group of cards, containing fixed or indicative information for that group, is known as a "master card". Cards that are not master cards are "detail cards".

The Hollerith punched cards used for the 1890 U.S. census were blank. Following that, cards commonly had printing such that the row and column position of a hole could be easily seen. Printing could include having fields named and marked by vertical lines, logos, and more. "General purpose" layouts (see, for example, the IBM 5081 below) were also available. For applications requiring master cards to be separated from following detail cards, the respective cards had different upper corner diagonal cuts and thus could be separated by a sorter. Other cards typically had one upper corner diagonal cut so that cards not oriented correctly, or cards with different corner cuts, could be identified.

Herman Hollerith was awarded three patents in 1889 for electromechanical tabulating machines. These patents described both paper tape and rectangular cards as possible recording media. The card shown in of January 8 was printed with a template and had hole positions arranged close to the edges so they could be reached by a railroad conductor's ticket punch, with the center reserved for written descriptions. Hollerith was originally inspired by railroad tickets that let the conductor encode a rough description of the passenger:

When use of the ticket punch proved tiring and error prone Hollerith developed the pantograph "keyboard punch". It featured an enlarged diagram of the card, indicating the positions of the holes to be punched. A printed reading board could be placed under a card that was to be read manually.

Hollerith envisioned a number of card sizes. In an article he wrote describing his proposed system for tabulating the 1890 U.S. census, Hollerith suggested a card 3 inches by 5½ inches of Manila stock "would be sufficient to answer all ordinary purposes." The cards used in the 1890 census had round holes, 12 rows and 24 columns. A reading board for these cards can be seen at the Columbia University Computing History site. At some point, became the standard card size. These are the dimensions of the then current paper currency of 1862–1923.

Hollerith's original system used an ad-hoc coding system for each application, with groups of holes assigned specific meanings, e.g. sex or marital status. His tabulating machine had up to 40 counters, each with a dial divided into 100 divisions, with two indicator hands; one which stepped one unit with each counting pulse, the other which advanced one unit every time the other dial made a complete revolution. This arrangement allowed a count up to 9,999. During a given tabulating run counters were assigned specific holes or, using relay logic, combination of holes.

Later designs led to a card with ten rows, each row assigned a digit value, 0 through 9, and 45 columns. 
This card provided for fields to record multi-digit numbers that tabulators could sum, instead of their simply counting cards. Hollerith's 45 column punched cards are illustrated in Comrie's "The application of the Hollerith Tabulating Machine to Brown's Tables of the Moon".

By the late 1920s customers wanted to store more data on each punched card. Thomas J. Watson Sr., IBM’s head, asked two of his top inventors, Clair D. Lake and J. Royden Pierce, to independently develop ways to increase data capacity without increasing the size of the punched card. Pierce wanted to keep round holes and 45 columns, but allow each column to store more data. Lake suggested rectangular holes, which could be spaced more tightly, allowing 80 columns per punched card, thereby nearly doubling the capacity of the older format. Watson picked the latter solution, introduced as "The IBM Card", in part because it was compatible with existing tabulator designs and in part because it could be protected by patents and give the company a distinctive advantage.

This IBM card format, introduced in 1928,
has rectangular holes, 80 columns, and 12 rows. Card size is exactly by inches (187.325 mm × 82.55 mm). The cards are made of smooth stock, thick. There are about 143 cards to the inch (143/2.54round0/cm). In 1964, IBM changed from square to round corners. They come typically in boxes of 2000 cards or as continuous form cards. Continuous form cards could be both pre-numbered and pre-punched for document control (checks, for example).

Initially designed to record responses to Yes–no questions, support for numeric, alphabetic and special characters was added through the use of columns and zones. The top three positions of a column are called zone punching positions, 12 (top), 11, and 0 (0 may be either a zone punch or a digit punch). For decimal data the lower ten positions are called digit punching positions, 0 (top) through 9. An arithmetic sign can be specified for a decimal field by overpunching the field's rightmost column with a zone punch: 12 for plus, 11 for minus (CR). For Pound sterling pre-decimalization currency a penny column represents the values zero through eleven; 10 (top), 11, then 0 through 9 as above. An arithmetic sign can be punched in the adjacent shilling column. Zone punches had other uses in processing, such as indicating a master card.
　Reference: Note: The 11 and 12 zones were also called the X and Y zones, respectively.

In 1931 IBM began introducing upper-case letters and special characters (Powers-Samas had developed the first commercial alphabetic punched card representation in 1921). The 26 letters have two punches (zone [12,11,0] + digit [1–9]). The languages of Germany, Sweden, Denmark, Norway, Spain, Portugal and Finland require up to three additional letters; their punching is not shown here. Most special characters have two or three punches (zone [12,11,0, or none] + digit [2–7] + 8); a few special characters were exceptions: "&" is 12 only, "-" is 11 only, and "/" is 0 + 1). The Space character has no punches. The information represented in a column by a combination of zones [12, 11, 0] and digits [0–9] is dependent on the use of that column. For example, the combination "12-1" is the letter "A" in an alphabetic column, a plus signed digit "1" in a signed numeric column, or an unsigned digit "1" in a column where the "12" has some other use. The introduction of EBCDIC in 1964 defined columns with as many as six punches (zones [12,11,0,8,9] + digit [1–7]). IBM and other manufacturers used many different 80-column card character encodings. A 1969 American National Standard defined the punches for 128 characters and was named the "Hollerith Punched Card Code" (often referred to simply as "Hollerith Card Code"), honoring Hollerith.
For some computer applications, binary formats were used, where each hole represented a single binary digit (or "bit"), every column (or row) is treated as a simple bit field, and every combination of holes is permitted.

For example, on the IBM 701 and IBM 704, card data was read, using an IBM 711, into memory in row binary format. For each of the twelve rows of the card, 72 of the 80 columns would be read into two 36-bit words; a control panel was used to select the 72 columns to be read. Software would translate this data into the desired form. One convention was to use columns 1 through 72 for data, and columns 73 through 80 to sequentially number the cards, as shown in the picture above of a punched card for FORTRAN. Such numbered cards could be sorted by machine so that if a deck was dropped the sorting machine could be used to arrange it back in order. This convention continued to be used in FORTRAN, even in later systems where the data in all 80 columns could be read.
As a prank punched cards could be made where every possible punch position had a hole. Such "lace cards" lacked structural strength, and would frequently buckle and jam inside the machine.

The IBM 80-column punched card format dominated the industry, becoming known as just IBM cards, even though other companies made cards and equipment to process them.
One of the most common punched card formats is the IBM 5081 card format, a general purpose layout with no field divisions. This format has digits printed on it corresponding to the punch positions of the digits in each of the 80 columns. Other punched card vendors manufactured cards with this same layout and number.

Long cards were available with a scored stub on either end which, when torn off, left an 80 column card. The torn off card is called a "stub card".

80-column cards were available scored, on either end, creating both a "short card" and a "stub card" when torn apart. Short cards can be processed by other IBM machines. A common length for stub cards was 51 columns. Stub cards were used in applications requiring tags, labels, or carbon copies.

According to the IBM Archive: "IBM's Supplies Division introduced the Port-A-Punch in 1958 as a fast, accurate means of manually punching holes in specially scored IBM punched cards. Designed to fit in the pocket, Port-A-Punch made it possible to create punched card documents anywhere. The product was intended for "on-the-spot" recording operations—such as physical inventories, job tickets and statistical surveys—because it eliminated the need for preliminary writing or typing of source documents."

In 1969 IBM introduced a new, smaller, round-hole, 96-column card format along with the IBM System/3 low-end business computer. These cards have tiny (1 mm), circular holes, smaller than those in paper tape. Data is stored in 6-bit BCD, with three rows of 32 characters each, or 8-bit EBCDIC. In this format, each column of the top tiers are combined with two punch rows from the bottom tier to form an 8-bit byte, and the middle tier is combined with two more punch rows, so that each card contains 64 bytes of 8-bit-per-byte binary coded data.
This format was never very widely used; It was IBM-only, but they did not support it on any equipment beyond the System/3, where it was quickly superseded by the 1973 IBM 3740 Data Entry System using 8-inch floppy disks.

The Powers/Remington Rand card format was initially the same as Hollerith's; 45 columns and round holes. In 1930, Remington Rand leap-frogged IBM's 80 column format from 1928 by coding two characters in each of the 45 columns – producing what is now commonly called the 90-column card. There are two sets of six rows across each card. The rows in each set are labeled 0, 1/2, 3/4, 5/6, 7/8 and 9. The even numbers in a pair are formed by combining that punch with a 9 punch. Alphabetic and special characters use 3 or more punches.

The British Powers-Samas company used a variety of card formats for their unit record equipment. They began with 45 columns and round holes. Later 36, 40 and 65 column cards were provided. A 130 column card was also available - formed by dividing the card into two rows, each row with 65 columns and each character space with 5 punch positions. A 21 column card was comparable to the IBM Stub card.



IBM's Fred M. Carroll developed a series of rotary presses that were used to produce punched cards, including a 1921 model that operated at 460 cards per minute (cpm). In 1936 he introduced a completely different press that operated at 850 cpm. Carroll's high-speed press, containing a printing cylinder, revolutionized the company's manufacturing of punched cards. It is estimated that between 1930 and 1950, the Carroll press accounted for as much as 25 percent of the company's profits.

Discarded printing plates from these card presses, each printing plate the size of an IBM card and formed into a cylinder, often found use as desk pen/pencil holders, and even today are collectible IBM artifacts (every card layout had its own printing plate).

In the mid-1930s a box of 1,000 cards cost $1.05.

While punched cards have not been widely used for a generation, the impact was so great for most of the 20th century that they still appear from time to time in popular culture. For example:

metaphor... symbol of the "system"—first the registration system and then bureaucratic systems more generally ... a symbol of alienation ... Punched cards were the symbol of information machines, and so they became the symbolic point of attack. Punched cards, used for class registration, were first and foremost a symbol of uniformity. ... A student might feel "he is one of out of 27,500 IBM cards" ... The president of the Undergraduate Association criticized the University as "a machine ... IBM pattern of education."... Robert Blaumer explicated the symbolism: he referred to the "sense of impersonality... symbolized by the IBM technology."...
––Steven Lubar


A common example of the requests often printed on punched cards which were to be individually handled, especially those intended for the public to use and return is "Do Not Fold, Spindle or Mutilate" (in the UK - "Do not bend, spike, fold or mutilate"). Coined by Charles A. Phillips, it became a motto for the post-World War II era (even though many people had no idea what spindle meant), and was widely mocked and satirized. Some 1960s students at Berkeley wore buttons saying: "Do not fold, spindle or mutilate. I am a student". The motto was also used for a 1970 book by Doris Miles Disney with a plot based around an early computer dating service and a 1971 made-for-TV movie based on that book, and a similarly titled 1967 Canadian short film, "Do Not Fold, Staple, Spindle or Mutilate".


Processing of punched cards was handled by a variety of machines, including:






</doc>
<doc id="24421" url="https://en.wikipedia.org/wiki?curid=24421" title="Profiler">
Profiler

Profiler or profiling may refer to:



</doc>
<doc id="24422" url="https://en.wikipedia.org/wiki?curid=24422" title="Pasteur (disambiguation)">
Pasteur (disambiguation)

Louis Pasteur (1822–1895) was a French chemist and microbiologist.

Pasteur may also refer to:







</doc>
<doc id="24423" url="https://en.wikipedia.org/wiki?curid=24423" title="Pope Innocent I">
Pope Innocent I

Pope Innocent I () was the bishop of Rome from 401 to his death on 12 March 417. He may have been the son of his predecessor, Anastasius I. From the beginning of his papacy, he was seen as the general arbitrator of ecclesiastical disputes in both the East and the West. He confirmed the prerogatives of the Archbishop of Thessalonica, and issued a decretal on disciplinary matters referred to him by the Bishop of Rouen. He defended the exiled John Chrysostom and consulted with the bishops of Africa concerning the Pelagian controversy, confirming the decisions of the African synods. The Catholic priest-scholar Johann Peter Kirsch, 1500 years later, described Innocent as a very energetic and highly gifted individual "...who fulfilled admirably the duties of his office".

According to his biographer in the "Liber Pontificalis", Innocent was a native of Albano Laziale and the son of a man called Innocentius, but his contemporary Jerome referred to him as the son of the previous pope, Anastasius I, probably a unique case of a son succeeding his father in the papacy. According to Urbano Cerri, Pope Innocent was a native of Albania.
Innocent I lost no opportunity in maintaining and extending the authority of the Roman apostolic See, which was seen as the ultimate resort for the settlement of all ecclesiastical disputes. His communications with Victricius of Rouen, Exuperius of Toulouse, Alexander of Antioch and others, as well as his actions on the appeal made to him by John Chrysostom against Theophilus of Alexandria, show that opportunities of this kind were numerous and varied. He took a decided view on the Pelagian controversy, confirming the decisions of the synod of the province of proconsular Africa, held in Carthage in 416, confirming the condemnation which had been pronounced in 411 against Cælestius, who shared the views of Pelagius. He also wrote in the same year in a similar sense to the fathers of the Numidian synod of Mileve who had addressed him. Soon after this, five African bishops, among them St. Augustine, wrote a personal letter to Innocent regarding their own position in the matter of Pelagianism. In addition he acted as metropolitan over the bishops of Italia Suburbicaria.

The historian Zosimus in his "Historia Nova" suggests that during the sack of Rome in 410 by Alaric I, Innocent I was willing to permit private pagan practices as a temporary measure. However, Zosimus also suggests that this attempt by pagans to restore public worship failed due to lack of public interest, suggesting that Rome had been successfully Christianized in the last century.

Among Innocent I's letters is one to Jerome and another to John II, Bishop of Jerusalem, regarding annoyances to which the former had been subjected by the Pelagians at Bethlehem.

He died on 12 March 417. Accordingly, his feast day is now celebrated on 12 March, though from the thirteenth to the twentieth century he was commemorated on 28 July. His successor was Zosimus.

It is accepted that the canon of the Bible was closed c. 405 AD by Pope Innocent, when he sent a list of the sacred books to a Gallic bishop, Exsuperius of Toulouse, identical with that of Trent (which took place more than 1000 years later), except for some uncertainty in the manuscript tradition about whether the letters ascribed to Paul were 14 or only 13, in the latter case possibly implying omission of the Epistle to the Hebrews.

In 846, Pope Sergius II gave approval for the relics of St. Innocent to be moved by Duke Liudolf of Saxony, along with those of his father and predecessor Anastasius, to the crypt of the former collegiate church of Gandersheim, now Gandersheim Abbey, where most rest until this day. Relics were also brought to The Church of Our Lady St Mary of Glastonbury upon its consecration.




</doc>
<doc id="24425" url="https://en.wikipedia.org/wiki?curid=24425" title="Philippi">
Philippi

Philippi (; , "Philippoi") was a major Greek city northwest of the nearby island, Thasos. Its original name was Crenides (, "Krenides" "Fountains") after its establishment by Thasian colonists in 360/359 BC. The city was renamed by Philip II of Macedon in 356 BC and abandoned in the 14th century after the Ottoman conquest. The present municipality, Filippoi, is located near the ruins of the ancient city and is part of the region of East Macedonia and Thrace in Kavala, Greece. It was classified as a UNESCO World Heritage Site in 2016.

Thasian colonists established a settlement at Krenides (meaning "springs") in Thrace in 360/359 BC near the head of the Aegean Sea at the foot of Mt. Orbelos, now called Mt. Lekani, about north-west of Kavalla, on the northern border of the marsh that, in antiquity, covered the entire plain separating it from the Pangaion Hills to the south. In 356 BC, King Philip II of Macedon conquered the city and renamed it to Philippi.

The Macedonian conquerors of the town aimed to take control of the neighbouring gold mines and to establish a garrison at a strategic passage: the site controlled the route between Amphipolis and Neapolis, part of the great royal route which runs east-west across Macedonia and which the Roman Republic reconstructed in the 2nd century BC as part of the "Via Egnatia". Philip II endowed the city with important fortifications, which partially blocked the passage between the swamp and Mt. Orbelos, and sent colonists to occupy it. Philip also had the marsh partially drained, as the writer Theophrastus ( 371 – 287 BC) attests. Philippi preserved its autonomy within the kingdom of Macedon and had its own political institutions (the "Assembly" of the "demos"). The discovery of new gold mines near the city, at Asyla, contributed to the wealth of the kingdom and Philip established a mint there. The city became fully integrated into the kingdom during the reign (221 to 179 BC) of Philip V of Macedon.

The city contained 2,000 people.

When the Romans destroyed the Antigonid dynasty of Macedon in the Third Macedonian War (168 BC), they divided the kingdom into four separate states ("merides"). Amphipolis (rather than Philippi) became the capital of the eastern Macedonian state.

Almost nothing is known about the city in this period, but archeological remains include walls, the Greek theatre, the foundations of a house under the Roman forum and a little temple dedicated to a hero cult. This monument covers the tomb of a certain Exekestos, is possibly situated on the agora and is dedicated to the κτίστης ("ktistēs"), the foundation hero of the city.

The city reappears in the sources during the Liberators' civil war (43–42 BC) that followed the assassination of Julius Caesar in 44 BC. Caesar's heirs Mark Antony and Octavian confronted the forces of the assassins Marcus Junius Brutus and Gaius Cassius Longinus at the Battle of Philippi on the plain to the west of the city during October in 42 BC. Antony and Octavian won this final battle against the partisans of the Republic. They released some of their veteran soldiers, probably from Legion XXVIII, to colonize the city, which was refounded as "Colonia Victrix Philippensium". From 30 BC Octavian established his control of the Roman state, becoming Roman emperor from 27 BC. He reorganized the colony and established more settlers there, veterans (possibly from the Praetorian Guard) and other Italians. The city was renamed "Colonia Iulia Philippensis", and then "Colonia Augusta Iulia Philippensis" after January, 27 BC, when Octavian received the title Augustus from the Roman Senate.

Following this second renaming, and perhaps after the first, the territory of Philippi was centuriated (divided into squares of land) and distributed to the colonists. The city kept its Macedonian walls, and its general plan was modified only partially by the construction of a forum, a little to the east of the site of Greek agora. It was a "miniature Rome", under the municipal law of Rome, and governed by two military officers, the "duumviri", who were appointed directly from Rome, similar to Roman colonies

The colony recognized its dependence on the mines that brought it its privileged position on the "Via Egnatia". Many monuments evidence its wealth - particularly imposing considering the relatively small size of the urban area: the forum, laid out in two terraces on both sides of the main road, was constructed in several phases between the reigns of the Emperors Claudius (41–54 AD) and Antoninus Pius (138–161), and the theatre was enlarged and expanded in order to hold Roman games. An abundance of Latin inscriptions also testifies to the prosperity of the city.

The New Testament records a visit to the city by the apostle Paul during his second missionary journey (likely in AD 49 or 50)(). On the basis of the Acts of the Apostles and the letter to the Philippians, early Christians concluded that Paul had founded their community. Accompanied by Silas, by Timothy and possibly by Luke (the author of the Acts of the Apostles), Paul is believed to have preached for the first time on European soil in Philippi. According to the New Testament, Paul visited the city on two other occasions, in 56 and 57. The Epistle to the Philippians dates from around 61-62 and is believed to show the immediate effects of Paul's instruction.

The development of Christianity in Philippi is indicated by a letter from Polycarp of Smyrna addressed to the community in Philippi around AD 160 and by funerary inscriptions.

The first church described in the city is a small building that was probably originally a small prayer-house. This "Basilica of Paul", identified by a mosaic inscription on the pavement, is dated around 343 from a mention by the bishop Porphyrios, who attended the Council of Serdica that year.

Despite Philippi having one of the oldest congregations in Europe, attestation of a bishopric dates only from the 4th century.

The prosperity of the city in the 5th and 6th centuries was attributed to Paul and to his ministry. As in other cities, many new ecclesiastical buildings were constructed at this time. Seven different churches were built in Philippi between the mid-4th century and the end of the 6th, some of which competed in size and decoration with the most beautiful buildings in Thessalonica, or with those of Constantinople. The relationship of the plan and of the architectural decoration of Basilica B with Hagia Sophia and with Saint Irene in Constantinople accorded a privileged place to this church in the history of early Christian art. The complex cathedral which took the place of the Basilica of Paul at the end of the 5th century, constructed around an octagonal church, also rivaled the churches of Constantinople.

In the same age, the Empire rebuilt the fortifications of the city in order to better defend against growing instability in the Balkans. In 473 Ostrogothic troops of Theodoric Strabo besieged the city; they failed to take it but burned down the surrounding villages.

Already weakened by the Slavic invasions at the end of the 6th century - which ruined the agrarian economy of Macedonia - and probably also by the Plague of Justinian in 547, the city was almost totally destroyed by an earthquake around 619, from which it never recovered. There was a small amount of activity there in the 7th century, but the city was now hardly more than a village.

The Byzantine Empire possibly maintained a garrison there, but in 838 the Bulgarians under "kavhan" Isbul took the city and celebrated their victory with a monumental inscription on the stylobate in Basilica B, now partially in ruins. The site of Philippi was so strategically sound that the Byzantines attempted to recapture it around 850. Several seals of civil servants and other Byzantine officials, dated to the first half of the 9th century, prove the presence of Byzantine armies in the city.

Around 969, Emperor Nicephorus II Phocas rebuilt the fortifications on the acropolis and in part of the city. These gradually helped to weaken Bulgar power and to strengthen the Byzantine presence in the area. In 1077 Bishop Basil Kartzimopoulos rebuilt part of the defenses inside the city. The city began to prosper once more, as witnessed by the Arab geographer Al Idrisi, who mentions it as a centre of business and wine production around 1150.

After a brief occupation by the Franks after the Fourth Crusade and the capture of Constantinople in 1204, the city was captured by the Serbs. Still, it remained a notable fortification on the route of the ancient "Via Egnatia"; in 1354, the pretender to the Byzantine throne, Matthew Cantacuzenus, was captured there by the Serbs.

The city was abandoned at an unknown date. When the French traveller Pierre Belon visited the area in the 1540s there remained nothing but ruins, used by the Turks as a quarry. The name of the city survived - at first in a Turkish village on the nearby plain, Philibedjik (Filibecik, "Little Filibe" in Turkish), which has since disappeared, and then in a Greek village in the mountains.

Noted or briefly described by 16th century travellers, the first archaeological description of the city was made in 1856 by Perrot, then in 1861 by Léon Heuzey and Henri Daumet in their famous "Mission archéologique de Macédoine". The first excavations did not begin until the summer of 1914, and were soon interrupted by the First World War. The excavations, carried out by the École française d'Athènes, were renewed in 1920 and continued until 1937. During this time the Greek theatre, the forum, Basilicas A and B, the baths and the walls were excavated. After the Second World War, Greek archaeologists returned to the site. From 1958 to 1978, the Société Archéologique, then the Service archéologique and the University of Thessalonica uncovered the bishop's quarter and the octagonal church, large private residences, a new basilica near the Museum and two others in the necropolis to the east of the city.


Translated from the , retrieved February 11, 2005. That article, in turn, gives the following references:



</doc>
<doc id="24427" url="https://en.wikipedia.org/wiki?curid=24427" title="Victoria, Crown Princess of Sweden">
Victoria, Crown Princess of Sweden

Victoria, Crown Princess of Sweden, Duchess of Västergötland (Victoria Ingrid Alice Désirée, born 14 July 1977) is the heir apparent to the Swedish throne, as the eldest child of King Carl XVI Gustaf. If she ascends to the throne as expected, she would be Sweden's fourth queen regnant (after Margaret, Christina and Ulrika Eleonora) and the first since 1720.

Victoria was born on 14 July 1977 at 21:45 CET at the Karolinska Hospital in Solna, Stockholm County, Sweden, and is the oldest child of King Carl XVI Gustaf and Queen Silvia. She is a member of the House of Bernadotte. Born as a princess of Sweden, she was designated crown princess in 1979 (SFS 1979:932) ahead of her younger brother. Her place as first in the line of succession formally went into effect on 1 January 1980 with the parliamentary change to the Act of Succession that introduced absolute primogeniture.

Her given names honour various relatives. Her first name comes primarily from her great-great-grandmother Victoria of Baden, queen consort of Sweden. Her other names honour her great-aunt Ingrid of Sweden; her maternal grandmother, Alice Soares de Toledo; and her ancestor Désirée Clary, queen consort of Sweden; and her paternal aunt and godmother, Princess Désirée.

She was baptised at The Royal Palace Church on 27 September 1977. Her godparents were Crown Prince Harald of Norway (later king of Norway), her maternal uncle, Ralf Sommerlath, Princess Beatrix of the Netherlands (later queen of the Netherlands, 1980–2013), and her aunt Princess Désirée, Baroness Silfverschiöld. The Crown Princess was confirmed in the summer of 1992 at Räpplinge church on the island of Öland.

Victoria studied for a year (1996–97) at the Catholic University of the West at Angers in France, and in the fall term of 1997 participated in a special program following the work of the "Riksdag". From 1998 to 2000, Victoria resided in the United States, where she studied various subjects at Yale University, New Haven, Connecticut.

In May 1999, she was an intern at the Swedish Embassy in Washington, D.C. Victoria completed a study program at the Government Offices in 2001. In 2003, Victoria's education continued with visits to Swedish businesses, a study and intern program in agriculture and forestry, as well as completion of the basic soldier training at SWEDINT (the Swedish Armed Forces International Centre).

In 2006, Victoria enrolled in the Ministry for Foreign Affairs' Diplomat Program, running from September 2006 to June 2007. The program is a training program for young future diplomats and gives an insight to the ministry's work, Swedish foreign and security policies and Sweden's relations with the rest of the world. In June 2009, she graduated with a Bachelor of Arts degree from Uppsala University.

She speaks Swedish, English, French and German.

Victoria was made crown princess on 1 January 1980 by the 1979 change to the Act of Succession of 1810 ("Successionsordningen"). This constitutional introduced absolute primogeniture, meaning that the throne would be inherited by the monarch's eldest child without regard to gender. King Carl XVI Gustaf objected to the reform after it occurred—not because he objected to women entering the line of succession, but because he was upset about his son being stripped of the crown prince status he had held since birth.

When she became heir, she also was made Duchess of Västergötland, one of the historical provinces of Sweden. Prior to this constitutional change, the heir apparent to the throne was her younger brother, Carl Philip. He is now fourth in line to the throne, behind Victoria and her children.

Victoria's declaration of majority took place in the Hall of State at the Royal Palace of Stockholm on 14 July 1995. As of the day she turned 18, she became eligible to act as Head of State when the King is not in country. Victoria made her first public speech on this occasion. Located on the dais in the background was the same silver throne on which her father was seated at his enthronement, in actual use from 1650 and up until this ceremony.

As heir apparent to the throne, Victoria is a working member of the Swedish Royal Family with her own agenda of official engagements. Victoria attends the regular Advisory Council on Foreign Affairs and the information councils with Government ministers headed by the King, and steps in as a temporary regent (Riksföreståndare) when needed.

Victoria has made many official trips abroad as a representative of Sweden. Her first major official visit on her own was to Japan in 2001, where she promoted Swedish tourism, design, music, gastronomy and environmental sustainability during the "Swedish Style" event. That same year, Victoria also travelled to the West Coast of the United States, where she participated in the celebrations of the Nobel centenary.

In 2002, she paid official visits to United States, Spain, Uganda, Ethiopia, and Kosovo where she visited Camp Victoria. In 2003, she made official visits to Egypt and the United States. In early 2004, she paid an official visit to Saudi Arabia, as a part of a large official business delegation from Sweden, and in October 2004, she travelled to Hungary.

Crown Princess Victoria was given her own household in October 2004. It is headed by the Marshal of the Court, and serves to coordinate the official engagements of The Crown Princess.

In January 2005, Victoria made a long official visit to Australia, promoting Swedish style and businesses, and in April she visited Bangladesh and Sri Lanka to follow aid work and become informed about the work in the aftermath of the tsunami. In April 2005, Victoria made an official visit to Japan where she visited the Expo 2005 in Aichi, laid the foundation for a new IKEA store in Yokohama together with Princess Takamado and met with Emperor Akihito, Empress Michiko, Crown Prince Naruhito and Sayako Kuroda. In June 2005, Victoria travelled to Turkey on an official visit where she participated in the Swedish Business Seminar and Sweden Day celebrations in Ankara during a historic visit, which was organised by the Swedish Embassy in Ankara and Swedish Trade Council in Istanbul. Victoria also visited the historic sights such as the Blue Mosque, Topkapı Palace and Hagia Sophia. This was the first official Royal visit from Sweden to Turkey since 1934. In September 2005, she made an official visit to China.

In March 2006, Victoria made an official visit to Brazil where she followed the Volvo Ocean Race and visited projects supported by the World Childhood Foundation, such as the Abrigo Rainha Sílvia. In December, she paid a four-day official visit to Paris where she attended a French-Swedish soirée arranged by the Swedish Chamber of Commerce, the Swedish Trade Council and the Swedish Embassy, during which she also awarded the Prix d’Excellence 2006. The visit to Paris also included events with the Swedish Club in Paris, attendance at a church service in the Sofia Church (the Swedish church in Paris), a study visit to the OECD headquarters and meetings with the Secretary-General José Ángel Gurría, the Swedish Ambassador to the OECD, Gun-Britt Andersson, and other senior officials. She also attended a gala dinner hosted by La Fondation Pour L’Enfance at Versailles.

She is a member of the Honorary Board of the International Paralympic Committee.

In 2011, it was announced that Victoria would continue working throughout her pregnancy. In 2012, she took her maternity leave one day prior to the birth of her daughter Estelle and her husband Daniel revealed that he would take his paternity leave and switch parental roles with Victoria when Estelle began preschool.

In January 2016, UN Secretary General Ban Ki-moon appointed The Crown Princess as a member of Sustainable Development Goals Advocates for Agenda 2030. The Crown Princess is therefore one of 16 ambassadors in the Sustainable Development Goals (SDG) Advocacy Group. The group's task is to promote the UN's Sustainable Development Goals – Agenda 2030 – in various ways. The Crown Princess primarily works with issues concerning water and health.

The Crown Princess Victoria's Fund was set up in 1997 and is run as a part of Radiohjälpen, the fundraising branch of Sveriges Television and Sveriges Radio. The fund's aim is to provide support for leisure and recreational activities for children and young people with functional disabilities or chronic illnesses.

The Crown Princess Victoria Fund's means mainly derive from donations by the public, but large companies such as Arla Foods, Swedbank and AB Svenska Returpack are constant sponsor partners. Additional support comes from The Association of Swedish Bakers & Confectioners who every year arrange a national “princess cake week” during which the participating cafés and bakeries give 2.50 SEK per sold princess pastry and 10 SEK per sold princess cake to the fund. The result of this fund-raising drive is usually presented to Victoria herself on her name day on 12 March every year; in 2007, the total amount was 200,000 SEK. Congratulatory and memorial cards are also issued by Radiohjälpen benefitting the fund, a simple way to pay respects and do a good deed in one act. In 2006, The Crown Princess Victoria Fund raised a total of 5.5 million SEK.

Every year Victoria visits one or several clubs or projects that have been granted money. These visits are not announced via the official royal diary but kept private; instead Sveriges Television often accompanies her and airs short programs from these visits at some time during the year.

Victoria's first boyfriend was Daniel Collert. They socialised in the same circles, went to the same school and were already friends when their romance developed in the mid-1990s. When Victoria moved to the United States in 1998 to study and recover from her eating disorders, Collert moved with her across the Atlantic and settled in New York. In September 2000, Victoria's relationship with Collert was confirmed in an interview with her at Expo 2000. The relationship ended in 2001.

In May 2002, Swedish newspaper "Expressen" reported that Victoria had a new boyfriend, her personal trainer at Master Training, Daniel Westling. When the news broke and the media turned its attention on him, it was obvious that he did not like being in the public eye. Once Westling was photographed crossing a street against a red light in order to avoid a camera. In July 2002, Victoria and Daniel Westling were pictured kissing for the first time at a birthday party for Caroline Kreuger, a close friend of Victoria.

In a popular personal report called "Tre dagar med Victoria", which profiled her work during a three-day period that aired on TV4 in December 2004, Victoria commented on criticism directed at Westling, “Many unfair things are written. I understand that there is speculation, but some day justice will be done there, too.” Victoria also gave her opinion that happiness is important, and that these days it is not so much about background and pedigree but about two people who have to live with each other. She said that if they are not happy and comfortable with each other, it is impossible to do a good job.

Swedish media often speculated about upcoming engagements and marriages for Victoria. On 24 February 2009, rumours that wedding plans were imminent became particularly intense preceding an information council between the King and Prime Minister Fredrik Reinfeldt. Under the terms of the Swedish Act of Succession, the Government, upon the request of the King, gives the final consent for a dynastic marriage of a prince or princess of Sweden. The prince or princess otherwise loses their right to the throne. Later that day, it was confirmed that permission had been granted and that Victoria would marry Daniel Westling in the summer of 2010. The wedding date was set in Stockholm Cathedral for 19 June 2010, the 34th anniversary of her parents' marriage. Her engagement ring features a solitaire round brilliant-cut diamond mounted on white gold.

The wedding took place on 19 June 2010. Guests including royalty and ambassadors from various countries were invited to the wedding ceremony which took place at Stockholm Cathedral. After the wedding the newlyweds were driven through Stockholm in a coach and then rowed in the antique royal barge "Vasaorden" to the royal palace where the wedding banquet was held. On the evening before the wedding, there was a gala concert dedicated to the couple in the Stockholm Concert Hall.

On 17 August 2011, the Swedish royal court announced that Crown Princess Victoria was pregnant and expecting the couple's first child in March 2012. On 23 February 2012, Victoria gave birth to Princess Estelle, Duchess of Östergötland, in the Karolinska University Hospital. Their second child, Prince Oscar, Duke of Skåne, was born on 2 March 2016 in the same hospital.

Victoria has dyslexia, as do her father King Carl XVI Gustaf and her brother Prince Carl Philip.

In 1996, it was established that Victoria suffered from anorexia; this was not confirmed until the next year. Already at that time she was getting professional help, but given her public position in Sweden it was getting increasingly difficult to handle the situation. Victoria had planned to study at Uppsala University, but after intense media speculation and public discussion when pictures of an evidently emaciated Victoria in sleeveless dresses at the Order of the Innocence's ball and the gala dinner for the incoming state visit from Austria surfaced in April 1997, the Royal Court decided to confirm what was feared.

After a press release from the Royal Court in November 1997 announced that Victoria had eating disorders, plans changed for her and she moved to the United States where she received professional help and studied at Yale University. By making this drastic decision, Victoria lived an anonymous life while getting professional help and recovering without having to worry about media speculations or if people were recognizing her on the streets.

In June 1999, Victoria said, "It was a really hard time. This kind of illness is hard, not only for the individual but also for the people close to him or her. Today I'm fine."

In November 2002, the book "Victoria, Victoria!" came out, speaking further about her eating disorder. Victoria said: "I felt like an accelerating train, going right down... during the whole period. I had eating disorders and was aware of it, my anguish was enormous. I really hated how I looked like, how I was... I, Victoria, didn’t exist. It felt like everything in my life and around me was controlled by others. The one thing I could control was the food I put in me". She further said that "What happened cost and I was the one who stood for the payments. Now I’m feeling well and with the insights I’ve acquired through this I can hopefully help someone else".

Victoria suffers from prosopagnosia, which makes it difficult to recognize familiar faces. In an interview in 2008, she called it a "big drawback" in her capacity because she finds it very hard to remember names and faces.






</doc>
<doc id="24428" url="https://en.wikipedia.org/wiki?curid=24428" title="Pope Innocent II">
Pope Innocent II

Pope Innocent II (; died 23 September 1143), born Gregorio Papareschi, was head of the Catholic Church and ruler of the Papal States from 14 February 1130 to his death in 1143. His election as pope was controversial and the first eight years of his reign were marked by a struggle for recognition against the supporters of Anacletus II. He reached an understanding with King Lothair III of Germany who supported him against Anacletus and whom he crowned as Holy Roman emperor. Innocent went on to preside over the Second Lateran council.

Gregorio Papareschi came from a Roman family, probably of the "rione" Trastevere. He was probably one of the clergy in personal attendance on the Antipope Clement III (Guibert of Ravenna).

Pope Urban II made Papareschi a cardinal deacon in 1088. In this capacity, he accompanied Pope Gelasius II when he was driven into France. He was selected by Pope Callixtus II for various important and difficult missions, such as the one to Worms for the conclusion of the Concordat of Worms, the peace accord made with Holy Roman Emperor Henry V in 1122, and also the one that made peace with King Louis VI of France in 1123.

In 1130, as Pope Honorius II lay dying, the cardinals decided to entrust the election to a commission of eight men led by papal chancellor Haimeric, who had his candidate Cardinal Gregorio Papareschi hastily elected as Pope Innocent II. He was consecrated on 14 February, the day after Honorius' death. The other cardinals announced that Innocent had not been canonically elected and chose Anacletus II, a Roman whose family were the enemy of Haimeric's supporters, the Frangipani. Anacletus' mixed group of supporters were powerful enough to take control of Rome while Innocent was forced to flee north. Based on a simple majority of the entire college of cardinals, Anacletus was the canonically elected pope, and Innocent was the antipope. However, the legislation of Pope Nicholas II pre-empted the choice of the majority of the cardinal priests and cardinal deacons. This rule was changed by the Second Lateran council of 1139.

Anacletus had control of Rome, so Innocent II took ship for Pisa, and thence sailed by way of Genoa to France, where the influence of Bernard of Clairvaux readily secured his cordial recognition by the clergy and the court. In October of the same year he was duly acknowledged by King Lothair III of Germany and his bishops at the synod of Würzburg. In January 1131, he had also a favourable interview with Henry I of England, and in August 1132 Lothar III undertook an expedition to Italy for the double purpose of setting aside Anacletus as antipope and of being crowned by Innocent. Anacletus and his supporters being in secure control of St. Peter's Basilica, the coronation ultimately took place in the Lateran Church (4 June 1133), but otherwise the expedition proved abortive. At the investiture of Lothair as emperor he gained the territories belonging to Matilda of Tuscany in return for an annuity to be paid to the pope, in consequence of which the curial party based the contention that the emperor was a vassal of the papacy.

A second expedition by Lothar III in 1136 was not more decisive in its results, and the protracted struggle between the rival pontiffs was terminated only by the death of Anacletus II on 25 January 1138.

Innocent took as cardinal-nephew first his nephew, Gregorio Papareschi, whom he made cardinal in 1134, and then his brother Pietro Papareschi, whom he made cardinal in 1142. Another nephew, Cinzio Papareschi (died 1182), was also a cardinal, raised to the cardinalate in 1158, after Innocent's death.

By the Second Lateran council of 1139, at which King Roger II of Sicily, Innocent II's most uncompromising foe, was excommunicated, peace was at last restored to the Church. Aside from the complete rebuilding of the ancient church of Santa Maria in Trastevere, which boldly features Ionic capitals from former colonnades in the Baths of Caracalla and other richly detailed "spolia" from Roman monuments, the remaining years of this Pope's life were almost as barren of permanent political results as the first had been. His efforts to undo the mischief wrought in Rome by the long schism were almost entirely neutralized by a quarrel with his erstwhile supporter, Louis VII of France over the candidate for archbishop of Bourges, in the course of which that kingdom was laid under an interdict to press for the papal candidate, and by a struggle with the town of Tivoli in which he became involved. As a result, Roman factions that wished Tivoli annihilated took up arms against Innocent.

It was also in 1139 that, in the "Omne Datum Optimum", Innocent II declared that the Knights Templar—a religious and military organization then twenty-one years old—should in the future be answerable only to the papacy. This was a keystone in the Templars' ever increasing power and wealth, and ironically helped to bring about their violent suppression in October 1307.

Can. 29 of the Second Lateran Council under Pope Innocent II in 1139 banned the use of crossbows, as well as slings and bows, against Christians.

On 22 July 1139, at Galluccio, Roger II's son Roger III of Apulia ambushed the papal troops with a thousand knights and captured Innocent. On 25 July 1139, Innocent was forced to acknowledge the kingship and possessions of Roger with the Treaty of Mignano. In 1143, Innocent refused to recognise the Treaty of Mignano with Roger of Sicily, who sent Robert of Selby to march on papal Benevento. The terms agreed upon at Mignano were then recognised. Innocent II died on 24 September 1143 and was succeeded by Pope Celestine II.

The doctrinal questions which he was called on to decide were those that condemned the opinions of Pierre Abélard and of Arnold of Brescia.

In 1143, as the pope lay dying, the Commune of Rome, to resist papal power, began deliberations that officially reinstated the Roman Senate the following year. The pope was interred in a porphyry sarcophagus that contemporary tradition asserted had been the Emperor Hadrian's.



</doc>
<doc id="24429" url="https://en.wikipedia.org/wiki?curid=24429" title="Pope Zosimus">
Pope Zosimus

Pope Zosimus was the bishop of Rome from 18 March 417 to his death on 26 December 418. He was born in Mesoraca, Calabria. Zosimus took a decided part in the protracted dispute in Gaul as to the jurisdiction of the See of Arles over that of Vienne, giving energetic decisions in favour of the former, but without settling the controversy. His fractious temper coloured all the controversies in which he took part, in Gaul, Africa and Italy, including Rome, where at his death the clergy were very much divided.

According to the "Liber Pontificalis", Zosimus was a Greek and his father's name was Abramius. Historian Adolf von Harnack deduced from this that the family was of Jewish origin, but this has been rejected by Louis Duchesne.

The consecration of Zosimus as bishop of Rome took place on 18 March 417. The festival was attended by Bishop Patroclus of Arles, who had been raised to that see in place of Bishop Heros of Arles, who had been deposed by Constantius III. Patroclus gained the confidence of the new pope at once; as early as 22 March he received a papal letter which conferred upon him the rights of a metropolitan over all the bishops of the Gallic provinces of Viennensis and Narbonensis I and II. In addition, he was made a kind of papal vicar for the whole of Gaul, with no Gallic ecclesiastic being permitted to journey to Rome without bringing with him a certificate of identity from Patroclus.

In the year 400, Arles had been substituted for Trier as the residence of the chief government official of the civil Diocese of Gaul, the "Prefectus Praetorio Galliarum". Patroclus, who enjoyed the support of the commander Constantine, used this opportunity to procure for himself the position of supremacy above mentioned, by winning over Zosimus to his ideas. The bishops of Vienne, Narbonne, and Marseille regarded this elevation of the See of Arles as an infringement of their rights, and raised objections which occasioned several letters from Zosimus. The dispute, however, was not settled until the pontificate of Pope Leo I.

Caelestius, a proponent of Pelagianism who had been condemned by the preceding pope, Innocent I, came to Rome to appeal to the new pope, having been expelled from Constantinople. In the summer of 417, Zosimus held a meeting of the Roman clergy in the Basilica of St. Clement before which Caelestius appeared. The propositions drawn up by the deacon Paulinus of Milan, on account of which Caelestius had been condemned at Carthage in 411, were laid before him. Caelestius refused to condemn these propositions, at the same time declaring in general that he accepted the doctrine expounded in the letters of Pope Innocent and making a confession of faith which was approved. The pope was won over by the conduct of Caelestius, and said that it was not certain whether he had really maintained the false doctrine rejected by Innocent, and therefore Zosimus considered the action of the African bishops against Caelestius too hasty. He wrote at once in this sense to the bishops of the African province, and called upon those who had anything to bring against Caelestius to appear at Rome within two months.

After he received from Pelagius a confession of faith, together with a new treatise on free will, Zosimus held a new synod of the Roman clergy, before which both these writings were read. The assembly held the statements to be orthodox, and Zosimus again wrote to the African bishops defending Pelagius and reproving his accusers, among whom were the Gallic bishops Hero and Lazarus. Archbishop Aurelius of Carthage quickly called a synod, which sent a reply to Zosimus in which it was argued that the pope had been deceived by heretics. In his answer Zosimus declared that he had settled nothing definitely, and wished to settle nothing without consulting the African bishops. After the new synodal letter of the African council of 1 May 418 to the pope, and after the steps taken by the emperor Honorius against the Pelagians, Zosimus issued his "Tractoria", in which Pelagianism and its authors were finally condemned.

Shortly after this, Zosimus became involved in a dispute with the African bishops in regard to the right of clerics who had been condemned by their bishops to appeal to the Roman See. When the priest Apiarius of Sicca had been excommunicated by his bishop on account of his crimes, he appealed directly to the pope, without regard to the regular course of appeal in Africa, which was exactly prescribed. The pope at once accepted the appeal, and sent legates with credentials to Africa to investigate the matter. Another, potentially wiser, course would have been to have first referred the case of Apiarius to the ordinary course of appeal in Africa itself. Zosimus next made the further mistake of basing his action on a reputed canon of the First Council of Nicaea, which was in reality a canon of the Council of Sardica. In the Roman manuscripts the canons of Sardica followed those of Nicaea immediately, without an independent title, while the African manuscripts contained only the genuine canons of Nicaea, so that the canon appealed to by Zosimus was not contained in the African copies of the Nicene canons. This mistake ignited a serious disagreement over the appeal, which continued after the death of Zosimus.

Besides the writings of the pope already mentioned, there are extant other letters to the bishops of the Byzantine province in Africa, in regard to a deposed bishop, and to the bishops of Gaul and Spain in respect to Priscillianism and ordination to the different grades of the clergy. The "Liber Pontificalis" attributes to Zosimus a decree on the wearing of the maniple by deacons, and on the dedication of Easter candles in the country parishes; also a decree forbidding clerics to visit taverns. Zosimus was buried in the sepulchral Basilica of Saint Lawrence outside the Walls.





</doc>
<doc id="24430" url="https://en.wikipedia.org/wiki?curid=24430" title="Pope Innocent IV">
Pope Innocent IV

Pope Innocent IV (; c. 1195 – 7 December 1254), born Sinibaldo Fieschi, was the head of the Catholic Church from 25 June 1243 to his death in 1254.

Fieschi was born in Genoa and studied at the universities of Parma and Bologna. Considered a fine canonist, he served in the Curia for Pope Honorius III. Pope Gregory IX made Fieschi a cardinal and appointed him governor of the March of Ancona in 1235. He was elected pope in 1243 and took the name Innocent IV. He inherited an ongoing dispute over lands seized by the Holy Roman Emperor, and the following year relocated to France to escape imperial plots against him in Rome. He returned to Rome after the death of the Emperor in 1250.

Born in Genoa (although some sources say Manarola) in an unknown year, Sinibaldo was the son of Beatrice Grillo and Ugo Fieschi, Count of Lavagna. The Fieschi were a noble merchant family of Liguria. Sinibaldo received his education at the universities of Parma and Bologna and, for a time, taught canon law at Bologna. It is pointed out by Agostino Paravicini-Bagliani, however, that there is no "documentary" evidence of such a professorship. From 1216-1227 he was Canon of the Cathedral of Parma. He was considered one of the best canonists of his time, and was called to serve Pope Honorius III in the Roman Curia as "Auditor causarum", from 11 November 1226 to 30 May 1227. He was then promoted to the office of Vice-Chancellor of the Holy Roman Church (from 31 May to 23 September 1227), though he retained the office and the title for a time after he was named Cardinal.

Vice-Chancellor Sinibaldo Fieschi was created Cardinal-Priest of San Lorenzo in Lucina on 18 September 1227 by Pope Gregory IX (1227-1241). He later served as papal governor of the March of Ancona, from 17 October 1235 until 1240.

It is widely repeated, from the 17th century on, that he became bishop of Albenga in 1235, but there is no foundation to this claim.

Innocent's immediate predecessor was Pope Celestine IV, elected 25 October 1241, whose reign lasted a mere fifteen days. The events of Innocent IV's pontificate are therefore inextricably linked to the policies dominating the reigns of popes Innocent III, Honorius III and Gregory IX.

Gregory had been demanding the return of portions of the Papal States taken over by Holy Roman Emperor Frederick II when he died. The Pope had called a general council so he could depose the emperor with the support of Europe's spiritual leaders, but Frederick had seized two cardinals traveling to the council in hopes of intimidating the curia. The two prelates remained incarcerated and missed the conclave that immediately elected Celestine. The conclave that reconvened after his death fell into camps supporting contradictory policies about how to treat with the emperor.

After a year and a half of contentious debate and coercion, a papal election finally reached a unanimous decision. Cardinal de' Fieschi very reluctantly accepted election as Pope 25 June 1243, taking the name Innocent IV. As Cardinal de' Fieschi, Sinibaldo had been on friendly terms with Frederick, even after his excommunication. The Emperor also greatly admired the cardinal's wisdom, having enjoyed discussions with him from time to time.

Following the election the witty Frederick remarked that he had lost the friendship of a cardinal but made up for it by gaining the enmity of a pope.

His jest notwithstanding, Frederick's letter to the new pontiff was couched in respectful terms, offering Innocent congratulations and success, also expressing hope for an amicable settlement of the differences between the empire and the papacy. Negotiations leading to this objective began shortly afterwards, but proved abortive. Innocent refused to back down from his demands, Frederick II refused to acquiesce, and the dispute continued, its major point of contention being the reinstatement of Lombardy to the Patrimony of St Peter.

The Emperor's machinations caused a good deal of anti-papal feeling to rise in Italy, particularly in the Papal States, and imperial agents encouraged plots against papal rule. Realizing how untenable his position in Rome was growing, Innocent IV secretly and hurriedly withdrew, fleeing Rome on 7 June 1244. Traveling in disguise, Innocent made his way to Sutri and Civitavecchia, to Genoa, his birthplace, where he arrived on 7 July. From there, on 5 October, he fled to France, where he was joyously welcomed. Making his way to Lyon, where he arrived on November 29, 1244, Innocent was happily greeted by the magistrates of the city.

Finding himself now in secure surroundings and out of the reach of Frederick II, Innocent summoned, in a sermon preached on December 27, 1244, as many bishops as could get to Lyon (140 bishops were present), to attend what became the 13th General (Ecumenical) Council of the Church, the first to be held in Lyon. The bishops met for three public sessions: 28 June, 5 July, and 17 July 1245. Their principal business was to subjugate the Emperor Frederick II.

An earlier pope, Gregory IX (1227-1241), had issued letters on 9 June 1239, ordering all the bishops of France to confiscate all Talmuds in the possession of the Jews. Agents were to raid each synagogue on the first Saturday of Lent of 1240, and seize the books, placing them in the custody of the Dominicans or the Franciscans. The Bishop of Paris was ordered to see to it that copies of the Pope's mandate reached all the bishops of France, England, Aragon, Navarre, Castile and León, and Portugal. On 20 June 1239, there was another letter, addressed to the Bishop of Paris, the Prior of the Dominicans and the Minister of the Franciscans, calling for the burning of all copies of the Talmud, and any obstructionists to be visited with ecclesiastical censures. On the same day he wrote to the King of Portugal ordering him to see to it that all copies of the Talmud be seized and turned over to the Dominicans or Franciscans. King Louis IX of France on account of these letters held a trial in Paris in 1240, which ultimately found the Talmud guilty of 35 alleged charges. 24 cartloads of the Talmud were burned.

Initially, Innocent IV continued Gregory IX's policy. In a letter of 9 May 1244, he wrote to King Louis IX, ordering the Talmud and any books with Talmudic glosses to be examined by the Regent Doctors of the University of Paris, and if condemned by them, to be burned. However, an argument was presented that this policy was a negation of the Church's traditional stance of tolerance toward Judaism. On 5 July 1247, Pope Innocent wrote to the bishops of Germany and the Bishops of Gallia (France) that, because both ecclesiastical and lay persons were lawlessly plundering the property of the Jews, and falsely stating that at Eastertime they sacrificed and ate the heart of a little child, the bishops should see to it that the Jews not be attacked or molested because of these or other reasons. In the year 1247, in a letter of 2 August to King Louis of France, he reversed his stance on the Talmud, and wrote letters to the effect that the Talmud should be censored rather than burned. Innocent IV's words were met with the disapproval of Odo of Châteauroux, Cardinal Bishop of Tusculum and former Chancellor of the University of Paris. Nonetheless, Pope Innocent IV's policy was continued by subsequent popes.

The First Council of Lyon of 1245 had the fewest participants of any General Council before it. However three patriarchs and the Latin emperor of Constantinople attended, along with about 150 bishops, most of them prelates from France and Spain. They were able to come quickly, and Innocent could rely on their help. Bishops from the rest of Europe outside Spain and France feared retribution from Frederick, while many other bishops were prevented from attending either by the invasions of the Mongols (Tartars) in the Far East or Muslim incursions in the Middle East.

In session, Frederick II's position was defended by Taddeo of Suessa, who renewed in his master's name all the promises made before, but refused to give the guarantees the pope demanded. Unable to end the impasse Taddeo was horrified to hear the fathers of the Council solemnly depose and excommunicate the Emperor on 17 July, while absolving all his subjects from allegiance.

The political agitation over these acts convulsed Europe. The turmoil relaxed only with Frederick's death in December 1250, which removed the proximate threat to Innocent's life and permitted his return to Italy. He departed Lyon on 19 April 1251, and arrived in Genoa on 18 May. On 1 July, he was at Milan, accompanied by only three cardinals and the Latin Patriarch of Constantinople. He stayed there until mid-September, when he began an inspection tour of Lombardy, heading for Bologna. On 5, November, he reached Perugia. From 1251–53 the Pope stayed at Perugia until it was safe for him to bring the papal court back to Rome. He finally saw Rome again in the first week of October, 1253. He left Rome on 27 April 1254, for Assisi and then Anagni. He immediately threw himself into the problems surrounding the succession to the possessions of Frederick II, both as German Emperor and as King of Sicily. In both cases, Innocent continued Pope Gregory IX's policy of opposition to the Hohenstaufen, supporting whatever opposition there could be found to that House. This papal stance embroiled Italy in one conflict after another for the next three decades. Innocent IV himself, following after the papal army which was seeking to destroy Frederick's son Manfred, died in Naples on 7 December 1254.

While in Perugia, on 15 May 1252, Innocent IV issued the papal bull "Ad extirpanda", composed of thirty-eight 'laws', and advised civil authorities in Italy to treat heretics as criminals, and proscribed parameters limiting the use of torture to compel disclosures "as thieves and robbers of material goods are made to accuse their accomplices and confess the crimes they have committed."

As Innocent III had before him, Innocent IV saw himself as the Vicar of Christ, whose power was above earthly kings. Innocent, therefore, had no objection to intervening in purely secular matters. He appointed Afonso III administrator of Portugal, and lent his protection to Ottokar, the son of the King of Bohemia. The Pope even sided with King Henry III against both nobles and bishops of England, despite the king's harassment of Edmund Rich, the Archbishop of Canterbury and Primate of All England, and the royal policy of having the income of a vacant bishopric or benefice delivered to the royal coffers, rather than handed over to a papal Administrator (usually a member of the Curia) or a Papal collector of revenue, or delivered directly to the Pope.

The warlike tendencies of the Mongols also concerned the Pope, and he sent a papal nuncio to the Mongol Empire in an attempt to strike an agreement. Innocent decreed that he, as Vicar of Christ, could make non-Christians accept his dominion and even exact punishment should they violate the non-God centered commands of the Ten Commandments. This policy was held more in theory than in practice and was eventually repudiated centuries later.

The papal preoccupation with imperial matters and secular princes caused the spirituality of the Church to suffer. Taxation increased in the Papal States and the complaints of the inhabitants grew considerably.

In August 1253, after much worry about the order's insistence on absolute poverty, Innocent finally approved the rule of the 2nd Order of the Franciscans, the Poor Clares, founded by St. Clare of Assisi, the friend of St Francis.

In 1246 Edmund Rich, former archbishop of Canterbury (died 1240), was named a saint. In 1250 Innocent proclaimed the pious Queen Margaret (died 1093), wife of King Malcolm III of Scotland, a saint. The Dominican priest Peter of Verona, martyred by Albigensian heretics in 1252, was canonized, as was Stanislaus of Szczepanów, the Polish Archbishop of Cracow, both in 1253.

Innocent IV is often credited as helping to create the idea of legal personality, "persona ficta" as it was originally written, which has led to the idea of corporate personhood. This allowed monasteries and universities the ability to act as a single legal entity, allowing for their existence to be more continuous and for monks pledged to poverty to nonetheless be part of an organization that could own infrastructure, but as "fictional people" they could not be excommunicated or considered guilty of delict, that is, negligence to action that is not contractually required. This meant that punishment of individuals within an organization would reflect less on the organization itself than it would if the person running such an organization was said to own it rather than be a constituent of it, and was meant to provide stability.

Innocent IV was responsible for the eventual deposition of King Sancho II of Portugal at the request of his brother Afonso (later King Afonso III of Portugal). One of the arguments he used against Sancho II in his "Grandi non immerito" text was his status as a minor upon inheriting the throne from his father Afonso II.

In 1245, Innocent IV issued bulls and sent an envoy in the person of Giovanni da Pian del Carpine (accompanied by Benedict the Pole) to the "Emperor of the Tartars". The message asked the Mongol ruler to become a Christian and stop his aggression against Europe. The Khan Güyük replied in 1246 in a letter written in Persian that is still preserved in the Vatican Library, demanding the submission of the Pope and the other rulers of Europe.
In 1245 Innocent had sent another mission, through another route, led by Ascelin of Lombardia, also bearing letters. The mission met with the Mongol ruler Baichu near the Caspian Sea in 1247. The reply of Baichu was in accordance with that of Güyük, but it was accompanied by two Mongolian envoys to the Papal seat in Lyon, Aïbeg and Serkis. In the letter Guyuk demanded that the Pope appear in person at the Mongol imperial headquarters, Karakorum in order that “we might cause him to hear every command that there is of the jasaq”. The envoys met with Innocent IV in 1248, who again appealed to the Mongols to stop their killing of Christians.

Innocent IV would also send other missions to the Mongols in 1245: the mission of André de Longjumeau and the possibly aborted mission of Laurent de Portugal.

The remainder of Innocent's life was largely directed to schemes for compassing the overthrow of Manfred of Sicily, the natural son of Frederick II, whom the towns and the nobility had for the most part received as his father's successor. Innocent aimed to incorporate the whole Kingdom of Sicily into the Papal States, but he lacked the necessary economic and political power. Therefore, after a failed agreement with Charles of Anjou, he invested Edmund Crouchback, the nine-year-old son of King Henry III of England, with that kingdom on 14 May 1254.

In the same year, Innocent excommunicated Frederick II's other son, Conrad IV, King of Germany, but the latter died a few days after the investiture of Edmund. At the beginning of June, 1254, Innocent moved to Anagni, where he awaited Manfred's reaction to the event, especially considering that Conrad's heir, Conradin, had been entrusted to Papal tutelage by King Conrad's testament. Manfred submitted, although probably only to gain time and counter the menace from Edmund, and accepted the title of papal vicar for southern Italy. Innocent could therefore enjoy a moment in which he was the acknowledged sovereign, in theory at least, of most of the peninsula. Innocent overplayed his hand, however, by accepting the fealty of Amalfi directly to the Papacy instead of to the Kingdom of Sicily on 23 October. Manfred immediately, on October 26, fled from Teano, where he had established his headquarters, and headed to Lucera to his Saracen troops.
Manfred had not lost his nerve, and organized resistance to papal aggression. Supported by his faithful Saracen troops, he began using military force to make rebellious barons and towns submit to his authority as Regent for his nephew. Realizing that Manfred had no intention of submitting to the Papacy or to anyone else, Innocent and his papal army headed south from his summer residence at Anagni on October 8, intending to confront Manfred's forces. On 27 October 1254 the Pope entered the city of Naples. It was on a sick bed at Naples that Innocent IV heard of Manfred's victory at Foggia on December 2 against the Papal forces, led by the new Papal Legate, Cardinal Guglielmo Fiesch, the Pope's nephew. The tidings are said to have precipitated Pope Innocent's death on 7 December 1254 in Naples. From triumph to disaster had taken only a few months.

Innocent's learning gave to the world an "Apparatus in quinque libros decretalium", a commentary on papal decrees. He is also remembered for issuing the papal bull "Ad extirpanda", which authorized the use of torture by the Inquisition for eliciting confessions from heretics.

Shortly after Innocent's election as pope, his nephew Opizzo was elevated to the Latin Patriarchate of Antioch. In December, 1251, Innocent IV appointed another nephew, Ottobuono, Cardinal Deacon of S. Andriano. Ottobuono was elected Pope Adrian V in 1276.

Innocent was succeeded by Pope Alexander IV (Rinaldo de' Conti).




</doc>
<doc id="24434" url="https://en.wikipedia.org/wiki?curid=24434" title="Pope Innocent V">
Pope Innocent V

Pope Innocent V (; c. 1225 – 22 June 1276), born Pierre de Tarentaise, was head of the Catholic Church and ruler of the Papal States from 21 January to 22 June 1276. A member of the Order of Preachers, he acquired a reputation as an effective preacher. He held one of the two "Dominican Chairs" at university of Paris, and was instrumental in help drawing up the "program of studies" for the Order. In 1269, Peter of Tarentaise was Provincial of the French Province of Dominicans. He was a close collaborator of Pope Gregory X, who named him Bishop of Ostia and raised him to cardinal in 1273.

Upon the death of Gregory in 1276, Peter was elected pope, taking the name Innocent V. He died about five months later, but during his brief tenure facilitated a peace between Genoa and King Charles I of Sicily. Peter of Tarentaise was beatified in 1898 by Pope Leo XIII.

He was born around 1225 near Moûtiers in the Tarentaise region of the County of Savoy. An alternative popular hypothesis, however, suggests that he was born in La Salle in the Aosta valley in Italy. Both places were then part of the Kingdom of Arles in the Holy Roman Empire, but now the first is in southeastern France and the second in northwestern Italy. Another hypothesis, favored by some French scholars, is that Peter originated in a Tarantaise in Burgundy, or Tarantaise in the Department of the Loire in the Arrondisement of S. Etienne. In early life, around 1240, he joined the Dominican Order at their convent in Lyons. In the summer of 1255, he was transferred to the "studium generale" of the Convent of S. Jacques in Paris. This move was essential for someone who was likely to study at the University of Paris. He obtained the degree of Master of Theology, and quickly acquired great fame as a preacher.

Between 1259 and 1264 he held the "Chair of the French", one of the two chairs (professorships) that were allocated to the Dominicans.

In 1259, Peter took part, perhaps because of his status as a Master at Paris, perhaps as an elected "Definitor" (delegate) for the Province of France, in the General Chapter of the Dominican Order at Valenciennes, under the leadership of the Master General, Humbertus de Romans. Peter participated together with Albert the Great, Thomas Aquinas, Bonushomo Britto, and Florentius. This General Chapter established a "ratio studiorum", or program of studies, which was to be implemented for the entire Dominican Order, that featured the study of philosophy as a preparative for those not sufficiently trained to study theology. This innovation initiated the tradition of Dominican scholastic philosophy which was to be put into practice in every Dominican convent, if possible, for example, in 1265 at the Order's "studium provinciale" at the convent of Santa Sabina in Rome. Each convent was expected to have an elected "Lector" to supervise the preparative studies and an elected Master for theological studies. In the next year he was assigned the title of Preacher General.

In 1264 a new Master General of the Order of Preachers was elected, John of Vercelli. It was taken as an opportunity to engage in some academic politics, since Humbertus de Romans, Peter's patron, was dead. One hundred and eight of Peter's statements in his "Commentary on the Sentences of Peter Lombard" were denounced as heretical. But, though Peter withdrew from his professorship, John of Vercelli appointed Thomas Aquinas to write a defense of the 108 propositions. Peter's reputation was such that he was immediately elected Provincial of the French Province for a three-year term (1264-1267). He was granted his release from office at the General Chapter, which was held in Bologna in May, 1267. At the conclusion of his term, and after Thomas of Aquinas' rejoinder to his critics was circulated, Peter returned to his Chair at the University of Paris (1267). In 1269 he was reelected to the office of Provincial of the French Province, and he held the post until he was named Archbishop of Lyons.

On 6 June 1272, Pope Gregory X himself named Peter of Tarantaise to be Archbishop of Lyons, a post he held until he was appointed to be Bishop of Ostia. It is said, however, that Peter was never consecrated. He did, however, take the oath of fealty in early December, 1272, to King Philip III of France. Pope Gregory himself arrived in Lyons in mid-November, 1273, intent upon bringing as many prelates as possible to his planned ecumenical council. He met immediately with King Philip III of France. Their conversations were obviously harmonious, since Philip ceded to the Church the Comtat Venaissin, which he had inherited from his uncle Alphonse, Count of Toulouse. The Second Council of Lyons opened on 1 May 1274. The first session was held on Monday, 7 May. The principal items on the agenda were the Crusade, and the reunion of the Eastern and Western Churches.

Peter of Tarantaise was elevated to the cardinalate on 3 June 1273, in a Consistory held at Orvieto by Pope Gregory X, and named Bishop of the suburbicarian See of Ostia. He participated in the Second Ecumenical Council of Lyons. During the Council, he sang the Funeral Mass and delivered the sermon at the funeral of Cardinal Bonaventure, Bishop of Albano, who had died on 15 July 1274, and was buried on the same day in the Church of the Franciscans in Lyons. Pope Gregory, the Fathers of the Council and the Roman Curia all attended. After the conclusion of the Council, Pope Gregory spent the Autumn and Winter in Lyons. He and his suite departed Lyons in May, 1275; he left Vienne shortly after 30 September 1275, and arrived in Lausanne on 6 October. There he met with the Emperor-elect Rudolph, King of the Romans, and on October 20 received his oath of fealty. There were seven cardinals with the Pope at the time, and their names are mentioned in the record of the oath-taking: Petrus Ostiensis, Ancherus Pantaleone of S. Prassede, Guglelmus de Bray of S. Marco, Ottobono Fieschi of S. Adriano, Giacomo Savelli of S. Maria in Cosmedin, Gottifridus de Alatri of S. Giorgio in Velabro, and Mattheus Rosso Orsini of S. Maria in Porticu. The party reached Milan on Tuesday, 12 November 1275, and Florence on 18 December. The papal party reached Arezzo in time for Christmas, but the Pope was weak and ill. The stay in Arezzo was prolonged until Gregory X died, on 10 January 1276. Only three cardinals were at his deathbed: Peter of Tarantaise, Peter Juliani of Tusculum, and Bertrand de Saint-Martin of Sabina, all cardinal-bishops. According to the Constitution "Ubi Periculum" which had been approved by the Council of Lyons, the Conclave to elect his successor should begin ten days after the pope's death.

After the required ten days had passed, the Cardinals assembled on the Vigil of St. Agnes (20 January) to hear the customary Mass of the Holy Spirit. There were twelve cardinals present. Two cardinals, Simon de Brion, who was Papal Legate in France, and Giovanni Gaetano Orsini, did not attend. The next morning, 21 January, Cardinal Petrus was the unanimous choice of the electors, on the first ballot (scrutiny). Peter of Tarantaise was the first Dominican to become Pope. He chose the pontifical name of "Innocent". His decision was to be crowned in Rome, which had not seen a pope since the departure of Gregory X in the third week of June, 1272. By 7 February the Papal Curia had reached Viterbo. King Charles of Naples rode up to Viterbo to meet the new Pope and escort him to Rome. On 22 February 1276, the Feast of S. Peter's Chair, he was crowned at the Vatican Basilica by Cardinal Giovanni Gaetano Orsini.

On 2 March 1276, Pope Innocent granted to King Charles I of Sicily the privilege of retaining the Senatorship of Rome, the government of the city, and the Rectorship of Tuscia. In a letter of 4 March, the Pope testifies that King Charles had sworn fealty for the Kingdom of Naples and of Sicily. On 9 March, he wrote to Rudolf, King of the Romans, begging him not to come to Italy, and if he had already started his journey, to break it off, until an agreement between him and the Papacy could be finalized. This meant that Rudolf's coronation, which had been agreed to by Gregory X, would not immediately take place. On the 17th, he wrote to the King of the Romans again, advising him to meet with the papal nuncios, and that, in their negotiations, he should by no means introduce the topic of the Exarchate of Ravenna, the Pentapolis, and the Romandiola. This looked like extortion. The French Innocent's favoritism toward King Charles, the brother of Louis IX and uncle of Philip III, and his harshness toward Rudolf was beginning to change the balance of power in Italy again, and was pointing in the direction of war. Pope Gregory's efforts to bring about peace had been ruined.

On the 26th he ordered the Bishops of Parma and Comacchio to see to it that Boniface de Lavania (Lavagna) be installed as Archbishop of Ravenna, as Pope Gregory X had decided. Innocent was able to arrange a peace treaty between Genoa and King Charles I, which was signed on 18 June 1276.

On 18 May 1276, Pope Innocent V notified King Philip III of France that he had appointed his friend Fr. Guy de Sully, OP, the Dominican Provincial of Paris (a post that Innocent himself had held until 1272, when he was appointed Archbishop of Lyon), to the See of Bourges.
A noteworthy feature of his brief pontificate was the practical form assumed by his desire for reunion with the Eastern Church. He wrote to Michael Palaeologus, informing him of the death of Gregory X, and apologizing for the fact that the Emperor's representatives, George, the Archdeacon of Constantinople, and Theodore, the Dispensator of the Imperial Curia, had not yet been released to return to Constantinople. He was proceeding to send legates to Michael VIII Palaeologus, the Byzantine emperor, in connection with the recent decisions of the Second Council of Lyons, hoping to broker a peace between Constantinople and King Charles I of Sicily. King Charles, however, was interested in conquest, not in concord. Innocent was interested in sending people to negotiate the reunion. He appointed Fr. Bartolommeo, O.Min., of Bologna, a Doctor of Sacred Scripture, to travel to the East, but he ordered him to come to Rome first, so that a suitable suite could be chosen for him.

Death intervened. Pope Innocent V died at Rome on 22 June 1276, after a reign of five months and one (or two) days. He was buried in the Lateran Basilica, in a magnificent tomb built by King Charles. Unfortunately, the tomb was destroyed by the two fourteenth century fires at the Basilica, in 1307 and 1361.

Innocent V had created no new cardinals at all, and therefore the cast of characters at the Conclave of July, 1276, was the same as in January. King Charles, however, was in Rome the entire time, and was in the position as Senator of Rome, to be the Governor of the Conclave. His wishes could not be ignored.

Pope Innocent V was the author of several works of philosophy, theology, and canon law, including commentaries on the Pauline epistles, and on the "Sentences" of Peter Lombard. He is sometimes referred to as "famosissimus doctor".

Pope Leo XIII beatified Peter of Tarantaise (Innocent V) on 9 March 1898, on account of his reputation for holiness and saintliness.




 


</doc>
<doc id="24437" url="https://en.wikipedia.org/wiki?curid=24437" title="Paranasal sinuses">
Paranasal sinuses

Paranasal sinuses are a group of four paired air-filled spaces that surround the nasal cavity. The maxillary sinuses are located under the eyes; the frontal sinuses are above the eyes; the ethmoidal sinuses are between the eyes and the sphenoidal sinuses are behind the eyes. The sinuses are named for the facial bones in which they are located.

Humans possess four paired paranasal sinuses, divided into subgroups that are named according to the bones within which the sinuses lie:

The paranasal air sinuses are lined with respiratory epithelium (ciliated pseudostratified columnar epithelium).

Paranasal sinuses form developmentally through excavation of bone by air-filled sacs (pneumatic diverticula) from the nasal cavity. This process begins prenatally (intrauterine life), and it continues through the course of an organism's lifetime.

The results of experimental studies suggest that the natural ventilation rate of a sinus with a single sinus ostium (opening) is extremely slow. Such limited ventilation may be protective for the sinus, as it would help prevent drying of its mucosal surface and maintain a near-sterile environment with high carbon dioxide concentrations and minimal pathogen access. Thus composition of gas content in the maxillary sinus is similar to venous blood, with high carbon dioxide and lower oxygen levels compared to breathing air.

At birth only the maxillary sinus and the ethmoid sinus are developed but not yet pneumatized; only by the age of seven they are fully aerated. The sphenoid sinus appears at the age of three, and the frontal sinuses first appear at the age of six, and fully develop during adulthood.

The paranasal sinuses are joined to the nasal cavity via small orifices called ostia. These become blocked easily by allergic inflammation, or by swelling in the nasal lining that occurs with a cold. If this happens, normal drainage of mucus within the sinuses is disrupted, and sinusitis may occur. Because the maxillary posterior teeth are close to the maxillary sinus, this can also cause clinical problems if any disease processes are present, such as an infection in any of these teeth. These clinical problems can include secondary sinusitis, the inflammation of the sinuses from another source such as an infection of the adjacent teeth.

These conditions may be treated with drugs such as decongestants, which cause vasoconstriction in the sinuses; reducing inflammation; by traditional techniques of nasal irrigation; or by corticosteroid.

Malignancies of the paranasal sinuses comprise approximately 0.2% of all malignancies. About 80% of these malignancies arise in the maxillary sinus. Men are much more often affected than women. They most often occur in the age group between 40 and 70 years. Carcinomas are more frequent than sarcomas. Metastases are rare. Tumours of the sphenoid and frontal sinuses are extremely rare.

Sinus is a Latin word meaning a "fold", "curve", or "bay". Compare "sine".

Paranasal sinuses occur in many other animals, including most mammals, birds, non-avian dinosaurs, and crocodilians. The bones occupied by sinuses are quite variable in these other species.


</doc>
<doc id="24438" url="https://en.wikipedia.org/wiki?curid=24438" title="PAL">
PAL

Phase Alternating Line (PAL) is a colour encoding system for analogue television used in broadcast television systems in most countries broadcasting at 625-line / 50 field (25 frame) per second (576i). It was one of three major analogue colour television standards, the others being NTSC and SECAM. 

Almost all of the countries using PAL are currently in the process of conversion, or have already converted transmission standards to DVB, ISDB or DTMB.

This page primarily discusses the PAL colour encoding system. The articles on broadcast television systems and analogue television further describe frame rates, image resolution and audio modulation.

PAL was adopted by most European countries, by all African countries that had never been a Belgian or French colony, by Argentina, Brazil, Paraguay, and Uruguay, by most of Asia, and by Oceania.

There were some countries in those regions that did not adopt PAL. They were France, most ex-Soviet states, Japan, Myanmar, the Philippines, and Taiwan.

In the 1950s, the Western European countries began plans to introduce colour television, and were faced with the problem that the NTSC standard demonstrated several weaknesses, including colour tone shifting under poor transmission conditions, which became a major issue considering Europe's geographical and weather-related particularities. To overcome NTSC's shortcomings, alternative standards were devised, resulting in the development of the PAL and SECAM standards. The goal was to provide a colour TV standard for the European picture frequency of 50 fields per second (50 hertz), and finding a way to eliminate the problems with NTSC.

PAL was developed by Walter Bruch at Telefunken in Hanover, West Germany, with important input from Dr. Kruse and . The format was patented by Telefunken in 1962, citing Bruch as inventor, and unveiled to members of the European Broadcasting Union (EBU) on 3 January 1963. When asked why the system was named "PAL" and not "Bruch" the inventor answered that a "Bruch system" would probably not have sold very well ("Bruch" is the German word for "breakage"). The first broadcasts began in the United Kingdom in June 1967, followed by West Germany later that year. The one BBC channel initially using the broadcast standard was BBC2, which had been the first UK TV service to introduce "625-lines" in 1964. Telefunken PALcolour 708T was the first PAL commercial TV set. It was followed by Loewe-Farbfernseher S 920 & F 900.

Telefunken was later bought by the French electronics manufacturer Thomson. Thomson also bought the "Compagnie Générale de Télévision" where Henri de France developed SECAM, the first European Standard for colour television. Thomson, now called Technicolour SA, also owns the RCA brand and licences it to other companies; Radio Corporation of America, the originator of that brand, created the NTSC colour TV standard before Thomson became involved.

The term PAL was often used informally and somewhat imprecisely to refer to the 625-line/50 Hz (576i) television system in general, to differentiate from the 525-line/60 Hz (480i) system generally used with NTSC. Accordingly, DVDs were labelled as PAL or NTSC (referring to the line count and frame rate) even though technically the discs carry neither PAL nor NTSC encoded signal. CCIR 625/50 and EIA 525/60 are the proper names for these (line count and field rate) standards; PAL and NTSC on the other hand are methods of encoding colour information in the signal.

Both the PAL and the NTSC system use a quadrature amplitude modulated subcarrier carrying the chrominance information added to the luminance video signal to form a composite video baseband signal. The frequency of this subcarrier is 4.43361875 MHz for PAL 4.43, compared to 3.579545 MHz for NTSC 3.58. The SECAM system, on the other hand, uses a frequency modulation scheme on its two line alternate colour subcarriers 4.25000 and 4.40625 MHz.

The name "Phase Alternating Line" describes the way that the phase of part of the colour information on the video signal is reversed with each line, which automatically corrects phase errors in the transmission of the signal by cancelling them out, at the expense of vertical frame colour resolution. Lines where the colour phase is reversed compared to NTSC are often called PAL or phase-alternation lines, which justifies one of the expansions of the acronym, while the other lines are called NTSC lines. Early PAL receivers relied on the human eye to do that cancelling; however, this resulted in a comb-like effect known as Hanover bars on larger phase errors. Thus, most receivers now use a chrominance analogue delay line, which stores the received colour information on each line of display; an average of the colour information from the previous line and the current line is then used to drive the picture tube. The effect is that phase errors result in saturation changes, which are less objectionable than the equivalent hue changes of NTSC. A minor drawback is that the vertical colour resolution is poorer than the NTSC system's, but since the human eye also has a colour resolution that is much lower than its brightness resolution, this effect is not visible. In any case, NTSC, PAL, and SECAM all have chrominance bandwidth (horizontal colour detail) reduced greatly compared to the luminance signal.

The 4.43361875 MHz frequency of the colour carrier is a result of 283.75 colour clock cycles per line plus a 25 Hz offset to avoid interferences. Since the line frequency (number of lines per second) is 15625 Hz (625 lines × 50 Hz ÷ 2), the colour carrier frequency calculates as follows: 4.43361875 MHz = 283.75 × 15625 Hz + 25 Hz.
The frequency 50 Hz is the optional refresh frequency of the monitor to be able to create an illusion of motion, while 625 lines means the vertical lines or resolution that the PAL system supports.
The original colour carrier is required by the colour decoder to recreate the colour difference signals. Since the carrier is not transmitted with the video information it has to be generated locally in the receiver. In order that the phase of this locally generated signal can match the transmitted information, a 10 cycle burst of colour subcarrier is added to the video signal shortly after the line sync pulse, but before the picture information, during the so-called back porch. This colour burst is not actually in phase with the original colour subcarrier, but leads it by 45 degrees on the odd lines and lags it by 45 degrees on the even lines. This swinging burst enables the colour decoder circuitry to distinguish the phase of the R-Y vector which reverses every line.

PAL usually has 576 visible lines compared with 480 lines with NTSC, meaning that PAL has a 20% higher resolution, in fact it even has a higher resolution than Enhanced Definition standard (852x480). Most TV output for PAL and NTSC use interlaced frames meaning that even lines update on one field and odd lines update on the next field. Interlacing frames gives a smoother motion with half the frame rate. NTSC is used with a frame rate of 60i or 30p whereas PAL generally uses 50i or 25p; both use a high enough frame rate to give the illusion of fluid motion. This is due to the fact that NTSC is generally used in countries with a utility frequency of 60 Hz and PAL in countries with 50 Hz, although there are many exceptions. Both PAL and NTSC have a higher frame rate than film which uses 24 frames per second. PAL has a closer frame rate to that of film, so most films are sped up 4% to play on PAL systems, shortening the runtime of the film and, without adjustment, slightly raising the pitch of the audio track. Film conversions for NTSC instead use 3:2 pull down to spread the 24 frames of film across 60 interlaced fields. This maintains the runtime of the film and preserves the original audio, but may cause worse interlacing artefacts during fast motion.

NTSC receivers have a tint control to perform colour correction manually. If this is not adjusted correctly, the colours may be faulty. The PAL standard automatically cancels hue errors by phase reversal, so a tint control is unnecessary yet Saturation control can be more useful. Chrominance phase errors in the PAL system are cancelled out using a 1H delay line resulting in lower saturation, which is much less noticeable to the eye than NTSC hue errors.

However, the alternation of colour information—Hanover bars—can lead to picture grain on pictures with extreme phase errors even in PAL systems, if decoder circuits are misaligned or use the simplified decoders of early designs (typically to overcome royalty restrictions). In most cases such extreme phase shifts do not occur. This effect will usually be observed when the transmission path is poor, typically in built up areas or where the terrain is unfavourable. The effect is more noticeable on UHF than VHF signals as VHF signals tend to be more robust.

In the early 1970s some Japanese set manufacturers developed decoding systems to avoid paying royalties to Telefunken. The Telefunken licence covered any decoding method that relied on the alternating subcarrier phase to reduce phase errors. This included very basic PAL decoders that relied on the human eye to average out the odd/even line phase errors. One solution was to use a 1H analogue delay line to allow decoding of only the odd or even lines. For example, the chrominance on odd lines would be switched directly through to the decoder and also be stored in the delay line. Then, on even lines, the stored odd line would be decoded again. This method effectively converted PAL to NTSC. Such systems suffered hue errors and other problems inherent in NTSC and required the addition of a manual hue control.

PAL and NTSC have slightly divergent colour spaces, but the colour decoder differences here are ignored.

The SECAM patents predate those of PAL by several years (1956 vs. 1962). Its creator, Henri de France, in search of a response to known NTSC hue problems, came up with ideas that were to become fundamental to both European systems, namely:
1) colour information on two successive TV lines is very similar and vertical resolution can be halved without serious impact on perceived visual quality
2) more robust colour transmission can be achieved by spreading information on two TV lines instead of just one
3) information from the two TV lines can be recombined using a delay line.

SECAM applies those principles by transmitting alternately only one of the U and V components on each TV line, and getting the other from the delay line. QAM is not required, and frequency modulation of the subcarrier is used instead for additional robustness (sequential transmission of U and V was to be reused much later in Europe's last "analog" video systems: the MAC standards).

SECAM is free of both hue and saturation errors. It is not sensitive to phase shifts between the colour burst and the chrominance signal, and for this reason was sometimes used in early attempts at colour video recording, where tape speed fluctuations could get the other systems into trouble. In the receiver, it did not require a quartz crystal (which was an expensive component at the time) and generally could do with lower accuracy delay lines and components.

SECAM transmissions are more robust over longer distances than NTSC or PAL. However, owing to their FM nature, the colour signal remains present, although at reduced amplitude, even in monochrome portions of the image, thus being subject to stronger cross colour.

One serious drawback for studio work is that the addition of two SECAM signals does not yield valid colour information, due to its use of frequency modulation. It was necessary to demodulate the FM and handle it as AM for proper mixing, before finally remodulating as FM, at the cost of some added complexity and signal degradation. In its later years, this was no longer a problem, due to the wider use of component and digital equipment.

PAL can work without a delay line, but this configuration, sometimes referred to as "poor man's PAL", could not match SECAM in terms of picture quality. To compete with it at the same level, it had to make use of the main ideas outlined above, and as a consequence PAL had to pay licence fees to SECAM. Over the years, this contributed significantly to the estimated 500 million francs gathered by the SECAM patents (for an initial 100 million francs invested in research).

Hence, PAL could be considered as a hybrid system, with its signal structure closer to NTSC, but its decoding borrowing much from SECAM.

There were initial specifications to use colour with the French 819 line format (system E). However, "SECAM E" only ever existed in development phases. Actual deployment used the 625 line format. This made for easy interchange and conversion between PAL and SECAM in Europe. Conversion was often not even needed, as more and more receivers and VCRs became compliant with both standards, helped in this by the common decoding steps and components. When the SCART plug became standard, it could take RGB as an input, effectively bypassing all the colour coding formats' peculiarities.

When it comes to home VCRs, all video standards use what is called "colour under" format. Colour is extracted from the high frequencies of the video spectrum, and moved to the lower part of the spectrum available from tape. Luminance then uses what remains of it, above the colour frequency range. This is usually done by heterodyning for PAL (as well as NTSC). But the FM nature of colour in SECAM allows for a cheaper trick: division by 4 of the subcarrier frequency (and multiplication on replay). This became the standard for SECAM VHS recording in France. Most other countries kept using the same heterodyning process as for PAL or NTSC and this is known as MESECAM recording (as it was more convenient for some Middle East countries that used both PAL and SECAM broadcasts).
Another difference in colour management is related to the proximity of successive tracks on the tape, which is a cause for chroma crosstalk in PAL. A cyclic sequence of 90° chroma phase shifts from one line to the next is used to overcome this problem. This is not needed in SECAM, as FM provides sufficient protection.

Regarding early (analogue) videodiscs, the established Laserdisc standard supported only NTSC and PAL. However, a different optical disc format, the Thomson transmissive optical disc made a brief appearance on the market. At some point, it used a modified SECAM signal (single FM subcarrier at 3.6 MHz). The media's flexible and transmissive material allowed for direct access to both sides without flipping the disc, a concept that reappeared in multi-layered DVDs about fifteen years later.

For PAL-B/G the signal has these characteristics.

After 0.9 µs a colourburst of cycles is sent. Most rise/fall times are in range. Amplitude is 100% for white level, 30% for black, and 0% for sync.
The CVBS electrical amplitude is Vpp and impedance of 75 Ω.

The vertical timings are:

As PAL is interlaced, every two fields are summed to make a complete picture frame.

Luminance, formula_1, is derived from red, green, and blue (formula_2) signals:

formula_4 and formula_5 are used to transmit chrominance. Each has a typical bandwidth of 1.3 MHz.

Composite PAL signal formula_8timing where formula_9.

Subcarrier frequency formula_10 is 4.43361875 MHz (±5 Hz) for PAL-B/D/G/H/I/N.

Many countries have turned off analogue transmissions, so the following does not apply anymore, except for using devices which output broadcast signals, such as video recorders.

The majority of countries using or having used PAL have television standards with 625 lines and 50 fields per second, differences concern the audio carrier frequency and channel bandwidths. The variants are:


Systems B and G are similar. System B specifies 7 MHz channel bandwidth, while System G specifies 8 MHz channel bandwidth. Australia used System B for VHF and UHF channels. Similarly, Systems D and K are similar except for the bands they use: System D is only used on VHF (except in mainland China), while System K is only used on UHF. Although System I is used on both bands, it has only been used on UHF in the United Kingdom.

In Brazil, PAL is used in conjunction with the 525 line, 59.94 field/s system M, using (very nearly) the NTSC colour subcarrier frequency. Exact colour subcarrier frequency of PAL-M is 3.575611 MHz, or 227.25 times System M's horizontal scan frequency. Almost all other countries using system M use NTSC.

The PAL colour system (either baseband or with any RF system, with the normal 4.43 MHz subcarrier unlike PAL-M) can also be applied to an NTSC-like 525-line (480i) picture to form what is often known as "PAL-60" (sometimes "PAL-60/525", "Quasi-PAL" or "Pseudo PAL"). PAL-M (a broadcast standard) however should not be confused with "PAL-60" (a video playback system—see below).

In Argentina, Paraguay and Uruguay the PAL-N variant is used. It employs the 625 line/50 field per second waveform of PAL-B/G, D/K, H, and I, but on a 6 MHz channel with a chrominance subcarrier frequency of 3.582056 MHz (917/4*H) very similar to NTSC (910/4*H).

PAL-N uses the YDbDr colour space.

VHS tapes recorded from a PAL-N or a PAL-B/G, D/K, H, or I broadcast are indistinguishable because the downconverted subcarrier on the tape is the same. A VHS recorded off TV (or released) in Europe will play in colour on any PAL-N VCR and PAL-N TV in Argentina, Paraguay and Uruguay. Likewise, any tape recorded in Argentina, Paraguay or Uruguay off a PAL-N TV broadcast can be sent to anyone in European countries that use PAL (and Australia/New Zealand, etc.) and it will display in colour. This will also play back successfully in Russia and other SECAM countries, as the USSR mandated PAL compatibility in 1985—this has proved to be very convenient for video collectors.

People in Argentina, Paraguay and Uruguay usually own TV sets that also display NTSC-M, in addition to PAL-N. Direct TV also conveniently broadcasts in NTSC-M for North, Central, and South America. Most DVD players sold in Argentina, Paraguay and Uruguay also play PAL discs—however, this is usually output in the European variant (colour subcarrier frequency 4.433618 MHz), so people who own a TV set which only works in PAL-N (plus NTSC-M in most cases) will have to watch those PAL DVD imports in black and white (unless the TV supports RGB SCART) as the colour subcarrier frequency in the TV set is the PAL-N variation, 3.582056 MHz.

In the case that a VHS or DVD player works in PAL (and not in PAL-N) and the TV set works in PAL-N (and not in PAL), there are two options:


Some DVD players (usually lesser known brands) include an internal transcoder and the signal can be output in NTSC-M, with some video quality loss due to the system's conversion from a 625/50 PAL DVD to the NTSC-M 525/60 output format. A few DVD players sold in Argentina, Paraguay and Uruguay also allow a signal output of NTSC-M, PAL, or PAL-N. In that case, a PAL disc (imported from Europe) can be played back on a PAL-N TV because there are no field/line conversions, quality is generally excellent.

Extended features of the PAL specification, such as Teletext, are implemented quite differently in PAL-N. PAL-N supports a modified 608 closed captioning format that is designed to ease compatibility with NTSC originated content carried on line 18, and a modified teletext format that can occupy several lines.

Some special VHS video recorders are available which can allow viewers the flexibility of enjoying PAL-N recordings using a standard PAL ( 625/50 Hz ) colour TV, or even through multi-system TV sets. Video recorders like Panasonic NV-W1E (AG-W1 for the US), AG-W2, AG-W3, NV-J700AM, Aiwa HV-M110S, HV-M1U, Samsung SV-4000W and SV-7000W feature a digital TV system conversion circuitry.

The PAL L (Phase Alternating Line with L-sound system) standard uses the same video system as PAL-B/G/H (625 lines, 50 Hz field rate, 15.625 kHz line rate), but with 6 MHz video bandwidth rather than 5.5 MHz. This requires the audio subcarrier to be moved to 6.5 MHz. An 8 MHz channel spacing is used for PAL-L.

The BBC tested their pre-war 405 line monochrome system with all three colour standards including PAL, before the decision was made to abandon 405 and transmit colour on 625/System I only.

The PAL colour system is usually used with a video format that has 625 lines per frame (576 visible lines, the rest being used for other information such as sync data and captioning) and a refresh rate of 50 interlaced fields per second (compatible with 25 full frames per second), such systems being B, G, H, I, and N (see broadcast television systems for the technical details of each format).

This ensures video interoperability. However, as some of these standards (B/G/H, I and D/K) use different sound carriers (5.5 MHz, 6.0 MHz 6.5 MHz respectively), it may result in a video image without audio when viewing a signal broadcast over the air or cable. Some countries in Eastern Europe which formerly used SECAM with systems D and K have switched to PAL while leaving other aspects of their video system the same, resulting in the different sound carrier. Instead, other European countries have changed completely from SECAM-D/K to PAL-B/G.

The PAL-N system has a different sound carrier, and also a different colour subcarrier, and decoding on incompatible PAL systems results in a black-and-white image without sound. 
The PAL-M system has a different sound carrier and a different colour subcarrier, and does not use 625 lines or 50 frames/second. This would result in no video or audio at all when viewing a European signal.

Recently manufactured PAL television receivers can typically decode all of these systems except, in some cases, PAL-M and PAL-N. Many of receivers can also receive Eastern European and Middle Eastern SECAM, though rarely French-broadcast SECAM (because France used a quasi-unique positive video modulation, system L) unless they are manufactured for the French market. They will correctly display plain CVBS or S-video SECAM signals. Many can also accept baseband NTSC-M, such as from a VCR or game console, and RF modulated NTSC with a PAL standard audio subcarrier (i.e., from a modulator), though not usually broadcast NTSC (as its 4.5 MHz audio subcarrier is not supported). Many sets also support NTSC with a 4.43 MHz subcarrier.

Many 1990s-onwards videocassette recorders sold in Europe can play back NTSC tapes. When operating in this mode most of them do not output a true (625/25) PAL signal, but rather a hybrid consisting of the original NTSC line standard (525/30), but with colour converted to PAL 4.43 MHz—this is known as "PAL 60" (also "quasi-PAL" or "pseudo-PAL") with "60" standing for 60 Hz (for 525/30), instead of 50 Hz (for 625/25). Some video game consoles also output a signal in this mode. Notably, the PlayStation 2 did not actually offer a true PAL 60 mode; while many PlayStation 2 games did offer a "PAL 60" mode as an option, the console would in fact generate an NTSC signal during 60 Hz operation. Most newer television sets can display such a signal correctly, but some will only do so (if at all) in black and white and/or with flickering/foldover at the bottom of the picture, or picture rolling (however, many old TV sets can display the picture properly by means of adjusting the V-Hold and V-Height knobs—assuming they have them). Some TV tuner cards or video capture cards will support this mode (although software/driver modification can be required and the manufacturers' specs may be unclear). A "PAL 60" signal is similar to an NTSC (525/30) signal, but with the usual PAL chrominance subcarrier at 4.43 MHz (instead of 3.58 as with NTSC and South American PAL variants) and with the PAL-specific phase alternation of the red colour difference signal between the lines.

Some DVD players offer a choice of PAL vs NTSC output for NTSC discs.

Below countries and territories currently use or once used the PAL system. Many of these have converted or are currently converting PAL to DVB-T (most countries), DVB-T2 (most countries), DTMB (China, Hong Kong and Macau) or ISDB (Sri Lanka, Maldives, Botswana and part of South America).





The following countries no longer use PAL for terrestrial broadcasts, and are in process of converting from PAL (cable) to DVB-T.

The PAL system is analog. There was an attempt to manufacture equipment that digitizes the PAL signal in the 1980s, but it was not commercially successful. Digital devices such as digital television, modern game consoles, computers, etc., use color component systems where the R, G, and B signals are transmitted over three different cables, or Y (luminance), RY, and BY (difference from color). In these cases only the number of total horizontal lines is taken into account—625 in digital PAL and 525 in NTSC—and the frame rate—25 frames/s in PAL Digital and 30 frames/s in digital NTSC. Systems using the MPEG-2 standard, such as DVD and satellite television, cable television, or digital terrestrial television (DTT) have practically nothing to do with PAL.</div>




</doc>
<doc id="24442" url="https://en.wikipedia.org/wiki?curid=24442" title="Philemon">
Philemon

Philemon may refer to:






</doc>
<doc id="24443" url="https://en.wikipedia.org/wiki?curid=24443" title="Polo">
Polo

Polo is a horseback mounted team sport. It is one of the world's oldest known team sports.

The concept of the game and its variants date back from the 6th century BC to the 1st century AD. The sport originated from equestrian games played by nomadic Iranian peoples and Turkic peoples. Polo was at first a training game for cavalry units, usually the Persian king’s guard or other elite troops. A notable example is Saladin, who was known for being a skilled polo player which contributed to his cavalry training. It is now popular around the world, with well over 100 member countries in the Federation of International Polo. It is played professionally in 16 countries. It was an Olympic sport from 1900 to 1936.

Polo has been called "the sport of kings". It has become a spectator sport for equestrians and society, often supported by sponsorship.

The game is played by two opposing teams with the objective of scoring goals by using a long-handled wooden mallet to hit a small hard ball through the opposing team's goal. Each team has four mounted riders, and the game usually lasts one to two hours, divided into periods called chukkas (or "chukkers").

Arena polo has similar rules, and is played with three players per team. The playing area is smaller, enclosed, and usually of compacted sand or fine aggregate, often indoors. Arena polo has more maneuvering due to space limitations, and uses an air inflated ball, slightly larger than the hard field polo ball. Standard mallets are used, though slightly larger head "arena mallets" are an option.

Although the exact origins of the game are unknown, it most likely began as a simple game played by mounted Iranian and Turkic nomads in Central Asia, with the current form originating in Iran (Persia) and spreading east and west. In time polo became a Persian national sport played extensively by the nobility. Women played as well as men. During the period of the Parthian Empire (247 BC to 224 AD), the sport had great patronage under the kings and noblemen. According to the "Oxford Dictionary of Late Antiquity", polo (known as "čowgān" in Middle Persian, i.e. chovgan), was a Persian ball game and an important pastime in the court of the Sasanian Empire (224–651). It was also part of royal education for the Sasanian ruling class. Emperor Shapur II learnt to play polo when he was seven years old in 316 AD. Known as "chowgan", it is still played in the region today.

Valuable for training cavalry, the game was played from Constantinople to Japan by the Middle Ages. The game also spread south to Arabia and to India and Tibet.

Abbasid Baghdad had a large polo ground outside its walls, and one of the city's early 13th century gates, the Bab al Halba, was named after these nearby polo grounds. The game continued to be supported by Mongol rulers of Persia in the 13th century, as well as under the Safavid dynasty. In the 17th century, Naqsh-i Jahan Square in Isfahan was built as a polo field by King Abbas I. The game was also learnt by the neighbouring Byzantine Empire at an early date. A "tzykanisterion" (stadium for playing "tzykanion", the Byzantine name for polo) was built by emperor Theodosius II (r. 408–450) inside the Great Palace of Constantinople. Emperor Basil I (r. 867–886) excelled at it; Emperor Alexander (r. 912–913) died from exhaustion while playing and John I of Trebizond (r. 1235–1238) died from a fatal injury during a game.

After the Muslim conquests to the Ayyubid and Mameluke dynasties of Egypt and the Levant, their elites favoured it above all other sports. Notable sultans such as Saladin and Baybars were known to play it and encourage it in their court. Polo sticks were featured on the Mamluk precursor to modern-day playing cards.

The game spread to South Asia where it has had a strong presence in the north western areas of present-day Pakistan (including Gilgit, Chitral, Hunza and Baltistan) since at least the 15th–16th century. The name "polo" is said to have been derived from the Balti word "pulu", meaning ball. Qutubuddin Aibak, the Turkic slave from Central Asia who later became the Sultan of Delhi in Northern India from 1206 to 1210, suffered an accidental death during a game of polo when his horse fell and he was impaled on the pommel of his saddle. Polo likely travelled via the Silk Road to China where it was popular in the Tang dynasty capital of Chang'an, and also played by women, who wore male dress to do so; many Tang dynasty tomb figures of female players survive. According to the Oxford Dictionary of Late Antiquity, the popularity of polo in Tang China was "bolstered, no doubt, by the presence of the Sasanian court in exile".

A polo-obsessed noblewoman was buried with her donkeys on 6 October 878 AD in Xi’an, China.

An archaic variation of polo, regionally referred to as "buzkashi" or "kokpar", is still played in parts of Asia.

The modern game of polo is derived from Manipur, India, where the game was known as 'sagol kangjei', 'kanjai-bazee', or 'pulu'. It was the anglicised form of the last, referring to the wooden ball that was used, which was adopted by the sport in its slow spread to the west. The first polo club was established in the town of Silchar in Assam, India, in 1833.

The origins of the game in Manipur are traced to early precursors of Sagol Kangjei. This was one of three forms of hockey in Manipur, the other ones being field hockey (called khong kangjei) and wrestling-hockey (called mukna kangjei). Local rituals such as those connected to the Marjing, the winged-pony god of polo and the creation-ritual episodes of the Lai Haraoba festival enacting the life of his son, Khoriphaba, the polo-playing god of sports. These may indicate an origin earlier than the historical records of Manipur. Later, according to "Chaitharol-Kumbaba", a royal chronicle of King Kangba who ruled Manipur much earlier than Nongda Lairen Pakhangba (33 AD) introduced sagol kangjei (kangjei on horseback). Further regular playing of this game commenced in 1605 during the reign of King Khagemba under newly framed rules of the game. However it was the first Mughal emperor, Babur, who popularised the sport in India and ultimately made a significant influence on England.
In Manipur, polo is traditionally played with seven players to a side. The players are mounted on the indigenous Manipuri pony, which stands less than . There are no goal posts, and a player scores simply by hitting the ball out of either end of the field. Players strike the ball with the long side of the mallet head, not the end. Players are not permitted to carry the ball, although blocking the ball with any part of the body except the open hand is permitted. The sticks are made of cane, and the balls are made from the roots of bamboo. Players protected their legs by attaching leather shields to their saddles and girths.

In Manipur, the game was played even by commoners who owned a pony. The kings of Manipur had a royal polo ground within the ramparts of their Kangla Fort. Here they played manung kangjei bung (literally, "inner polo ground"). Public games were held, as they are still today, at the Mapan Kangjei Bung (literally "Outer Polo Ground"), a polo ground just outside the Kangla. Weekly games called Hapta Kangjei (Weekly Polo) were also played in a polo ground outside the current Palace.

The oldest polo ground in the world is the Imphal Polo Ground in Manipur State. The history of this polo ground is contained in the royal chronicle "Cheitharol Kumbaba" starting from AD 33. Lieutenant (later Major General) Joseph Ford Sherer, the father of modern polo visited the state and played on this polo ground in the 1850s. Lord Curzon, the Viceroy of India visited the state in 1901 and measured the polo ground as "225 yards long and 110 yards wide" .

The Cachar Club established in 1859 is located on Club Road in the heart of Silchar city in Assam. In 1862 the oldest polo club still in existence, Calcutta Polo Club, was established by two British soldiers, Sherer and Captain Robert Stewart. Later they spread the game to their peers in England. The British are credited with spreading polo worldwide in the late 19th century and the early 20th century at the height of its empire. Military officers imported the game to Britain in the 1860s. The establishment of polo clubs throughout England and western Europe followed after the formal codification of rules. The 10th Hussars at Aldershot, Hants, introduced polo to England in 1834. The game's governing body in the United Kingdom is the Hurlingham Polo Association, which drew up the first set of formal British rules in 1874, many of which are still in existence.

This version of polo played in the 19th century was different from the faster form that was played in Manipur. The game was slow and methodical, with little passing between players and few set plays that required specific movements by participants without the ball. Neither players nor horses were trained to play a fast, non-stop game. This form of polo lacked the aggressive methods and required fewer equestrian skills. From the 1800s to the 1910s, a host of teams representing Indian principalities dominated the international polo scene.

The Champions polo league was launched in Jaipur in 2016. It is a new version of polo, similar to the T20 format of cricket. The pitch was made smaller and accommodated a huge audience. The first event of the World Champions Polo League took place in Bhavnagar, Gujarat, with six teams and room for 10,000 spectators. The rules were changed and the duration was made shorter.

British settlers in the Argentine pampas started practising polo during their free time. Among them, David Shennan is credited with having organised the first formal polo game of the country in 1875, at Estancia El Negrete, located in the province of Buenos Aires.

The sport spread quickly between the skilful gauchos, and several clubs opened in the following years in the towns of Venado Tuerto, Cañada de Gómez, Quilmes, Flores and later (1888) Hurlingham. In 1892 The River Plate Polo Association was founded and constituted the basis for the current Asociación Argentina de Polo. In the Olympic Games held in Paris in 1924 a team composed by Juan Miles, Enrique Padilla, Juan Nelson, Arturo Kenny, G. Brooke Naylor and A. Peña obtained the first gold medal for the country's olympic history; this also occurred in Berlin 1936 with players Manuel Andrada, Andrés Gazzotti, Roberto Cavanagh, Luis Duggan, Juan Nelson, Diego Cavanagh, and Enrique Alberdi.

The game spread across the country, and Argentina is credited globally as the capital of polo; Argentina is notably the country with the largest number ever of 10 handicap players in the world.

Five teams were able to gather four 10 handicap players each, to make 40 handicap teams: Coronel Suárez, 1975, 1977–1979 (Alberto Heguy, Juan Carlos Harriott, Alfredo Harriot and Horacio Heguy); La Espadaña, 1989–1990 (Carlos Gracida, Gonzalo Pieres, Alfonso Pieres y Ernesto Trotz Jr.); Indios Chapaleufú, 1992–1993 (Bautista Heguy, Gonzalo Heguy, Horacio Heguy Jr. and Marcos Heguy); La Dolfina, 2009–2010 (Adolfo Cambiaso Jr., Lucas Monteverde, Mariano Aguerre y Bartolomé Castagnola); Ellerstina, 2009 (Facundo Pieres, Gonzalo Pieres Jr., Pablo Mac Donough and Juan Martín Nero).

The three major polo tournaments in Argentina, known as "Triple Corona" ("Triple Crown"), are Hurlingham Polo Open, Tortugas Polo Open and Palermo Polo Open. Polo season usually lasts from October to December.

Polo has found popularity throughout the rest of the Americas, including Brazil, Chile, Mexico, and the United States of America.

James Gordon Bennett Jr. on 16 May 1876 organised what was billed as the first polo match in the United States at Dickel's Riding Academy at 39th Street and Fifth Avenue in New York City. The historical record states that James Gordon Bennett established the Westchester Polo Club on 6 May 1876, and on 13 May 1876, the Jerome Park Racetrack in Westchester County (now Bronx County) was the site of the "first" American outdoor polo match.

H. L. Herbert, James Gordon Bennett and August Belmont financed the original New York Polo Grounds. Herbert stated in a 1913 article that they formed the Westchester Club "after" the "first" outdoor game was played on 13 May 1876. This contradicts the historical record of the club being established before the Jerome Park game.

There is ample evidence that the first to play polo in America were actually the English Texans. "The Galveston News" reported on 2 May 1876 that Denison Texas had a polo club which was before James Gordon Bennett established his Westchester Club or attempted to play the "first" game. The Denison team sent a letter to James Gordon Bennett challenging him to a match. The challenge was published 2 June 1876, in "The Galveston Daily News". By the time the article came out on 2 June, the Denison Club had already received a letter from Bennett indicating the challenge was offered before the "first" games in New York.

There is also an urban legend that the first game of polo in America was played in Boerne, Texas, at retired British officer Captain Glynn Turquand's famous Balcones Ranch The Boerne, Texas, legend also has plenty of evidence pointing to the fact that polo was played in Boerne before James Gordon Bennett Jr. ever picked up a polo mallet.

During the early part of the 20th century, under the leadership of Harry Payne Whitney, polo changed to become a high-speed sport in the United States, differing from the game in England, where it involved short passes to move the ball towards the opposition's goal. Whitney and his teammates used the fast break, sending long passes downfield to riders who had broken away from the pack at a full gallop.

In the late 1950s, champion polo player and Director of the Long Island Polo Association, Walter Scanlon, introduced the "short form", or "European" style, four period match, to the game of polo.
All tournaments and levels of play and players are organized within and between polo clubs, including membership, rules, safety, fields and arenas.

The rules of polo are written for the safety of both players and horses. Games are monitored by umpires. A whistle is blown when an infraction occurs, and penalties are awarded. Strategic plays in polo are based on the "line of the ball", an imaginary line that extends through the ball in the line of travel. This line traces the ball's path and extends past the ball along that trajectory. The line of the ball defines rules for players to approach the ball safely. The "line of the ball" changes each time the ball changes direction. The player who hits the ball generally has the right of way, and other players cannot cross the line of the ball in front of that player. As players approach the ball, they ride on either side of the line of the ball giving each access to the ball. A player can cross the line of the ball when it does not create a dangerous situation. Most infractions and penalties are related to players improperly crossing the line of the ball or the right of way. When a player has the line of the ball on his right, he has the right of way. A "ride-off" is when a player moves another player off the line of the ball by making shoulder-to-shoulder contact with the other players' horses.

The defending player has a variety of opportunities for his team to gain possession of the ball. He can push the opponent off the line or steal the ball from the opponent. Another common defensive play is called "hooking." While a player is taking a swing at the ball, his opponent can block the swing by using his mallet to hook the mallet of the player swinging at the ball. A player may hook only if he is on the side where the swing is being made or directly behind an opponent. A player may not purposely touch another player, his tack or pony with his mallet. Unsafe hooking is a foul that will result in a penalty shot being awarded. For example, it is a foul for a player to reach over an opponent's mount in an attempt to hook.

The other basic defensive play is called the bump or ride-off. It's similar to a body check in hockey. In a ride-off, a player rides his pony alongside an opponent's mount in order to move an opponent away from the ball or to take him out of a play. It must be executed properly so that it does not endanger the horses or the players. The angle of contact must be safe and can not knock the horses off balance, or harm the horses in any way. Two players following the line of the ball and riding one another off have the right of way over a single man coming from any direction.

Like in hockey or basketball, fouls are potentially dangerous plays that infringe on the rules of the game. To the novice spectator, fouls may be difficult to discern. There are degrees of dangerous and unfair play and penalty shots are awarded depending based on the severity of the foul and where the foul was committed on the polo field. White lines on the polo field indicate where the mid-field, sixty, forty and thirty yard penalties are taken.

The official set of rules and rules interpretations are reviewed and published annually by each country's polo association. Most of the smaller associations follow the rules of the Hurlingham Polo Association, the national governing body of the sport of polo in the United Kingdom, and the United States Polo Association.

Outdoor or field polo lasts about one and a half to two hours and consists of four to eight seven-minute chukkas, between or during which players change mounts. At the end of each seven-minute chukka, play continues for an additional 30 seconds or until a stoppage in play, whichever comes first. There is a four-minute interval between chukkas and a ten-minute halftime. Play is continuous and is only stopped for rule infractions, broken tack (equipment) or injury to horse or player. The object is to score goals by hitting the ball between the goal posts, no matter how high in the air. If the ball goes wide of the goal, the defending team is allowed a free 'knock-in' from the place where the ball crossed the goal line, thus getting ball back into play.

Arena polo has rules similar to the field version, and is less strenuous for the player. It is played in a enclosed arena, much like those used for other equestrian sports; the minimum size is . There are many arena clubs in the United States, and most major polo clubs, including the Santa Barbara Polo & Racquet Club, have active arena programmes. The major differences between the outdoor and indoor games are: speed (outdoor being faster), physicality/roughness (indoor/arena is more physical), ball size (indoor is larger), goal size (because the arena is smaller the goal is smaller), and some penalties. In the United States and Canada, collegiate polo is arena polo; in the UK, collegiate polo is both.

Forms of arena polo include beach polo, played in many countries between teams of three riders on a sand surface, and cowboy polo, played almost exclusively in the western United States by teams of five riders on a dirt surface.

Another modern variant is snow polo, which is played on compacted snow on flat ground or a frozen lake. The format of snow polo varies depending on the space available. Each team generally consists of three players and a bright coloured light plastic ball is preferred.

Snow polo is not the same sport as ice polo, which was popular in the US in the late 1890s. The sport resembled ice hockey and bandy but died out entirely in favour of the Canadian ice hockey rules.

A popular combination of the sports of polo and lacrosse is the game of polocrosse, which was developed in Australia in the late 1930s.

These sports are considered as separate sports because of the differences in the composition of teams, equipment, rules, game facilities etc.

Polo is not played exclusively on horseback. Such polo variants are mostly played for recreational or tourist purposes; they include canoe polo, cycle polo, camel polo, elephant polo, golfcart polo, Segway polo and yak polo. In the early 1900s in the United States, cars were used instead of horses in the sport of Auto polo. Hobby Horse Polo is using hobby horses instead of ponies. It uses parts of the polo rules but has its own specialities, as e.g. 'punitive sherries'. The Hobby Horse variant started 1998 as a fun sport in south western Germany and lead 2002 to the foundation of the First Kurfürstlich-Kurpfälzisch Polo-Club in Mannheim. In the meantime it gained further interest in other German cities.

The mounts used are called 'polo ponies', although the term pony is purely traditional and the mount is actually a full-sized horse. They range from high at the withers, and weigh . The polo pony is selected carefully for quick bursts of speed, stamina, agility and manoeuvrability. Temperament is critical; the horse must remain responsive under pressure and not become excited or difficult to control. Many are Thoroughbreds or Thoroughbred crosses. They are trained to be handled with one hand on the reins, and to respond to the rider's leg and weight cues for moving forward, turning and stopping. A well trained horse will carry its rider smoothly and swiftly to the ball and can account for 60 to 75 percent of the player's skill and net worth to his team.

Polo pony training generally begins at age three and lasts from about six months to two years. Most horses reach full physical maturity at about age five, and ponies are at their peak of athleticism and training at around age six or seven. However, without any accidents, polo ponies may have the ability to play until they are 18 to 20 years of age.

Each player must have more than one horse, to allow for tired mounts to be replaced by fresh ones between or even during chukkas. A player's "string" of polo ponies may number two or three in Low Goal matches (with ponies being rested for at least a chukka before reuse), four or more for Medium Goal matches (at least one per chukka), and even more for the highest levels of competition.

Each team consists of four mounted players, which can be mixed teams of both men and women.

Each position assigned to a player has certain responsibilities:


Polo must be played right-handed in order to prevent head-on collisions.

The rules for equipment vary in details between the hosting authorities, but are always for the safety of the players and mounts.

Mandatory equipment includes a protective helmet with chinstrap worn at all times by all players and mounted grooms. They must be to the locally accepted safety standard, "PAS015" (UK), "NOCSAE" (USA). A faceguard is commonly integral with a helmet.

Polo boots and kneeguards are mandatory in the UK during official play, and boots are recommended for all play everywhere. The UK also recommends goggles, elbow pads and gum shields. A shirt or jersey is required that distinguishes the player's team, and is not black and white stripes like an umpire shirt.

White polo pants or trousers are worn during official play. Polo gloves are commonly worn to protect from working the reins and mallet.

Not permitted is any equipment that may harm horses, such as certain spurs or whips.

The modern outdoor polo ball is made of a high-impact plastic. Historically they have been made of bamboo, leather covered cork, hard rubber, and for many years willow root. Originally the British used a white painted leather covered cricket ball.

The regulation outdoor polo ball is to in diameter and weighs to .

Plastic balls were introduced in the 1970s. They are less prone to breakage and much cheaper.

The indoor and arena polo ball is leather-covered and inflated, and is about in diameter.

It must be not less than or more than in circumference. The weight must be not less than or more than . In a bounce test from on concrete at , the rebound should be a minimum of and a maximum of at the inflation rate specified by the manufacturer. This provides for a hard and lively ball.

The polo mallet comprises a cane shaft with a rubber-wrapped grip, a webbed thong, called a sling, for wrapping around the thumb, and a wooden cigar-shaped head. The shaft is made of manau-cane (not bamboo, which is hollow) although a small number of mallets today are made from composite materials. Composite materials are usually not preferred by top players because the shaft of composite mallets can't absorb vibrations as well as traditional cane mallets. The mallet head is generally made from a hardwood called tipa, approximately 9" inches long. The mallet head weighs from to , depending on player preference and the type of wood used, and the shaft can vary in weight and flexibility depending on the player's preference. The weight of the mallet head is of important consideration for the more seasoned players. Female players often use lighter mallets than male players. For some polo players, the length of the mallet depends on the size of the horse: the taller the horse, the longer the mallet. However, some players prefer to use a single length of mallet regardless of the height of the horse. Either way, playing horses of differing heights requires some adjustment by the rider. Variable lengths of the mallet typically range from to . The term "mallet" is used exclusively in US English; British English prefers the term "polo stick". The ball is struck with the broad sides of the mallet head rather than its round and flat tips.

Polo saddles are English-style, close contact, similar to jumping saddles; although most polo saddles lack a flap under the billets. Some players will not use a saddle blanket. The saddle has a flat seat and no knee support; the rider adopting a forward-leaning seat and closed knees dissimilar to a classical dressage seat. A breastplate is added, usually attached to the front billet. A standing martingale must be used: so, a breastplate is a necessity for safety. The tie-down is usually supported by a neck strap. Many saddles also have an overgirth. The stirrup irons are heavier than most, and the stirrup leathers are wider and thicker, for added safety when the player stands in the stirrups. The legs of the pony are wrapped with polo wraps from below the knee to the fetlock to minimize pain. Jumping (open front) or gallop boots are sometimes used along with the polo wraps for added protection. Often, these wraps match the team colours. The pony's mane is most often roached (hogged), and its tail is docked or braided so that it will not snag the rider's mallet.

Polo is ridden with double reins for greater accuracy of signals. The bit is frequently a gag bit or Pelham bit. In both cases, the gag or shank rein will be the bottom rein in the rider's hands, while the snaffle rein will be the top rein. If a gag bit is used, there will be a drop noseband in addition to the cavesson, supporting the tie-down. One of the rein sets may alternately be draw reins.

The playing field is , the area of approximately six soccer fields or 9 American football fields (10 acres)., while arena polo is 96 x 46 metres. The playing field is carefully maintained with closely mowed turf providing a safe, fast playing surface. Goals are posts which are set eight yards apart, centred at each end of the field. The surface of a polo field requires careful and constant grounds maintenance to keep the surface in good playing condition. During half-time of a match, spectators are invited to go onto the field to participate in a polo tradition called "divot stamping", which was developed not only to help replace the mounds of earth (divots) that are torn up by the horses' hooves, but also to afford spectators the opportunity to walk about and socialise.

Polo is played professionally in many countries, notably Argentina, Australia, Brazil, Canada, Chile, Dominican Republic, France, Germany, Iran, India, New Zealand, Mexico, Pakistan, Jamaica, Spain, South Africa, Switzerland, the United Kingdom, and the United States, and is now an active sport in 77 countries. Although its tenure as an Olympic sport was limited to 1900–1939, in 1998 the International Olympic Committee recognised it as a sport with a bona fide international governing body, the Federation of International Polo. The World Polo Championship is held every three years by the Federation.

Polo is unique among team sports in that amateur players, often the team patrons, routinely hire and play alongside the sport's top professionals.

The most important tournaments of the world, at club level, are Abierto de Tortugas, Abierto de Hurlingham and Abierto Argentino de Polo, all of them in Argentina (la "Triple Corona").

Polo has been played in Malaysia and Singapore, both of which are former British colonies, since being introduced to Malaya during the late 19th century. Royal Johor Polo Club was formed in 1884 and Singapore Polo Club was formed in 1886. The oldest polo club in the modern country of Malaysia is Selangor Polo Club, founded in 1902. It was largely played by royalty and the political and business elite.

Polo was played at the 2007 Southeast Asian Games and 2017 Southeast Asian Games. Nations that competed in the tournament were Indonesia, Singapore, Malaysia, Thailand and Philippines (2007) and Brunei, Malaysia, Singapore and Thailand (2017). The 2007 tournament's gold medal was won by the Malaysian team, followed by Singapore with silver and Thailand with bronze while the 2017 tournament's gold medal was won by Malaysia, followed by Thailand with silver and Brunei with bronze.

The traditional or 'free style' "Polo" or "Pulu" of Northern Pakistan is still played avidly in its native region, and the annual Shandur Polo Festival at Shandur Top in Chitral District. It is an internationally famed event attended by many enthusiasts from all over the world. The Shandur polo ground is said to be the highest polo ground in the world, at approximately 3,734 metres,

The recent resurgence in south-east Asia has resulted in its popularity in cities such as Pattaya, Kuala Lumpur and Jakarta. In Pattaya alone, there are three active polo clubs: Polo Escape, Siam Polo Park and the Thai Polo and Equestrian Club. Indonesia has a polo club (Nusantara Polo Club). More recently, Janek Gazecki and Australian professional Jack "Ruki" Baillieu have organised polo matches in parks "around metropolitan Australia, backed by wealthy sponsors."

A Chinese Equestrian Association has been formed with two new clubs in China itself: the Beijing Sunny Time Polo Club, founded by Xia Yang in 2004 and the Nine Dragons Hill Polo Club in Shanghai, founded in 2005.

Polo is not widely spread in West Asia, but still counts five active clubs in Iran, four active polo clubs in the UAE, one club in Bahrain and The Royal Jordanian Polo Club in Amman, Jordan.

Polo in Iran is governed by the Polo Federation of Iran. There are five polo clubs in Iran: Ghasr-e Firoozeh, Nowroozabad, Army Ground Forces, Kanoon-e Chogan and Nesf-e Jahan. Iran possesses some of the best grass polo fields in the region. The country currently has over 100 registered players of which approximately 15% are women. Historically, Kurdish and Persian Arabian horses were the most widely used for polo. This was probably also the case in ancient times. Today Thoroughbreds are being increasingly used alongside the Kurdish and Persian Arabian horses. Some players have also been experimenting with Anglo-Arabians. Iranians still refer to the game of polo by its original Persian name of "Chogan", which means mallet. Iranians still maintain some of the ancient rituals of the game in official polo matches.

The governing body of polo in India is the Indian Polo Association.

Polo first began its Irish history in 1870 with the first official game played on Gormanstown Strand, Co. Meath. Three years later the All Ireland Polo Club was founded by Mr. Horace Rochford in the Phoenix Park. Since then the sport has continued to grow with a further seven clubs opening around the country. The sport has also been made more accessible by these clubs by the creation of more affordable training programmes, such as the beginner to pro programme at Polo Wicklow.

The governing body in the United Kingdom is the Hurlingham Polo Association, dating from 1875, which amalgamated with the County Polo Association in 1949. The UK Armed Forces Polo Association oversees the sport in the three armed services.

The United States Polo Association (USPA) is the governing body for polo in the U.S. The U.S. is the only country that has separate women's polo, run by the United States Women's Polo Federation.

Sagol Kangjei, discussed above, is arguably a version of polo though it can also be seen as the precursor of modern outdoor polo.






</doc>
<doc id="24444" url="https://en.wikipedia.org/wiki?curid=24444" title="Page description language">
Page description language

In digital printing, a page description language (PDL) is a computer language that describes the appearance of a printed page in a higher level than an actual output bitmap (or generally raster graphics). An overlapping term is printer control language, which includes Hewlett-Packard's Printer Command Language (PCL). PostScript is one of the most noted page description languages. The markup language adaptation of the PDL is the page description markup language.

Page description languages are text (human-readable) or binary data streams, usually intermixed with text or graphics to be printed. They are distinct from graphics application programming interfaces (APIs) such as GDI and OpenGL that can be called by software to generate graphical output.

Various page description languages exist:





</doc>
<doc id="24445" url="https://en.wikipedia.org/wiki?curid=24445" title="Pope Felix I">
Pope Felix I

Pope Felix I was the bishop of Rome from 5 January 269 to his death on 30 December 274.

A Roman by birth, Felix was chosen to be pope on 5 January 269, in succession to Dionysius, who had died on 26 December 268.

Felix was the author of an important dogmatic letter on the unity of Christ's Person. He received Emperor Aurelian's aid in settling a theological dispute between the anti-Trinitarian Paul of Samosata, who had been deprived of the bishopric Antioch by a council of bishops for heresy, and the orthodox Domnus, Paul's successor. Paul refused to give way, and in 272 Aurelian was asked to decide between the rivals. He ordered the church building to be given to the bishop who was "recognized by the bishops of Italy and of the city of Rome" (Felix). See Eusebius, Hist. Ecc. vii. 30.

The text of that letter was later interpolated by a follower of Apollinaris in the interests of his sect.

The notice about Felix in the "Liber Pontificalis" ascribes to him a decree that Masses should be celebrated on the tombs of martyrs ("Hic constituit supra memorias martyrum missas celebrare"). The author of this entry was evidently alluding to the custom of celebrating Mass privately at the altars near or over the tombs of the martyrs in the crypts of the catacombs (missa ad corpus), while the solemn celebration always took place in the basilicas built over the catacombs. This practice, still in force at the end of the fourth century, dates apparently from the period when the great cemeterial basilicas were built in Rome, and owes its origin to the solemn commemoration services of martyrs, held at their tombs on the anniversary of their burial, as early as the third century. Felix probably issued no such decree, but the compiler of the "Liber Pontificalis" attributed it to him because he made no departure from the custom in force in his time.

The acts of the Council of Ephesus give Pope Felix as a martyr; but this detail, which occurs again in the biography of the pope in the "Liber Pontificalis", is unsupported by any authentic earlier evidence and is manifestly due to a confusion of names. According to the notice in the "Liber Pontificalis", Felix erected a basilica on the Via Aurelia; the same source also adds that he was buried there. The latter detail is evidently an error, for the fourth-century Roman calendar of feasts says that Pope Felix was interred in the Catacomb of Callixtus on the Via Appia. The statement of the "Liber Pontificalis" concerning the pope's martyrdom results obviously from a confusion with a Roman martyr of the same name buried on the Via Aurelia, and over whose grave a church was built. In the Roman "Feriale" or calendar of feasts, referred to above, the name of Felix occurs in the list of Roman bishops ("Depositio episcoporum"), and not in that of the martyrs.

According to the above-mentioned detail of the "Depositio episcoporum", Felix was interred in the catacomb of Callixtus on 30 December, "III Kal. Jan." (third day to the calends of January) in the Roman dating system. Saint Felix I is mentioned as Pope and Martyr, with a simple feast, on 30 May. This date, given in the "Liber Pontificalis" as that of his death (III Kal. Jun.), is probably an error which could easily occur through a transcriber writing "Jun." for "Jan." This error persisted in the General Roman Calendar until 1969 (see General Roman Calendar of 1960), by which time the mention of Saint Felix I was reduced to a commemoration in the weekday Mass by decision of Pope Pius XII (see General Roman Calendar of Pope Pius XII). Thereafter, the feast of Saint Felix I, no longer mentioned in the General Roman Calendar, is celebrated on his true day of death, 30 December, and without the qualification of "martyr".

According to more recent studies, the oldest liturgical books indicate that the saint honoured on 30 May was a little-known martyr buried on the Via Aurelia, who was mistakenly identified with Pope Felix I, an error similar to but less curious than the identification in the liturgical books, until the mid-1950s, of the martyr saint celebrated on 30 July with the antipope Felix II.




</doc>
<doc id="24446" url="https://en.wikipedia.org/wiki?curid=24446" title="Peptide bond">
Peptide bond

A peptide bond is an amide type of covalent chemical bond linking two consecutive alpha-amino acids from C1 (carbon number one) of one alpha-amino acid and N2 (nitrogen number two) of another, along a peptide or protein chain.

It can also be called an eupeptide bond to separate it from an isopeptide bond, a different type of amide bond between two amino acids.

When two amino acids form a "dipeptide" through a "peptide bond" it is a type of condensation reaction. In this kind of condensation, two amino acids approach each other, with the non-side chain (C1) carboxylic acid moiety of one coming near the non-side chain (N2) amino moiety of the other. One loses a hydrogen and oxygen from its carboxyl group (COOH) and the other loses a hydrogen from its amino group (NH). This reaction produces a molecule of water (HO) and two amino acids joined by a peptide bond (-CO-NH-). The two joined amino acids are called a dipeptide.

The amide bond is synthesized when the carboxyl group of one amino acid molecule reacts with the amino group of the other amino acid molecule, causing the release of a molecule of water (HO), hence the process is a dehydration synthesis reaction.

The formation of the peptide bond consumes energy, which, in organisms, is derived from ATP. Peptides and proteins are chains of amino acids held together by peptide bonds (and sometimes by a few isopeptide bonds). Organisms use enzymes to produce nonribosomal peptides, and ribosomes to produce proteins via reactions that differ in details from dehydration synthesis.

Some peptides, like alpha-amanitin, are called ribosomal peptides as they are made by ribosomes, but many are nonribosomal peptides as they are synthesized by specialized enzymes rather than ribosomes. For example, the tripeptide glutathione is synthesized in two steps from free amino acids, by two enzymes: glutamate–cysteine ligase (forms an isopeptide bond, which is not a peptide bond) and glutathione synthetase (forms a peptide bond).

A peptide bond can be broken by hydrolysis (the addition of water). In the presence of water they will break down and release 8–16 kilojoule/mol (2–4 kcal/mol) of Gibbs energy. This process is extremely slow, with the half life at 25 °C of between 350 and 600 years per bond.

In living organisms, the process is normally catalyzed by enzymes known as peptidases or proteases, although there are reports of peptide bond hydrolysis caused by conformational strain as the peptide/protein folds into the native structure. This non-enzymatic process is thus not accelerated by transition state stabilization, but rather by ground state destabilization.

The wavelength of absorption A for a peptide bond is 190–230 nm (which makes it particularly susceptible to UV radiation).

Significant delocalisation of the lone pair of electrons on the nitrogen atom gives the group a partial double bond character. The partial double bond renders the amide group planar, occurring in either the cis or trans isomers. In the unfolded state of proteins, the peptide groups are free to isomerize and adopt both isomers; however, in the folded state, only a single isomer is adopted at each position (with rare exceptions). The trans form is preferred overwhelmingly in most peptide bonds (roughly 1000:1 ratio in trans:cis populations). However, X-Pro peptide groups tend to have a roughly 30:1 ratio, presumably because the symmetry between the formula_1 and formula_2 atoms of proline makes the cis and trans isomers nearly equal in energy (see figure, below).The dihedral angle associated with the peptide group (defined by the four atoms formula_3) is denoted formula_4; formula_5 for the cis isomer (synperiplanar conformation) and formula_6 for the trans isomer (antiperiplanar conformation). Amide groups can isomerize about the C'-N bond between the cis and trans forms, albeit slowly (formula_720 seconds at room temperature). The transition states formula_8 requires that the partial double bond be broken, so that the activation energy is roughly 80 kilojoule/mol (20 kcal/mol). However, the activation energy can be lowered (and the isomerization catalyzed) by changes that favor the single-bonded form, such as placing the peptide group in a hydrophobic environment or donating a hydrogen bond to the nitrogen atom of an X-Pro peptide group. Both of these mechanisms for lowering the activation energy have been observed in peptidyl prolyl isomerases (PPIases), which are naturally occurring enzymes that catalyze the cis-trans isomerization of X-Pro peptide bonds.

Conformational protein folding is usually much faster (typically 10–100 ms) than cis-trans isomerization (10–100 s). A nonnative isomer of some peptide groups can disrupt the conformational folding significantly, either slowing it or preventing it from even occurring until the native isomer is reached. However, not all peptide groups have the same effect on folding; nonnative isomers of other peptide groups may not affect folding at all.

Due to its resonance stabilization, the peptide bond is relatively unreactive under physiological conditions, even less than similar compounds such as esters. Nevertheless, peptide bonds can undergo chemical reactions, usually through an attack of an electronegative atom on the carbonyl carbon, breaking the carbonyl double bond and forming a tetrahedral intermediate. This is the pathway followed in proteolysis and, more generally, in N-O acyl exchange reactions such as those of inteins. When the functional group attacking the peptide bond is a thiol, hydroxyl or amine, the resulting molecule may be called a cyclol or, more specifically, a thiacyclol, an oxacyclol or an azacyclol, respectively.



</doc>
<doc id="24449" url="https://en.wikipedia.org/wiki?curid=24449" title="Pumping lemma">
Pumping lemma

In the theory of formal languages, the pumping lemma may refer to:



</doc>
<doc id="24451" url="https://en.wikipedia.org/wiki?curid=24451" title="Privy Council of the United Kingdom">
Privy Council of the United Kingdom

The Privy Council of the United Kingdom is a formal body of advisers to the Sovereign of the United Kingdom. Its membership mainly comprises senior politicians who are current or former members of either the House of Commons or the House of Lords.

The Privy Council formally advises the sovereign on the exercise of the Royal Prerogative, and corporately (as Queen-in-Council) it issues executive instruments known as Orders in Council, which among other powers enact Acts of Parliament. The Council also holds the delegated authority to issue Orders of Council, mostly used to regulate certain public institutions. The Council advises the sovereign on the issuing of Royal Charters, which are used to grant special status to incorporated bodies, and city or borough status to local authorities. Otherwise, the Privy Council's powers have now been largely replaced by its executive committee, the Cabinet of the United Kingdom.

Certain judicial functions are also performed by the Queen-in-Council, although in practice its actual work of hearing and deciding upon cases is carried out day-to-day by the Judicial Committee of the Privy Council. The Judicial Committee consists of senior judges appointed as Privy Counsellors: predominantly Justices of the Supreme Court of the United Kingdom and senior judges from the Commonwealth. The Privy Council formerly acted as the High Court of Appeal for the entire British Empire (other than for the United Kingdom itself). It continues to hear judicial appeals from some other independent Commonwealth countries, as well as Crown Dependencies and British Overseas Territories.

The Privy Council of the United Kingdom was preceded by the Privy Council of Scotland and the Privy Council of England. The key events in the formation of the modern Privy Council are given below:

In Anglo-Saxon England, Witenagemot was an early equivalent to the Privy Council of England. During the reigns of the Norman monarchs, the English Crown was advised by a royal court or "curia regis", which consisted of magnates, ecclesiastics and high officials. The body originally concerned itself with advising the sovereign on legislation, administration and justice. Later, different bodies assuming distinct functions evolved from the court. The courts of law took over the business of dispensing justice, while Parliament became the supreme legislature of the kingdom. Nevertheless, the Council retained the power to hear legal disputes, either in the first instance or on appeal. Furthermore, laws made by the sovereign on the advice of the Council, rather than on the advice of Parliament, were accepted as valid. Powerful sovereigns often used the body to circumvent the Courts and Parliament. For example, a committee of the Council—which later became the Court of the Star Chamber—was during the 15th century permitted to inflict any punishment except death, without being bound by normal court procedure. During Henry VIII's reign, the sovereign, on the advice of the Council, was allowed to enact laws by mere proclamation. The legislative pre-eminence of Parliament was not restored until after Henry VIII's death. Though the royal Council retained legislative and judicial responsibilities, it became a primarily administrative body. The Council consisted of forty members in 1553, but the sovereign relied on a smaller committee, which later evolved into the modern Cabinet.

By the end of the English Civil War, the monarchy, House of Lords, and Privy Council had been abolished. The remaining parliamentary chamber, the House of Commons, instituted a Council of State to execute laws and to direct administrative policy. The forty-one members of the Council were elected by the House of Commons; the body was headed by Oliver Cromwell, "de facto" military dictator of the nation. In 1653, however, Cromwell became Lord Protector, and the Council was reduced to between thirteen and twenty-one members, all elected by the Commons. In 1657, the Commons granted Cromwell even greater powers, some of which were reminiscent of those enjoyed by monarchs. The Council became known as the Protector's Privy Council; its members were appointed by the Lord Protector, subject to Parliament's approval.

In 1659, shortly before the restoration of the monarchy, the Protector's Council was abolished. Charles II restored the Royal Privy Council, but he, like previous Stuart monarchs, chose to rely on a small group of advisers. Under George I even more power transferred to this committee. It now began to meet in the absence of the sovereign, communicating its decisions to him after the fact.

Thus, the British Privy Council, as a whole, ceased to be a body of important confidential advisers to the sovereign; the role passed to a committee of the Council, now known as the Cabinet.

The sovereign, when acting on the Council's advice, is known as the "King-in-Council" or "Queen-in-Council". The members of the Council are collectively known as "The Lords of Her Majesty's Most Honourable Privy Council" (sometimes "The Lords and others of ..."). The chief officer of the body is the Lord President of the Council, who is the fourth highest Great Officer of State, a Cabinet member and normally, either the Leader of the House of Lords or of the House of Commons. Another important official is the Clerk, whose signature is appended to all orders made in the Council.

Both "Privy Counsellor" and "Privy Councillor" may be correctly used to refer to a member of the Council. The former, however, is preferred by the Privy Council Office, emphasising English usage of the term "Counsellor" as "one who gives counsel", as opposed to "one who is a member of a council". A Privy Counsellor is traditionally said to be ""sworn of"" the Council after being received by the sovereign.

The sovereign may appoint anyone a Privy Counsellor, but in practice appointments are made only on the advice of Her Majesty's Government. The majority of appointees are senior politicians, including Ministers of the Crown, the few most senior figures of the Loyal Opposition, the Parliamentary leader of the third-largest party, a couple of the most senior figures in the devolved British governments and senior politicians from Commonwealth countries. Besides these, the Council includes a very few members of the Royal Family (usually the consort and heir apparent only), a few dozen judges from British and Commonwealth countries, a few clergy and a small number of senior civil servants.

There is no statutory limit to its membership. Members have no automatic right to attend all Privy Council meetings, and only some are summoned regularly to meetings (in practice at the Prime Minister's discretion).

The Church of England's three senior bishops – the Archbishop of Canterbury, the Archbishop of York and the Bishop of London – become Privy Counsellors upon appointment. Senior members of the Royal Family may also be appointed, but this is confined to the current consort and heir apparent and consort. The Private Secretary to the Sovereign is always appointed a Privy Counsellor, as are the Lord Chamberlain, the Speaker of the House of Commons, and the Lord Speaker. Justices of the Supreme Court of the United Kingdom, judges of the Court of Appeal of England and Wales, senior judges of the Inner House of the Court of Session (Scotland's highest law court) and the Lord Chief Justice of Northern Ireland also join the Privy Council "ex officio".

The balance of Privy Counsellors is largely made up of politicians. The Prime Minister, Cabinet ministers and the Leader of HM Opposition are traditionally sworn of the Privy Council upon appointment. Leaders of major parties in the House of Commons, First Ministers of the devolved assemblies, some senior Ministers outside Cabinet, and on occasion other respected senior parliamentarians are appointed Privy Counsellors.

Because Privy Counsellors are bound by oath to keep matters discussed at Council meetings secret, the appointment of the Leaders of Opposition Parties as Privy Counsellors allows the Government to share confidential information with them "on Privy Council terms". This usually only happens in special circumstances, such as in matters of national security. For example, Tony Blair met Iain Duncan Smith (then Leader of HM Opposition) and Charles Kennedy (then Leader of the Liberal Democrats) "on Privy Council terms" to discuss the evidence for Iraq's weapons of mass destruction.

Although the Privy Council is primarily a British institution, officials from some other Commonwealth realms are also appointed. By 2000, the most notable instance was New Zealand, whose Prime Minister, senior politicians, Chief Justice and Court of Appeal Justices were traditionally appointed Privy Counsellors. However, appointments of New Zealand members have since been discontinued. The Prime Minister, the Speaker, the Governor-General and the Chief Justice of New Zealand are still accorded the style "Right Honourable", but without membership of the Council. Until the late 20th century, the Prime Ministers and Chief Justices of Canada and Australia were also appointed Privy Counsellors. Canada also has its own Privy Council, the Queen's Privy Council for Canada ("see" below). Prime Ministers of some other Commonwealth countries that retain the Queen as their sovereign continue to be sworn of the Council.

It was formerly regarded by the Privy Council as criminal, and possibly treasonous, to disclose the oath administered to Privy Counsellors as they take office. However, the oath was officially made public by the Blair Government in a written parliamentary answer in 1998, as follows. It had also been read out in full in the House of Lords during debate by Lord Rankeillour on 21 December 1932.

A form of this oath dates back to at least 1570.

Privy counsellors can choose to affirm their allegiance in similar terms, should they prefer not to take a religious oath. At the induction ceremony, the order of precedence places Anglicans (being those of the established church) before others.

The initiation ceremony for newly appointed privy counsellors is held in private, and typically requires kneeling on a stool before the sovereign and then kissing hands. According to "The Royal Encyclopaedia": "The new privy counsellor or minister will extend his or her right hand, palm upwards, and, taking the Queen's hand lightly, will kiss it with no more than a touch of the lips." The ceremony has caused difficulties for privy counsellors who advocate republicanism; Tony Benn said in his diaries that he kissed his own thumb, rather than the Queen's hand, while Jeremy Corbyn reportedly did not kneel. Not all members of the privy council go through the initiation ceremony; appointments are frequently made by an Order in Council, although it is "rare for a party leader to use such a course."

Membership is conferred for life. Formerly, the death of a monarch ("demise of the Crown") brought an immediate dissolution of the Council, as all Crown appointments automatically lapsed. By the 18th century, it was enacted that the Council would not be dissolved until up to six months after the demise of the Crown. By convention, however, the sovereign would reappoint all members of the Council after its dissolution. In practice, therefore, membership continued without a break. In 1901, the law was changed to ensure that Crown Appointments became wholly unaffected by any succession of monarch.

The sovereign, however, may remove an individual from the Privy Council. Former MP Elliot Morley was expelled on 8 June 2011, following his conviction on charges of false accounting in connection with the British parliamentary expenses scandal. Before this, the last individual to be expelled from the Council against his will was Sir Edgar Speyer, , who was removed on 13 December 1921 for collaborating with the enemy German Empire, during the First World War.

Individuals can choose to resign, sometimes to avoid expulsion. Three members voluntarily left the Privy Council in the 20th century: John Profumo, who resigned on 26 June 1963; John Stonehouse, who resigned on 17 August 1976 and Jonathan Aitken, who resigned on 25 June 1997 following allegations of perjury.

So far, three Privy Counsellors have resigned in the 21st century, coincidentally all in the same year. On 4 February 2013, Chris Huhne announced that he would voluntarily leave the Privy Council after pleading guilty to perverting the course of justice. Lord Prescott stood down on 6 July 2013, in protest against delays in the introduction of press regulation, expecting others to follow. Denis MacShane resigned on 9 October 2013, before a High Court hearing at which he pleaded guilty of false accounting and was subsequently imprisoned.

Meetings of the Privy Council are normally held once each month wherever the sovereign may be in residence at the time. The quorum, according to the Privy Council Office, is three, though some statutes provide for other quorums (for example, section 35 of the Opticians Act 1989 provides for a lower quorum of two).

The sovereign attends the meeting, though his or her place may be taken by two or more Counsellors of State. Under the Regency Acts 1937 to 1953, Counsellors of State may be chosen from among the sovereign's spouse and the four individuals next in the line of succession who are over 21 years of age (18 for the heir to the throne). Customarily the sovereign remains standing at meetings of the Privy Council, so that no other members may sit down, thereby keeping meetings short. The Lord President reads out a list of Orders to be made, and the sovereign merely says "Approved".

Few Privy Counsellors are required to attend regularly. The settled practice is that day-to-day meetings of the Council are attended by four Privy Counsellors, usually the relevant Minister to the matters pertaining. The Cabinet Minister holding the office of Lord President of the Council, currently Jacob Rees-Mogg , invariably presides. Under Britain's modern conventions of parliamentary government and constitutional monarchy, every order made in Council is drafted by a Government Department and has already been approved by the Minister responsible – thus actions taken by the Queen-in-Council are formalities required for validation of each measure.

Full meetings of the Privy Council are held only when the reigning sovereign announces his or her own engagement (which last happened on 23 November 1839, in the reign of Queen Victoria); or when there is a demise of the Crown, either by the death or abdication of the monarch. A full meeting of the Privy Council was also held on 6 February 1811, when George, Prince of Wales was sworn in as Prince Regent by Act of Parliament. The current statutes regulating the establishment of a regency in the case of minority or incapacity of the sovereign also require any regents to swear their oaths before the Privy Council.

In the case of a demise of the Crown, the Privy Council – together with the Lords Spiritual, the Lords Temporal, the Lord Mayor and Aldermen of the City of London as well as representatives of Commonwealth realms – makes a proclamation declaring the accession of the new sovereign and receives an oath from the new monarch relating to the security of the Church of Scotland, as required by law. It is also customary for the new sovereign to make an allocution to the Privy Council on that occasion, and this Sovereign's Speech is formally published in "The London Gazette". Any such Special Assembly of the Privy Council, convened to proclaim the accession of a new sovereign and witness the monarch's statutory oath, is known as an Accession Council. The last such meetings were held on 6 and 8 February 1952: as Elizabeth II was abroad when the last demise of the Crown took place, the Accession Council met twice, once to proclaim the sovereign (meeting of 6 February 1952), and then again after the new queen had returned to Britain, to receive from her the oath required by statute (meeting of 8 February 1952).

The sovereign exercises executive authority by making Orders in Council upon the advice of the Privy Council. Orders-in-Council, which are drafted by the government rather than by the sovereign, are secondary legislation and are used to make government regulations and to make government appointments. Furthermore, Orders-in-Council are used to grant Royal Assent for Measures of the National Assembly for Wales, and laws passed by the legislatures of British Crown dependencies.

Distinct from Orders-in-Council are Orders of Council: the former are issued by the sovereign upon the advice of the Privy Council, whereas the latter are made by members of the Privy Council without requiring the sovereign's approval. They are issued under the specific authority of Acts of Parliament, and most commonly are used for the regulation of public institutions.

The sovereign also grants Royal Charters on the advice of the Privy Council. Charters bestow special status to incorporated bodies; they are used to grant "chartered" status to certain professional, educational or charitable bodies, and sometimes also city and borough status to towns. The Privy Council therefore deals with a wide range of matters, which also includes university and livery company statutes, churchyards, coinage and the dates of bank holidays. The Privy Council formerly had sole power to grant academic degree-awarding powers and the title of "university", but following the Higher Education and Research Act 2017 these powers have been given to the Office for Students for educational institutions in England.

The Privy Council comprises a number of committees:

The Accession Council is made up of Privy Counsellors, Great Officers of State, members of the House of Lords, the Lord Mayor of the City of London, the Aldermen of the City of London, High Commissioners of Commonwealth realms, and senior civil servants. It is a ceremonial body which assembles in St James's Palace upon the death of a monarch, to make formal proclamation of the accession of the successor to the throne.

The Baronetage Committee was established by a 1910 Order in Council, during Edward VII's reign, to scrutinise all succession claims (and thus reject doubtful ones) to be placed on the Roll of Baronets.

The Cabinet of the United Kingdom is the collective decision-making body of Her Majesty's Government of the United Kingdom, composed of the Prime Minister and 21 cabinet ministers, the most senior of the government ministers.

The Committee for the Affairs of Jersey and Guernsey recommends approval of Channel Islands legislation.

The Committee for the purposes of the Crown Office Act 1877 consists of the Lord Chancellor and Lord Privy Seal as well as a Secretary of State. The Committee, which last met in 1988, is concerned with the design and usage of wafer seals.

The Judicial Committee of the Privy Council, consists of senior judges who are Privy Counsellors. The decision of the Committee is presented in the form of "advice" to the monarch, but in practice it is always followed by the sovereign (as Crown-in-Council), who formally approves the recommendation of the Judicial Committee.

Within the United Kingdom, the Judicial Committee hears appeals from ecclesiastical courts, the Court of Admiralty of the Cinque Ports, prize courts and the Disciplinary Committee of the Royal College of Veterinary Surgeons, appeals against schemes of the Church Commissioners and appeals under certain Acts of Parliament (e.g., the House of Commons Disqualification Act 1975). The Crown-in-Council was formerly the Supreme Appeal Court for the entire British Empire, but a number of Commonwealth countries have now abolished the right to such appeals. The Judicial Committee continues to hear appeals from several Commonwealth countries, from British Overseas Territories, Sovereign Base Areas and Crown dependencies. The Judicial Committee had direct jurisdiction in cases relating to the Scotland Act 1998, the Government of Wales Act 1998 and the Northern Ireland Act 1998, but this was transferred to the new Supreme Court of the United Kingdom in 2009.

The Lords Commissioners are Privy Counsellors appointed by the Monarch of the United Kingdom to exercise, on their behalf, certain functions relating to Parliament which would otherwise require the monarch's attendance at the Palace of Westminster. These include the opening and prorogation of Parliament, the confirmation of a newly elected Speaker of the House of Commons and the granting of Royal Assent. In current practice, the Lords Commissioners usually include the Lord Chancellor, the Archbishop of Canterbury, the leaders of the three major parties in the House of Lords, the convener of the House of Lords Crossbenchers, and the Lord Speaker.

The Scottish Universities Committee considers proposed amendments to the statutes of Scotland's four ancient universities. 

The Universities Committee, which last met in 1995, considers petitions against statutes made by Oxford and Cambridge universities and their colleges.

In addition to the Standing Committees, "ad hoc" Committees are notionally set up to consider and report on Petitions for Royal charters of Incorporation and to approve changes to the bye-laws of bodies created by Royal Charter.

Committees of Privy Counsellors are occasionally established to examine specific issues. Such Committees are independent of the
Privy Council Office and therefore do not report directly to the Lord President of the Council. Examples of such Committees include:

The Civil Service is formally governed by Privy Council Orders, as an exercise of the Royal prerogative. One such order implemented HM Government's ban of GCHQ staff from joining a Trade Union. Another, the Civil Service (Amendment) Order in Council 1997, permitted the Prime Minister to grant up to three political advisers management authority over some Civil Servants.

In the 1960s, the Privy Council made an order to evict the 2,000 inhabitants of the 65-island Chagos Archipelago in the Indian Ocean, in preparation for the establishment of a joint United States–United Kingdom military base on the largest outlying island, Diego Garcia, some distant. In 2000 the Court of Appeal ruled the 1971 Immigration Ordinance preventing resettlement unlawful. In 2004, the Privy Council, under Jack Straw's tenure, overturned the ruling. In 2006 the High Court of Justice found the Privy Council's decision to be unlawful. Sir Sydney Kentridge described the treatment of the Chagossians as "outrageous, unlawful and a breach of accepted moral standards": Justice Kentridge stated that there was no known precedent "for the lawful use of prerogative powers to remove or exclude an entire population of British subjects from their homes and place of birth", and the Court of Appeal were persuaded by this argument, but the Law Lords (at that time the UK's highest law court) found its decision to be flawed and overturned the ruling by a 3–2 decision thereby upholding the terms of the Ordinance.

The Privy Council as a whole is termed "The Most Honourable" whilst its members individually, the Privy Counsellors, are entitled to be styled "The Right Honourable".

Each Privy Counsellor has the right of personal access to the sovereign. Peers were considered to enjoy this right individually; members of the House of Commons possess the right collectively. In each case, personal access may only be used to tender advice on public affairs.

Only Privy Counsellors can signify royal consent to the examination of a Bill affecting the rights of the Crown.

Members of the Privy Council are privileged to be given advance notice of any prime ministerial decision to commit HM Armed Forces in enemy action.

Privy Counsellors have the right to sit on the steps of the Sovereign's Throne in the Chamber of the House of Lords during debates, a privilege which was shared with heirs apparent of those hereditary peers who were to become members of the House of Lords before Labour's partial Reform of the Lords in 1999, diocesan bishops of the Church of England yet to be Lords Spiritual, retired bishops who formerly sat in the House of Lords, the Dean of Westminster, Peers of Ireland, the Clerk of the Crown in Chancery, and the Gentleman Usher of the Black Rod. While Privy Counsellors have the right to sit on the steps of the Sovereign's Throne they do so only as observers and are not allowed to participate in any of the workings of the House of Lords. Nowadays this privilege is rarely exercised. A notable recent instance of the exercising of this privilege was used by the Prime Minister, Theresa May, and David Lidington, who watched the opening of the debate of the European Union (Notification of Withdrawal) Bill 2017 in the House of Lords.

Privy Counsellors are accorded a formal rank of precedence, if not already having a higher one. At the beginning of each new Parliament, and at the discretion of the Speaker, those members of the House of Commons who are Privy Counsellors usually take the oath of allegiance before all other members except the Speaker and the Father of the House (who is the member of the House who has the longest continuous service). Should a Privy Counsellor rise to speak in the House of Commons at the same time as another Honourable Member, the Speaker usually gives priority to the "Right Honourable" Member. This parliamentary custom, however, was discouraged under New Labour after 1998, despite the Government not being supposed to exert influence over the Speaker.

All those sworn of the Privy Council are accorded the style "The Right Honourable", but some nobles automatically have higher styles: non-royal dukes are styled "The Most Noble" and marquesses, "The Most Honourable". Modern custom as recommended by "Debrett's" is to use the post-nominal letters "PC" in a social style of address for peers who are Privy Counsellors. For commoners, "The Right Honourable" is sufficient identification of their status as a Privy Counsellor and they do not use the post-nominal letters "PC". The Ministry of Justice revises current practice of this convention from time to time.

The Privy Council is one of the four principal councils of the sovereign. The other three are the courts of law, the "Commune Concilium" (Common Council, i.e. Parliament) and the "Magnum Concilium" (Great Council, i.e. the assembly of all the Peers of the Realm). All are still in existence, or at least have never been formally abolished, but the "Magnum Concilium" has not been summoned since 1640 and was considered defunct even then.

Several other Privy Councils have advised the sovereign. England and Scotland once had separate Privy Councils (the Privy Council of England and Privy Council of Scotland). The Acts of Union 1707 united the two countries into the Kingdom of Great Britain and in 1708 the Parliament of Great Britain abolished the Privy Council of Scotland. Thereafter there was one Privy Council of Great Britain sitting in London. Ireland, on the other hand, continued to have a separate Privy Council even after the Act of Union 1800. The Privy Council of Ireland was abolished in 1922, when the greater part of Ireland separated from the United Kingdom; it was succeeded by the Privy Council of Northern Ireland, which became dormant after the suspension of the Parliament of Northern Ireland in 1972. No further appointments have been made since then, and only three appointees were still living as of November 2017.

Canada has had its own Privy Council—the Queen's Privy Council for Canada—since 1867. While the Canadian Privy Council is specifically "for Canada", the Privy Council discussed above is not "for the United Kingdom"; to clarify the ambiguity where necessary, the latter was traditionally referred to as the Imperial Privy Council. Equivalent organs of state in other Commonwealth realms, such as Australia and New Zealand, are called Executive Councils.





</doc>
<doc id="24452" url="https://en.wikipedia.org/wiki?curid=24452" title="Prime Minister of India">
Prime Minister of India

The prime minister of India (IAST: "Bhārat ke Pradhānamantrī") is the leader of the executive of the Government of India. The prime minister is the chief adviser to the President of India and the head of the Union Council of Ministers. They can be a member of any of the two houses of the Parliament of India—the Lok Sabha (House of the People) and the Rajya Sabha (Council of the States); but has to be a member of the political party or coalition, having a majority in the Lok Sabha.

The prime minister is the senior-most member of cabinet in the executive of government in a parliamentary system. The prime minister selects and can dismiss members of the cabinet; allocates posts to members within the government; and is the presiding member and chairperson of the cabinet.

The Union Cabinet headed by the prime minister is appointed by the President of India to assist the latter in the administration of the affairs of the executive. Union cabinet is collectively responsible to the Lok Sabha as per of the Constitution of India. The prime minister has to enjoy the confidence of a majority in the Lok Sabha and shall resign if they are unable to prove majority when instructed by the president.

India follows a parliamentary system in which the prime minister is the presiding head of the government and chief of the executive of the government. In such systems, the head of state, or, the head of state's official representative (i.e., the monarch, president, or governor-general) usually holds a purely ceremonial position and acts—on most matters—only on the advice of the prime minister.

The prime minister—if they are not already—shall become a member of parliament within six months of beginning his/her tenure. A prime minister is expected to work with other central ministers to ensure the passage of bills by the parliament.

Since 1947, there have been 14 different prime ministers. The first few decades after 1947 saw the Indian National Congress' (INC) almost complete domination over the political map of India. India's first prime minister—Jawaharlal Nehru—took oath on 15 August 1947. Nehru went on to serve as prime minister for 17 consecutive years, winning four general elections in the process. His tenure ended in May 1964, on his death. After the death of Nehru, Lal Bahadur Shastri—a former home minister and a leader of the Congress party—ascended to the position of Prime Minister. Shastri's tenure saw the Indo-Pakistani War of 1965. Shashtri subsequently died of a reported heart attack in Tashkent, after signing the Tashkent Declaration.

After Shastri, Indira Gandhi—Nehru's daughter—was elected as the country's first woman prime minister. Indira's first term in office lasted 11 years, in which she took steps such as nationalisation of banks; end of allowances and political posts, which were received by members of the royal families of the erstwhile princely states of British India. In addition, events such as the Indo-Pakistani War of 1971; the establishment of a sovereign Bangladesh; accession of Sikkim to India, through a referendum in 1975; and India's first nuclear test in Pokhran occurred during Indira's first term. In 1975, President Fakhruddin Ali Ahmed—on Indira's advice—imposed a state of emergency, therefore, bestowing the government with the power to rule by decree, the period is known for human right violations.

After widespread protests, the emergency was lifted in 1977, and a general election was to be held. All of the political parties of the opposition—after the conclusion of the emergency—fought together against the Congress, under the umbrella of the Janata Party, in the general election of 1977, and were successful in defeating the Congress. Subsequently, Morarji Desai—a former deputy prime minister—became the first non-Congress prime minister of the country. The government of Prime Minister Desai was composed of groups with opposite ideologies, in which unity and co-ordination were difficult to maintain. Ultimately, after two and a half years as PM; on 28 July 1979, Morarji tendered his resignation to the president; and his government fell. Thereafter, Charan Singh—a deputy prime minister in Desai's cabinet—with outside, conditional support from Congress, proved a majority in Lok Sabha and took oath as prime minister. However, Congress pulled its support shortly after, and Singh had to resign; he had a tenure of 5 months, the shortest in the history of the office.

In 1980, after a three-year absence, the Congress returned to power with an absolute majority. Indira Gandhi was elected prime minister a second time. During her second tenure, Operation Blue Star—an Indian Army operation inside the Golden Temple, the most sacred site in Sikhism—was conducted, resulting in reportedly thousands of deaths. Subsequently, on 31 October 1984, Gandhi was shot dead by Satwant Singh and Beant Singh—two of her bodyguards—in the garden of her residence at 1, Safdarjung Road, New Delhi.

After Indira, Rajiv—her eldest son and 40 years old at the time—was sworn in on the evening of 31 October 1984, becoming the youngest person ever to hold the office of prime minister. Rajiv immediately called for a general election. In the subsequent general election, the Congress secured an absolute majority, winning 401 of 552 seats in the Lok Sabha, the maximum number received by any party in the history of India. Vishwanath Pratap Singh—first finance minister and then later defence minister in Gandhi's cabinet—uncovered irregularities, in what became to be known as the Bofors scandal, during his stint at the Ministry of Defence; Singh was subsequently expelled from Congress and formed the Janata Dal and—with the help of several anti-Congress parties—also formed the National Front, a coalition of many political parties.

In the general election of 1989, the National Front—with outside support from the Bharatiya Janata Party (BJP) and the Left Front—came to power. V. P. Singh was elected prime minister. During a tenure of less than a year, Singh and his government accepted the Mandal Commission's recommendations. Singh's tenure came to an end after he ordered the arrest of BJP member Lal Krishna Advani, as a result, BJP withdrew its outside support to the government, V. P. Singh lost the subsequent vote-of-no-confidence 146–320 and had to resign. After V. P. Singh's resignation, Chandra Shekhar along with 64 members of parliament (MPs) floated the Samajwadi Janata Party (Rashtriya), and proved a majority in the Lok Sabha with support from Congress. But Shekhar's premiership did not last long, Congress proceeded to withdraw its support; Shekhar's government fell as a result, and new elections were announced.

In the general election of 1991, Congress—under the leadership of P. V. Narasimha Rao—formed a minority government; Rao became the first PM of South Indian origin. After the dissolution of the Soviet Union, India was on the brink of bankruptcy, so, Rao took steps to liberalise the economy, and appointed Manmohan Singh—an economist and a former governor of the Reserve Bank of India—as finance minister. Rao and Singh then took various steps to liberalise the economy, these resulted in an unprecedented economic growth in India. His premiership, however, was also a witness to the demolition of the Babri Masjid, which resulted in the death of about 2,000 people. Rao, however, did complete five continuous years in office, becoming the first prime minister outside of the Nehru—Gandhi family to do so.

After the end of Rao's tenure in May 1996, the nation saw four prime ministers in a span of three years, "", two tenures of Atal Bihari Vajpayee; one tenure of H. D. Deve Gowda from 1 June 1996 to 21 April 1997; and one tenure of I. K. Gujral from 21 April 1997 to 19 March 1998. The government of Prime Minister Vajpayee—elected in 1998—took some concrete steps. In May 1998—after a month in power—the government announced the conduct of five underground nuclear explosions in Pokhran. In response to these tests, many western countries, including the United States, imposed economic sanctions on India, but, due to the support received from Russia, France, the Gulf countries and some other nations, the sanctions—were largely—not considered successful. A few months later in response to the Indian nuclear tests, Pakistan also conducted nuclear tests. Given the deteriorating situation between the two countries, the governments tried to improve bilateral relations. In February 1999, the India and Pakistan signed the Lahore Declaration, in which the two countries announced their intention to annul mutual enmity, increase trade and use their nuclear capabilities for peaceful purposes. In May 1999, All India Anna Dravida Munnetra Kazhagam withdrew from the ruling National Democratic Alliance (NDA) coalition; Vajpayee's government, hence, became a caretaker one after losing a motion-of-no-confidence 269–270, this coincided with the Kargil War with Pakistan. In the subsequent October 1999 general election, the BJP-led NDA and its affiliated parties secured a comfortable majority in the Lok Sabha, winning 299 of 543 seats in the lower house.

Vajpayee continued the process of economic liberalisation during his reign, resulting in economic growth. In addition to the development of infrastructure and basic facilities, the government took several steps to improve the infrastructure of the country, such as, the National Highways Development Project (NHDP) and the "Pradhan Mantri Gram Sadak Yojana" (PMGSY; IAST: ; Prime Minister Rural Road Scheme), for the development of roads. But during his reign, the 2002 Gujarat communal riots in the state of Gujarat took place; resulting in about 2,000 deaths. Vajpayee's tenure as prime minister came to an end in May 2004, making him the first non-Congress PM to complete a full five-year tenure.

In the 2004 election, the Congress emerged as the largest party in a hung parliament; Congress-led United Progressive Alliance (UPA)—with outside support from the Left Front, the Samajwadi Party (SP) and Bahujan Samaj Party (BSP) among others—proved a majority in the Lok Sabha, and Manmohan Singh was elected prime minister; becoming the first Sikh prime minister of the nation. During his tenure, the country retained the economic momentum gained during Prime Minister Vajpayee's tenure. Apart from this, the government succeeded in getting the "National Rural Employment Guarantee Act, 2005", and the "Right to Information Act, 2005" passed in the parliament. Further, the government strengthened India's relations with nations like Afghanistan; Russia; the Gulf states; and the United States, culminating with the ratification of India–United States Civil Nuclear Agreement near the end of Singh's first term. At the same time, the November 2008 Mumbai terrorist attacks also happened during Singh's first term in office. In the general election of 2009, the mandate of UPA increased. Prime Minister Singh's second term, however, was surrounded by accusations of high-level scandals and corruption. Singh resigned as prime minister on 17 May 2014, after Congress' defeat in the 2014 general election.

In the general election of 2014, the BJP-led NDA got an absolute majority, winning 336 out of 543 Lok Sabha seats; the BJP itself became the first party since 1984 to get a majority in the Lok Sabha. Narendra Modi—the Chief Minister of Gujarat—was elected prime minister, becoming the first prime minister to have been born in an independent India.

Narendra Modi was re-elected as prime minister in 2019 with a bigger mandate than that of 2014. The BJP-led NDA winning 354 seats out of which BJP secured 303 seats.

The Constitution envisions a scheme of affairs in which the president of India is the head of state; in terms of Article 53 with office of the prime minister being the head of Council of Ministers to assist and advise the president in the discharge of his/her constitutional functions. To quote, Article 53, and 75 provide as under;

Like most parliamentary democracies, the president's duties are mostly ceremonial as long as the constitution and the rule of law is obeyed by the cabinet and the legislature. The prime minister of India is the head of government and has the responsibility for executive power. The president's constitutional duty is to preserve, protect and defend the Constitution and the law per . In the constitution of India, the prime minister is mentioned in only four of its articles (articles 74, 75, 78 and 366), however he/she plays a crucial role in the government of India by enjoying majority in the Lok Sabha.

According to Article 84 of the Constitution of India, which sets the principle qualification for member of Parliament, and Article 75 of the Constitution of India, which sets the qualifications for the minister in the Union Council of Ministers, and the argument that the position of prime minister has been described as "primus inter pares" (the first among equals), A prime minister must:


If however a candidate is elected as the prime minister they must vacate their post from any private or government company and may take up the post only on completion of their term.

The prime minister is required to make and subscribe in the presence of the President of India before entering office, the oath of office and secrecy, as per the Third Schedule of the Constitution of India.

Oath of office:
Oath of secrecy:
The prime minister serves on 'the pleasure of the president', hence, a prime minister may remain in office indefinitely, so long as the president has confidence in him/her. However, a prime minister must have the confidence of Lok Sabha, the lower house of the Parliament of India.

However, the term of a prime minister can end before the end of a Lok Sabha's term, if a simple majority of its members no longer have confidence in him/her, this is called a vote-of-no-confidence. Three prime ministers, I. K. Gujral , H. D. Deve Gowda and Atal Bihari Vajpayee have been voted out from office this way. In addition, a prime minister can also resign from office; Morarji Desai was the first prime minister to resign while in office.

Upon ceasing to possess the requisite qualifications to be a member of Parliament subject to the "Representation of the People Act, 1951".

The prime minister leads the functioning and exercise of authority of the government of India. The president of India—subject to eligibility—invites a person who is commanding support of majority members of Lok Sabha to form the government of India—also known as the central government or Union government—at the national level and exercise its powers. In practice the prime minister nominates the members of their council of ministers to the president. They also work upon to decide a core group of ministers (known as the cabinet), as in charge of the important functions and ministries of the government of India.

The prime minister is responsible for aiding and advising the president in distribution of work of the government to various ministries and offices and in terms of the "Government of India (Allocation of Business) Rules, 1961". The co-ordinating work is generally allocated to the Cabinet Secretariat. While the work of the government is generally divided into various Ministries, the prime minister may retain certain portfolios if they are not allocated to any member of the cabinet.

The prime minister—in consultation with the cabinet—schedules and attends the sessions of the houses of parliament and is required to answer the question from the Members of Parliament to them as the in-charge of the portfolios in the capacity as prime minister of India.

Some specific ministries/department are not allocated to anyone in the cabinet but the prime minister themself. The prime minister is usually always in charge/head of:

The prime minister represents the country in various delegations, high level meetings and international organisations that require the attendance of the highest government office, and also addresses to the nation on various issues of national or other importance.

Per of the constitution, the official communication between the union cabinet and the president are through the prime minister. Other wise constitution recognises the prime minister as a member of the union cabinet only outside the sphere of union cabinet.

The prime minister recommends to the president—among others—names for the appointment of:


As the chairperson of Appointments Committee of the Cabinet (ACC), the prime minister—on the non-binding advice of the Cabinet Secretary of India led-Senior Selection Board (SSB)—decides the postings of top civil servants, such as, secretaries, additional secretaries and joint secretaries in the government of India. Further, in the same capacity, the PM decides the assignments of top military personnel such as the Chief of the Army Staff, Chief of the Air Staff, Chief of the Naval Staff and commanders of operational and training commands. In addition, the ACC also decides the posting of Indian Police Service officers—the All India Service for policing, which staffs most of the higher level law enforcement positions at federal and state level—in the government of India.

Also, as the Minister of Personnel, Public Grievances and Pensions, the PM also exercises control over the Indian Administrative Service (IAS), the country's premier civil service, which staffs most of the senior civil service positions; the Public Enterprises Selection Board (PESB); and the Central Bureau of Investigation (CBI), except for the selection of its director, who is chosen by a committee of: (a) the prime minister, as chairperson; (b) the leader of the opposition in Lok Sabha; and (c) the chief justice.

Unlike most other countries, the prime minister does not have much influence over the selection of judges, that is done by a collegium of judges consisting of the Chief Justice of India, four senior most judges of the Supreme Court of India and the chief justice—or the senior-most judge—of the concerned state high court. The executive as a whole, however, has the right to send back a recommended name to the collegium for reconsideration, this, however, is not a full Veto power, and the collegium can still put forward rejected name.

The prime minister acts as the leader of the house of the chamber of parliament—generally the Lok Sabha—he/she belongs to. In this role, the prime minister is tasked with representing the executive in the legislature, he/she is also expected to announce important legislation, and is further expected to respond to the opposition's concerns. Article 85 of the Indian constitution confers the president with the power to convene and end extraordinary sessions of the parliament, this power, however, is exercised only on the advise of the prime minister and his/her council, so, in practice, the prime minister does exercise some control over affairs of the parliament.

Article 75 of the Constitution of India confers the parliament with the power to decide the remuneration and other benefits of the prime minister and other ministers are to be decided by the Parliament. and is renewed from time to time. The original remuneration for the prime minister and other ministers were specified in the Part B of the second schedule of the constitution, which was later removed by an amendment.

In 2010, the prime minister's office reported that he/she does not receive a formal salary, but was only entitled to monthly allowances. That same year "The Economist" reported that, on a purchasing power parity basis, the prime minister received an equivalent of $4106 per year. As a percentage of the country's per-capita GDP (gross domestic product), this is the lowest of all countries "The Economist" surveyed.

The 7, Lok Kalyan Marg—previously called the 7, Race Course Road—in New Delhi, currently serves as the official place of residence for the prime minister of India. 

The first residence of the Indian prime minister was Teen Murti Bhavan. His successor Lal Bahadur Shastri chose 10, Janpath as an official residence. Indira Gandhi resided at 1, Safdarjung Road. Rajiv Gandhi became the first prime minister to use 7, Race Course Road as his residence, which was used by his successors.

For ground travel, the prime minister uses a highly modified, armoured version of a Range Rover. The prime minister's motorcade comprises a fleet of vehicles, the core of which consists of at least three armoured BMW 7 Series sedans, two armoured Range Rovers, at least 8-10 BMW X5s, six Toyota Fortuners/Land Cruisers and at least two Mercedes-Benz Sprinter ambulances.

For air travel, Boeing 777-300ERs—designated by the call sign Air India One (AI-1 or AIC001), and maintained by the Indian Air Force—are used. Apart from aircraft, there are several helicopters used such as Mi-8 for carrying the prime minister for travelling a short distance. These aircraft and helicopters are operated by the Indian Air Force.

The Special Protection Group (SPG) is charged with protecting the sitting prime minister and his/her family.

The prime minister's Office (PMO) acts as the principal workplace of the prime minister. The office is located at South Block, and is a 20-room complex, and has the Cabinet Secretariat, the Ministry of Defence and the Ministry of External Affairs adjacent to it. The office is headed by the principal secretary to the prime minister of India, generally a former civil servant, mostly from the Indian Administrative Service (IAS) and rarely from the Indian Foreign Service (IFS).

The prime minister's spouse sometimes accompany him/her on foreign visits. The prime minister's family is also assigned protection by the Special Protection Group.

Former prime ministers are entitled to a bungalow, former prime ministers are also entitled the same facilities as those given to a serving cabinet minister, this includes a fourteen-member secretarial staff, for a period of five years; reimbursement of office expenses; six domestic executive-class air tickets each year; and security cover from the Special Protection Group. 

In addition, former prime ministers rank seventh on the Indian order of precedence, equivalent to chief ministers of states (within their respective states) and cabinet ministers As a former member of the parliament, the prime minister also receives pension after they leave office. In 2015, an former MP receives a minimum pension of per month, plus—if he/she served as an MP for more than five years— for every year served.

Some prime ministers have had significant careers after their tenure, including H. D. Deve Gowda, who remained a Member of the Lok Sabha until 2019, and Manmohan Singh continues to be a Member of the Rajya Sabha.

Prime ministers are accorded a state funeral. It is customary for states and union territories to declare a day of mourning on the occasion of death of any former prime minister.

Several institutions are named after prime ministers of India. The birth-date of Jawaharlal Nehru is celebrated as children's day in India. Prime ministers are also commemorated on postage stamps of several countries.

The prime minister presides over various funds.

The National Defence Fund (NDF) was set up the Indian government in 1962, in the aftermath of 1962 Sino-Indian War. The prime minister acts as chairperson of the fund's executive committee, while, the ministers of defence, finance and home act as the members of the executive committee, the finance minister also acts the treasurer of the committee. The secretary of the fund's executive committee is a joint secretary in the prime minister's office, dealing with the subject of NDF. The fund—according to its website—is "entirely dependent on voluntary contributions from the public and does not get any budgetary support.". Donations to the fund are 100% tax-deductible under section 80G of the "Income Tax Act, 1961".

The Prime Minister's National Relief Fund (PMNRF) was set up by the first prime minister of India—Jawaharlal Nehru—in 1948, to assist displaced people from Pakistan. The fund, now, is primarily used to assist the families of those who are killed during natural disasters such as earthquakes, cyclones and flood and secondarily to reimburse medical expenses of people with chronic and deadly diseases. Donations to the PMNRF are 100% tax-deductible under section 80G of the "Income Tax Act, 1961".

The post of Deputy Prime Minister of India is not technically a constitutional post, nor is there any mention of it in an Act of the parliament. But historically, on various occasions, different governments have assigned one of their senior ministers as the deputy prime minister. There is neither constitutional requirement for filling the post of deputy PM, nor does the post provide any kind of special powers. Typically, senior cabinet ministers like the finance minister or the home minister are appointed as Deputy Prime Minister. The post is considered to be the senior most in the cabinet after the prime minister and represents the government in his/her absence. Generally, deputy prime ministers have been appointed to strengthen the coalition governments. The first holder of this post was Vallabhbhai Patel, who was also the home minister in Jawaharlal Nehru's cabinet.




</doc>
<doc id="24454" url="https://en.wikipedia.org/wiki?curid=24454" title="Paraphyly">
Paraphyly

In taxonomy, a group is paraphyletic if it consists of the group's last common ancestor and all descendants of that ancestor excluding a few—typically only one or two—monophyletic subgroups. The group is said to be paraphyletic "with respect to" the excluded subgroups. The arrangement of the members of a paraphyletic group is called a paraphyly. The term is commonly used in phylogenetics (a subfield of biology) and in linguistics. 

The term was coined to apply to well-known taxa like Reptilia (reptiles) which, as commonly named and traditionally defined, is paraphyletic with respect to mammals and birds. Reptilia contains the last common ancestor of reptiles and all descendants of that ancestor, including all extant reptiles as well as the extinct synapsids, except for mammals and birds. Other commonly recognized paraphyletic groups include fish, monkeys, and lizards.

If many subgroups are missing from the named group, it is said to be polyparaphyletic. A paraphyletic group cannot be a clade, or monophyletic group, which is any group of species that includes a common ancestor and "all" of its descendants. Formally, a paraphyletic group is the relative complement of one or more subclades within a clade: removing one or more subclades leaves a paraphyletic group.

The term "paraphyly", or "paraphyletic", derives from the two Ancient Greek words (), meaning "beside, near", and (), meaning "genus, species", and refers to the situation in which one or several monophyletic subgroups of organisms (e.g., genera, species) are "left apart" from all other descendants of a unique common ancestor.

Conversely, the term "monophyly", or "monophyletic", builds on the Ancient Greek prefix (), meaning "alone, only, unique", and refers to the fact that a monophyletic group includes organisms consisting of "all" the descendants of a "unique" common ancestor.

By comparison, the term "polyphyly", or "polyphyletic", uses the Ancient Greek prefix (), meaning "many, a lot of", and refers to the fact that a polyphyletic group includes organisms arising from "multiple" ancestral sources.

Groups that include all the descendants of a common ancestor are said to be "monophyletic". A paraphyletic group is a monophyletic group from which one or more subsidiary clades (monophyletic groups) are excluded to form a separate group. Ereshefsky has argued that paraphyletic taxa are the result of anagenesis in the excluded group or groups.

A group whose identifying features evolved convergently in two or more lineages is "polyphyletic" (Greek πολύς ["polys"], "many"). More broadly, any taxon that is not paraphyletic or monophyletic can be called polyphyletic.

These terms were developed during the debates of the 1960s and 1970s accompanying the rise of cladistics.

Paraphyletic groupings are considered problematic by many taxonomists, as it is not possible to talk precisely about their phylogenetic relationships, their characteristic traits and literal extinction. Related terminology that may be encountered are stem group, chronospecies, budding cladogenesis, anagenesis, or 'grade' groupings. Paraphyletic groups are often a relic from previous erroneous assessments about phylogenic relationships, or from before the rise of cladistics.

The prokaryotes (single-celled life forms without cell nuclei), because they exclude the eukaryotes, a descendant group. Bacteria and Archaea are prokaryotes, but archaea and eukaryotes share a common ancestor that is not ancestral to the bacteria. The prokaryote/eukaryote distinction was proposed by Edouard Chatton in 1937 and was generally accepted after being adopted by Roger Stanier and C.B. van Niel in 1962. The botanical code (the ICBN, now the ICN) abandoned consideration of bacterial nomenclature in 1975; currently, prokaryotic nomenclature is regulated under the ICNB with a starting date of 1 January 1980 (in contrast to a 1753 start date under the ICBN/ICN).

Among plants, dicotyledons (in the traditional sense) are paraphyletic because the group excludes monocotyledons. "Dicotyledon" has not been used as a botanic classification for decades, but is allowed as a synonym of Magnoliopsida. Phylogenetic analysis indicates that the monocots are a development from a dicot ancestor. Excluding monocots from the dicots makes the latter a paraphyletic group.

Among animals, several familiar groups are not, in fact, clades. The order Artiodactyla (even-toed ungulates) is paraphyletic because it excludes Cetaceans (whales, dolphins, etc.). In the ICZN Code, the two taxa are orders of equal rank. Molecular studies, however, have shown that the Cetacea descend from artiodactyl ancestors, although the precise phylogeny within the order remains uncertain. Without the Cetacean descendants the Artiodactyls must be paraphyletic.
The class Reptilia "as traditionally defined" is paraphyletic because it excludes birds (class Aves) and mammals. In the ICZN Code, the three taxa are classes of equal rank. However, mammals hail from the synapsids (which were once described as "mammal-like reptiles") and birds are descended from the dinosaurs (a group of Diapsida), both of which are reptiles. Alternatively, reptiles are paraphyletic because they gave rise to (only) birds. Birds and reptiles together make Sauropsids.
Osteichthyes, bony fish, are paraphyletic when they include only Actinopterygii (ray-finned fish) and Sarcopterygii (lungfish, etc.), excluding tetrapods; more recently, Osteichthyes is treated as a clade, including the tetrapods.
The wasps are paraphyletic, consisting of the narrow-waisted Apocrita without the ants and bees. The sawflies (Symphyta) are similarly paraphyletic, forming all of the Hymenoptera except for the Apocrita, a clade deep within the sawfly tree.
Crustaceans are not a clade because the Hexapoda (insects) are excluded. The modern clade that spans all of them is the Tetraconata.

Species have a special status in systematics as being an observable feature of nature itself and as the basic unit of classification. The phylogenetic species concept requires species to be monophyletic, but paraphyletic species are common in nature. Paraphyly is common in speciation, whereby a mother species (a paraspecies) gives rise to a daughter species without itself becoming extinct. Research indicates as many as 20 percent of all animal species and between 20 and 50 percent of plant species are paraphyletic. Accounting for these facts, some taxonomists argue that paraphyly is a trait of nature that should be acknowledged at higher taxonomic levels.

When the appearance of significant traits has led a subclade on an evolutionary path very divergent from that of a more inclusive clade, it often makes sense to study the paraphyletic group that remains without considering the larger clade. For example, the Neogene evolution of the Artiodactyla (even-toed ungulates, like deer) has taken place in an environment so different from that of the Cetacea (whales, dolphins, and porpoises) that the Artiodactyla are often studied in isolation even though the cetaceans are a descendant group. The prokaryote group is another example; it is paraphyletic because it excludes many of its descendant organisms (the eukaryotes), but it is very useful because it has a clearly defined and significant distinction (absence of a cell nucleus, a plesiomorphy) from its excluded descendants.

Also, paraphyletic groups are involved in evolutionary transitions, the development of the first tetrapods from their ancestors for example. Any name given to these ancestors to distinguish them from tetrapods—"fish", for example—necessarily picks out a paraphyletic group, because the descendant tetrapods are not included.

The term "evolutionary grade" is sometimes used for paraphyletic groups.
Moreover, the concepts of monophyly, paraphyly, and polyphyly have been used in deducing key genes for barcoding of diverse group of species.

Viviparity, the production of offspring without the laying of a fertilized egg, developed independently in the lineages that led to humans ("Homo sapiens") and southern water skinks ("Eulampus tympanum", a kind of lizard). Put another way, at least one of the lineages that led to these species from their last common ancestor contains nonviviparous animals, the pelycosaurs ancestral to mammals; vivipary appeared subsequently in the mammal lineage.

Independently-developed traits like these cannot be used to distinguish paraphyletic groups because paraphyly requires the excluded groups to be monophyletic. Pelycosaurs were descended from the last common ancestor of skinks and humans, so vivipary could be paraphyletic only if the pelycosaurs were part of an excluded monophyletic group. Because this group is monophyletic, it contains all descendants of the pelycosaurs; because it is excluded, it contains no viviparous animals. This does not work, because humans are among these descendants. Vivipary in a group that includes humans and skinks cannot be paraphyletic.


The following list recapitulates a number of paraphyletic groups proposed in the literature, and provides the corresponding monophyletic taxa.

The concept of paraphyly has also been applied to historical linguistics, where the methods of cladistics have found some utility in comparing languages. For instance, the Formosan languages form a paraphyletic group of the Austronesian languages because they consist of the nine branches of the Austronesian family that are not Malayo-Polynesian and are restricted to the island of Taiwan.



</doc>
<doc id="24455" url="https://en.wikipedia.org/wiki?curid=24455" title="Pope Innocent III">
Pope Innocent III

Pope Innocent III (; 1160 or 1161 – 16 July 1216), born Lotario dei Conti di Segni (anglicized as Lothar of Segni), held office from 8 January 1198 to his death.

Pope Innocent was one of the most powerful and influential of the medieval popes. He exerted a wide influence over the Christian states of Europe, claiming supremacy over all of Europe's kings. He was central in supporting the Catholic Church's reforms of ecclesiastical affairs through his decretals and the Fourth Lateran Council. This resulted in a considerable refinement of Western canon law. He is furthermore notable for using interdict and other censures to compel princes to obey his decisions, although these measures were not uniformly successful.

Innocent greatly extended the scope of the Crusades, directing crusades against Muslim Spain and the Holy Land as well as the Albigensian Crusade against the Cathars in southern France. 
He organized the Fourth Crusade of 1202–1204, which ended in the sack of Constantinople. Although the attack on Constantinople went against his explicit orders, and the Crusaders were subsequently excommunicated, Innocent reluctantly accepted this result, seeing it as the will of God to reunite the Latin and Orthodox Churches. In the event, the sack of Constantinople and the subsequent period of "Frankokratia" heightened the hostility between the Latin and Greek churches. (The Byzantine empire was restored in 1261 but never regained its former strength, finally falling in 1453.)

Lotario de' Conti was born in Gavignano, Italy, near Anagni. His father Count Trasimund of Segni was a member of a famous house, Conti di Segni (Earl of Segni), which produced nine popes including Gregory IX, Alexander IV and Innocent XIII. Lotario was the nephew of Pope Clement III; his mother, Claricia Scotti (Romani de Scotti), was from the same noble Roman family.

Lotario received his early education in Rome, probably at the Benedictine abbey of St Andrea al Celio, under Peter Ismael; he studied theology in Paris under the theologians Peter of Poitiers, Melior of Pisa, and Peter of Corbeil, and (possibly) jurisprudence in Bologna, according to the "Gesta" (between 1187 and 1189). As pope, Lotario was to play a major role in the shaping of canon law through conciliar canons and decretal letters.

Shortly after the death of Alexander III (30 August 1181) Lotario returned to Rome and held various ecclesiastical offices during the short reigns of Lucius III, Urban III, Gregory VIII, and Clement III, reaching the rank of Cardinal-Deacon in 1190.

As a cardinal, Lotario wrote "De miseria humanae conditionis" (On the Misery of the Human Condition). The work was very popular for centuries, surviving in more than 700 manuscripts. Although he never returned to the complementary work he intended to write, "On the Dignity of Human Nature", Bartolomeo Facio (1400–1457) took up the task writing "De excellentia ac praestantia hominis".

Celestine III died on 8 January 1198. Before his death he had urged the College of Cardinals to elect Giovanni di San Paolo as his successor, but Lotario de' Conti was elected pope in the ruins of the ancient Septizodium, near the Circus Maximus in Rome after only two ballots on the very day on which Celestine III died. He was only thirty-seven years old at the time. He took the name Innocent III, maybe as a reference to his predecessor Innocent II (1130–1143), who had succeeded in asserting the papacy's authority over the emperor (in contrast with Celestine III's recent policy).

As pope, Innocent III began with a very wide sense of his responsibility and of his authority. During Innocent III's reign, the papacy was at the height of its powers. He was considered to be the most powerful person in Europe at the time. In 1198, Innocent wrote to the prefect Acerbius and the nobles of Tuscany expressing his support of the medieval political theory of the sun and the moon. His papacy asserted the absolute spiritual authority of his office, while still respecting the temporal authority of kings.

The Muslim recapture of Jerusalem in 1187 was to him a divine judgment on the moral lapses of Christian princes. He was also determined to protect what he called "the liberty of the Church" from inroads by secular princes. This determination meant, among other things, that princes should not be involved in the selection of bishops, and it was focused especially on the "patrimonium" of the papacy, the section of central Italy claimed by the popes and later called the Papal States. The patrimonium was routinely threatened by Hohenstaufen German kings who, as Roman emperors, claimed it for themselves. Emperor Henry VI expected his infant son Frederick to bring Germany, Italy, and Sicily under a single ruler, which would leave the Papal States exceedingly vulnerable.

Henry's early death left his 3-year-old son, Frederick, as king of Sicily. Henry VI's widow, Constance of Sicily, ruled over Sicily for her young son before he reached the age of majority. She was as eager to remove German power from the kingdom of Sicily as was Innocent III. Before her death in 1198, she named Innocent as guardian of the young Frederick until he reached his maturity. In exchange, Innocent was also able to recover papal rights in Sicily that had been surrendered decades earlier to King William I of Sicily by Pope Adrian IV. The Pope invested the young Frederick II as King of Sicily in November 1198. He also later induced Frederick II to marry Constance of Aragon, the widow of King Emeric of Hungary, in 1209.

Innocent was concerned that the marriage of Henry VI and Constance of Sicily gave the Hohenstaufens a claim to all the Italian peninsula with the exception of the Papal States, which would be surrounded by Imperial territory.

After the death of Emperor Henry VI, who had recently also conquered the Kingdom of Sicily, the succession became disputed: as Henry's son Frederick was still a small child, the partisans of the Staufen dynasty elected Henry's brother, Philip, Duke of Swabia, king in March 1198, whereas the princes opposed to the Staufen dynasty elected Otto, Duke of Brunswick, of the House of Welf. King Philip II of France supported Philip's claim, whereas King Richard I of England supported his nephew Otto.

In 1201, the pope openly espoused the side of Otto IV, whose family had always been opposed to the house of Hohenstaufen.
It is the business of the pope to look after the interests of the Roman empire, since the empire derives its origin and its final authority from the papacy; its origin, because it was originally transferred from Greece by and for the sake of the papacy; ... its final authority, because the emperor is raised to his position by the pope who blesses him, crowns him and invests him with the empire. ...Therefore, since three persons have lately been elected king by different parties, namely the youth [Frederick, son of Henry VI], Philip [of Hohenstaufen, brother of Henry VI], and Otto [of Brunswick, of the Welf family], so also three things must be taken into account in regard to each one, namely: the legality, the suitability and the expediency of his election. ...Far be it from us that we should defer to man rather than to God, or that we should fear the countenance of the powerful. ...On the foregoing grounds, then, we decide that the youth should not at present be given the empire; we utterly reject Philip for his manifest unfitness and we order his usurpation to be resisted by all ... since Otto is not only himself devoted to the church, but comes from devout ancestors on both sides, ... therefore we decree that he ought to be accepted and supported as king, and ought to be given the crown of empire, after the rights of the Roman church have been secured. "Papal Decree on the choice of a German King, 1201"
The confusion in the Empire allowed Innocent to drive out the imperial feudal lords from Ancona, Spoleto and Perugia, who had been installed by Emperor Henry VI. On 3 July 1201, the papal legate, Cardinal-Bishop Guido of Palestrina, announced to the people, in the cathedral of Cologne, that Otto IV had been approved by the pope as Roman king and threatened with excommunication all those who refused to acknowledge him. At the same time, Innocent encouraged the cities in Tuscany to form a league called the League of San Genesio against German imperial interests in Italy, and they placed themselves under Innocent's protection.

In May 1202, Innocent issued the decree "Venerabilem", addressed to the Duke of Zähringen, in which he explained his thinking on the relation between the papacy and the Empire. This decree was afterwards embodied in the "Corpus Juris Canonici" and contained the following items:

Despite papal support, Otto could not oust his rival Philip until the latter was murdered in a private feud. His rule now undisputed, Otto reneged on his earlier promises and set his sights on reestablishing Imperial power in Italy and claiming even the Kingdom of Sicily. Given the papal interest to keep Germany and Sicily apart, Innocent now supported his ward, King Frederick of Sicily, to resist Otto's advances and restore the Staufen dynasty to the Holy Roman Empire. Frederick was duly elected by the Staufen partisans.

The conflict was decided by the Battle of Bouvines on 27 July 1214, which pitted Otto, allied to King John of England against Philip II Augustus. Otto was defeated by the French and thereafter lost all influence. He died on 19 May 1218, leaving Frederick II the undisputed emperor. Meanwhile, King John was forced to acknowledge the Pope as his feudal lord and accept Stephen Langton as Archbishop of Canterbury. In his turn, Frederick II would later become a bitter opponent of the papacy once his empire was secure.

Innocent III played further roles in the politics of Norway, France, Sweden, Bulgaria, Spain and England. At the request of England's King John, Pope Innocent III declared the Magna Carta annulled, resulting in a rebellion by the English Barons who rejected the disenfranchisement.

Innocent III was a vigorous opponent of religious heresy and undertook campaigns to extirpate it. At the beginning of his pontificate, he focused on the Albigenses (Cathars), a sect that had become widespread in southwestern France, then under the control of local princes such as the Counts of Toulouse. The Cathars rejected the Christian authority and teachings of the Catholic Church, denouncing it as corrupt. In 1198, Innocent III dispatched a monk named Rainier to visit France with the power to excommunicate heretics, and orders to local temporal authorities to confiscate the lands of heretics or "as became Christians to deal with them more severely."

In 1208, Innocent's legate Pierre de Castelnau was murdered by unknown assailants commonly believed to be friends of Count Raymond of Toulouse (who was not a Cathar himself but was seen as supportive of them). This caused Innocent to change his methods from words to weapons, calling upon King Philip II Augustus of France to suppress the Albigenses. Prosecuted primarily by the French crown under the generalship of Simon de Montfort, 5th Earl of Leicester, a campaign was launched. The Albigensian Crusade led to the deaths of approximately 20,000 men, women and children, Cathar and Catholic alike, decimating the number of practising Cathars and diminishing the region's distinct culture. The conflict took on a political flavor, directed not only against heretical Christians, but also the nobility of Toulouse and vassals of the Crown of Aragon, and finally brought the region firmly under the control of the king of France. King Peter II of Aragon, Count of Barcelona, was directly involved in the conflict, and was killed in the course of the Battle of Muret in 1213. The conflict largely ended with the Treaty of Paris of 1229, in which the integration of the Occitan territory in the French crown was agreed upon.

Pope Innocent III spent a majority of his tenure as Pope (1198–1216) preparing for a great crusade on the Holy Land. His first attempt was the Fourth Crusade (1202–1204) which he decreed by the papal bull "Post miserabile" in 1198. Unlike past popes, Innocent III displayed interest in leading the crusade himself, rather than simply instigating it and allowing secular leaders to organize the expedition according to their own aspirations.

Innocent III's first order of business in preaching the crusade was to send missionaries to every Catholic state to endorse the campaign. He sent Peter of Capua to the kings of France and England with specific instructions to convince them to settle their differences, resulting in a truce of five years between the two nations, beginning in 1199. The intent of the truce was not to allow the two kings to lead the crusade, but rather to free their resources to assist the Crusade. For the army's leadership, Innocent aimed his pleas at the knights and nobles of Europe, succeeding in France, where many lords answered the pope's call, including the army's two eventual leaders, Theobald of Champagne and Boniface, marquis of Montferrat. The pope's calls to action were not received with as much enthusiasm in England or Germany, and the expedition became mainly a French affair.

The Fourth Crusade was an expensive endeavor. Innocent III chose to raise funds with a new approach: requiring all clergy to donate one fortieth of their income. This marked the first time a pope ever imposed a direct tax on the clergy. He faced many difficulties in collecting this tax, including corrupt tax collectors and disregard in England. He also sent envoys to King John of England and King Philip of France, who pledged to contribute to the campaign, and John also declared his support for the clerical tax in his kingdom. The crusaders too contributed funds: Innocent declared that those who took the crusader's vow, but could no longer fulfill it, could be released by a contribution of funds. The pope put Archbishop Hubert Walter in charge of collecting these dues.

At the onset of the crusade, the intended destination was Egypt, as the Christians and Muslims were under a truce at the time. An agreement was made between the French Crusaders and the Venetians. The Venetians would supply vessels and supplies for the crusaders and in return the crusaders would pay 85,000 marks (£200,000). Innocent gave his approval of this agreement under two conditions: a representative of the pope must accompany the crusade, and the attack on any other Christians was strictly forbidden. The French failed to raise sufficient funds for payment of the Venetians. As a result, the Crusaders diverted the crusade to the Christian city of Zara at the will of the Venetians, to subsidize the debt. This diversion was adopted without the consent of Innocent III, who threatened excommunication to any who took part in the attack. A majority of the French ignored the threat and attacked Zara, and were excommunicated by Innocent III, but soon were forgiven so as to continue the crusade. A second diversion then occurred when the crusaders decided to conquer Constantinople, the capital of the Byzantine Empire. This diversion was taken without any knowledge by Innocent III, and he did not learn of it until after the city had been captured.

Innocent was deeply oposed and sent many warning letters not to sack the city. The pope excommunicated the crusaders who attacked Christian cities, but was unable to halt or overturn their actions. The crusade did lead to the start of the Latin Empire's rule of Constantinople, which lasted for the next sixty years.

In 1209, Francis of Assisi led his first eleven followers to Rome to seek permission from Pope Innocent III to found a new religious Order, which was ultimately granted. Upon entry to Rome, the brothers encountered Bishop Guido of Assisi, who had in his company Giovanni di San Paolo, the Cardinal Bishop of Sabina. The Cardinal, who was the confessor of Pope Innocent III, was immediately sympathetic to Francis and agreed to represent Francis to the pope. Reluctantly, Pope Innocent agreed to meet with Francis and the brothers the next day. After several days, the pope agreed to admit the group informally, adding that when God increased the group in grace and number, they could return for an official admittance. The group was tonsured. This was important in part because it recognized Church authority and protected his followers from possible accusations of heresy, as had happened to the Waldensians decades earlier. Though Pope Innocent initially had his doubts, following a dream in which he saw Francis holding up the Basilica of St. John Lateran (the cathedral of Rome, thus the 'home church' of all Christendom), he decided to endorse Francis's Order. This occurred, according to tradition, on 16 April 1210, and constituted the official founding of the Franciscan Order. The group, then the "Lesser Brothers" ("Order of Friars Minor" also known as the "Franciscan Order"), preached on the streets and had no possessions. They were centered in Porziuncola and preached first in Umbria, before expanding throughout Italy.

On 15 November 1215 Innocent opened the Fourth Lateran Council, considered the most important church council of the Middle Ages. By its conclusion it issued seventy reformatory decrees. Among other things, it encouraged creating schools and holding clergy to a higher standard than the laity. Canon 18 forbade clergymen to participate in the practice of the judicial ordeal, effectively banning its use.

In order to define fundamental doctrines, the council reviewed the nature of the Eucharist, the ordered annual confession of sins, and prescribed detailed procedures for the election of bishops. The council also mandated a strict lifestyle for clergy. Canon 68 states: Jews and Muslims shall wear a special dress to enable them to be distinguished from Christians so that no Christian shall come to marry them ignorant of who they are. Canon 69 forbade "that Jews be given preferment in public office since this offers them the pretext to vent their wrath against the Christians." It assumes that Jews blaspheme Christ, and therefore, as it would be "too absurd for a blasphemer of Christ to exercise power over Christians", Jews should not be appointed to public offices.

The Council had set the beginning of the Fifth Crusade for 1217, under the direct leadership of the Church. After the Council, in the spring of 1216, Innocent moved to northern Italy in an attempt to reconcile the maritime cities of Pisa and Genoa by removing the excommunication cast over Pisa by his predecessor Celestine III and concluding a pact with Genoa.

Innocent III, however, died suddenly at Perugia on 16 June 1216. He was buried in the cathedral of Perugia, where his body remained until Pope Leo XIII had it transferred to the Lateran in December 1891.

Innocent is one of two popes (the other being Gregory IX) among the 23 historical figures depicted in marble relief portraits above the gallery doors of the U.S. House of Representatives in honor of their influence on the development of American law.

His Latin works include "De miseria humanae conditionis", a tract on asceticism that Innocent III wrote before becoming pope, and "De sacro altaris mysterio", a description and exegesis of the liturgy.






</doc>
<doc id="24458" url="https://en.wikipedia.org/wiki?curid=24458" title="Polyvinyl chloride">
Polyvinyl chloride

Polyvinyl chloride (; colloquial: polyvinyl, vinyl; abbreviated: PVC) is the world's third-most widely produced synthetic plastic polymer (after polyethylene and polypropylene). About 40 million tons of PVC are produced each year.

PVC comes in two basic forms: rigid (sometimes abbreviated as RPVC) and flexible. The rigid form of PVC is used in construction for pipe and in profile applications such as doors and windows. It is also used in making bottles, non-food packaging, food-covering sheets, and cards (such as bank or membership cards). It can be made softer and more flexible by the addition of plasticizers, the most widely used being phthalates. In this form, it is also used in plumbing, electrical cable insulation, imitation leather, flooring, signage, phonograph records, inflatable products, and many applications where it replaces rubber. With cotton or linen, it is used in the production of canvas.

Pure polyvinyl chloride is a white, brittle solid. It is insoluble in alcohol but slightly soluble in tetrahydrofuran.

PVC was synthesized in 1872 by German chemist Eugen Baumann after extended investigation and experimentation. The polymer appeared as a white solid inside a flask of vinyl chloride that had been left on a shelf sheltered from sunlight for four weeks. In the early 20th century, the Russian chemist Ivan Ostromislensky and Fritz Klatte of the German chemical company Griesheim-Elektron both attempted to use PVC in commercial products, but difficulties in processing the rigid, sometimes brittle polymer thwarted their efforts. Waldo Semon and the B.F. Goodrich Company developed a method in 1926 to plasticize PVC by blending it with various additives. The result was a more flexible and more easily processed material that soon achieved widespread commercial use.

Polyvinyl chloride is produced by polymerization of the vinyl chloride monomer (VCM), as shown.
About 80% of production involves suspension polymerization. Emulsion polymerization accounts for about 12%, and bulk polymerization accounts for 8%. Suspension polymerization affords particles with average diameters of 100–180 μm, whereas emulsion polymerization gives much smaller particles of average size around 0.2 μm. VCM and water are introduced into the reactor along with a polymerization initiator and other additives. The contents of the reaction vessel are pressurized and continually mixed to maintain the suspension and ensure a uniform particle size of the PVC resin. The reaction is exothermic and thus requires cooling. As the volume is reduced during the reaction (PVC is denser than VCM), water is continually added to the mixture to maintain the suspension.

The polymerization of VCM is started by compounds called initiators that are mixed into the droplets. These compounds break down to start the radical chain reaction. Typical initiators include dioctanoyl peroxide and dicetyl peroxydicarbonate, both of which have fragile oxygen-oxygen bonds. Some initiators start the reaction rapidly but decay quickly, and other initiators have the opposite effect. A combination of two different initiators is often used to give a uniform rate of polymerization. After the polymer has grown by about 10 times, the short polymer precipitates inside the droplet of VCM, and polymerization continues with the precipitated, solvent-swollen particles. The weight average molecular weights of commercial polymers range from 100,000 to 200,000, and the number average molecular weights range from 45,000 to 64,000.

Once the reaction has run its course, the resulting PVC slurry is degassed and stripped to remove excess VCM, which is recycled. The polymer is then passed through a centrifuge to remove water. The slurry is further dried in a hot air bed, and the resulting powder is sieved before storage or pelletization. Normally, the resulting PVC has a VCM content of less than 1 part per million. Other production processes, such as micro-suspension polymerization and emulsion polymerization, produce PVC with smaller particle sizes (10 μm vs. 120–150 μm for suspension PVC) with slightly different properties and with somewhat different sets of applications.

PVC may be manufactured from either naphtha or ethylene feedstock. 
However, in China, where there are substantial stocks, coal is the main starting material for the calcium carbide process. The acetylene so generated is then converted to VCM which usually involves the use of a mercury-based catalyst. The process is also very energy intensive with much waste generated.

The polymers are linear and are strong. The monomers are mainly arranged head-to-tail, meaning that there are chlorides on alternating carbon centres. PVC has mainly an atactic stereochemistry, which means that the relative stereochemistry of the chloride centres are random. Some degree of syndiotacticity of the chain gives a few percent crystallinity that is influential on the properties of the material. About 57% of the mass of PVC is chlorine. The presence of chloride groups gives the polymer very different properties from the structurally related material polyethylene. The density is also higher than these structurally related plastics.

About half of the world's PVC production capacity is in China, despite the closure of many Chinese PVC plants due to issues complying with environmental regulations and poor capacities of scale. The largest single producer of PVC as of 2018 is Shin-Etsu Chemical of Japan, with a global share of around 30%. The other major suppliers are based in North America and Western Europe.

The product of the polymerization process is unmodified PVC. Before PVC can be made into finished products, it always requires conversion into a compound by the incorporation of additives (but not necessarily all of the following) such as heat stabilizers, UV stabilizers, plasticizers, processing aids, impact modifiers, thermal modifiers, fillers, flame retardants, biocides, blowing agents and smoke suppressors, and, optionally, pigments. The choice of additives used for the PVC finished product is controlled by the cost performance requirements of the end use specification (underground pipe, window frames, intravenous tubing and flooring all have very different ingredients to suit their performance requirements). Previously, polychlorinated biphenyls (PCBs) were added to certain PVC products as flame retardants and stabilizers.

Most vinyl products contain plasticizers which are used to make the material softer and more flexible, and lower the glass transition temperature. Plasticizers work by increasing the space and act as a lubricant between the PVC polymer chains. Higher levels of plasticizer result in softer PVC compounds and decrease tensile strength.

A wide variety of substances can be used as plasticizers including phthalates, adipates, trimellitates, polymeric plasticizers and expoxidized vegetable oils. PVC compounds can be created with a very wide range of physical and chemical properties based on the types and amounts of plasticizers and other additives used. Additional selection criteria include their compatibility with the polymer, volatility levels, cost, chemical resistance, flammability and processing characteristics. These materials are usually oily colourless substances that mix well with the PVC particles. About 90% of the plasticizer market, estimated to be millions of tons per year worldwide, is dedicated to PVC.

The most common class of plasticizers used in PVC is phthalates, which are diesters of phthalic acid. Phthalates can be categorized as high and low, depending on their molecular weight. Low phthalates such as DEHP and DBP have increased health risks and are generally being phased out. High-molecular-weight phthalates such as DINP, DIDP and DOP are generally considered safer.

While di-2-ethylhexylphthalate (DEHP) has been medically approved for many years for use in medical devices, it was permanently banned for use in children's products in the US in 2008 by US Congress; the PVC-DEHP combination had proved to be very suitable for making blood bags because DEHP stabilizes red blood cells, minimizing hemolysis (red blood cell rupture). However, DEHP is coming under increasing pressure in Europe. The assessment of potential risks related to phthalates, and in particular the use of DEHP in PVC medical devices, was subject to scientific and policy review by the European Union authorities, and on 21 March 2010, a specific labeling requirement was introduced across the EU for all devices containing phthalates that are classified as CMR (carcinogenic, mutagenic or toxic to reproduction). The label aims to enable healthcare professionals to use this equipment safely, and, where needed, take appropriate precautionary measures for patients at risk of over-exposure.

DEHP alternatives, which are gradually replacing it, are adipates, butyryltrihexylcitrate (BTHC), cyclohexane-1,2-dicarboxylic acid diisononylester (DINCH), di(2-ethylhexyl)terephthalate, polymerics and trimellitic acid, and 2-ethylhexylester (TOTM).

Liquid mixed metal stabilisers are used in several PVC flexible applications such as calendered films, extruded profiles, injection moulded soles and footwear, extruded hoses and plastisols where PVC paste is spread on to a backing (flooring, wall covering, artificial leather). Liquid mixed metal stabiliser systems are primarily based on barium, zinc and calcium carboxylates. In general liquid mixed metals like BaZn and CaZn require the addition of co-stabilisers, antioxidants and organophosphites to provide optimum performance.

BaZn stabilisers have successfully replaced cadmium-based stabilisers in Europe in many PVC semi-rigid and flexible applications.

In Europe, particularly Belgium, there has been a commitment to eliminate the use of cadmium (previously used as a part component of heat stabilizers in window profiles) and phase out lead-based heat stabilizers (as used in pipe and profile areas) such as liquid autodiachromate and calcium polyhydrocummate by 2015. According to the final report of "Vinyl 2010", cadmium was eliminated across Europe by 2007. The progressive substitution of lead-based stabilizers is also confirmed in the same document showing a reduction of 75% since 2000 and ongoing. This is confirmed by the corresponding growth in calcium-based stabilizers, used as an alternative to lead-based stabilizers, more and more, also outside Europe.

Tin-based stabilizers are mainly used in Europe for rigid, transparent applications due to the high temperature processing conditions used. The situation in North America is different where tin systems are used for almost all rigid PVC applications.
Tin stabilizers can be divided into two main groups, the first group containing those with tin-oxygen bonds and the second group with tin-sulfur bonds.

One of the most crucial additives are heat stabilizers. These agents minimize loss of HCl, a degradation process that starts above 70 °C (158 °F). Once dehydrochlorination starts, it is autocatalytic. Many diverse agents have been used including, traditionally, derivatives of heavy metals (lead, cadmium). Metallic soaps (metal "salts" of fatty acids) are common in flexible PVC applications, species such as calcium stearate. Addition levels vary typically from 2% to 4%. Tin mercaptides are widely used globally in rigid PVC applications due to their high efficiency and proven performance. Typical usage levels are 0.3 (pipe) to 2.5 phr (foam) depending on the application. Tin stabilizers are the preferred stabilizers for high output PVC and CPVC extrusion. Tin stabilizers have been in use for over 50 years by companies such as PMC organometallix and its predecessors. The choice of the best PVC stabilizer depends on its cost effectiveness in the end use application, performance specification requirements, processing technology and regulatory approvals.

PVC is a thermoplastic polymer. Its properties are usually categorized based on rigid and flexible PVCs.
PVC has high hardness and mechanical properties. The mechanical properties enhance with the molecular weight increasing but decrease with the temperature increasing. The mechanical properties of rigid PVC (uPVC) are very good; the elastic modulus can reach 1500–3,000 MPa. The soft PVC (flexible PVC) elastic limit is 1.5–15 MPa.

The heat stability of raw PVC is very poor, so the addition of a heat stabilizer during the process is necessary in order to ensure the product's properties. Traditional product PVC has a maximum operating temperature around 140 °F (60 °C) when heat distortion begins to occur. Melting temperatures range from to 500 °F (260 °C) depending upon manufacture additives to the PVC. The linear expansion coefficient of rigid PVC is small and has good flame retardancy, the limiting oxygen index (LOI) being up to 45 or more. The LOI is the minimum concentration of oxygen, expressed as a percentage, that will support combustion of a polymer and noting that air has 20% content of oxygen.

As a thermoplastic, PVC has an inherent insulation that aids in reducing condensation formation and resisting internal temperature changes for hot and cold liquids.

PVC is a polymer with good insulation properties, but because of its higher polar nature the electrical insulating property is inferior to non-polar polymers such as polyethylene and polypropylene.

Since the dielectric constant, dielectric loss tangent value, and volume resistivity are high, the corona resistance is not very good, and it is generally suitable for medium or low voltage and low frequency insulation materials.

PVC is chemically resistant to acids, salts, bases, fats, and alcohols, making it resistant to the corrosive effects of sewage, which is why it is so extensively utilized in sewer piping systems. It is also resistant to some solvents, this, however, is reserved mainly for uPVC (unplasticized PVC). Plasticized PVC, also known as PVC-P, is in some cases less resistant to solvents. For example, PVC is resistant to fuel and some paint thinners. Some solvents may only swell it or deform it but not dissolve it, but some, like tetrahydrofuran or acetone, may damage it.

Roughly half of the world's PVC resin manufactured annually is used for producing pipes for municipal and industrial applications. In the private homeowner market, it accounts for 66% of the household market in the US, and in household sanitary sewer pipe applications, it accounts for 75%. Buried PVC pipes in both water and sanitary sewer applications that are 100 mm (4 in) in diameter and larger are typically joined by means of a gasket-sealed joint. The most common type of gasket utilized in North America is a metal reinforced elastomer, commonly referred to as a Rieber sealing system. Its lightweight, low cost, and low maintenance make it attractive. However, it must be carefully installed and bedded to ensure longitudinal cracking and overbelling does not occur. Additionally, PVC pipes can be fused together using various solvent cements, or heat-fused (butt-fusion process, similar to joining high-density polyethylene (HDPE) pipe), creating permanent joints that are virtually impervious to leakage.

In February 2007 the California Building Standards Code was updated to approve the use of chlorinated polyvinyl chloride (CPVC) pipe for use in residential water supply piping systems. CPVC has been a nationally accepted material in the US since 1982; California, however, has permitted only limited use since 2001. The Department of Housing and Community Development prepared and certified an environmental impact statement resulting in a recommendation that the commission adopt and approve the use of CPVC. The commission's vote was unanimous, and CPVC has been placed in the 2007 California Plumbing Code.

PVC is commonly used as the insulation on electrical cables such as teck; PVC used for this purpose needs to be plasticized.
Flexible PVC coated wire and cable for electrical use has traditionally been stabilised with lead, but these are being replaced with calcium-zinc based systems.

In a fire, PVC-coated wires can form hydrogen chloride fumes; the chlorine serves to scavenge free radicals and is the source of the material's fire retardancy. While hydrogen chloride fumes can also pose a health hazard in their own right, it dissolves in moisture and breaks down onto surfaces, particularly in areas where the air is cool enough to breathe, and is not available for inhalation. Frequently in applications where smoke is a major hazard (notably in tunnels and communal areas), PVC-free cable insulation is preferred, such as low smoke zero halogen (LSZH) insulation.

PVC is a common, strong but lightweight plastic used in construction. It is made softer and more flexible by the addition of plasticizers. If no plasticizers are added, it is known as uPVC (unplasticized polyvinyl chloride) or rigid PVC.

uPVC is extensively used in the building industry as a low-maintenance material, particularly in Ireland, the United Kingdom, in the United States and Canada. In the US and Canada it is known as vinyl or vinyl siding. The material comes in a range of colors and finishes, including a photo-effect wood finish, and is used as a substitute for painted wood, mostly for window frames and sills when installing insulated glazing in new buildings; or to replace older single-glazed windows, as it does not decompose and is weather-resistant. Other uses include fascia, and siding or weatherboarding. This material has almost entirely replaced the use of cast iron for plumbing and drainage, being used for waste pipes, drainpipes, gutters and downspouts. uPVC is known as having strong resistance against chemicals, sunlight, and oxidation from water.
Polyvinyl chloride is formed in flat sheets in a variety of thicknesses and colors. As flat sheets, PVC is often expanded to create voids in the interior of the material, providing additional thickness without additional weight and minimal extra cost (see closed-cell PVC foamboard). Sheets are cut using saws and rotary cutting equipment. Plasticized PVC is also used to produce thin, colored, or clear, adhesive-backed films referred to simply as vinyl. These films are typically cut on a computer-controlled plotter (see vinyl cutter) or printed in a wide-format printer. These sheets and films are used to produce a wide variety of commercial signage products, including car body stripes and stickers.

PVC fabric is water-resistant, used for its weather-resistant qualities in coats, skiing equipment, shoes, jackets, aprons, and sports bags.

PVC fabric has a niche role in speciality clothing, either to create an artificial leather material or at times simply for its effect. PVC clothing is common in Goth, Punk, clothing fetish and alternative fashions. PVC is less expensive than rubber, leather or latex, which it is used to simulate.

The two main application areas for single-use medically approved PVC compounds are flexible containers and tubing: containers used for blood and blood components, for urine collection or for ostomy products and tubing used for blood taking and blood giving sets, catheters, heart-lung bypass sets, hemodialysis sets etc. In Europe the consumption of PVC for medical devices is approximately 85,000 tons each year. Almost one third of plastic-based medical devices are made from PVC.
The reasons for using flexible PVC in these applications for over 50 years are numerous and based on cost effectiveness linked to transparency, light weight, softness, tear strength, kink resistance, suitability for sterilization and biocompatibility.

Flexible PVC flooring is inexpensive and used in a variety of buildings, including homes, hospitals, offices, and schools. Complex and 3D designs are possible, which are then protected by a clear wear layer. A middle vinyl foam layer also gives a comfortable and safe feel. The smooth, tough surface of the upper wear layer prevents the buildup of dirt, which prevents microbes from breeding in areas that need to be kept sterile, such as hospitals and clinics.

PVC may be extruded under pressure to encase wire rope and aircraft cable used for general purpose applications. PVC coated wire rope is easier to handle, resists corrosion and abrasion, and may be color-coded for increased visibility. It is found in a variety of industries and environments both indoor and out.

PVC has been used for a host of consumer products. One of its earliest mass-market consumer applications was vinyl record production. More recent examples include wallcovering, greenhouses, home playgrounds, foam and other toys, custom truck toppers (tarpaulins), ceiling tiles and other kinds of interior cladding.

PVC piping is cheaper than metals used in musical instrument making; it is therefore a common alternative when making instruments, often for leisure or for rarer instruments such as the contrabass flute.

PVC can be usefully modified by chlorination, which increases its chlorine content to or above 67%. Chlorinated polyvinyl chloride, (CPVC), as it is called, is produced by chlorination of aqueous solution of suspension PVC particles followed by exposure to UV light which initiates the free-radical chlorination. The reaction produces CPVC, which can be used in hotter and more corrosive environments than PVC.

Degradation during service life, or after careless disposal, is a chemical change that drastically reduces the average molecular weight of the polyvinyl chloride polymer. Since the mechanical integrity of a plastic depends on its high average molecular weight, wear and tear inevitably weakens the material. Weathering degradation of plastics results in their surface embrittlement and microcracking, yielding microparticles that continue on in the environment. Also known as microplastics, these particles act like sponges and soak up persistent organic pollutants (POPs) around them. Thus laden with high levels of POPs, the microparticles are often ingested by organisms in the biosphere.

However there is evidence that three of the polymers (HDPE, LDPE, and PP) consistently soaked up POPs at concentrations an order of magnitude higher than did the remaining two (PVC and PET). After 12 months of exposure, for example, there was a 34-fold difference in average total POPs amassed on LDPE compared to PET at one location. At another site, average total POPs adhered to HDPE was nearly 30 times that of PVC. The researchers think that differences in the size and shape of the polymer molecules can explain why some accumulate more pollutants than others. 
The fungus "Aspergillus fumigatus" effectively degrades plasticized PVC. "Phanerochaete chrysosporium" was grown on PVC in a mineral salt agar. "Phanerochaete chrysosporium", "Lentinus tigrinus", "Aspergillus niger", and "Aspergillus sydowii" can effectively degrade PVC.

Phthalates, which are incorporated into plastics as plasticizers, comprise approximately 70% of the US plasticizer market; phthalates are by design not covalently bound to the polymer matrix, which makes them highly susceptible to leaching. Phthalates are contained in plastics at high percentages. For example, they can contribute up to 40% by weight to intravenous medical bags and up to 80% by weight in medical tubing. Vinyl products are pervasive—including toys, car interiors, shower curtains, and flooring—and initially release chemical gases into the air. Some studies indicate that this outgassing of additives may contribute to health complications, and have resulted in a call for banning the use of DEHP on shower curtains, among other uses. Japanese car companies Toyota, Nissan, and Honda eliminated the use of PVC in car interiors since 2007.

In 2004 a joint Swedish-Danish research team found a statistical association between allergies in children and indoor air levels of DEHP and BBzP (butyl benzyl phthalate), which is used in vinyl flooring. In December 2006, the European Chemicals Bureau of the European Commission released a final draft risk assessment of BBzP which found "no concern" for consumer exposure including exposure to children.

Risk assessments have led to the classification of low molecular weight and labeling as Category 1B Reproductive agents. Three of these phthalates, DBP, BBP and DEHP were included on annex XIV of the REACH regulation in February 2011 and will be phased out by the EU by February 2015 unless an application for authorisation is made before July 2013 and an authorisation granted. DIBP is still on the REACH Candidate List for Authorisation. "Environmental Science & Technology", a peer-reviewed journal published by the American Chemical Society states that it is completely safe.

In 2008 the European Union's Scientific Committee on Emerging and Newly Identified Health Risks (SCENIHR) reviewed the safety of DEHP in medical devices. The SCENIHR report states that certain medical procedures used in high risk patients result in a significant exposure to DEHP and concludes there is still a reason for having some concerns about the exposure of prematurely born male babies to medical devices containing DEHP. The Committee said there are some alternative plasticizers available for which there is sufficient toxicological data to indicate a lower hazard compared to DEHP but added that the functionality of these plasticizers should be assessed before they can be used as an alternative for DEHP in PVC medical devices. Risk assessment results have shown positive results regarding the safe use of High Molecular Weight Phthalates. They have all been registered for REACH and do not require any classification for health and environmental effects, nor are they on the Candidate List for Authorisation. High phthalates are not CMR (carcinogenic, mutagenic or toxic for reproduction), neither are they considered endocrine disruptors.

In the EU Risk Assessment the European Commission has confirmed that di-isononyl phthalate (DINP) and di-isodecyl phthalate (DIDP) pose no risk to either human health or the environment from any current use.
The European Commission's findings (published in the EU Official Journal on 13 April 2006) confirm the outcome of a risk assessment involving more than 10 years of extensive scientific evaluation by EU regulators.
Following the recent adoption of EU legislation with the regard to the marketing and use of DINP in toys and childcare articles, the risk assessment conclusions clearly state that there is no need for any further measures to regulate the use of DINP.
In Europe and in some other parts of the world, the use of DINP in toys and childcare items has been restricted as a precautionary measure. In Europe, for example, DINP can no longer be used in toys and childcare items that can be put in the mouth even though the EU scientific risk assessment concluded that its use in toys does not pose a risk to human health or the environment.
The rigorous EU risk assessments, which include a high degree of conservatism and built-in safety factors, have been carried out under the strict supervision of the European Commission and provide a clear scientific evaluation on which to judge whether or not a particular substance can be safely used.

The FDA Paper titled "Safety Assessment of Di(2-ethylhexyl)phthalate (DEHP) Released from PVC Medical Devices" states that critically ill or injured patients may be at increased risk of developing adverse health effects from DEHP, not only by virtue of increased exposure relative to the general population, but also because of the physiological and pharmacodynamic changes that occur in these patients compared to healthy individuals.

Lead had previously been frequently added to PVC to improve workability and stability. Lead has been shown to leach into drinking water from PVC pipes.

In Europe the use of lead-based stabilizers was gradually replaced. The VinylPlus voluntary commitment which began in 2000, saw European Stabiliser Producers Association (ESPA) members complete the replacement of Pb-based stabilisers in 2015.

In the early 1970s, the carcinogenicity of vinyl chloride (usually called vinyl chloride monomer or VCM) was linked to cancers in workers in the polyvinyl chloride industry. Specifically workers in polymerization section of a B.F. Goodrich plant near Louisville, Kentucky, were diagnosed with liver angiosarcoma also known as hemangiosarcoma, a rare disease. Since that time, studies of PVC workers in Australia, Italy, Germany, and the UK have all associated certain types of occupational cancers with exposure to vinyl chloride, and it has become accepted that VCM is a carcinogen. Technology for removal of VCM from products has become stringent, commensurate with the associated regulations.

PVC produces HCl upon combustion almost quantitatively related to its chlorine content. Extensive studies in Europe indicate that the chlorine found in emitted dioxins is not derived from HCl in the flue gases. Instead, most dioxins arise in the condensed solid phase by the reaction of inorganic chlorides with graphitic structures in char-containing ash particles. Copper acts as a catalyst for these reactions.

Studies of household waste burning indicate consistent increases in dioxin generation with increasing PVC concentrations. According to the EPA dioxin inventory, landfill fires are likely to represent an even larger source of dioxin to the environment. A survey of international studies consistently identifies high dioxin concentrations in areas affected by open waste burning and a study that looked at the homologue pattern found the sample with the highest dioxin concentration was "typical for the pyrolysis of PVC". Other EU studies indicate that PVC likely "accounts for the overwhelming majority of chlorine that is available for dioxin formation during landfill fires."

The next largest sources of dioxin in the EPA inventory are medical and municipal waste incinerators. Various studies have been conducted that reach contradictory results. For instance a study of commercial-scale incinerators showed no relationship between the PVC content of the waste and dioxin emissions. Other studies have shown a clear correlation between dioxin formation and chloride content and indicate that PVC is a significant contributor to the formation of both dioxin and PCB in incinerators.

In February 2007, the Technical and Scientific Advisory Committee of the US Green Building Council (USGBC) released its report on a PVC avoidance related materials credit for the LEED Green Building Rating system. The report concludes that "no single material shows up as the best across all the human health and environmental impact categories, nor as the worst" but that the "risk of dioxin emissions puts PVC consistently among the worst materials for human health impacts."

In Europe the overwhelming importance of combustion conditions on dioxin formation has been established by numerous researchers. The single most important factor in forming dioxin-like compounds is the temperature of the combustion gases. Oxygen concentration also plays a major role on dioxin formation, but not the chlorine content.

The design of modern incinerators minimises PCDD/F formation by optimising the stability of the thermal process. To comply with the EU emission limit of 0.1 ng I-TEQ/m modern incinerators operate in conditions minimising dioxin formation and are equipped with pollution control devices which catch the low amounts produced. Recent information is showing for example that dioxin levels in populations near incinerators in Lisbon and Madeira have not risen since the plants began operating in 1999 and 2002 respectively.

Several studies have also shown that removing PVC from waste would not significantly reduce the quantity of dioxins emitted. The EU Commission published in July 2000 a Green Paper on the Environmental Issues of PVC" The Commission states (on page 27) that it has been suggested that the reduction of the chlorine content in the waste can contribute to the reduction of dioxin formation, even though the actual mechanism is not fully understood. The influence on the reduction is also expected to be a second or third order relationship. It is most likely that the main incineration parameters, such as the temperature and the oxygen concentration, have a major influence on the dioxin formation". The Green Paper states further that at the current levels of chlorine in municipal waste, there does not seem to be a direct quantitative relationship between chlorine content and dioxin formation.

A study commissioned by the European Commission on "Life Cycle Assessment of PVC and of principal competing materials" states that "Recent studies show that the presence of PVC has no significant effect on the amount of dioxins released through incineration of plastic waste."

The European waste hierarchy refers to the five steps included in the article 4 of the Waste Framework Directive:


The European Commission has set new rules to promote the recovery of PVC waste for use in a number of construction products. It says: "The use of recovered PVC should be encouraged in the manufacture of certain construction products because it allows the reuse of old PVC ... This avoids PVC being discarded in landfills or incinerated causing release of carbon dioxide and cadmium in the environment".

In Europe, developments in PVC waste management have been monitored by Vinyl 2010, established in 2000. Vinyl 2010's objective was to recycle 200,000 tonnes of post-consumer PVC waste per year in Europe by the end of 2010, excluding waste streams already subject to other or more specific legislation (such as the European Directives on End-of-Life Vehicles, Packaging and Waste Electric and Electronic Equipment).

Since June 2011, it is followed by VinylPlus, a new set of targets for sustainable development. Its main target is to recycle 800,000 tonnes per year of PVC by 2020 including 100,000 tonnes of "difficult to recycle" waste. One facilitator for collection and recycling of PVC waste is Recovinyl. The reported and audited mechanically recycled PVC tonnage in 2016 was 568,695 tonnes which in 2018 had increased to 739,525 tonnes.

One approach to address the problem of waste PVC is also through the process called Vinyloop. It is a mechanical recycling process using a solvent to separate PVC from other materials. This solvent turns in a closed loop process in which the solvent is recycled. Recycled PVC is used in place of virgin PVC in various applications: coatings for swimming pools, shoe soles, hoses, diaphragms tunnel, coated fabrics, PVC sheets. This recycled PVC's primary energy demand is 46 percent lower than conventional produced PVC. So the use of recycled material leads to a significant better ecological footprint. The global warming potential is 39 percent lower.

In November 2005 one of the largest hospital networks in the US, Catholic Healthcare West, signed a contract with B. Braun Melsungen for vinyl-free intravenous bags and tubing.

In January 2012 a major US West Coast healthcare provider, Kaiser Permanente, announced that it will no longer buy intravenous (IV) medical equipment made with PVC and DEHP-type plasticizers.

In 1998, the US Consumer Product Safety Commission (CPSC) arrived at a voluntary agreement with manufacturers to remove phthalates from PVC rattles, teethers, baby bottle nipples and pacifiers.

Plasticized PVC is a common material for medical gloves. Due to vinyl gloves having less flexibility and elasticity, several guidelines recommend either latex or nitrile gloves for clinical care and procedures that require manual dexterity and/or that involve patient contact for more than a brief period. Vinyl gloves show poor resistance to many chemicals, including glutaraldehyde-based products and alcohols used in formulation of disinfectants for swabbing down work surfaces or in hand rubs. The additives in PVC are also known to cause skin reactions such as allergic contact dermatitis. These are for example the antioxidant bisphenol A, the biocide benzisothiazolinone, propylene glycol/adipate polyester and ethylhexylmaleate.

PVC is made from fossil fuels, including natural gas. The production process also uses sodium chloride. Recycled PVC is broken down into small chips, impurities removed, and the product refined to make pure PVC. It can be recycled roughly seven times and has a lifespan of around 140 years.

In the UK, approximately 400 tonnes of PVC are recycled every month. Property owners can recycle it through nationwide collection depots. The Olympic Delivery Authority (ODA), for example, after initially rejecting PVC as material for different temporary venues of the London Olympics 2012, has reviewed its decision and developed a policy for its use. This policy highlighted that the functional properties of PVC make it the most appropriate material in certain circumstances while taking into consideration the environmental and social impacts across the whole life cycle, e.g. the rate for recycling or reuse and the percentage of recycled content. Temporary parts, like roofing covers of the Olympic Stadium, the Water Polo Arena, and the Royal Artillery Barracks, would be deconstructed and a part recycled in the VinyLoop process.



</doc>
<doc id="24460" url="https://en.wikipedia.org/wiki?curid=24460" title="Profession">
Profession

A profession is an occupation founded upon specialized educational training, the purpose of which is to supply disinterested objective counsel and service to others, for a direct and definite compensation, wholly apart from expectation of other business gain. The term is a truncation of the term "liberal profession", which is, in turn, an Anglicization of the French term "profession libérale". Originally borrowed by English users in the 19th century, it has been re-borrowed by international users from the late 20th, though the (upper-middle) class overtones of the term do not seem to survive retranslation: "liberal professions" are, according to the European Union's Directive on Recognition of Professional Qualifications (2005/36/EC), "those practiced on the basis of relevant professional qualifications in a personal, responsible and professionally independent capacity by those providing intellectual and conceptual services in the interest of the client and the public".

It has been said that a profession is not a trade and not an industry.

Medieval and early modern tradition recognized only three professions: divinity, medicine, and law – the so-called "learned professions".

Major milestones which may mark an occupation being identified as a profession include:


Applying these milestones to the historical sequence of development in the United States shows surveying achieving professional status first (note that George Washington, Thomas Jefferson, and Abraham Lincoln all worked as land surveyors before entering politics), followed by medicine, actuarial science, law, dentistry, civil engineering, logistics, architecture and accounting.

With the rise of technology and occupational specialization in the 19th century, other bodies began to claim professional status: mechanical engineering, pharmacy, veterinary medicine, psychology, nursing, teaching, librarianship, optometry and social work, each of which could claim, using these milestones, to have become professions by 1900.

Just as some professions rise in status and power through various stages, others may decline. Disciplines formalized more recently, such as architecture, now have equally long periods of study associated with them.

Although professions may enjoy relatively high status and public prestige, not all professionals earn high salaries, and even within specific professions there exist significant differences in salary. In law, for example, a corporate defense lawyer working on an hourly basis may earn several times what a prosecutor or public defender earns.

A profession arises when any trade or occupation transforms itself through ""the development of formal qualifications based upon education, apprenticeship, and examinations, the emergence of regulatory bodies with powers to admit and discipline members, and some degree of monopoly rights.""

Originally, any regulation of the professions was self-regulation through bodies such as the College of Physicians or the Inns of Court. With the growing role of government, statutory bodies have increasingly taken on this role, their members being appointed either by the profession or (increasingly) by government. Proposals for the introduction or enhancement of statutory regulation may be welcomed by a profession as protecting clients and enhancing its quality and reputation, or as restricting access to the profession and hence enabling higher fees to be charged. It may be resisted as limiting the members' freedom to innovate or to practice as in their professional judgement they consider best.

An example was in 2008, when the British government proposed wide statutory regulation of psychologists. The inspiration for the change was a number of problems in the psychotherapy field, but there are various kinds of psychologist including many who have no clinical role and where the case for regulation was not so clear. Work psychology brought especial disagreement, with the British Psychological Society favoring statutory regulation of "occupational psychologists" and the Association of Business Psychologists resisting the statutory regulation of "business psychologists" – descriptions of professional activity which it may not be easy to distinguish.

Besides regulating access to a profession, professional bodies may set examinations of competence and enforce adherence to an ethical code. There may be several such bodies for one profession in a single country, an example being the accountancy bodies of the United Kingdom (ACCA, CAI, CIMA, CIPFA, ICAEW and ICAS), all of which have been given a Royal Charter, although their members are not necessarily considered to hold equivalent qualifications, and which operate alongside further bodies (AAPA, IFA, CPAA). Another example of a regulatory body that governs a profession is the Hong Kong Professional Teachers Union, which governs the conduct, rights, obligations and duties of salaried teachers working in educational institutions in Hong Kong.

The engineering profession is highly regulated in some countries (Canada and USA) with a strict licensing system for Professional Engineer that controls the practice but not in others (UK) where titles and qualifications are regulated Chartered Engineer but practice is not regulated.

Typically, individuals are required by law to be qualified by a local professional body before they are permitted to practice in that profession. However, in some countries, individuals may not be required by law to be qualified by such a professional body in order to practice, as is the case for accountancy in the United Kingdom (except for auditing and insolvency work which legally require qualification by a professional body). In such cases, qualification by the professional bodies is effectively still considered a prerequisite to practice as most employers and clients stipulate that the individual hold such qualifications before hiring their services. For example, in order to become a fully qualified teaching professional in Hong Kong working in a state or government-funded school, one needs to have successfully completed a Postgraduate Diploma in Education ("PGDE") or a bachelor's degree in Education ("BEd") at an approved tertiary educational institution or university. This requirement is set out by the Educational Department Bureau of Hong Kong, which is the governmental department that governs the Hong Kong education sector.

Professions tend to be autonomous, which means they have a high degree of control of their own affairs: "professionals are autonomous insofar as they can make independent judgments about their work". This usually means "the freedom to exercise their professional judgement."

However, it also has other meanings. "Professional autonomy is often described as a claim of professionals that has to serve primarily their own interests...this professional autonomy can only be maintained if members of the profession subject their activities and decisions to a critical evaluation by other members of the profession." The concept of autonomy can therefore be seen to embrace not only judgement, but also self-interest and a continuous process of critical evaluation of ethics and procedures from within the profession itself.

One major implication of professional autonomy is the traditional ban on corporate practice of the professions, especially accounting, architecture, medicine, and law. This means that in many jurisdictions, these professionals cannot do business through regular for-profit corporations and raise capital rapidly through initial public offerings or flotations. Instead, if they wish to practice collectively they must form special business entities such as partnerships or professional corporations, which feature (1) reduced protection against liability for professional negligence and (2) severe limitations or outright prohibitions on ownership by non-professionals. The obvious implication of this is that all equity owners of the professional business entity must be professionals themselves. This avoids the possibility of a non-professional owner of the firm telling a professional how to do his or her job and thereby protects professional autonomy. The idea is that the "only" non-professional person who should be telling the professional what to do is the "client"; in other words, professional autonomy preserves the integrity of the two-party professional-client relationship. Above this client-professional relationship the profession requires the professional to use their autonomy to follow the rules of ethics that the profession requires. But because professional business entities are effectively locked out of the stock market, they tend to grow relatively slowly compared to public corporations.

Professions tend to have a high social status, regarded by society as highly important. This high esteem arises primarily from the higher social function of their work. The typical profession involves technical, specialized, and highly skilled work. This skill and experience is often referred to as "professional expertise." In the modern era, training for a profession involves obtaining degrees and certifications. Often, entry to the profession is barred without licensure. Learning new skills that are required as a profession evolves is called continuing education. Standards are set by states and associations. Leading professionals tend to police and protect their area of expertise and monitor the conduct of their fellow professionals through associations, national or otherwise. Professionals often exercise a dominating influence over related trades, setting guidelines and standards. Socially powerful professionals consolidate their power in organizations for specific goals. Working together, they can reduce bureaucratic entanglements and increase a profession's adaptability to the changing conditions of the world.

There is considerable agreement about defining the characteristic features of a profession. They have a "professional association, cognitive base, institutionalized training, licensing, work autonomy, colleague control... (and) code of ethics", to which Larson then also adds, "high standards of professional and intellectual excellence," (Larson, p. 221) that "professions are occupations with special power and prestige", (Larson, p.x) and that they comprise "an exclusive elite group," (Larson, p. 20) in all societies. Members of a profession have also been defined as "workers whose qualities of detachment, autonomy, and group allegiance are more extensive than those found among other groups...their attributes include a high degree of systematic knowledge; strong community orientation and loyalty; self-regulation; and a system of rewards defined and administered by the community of workers."

A profession has been further defined as: "a special type of occupation...(possessing) corporate solidarity...prolonged specialized training in a body of abstract knowledge, and a collectivity or service orientation...a vocational sub-culture which comprises implicit codes of behavior, generates an esprit de corps among members of the same profession, and ensures them certain occupational advantages...(also) bureaucratic structures and monopolistic privileges to perform certain types of work...professional literature, legislation, etc."

A critical characteristic of a profession is the need to cultivate and exercise professional "discretion" - that is, the ability to make case by case "judgements" that cannot be determined by an absolute rule or instruction.


References

Abbott, A. (1998). The theory of professions. Chicago, IL: University of Chicago Press.

Addams, J. (1094). Problems of municipal administration. The American Journal of Sociology, x(4), In J. Shafritz & A. Hyde (Eds.), Classics of public administration, eighth edition (pp. 51–56). Boston, Massachusetts: Cengage Learning.

Albrow, M. (1996), The global age: state and society beyond modernity. Cambridge: Polity Press.

Allen, K. (2004). Max Weber: A critical introduction. London: Pluto Press.

Arnett, P. (1968). Grim decisions for the military "It became necessary to destroy the town to save it". Retrieved from <nowiki>http://www.thisdayinquotes.com/2010/02/it-became-necessary-to-destroy-town-to.html</nowiki>

Balducchi, D. E. & Wandner, S. A. (2008). Work sharing policy: Power sharing and stalemate in American federalism. Publius, 38(1), 111–136. Retrieved from <nowiki>http://www.jstor.org/stable/4624812</nowiki>

Bailey, J. D. (2007). Thomas Jefferson and executive power. Cambridge, U.K.: Cambridge University Press.

Bardach, E. (1977). The implementation game: What happens after a bill becomes law. In J. Shafritz & A. Hyde (Eds.), Classics of public administration, eighth edition (pp. 318–331). Boston, Massachusetts: Cengage Learning.

Bell, G. & Gonzalez, A. (2011). Adaptation and evolutionary rescue in metapopulations experiencing environmental deterioration. Science, 332(6035). 1327. doi: 10.1126/science.1203105

Berling, J.A. (2019). Confucianism. Retrieved from <nowiki>https://asiasociety.org/education/confucianism</nowiki>

Berman, H. J. (19775). The religious foundations of western law, Catholic University Law Review, 24(3), 490–508.Available at: <nowiki>https://scholarship.law.edu/lawreview/vol24/iss3/4</nowiki>

Bertalanffy, L. von, (1968). General systems theory. New York: George Braziller.

Beyer, W. C. (1959). The civil service of the ancient world. Public Administration Review, 19(4), 243–249. doi: 10.2307/973271

Bradscher, J. G. (1985). Ebla's royal archives. Information Development, 1(4), 238–243. <nowiki>https://doi.org/10.1177/026666698500100410</nowiki>

Brownlow, L., Merriam, C. E., & Gulick, L. (1937). Report on the president's committee on administrative management. In J. Shafritz & A. Hyde (Eds.), Classics of public administration, eighth edition (pp. 114–118). Boston, Massachusetts: Cengage Learning.

Bryant, J. M. (2019). Ashoka and Constantine: On mega-actors and the politics of empires and religions. In F. G. Duina(Ed), States and Nations, Power and Civility: HallsianPerspectives (pp. 263–302). Toronto: University of Toronto Press.

Burns, J. M. (1963). The deadlock of democracy. Englewood Cliffs, N.J.: Prentice-Hall.

Carey, G. W. (1978). Separation of powers and the Madisonian model: A reply to the critics. American Political Review, 72(1), 151–164. doi: 10.2307/1953605

Change, Y. N. (1976). Early Chinese management thought. California Management Review, 19(2), 71–77. doi10.2307/41164698

Clausewitz, C. P. G., von, (1873). On War. In J. Shafritz & A. Hyde (Eds.), Classics of public administration, eighth edition (pp. 32–34). Boston, Massachusetts: Cengage Learning.

Clausewitz, C. P. G., von, (1832). On War. Howard, M. & Paret, P., editors & translators. Princeton, NJ: Princeton University Press

Common features of classical civilizations. Retrieved from <nowiki>https://www.historyhaven.com/COMPARISON%20%20OF%20CLASSICAL%20CIVILIZATIONS.htm</nowiki>

Columbia University, Asia for Educators. (2009). Timeline of Chinese dynasties and other key events. Retrieved fromhttp://afe.easia.columbia.edu/timelines/china_timeline.htm

Cooper, P. J. (1990). Public administration review: The first fifty years. Public Administration Review, 50(2), 293–312. doi: 10.2307/976878

Cruess, S. R., Johnston, S. & Cruess R. L. (2004). "Profession": a working definition for medical educators. Teaching and learning in Medicine,16(1): 74–76.

Dahl, R. A. (1947). The science of public administration: Three problems. Public Administration Review, 7(1), 1-11. doi: 10.2307/972349

Dahl, R. A. & Lindblom, C. E. (1953). Politics, economics, and welfare. New York: Harper.

Denhardt, R.& Denhardt, J. (2000). The new public service: serving rather than steering. Public Administration Review,60/6, 549–559.

Denhardt, R.& Denhardt, J. (2006). Public administration: An action orientation. Belmont, CA: Thomson.

Dunn, D. D., and Legge, J. S. Jr. (2001). U.S. local government managers and the complexity of responsibility and accountability in democratic governance. Journal of Public Administration Research and Theory: J-PART, 11(1),73-88. doi:10.1093/oxfordjournals.jpart.a003495

Dyson, T. (2018). A population history of India: From the first modern people to the present day. Oxford, England: Oxford University Press.

Eaton, R. M. (1993). The rise of Islam and the Bengal frontier, 1204–1760. Berkeley, CA: University of California Press.

Ellis, J. J. (2000). Founding brothers: The revolutionary generation. New York, Random House.

Feldheim, M. A. (2003). Mary parker follett lost and found - again, and again, and again. International Journal of Organization Theory & Behavior, 7/3, (341-362). doi: 10.1108/IJOTB-07-03-2004-B003

Follett, M. P. (1926). The giving of orders. In J. Shafritz & A. Hyde (Eds.), Classics of public administration, eighth edition (pp. 76–83). Boston, Massachusetts: Cengage Learning.

Finer, H. (1941). Administrative responsibility in democratic government. Public Administration Review, 1(4), 335–350. Retrieved from <nowiki>https://www.jstor.org/stable/i239812</nowiki>

Frederickson, G. H. (1971). Toward a new public administration. In J. Shafritz & A. Hyde (Eds.), Classics of public administration, eighth edition (pp. 282–294). Boston, Massachusetts: Cengage Learning.

Freidson, E. (1994). Professionalism reborn: Theory, prophecyand policy. Chicago, IL: University of Chicago Press.

Friedrich, C. J. (1940). Public policy and the nature of administrative responsibility. In C. J. Friedrich & E. S. Mason (Eds.), Public policy: A yearbook of the graduate school of public administration (pp. 1–24). Cambridge, Massachusetts: Harvard University Press.

Fry, B. and Raadschelders, J. (2008). Mastering public administration. Washington D.C.: CQ Press.

Gailmard, S. & Patty, J. W. (2007). Slackers and zealots: Civil service, policy discretion, and bureaucratic expertise. American Journal of Political Science, 51(4), 873–889. doi: 10.1111/j.1540-5907.2007.00286.x

Green, R. T. (1988). The Hamiltonian image of the public administrator: Public administrators as prudent constitutionalists. Dialogue, 10(3), 25–53. Retrieved from.

Green, R. T. (2002). Alexander Hamilton: Founder of the American public administration. Administration and Society 34(5): 541–562. doi: 10.1177/009539902237275

Gailmard, S. and Patty, J. W. (2007). Slackers and zealots: Civil service, policy discretion, and bureaucratic expertise. American Journal of Political Science, 51(4), 873–889. doi: 10.1111/j.1540-5907.2007.00286.x

Goodnow, F. J. (1900). Politics and administration: A study in government. In J. Shafritz & A. Hyde (Eds.), Classics of public administration, eighth edition (pp. 48–50). Boston, Massachusetts: Cengage Learning.

Gulick, L. (1937). Notes on the theory of organization. In J. Shafritz & A. Hyde (Eds.), Classics of public administration, eighth edition (pp. 105–114). Boston, Massachusetts: Cengage Learning.

Gulick, L. (1987). Time and public administration. Public Administration Review 47(1): 115–119. doi: 10.2307/975478

Hamilton, A. (1788). Federalist No. 72, The same subject continued, and re-eligibility of the executive considered from the New York packet, Friday, 21 March 1788. Retrieved from <nowiki>https://www.congress.gov/resources/display/content/The+Federalist+Papers#TheFederalistPapers-72</nowiki>

Herring, P. J. (2017) The historical evolution of public administration in the United States. Retrieved from <nowiki>https://medium.com/@patrickherring/the-historical-evolution-of-public-administration-in-the-united-states-58f7e30c2eec</nowiki>

Howlett, M., McConnell, A., and Pearl, A. (2014). Streams and stages: Reconciling Kingdon and policy process theory. European Journal of Political Research, 54(3) 419–434. doi: 10.1111/1475-6765.12064

Hood, C. (1991). A Public Management for All Seasons, Public Administration, 69, 3–19. doi: 10.1111/j.1467-9299.1991.tb00779.

Jiang, Y. (2011). The mandate of heaven and the great Ming code. Seattle, WA: University of Washington Press.

Karl, B. D., (1963). Executive reorganization and reform in the new deal (pp. xx-292) Cambridge: Harvard University Press.

Katz, D. & Kahn, R. (1966). The social psychology of organizations. New York: John Wiley & Sons.

Kettl, D. F. (2002). Transformation of governance: Public administration for the twenty-first century America. Baltimore: Johns Hopkins University Press. doi: 10.1111/0033-3352.00112

Kingdon, J. W. (1984). Agendas, alternatives, and public policies. In J. Shafritz & A. Hyde (Eds.), Classics of public administration, eighth edition (pp. 433–443). Boston, Massachusetts: Cengage Learning.

Kramer, S. N. (1963). The Sumerians: Their history, culture, and character. Chicago, IL: University of Chicago.

Lambert, W. G. (1990). Ancient Mesopotamian godssuperstition, philosophy, theology. Revue de l'histoire des religions, 207(2), 115–130. Retrieved from <nowiki>http://www.jstor.org/stable/23671046</nowiki>

Levin, J. (2009). Hammurabi. New York: Infobase Printing.

Lindblom, C. E. (1959). The science of "muddling through". In J. Shafritz and A. Hyde (Eds.), Classics of public administration, eighth edition, (pp. 172–182). Boston, Massachusetts: Cengage Learning.

Luton, L. S. (1999). History and American public administration. Administration and Society 31(2), 205–221. doi: 10.1177/00953999922019094

Lynn, L. (2003). Public Administration in the Twenty-First Century. Public Administration Review, 63(5), 631–635. doi: 10.1111/1540-6210.00326

Mainzer, L. C. (1994). Public administration in search of a theory: The interdisciplinary delusion. Administration and Society 26(3): 359–394. doi: 10.1177/009539979402600305

Mark, J. J. (25 September 2017). Egyptian empire definition. Retrieved from <nowiki>https://www.ancient.eu/Egyptian_Empire/</nowiki>

Maslow, A. H. (1943). A theory of human motivation. Psychological Review, 50(4), 370–396. <nowiki>https://doi.org/10.1037/h0054346</nowiki>

Shafritz, Jay M.. Classics of Public Administration (p. 616). Wadsworth Publishing. Kindle Edition.

McGregor, D. (1957). Theory Y: The integration of individual and organizational goals. McGraw-Hill n J. Shafritz & A. Hyde (Eds.), Classics of public administration, eighth edition (pp. 166–171). Boston, Massachusetts: Cengage Learning.

Merton, R. K. (1940). Bureaucratic structure and personality. Social Forces, University of North Carolina Press,18(4). 560–568. In J. Shafritz & A. Hyde (Eds.), Classics of public administration, eighth edition (pp. 119–126). Boston, Massachusetts: Cengage Learning.

Michael, S. (n.d.). Mesopotamia: Overview and summary. Retrieved from <nowiki>https://www.historyonthenet.com/mesopotamia</nowiki>

Nagarajan, K. V. (2011). The Code of Hammurabi: An economic interpretation. International Journal of Business and Social Science, 2(8), 108–117.

Newbold, S. P. (2006). All but forgotten: Thomas Jefferson's contribution to the development of public administration in the United States. Retrieved from <nowiki>https://vtechworks.lib.vt.edu/handle/10919/30191</nowiki>

Newbold, S., & Rosenbloom, D. (2007). Brownlow report retrospective. Public Administration Review, 67(6), 1006–1009. Retrieved from <nowiki>http://www.jstor.org/stable/4624660</nowiki>

Niskanen, Jr. (1971). Bureaucracy and Representative Government. New York: Imprint Routledge. doi: 10.4324/9781315081878

Olson, C. (2005). The different paths of Buddhism: A narrative-historical introduction. New Brunswick, N.J.: Rutgers University Press.

Ostrom, V., & Ostrom, E. (1971). Public choice: A different approach to the study of public administration. Public Administration Review, 31(2), 203–216. doi: 10.2307/974676

Professional Standards Council. Retrieved from <nowiki>https://www.psc.gov.au/what-is-a-profession</nowiki>

Reagan, R. (1981). Inaugural address, Retrieve from <nowiki>https://www.reaganlibrary.gov/research/speeches/inaugural-address-january-20-1981</nowiki>

Riccucci, N. M. (March 2001). The "old" public management versus the "new" public management: Where does public administration fit in? Public Administration Review, 61(2), 172–175. doi: 10.1111/0033-3352.00019

Richards, J. F., (1995). The new Cambridge history of India, I.5, the Mughal Empire. New York, NY: Cambridge University Press.

Rivlin, A. M. (1971). Systematic thinking for social action. In J. Shafritz & A. Hyde (Eds.), Classics of public administration, eighth edition (pp. 307–317). Boston, Massachusetts: Cengage Learning.

Robinson. M. (). From old public administration to the new public service – implications for public sector reform in developing countries. Retrieved from <nowiki>https://cluelesspoliticalscientist.wordpress.com/2017/02/08/from-old-public-administration-to-the-new-public-service-by-mark-robinson-a-summary/</nowiki>

Rosenbloom, D. H. (1983). Public administrative theory and the separation of powers. In J. Shafritz & A. Hyde (Eds.), Classics of public administration, eighth edition (pp. 421–432). Boston, Massachusetts: Cengage Learning.

Simon, Herbert A. (1947). Administrative Behavior. New York: Free Press.

Sinek, S. (2019). The Infinite Game. New York: Random House

Shirley, J. J. (2010). Viceroys, viziers & the amun precinct: the power of heredity and strategic marriage in the early 18th dynasty. Journal of Egyptian History, 3(1), 73-113.

Smith, A. (1761). Theory of Moral Sentiments (2 ed.). Strand & Edinburgh: Millar A.; Kincaid, A. & Bell, J. Retrieved from <nowiki>https://books.google.com/books?id=bZhZAAAAcAAJ&dq=editions%3Au_L0P5LRqXkC&pg=PP3#v=onepage&q&f=true</nowiki>

Stearman, J. D. (2001). System dynamics modeling: tools for learning in a complex world. California Management Review, 43(4), pp 8–25. doi:10.2307/41166098

Stillman, R. II, (2003). The American Public Bureaucracy. Syracuse: Cengage Learning.

Stivers, C. (1995). Settlement women and bureau men: Constructing a usable past for public administration. Public Administration Review, 55(6), 522–529. doi:10.2307/3110343

Surowiecki, J. (2005). The wisdom of crowds. New York: Random House.

Taylor, F. W. (1912). The principles of scientific management. New York: Harper and Brothers.

Taylor, E. B. (1878). Researches into the early history of mankind and the development of civilization. Boston: Estes and Lauriat.

The National Academy of Public Administration, (n.d.). Standing Panel on Social Equity in Governance. Retrieve from <nowiki>https://web.archive.org/web/20090506083627/http://napawash.org/aa_social_equity/index.html</nowiki>

Ventriss, C. (1989). "Toward a Public Philosophy of Public Administration: A Civic Perspective of the Public". Public Administration Review, 49(2), p. 173-9.

Waldo, D. (1948). The Administrative State. New York: The Ronald Press Company.

Waldo, D. (1987). Politics and administration: on thinking about a complex relationship. In Ralph C. Chandler, (Ed.), A Centennial History of the American Administrative State (pp 89–112). New York: Free Press.

Wamsley, G. L. and Wolf, J. F. Eds. (1996). Refounding Democratic Public Administration. Thousand Oaks, CA: Sage.

Weber, J. L. (25 March 2013). Was Lincoln a Tyrant? Retrieved from <nowiki>https://opinionator.blogs.nytimes.com/2013/03/25/was-lincoln-a-tyrant/</nowiki>

Weber, M. K. E. (1925). Bureaucracy. In J. Shafritz & A. Hyde (Eds.), Classics of public administration, eighth edition(pp. 63–67). Boston, Massachusetts: Cengage Learning.

White, L. D. (1951). The Jeffersonians: A study in administrative history 1801–1829. New York: MacMillan Company.

White, L. D. (1955). Introduction to the study of public administration. In J. Shafritz & A. Hyde (Eds.), Classics of public administration, eighth edition (pp. 68–75). Boston, Massachusetts: Cengage Learning.

Wilson, W. (1887). The Study of Administration. In J. Shafritz & A. Hyde (Eds.), Classics of public administration, eighth edition (pp. 35–47). Boston, Massachusetts: Cengage Learning.

Wilson, Woodrow (1887) "The Study of Administration", Political Science Quarterly, 2(2), 197–222. doi: 10.2307/2139277

Wigfall, P. M. & Kalantari, B. (2001). Biographical dictionary of public administration. Westport, CT: Greenwood Publishing Group.



</doc>
<doc id="24464" url="https://en.wikipedia.org/wiki?curid=24464" title="Philip Henry Gosse">
Philip Henry Gosse

Philip Henry Gosse FRS (; 6 April 1810 – 23 August 1888), known to his friends as Henry, was an English naturalist and popularizer of natural science, virtually the inventor of the seawater aquarium, and a painstaking innovator in the study of marine biology. Gosse created and stocked the first public aquarium at the London Zoo in 1853, and coined the term "aquarium" when he published the first manual, "The Aquarium: An Unveiling of the Wonders of the Deep Sea", in 1854. His work was the catalyst for an aquarium craze in early Victorian England. 

Gosse was also the author of "Omphalos", an attempt to reconcile the geological ages presupposed by Charles Lyell with the biblical account of creation. After his death, Gosse was portrayed as a despotic father of uncompromising religious views in "Father and Son" (1907), a memoir written by his son, Edmund Gosse, a poet and critic.

Philip Henry Gosse was born in Worcester in 1810 of an itinerant painter of miniature portraits and a lady's maid. He spent his childhood mostly in Poole, Dorset, where his aunt, Susan Bell, taught him to draw and introduced him to zoology. She had similarly taught her own son, Thomas Bell, who was twenty years older and later became a great friend to Gosse.

At fifteen he began work as a clerk in the counting house of George Garland and Sons in Poole. In 1827 he sailed to Newfoundland to serve as a clerk in the Carbonear premises of Slade, Elson and Co. There he became a dedicated, self-taught student of Newfoundland entomology, "the first person systematically to investigate and to record the entomology" of the island. In 1832 Gosse experienced a religious conversion and, as he said, "solemnly, deliberately and uprightly, took God for my God."

In 1835 he left Newfoundland for Compton, Lower Canada (Quebec), where he farmed unsuccessfully for three years. He originally tried to establish a commune with two of his religious friends. The experience deepened his love for natural history, and locals referred to him as "that crazy Englishman who goes about picking up bugs." During this time he became a member of the Natural History Society of Montreal and submitted specimens to its museum.

In 1838 Gosse taught eight months for Reuben Saffold, the owner of Belvoir plantation, near Pleasant Hill, Alabama. In this period, planters often hired private tutors to teach their children. Gosse also studied and drew the local flora and fauna, assembling an unpublished volume, "Entomologia Alabamensis", on insect life in the state. The cotton plantation was in the Black Belt of Alabama, and Saffold held numerous enslaved laborers. Gosse recorded his negative impressions of slavery, later published as "Letters from Alabama" (1859).

Returning to England in 1839, Gosse was hard pressed to make a living, subsisting on eightpence a day ("one herring eaten as slowly as possible, and a little bread"). His fortunes began to improve when John Van Voorst, the leading publisher of naturalist writing, agreed, on the recommendation of Thomas Bell, to publish his "Canadian Naturalist" (1840). The book, set as a conversation between a father and his son (a son Gosse did not yet have), was widely praised. It is now considered to demonstrate that Gosse "had a practical grasp of the importance of conservation, far ahead of his time."

Gosse opened a "Classical and Commercial School for Young Gentlemen" while keeping detailed records of his microscopic investigations of pond life, especially cyclopidae and rotifera. He also began to preach to the Wesleyan Methodists and lead a Bible class. In 1842, he became so captivated by the doctrine of the Second Coming of Christ that he severed his connection with the Methodists and joined the Plymouth Brethren. These dissenters emphasized the Second Coming while rejecting liturgy and an ordained ministry—although they otherwise endorsed the traditional doctrines of Christianity as represented by the creeds of the Methodist and the Anglican Church.

In 1843, Gosse gave up the school to write "An Introduction to Zoology" for the Society for Promoting Christian Knowledge (SPCK) and to draw some of the illustrations. Writing the work inspired him to further his interest in the flora and fauna of the seashore. He showed in his book that he was a creationist, which was typical of pre-Darwinian naturalists.
In October 1844 Gosse sailed to Jamaica, where he served as a professional collector for dealer Hugh Cuming. Although Gosse worked hard during his eighteen months on the island, he later called this period his " 'holiday' in Jamaica." Gosse's study specialized in birds, and Gosse has been called "the father of Jamaican ornithology." Gosse hired black youths as assistants and especially praised one of them, Samuel Campbell, in his Jamaican books. For Christian companionship he enjoyed the company of Moravian missionaries and their black converts, and he preached regularly to the Moravian congregation.

On his return to London in 1846, Gosse wrote a trilogy on the natural history of Jamaica including "A Naturalist's Sojourn in Jamaica" (1851). It is described as "written in a congenial style and firmly established his reputation both as a naturalist and a writer." 

In the field of herpetology, Gosse described several new species of reptiles endemic to Jamaica.

Back in England, Gosse wrote books in his field and out; he produced a quick volume for the Society for Promoting Christian Knowledge (SPCK) was "Monuments of Ancient Egypt", a land he had never visited and never would. As his financial situation stabilized, Gosse courted Emily Bowes, a forty-one-year-old member of the Brethren, who was both a strong personality and a gifted writer of evangelical tracts. They married in November 1848, and their union was an extremely happy one. As D. J. Taylor has written, "the word 'uxorious' seems to have minted to define" Gosse. Gosse's only son was born on 21 September 1849. Gosse noted the event in his diary with the words, "E. delivered of a son. Received green swallow from Jamaica"—an amusing conjunction which Edmund later described as demonstrating only the order of events: the boy had arrived first.

Gosse wrote a succession of books and articles on natural history, some of which were (in his own words) "pot-boilers" for religious publications. (At the time, accounts of God's creation were considered appropriate Sabbath reading for children.) As L. C. Croft has written, 
"Much of Gosse's success was due to the fact that he was essentially a field naturalist who was able to impart to his readers something of the thrill of studying living animals at first hand rather than the dead disjointed ones of the museum shelf. In addition to this he was a skilled scientific draughtsman who was able to illustrate his books himself."

Suffering from headaches, perhaps the result of overwork, Gosse, with his family, began to spend more time away from London on the Devon coast. Here along the sea shore, Gosse began serious experimentation with ways to sustain sea creatures so that they could be examined "without diving to gaze on them." Although there had been attempts to construct what had previously been called an "aquatic vivarium" (a name Gosse found "awkward and uncouth"), Gosse published "The Aquarium" in 1854 and set off a mid-Victorian craze for household aquariums. The book was financially profitable for Gosse, and "the reviews were full of praise". Even in this work, Gosse used natural science to point to the necessity of salvation through the blood of Christ. In 1856 Gosse was elected a Fellow of the Royal Society, which, because he had no university position or inherited wealth, gave him "a standing he otherwise lacked."

A few months before Gosse was honoured, his wife discovered that she had breast cancer. Rather than undergo surgery (a risky procedure in 1856), the Gosses decided to submit to the ointments of an American doctor, Jesse Weldon Fell, who if not a charlatan, was certainly on the fringe of contemporary medical practice. After much suffering, Emily Gosse died on 9 February 1857. She entrusted her husband with their son's salvation, and perhaps her death drove Gosse into his "strange severities and eccentric prohibitions."

In the months following Emily's death, Gosse worked with remarkable diligence on a book that he may have viewed as the most important of his career. Although a failure both financially and intellectually, it is the book by which he is best remembered. Gosse believed that he had discovered a theory that might neatly resolve the seeming contradiction in the age of the earth between the evidence of God's Word and the evidence of His creation as expounded by such contemporary geologists as Charles Lyell. In 1857, two years before the publication of Charles Darwin's "Origin of Species", Gosse published "Omphalos: an Attempt to Untie the Geological Knot" and thereby created what has been called the Omphalos hypothesis.

In what Stephen Jay Gould has called "glorious purple prose," Gosse argued that if one assumed creation "ex nihilo", there would necessarily be traces of previous existence that had never actually occurred. ""Omphalos"" is Greek for ""navel"," and Gosse argued that the first man, Adam, did not require a navel because he was never born; nevertheless he must have had one, as do all complete human beings, just as God must have created trees with rings that they never grew. Thus, Gosse argued that the fossil record—even coprolites—might also be evidence of life that had never actually existed but which may have been instantly formed by God at the moment of creation.

The general response was "as the "Westminster Review" put it, that Gosse's theory was 'too monstrous for belief.'" Even his friend, the novelist Charles Kingsley, wrote that he had read "no other book which so staggered and puzzled" him, that he could not believe that God had "written on the rocks one enormous and superfluous lie for all mankind." Journalists later sniggered that God had apparently hidden fossils in the rocks to tempt geologists to infidelity.

"Omphalos" sold poorly and was eventually rebound with a new title, "Creation", "in case the obscure one had had an effect on sales." The problem was not with the title. In 1869 most of the edition was sold as waste paper.

According to Edmund Gosse, his father's career was destroyed by his "strange act of wilfulness" in publishing "Omphalos"; Edmund claimed his father had "closed the doors upon himself forever." But, during the next three years Gosse published more than thirty scientific papers and four books.

By this time Gosse and his son had moved permanently from London to St Marychurch, Devon. (Gosse refused to use the "St" and even gave his address as Torquay so as not to have anything to do with the "so-called Church of England.") He soon became the pastor and overseer of the Brethren meeting. It was first held in a loft over a stable but shortly, under Gosse's preaching and peacemaking, in finer quarters—which he perhaps financed himself. His son said "he soon lost confidence in the Plymouth Brethren also, and for the last thirty years of his life he was really unconnected with any Christian body whatsoever."

During this period, Gosse made a special study of sea anemone (Actiniae) and in 1860 published "Actinologia Britannica". Reviewers especially praised the colour lithographs made from Gosse's watercolours. The "Literary Gazette" said that Gosse now stood "alone and unrivalled in the extremely difficult art of drawing objects of zoology so as to satisfy the requirements of science" as well as providing "vivid aesthetic impressions."

In 1860 he met and married Eliza Brightwen (1813–1900), a kindly, tolerant Quaker who shared Gosse's intense interest in both natural history and the well-being of his son. Gosse's second marriage was as happy as his first. In 1862 he wrote that Eliza was "a true yoke-fellow, in love, in spirit and in service."

By this time Gosse was "very comfortably off" with the earnings from his books and dividends from his investments. In 1864 Eliza received a substantial legacy that allowed Gosse to retire from his career as a professional writer and live in "congenial obscurity." The Gosses lived simply, invested some of their income and gave more away to charity, especially to foreign missionaries, including ones sent to the "Popish, priest-ridden Irish."

To Gosse's great grief, his son rejected Christianity—though almost certainly not as early or as dramatically as Edmund portrayed the break in "Father and Son". But Gosse sponsored the publication of Edmund's early poetry, which gave the younger man entrée to new friends of literary importance, and the two men "came out of the years of conflict with their relationship wary but intact." Henry and Eliza welcomed Edmund's wife to the family and enjoyed visits with their three grandchildren.
Meanwhile, the ever active Gosse had taken up the study of orchids and exchanged a number of letters on the subject with Darwin, though he never published on this subject himself. His penultimate enthusiasm was with the genitalia of butterflies, about which he published a paper in the "Transactions of the Linnean Society" But before his death he returned to rotifera, with much of his research appearing in a two-volume study written with another zoologist, Charles Thomas Hudson.

His wife recalled that Gosse's final illness may have been caused by his becoming chilled while trying to adjust his telescope at an open window on a winter night. Gosse had prayed regularly that he might not taste death but meet Christ in the air at his Second Coming, and he was bitterly disappointed when he realized that he would die like everyone else.

After his father's death, Edmund Gosse published a typical Victorian biography, "The Life of Philip Henry Gosse" (1890). After reading it, the writer George Moore suggested to Edmund that it contained "the germ of a great book." Edmund Gosse revised his material and first published his notable memoir anonymously as "Father and Son" in 1907. It has never gone out of print in more than a hundred years. The reaction of readers to Henry's personality and character, as represented in "Father and Son", has included phrases such as "scientific crackpot," "bible-soaked romantic," "a stern and repressive father," and a "pulpit-thumping Puritan throwback to the seventeenth century."

A modern editor of "Father and Son" has rejected this portrait of Philip Henry Gosse, on the grounds that his own "writings reveal a genuinely sweet character." Ann Thwaite, the biographer of both Gosses, has established just how inaccurate Edmund's recollections of his childhood were. Henry James remarked that Edmund Gosse had "a genius for inaccuracy." Although Edmund went out of his way to declare that the story of "Father and Son" was "scrupulously true," Thwaite cites a dozen occasions on which either Edmund's "memory betray[ed] him—he admitted it was 'like a colander'"—or he "changed things deliberately to make a better story." Thwaite argues that Edmund could only preserve his self-respect, in comparison to his father's superior abilities, by demolishing the latter's character.

Playwright Dennis Potter adapted "Father and Son" as the television play "Where Adam Stood", first broadcast on BBC One in 1976. Gosse was played by Alan Badel. Reviewers said that the play portrayed Gosse more sympathetically than did Edmund Gosse's book.

"Father and Son" was also adapted for BBC Radio 4 in 2005 by Nick Warburton. Roger Allam played Gosse and Derek Jacobi, Edmund.

Ann Lingard's novel "Seaside Pleasures" (2014) explores the relationship of Gosse and his wife Emily from the point of view of a female student in his shore-class.






</doc>
<doc id="24466" url="https://en.wikipedia.org/wiki?curid=24466" title="List of Polish composers">
List of Polish composers

This is a list of notable and representative Polish composers.

Note: This list should contain notable composers, best with an existing article on Wikipedia. If a notable Polish composer is , please add the name .







A Quick Guide] 


</doc>
<doc id="24468" url="https://en.wikipedia.org/wiki?curid=24468" title="President of the European Commission">
President of the European Commission

The president of the European Commission is the head of the European Commission, the executive branch of the :European Union. The president of the Commission leads a cabinet of commissioners, referred to as the "college", collectively accountable to the European Parliament. The president is empowered to allocate portfolios amongst, reshuffle or dismiss commissioners as necessary. The college directs the Commission's civil service, sets the policy agenda and determines the legislative proposals it produces.

The president of the Commission also represents the EU abroad, together with the president of the European Council and the High Representative of the Union for Foreign Affairs and Security Policy.

The post was established in 1958. Each new president is nominated by the European Council and formally elected by the European Parliament, for a five-year term.

In July 2019, the European Council nominated Ursula von der Leyen to succeed Jean-Claude Juncker, and she was elected the 13th president of the European Commission by the European Parliament on 16 July. Von der Leyen assumed office on 1 December 2019, following the approval of her college of commissioners by the European Parliament.

The present Commission was established by the Treaty of Rome in 1957; it also replaced the High Authority and the Commission of Euratom in 1967. The Commission's first president was Walter Hallstein (see Hallstein Commission) who started consolidating European law and began to impact on national legislation. National governments at first took little heed of his administration, with the president having to stamp the Commission's authority early on. With the aid of the European Court of Justice, the Commission began to be taken more seriously.

In 1965, Hallstein put forward his proposals for the Common Agricultural Policy, which would give the Community its own financial resources while giving more power to the Commission and Parliament and removing the veto power over Agriculture in the Council. These proposals led to an immediate backlash from France. Hallstein knew the proposals would be contentious, and took personal charge of drafting them, over-riding the Agriculture Commissioner. However he did gain the support of Parliament through his proposals to increase its powers, and he also presented his policy to Parliament a week before he submitted them to the Council. He aimed to demonstrate how he thought the Community ought to be run, in the hopes of generating a wave of pro-Europeanism big enough to get past the objections of member states. However, in this it proved that, despite its past successes, Hallstein was overconfident in his risky proposals.

In reaction to Hallstein's proposals and actions, then-French president Charles de Gaulle, who was sceptical of the rising supranational power of the Commission, accused Hallstein of acting as if he were a head of state. France eventually withdrew its representative from the Council, triggering the notorious "empty chair crisis". Although this was resolved under the "Luxembourg compromise", Hallstein became the scapegoat for the crisis. The Council refused to renew his term, despite his being the most 'dynamic' leader until Jacques Delors.

Hallstein's work did position the Commission as a substantial power. The presidents were involved in the major political projects of the day in the 1970s, such as the European Monetary Union. In 1970, President Jean Rey secured the Community's own financial resources, and in 1977, President Roy Jenkins became the first Commission president to attend a G7 summit on behalf of the Community.

However, owing to problems such as the 1973 oil crisis and the 1979 energy crisis, economic hardship reduced the priority of European integration, with only the president trying to keep the idea alive. The member states had the upper hand, and they created the European Council to discuss topical problems, yet the Council was unable to keep the major projects on track such as the Common Agricultural Policy. The Community entered a period of eurosclerosis, owing to economic difficulties and disagreements on the Community budget, and by the time of the Thorn Commission the president was unable to exert his influence to any significant extent.

However, the Commission began to recover under President Jacques Delors' Commission. He is seen as the most successful president, being credited with having given the Community a sense of direction and dynamism. The "International Herald Tribune" noted the work of Delors at the end of his second term in 1992: "Mr. Delors rescued the European Community from the doldrums. He arrived when Europessimism was at its worst. Although he was a little-known (outside France) finance minister and former MEP, he breathed life and hope into the EC and into the dispirited Brussels Commission. In his first term, from 1985 to 1988, he rallied Europe to the call of the single market, and when appointed to a second term he began urging Europeans toward the far more ambitious goals of economic, monetary and political union."

But Delors not only turned the Community around, he signalled a change in the Presidency. Before he came to power, the Commission president still was a position of first among equals; when he left office, he was the undisputed icon and leader of the Community. His tenure had produced a strong Presidency and a strong Commission as the president became more important. Following treaties cemented this change, with the president being given control over the allocation of portfolios and being able to force the resignation of Commissioners. When President Romano Prodi took office with the new powers of the Treaty of Amsterdam, he was dubbed by the press as Europe's first Prime Minister. President Delors' work had increased the powers of the Parliament, whose support he had enjoyed. However, later Commissions did not enjoy the same support, and in 1999, the European Parliament used its powers to force the Santer Commission to resign.

Historically, the Council appointed the Commission president and the whole body by unanimity without input from Parliament. However, with the Treaty on European Union in 1993, the European Parliament, the body elected directly by the citizens of the European Union, gained the right to be consulted on the appointment of the president and to veto the Commission as a whole. Parliament decided to interpret its right to be consulted as a right to veto the president, which the Council reluctantly accepted. This right of veto was formalised in the Amsterdam Treaty. The Treaty of Nice changed the Council's vote from a unanimous choice to one that merely needed a qualified majority. This meant that the weight of the Parliament in the process increased resulting in a quasi-parliamentary system where one group could be in government. This became evident when numerous candidates were put forward in 2004, and a centre-right vote won out over left-wing groups, France and Germany. José Manuel Barroso, elected Commission president that year, was then forced to back down over his choice of Commissioners, owing to Parliament's threat that it would not approve his Commission.

In 2009, the European People's Party (EPP) endorsed Barroso as its candidate for Commission president, and the EPP subsequently retained its position as largest party in that year's election. The Socialists responded by pledging to put forward a rival candidate at future elections. Once again, Barroso was forced by Parliament to make a change to his proposed Commission, but eventually received assent. However, in exchange for approval, Parliament forced some concessions from Barroso in terms of Parliamentary representation at Commission and international meetings. On 7 September 2010, Barroso gave the first US-style State of the Union address to Parliament, which focused primarily on the EU's economic recovery and human rights. The speech was to be annual.

Article 17 of the Treaty on European Union, as amended by the Treaty of Lisbon, lays out the procedure for appointing the president and their team. The European Council votes by qualified majority for a nominee for the post of President, taking account of the latest European elections. This proposal is then put before Parliament which must approve or veto the appointment. If an absolute majority of MEPs support the nominee, they are elected. The president then, together with the Council, puts forward their team to the Parliament to be scrutinised. The Parliament normally insists that each one of them appear before the parliamentary committee that corresponds to their prospective portfolio for a public hearing. The Parliament then votes on the Commission as a whole; if approved, the European Council, acting by a qualified majority, appoints the president and their team to office.

Qualified majority in the Council has led to more candidates being fielded while there has been greater politicisation due to the involvement of Parliament and the change of policy direction in the EU from the creation of the single market to reform of it. However, despite this, the choice within the Council remains largely behind closed doors. During the appointment of Santer, discussions were kept "in camera" (private), with the media relying on insider leaks. MEPs were angry with the process, against the spirit of consultation that the new EU treaty brought in. Pauline Green MEP, leader of the Socialist group, stated that her group thought "Parliament should refuse to condone a practice which so sullies the democratic process". There were similar deals in 1999 and 2004 saw a repeat of Santer's appointment when Barroso was appointed through a series of secret meetings between leaders with no press releases on the negotiations being released. This was sharply criticised by MEPs such as the ALDE group leader Graham Watson who described the procedure as a "Justus Lipsius carpet market" producing only the "lowest common denominator"; while Green-EFA co-leader Daniel Cohn-Bendit asked Barroso after his first speech "If you are the best candidate, why were you not the first?"

The candidate selected by the Council has often been a leading national politician, but this is not a requirement. The choice of President must take into account the result of the latest Parliamentary elections (for example, by choosing the candidate supported by the largest European political party in particular, or at least someone from that political family the "Spitzenkandidat" principle, below but this is a convention not an obligation). That provision was not in force in the nomination in 2004, but the centre-right EPP, which won the election, pressured for a candidate from its own ranks. In the end, the EPP candidate, José Manuel Barroso, was chosen. On the same basis, the EPP endorsed again Barroso for a second term during the 2009 European election campaign and, being largest again after that election, was able to secure his nomination by the European Council.

Further criteria seen to be influencing the choice of the Council include: which area of Europe the candidate comes from, favoured as Southern Europe in 2004; the candidate's political influence, credible yet not overpowering members; language, proficiency in French considered necessary by France; and degree of integration, their state being a member of both the eurozone and the Schengen Agreement.

There has been an assumption that there is a rolling agreement along these lines, that a president from a large state is followed by a president from a small state, and one from the political left will be followed by one from the political right: Roy Jenkins (British socialist) was followed by Gaston Thorn (Luxembourgish liberal), Jacques Delors (French socialist), Jacques Santer (Luxembourgish Christian democrat), Romano Prodi (Italian left-wing Christian democrat) and Jose Barroso (Portuguese Christian democrat). However, despite these assumptions, these presidents have usually been chosen during political battles and coalition-building. Delors was chosen following a Franco-British disagreement over Claude Cheysson, Santer was a compromise after Britain vetoed Jean-Luc Dehaene, and Prodi was backed by a coalition of thirteen states against the Franco-German preference for Guy Verhofstadt.

In February 2008, President Barroso admitted that despite the president having in theory as much legitimacy as heads of governments, in practice it was not the case. The low voter turnout creates a problem for the president's legitimacy, with the lack of a "European political sphere", but analysts claim that if citizens were voting for a list of candidates for the post of President, turn out would be much higher than that seen in recent years.

Under the Treaty of Lisbon the European Council has to take into account the results of the latest European elections and, furthermore, the Parliament elects, rather than simply approve, the Council's proposed candidate. This was taken as the parliament's cue to have its parties run with candidates for the president of the Commission with the candidate of the winning party being proposed by the Council. This was partly put into practice in 2004 when the European Council selected a candidate from the political party which secured a plurality of votes in that year's election. However, at that time only a minor party had run with a specific candidate: the then fourth-placed European Green Party, which had the first true pan-European political party with a common campaign, put forward Daniel Cohn-Bendit and lost even its fourth place in the following election, becoming only the fifth-largest group in 2009 and diminishing its candidate's chances further. However, the victorious EPP only mentioned four or five people as candidates for president.

There have been plans to strengthen the European political parties for them to propose candidates for future elections. The European Liberal Democrat and Reform Party indicated, in its October 2007 congress, its intention to forward a candidate for the post as part of a common campaign but failed to do so. However, the EPP selected Barroso as its candidate and, as the largest party, it was able to ensure his turn was renewed.

The Socialists, disappointed at the 2009 election, agreed to put forward a candidate for Commission President at all subsequent elections. After a campaign within that party to have open primaries for said candidate, the PES Congress gathering in Brussels in November 2011 decided that the PES would designate its candidate for Commission president through primaries taking place in January 2014 in each of its member parties and organisations, before a ratification of the results by an Extraordinary PES Congress in February 2014.

The (German for 'lead candidate') process is the method of linking European Parliament elections by having each major political group in Parliament nominating their candidate for Commission President prior to the Parliamentary elections. The of the largest party would then have a mandate to assume the Commission Presidency. This process was first run in 2014, and its legitimacy was contested by the Council.

According to the treaties, the president of the European Commission is nominated by the European Council. Until 2004, this nomination was based on an informal consensus for a common candidate. However, in 2004 the centre-right EPP rejected the consensus approach ahead of the European Council meeting, and pushed through its own candidate, Barroso. The approach of national governments was to appoint the various high-profile jobs in EU institutions (European Council president, High Representative and so on) dividing them according along geographic, political and gender lines. This also led to fairly low-profile figures in some cases, for it avoided candidates who had either made enemies of some national governments or who were seen as potentially challenging the Council or certain member states.

Unease had built up around the secretive power play that was involved in these appointments, leading to a desire for a more democratic process. At the end of 2009, the Treaty of Lisbon entered into force. It amended the appointment of the Commission President in the Treaty on European Union Article 17.7 to add the wording "taking into account the elections to the European Parliament", so that Article 17.7 now included the wording 

In 2013, in preparation for the European election of 2014, Martin Schulz, then President of the European Parliament campaigned for European political parties to name lead candidates for the post of President of the European Commission; his own party group, the centre-left Party of European Socialists named Schulz as its lead candidate (). The EPP held an election Congress in Dublin, where Jean-Claude Juncker beat his rival Michel Barnier and subsequently ran as the EPP's lead candidate. The Alliance of Liberals and Democrats for Europe Party and the European Green Party also selected lead candidates. The Alliance of European Conservatives and Reformists did not name a candidate, objecting to the principle of and its "tenuous" basis in law. The German term for lead candidates caught on, and they became known informally as .

The EPP won a plurality (29%) in the 2014 election, and Jean-Claude Juncker, its lead candidate, was nominated by the European Council. British Prime Minister David Cameron and Hungarian Prime Minister Viktor Orbán were the only members of the council to object to his selection.

Some commentators argued that this amendment did not entitle the political parties of the Parliament to nominate candidates for the president of the Commission, and that such an interpretation would amount to a "power grab" at the expense of the European Council. The Council found itself taken off guard by how the process took off, and had backed themselves into a corner in having to approve the Parliament's candidate. Following the appointment, leaders vowed to review the process.

On the other hand, it has also been argued that it is still insufficiently democratic and needs to be replaced with a more direct system. Some suggestions toward this have been electing the president via a transnational list, having a direct election, and holding primary elections. Parliamentary proposals to enact some of these in advance of the 2019 election have been opposed by some in the Council.

The president is elected for a renewable five-year term starting five months after the elections to the European Parliament. These were brought into alignment via the Maastricht Treaty (prior to which the Commission had a four-year term of office) and the elections take place in June every five years (in years ending in 4 and 9). This alignment has led to a closer relationship between the elections and the president himself with the above-mentioned proposals for political parties running with candidates.

The president and his Commission may be removed from office by a vote of censure from Parliament. Parliament has never done this to date, however the imminence of such a vote in 1999, due to allegations of financial mismanagement, led to the Santer Commission resigning on its own accord, before the Parliamentary vote.

The president of the European Commission is the most powerful position in the European Union, controlling the Commission which collectively has the right of initiative on Union legislation (only on matters delegated to it by member states for collective action, as determined by the treaties) and is responsible for ensuring its enforcement. The president controls the policy agenda of the Commission for their term and in practice no policy can be proposed without the president's agreement.

The role of the president is to lead the Commission, and give direction to the Commission and the Union as a whole. The treaties state that "the Commission shall work under the political guidance of its president" (Article 219 TEC), this is conducted through their calling and chairing of meetings of the college of Commissioners, their personal cabinet and the meetings of the heads of each commissioner's cabinet (the Hebdo). The president may also force a Commissioner to resign. The work of the Commission as a body is based on the principle of Cabinet collective responsibility, however in their powers they act as more than a first among equals. The role of the president is similar to that of a national Prime Minister chairing a cabinet.

The president also has responsibility for representing the Commission in the Union and beyond. For example, they are a member of the European Council and takes part in debates in Parliament and the Council of Ministers. Outside the Union they attend the meetings of the G8 to represent the Union. However, in foreign affairs, the president does have to compete with several Commissioners with foreign affairs related portfolios: the High Representative for the Common Foreign and Security Policy and the president of the European Council.

The presidential system had started to develop since Jacques Delors and has since been cemented. However, externally they are still dependent on support from the Council and Parliament. Delors had enjoyed the Parliament's and the Council's support for his whole term, during which, through treaty changes, the Parliament increased in powers and, through the accession of new Member States, the Council increased in membership. The membership is now so large the president is increasingly unable to garner the support of all the states, even though the job is supposed to try to keep everyone happy. The Parliament now has more powers over the Commission and can reject its proposals, although the Commission has little power over Parliament, such as the ability to dissolve it to call new elections.

The president's office is on the top, 13th, floor of the Berlaymont building in Brussels. The president receives their political guidance from their cabinet, the head of which acts as a political bodyguard for the president. Such factors can lead to an isolation of the president from outside events. For the European Civil Service the president has a very high status, due to their immense authority and symbolism within the body. The president exercises further authority through the legal service and Secretariat-General of the Commission. The former has the power to strike down proposals on legal technicalities while the latter organises meetings, agendas and minutes. The president's control over these areas gives them further political tools when directing the work of the Commission. This has also increased the presidential style of the Commission president.

With the reorganisation of leading EU posts under the Lisbon Treaty, there was some criticism of each post's vague responsibilities. Ukrainian ambassador to the EU Andriy Veselovsky praised the framework and clarified it in his own terms: The Commission president speaks as the EU's "government" while the president of the European Council is a "strategist". The High Representative specialises in "bilateral relations" while the European Commissioner for Enlargement and European Neighbourhood Policy deals in technical matters such as the free trade agreement with Ukraine. The president of the European Parliament meanwhile articulates the EU's values.

The MEP and author of several EU text books Richard Corbett has suggested that, instead of every EU institution having a "president", it would have been clearer if they had been named differently, with a "Speaker" of the Parliament, a "Governor" of the Central Bank, a "Chairman" of the (ordinary) Council of Ministers, a "president" of the European Council, and a "Prime Commissioner".

Despite the recent presidential style, the president has also begun to lose ground to the larger member states as countries such as France, Italy, the UK and Germany seek to sideline its role. This may increase with the recent creation of the permanent president of the European Council. There has been disagreement and concern over competition between the president of the European Council Van Rompuy and the Commission president Barroso due to the vague language of the treaty. Some clarifications see Van Rompuy as the "strategist" and Barroso as a head of government. In terms of economic planning Van Rompuy saw the Commission as dealing with the content of the plan and the European Council as dealing with the means and implementing it. Despite weekly breakfasts together there was a certain extent of rivalry between the two, as well as with the High Representative. At international summits, both presidents go at the same time to represent the Union, with, in principle, the Commission president speaking on economic questions and the European Council president on political questions, although this division is often hard to maintain in practice.

Although there are concerns that this competition with the new European Council president would lead to increased infighting, there are provisions for combining the two offices. The European Council president may not hold a national office, such as a Prime Minister of a member state, but there is no such restraint on European offices. So the Commission president, who already sits in the European Council, could also be appointed as its president. This would allow the European Council to combine the position, with its powers, of both executive bodies into a single president of the European Union.

The basic monthly salary of the president is fixed at 138% of the top civil service grade which, in 2013, amounted to €25,351 per month or €304,212 per year plus an allowance for a residence equal to 15% of salary as well as other allowances including for children's schooling and household expenses.

This section firstly presents a lists over presidents of the three executives that were merged in 1967 following the Merger Treaty, namely the High Authority of the European Coal and Steel Community (from 1952), and the commissions of the European Atomic Energy Community and the European Economic Community (both from 1958). Secondly, a list is given over the presidents after the merger, when the single position presided over the Commission of the European Communities, until 2009 when the Treaty of Lisbon renamed of the institution, creating the president of the European Commission.

The European Economic Community was established by the Treaty of Rome, presently known as the Treaty on the Functioning of the European Union; a founding treaty of the union, which explains that the enumeration of presidents which ends with the present position starts with the first president of the Commission of the European Economic Community. The European Union is also the legal successor of the European Economic Community, or the European Community as it was named between 1993 and 2009. The establishment of the European Union in 1993 upon the entry into force of the Maastricht Treaty (formally the Treaty on European Union) did not affect the name of the position.

Upon its entry into force in 2009, the Treaty of Lisbon renamed the Commission of the European Communities the European Commission, reflecting the "de facto" name as well as the fact that the European Communities pillar was abolished along with the rest of the pillar system.

Parties

<nowiki>*</nowiki> Von der Leyen's term will begin on 1 December 2019. It was postponed due to a need to select a Romanian commissioner after the original appointee was rejected by European Parliament.





</doc>
<doc id="24471" url="https://en.wikipedia.org/wiki?curid=24471" title="Phonograph">
Phonograph

A phonograph, in its later forms also called a gramophone (as a trademark since 1887, as a generic name in the UK since 1910) or since the 1940s called a record player, is a device for the mechanical recording and reproduction of sound. The sound vibration waveforms are recorded as corresponding physical deviations of a spiral groove engraved, etched, incised, or impressed into the surface of a rotating cylinder or disc, called a "record". To recreate the sound, the surface is similarly rotated while a playback stylus traces the groove and is therefore vibrated by it, very faintly reproducing the recorded sound. In early acoustic phonographs, the stylus vibrated a diaphragm which produced sound waves which were coupled to the open air through a flaring horn, or directly to the listener's ears through stethoscope-type earphones.

The phonograph was invented in 1877 by Thomas Edison. Alexander Graham Bell's Volta Laboratory made several improvements in the 1880s and introduced the "graphophone", including the use of wax-coated cardboard cylinders and a cutting stylus that moved from side to side in a zigzag groove around the record. In the 1890s, Emile Berliner initiated the transition from phonograph cylinders to flat discs with a spiral groove running from the periphery to near the center, coining the term "gramophone" for disc record players, which is predominantly used in many languages. Later improvements through the years included modifications to the turntable and its drive system, the stylus or needle, and the sound and equalization systems.

The disc phonograph record was the dominant audio recording format throughout most of the 20th century. In the 1980s, phonograph use on a standard record player declined sharply due to the rise of the cassette tape, compact disc, and other digital recording formats. However, records are still a favorite format for some audiophiles, DJs and turntablists (particularly in hip hop and electronic dance music), and have undergone a revival since the 1990s. The original recordings of musicians, which may have been recorded on tape or digital methods, are sometimes re-issued on vinyl.

Usage of terminology is not uniform across the English-speaking world (see below). In more modern usage, the playback device is often called a "turntable", "record player", or "record changer", although each of these terms denote categorically distinct items. When used in conjunction with a mixer as part of a DJ setup, turntables are often colloquially called "decks". In later electric phonographs (more often known since the 1940s as record players or, most recently, turntables), the motions of the stylus are converted into an analogous electrical signal by a transducer, then converted back into sound by a loudspeaker.The term "phonograph" ("sound writing") was derived from the Greek words ("phonē", "sound" or "voice") and ("graphē", "writing"). The similar related terms "gramophone" (from the Greek γράμμα "gramma" "letter" and φωνή "phōnē" "voice") and "graphophone" have similar root meanings. The roots were already familiar from existing 19th-century words such as "photograph" ("light writing"), "telegraph" ("distant writing"), and "telephone" ("distant sound"). The new term may have been influenced by the existing words "phonographic" and "phonography", which referred to a system of phonetic shorthand; in 1852 "The New York Times" carried an advertisement for "Professor Webster's phonographic class", and in 1859 the New York State Teachers Association tabled a motion to "employ a phonographic recorder" to record its meetings.

Arguably, any device used to record sound or reproduce recorded sound could be called a type of "phonograph", but in common practice the word has come to mean historic technologies of sound recording, involving audio-frequency modulations of a physical trace or groove. In the late-19th and early-20th centuries, "Phonograph", "Gramophone", "Graphophone", "Zonophone", "Graphonole" and the like were still brand names specific to various makers of sometimes very different (i.e. cylinder and disc) machines; so considerable use was made of the generic term "talking machine", especially in print. "Talking machine" had earlier been used to refer to complicated devices which produced a crude imitation of speech, by simulating the workings of the vocal cords, tongue, and lips – a potential source of confusion both then and now.

In British English, "gramophone" may refer to any sound-reproducing machine using disc records, which were introduced and popularized in the UK by the Gramophone Company. Originally, "gramophone" was a proprietary trademark of that company and any use of the name by competing makers of disc records was vigorously prosecuted in the courts, but in 1910 an English court decision decreed that it had become a generic term; it has been so used in the UK and most Commonwealth countries ever since. The term "phonograph" was usually restricted to machines that used cylinder records.

"Gramophone" generally referred to a wind-up machine. After the introduction of the softer vinyl records, -rpm LPs (long-playing records) and 45-rpm "single" or two-song records, and EPs (extended-play recordings), the common name became "record player" or "turntable". Often the home record player was part of a system that included a radio ("radiogram") and, later, might also play audiotape cassettes. From about 1960, such a system began to be described as a "hi-fi" (high-fidelity, monophonic) or a "stereo" (most systems being stereophonic by the mid-1960s).

In American English, "phonograph", properly specific to machines made by Edison, was sometimes used in a generic sense as early as the 1890s to include cylinder-playing machines made by others. But it was then considered strictly incorrect to apply it to Emile Berliner's upstart Gramophone, a very different machine which played discs (although Edison's original Phonograph patent included the use of discs). "Talking machine" was the comprehensive generic term, but from about 1902 on, the general public was increasingly applying the word "phonograph" indiscriminately to both cylinder and disc machines and to the records they played. By the time of the First World War, the mass advertising and popularity of the Victrola (a line of disc-playing machines characterized by their concealed horns) sold by the Victor Talking Machine Company was leading to widespread generic use of the word "victrola" for any machine that played discs, which were generally called "phonograph records" or simply "records", but almost never "Victrola records".

After electrical disc-playing machines appeared on the market in the late 1920s, often combined with a radio receiver, the term "record player" was increasingly favored by the public. Manufacturers, however, typically advertised such combinations as "radio-phonographs". Portable record players (no radio included), with a latched cover and an integrated power amplifier and loudspeaker, were becoming popular as well, especially in schools and for use by children and teenagers.

In the years following the Second World War, as "hi-fi" (high-fidelity, monophonic) and, later, "stereo" (stereophonic) component sound systems slowly evolved from an exotic specialty item into a common feature of American homes, the description of the record-spinning component as a "record changer" (which could automatically play through a stacked series of discs) or a "turntable" (which could hold only one disc at a time) entered common usage. By the 1980s, the use of a "record changer" was widely disparaged. So, the "turntable" emerged triumphant and retained its position to the present. Through all these changes, however, the discs have continued to be known as "phonograph records" or, much more commonly, simply as "records".

"Gramophone", as a brand name, was not used in the United States after 1902, and the word quickly fell out of use there, although it has survived in its nickname form, "Grammy", as the name of the Grammy Awards. The Grammy trophy itself is a small rendering of a gramophone, resembling a Victor disc machine with a taper arm.

Modern amplifier-component manufacturers continue to label the input jack which accepts the output from a modern magnetic pickup cartridge as the "phono" input, abbreviated from "phonograph".

In Australian English, "record player" was the term; "turntable" was a more technical term; "gramophone" was restricted to the old mechanical (i.e., wind-up) players; and "phonograph" was used as in British English. The "phonograph" was first demonstrated in Australia on 14 June, 1878 to a meeting of the Royal Society of Victoria by the Society's Honorary Secretary, Alex Sutherland who published "The Sounds of the Consonants, as Indicated by the Phonograph" in the Society's journal in November that year. On 8 August, 1878 the phonograph was publicly demonstrated at the Society's annual "conversazione", along with a range of other new inventions, including the microphone.

Several inventors devised machines to record sound prior to Thomas Edison's phonograph, Edison being the first to invent a device that could both record and reproduce sound. The phonograph's predecessors include Édouard-Léon Scott de Martinville's phonautograph, and Charles Cros's paleophone. Recordings made with the phonautograph were intended to be visual representations of the sound, but were never sonically reproduced until 2008. Cros's paleophone was intended to both record and reproduce sound but had not been developed beyond a basic concept at the time of Edison's successful demonstration of the Phonograph in 1877.

Direct tracings of the vibrations of sound-producing objects such as tuning forks had been made by English physician Thomas Young in 1807, but the first known device for recording airborne speech, music and other sounds is the phonautograph, patented in 1857 by French typesetter and inventor Édouard-Léon Scott de Martinville. In this device, sound waves travelling through the air vibrated a parchment diaphragm which was linked to a bristle, and the bristle traced a line through a thin coating of soot on a sheet of paper wrapped around a rotating cylinder. The sound vibrations were recorded as undulations or other irregularities in the traced line. Scott's phonautograph was intended purely for the visual study and analysis of the tracings. Reproduction of the recorded sound was not possible with the original phonautograph.

In 2008, phonautograph recordings made by Scott were played back as sound by American audio historians, who used optical scanning and computer processing to convert the traced waveforms into digital audio files. These recordings, made circa 1860, include fragments of two French songs and a recitation in Italian.

Charles Cros, a French poet and amateur scientist, is the first person known to have made the conceptual leap from recording sound as a traced line to the theoretical possibility of reproducing the sound from the tracing and then to devising a definite method for accomplishing the reproduction. On April 30, 1877, he deposited a sealed envelope containing a summary of his ideas with the French Academy of Sciences, a standard procedure used by scientists and inventors to establish priority of conception of unpublished ideas in the event of any later dispute.

Cros proposed the use of photoengraving, a process already in use to make metal printing plates from line drawings, to convert an insubstantial phonautograph tracing in soot into a groove or ridge on a metal disc or cylinder. This metal surface would then be given the same motion and speed as the original recording surface. A stylus linked to a diaphragm would be made to ride in the groove or on the ridge so that the stylus would be moved back and forth in accordance with the recorded vibrations. It would transmit these vibrations to the connected diaphragm, and the diaphragm would transmit them to the air, reproducing the original sound.

An account of his invention was published on October 10, 1877, by which date Cros had devised a more direct procedure: the recording stylus could scribe its tracing through a thin coating of acid-resistant material on a metal surface and the surface could then be etched in an acid bath, producing the desired groove without the complication of an intermediate photographic procedure. The author of this article called the device a "phonographe", but Cros himself favored the word "paleophone", sometimes rendered in French as "voix du passé" (voice of the past) but more literally meaning "ancient sound", which accorded well with his vision of his invention's potential for creating an archive of sound recordings that would be available to listeners in the distant future.

Cros was a poet of meager means, not in a position to pay a machinist to build a working model, and largely content to bequeath his ideas to the public domain free of charge and let others reduce them to practice, but after the earliest reports of Edison's presumably independent invention crossed the Atlantic he had his sealed letter of April 30 opened and read at the December 3, 1877 meeting of the French Academy of Sciences, claiming due scientific credit for priority of conception.

Throughout the first decade (1890–1900) of commercial production of the earliest crude disc records, the direct acid-etch method first invented by Cros was used to create the metal master discs, but Cros was not around to claim any credit or to witness the humble beginnings of the eventually rich phonographic library he had foreseen. He had died in 1888 at the age of 45.

Thomas Alva Edison conceived the principle of recording and reproducing sound between May and July 1877 as a byproduct of his efforts to "play back" recorded telegraph messages and to automate speech sounds for transmission by telephone. His first experiments were with waxed paper.
He announced his invention of the first "phonograph", a device for recording and replaying sound, on November 21, 1877 (early reports appear in Scientific American and several newspapers in the beginning of November, and an even earlier announcement of Edison working on a 'talking-machine' can be found in the Chicago Daily Tribune on May 9), and he demonstrated the device for the first time on November 29 (it was patented on February 19, 1878 as US Patent 200,521). "In December, 1877, a young man came into the office of the SCIENTIFIC AMERICAN, and placed before the editors a small, simple machine about which very few preliminary remarks were offered. The visitor without any ceremony whatever turned the crank, and to the astonishment of all present the machine said: "Good morning. How do you do? How do you like the phonograph?" The machine thus spoke for itself, and made known the fact that it was the phonograph..."

Edison presented his own account of inventing the phonograph: I was experimenting," he said, "on an automatic method of recording telegraph messages on a disk of paper laid on a revolving platen, exactly the same as the disk talking-machine of to-day. The platen had a spiral groove on its surface, like the disk. Over this was placed a circular disk of paper; an electromagnet with the embossing point connected to an arm traveled over the disk; and any signals given through the magnets were embossed on the disk of paper. If this disc was removed from the machine and put on a similar machine provided with a contact point, the embossed record would cause the signals to be repeated into another wire. The ordinary speed of telegraphic signals is thirty-five to forty words a minute; but with this machine several hundred words were possible.

From my experiments on the telephone I knew of how to work a pawl connected to the diaphragm; and this engaging a ratchet-wheel served to give continuous rotation to a pulley. This pulley was connected by a cord to a little paper toy representing a man sawing wood. Hence, if one shouted: ' Mary had a little lamb,' etc., the paper man would start sawing wood. I reached the conclusion that if I could record the movements of the diaphragm properly, I could cause such records to reproduce the original movements imparted to the diaphragm by the voice, and thus succeed in recording and reproducing the human voice.

Instead of using a disk I designed a little machine using a cylinder provided with grooves around the surface. Over this was to be placed tinfoil, which easily received and recorded the movements of the diaphragm. A sketch was made, and the piece-work price, $18, was marked on the sketch. I was in the habit of marking the price I would pay on each sketch. If the workman lost, I would pay his regular wages; if he made more than the wages, he kept it. The workman who got the sketch was John Kruesi. I didn't have much faith that it would work, expecting that I might possibly hear a word or so that would give hope of a future for the idea. Kruesi, when he had nearly finished it, asked what it was for. I told him I was going to record talking, and then have the machine talk back. He thought it absurd. However, it was finished, the foil was put on; I then shouted 'Mary had a little lamb', etc. I adjusted the reproducer, and the machine reproduced it perfectly. I was never so taken aback in my life. Everybody was astonished. I was always afraid of things that worked the first time. Long experience proved that there were great drawbacks found generally before they could be got commercial; but here was something there was no doubt of.

The music critic Herman Klein attended an early demonstration (1881–2) of a similar machine. On the early phonograph's reproductive capabilities he writes "It sounded to my ear like someone singing about half a mile away, or talking at the other end of a big hall; but the effect was rather pleasant, save for a peculiar nasal quality wholly due to the mechanism, though there was little of the scratching which later was a prominent feature of the flat disc. Recording for that primitive machine was a comparatively simple matter. I had to keep my mouth about six inches away from the horn and remember not to make my voice too loud if I wanted anything approximating to a clear reproduction; that was all. When it was played over to me and I heard my own voice for the first time, one or two friends who were present said that it sounded rather like mine; others declared that they would never have recognised it. I daresay both opinions were correct."

The Argus (Melbourne) newspaper reported on an 1878 demonstration at the Royal Society of Victoria, writing "There was a large attendance of ladies and gentlemen, who appeared greatly interested in the various scientific instruments exhibited. Among these the most interesting, perhaps, was the trial made by Mr. Sutherland with the phonograph, which was most amusing. Several trials were made, and were all more or less successful. "Rule Britannia" was distinctly repeated, but great laughter was caused by the repetition of the convivial song of "He's a jolly good fellow," which sounded as if it was being sung by an old man of 80 with a very cracked voice."

Edison's early phonographs recorded onto a thin sheet of metal, normally tinfoil, which was temporarily wrapped around a helically grooved cylinder mounted on a correspondingly threaded rod supported by plain and threaded bearings. While the cylinder was rotated and slowly progressed along its axis, the airborne sound vibrated a diaphragm connected to a stylus that indented the foil into the cylinder's groove, thereby recording the vibrations as "hill-and-dale" variations of the depth of the indentation.

Playback was accomplished by exactly repeating the recording procedure, the only difference being that the recorded foil now served to vibrate the stylus, which transmitted its vibrations to the diaphragm and onward into the air as audible sound. Although Edison's very first experimental tinfoil phonograph used separate and somewhat different recording and playback assemblies, in subsequent machines a single diaphragm and stylus served both purposes. One peculiar consequence was that it was possible to overdub additional sound onto a recording being played back. The recording was heavily worn by each playing, and it was nearly impossible to accurately remount a recorded foil after it had been removed from the cylinder. In this form, the only practical use that could be found for the phonograph was as a startling novelty for private amusement at home or public exhibitions for profit.
Edison's early patents show that he was aware that sound could be recorded as a spiral on a disc, but Edison concentrated his efforts on cylinders, since the groove on the outside of a rotating cylinder provides a constant velocity to the stylus in the groove, which Edison considered more "scientifically correct".

Edison's patent specified that the audio recording be embossed, and it was not until 1886 that vertically modulated incised recording using wax-coated cylinders was patented by Chichester Bell and Charles Sumner Tainter. They named their version the Graphophone.

The use of a flat recording surface instead of a cylindrical one was an obvious alternative which thought-experimenter Charles Cros initially favored and which practical experimenter Thomas Edison and others actually tested in the late 1870s and early 1880s. The oldest surviving example is a copper electrotype of a recording cut into a wax disc in 1881. The commercialization of sound recording technology was initially aimed at use for business correspondence and transcription into writing, in which the cylindrical form offered certain advantages. However, the storage of large numbers of records seemed impractical, and the ease of producing multiple copies was not a consideration. 

In 1887, Emile Berliner patented a variant of the phonograph which he named the Gramophone. Berliner's approach was essentially the same one proposed, but never implemented, by Charles Cros in 1877. The diaphragm was linked to the recording stylus in a way that caused it to vibrate laterally (side to side) as it traced a spiral onto a zinc disc very thinly coated with a compound of beeswax. The zinc disc was then immersed in a bath of chromic acid; this etched a groove into the disc where the stylus had removed the coating, after which the recording could be played. With some later improvements the flat discs of Berliner could be produced in large quantities at much lower cost than the cylinders of Edison's system.

In May 1889, in San Francisco, the first "phonograph parlor" opened. It featured a row of coin-operated machines, each supplied with a different wax cylinder record. The customer selected a machine according to the title that it advertised, inserted a nickel, then heard the recording through stethoscope-like listening tubes. By the mid-1890s, most American cities had at least one phonograph parlor. The coin-operated mechanism was invented by Louis T. Glass and William S. Arnold. The cabinet contained an Edison Class M or Class E phonograph. The Class M was powered by a wet-cell glass battery that would spill dangerous acid if it tipped over or broke. The Class E sold for a lower price and ran on 120 V DC. 
The phenomenon of phonograph parlors peaked in Paris around 1900: in Pathé's luxurious salon, patrons sat in plush upholstered chairs and chose from among many hundreds of available cylinders by using speaking tubes to communicate with attendants on the floor below. 
By 1890, record manufacturers had begun using a rudimentary duplication process to mass-produce their product. While the live performers recorded the master phonograph, up to ten tubes led to blank cylinders in other phonographs. Until this development, each record had to be custom-made. Before long, a more advanced pantograph-based process made it possible to simultaneously produce 90–150 copies of each record. However, as demand for certain records grew, popular artists still needed to re-record and re-re-record their songs. Reportedly, the medium's first major African-American star George Washington Johnson was obliged to perform his "The Laughing Song" (or the separate "The Whistling Coon") literally thousands of times in a studio during his recording career. Sometimes he would sing "The Laughing Song" more than fifty times in a day, at twenty cents per rendition. (The average price of a single cylinder in the mid-1890s was about fifty cents.)

Lambert's lead cylinder recording for an experimental talking clock is often identified as the oldest surviving playable sound recording,
although the evidence advanced for its early date is controversial.
Wax phonograph cylinder recordings of Handel's choral music made on June 29, 1888, at The Crystal Palace in London were thought to be the oldest-known surviving musical recordings, until the recent playback by a group of American historians of a phonautograph recording of "Au clair de la lune" made on April 9, 1860.
The 1860 phonautogram had not until then been played, as it was only a transcription of sound waves into graphic form on paper for visual study. Recently developed optical scanning and image processing techniques have given new life to early recordings by making it possible to play unusually delicate or physically unplayable media without physical contact.

A recording made on a sheet of tinfoil at an 1878 demonstration of Edison's phonograph in St. Louis, Missouri has been played back by optical scanning and digital analysis. A few other early tinfoil recordings are known to survive, including a slightly earlier one which is believed to preserve the voice of U.S. President Rutherford B. Hayes, but as of May 2014 they have not yet been scanned. These antique tinfoil recordings, which have typically been stored folded, are too fragile to be played back with a stylus without seriously damaging them. Edison's 1877 tinfoil recording of "Mary Had a Little Lamb", not preserved, has been called the first instance of recorded verse.
On the occasion of the 50th anniversary of the phonograph, Edison recounted reciting "Mary Had a Little Lamb" to test his first machine. The 1927 event was filmed by an early sound-on-film newsreel camera, and an audio clip from that film's soundtrack is sometimes mistakenly presented as the original 1877 recording.
Wax cylinder recordings made by 19th century media legends such as P. T. Barnum and Shakespearean actor Edwin Booth are amongst the earliest verified recordings by the famous that have survived to the present.

Alexander Graham Bell and his two associates took Edison's tinfoil phonograph and modified it considerably to make it reproduce sound from wax instead of tinfoil. They began their work at Bell's Volta Laboratory in Washington, D. C., in 1879, and continued until they were granted basic patents in 1886 for recording in wax.

Although Edison had invented the phonograph in 1877 the fame bestowed on him for this invention was not due to its efficiency. Recording with his tinfoil phonograph was too difficult to be practical, as the tinfoil tore easily, and even when the stylus was properly adjusted, its reproduction of sound was distorted, and good for only a few playbacks; nevertheless Edison had discovered the idea of sound recording. However immediately after his discovery he did not improve it, allegedly because of an agreement to spend the next five years developing the New York City electric light and power system.

Meanwhile, Bell, a scientist and experimenter at heart, was looking for new worlds to conquer after his invention of the telephone. According to Sumner Tainter, it was through Gardiner Green Hubbard that Bell took up the phonograph challenge. Bell had married Hubbard's daughter Mabel in 1879 while Hubbard was president of the Edison Speaking Phonograph Co., and his organization, which had purchased the Edison patent, was financially troubled because people did not want to buy a machine which seldom worked well and proved difficult for the average person to operate.

In 1879 Hubbard got Bell interested in improving the phonograph, and it was agreed that a laboratory should be set up in Washington. Experiments were also to be conducted on the transmission of sound by light, which resulted in the selenium-celled Photophone.

By 1881, the Volta associates had succeeded in improving an Edison tinfoil machine to some extent. Wax was put in the grooves of the heavy iron cylinder, and no tinfoil was used. Rather than apply for a patent at that time, however, they deposited the machine in a sealed box at the Smithsonian, and specified that it was not to be opened without the consent of two of the three men.

The sound vibrations had been indented in the wax which had been applied to the Edison phonograph. The following was the text of one of their recordings: "There are more things in heaven and earth, Horatio, than are dreamed of in your philosophy. I am a Graphophone and my mother was a phonograph." Most of the disc machines designed at the Volta Lab had their disc mounted on vertical turntables. The explanation is that in the early experiments, the turntable, with disc, was mounted on the shop lathe, along with the recording and reproducing heads. Later, when the complete models were built, most of them featured vertical turntables.

One interesting exception was a horizontal seven inch turntable. The machine, although made in 1886, was a duplicate of one made earlier but taken to Europe by Chichester Bell. Tainter was granted on July 10, 1888. The playing arm is rigid, except for a pivoted vertical motion of 90 degrees to allow removal of the record or a return to starting position. While recording or playing, the record not only rotated, but moved laterally under the stylus, which thus described a spiral, recording 150 grooves to the inch.

The preserved Bell and Tainter records are of both the lateral cut and the Edison-style "hill-and-dale" (up-and-down) styles. Edison for many years used the "hill-and-dale" method on both his cylinders and Diamond Disc records, and Emile Berliner is credited with the invention of the lateral cut, acid-etched Gramophone record in 1887. The Volta associates, however, had been experimenting with both formats and directions of groove modulation as early as 1881.

The basic distinction between the Edison's first phonograph patent and the Bell and Tainter patent of 1886 was the method of recording. Edison's method was to indent the sound waves on a piece of tin foil, while Bell and Tainter's invention called for cutting, or "engraving", the sound waves into a wax record with a sharp recording stylus.

In 1885, when the Volta Associates were sure that they had a number of practical inventions, they filed patent applications and began to seek out investors. The Volta Graphophone Company of Alexandria, Virginia, was created on January 6, 1886 and incorporated on February 3, 1886. It was formed to control the patents and to handle the commercial development of their sound recording and reproduction inventions, one of which became the first Dictaphone.

After the Volta Associates gave several demonstrations in the City of Washington, businessmen from Philadelphia created the American Graphophone Company on March 28, 1887, in order to produce and sell the machines for the budding phonograph marketplace. The Volta Graphophone Company then merged with American Graphophone, which itself later evolved into Columbia Records.

Shortly after American Graphophone's creation, Jesse H. Lippincott used nearly $1 million of an inheritance to gain control of it, as well as the rights to the Graphophone and the Bell and Tainter patents. Not long later Lippincott purchased the Edison Speaking Phonograph Company. He then created the North American Phonograph Company to consolidate the national sales rights of both the Graphophone and the Edison Speaking Phonograph. In the early 1890s Lippincott fell victim to the unit's mechanical problems and also to resistance from stenographers.

A coin-operated version of the Graphophone, , was developed by Tainter in 1893 to compete with "nickel-in-the-slot" entertainment phonograph demonstrated in 1889 by Louis T. Glass, manager of the Pacific Phonograph Company.

The work of the Volta Associates laid the foundation for the successful use of dictating machines in business, because their wax recording process was practical and their machines were durable. But it would take several more years and the renewed efforts of Edison and the further improvements of Emile Berliner and many others, before the recording industry became a major factor in home entertainment.

Discs are not inherently better than cylinders at providing audio fidelity. Rather, the advantages of the format are seen in the manufacturing process: discs can be stamped; cylinders could not be until 1901–1902 when the gold moulding process was introduced by Edison.

Recordings made on a cylinder remain at a constant linear velocity for the entirety of the recording, while those made on a disc have a higher linear velocity at the outer portion of the groove compared to the inner portion.

Edison's patented recording method recorded with vertical modulations in a groove. Berliner utilized a laterally modulated groove.
Though Edison's recording technology was better than Berliner's, there were commercial advantages to a disc system since the disc could be easily mass-produced by molding and stamping and it required less storage space for a collection of recordings.

Berliner successfully argued that his technology was different enough from Edison's that he did not need to pay royalties on it, which reduced his business expenses.

Through experimentation, in 1892 Berliner began commercial production of his disc records, and "gramophones". His "gramophone record" was the first disc record to be offered to the public. They were five inches (12.7 cm) in diameter and recorded on one side only. Seven-inch (17.5 cm) records followed in 1895. Also in 1895 Berliner replaced the hard rubber used to make the discs with a shellac compound. Berliner's early records had very poor sound quality, however. Work by Eldridge R. Johnson eventually improved the sound fidelity to a point where it was as good as the cylinder. By late 1901, ten-inch (25 cm) records were marketed by Johnson and Berliner's Victor Talking Machine Company, and Berliner had sold his interests. In 1904, discs were first pressed with music on both sides and capable of around seven minutes total playing time, as opposed to the cylinder's typical duration on two minutes at that time. As a result of this and the fragility of wax cylinders in transit and storage, cylinders sales declined. Edison felt the increasing commercial pressure for disc records, and by 1912, though reluctant at first, his production of disc records was in full swing. This was the Edison Disc Record. Nevertheless, he continued to manufacture cylinders until 1929 and was last to withdraw from that market.

From the mid-1890s until World War I, both phonograph cylinder and disc recordings and machines to play them on were widely mass-marketed and sold. The disc system superseded the cylinder in Europe by 1906 when both Columbia and Pathe withdrew from that market. By 1913, Edison was the only company still producing cylinders in the USA although in Great Britain small manufacturers pressed on until 1922.

Berliner's lateral disc record was the ancestor of the 78 rpm, 45 rpm, 33⅓ rpm, and all other analogue disc records popular for use in sound recording. See gramophone record.

The 1920s brought improved radio technology. Radio sales increased, bringing many phonograph dealers to near financial ruin. With efforts at improved audio fidelity, the big record companies succeeded in keeping business booming through the end of the decade, but the record sales plummeted during the Great Depression, with many companies merging or going out of business.

Record sales picked up appreciably by the late 30s and early 40s, with greater improvements in fidelity and more money to be spent. By this time home phonographs had become much more common, though it wasn't until the 1940s that console radio/phono set-ups with automatic record changers became more common.

In the 1930s, vinyl (originally known as vinylite) was introduced as a record material for radio transcription discs, and for radio commercials. At that time, virtually no discs for home use were made from this material. Vinyl was used for the popular 78-rpm V-discs issued to US soldiers during World War II. This significantly reduced breakage during transport. The first commercial vinylite record was the set of five 12" discs "Prince Igor" (Asch Records album S-800, dubbed from Soviet masters in 1945). Victor began selling some home-use vinyl 78s in late 1945; but most 78s were made of a shellac compound until the 78-rpm format was completely phased out. (Shellac records were heavier and more brittle.) 33s and 45s were, however, made exclusively of vinyl, with the exception of some 45s manufactured out of polystyrene.

Booms in record sales returned after the Second World War, as industry standards changed from 78s to vinyl, long-playing records (commonly called record albums), which could contain an entire symphony, and 45s which usually contained one hit song popularized on the radio – thus the term "single" record – plus another song on the back or "flip" side. An "extended play" version of the 45 was also available, designated 45 EP, which provided capacity for longer musical selections, or for two regular-length songs per side.

Shortcomings include surface noise caused by dirt or abrasions (scratches) and failure caused by deep surface scratches causing skipping of the stylus forward and missing a section, or groove lock, causing a section to repeat, usually punctuated by a popping noise. This was so common that the phrase: "you sound like a broken record,” was coined, referring to someone who is being annoyingly repetitious.

In 1955, Philco developed and produced the world's first all-transistor phonograph models TPA-1 and TPA-2, which were announced in the June 28, 1955 edition of the "Wall Street Journal". Philco started to sell these all-transistor phonographs in the fall of 1955, for the price of $59.95. The October 1955 issue of "Radio & Television News" magazine (page 41), had a full page detailed article on Philco's new consumer product. The all-transistor portable phonograph TPA-1 and TPA-2 models played only 45rpm records and used four 1.5 volt "D" batteries for their power supply. The "TPA" stands for "Transistor Phonograph Amplifier". Their circuitry used three Philco germanium PNP alloy-fused junction audio frequency transistors. After the 1956 season had ended, Philco decided to discontinue both models, for transistors were too expensive compared to vacuum tubes, but by 1961 a $49.95 ($ in ) portable, battery-powered radio-phonograph with seven transistors was available.

By the 1960s, cheaper portable record players and record changers which played stacks of records in wooden console cabinets were popular, usually with heavy and crude tonearms in the portables. The consoles were often equipped with better quality pick-up cartridges. Even pharmacies stocked 45 rpm records at their front counters. Rock music played on 45s became the soundtrack to the 1960s as people bought the same songs that were played free of charge on the radio. Some record players were even tried in automobiles, but were quickly displaced by 8-track and cassette tapes.

The fidelity of sound reproduction made great advances during the 1970s, as turntables became very precise instruments with belt or direct drive, jewel-balanced tonearms, some with electronically controlled linear tracking and magnetic cartridges. Some cartridges had frequency response above 30 kHz for use with CD-4 quadraphonic 4 channel sound. A high fidelity component system which cost well under $1,000 could do a very good job of reproducing very accurate frequency response across the human audible spectrum from 20 Hz to 20,000 Hz with a $200 turntable which would typically have less than 0.05% wow and flutter and very low rumble (low frequency noise). A well-maintained record would have very little surface noise. 

A novelty variation on the standard format was the use of multiple concentric spirals with different recordings. Thus when the record was played multiple times, different recordings would play, seemingly at random. These were often utilized in talking toys and games.

Records themselves became an art form because of the large surface onto which graphics and books could be printed, and records could be molded into unusual shapes, colors, or with images (picture discs). The turntable remained a common element of home audio systems well after the introduction of other media, such as audio tape and even the early years of the compact disc as a lower-priced music format. However, even though the cost of producing CDs fell below that of records, CDs remained a higher-priced music format than either cassettes or records. Thus, records were not uncommon in home audio systems into the early 1990s.

By the turn of the 21st century, the turntable had become a niche product, as the price of CD players, which reproduce music free of pops and scratches, fell far lower than high-fidelity tape players or turntables. Nevertheless, there is some increase in interest; many big-box media stores carry turntables, as do professional DJ equipment stores. Most low-end and mid-range amplifiers omit the phono input; but on the other hand, low-end turntables with built-in phono pre-amplifiers are widely available. Some combination systems include a basic turntable, a CD player, a cassette deck. and a radio, in a retro-styled cabinet. Records also continue to be manufactured and sold today, albeit in smaller quantities than in the disc phonograph's heyday.

Inexpensive record players typically used a flanged steel stamping for the turntable structure. A rubber disc would be secured to the top of the stamping to provide traction for the record, as well as a small amount of vibration isolation. The spindle bearing usually consisted of a bronze bushing. The flange on the stamping provided a convenient place to drive the turntable by means of an "idler wheel" (see below). While light and cheap to manufacture, these mechanisms had low inertia, making motor speed instabilities more pronounced.

Costlier turntables made from heavy aluminium castings have greater balanced mass and inertia, helping minimize vibration at the stylus, and maintaining constant speed without wow or flutter, even if the motor exhibits cogging effects. Like stamped steel turntables, they were topped with rubber. Because of the increased mass, they usually employed ball bearings or roller bearings in the spindle to reduce friction and noise. Most are belt or direct drive, but some use an idler wheel. A specific case was the Swiss "Lenco" drive, which possessed a very heavy turntable coupled via an idler wheel to a long, tapered motor drive shaft. This enabled stepless rotation or speed control on the drive. Because of this feature the Lenco became popular in the late 1950s with dancing schools, because the dancing instructor could lead the dancing exercises at different speeds.

By the early 1980s, some companies started producing very inexpensive turntables that displaced the products of companies like BSR. Commonly found in "all-in-one" stereos from assorted far-east manufacturers, they used a thin plastic table set in a plastic plinth, no mats, belt drive, weak motors, and often, lightweight plastic tonearms with no counterweight. Most used sapphire pickups housed in ceramic cartridges, and they lacked several features of earlier units, such as auto-start and record-stacking. While not as common now that turntables are absent from the cheap "all-in-one" units, this type of turntable has made a strong resurgence in nostalgia-marketed record players.

From the earliest phonograph designs, many of which were powered by spring-wound mechanisms, a speed governor was essential. Most of these employed some type of flywheel-friction disc to control the speed of the rotating cylinder or turntable; as the speed increased, centrifugal force caused a brake—often a felt pad—to rub against a smooth metal surface, slowing rotation. Electrically powered turntables, whose rotational speed was governed by other means, eventually made their mechanical counterparts obsolete. The mechanical governor was, however, still employed in some toy phonographs (such as those found in talking dolls) until they were replaced by digital sound generators in the late 20th century.

Many modern players have platters with a continuous series of strobe markings machined or printed around their edge. Viewing these markings in artificial light at mains frequency produces a stroboscopic effect, which can be used to verify proper rotational speed. Additionally, the edge of the turntable can contain magnetic markings to provide feedback pulses to an electronic speed-control system.

Earlier designs used a rubberized idler-wheel drive system. However, wear and decomposition of the wheel, as well as the direct mechanical coupling to a vibrating motor, introduced low-frequency noise ("rumble") and speed variations ("wow and flutter") into the sound. These systems generally used a synchronous motor which ran at a speed synchronized to the frequency of the AC power supply. Portable record players typically used an inexpensive shaded-pole motor. At the end of the motor shaft there was a stepped driving capstan; to obtain different speeds, the rubber idler wheel was moved to contact different steps of this capstan. The idler was pinched against the bottom or inside edge of the platter to drive it.

Until the 1970s, the idler-wheel drive was the most common on turntables, except for higher-end audiophile models. However, even some higher-end turntables, such as the Lenco, Garrard, EMT, and Dual turntables, used idler-wheel drive.

Belt drives brought improved motor and platter isolation compared to idler-wheel designs. Motor noise, generally heard as low-frequency rumble, is greatly reduced. The design of the belt drive turntable allows for a less expensive motor than the direct-drive turntable to be used. The elastomeric belt absorbs motor vibrations and noise which could otherwise be picked up by the stylus. It also absorbs small, fast speed variations, caused by "cogging", which in other designs are heard as "flutter."

The "Acoustical professional" turntable (earlier marketed under Dutch ""Jobo prof"") of the 1960s however possessed an expensive German drive motor, the ""Pabst Aussenläufer"" ("Pabst outrunner"). As this motor name implied, the rotor was on the outside of the motor and acted as a flywheel ahead of the belt-driven turntable itself. In combination with a steel to nylon turntable bearing (with molybdenum disulfide inside for lifelong lubrication) very low wow, flutter and rumble figures were achieved.

Direct-drive turntables drive the platter directly without utilizing intermediate wheels, belts, or gears as part of a drive train. The platter functions as a motor armature. This requires good engineering, with advanced electronics for acceleration and speed control. Matsushita's Technics division introduced the first commercially successful direct drive platter, model SP10, in 1969, which was joined by the Technics SL-1200 turntable, in 1972. Its updated model, SL-1200MK2, released in 1978, had a stronger motor, a convenient pitch control slider for beatmatching and a stylus illuminator, which made it the long-standing favourite among disc jockeys ("see "Turntablism""). By the beginnings of the 80s, lowering of costs in microcontroller electronics made direct drive turntables more affordable.

The evaluation of the "best" drive technology is not clear and more depending on the implementation than on the drive technology itself. Technical measurements show that similarly low flutter (0.025% WRMS) and rumble (−78 dB weighed) figures are possible for high quality turntables, be they belt drive or direct drive.

Audiophile grade turntables start at a few hundred dollars and range upwards of $100,000, depending on the complexity and quality of design and manufacture. The common view is that there are diminishing returns with an increase in price – a turntable costing $1,000 would not sound significantly better than a turntable costing $500; nevertheless, there exists a large choice of expensive turntables.

The tone arm (or tonearm) holds the pickup cartridge over the groove, the stylus tracking the groove with the desired force to give the optimal compromise between good tracking and minimizing wear of the stylus and record groove. At its simplest, a tone arm is a pivoted lever, free to move in two axes (vertical and horizontal) with a counterbalance to maintain tracking pressure.
However, the requirements of high-fidelity reproduction place more demands upon the arm design. In a perfect world:


These demands are contradictory and impossible to realize (massless arms and zero-friction bearings do not exist in the real world), so tone arm designs require engineering compromises. Solutions vary, but all modern tonearms are at least relatively lightweight and stiff constructions, with precision, very low friction pivot bearings in both the vertical and horizontal axes. Most arms are made from some kind of alloy (the cheapest being aluminium), but some manufacturers use balsa wood, while others use carbon fiber or graphite. The latter materials favor a straight arm design; alloys' properties lend themselves to S-type arms.

The tone arm got its name before the age of electronics. It originally served to conduct actual sound waves from a purely mechanical "pickup" called a "sound box" or "reproducer" to a so-described "amplifying" horn. The earliest electronic record players, introduced at the end of 1925, had massive electromagnetic pickups that contained a horseshoe magnet, used disposable steel needles, and weighed several ounces. Their full weight rested on the record, providing ample "tracking force" to overcome their low compliance but causing rapid record wear. The tone arms were rudimentary and remained so even after lighter crystal pickups appeared about ten years later. When fine-grooved vinyl records were introduced in the late 1940s, still smaller and lighter crystal (later, ceramic) cartridges with semi-permanent jewel styluses became standard. In the mid-1950s these were joined by a new generation of magnetic cartridges that bore little resemblance to their crude ancestors. Far smaller tracking forces became possible and the "balanced arm" came into use.

Prices varied widely. The well-known and extremely popular high-end S-type SME arm of the 1970–1980 era not only had a complicated design, it was also very costly. On the other hand, even some cheaper arms could be of professional quality: the "All Balance" arm, made by the now-defunct Dutch company Acoustical, was only €30 [equivalent]. It was used during that period by all official radio stations in the Dutch Broadcast studio facilities of the NOS, as well as by the pirate radio station Veronica. Playing records from a boat in international waters, the arm had to withstand sudden ship movements. Anecdotes indicate this low-cost arm was the only one capable of keeping the needle firmly in the groove during heavy storms at sea.

Quality arms employ an adjustable counterweight to offset the mass of the arm and various cartridges and headshells. On this counterweight, a calibrated dial enables easy adjustment of stylus force. After perfectly balancing the arm, the dial itself is "zeroed"; the stylus force can then be dialed in by screwing the counterweight towards the fulcrum. (Sometimes a separate spring or smaller weight provides fine tuning.) Stylus forces of 10 to 20 mN (1 to 2 grams-force) are typical for modern consumer turntables, while forces of up to 50 mN (5 grams) are common for the tougher environmental demands of party deejaying or turntablism. 
Of special adjustment consideration, Stanton cartridges of the 681EE(E) series [and others like them] feature a small record brush ahead of the cartridge. The upforce of this brush, and its added drag require compensation of both tracking force (add 1 gram) and anti-skating adjustment values (see next paragraph for description).
Even on a perfectly flat LP, tonearms are prone to two types of tracking errors that affect the sound. As the tonearm tracks the groove, the stylus exerts a frictional force tangent to the arc of the groove, and since this force does not intersect the tone arm pivot, a clockwise rotational force (moment) occurs and a reaction "skating force" is exerted on the stylus by the record groove wall away from center of the disc. Modern arms provide an "anti-skate" mechanism, using springs, hanging weights, or magnets to produce an offsetting counter-clockwise force at the pivot, making the net lateral force on the groove walls near zero. 
The second error occurs as the arm sweeps in an arc across the disc, causing the angle between the cartridge head and groove to change slightly. A change in angle, albeit small, will have a detrimental effect (especially with stereo recordings) by creating different forces on the two groove walls, as well as a slight timing shift between left/right channels. Making the arm longer to reduce this angle is a partial solution, but less than ideal. A longer arm weighs more, and only an infinitely long [pivoted] arm would reduce the error to zero. Some designs (Burne-Jones, and Garrard "Zero" series) use dual arms in a parallelogram arrangement, pivoting the cartridge head to maintain a constant angle as it moves across the record. Unfortunately this "solution" creates more problems than it solves, compromising rigidity and creating sources of unwanted noise.

The pivoted arm produces yet another problem which is unlikely to be significant to the audiophile, though. As the master was originally cut in a linear motion from the edge towards the center, but the stylus on the pivoted arm always draws an arc, this causes a timing drift that is most significant when digitizing music and beat mapping the data for synchronization with other songs in a DAW or DJ software unless the software allows building a non-linear beat map. As the contact point of the stylus on the record wanders farther from the linear path between the starting point and center hole, the tempo and pitch tend to decrease towards the middle of the record, until the arc reaches its apex. After that the tempo and pitch increase towards the end as the contact point comes closer to the linear path again. Because the surface speed of the record is lower at the end, the relative speed error from the same absolute distance error is higher at the end, and the increase in tempo is more notable towards the end than the decrease towards the middle. This can be somewhat reduced by a curved arm pivoted so that the end point of the arc stays farther from the linear path than the starting point, or by a long straight arm that pivots perpendicularly to the linear path in the middle of the record. However the tempo droop at the middle can only be completely avoided by a linear tracking arm.

If the arm is not pivoted, but instead carries the stylus along a radius of the disc, there is no skating force and little to no cartridge angle error. Such arms are known as "linear tracking" or "tangential" arms. These are driven along a track by various means, from strings and pulleys, to worm gears or electromagnets. The cartridge's position is usually regulated by an electronic servomechanism or mechanical interface, moving the stylus properly over the groove as the record plays, or for song selection.

There are long-armed and short-armed linear arm designs. On a perfectly flat record a short arm will do, but once the record is even slightly warped, a short arm will be troublesome. Any vertical motion of the record surface at the stylus contact point will cause the stylus to considerably move longitudinally in the groove. This will cause the stylus to ride non-tangentially in the groove and cause a stereo phase error as well as pitch error every time the stylus rides over the warp. Also the arm track can come into touch with the record. A long arm will not completely eliminate this problem but will tolerate warped records much better.

Early developments in linear turntables were from Rek-O-Kut (portable lathe/phonograph) and Ortho-Sonic in the 1950s, and Acoustical in the early 1960s. These were eclipsed by more successful implementations of the concept from the late 1960s through the early 1980s.
Of note are Rabco's SL-8, followed by Bang & Olufsen with its Beogram 4000 model in 1972. These models positioned the track outside the platter's edge, as did turntables by Harman Kardon, Mitsubishi, Pioneer, Yamaha, Sony, etc. A 1970s design from Revox harkened back to the 1950s attempts (and, record lathes), positioning the track directly over the record. An enclosed bridge-like assembly is swung into place from the platter's right edge to its middle. Once in place, a short tonearm under this "bridge" plays the record, driven across laterally by a motor. The Sony PS-F5/F9 (1983) uses a similar, miniaturized design, and can operate in a vertical or horizontal orientation. The Technics SL-10, introduced in 1981, was the first direct drive linear tracking turntable, and placed the track and arm on the underside of the rear-hinged dust cover, to fold down over the record, similar to the SL-Q6 pictured.

The earliest Edison phonographs used horizontal, spring-powered drives to carry the stylus across the recording at a pre-determined rate. But, historically as a whole, the linear tracking systems never gained wide acceptance, due largely to their complexity and associated production/development costs. The resources it takes to produce one incredible linear turntable could produce several excellent ones. Some of the most sophisticated and expensive tonearms and turntable units ever made are linear trackers, from companies such as Rockport and Clearaudio. In theory, it seems nearly ideal; a stylus replicating the motion of the recording lathe used to cut the "master" record could result in minimal wear and maximum sound reproduction. In practice, in vinyl's heyday it was generally too much too late.

Since the early 1980s, an elegant solution has been the near-frictionless air bearing linear arm that requires no tracking drive mechanism other than the record groove. This provides a similar benefit as the electronic linear tonearm without the complexity and necessity of servo-motor correction for tracking error. In this case the trade-off is the introduction of pneumatics in the form of audible pumps and tubing. A more elegant solution is the mechanically driven low-friction design, also driven by the groove. Examples include Souther Engineering (U.S.A.), Clearaudio (Germany), and Aura (Czech Republic). This design places an exceeding demand upon precision engineering due to the lack of pneumatics.

Historically, most high-fidelity "component" systems (preamplifiers or receivers) that accepted input from a phonograph turntable had separate inputs for both ceramic and magnetic cartridges (typically labeled "CER" and "MAG"). One piece systems often had no additional phono inputs at all, regardless of type.

Most systems today, if they accept input from a turntable at all, are configured for use only with magnetic cartridges. Manufacturers of high-end systems often have in-built moving coil amplifier circuitry, or outboard head-amplifiers supporting either moving magnet or moving coil cartridges that can be plugged into the line stage.

Additionally, cartridges may contain styli or needles that can be separated according to their tip: Spherical styli, and elliptical styli. Spherical styli have their tip shaped like one half of a sphere, and elliptical styli have their tip shaped like one end of an ellipse. Spherical styli preserve more of the groove of the record than elliptical styli, while elliptical styli offer higher sound quality.

Early electronic phonographs used a piezo-electric "crystal" for pickup (though the earliest electronic phonographs used crude magnetic pick-ups), where the mechanical movement of the stylus in the groove generates a proportional electrical voltage by creating stress within a crystal (typically Rochelle salt). Crystal pickups are relatively robust, and produce a substantial signal level which requires only a modest amount of further amplification. The output is not very linear however, introducing unwanted distortion. It is difficult to make a crystal pickup suitable for quality stereo reproduction, as the stiff coupling between the crystal and the long stylus prevents close tracking of the needle to the groove modulations. This tends to increase wear on the record, and introduces more distortion. Another problem is the hygroscopic nature of the crystal itself: it absorbs moisture from the air and may dissolve. The crystal was protected by embedding it in other materials, without hindering the movement of the pickup mechanism itself. After a number of years, the protective jelly often deteriorated or leaked from the cartridge case and the full unit needed replacement.

The next development was the "ceramic" cartridge, a piezoelectric device that used newer and better materials. These were more sensitive, and offered greater "compliance", that is, lack of resistance to movement and so increased ability to follow the undulations of the groove without gross distorting or jumping out of the groove. Higher compliance meant lower tracking forces and reduced wear to both the disc and stylus. It also allowed ceramic stereo cartridges to be made.

During the 1950s to 1970s, ceramic cartridge became common in low quality phonographs, but better high-fidelity (or "hi-fi") systems used magnetic cartridges, and the availability of low cost magnetic cartridges from the 1970s onwards made ceramic cartridges obsolete for essentially all purposes. At the seeming end of the market lifespan of ceramic cartridges, someone accidentally discovered that by terminating a specific ceramic mono cartridge (the Ronette TX88) not with the prescribed 47 kΩ resistance, but with approx. 10 kΩ, it could be connected to the moving magnet (MM) input too. The result, a much smoother frequency curve extended the lifetime for this popular and very cheap type.

There are two common designs for magnetic cartridges, moving magnet (MM) and moving coil (MC) (originally called "dynamic"). Both operate on the same physics principle of electromagnetic induction. The moving magnet type was by far the most common and more robust of the two, though audiophiles often claim that the moving coil system yields higher fidelity sound.

In either type, the stylus itself, usually of diamond, is mounted on a tiny metal strut called a cantilever, which is suspended using a collar of highly compliant plastic. This gives the stylus the freedom to move in any direction. On the other end of the cantilever is mounted a tiny permanent magnet (moving magnet type) or a set of tiny wound coils (moving coil type). The magnet is close to a set of fixed pick-up coils, or the moving coils are held within a magnetic field generated by fixed permanent magnets. In either case, the movement of the stylus as it tracks the grooves of a record causes a fluctuating magnetic field, which causes a small electric current to be induced in the coils. This current closely follows the sound waveform cut into the record, and may be transmitted by wires to an electronic amplifier where it is processed and amplified in order to drive a loudspeaker. Depending upon the amplifier design, a phono-preamplifier may be necessary.

In most moving magnet designs, the stylus itself is detachable from the rest of the cartridge so it can easily be replaced. There are three primary types of cartridge mounts. The most common type is attached using two small screws to a headshell that then plugs into the tonearm, while another is a standardized "P-mount" or "T4P" cartridge (invented by Technics in 1980 and adopted by other manufacturers) that plugs directly into the tonearm. Many P-mount cartridges come with adapters to allow them to be mounted to a headshell. The third type is used mainly in cartridges designed for DJ use and it has a standard round headshell connector. Some mass market turntables use a proprietary integrated cartridge that cannot be upgraded.

An alternative design is the "moving iron" variation on moving magnet used by ADC, Grado, Stanton/Pickering 681 series, Ortofon OM and VMS series, and the MMC cartridge of Bang & Olufsen. In these units, the magnet itself sits behind the four coils and magnetises the cores of all four coils. The moving iron cross at the other end of the coils varies the gaps between itself and each of these cores, according to its movements. These variations lead to voltage variations as described above.

Famous brands for magnetic cartridges are: Grado, Stanton/Pickering (681EE/EEE), B&O (MM types for its two, non-compatible generations of parallel arm design), Shure (V15 Type I to V), Audio-Technica, Nagaoka, Dynavector, Koetsu, Ortofon, Technics, Denon and ADC.

Strain gauge or "semiconductor" cartridges do not generate a voltage, but act like a variable resistor, whose resistance directly depends on the movement of the stylus. Thus, the cartridge "modulates" an external voltage supplied by the (special) preamplifier. These pickups were marketed by Euphonics, Sao Win, and Panasonic/Technics, amongst others.

The main advantages (compared to magnetic carts are):


The main disadvantage is the need of a special preamplifier that supplies a steady current (typically 5mA) to the semiconductor elements and handles a special equalization than the one needed for magnetic cartridges.

A high-end strain-gauge cartridge is currently sold by an audiophile company, with special preamplifiers available.

Electrostatic cartridges were marketed by Stax in the 1950 and 1960 years. They needed individual operating electronics or preamplifiers.

A few specialist laser turntables read the groove optically using a laser pickup. Since there is no physical contact with the record, no wear is incurred. However, this "no wear" advantage is debatable, since vinyl records have been tested to withstand even 1200 plays with no significant audio degradation, provided that it is played with a high quality cartridge and that the surfaces are clean.

An alternative approach is to take a high-resolution photograph or scan of each side of the record and interpret the image of the grooves using computer software. An amateur attempt using a flatbed scanner lacked satisfactory fidelity. A professional system employed by the Library of Congress produces excellent quality.

A smooth-tipped "stylus" (in popular usage often called a "needle" due to the former use of steel needles for the purpose) is used to play the recorded groove. A special chisel-like stylus is used to engrave the groove into the "master record".

The stylus is subject to hard wear as it is the only small part that comes into direct contact with the spinning record. In terms of the pressure imposed on its minute areas of actual contact, the forces it must bear are enormous. There are three desired qualities in a stylus: first, that it faithfully follows the contours of the recorded groove and transmits its vibrations to the next part in the chain; second, that it does not damage the recorded disc; and third, that it is resistant to wear. A worn-out, damaged or defective stylus tip will degrade audio quality and injure the groove.

Different materials for the stylus have been used over time. Thomas Edison introduced the use of sapphire in 1892 and the use of diamond in 1910 for his cylinder phonographs. The Edison Diamond Disc players (1912–1929), when properly played, hardly ever required the stylus to be changed. The styli for vinyl records were also made out of sapphire or diamond. A specific case is the specific stylus type of Bang & Olufsen's (B&O) moving magnet cartridge MMC 20CL, mostly used in parallel arm B&O turntables in the 4002/6000 series. It uses a sapphire stem on which a diamond tip is fixed by a special adhesive. A stylus tip mass as low as 0.3 milligram is the result and full tracking only requires 1 gram of stylus force, reducing record wear even further. Maximum distortion (2nd harmonic) fell below 0.6%.

Other than the Edison and European Pathé disc machines, early disc players, both external horn and internal horn "Victrola" style models, normally used very short-lived disposable needles. The most common material was steel, although other materials such as copper, tungsten, bamboo and cactus were used. Steel needles needed to be replaced frequently, preferably after each use, due to their very rapid wear from bearing down heavily on the mildly abrasive shellac record. Rapid wear was an essential feature so that their imprecisely formed tips would be quickly worn into compliance with the groove's contours. Advertisements implored customers to replace their steel needles after each record side. Steel needles were inexpensive, e.g., a box of 500 for 50 US cents, and were widely sold in packets and small tins. They were available in different thicknesses and lengths. Thick, short needles produced strong, loud tones while thinner, longer needles produces softer, muted tones. In 1916, in the face of a wartime steel shortage, Victor introduced their "Tungs-Tone" brand extra-long-playing needle, which was advertised to play between 100 and 300 records. It consisted of a brass shank into which a very hard and strong tungsten wire, somewhat narrower than the standard record groove, had been fitted. The protruding wire wore down, but not out, until it was worn too short to use. Later in the 78 rpm era, hardened steel and chrome-plated needles came on the market, some of which were claimed to play 10 to 20 record sides each.

When sapphires were introduced for the 78 rpm disc and the LP, they were made by tapering a stem and polishing the tip to a sphere with a radius of around 70 and 25 micrometers respectively. A sphere is not equal to the form of the cutting stylus and by the time diamond needles came to the market, a whole discussion was started on the effect of circular forms moving through a non-circular cut groove. It can be easily shown that vertical, so called "pinching" movements were a result and when stereophonic LPs were introduced, unwanted vertical modulation was recognized as a problem. Also, the needle started its life touching the groove on a very small surface, giving extra wear on the walls.
Another problem is in the tapering along a straight line, while the side of the groove is far from straight. Both problems were attacked together: by polishing the diamond in a certain way that it could be made doubly elliptic. 1) the side was made into one ellipse as seen from behind, meaning the groove touched along a short line and 2) the ellipse form was also polished as seen from above and curvature in the direction of the groove became much smaller than 25 micrometers e.g. 13 micrometers. With this approach a number of irregularities were eliminated. Furthermore, the angle of the stylus, which used to be always sloping backwards, was changed into the forward direction, in line with the slope the original cutting stylus possessed. These styli were expensive to produce, but the costs were effectively offset by their extended lifespans.

The next development in stylus form came about by the attention to the CD-4 quadraphonic sound modulation process, which requires up to 50 kHz frequency response, with cartridges like Technics EPC-100CMK4 capable of playback on frequencies up to 100 kHz. This requires a stylus with a narrow side radius, such as 5 µm (or 0.2 mil). A narrow-profile elliptical stylus is able to read the higher frequencies (greater than 20 kHz), but at an increased wear, since the contact surface is narrower. For overcoming this problem, the Shibata stylus was invented around 1972 in Japan by Norio Shibata of JVC, fitted as standard on quadraphonic cartridges, and marketed as an extra on some high-end cartridges.

The Shibata-designed stylus offers a greater contact surface with the groove, which in turn means less pressure over the vinyl surface and thus less wear. A positive side effect is that the greater contact surface also means the stylus will read sections of the vinyl that were not touched (or "worn") by the common spherical stylus. In a demonstration by JVC records "worn" after 500 plays at a relatively very high 4.5 gf tracking force with a spherical stylus, played "as new" with the Shibata profile.

Other advanced stylus shapes appeared following the same goal of increasing contact surface, improving on the Shibata. Chronologically: "Hughes" Shibata variant (1975), "Ogura" (1978), Van den Hul (1982). Such a stylus may be marketed as "Hyperelliptical" (Shure), "Alliptic", "Fine Line" (Ortofon), "Line contact" (Audio Technica), "Polyhedron", "LAC", or "Stereohedron" (Stanton).

A keel-shaped diamond stylus appeared as a byproduct of the invention of the CED Videodisc. This, together with laser-diamond-cutting technologies, made possible the "ridge" shaped stylus, such as the Namiki (1985) design, and Fritz Gyger (1989) design. This type of stylus is marketed as "MicroLine" (Audio technica), "Micro-Ridge" (Shure), or "Replicant" (Ortofon).

It is important to point out that most of those stylus profiles are still being manufactured and sold, together with the more common spherical and elliptical profiles. This is despite the fact that production of CD-4 quadraphonic records ended by the late 1970s.

Early materials in the 19th century were hardened rubber, wax, and celluloid, but early in the 20th century a shellac compound became the standard. Since shellac is not hard enough to withstand the wear of steel needles on heavy tone arms, filler made of pulverized shale was added. Shellac was also fragile, and records often shattered or cracked. This was a problem for home records, but it became a bigger problem in the late 1920s with the Vitaphone sound-on-disc motion picture "talkie" system, developed in 1927.

To solve this problem, in 1930, RCA Victor made unbreakable records by mixing polyvinyl chloride with plasticisers, in a proprietary formula they called Victrolac, which was first used in 1931, in motion picture discs, and experimentally, in home records, the same year. However, with Sound-on-film achieving supremacy over sound-on-disc by 1931, the need for unbreakable records diminished and the production of vinyl home recordings was dropped as well, for the time being.

The Victrolac formula improved throughout the 1930s, and by the late 30s the material, by then called vinylite, was being used in records sent to radio stations for radio program records, radio commercials, and later, DJ copies of phonograph records, because vinyl records could be sent through the mail to radio stations without breaking. During WWII, there was a shortage of shellac, which had to be imported from Asia, and the U.S. government banned production of shellac records for the duration of the war. Vinylite was made domestically, though, and was being used for V-discs during the war. Record company engineers took a much closer look at the possibilities of vinyl, possibly that it might even replace shellac as the basic record material. After the war, RCA Victor and Columbia, by far the two leading records companies in America, perfected two new vinyl formats, which were both introduced in 1948, when the 33 RPM LP was introduced by Columbia and the 45 RPM single was introduced by RCA Victor. For a few years thereafter, however, 78 RPM records continued to be made in shellac until that format was phased out around 1958.

Early "acoustical" record players used the stylus to vibrate a diaphragm that radiated the sound through a horn. Several serious problems resulted from this:


The introduction of electronic amplification allowed these issues to be addressed. Records are made with boosted high frequencies and reduced low frequencies, which allow for different ranges of sound to be produced. This reduces the effect of background noise, including clicks or pops, and also conserves the amount of physical space needed for each groove, by reducing the size of the low-frequency undulations.

During playback, the high frequencies must be rescaled to their original, flat frequency response—known as "equalization"—as well as being amplified. A phono input of an amplifier incorporates such equalization as well as amplification to suit the very low level output from a modern cartridge. Most hi-fi amplifiers made between the 1950s and the 1990s and virtually all DJ mixers are so equipped.

The widespread adoption of digital music formats, such as CD or satellite radio, has displaced phonograph records and resulted in phono inputs being omitted in most modern amplifiers. Some newer turntables include built-in preamplifiers to produce line-level outputs. Inexpensive and moderate performance discrete phono preamplifiers with RIAA equalization are available, while high-end audiophile units costing thousands of dollars continue to be available in very small numbers. Phono inputs are starting to reappear on amplifiers in the 2010s due to the vinyl revival.

Since the late 1950s, almost all phono input stages have used the RIAA equalization standard. Before settling on that standard, there were many different equalizations in use, including EMI, HMV, Columbia, Decca FFRR, NAB, Ortho, BBC transcription, etc. Recordings made using these other equalization schemes will typically sound odd if they are played through a RIAA-equalized preamplifier. High-performance (so-called "multicurve disc") preamplifiers, which include multiple, selectable equalizations, are no longer commonly available. However, some vintage preamplifiers, such as the LEAK varislope series, are still obtainable and can be refurbished. Newer preamplifiers like the Esoteric Sound Re-Equalizer or the K-A-B MK2 Vintage Signal Processor are also available. These kinds of adjustable phono equalizers are used by consumers wishing to play vintage record collections (often the only available recordings of musicians of the time) with the equalization used to make them.

Turntables continue to be manufactured and sold in the 2010s, although in small numbers. While some audiophiles still prefer the sound of vinyl records over that of digital music sources (mainly compact discs), they represent a minority of listeners. As of 2015 the sale of vinyl LP's has increased 49–50% percent from the previous year although small in comparison to the sale of other formats which although more units were sold (Digital Sales, CDs) the more modern formats experienced a decline in sales. The quality of available record players, tonearms, and cartridges has continued to improve, despite diminishing demand, allowing turntables to remain competitive in the high-end audio market. Vinyl enthusiasts are often committed to the refurbishment and sometimes tweaking of vintage systems.

In 2017, vinyl LP sales were slightly decreased, at a rate of 5%, in comparison to previous years' numbers, regardless of the noticeable rise of vinyl records sales worldwide. 

Updated versions of the 1970s era Technics SL-1200 (production ceased in 2010) have remained an industry standard for DJs to the present day. Turntables and vinyl records remain popular in mixing (mostly dance-oriented) forms of electronic music, where they allow great latitude for physical manipulation of the music by the DJ.

In hip hop music and occasionally in other genres, the turntable is used as a musical instrument by DJs, who use turntables along with a DJ mixer to create unique rhythmic sounds. Manipulation of a record as part of the music, rather than for normal playback or mixing, is called turntablism. The basis of turntablism, and its best known technique, is "scratching", pioneered by Grand Wizzard Theodore. It was not until Herbie Hancock's "Rockit" in 1983 that the turntablism movement was recognized in popular music outside of a hip hop context. In the 2010s, many hip hop DJs use DJ CD players or digital record emulator devices to create scratching sounds; nevertheless, some DJs still scratch with vinyl records.

The laser turntable uses a laser as the pickup instead of a stylus in physical contact with the disk. It was conceived of in the late 1980s, although early prototypes were not of usable audio quality. Practical laser turntables are now being manufactured by ELPJ. They are favoured by record libraries and some audiophiles since they eliminate physical wear completely.
Experimentation is in progress in retrieving the audio from old records by scanning the disc and analysing the scanned image, rather than using any sort of turntable.

Although largely replaced since the introduction of the compact disc in 1982, record albums still sell in small numbers and are available through numerous sources. In 2008, LP sales grew by 90% over 2007, with 1.9 million records sold.

USB turntables have a built-in audio interface, which transfers the sound directly to the connected computer. Some USB turntables transfer the audio without equalization, but are sold with software that allows the EQ of the transferred audio file to be adjusted. There are also many turntables on the market designed to be plugged into a computer via a USB port for needle dropping purposes.

Responding to longtime calls by fans and disc jockeys, Panasonic Corp. said it is reviving Technics turntables–the series that remains a de facto standard player supporting nightclub music scenes.
The new analog turntable, which would come with new direct-drive motor technologies that Panasonic says would improve the quality of sound, would be released sometime between April 2016 and March 2017, the Japanese electronics company announced on September 2, 2015.


anged Ethnography." Jackson: University Press of Mississippi, 1999.



</doc>
<doc id="24472" url="https://en.wikipedia.org/wiki?curid=24472" title="Paul Cézanne">
Paul Cézanne

Paul Cézanne ( , , , ; 19 January 1839 – 22 October 1906) was a French artist and Post-Impressionist painter whose work laid the foundations of the transition from the 19th-century conception of artistic endeavor to a new and radically different world of art in the 20th century. 

Cézanne is said to have formed the bridge between late 19th-century Impressionism and the early 20th century's new line of artistic enquiry, Cubism. Cézanne's often repetitive, exploratory brushstrokes are highly characteristic and clearly recognizable. He used planes of colour and small brushstrokes that build up to form complex fields. The paintings convey Cézanne's intense study of his subjects. Both Matisse and Picasso are said to have remarked that Cézanne "is the father of us all".

The Cézannes came from the commune of Saint-Sauveur (Hautes-Alpes, Occitania). Paul Cézanne was born on 19 January 1839 in Aix-en-Provence. On 22 February, he was baptized in the Église de la Madeleine, with his grandmother and uncle Louis as godparents, and became a devout Catholic later in life. His father, Louis Auguste Cézanne (1798–1886), a native of Saint-Zacharie (Var), was the co-founder of a banking firm (Banque Cézanne et Cabassol) that prospered throughout the artist's life, affording him financial security that was unavailable to most of his contemporaries and eventually resulting in a large inheritance.
His mother, Anne Elisabeth Honorine Aubert (1814–1897), was "vivacious and romantic, but quick to take offence". It was from her that Cézanne got his conception and vision of life. He also had two younger sisters, Marie and Rose, with whom he went to a primary school every day.

At the age of ten Cézanne entered the Saint Joseph school in Aix. In 1852 Cézanne entered the Collège Bourbon in Aix (now Collège Mignet), where he became friends with Émile Zola, who was in a less advanced class, as well as Baptistin Baille—three friends who came to be known as "Les Trois Inséparables" (The Three Inseparables). He stayed there for six years, though in the last two years he was a day scholar. In 1857, he began attending the Free Municipal School of Drawing in Aix, where he studied drawing under Joseph Gibert, a Spanish monk. From 1858 to 1861, complying with his father's wishes, Cézanne attended the law school of the University of Aix, while also receiving drawing lessons.

Going against the objections of his banker father, he committed himself to pursue his artistic development and left Aix for Paris in 1861. He was strongly encouraged to make this decision by Zola, who was already living in the capital at the time. Eventually, his father reconciled with Cézanne and supported his choice of career. Cézanne later received an inheritance of 400,000 francs from his father, which rid him of all financial worries.

In Paris, Cézanne met the Impressionist Camille Pissarro. Initially the friendship formed in the mid-1860s between Pissarro and Cézanne was that of master and disciple, in which Pissarro exerted a formative influence on the younger artist. Over the course of the following decade their landscape painting excursions together, in Louveciennes and Pontoise, led to a collaborative working relationship between equals.

Cézanne's early work is often concerned with the figure in the landscape and includes many paintings of groups of large, heavy figures in the landscape, imaginatively painted. Later in his career, he became more interested in working from direct observation and gradually developed a light, airy painting style. Nevertheless, in Cézanne's mature work there is the development of a solidified, almost architectural style of painting. Throughout his life he struggled to develop an authentic observation of the seen world by the most accurate method of representing it in paint that he could find. To this end, he structurally ordered whatever he perceived into simple forms and colour planes. His statement "I want to make of impressionism something solid and lasting like the art in the museums", and his contention that he was recreating Poussin "after nature" underscored his desire to unite observation of nature with the permanence of classical composition.

Cézanne was interested in the simplification of naturally occurring forms to their geometric essentials: he wanted to "treat nature in terms of the cylinder, the sphere and the cone" (a tree trunk may be conceived of as a cylinder, an apple or orange a sphere, for example). Additionally, Cézanne's desire to capture the truth of perception led him to explore binocular vision graphically, rendering slightly different, yet simultaneous visual perceptions of the same phenomena to provide the viewer with an aesthetic experience of depth different from those of earlier ideals of perspective, in particular single-point perspective. His interest in new ways of modelling space and volume derived from the stereoscopy obsession of his era and from reading Hippolyte Taine’s Berkelean theory of spatial perception. Cézanne's innovations have prompted critics to suggest such varied explanations as sick retinas, pure vision, and the influence of the steam railway.

Cézanne's paintings were shown in the first exhibition of the Salon des Refusés in 1863, which displayed works not accepted by the jury of the official Paris Salon. The Salon rejected Cézanne's submissions every year from 1864 to 1869. He continued to submit works to the Salon until 1882. In that year, through the intervention of fellow artist Antoine Guillemet, he exhibited "Portrait de M. L. A.", probably "Portrait of Louis-Auguste Cézanne, The Artist's Father, Reading "L'Événement"", 1866 (National Gallery of Art, Washington, D.C.), his first and last successful submission to the Salon.

Before 1895 Cézanne exhibited twice with the Impressionists (at the first Impressionist exhibition in 1874 and the third Impressionist exhibition in 1877). In later years a few individual paintings were shown at various venues, until 1895, when the Parisian dealer, Ambroise Vollard, gave the artist his first solo exhibition. Despite the increasing public recognition and financial success, Cézanne chose to work in increasing artistic isolation, usually painting in the south of France, in his beloved Provence, far from Paris.

He concentrated on a few subjects and was equally proficient in each of these genres: still lifes, portraits, landscapes and studies of bathers. For the last, Cézanne was compelled to design from his imagination, due to a lack of available nude models. Like the landscapes, his portraits were drawn from that which was familiar, so that not only his wife and son but local peasants, children and his art dealer served as subjects. His still lifes are at once decorative in design, painted with thick, flat surfaces, yet with a weight reminiscent of Gustave Courbet. The 'props' for his works are still to be found, as he left them, in his studio (atelier), in the suburbs of modern Aix.
Cézanne's paintings were not well received among the petty bourgeoisie of Aix. In 1903 Henri Rochefort visited the auction of paintings that had been in Zola's possession and published on 9 March 1903 in L'Intransigeant a highly critical article entitled "Love for the Ugly". Rochefort describes how spectators had supposedly experienced laughing fits, when seeing the paintings of "an ultra-impressionist named Cézanne". The public in Aix was outraged, and for many days, copies of L'Intransigeant appeared on Cézanne's door-mat with messages asking him to leave the town "he was dishonouring".

One day, Cézanne was caught in a storm while working in the field. After working for two hours he decided to go home; but on the way he collapsed. He was taken home by a passing driver. His old housekeeper rubbed his arms and legs to restore the circulation; as a result, he regained consciousness. On the following day, he intended to continue working, but later on he fainted; the model with whom he was working called for help; he was put to bed, and he never left it. He died a few days later, on 22 October 1906 of pneumonia and was buried at the Saint-Pierre Cemetery in his hometown of Aix-en-Provence.

Various periods in the work and life of Cézanne have been defined.

In 1863 Napoleon III created by decree the Salon des Refusés, at which paintings rejected for display at the Salon of the Académie des Beaux-Arts were to be displayed. The artists of the refused works included the young Impressionists, who were considered revolutionary. Cézanne was influenced by their style but his social relations with them were inept—he seemed rude, shy, angry, and given to depression. His works of this period are characterized by dark colours and the heavy use of black. They differ sharply from his earlier watercolours and sketches at the École Spéciale de dessin at Aix-en-Provence in 1859, and their violence of expression is in contrast to his subsequent works.

In 1866–67, inspired by the example of Courbet, Cézanne painted a series of paintings with a palette knife. He later called these works, mostly portraits, "une couillarde" ("a coarse word for ostentatious virility"). Lawrence Gowing has written that Cézanne's palette knife phase "was not only the invention of modern expressionism, although it was incidentally that; the idea of art as emotional ejaculation made its first appearance at this moment".

Among the "couillarde" paintings are a series of portraits of his uncle Dominique in which Cézanne achieved a style that "was as unified as Impressionism was fragmentary".
Later works of the dark period include several erotic or violent subjects, such as "Women Dressing" (), "The Rape" (), and "The Murder" (), which depicts a man stabbing a woman who is held down by his female accomplice.

After the start of the Franco-Prussian War in July 1870, Cézanne and his mistress, Marie-Hortense Fiquet, left Paris for L'Estaque, near Marseille, where he changed themes to predominantly landscapes. He was declared a draft dodger in January 1871, but the war ended the next month, in February, and the couple moved back to Paris, in the summer of 1871. After the birth of their son Paul in January 1872, in Paris, they moved to Auvers in Val-d'Oise near Paris. Cézanne's mother was kept a party to family events, but his father was not informed of Hortense for fear of risking his wrath. The artist received from his father a monthly allowance of 100 francs.
Camille Pissarro lived in Pontoise. There and in Auvers he and Cézanne painted landscapes together. For a long time afterwards, Cézanne described himself as Pissarro's pupil, referring to him as "God the Father", as well as saying: "We all stem from Pissarro." Under Pissarro's influence Cézanne began to abandon dark colours and his canvases grew much brighter.

Leaving Hortense in the Marseille region, Cézanne moved between Paris and Provence, exhibiting in the first (1874) and third Impressionist shows (1877). In 1875, he attracted the attention of the collector Victor Chocquet, whose commissions provided some financial relief. But Cézanne's exhibited paintings attracted hilarity, outrage, and sarcasm. Reviewer Louis Leroy said of Cézanne's portrait of Chocquet: "This peculiar looking head, the colour of an old boot might give [a pregnant woman] a shock and cause yellow fever in the fruit of her womb before its entry into the world."

In March 1878, Cézanne's father found out about Hortense and threatened to cut Cézanne off financially, but, in September, he relented and decided to give him 400 francs for his family. Cézanne continued to migrate between the Paris region and Provence until Louis-Auguste had a studio built for him at his home, Bastide du Jas de Bouffan, in the early 1880s. This was on the upper floor, and an enlarged window was provided, allowing in the northern light but interrupting the line of the eaves; this feature remains. Cézanne stabilized his residence in L'Estaque. He painted with Renoir there in 1882 and visited Renoir and Monet in 1883.

In the early 1880s the Cézanne family stabilized their residence in Provence where they remained, except for brief sojourns abroad, from then on. The move reflects a new independence from the Paris-centered impressionists and a marked preference for the south, Cézanne's native soil. Hortense's brother had a house within view of Montagne Sainte-Victoire at Estaque. A run of paintings of this mountain from 1880 to 1883 and others of Gardanne from 1885 to 1888 are sometimes known as "the Constructive Period".

The year 1886 was a turning point for the family. Cézanne married Hortense. In that year also, Cézanne's father died, leaving him the estate purchased in 1859; he was 47. By 1888 the family was in the former manor, Jas de Bouffan, a substantial house and grounds with outbuildings, which afforded a new-found comfort. This house, with much-reduced grounds, is now owned by the city and is open to the public on a restricted basis.

For many years it was believed that Cézanne broke off his friendship with Émile Zola, after the latter used him, in large part, as the basis for the unsuccessful and ultimately tragic fictitious artist Claude Lantier, in the novel "L'Œuvre".

Recently letters have been discovered that refute this. A letter from 1887 demonstrates that their friendship did endure for at least some time after.

Cézanne's idyllic period at Jas de Bouffan was temporary. From 1890 until his death he was beset by troubling events and he withdrew further into his painting, spending long periods as a virtual recluse. His paintings became well-known and sought after and he was the object of respect from a new generation of painters.
The problems began with the onset of diabetes in 1890, destabilizing his personality to the point where relationships with others were again strained. He traveled in Switzerland, with Hortense and his son, perhaps hoping to restore their relationship. Cézanne, however, returned to Provence to live; Hortense and Paul junior, to Paris. Financial need prompted Hortense's return to Provence but in separate living quarters. Cézanne moved in with his mother and sister. In 1891 he turned to Catholicism.

Cézanne alternated between painting at Jas de Bouffan and in the Paris region, as before. In 1895, he made a germinal visit to Bibémus Quarries and climbed Montagne Sainte-Victoire. The labyrinthine landscape of the quarries must have struck a note, as he rented a cabin there in 1897 and painted extensively from it. The shapes are believed to have inspired the embryonic "Cubist" style. Also in that year, his mother died, an upsetting event but one which made reconciliation with his wife possible. He sold the empty nest at Jas de Bouffan and rented a place on Rue Boulegon, where he built a studio.

The relationship, however, continued to be stormy. He needed a place to be by himself. In 1901 he bought some land along the Chemin des Lauves, an isolated road on some high ground at Aix, and commissioned a studio to be built there (now open to the public). He moved there in 1903. Meanwhile, in 1902, he had drafted a will excluding his wife from his estate and leaving everything to his son. The relationship was apparently off again; she is said to have burned the mementos of his mother.

From 1903 to the end of his life he painted in his studio, working for a month in 1904 with Émile Bernard, who stayed as a house guest. After his death it became a monument, Atelier Paul Cézanne, or les Lauves.

Cézanne's stylistic approaches and beliefs regarding how to paint were analyzed and written about by the French philosopher Maurice Merleau-Ponty who is primarily known for his association with phenomenology and existentialism. In his 1945 essay entitled "Cézanne's Doubt", Merleau-Ponty discusses how Cézanne gave up classic artistic elements such as pictorial arrangements, single view perspectives, and outlines that enclosed color in an attempt to get a "lived perspective" by capturing all the complexities that an eye observes. He wanted to see and sense the objects he was painting, rather than think about them. Ultimately, he wanted to get to the point where "sight" was also "touch". He would take hours sometimes to put down a single stroke because each stroke needed to contain "the air, the light, the object, the composition, the character, the outline, and the style". A still life might have taken Cézanne one hundred working sessions while a portrait took him around one hundred and fifty sessions. Cèzanne believed that while he was painting, he was capturing a moment in time, that once passed, could not come back. The atmosphere surrounding what he was painting was a part of the sensational reality he was painting. Cèzanne claimed: "Art is a personal apperception, which I embody in sensations and which I ask the understanding to organize into a painting."

Cézanne's works were rejected numerous times by the official Salon in Paris and ridiculed by art critics when exhibited with the Impressionists. Yet during his lifetime Cézanne was considered a master by younger artists who visited his studio in Aix.

Along with the work of Vincent van Gogh and Paul Gauguin, the work of Cézanne, with its sense of immediacy and incompletion, critically influenced Matisse and others prior to Fauvism and Expressionism. After Cézanne died in 1906, his paintings were exhibited in a large museum-like retrospective in Paris, September 1907. The 1907 Cézanne retrospective at the Salon d'Automne greatly affected the direction that the avant-garde in Paris took, lending credence to his position as one of the most influential artists of the 19th century and to the advent of Cubism.

Inspired by Cézanne, Albert Gleizes and Jean Metzinger wrote:
Cézanne is one of the greatest of those who changed the course of art history . . . From him we have learned that to alter the coloring of an object is to alter its structure. His work proves without doubt that painting is not—or not any longer—the art of imitating an object by lines and colors, but of giving plastic [solid, but alterable] form to our nature. ("Du "Cubisme"", 1912)

Ernest Hemingway compared his writing to Cézanne’s landscapes. As he describes in A Moveable Feast, I was "learning something from the painting of Cezanne that made writing simple true sentences far from enough to make the stories have the dimensions that I was trying to put in them."

Cézanne's explorations of geometric simplification and optical phenomena inspired Picasso, Braque, Metzinger, Gleizes, Gris and others to experiment with ever more complex views of the same subject and eventually to the fracturing of form. Cézanne thus sparked one of the most revolutionary areas of artistic enquiry of the 20th century, one which was to affect profoundly the development of modern art. Picasso referred to Cézanne as "the father of us all" and claimed him as "my one and only master!" Other painters such as Edgar Degas, Pierre-Auguste Renoir, Paul Gauguin, Kasimir Malevich, Georges Rouault, Paul Klee, and Henri Matisse acknowledged Cézanne's genius.

Cézanne's painting "The Boy in the Red Vest" was stolen from a Swiss museum in 2008. It was recovered in a Serbian police raid in 2012.

The 2016 film "Cézanne and I" explores the friendship between the artist and Émile Zola.

On 10 May 1999, Cézanne's painting "Rideau, Cruchon et Compotier" sold for $60.5 million, the fourth-highest price paid for a painting up to that time. As of 2006, it was the most expensive still life ever sold at an auction. One of Cézanne's "The Card Players" was sold in 2011 to the Royal Family of Qatar for a price variously estimated at between $250 million ($ million today) and possibly as high as $300 million ($ million today), either price signifying a new mark for highest price for a painting up to that date. The record price was surpassed in November 2017 by Leonardo da Vinci's "Salvator Mundi".






</doc>
<doc id="24473" url="https://en.wikipedia.org/wiki?curid=24473" title="Pope Innocent VI">
Pope Innocent VI

Pope Innocent VI (; 1282 or 1295 – 12 September 1362), born Étienne Aubert, was head of the Catholic Church and ruler of the Papal States from 18 December 1352 to his death in 1362. He was the fifth Avignon pope and the only one with the pontifical name of "Innocent".

Étienne's father was Adhemar Aubert (1260-?), seigneur de Montel-de-Gelat in Limousin province. He was a native of the hamlet of Les Monts, Diocese of Limoges (today part of the commune of Beyssac, "département" of Corrèze), and, after having taught civil law at Toulouse, he became successively Bishop of Noyon in 1338 and Bishop of Clermont in 1340. On 20 September 1342, he was raised to the position of Cardinal Priest of SS. John and Paul. He was made cardinal-bishop of Ostia and Velletri on 13 February 1352, by Pope Clement VI, whom he succeeded.

Etienne was crowned pope on 30 December 1352 by Cardinal Gaillard de la Mothe after the papal conclave of 1352. Upon his election, he revoked a signed agreement stating the college of cardinals was superior to the pope. His subsequent policy compares favourably with that of the other Avignon Popes. He introduced many needed reforms in the administration of church affairs, and through his legate, Cardinal Albornoz, who was accompanied by Rienzi, he sought to restore order in Rome. In 1355, Charles IV, Holy Roman Emperor, was crowned in Rome with Innocent's permission, after having made an oath that he would quit the city on the day of the ceremony.

It was largely through the exertions of Innocent VI that the Treaty of Brétigny (1360) between France and England was brought about. During his pontificate, the Byzantine emperor John V Palaeologus offered to submit the Greek Orthodox Church to the Roman See in return for assistance against John VI Cantacuzenus. The resources at the disposal of the Pope, however, were all required for exigencies nearer home, and the offer was declined.

Most of the wealth accumulated by John XXII and Benedict XII had been lost during the extravagant pontificate of Clement VI. Innocent VI economised by cutting the chapel staff ("capellani capelle") from twelve to eight. Works of art were sold rather than commissioned. His pontificate was dominated by the war in Italy and by Avignon's recovery from the plague, both of which made draining demands on his treasury. By 1357, he was complaining of poverty.

Innocent VI was a liberal patron of letters. If the extreme severity of his measures against the Fraticelli is ignored, he retains a high reputation for justice and mercy. However, St. Bridget of Sweden denounced him as a persecutor of Christians. He died on 12 September 1362 and was succeeded by Urban V. Today his tomb can be found in the Chartreuse du Val de Bénédiction, the Carthusian monastery in Villeneuve-lès-Avignon.



 


</doc>
<doc id="24474" url="https://en.wikipedia.org/wiki?curid=24474" title="Polyandry">
Polyandry

Polyandry (; from "poly-", "many" and ἀνήρ "anēr", "man") is a form of polygamy in which a woman takes two or more husbands at the same time. Polyandry is contrasted with polygyny, involving one male and two or more females. If a marriage involves a plural number of "husbands and wives" participants of each gender, then it can be called polygamy, group or conjoint marriage. In its broadest use, polyandry refers to sexual relations with multiple males within or without marriage.

Of the 1,231 societies listed in the 1980 Ethnographic Atlas, 186 were found to be monogamous; 453 had occasional polygyny; 588 had more frequent polygyny; and 4 had polyandry. Polyandry is less rare than this figure suggests, as it considered only those examples found in the Himalayan mountains (28 societies). More recent studies have found more than 50 other societies practicing polyandry.<ref name="Starkweather/Hames 2012"></ref>

Fraternal polyandry is practiced among Tibetans in Nepal and parts of China, in which two or more brothers are married to the same wife, with the wife having equal "sexual access" to them. It is associated with "partible paternity", the cultural belief that a child can have more than one father.

Polyandry is believed to be more likely in societies with scarce environmental resources. It is believed to limit human population growth and enhance child survival. It is a rare form of marriage that exists not only among peasant families but also among the elite families. For example, polyandry in the Himalayan mountains is related to the scarcity of land. The marriage of all brothers in a family to the same wife allows family land to remain intact and undivided. If every brother married separately and had children, family land would be split into unsustainable small plots. In contrast, very poor persons not owning land were less likely to practice polyandry in Buddhist Ladakh and Zanskar. In Europe, the splitting up of land was prevented through the social practice of impartible inheritance. With most siblings disinherited, many of them became celibate monks and priests.

Polyandrous mating systems are also a common phenomenon in the animal kingdom.

Unlike fraternal polyandry where a woman will receive a number of husbands simultaneously, a woman will acquire one husband after another in sequence.

This form is flexible. These men may or may not be related. And it may or may not incorporate a hierarchical system, where one husband is considered "primary" and may be allotted certain rights or privileges not awarded to secondary husbands, such as biologically fathering a child.

In this particular system, the "secondary" husbands have the power to succeed the primary if he were to become severely ill or be away from the home for a long period of time or is otherwise rendered incapable of fulfilling his husbandly duties.

Successional polyandry can likewise be egalitarian, where all husbands are equal in status and receive the same rights and privileges. In this system, each husband will have a wedding ceremony and share paternity of whatever children she may bear.

"Other Classifications: Secondary"

Another form of polyandry is a combination of polyandry and polygyny, as women are married to several men simultaneously and the same men are married to several women. It is found in some tribes of native Americans as well as villages in northern Nigeria and the northern cameroons.

"Other Classifications: Equal polygamy, Polygynandry"

The system results in less land fragmentation, and a diversification of domestic activities.

Fraternal polyandry (from the Latin "frater"—brother), also called adelphic polyandry (from the Greek ""—brother), is a form of polyandry in which a woman is married to two or more men who are brothers. Fraternal polyandry was (and sometimes still is) found in certain areas of Tibet, Nepal, and Northern India, central African cultures where polyandry was accepted as a social practice. The Toda people of southern India practice fraternal polyandry, but monogamy has become prevalent recently. In contemporary Hindu society, polyandrous marriages in agrarian societies in the Malwa region of Punjab seem to occur to avoid division of farming land.

Fraternal polyandry achieves a similar goal to that of primogeniture in 19th-century England. Primogeniture dictated that the eldest son inherited the family estate, while younger sons had to leave home and seek their own employment. Primogeniture maintained family estates intact over generations by permitting only one heir per generation. Fraternal polyandry also accomplishes this, but does so by keeping all the brothers together with just one wife so that there is only one set of heirs per generation. This strategy appears less successful the larger the fraternal sibling group is.

Some forms of polyandry appear to be associated with a perceived need to retain aristocratic titles or agricultural lands within kin groups, and/or because of the frequent absence, for long periods, of a man from the household. In Tibet the practice was particularly popular among the priestly Sakya class.

The female equivalent of fraternal polyandry is sororate marriage.

Anthropologist Stephen Beckerman points out that at least 20 tribal societies accept that a child could, and ideally should, have more than one father, referring to it as "partible paternity". This often results in the shared nurture of a child by multiple fathers in a form of polyandric relation to the mother, although this is not always the case. One of the most well known examples is that of Trobriand "virgin birth". The matrilineal Trobriand Islanders recognize the importance of sex in reproduction but do not believe the male makes a contribution to the constitution of the child, who therefore remains attached to their mother's lineage alone. The mother's non-resident husbands are not recognized as fathers, although the mother's co-resident brothers are, since they are part of the mother's lineage.

According to inscriptions describing the reforms of the Sumerian king Urukagina of Lagash (ca. 2300 BC), the earlier custom of polyandry in his country was abolished, on pain of the woman taking multiple husbands being stoned upon which her crime is written.

An extreme gender imbalance has been suggested as a justification for polyandry. For example, the selective abortion of female fetuses in India has led to a significant margin in sex ratio and, it has been suggested, results in related men "sharing" a wife.

Polyandry in Tibet was a common practice and continues to a lesser extent today. A survey of 753 Tibetan families by Tibet University in 1988 found that 13% practiced polyandry. Polyandry in India still exists among minorities, and also in Bhutan, and the northern parts of Nepal. Polyandry has been practised in several parts of India, such as Rajasthan, Ladakh and Zanskar, in the Jaunsar-Bawar region in Uttarakhand, among the Toda of South India.

It also occurs or has occurred in Nigeria, the Nymba, and some pre-contact Polynesian societies, though probably only among higher caste women. It is also encountered in some regions of Yunnan and Sichuan regions of China, among the Mosuo people in China (who also practice polygyny as well), and in some sub-Saharan African such as the Maasai people in Kenya and northern Tanzania and American indigenous communities. The Guanches, the first known inhabitants of the Canary Islands, practiced polyandry until their disappearance. The Zo'e tribe in the state of Pará on the Cuminapanema River, Brazil, also practice polyandry.







There is at least one reference to polyandry in the ancient Hindu epic "Mahabharata". Draupadi married the five Pandava brothers, as this is what she chose in a previous life. This ancient text remains largely neutral to the concept of polyandry, accepting this as her way of life. However, in the same epic, when questioned by Kunti to give an example of polyandry, Yudhishthira cites Gautam-clan Jatila (married to seven Saptarishis) and Hiranyaksha's sister Pracheti (married to ten brothers), thereby implying a more open attitude toward polyandry in Vedic society.

The Hebrew Bible contains no examples of women married to more than one man, but its description of adultery clearly implies that polyandry is unacceptable and the practice is unknown in Jewish tradition. In addition, the children from other than the first husband are considered illegitimate, unless he has already divorced her or died (i.e., a mamzer), being a product of an adulterous relationship.

Most Christian denominations in the Western world strongly advocate monogamous marriage, and a passage from the Pauline epistles () can be interpreted as forbidding polyandry.

Joseph Smith and Brigham Young, and other early Latter-day Saints, practiced polygynous marriages. The practice was officially ended with the 1890 Manifesto. Polyandrous marriages did exist, albeit in significantly less numbers, in early LDS history.

Although Islamic marital law allows men to have up to four wives, polyandry is prohibited in Islam.

Polyandrous marriages were practiced in pre-Islamic Arabian cultures, but were outlawed during the rise of Islam. Nikah Ijtimah was a pagan tradition of polyandry in older Arab regions which was condemned and abolished during the rise of Islam.

Polyandrous behavior is quite widespread in the animal kingdom, occurring for example in insects, fish, birds, and mammals.

Types of mating, marriage and lifestyle:



</doc>
<doc id="24475" url="https://en.wikipedia.org/wiki?curid=24475" title="Polygamy">
Polygamy

Polygamy (from Late Greek , "polygamía", "state of marriage to many spouses") is the practice of marrying multiple spouses. When a man is married to more than one wife at the same time, sociologists call this polygyny. When a woman is married to more than one husband at a time, it is called polyandry. If a marriage includes multiple husbands and wives, it can be called a group marriage.

In contrast, monogamy is marriage consisting of only two parties. Like "monogamy", the term "polygamy" is often used in a "de facto" sense, applied regardless of whether the state recognizes the relationship. In sociobiology and zoology, researchers use "polygamy" in a broad sense to mean any form of multiple mating.

Worldwide, different societies variously encourage, accept or outlaw polygamy. Of societies which allow or tolerate polygamy, in the vast majority of cases the form accepted is polygyny. According to the Ethnographic Atlas (1998), of 1,231 societies noted, 588 had frequent polygyny, 453 had occasional polygyny, 186 were monogamous and 4 had polyandry; although more recent research suggests polyandry may be more common than previously thought.<ref name="Starkweather/Hames 2012"></ref> In cultures which practice polygamy, its prevalence among that population is often connected to class and socioeconomic status.

From a legal point of view, in many countries, although marriage is legally monogamous (a person can only have one spouse, and bigamy is illegal), adultery is not illegal, leading to a situation of "de facto" polygamy being allowed, although without legal recognition for non-official "spouses".

According to scientific studies, the human mating system is considered to be primarily monogamous, with cultural practice of polygamy to be in the minority, based on both surveys of world populations, and on characteristics of human reproductive physiology.

Polygamy exists in three specific forms:

Polygyny, the practice wherein a man has more than one wife at the same time, is by far the most common form of polygamy. Many Muslim-majority countries and some countries with sizable Muslim minorities accept polygyny to varying extents both legally and culturally; some secular countries like India also accept it to varying degrees. Islamic law or sharia law is a religious law forming part of the Islamic tradition which allows polygyny. It is derived from the religious precepts of Islam, particularly the Quran and the hadith. In Arabic, the term "sharīʿah" refers to God's (Arabic: الله‎ Allāh) immutable divine law and is contrasted with "fiqh", which refers to its human scholarly interpretations.

Polygyny is more widespread in Africa than on any other continent, especially in West Africa, and some scholars see the slave trade's impact on the male-to-female sex ratio as a key factor in the emergence and fortification of polygynous practices in regions of Africa.

Anthropologist Jack Goody's comparative study of marriage around the world utilizing the Ethnographic Atlas demonstrated an historical correlation between the practice of extensive shifting horticulture and polygamy in the majority of sub-Saharan African societies. Drawing on the work of Ester Boserup, Goody notes that the sexual division of labour varies between the male-dominated intensive plough-agriculture common in Eurasia and the extensive shifting horticulture found in sub-Saharan Africa. In some of the sparsely-populated regions where shifting cultivation takes place in Africa, women do much of the work. This favours polygamous marriages in which men seek to monopolize the production of women "who are valued both as workers and as child bearers". Goody however, observes that the correlation is imperfect and varied, and also discusses more traditionally male-dominated though relatively extensive farming systems such as those that exist in much of West Africa, especially in the West African savanna, where more agricultural work is done by men, and where polygyny is desired by men more for the generation of male offspring whose labor is valued.

Anthropologists Douglas R. White and Michael L. Burton discuss and support Jack Goody's observation regarding African male farming systems in "Causes of Polygyny: Ecology, Economy, Kinship, and Warfare" where these authors note:

An analysis by James Fenske (2012) found that child mortality and ecologically-related economic shocks had a significant association with rates of polygamy in subsaharan Africa, rather than female agricultural contributions (which are typically relatively small in the West African savanna and sahel, where polygyny rates are higher), finding that polygyny rates decrease significantly with child mortality rates.

Polygynous marriages fall into two types: "sororal polygyny", in which the co-wives are sisters, and "non-sororal", where the co-wives are not related. Polygyny offers husbands the benefit of allowing them to have more children, may provide them with a larger number of productive workers (where workers are family), and allows them to establish politically useful ties with a greater number of kin groups. Senior wives can benefit as well when the addition of junior wives to the family lightens their workload. Wives', especially senior wives', status in a community can increase through the addition of other wives, who add to the family's prosperity or symbolize conspicuous consumption (much as a large house, domestic help, or expensive vacations operate in a western country). For such reasons, senior wives sometimes work hard or contribute from their own resources to enable their husbands to accumulate the bride price for an extra wife.

Polygyny may also result from the practice of levirate marriage. In such cases, the deceased man's heir may inherit his assets and wife; or, more usually, his brothers may marry the widow. This provides support for the widow and her children (usually also members of the brothers' kin group) and maintains the tie between the husbands' and wives' kin groups. The sororate resembles the levirate, in that a widower must marry the sister of his dead wife. The family of the late wife, in other words, must provide a replacement for her, thus maintaining the marriage alliance. Both levirate and sororate may result in a man having multiple wives.
In monogamous societies, wealthy and powerful men established enduring relationships with, and established separate household for, multiple female partners, aside from their legitimate wives; a practice accepted in Imperial China up until the Qing Dynasty of 1636–1912. This constitutes a form of "de facto" polygyny referred to as concubinage.

Marriage is the moment at which a new household is formed, but different arrangements may occur depending upon the type of marriage and some polygamous marriages do not result in the formation of a single household. In many polygynous marriages the husband's wives may live in separate households. They can thus be described as a "series of linked nuclear families with a 'father' in common".

Polyandry, the practice of a woman having more than one husband at the one time, is much less prevalent than polygyny and is now illegal in virtually every country in the world. It takes place only in remote communities.

Polyandry is believed to be more common in societies with scarce environmental resources, as it is believed to limit human population growth and enhance child survival. It is a rare form of marriage that exists not only among poor families, but also the elite. For example, in the Himalayan Mountains polyandry is related to the scarcity of land; the marriage of all brothers in a family to the same wife allows family land to remain intact and undivided. If every brother married separately and had children, family land would be split into unsustainable small plots. In Europe, this outcome was avoided through the social practice of impartible inheritance, under which most siblings would be disinherited.

"Fraternal polyandry" was traditionally practiced among nomadic Tibetans in Nepal, parts of China and part of northern India, in which two or more brothers would marry the same woman. It is most common in societies marked by high male mortality. It is associated with "partible paternity", the cultural belief that a child can have more than one father.

"Non-fraternal polyandry" occurs when the wives' husbands are unrelated, as among the Nayar tribe of India, where girls undergo a ritual marriage before puberty, and the first husband is acknowledged as the father of all her children. However, the woman may never cohabit with that man, taking multiple lovers instead; these men must acknowledge the paternity of their children (and hence demonstrate that no caste prohibitions have been breached) by paying the midwife. The women remain in their maternal home, living with their brothers, and property is passed matrilineally. A similar form of matrilineal, de facto polyandry can be found in the institution of walking marriage among the Mosuo tribe of China.

Serial monogamy refers to remarriage after divorce or death of a spouse from a monogamous marriage, i.e. multiple marriages but only one legal spouse at a time (a series of monogamous relationships).

According to Danish scholar Miriam K. Zeitzen, anthropologists treat serial monogamy, in which divorce and remarriage occur, as a form of polygamy as it also can establish a series of households that may continue to be tied by shared paternity and shared income. As such, they are similar to the household formations created through divorce and serial monogamy.

Serial monogamy creates a new kind of relative, the "ex-". The "ex-wife", for example, can remain an active part of her "ex-husband's" life, as they may be tied together by legally or informally mandated economic support, which can last for years, including by alimony, child support, and joint custody. Bob Simpson, the British social Anthropologist, notes that it creates an "extended family" by tying together a number of households, including mobile children. He says that Britons may have ex‑wives or ex‑brothers‑in‑law, but not an "ex‑child". According to him, these "unclear families" do not fit the mold of the monogamous nuclear family.

Group marriage is a non-monogamous marriage-like arrangement where three or more adults live together, all considering themselves partners, sharing finances, children, and household responsibilities. Polyamory is on a continuum of family-bonds that includes group marriage. The term does not refer to bigamy as no claim to being married in formal legal terms is made.

Buddhism does not regard marriage as a sacrament - it is purely a secular affair, and normally Buddhist monks do not participate in it (though in some sects priests and monks do marry). Hence marriage receives no religious sanction. Forms of marriage, in consequence, vary from country to country. The Parabhava Sutta states that "a man who is not satisfied with one woman and seeks out other women is on the path to decline". Other fragments in the Buddhist scripture seem to treat polygamy unfavorably, leading some authors to conclude that Buddhism generally does not approve of it or alternatively regards it as a tolerated, but subordinate, marital model.

Thailand legally recognized polygamy until 1955. Myanmar outlawed polygyny from 2015. In Sri Lanka, polyandry was legal in the kingdom of Kandy, but outlawed by British after conquering the kingdom in 1815. When the Buddhist texts were translated into Chinese, the concubines of others were added to the list of inappropriate partners. Polyandry in Tibet was common traditionally, as was polygyny, and having several wives or husbands was never regarded as having sex with inappropriate partners.
Most typically, fraternal polyandry is practiced, but sometimes father and son have a common wife, which is a unique family structure in the world. Other forms of marriage are also present, like group marriage and monogamous marriage. Polyandry (especially fraternal polyandry) is also common among Buddhists in Bhutan, Ladakh, and other parts of the Indian subcontinent.

Some pre-Christian Celtic pagans were known to practice polygamy, although the Celtic peoples wavered between it, monogamy and polyandry depending on the time period and area. In some areas this continued even after Christianization began, for instance the Brehon Laws of Gaelic Ireland explicitly allowed for polygamy, especially amongst the noble class. Some modern Celtic pagan religions accept the practice of polygamy to varying degrees, though how widespread the practice is within these religions is unknown.

Polygamy is not forbidden in the Old Testament. Although the New Testament is largely silent on polygamy, some point to Jesus's repetition of the earlier scriptures, noting that a man and a wife "shall become one flesh". However, some look to Paul's writings to the Corinthians: "Do you not know that he who is joined to a prostitute becomes one body with her? For, as it is written, 'The two will become one flesh. Supporters of polygamy claim this indicates that the term refers to a physical, rather than spiritual, union.

Some Christian theologians argue that in and referring to Jesus explicitly states a man should have only one wife:
The Bible states in the New Testament that polygamy (including adulterous relationships, as evident from verses condemning sexual immorality) should not be practiced by certain church leaders. 1 Timothy states that certain Church leaders should have but one wife: "A "bishop" then must be blameless, the husband of one wife, vigilant, sober, of good behavior, given to hospitality, apt to teach" (chapter 3, verse 2; see also verse 12 regarding deacons having only one wife). Similar counsel is repeated in the first chapter of the Epistle to Titus.

Periodically, Christian reform movements that have aimed at rebuilding Christian doctrine based on the Bible alone ("sola scriptura") have at least temporarily accepted polygyny as a Biblical practice. For example, during the Protestant Reformation, in a document referred to simply as ""Der Beichtrat"" (or ""The Confessional Advice"" ), Martin Luther granted the Landgrave Philip of Hesse, who, for many years, had been living "constantly in a state of adultery and fornication", a dispensation to take a second wife. The double marriage was to be done in secret, however, to avoid public scandal. Some fifteen years earlier, in a letter to the Saxon Chancellor Gregor Brück, Luther stated that he could not "forbid a person to marry several wives, for it does not contradict Scripture." (""Ego sane fateor, me non posse prohibere, si quis plures velit uxores ducere, nec repugnat sacris literis."")

In Sub-Saharan Africa, there has often been a tension between the Christian insistence on monogamy and traditional polygamy. For instance, Mswati III, the Christian king of Swaziland, has 15 wives. In some instances in recent times there have been moves for accommodation; in other instances, churches have resisted such moves strongly. African Independent Churches have sometimes referred to those parts of the Old Testament that describe polygamy in defending the practice.

The Roman Catholic Church condemns polygamy; the "Catechism of the Catholic Church" lists it in paragraph 2387 under the head "Other offenses against the dignity of marriage" and states that it "is not in accord with the moral law." Also in paragraph 1645 under the head "The Goods and Requirements of Conjugal Love" states "The unity of marriage, distinctly recognized by our Lord, is made clear in the equal personal dignity which must be accorded to husband and wife in mutual and unreserved affection. Polygamy is contrary to conjugal love which is undivided and exclusive."

Saint Augustine saw a conflict with Old Testament polygamy. He refrained from judging the patriarchs, but did not deduce from their practice the ongoing acceptability of polygyny. On the contrary, he argued that the polygamy of the Fathers, which was tolerated by the Creator because of fertility, was a diversion from His original plan for human marriage. Augustine wrote: "That the good purpose of marriage, however, is better promoted by one husband with one wife, than by a husband with several wives, is shown plainly enough by the very first union of a married pair, which was made by the Divine Being Himself."

Augustine taught that the reason patriarchs had many wives was not because of fornication, but because they wanted more children. He supported his premise by showing that their marriages, in which husband was the head, were arranged according to the rules of good management: those who are "in command" ("quae principantur") in their society were always singular, while "subordinates" ("subiecta") were multiple. He gave two examples of such relationships: "dominus-servus" - master-servant (in older translation: "slave") and "God-soul". The Bible often equates worshiping multiple gods, i.e. idolatry to fornication. Augustine relates to that: "On this account there is no True God of souls, save One: but one soul by means of many false gods may commit fornication, but not be made fruitful."

As tribal populations grew, fertility was no longer a valid justification of polygamy: it "was lawful among the ancient fathers: whether it be lawful now also, I would not hastily pronounce (utrum et nunc fas sit, non temere dixerim). For there is not now necessity of begetting children, as there then was, when, even when wives bear children, it was allowed, in order to a more numerous posterity, to marry other wives in addition, which now is certainly not lawful."

Augustine saw marriage as a covenant between one man and one woman, which may not be broken. It was the Creator who established monogamy: "Therefore, the first natural bond of human society is man and wife." Such marriage was confirmed by the Saviour in the Gospel of Matthew (Mat 19:9) and by His presence at the wedding in Cana (John 2:2). In the Church—the City of God—marriage is a sacrament and may not and cannot be dissolved as long as the spouses live: "But a marriage once for all entered upon in the City of our God, where, even from the first union of the two, the man and the woman, marriage bears a certain sacramental character, can in no way be dissolved but by the death of one of them." In chapter 7, Augustine pointed out that the Roman Empire forbad polygamy, even if the reason of fertility would support it: "For it is in a man's power to put away a wife that is barren, and marry one of whom to have children. And yet it is not allowed; and now indeed in our times, and after the usage of Rome (nostris quidem iam temporibus ac more Romano), neither to marry in addition, so as to have more than one wife living." Further on he notices that the Church's attitude goes much further than the secular law regarding monogamy: It forbids remarrying, considering such to be a form of fornication: "And yet, save in the City of our God, in His Holy Mount, the case is not such with the wife. But, that the laws of the Gentiles are otherwise, who is there that knows not."

The Council of Trent condemns polygamy: "If any one saith, that it is lawful for Christians to have several wives at the same time, and that this is not prohibited by any divine law; let him be anathema.."

In modern times a minority of Roman Catholic theologians have argued that polygamy, though not ideal, can be a legitimate form of Christian marriage in certain regions, in particular Africa. The Roman Catholic Church teaches in its Catechism that 
polygamy is not in accord with the moral law. [Conjugal] communion is radically contradicted by polygamy; this, in fact, directly negates the plan of God that was revealed from the beginning, because it is contrary to the equal personal dignity of men and women who in matrimony give themselves with a love that is total and therefore unique and exclusive.

The illegality of polygamy in certain areas creates, according to certain Bible passages, additional arguments against it. Paul the Apostle writes "submit to the authorities, not only because of possible punishment but also because of conscience" (Romans 13:5), for "the authorities that exist have been established by God." (Romans 13:1) St Peter concurs when he says to "submit yourselves for the Lord's sake to every authority instituted among men: whether to the king, as the supreme authority, or to governors, who are sent by him to punish those who do wrong and to commend those who do right." (1 Peter 2:13,14) Pro-polygamists argue that, as long as polygamists currently do not obtain legal marriage licenses nor seek "common law marriage status" for additional spouses, no enforced laws are being broken any more than when monogamous couples similarly co-habitate without a marriage license.

The Lutheran World Federation hosted a regional conference in Africa, in which the acceptance of polygamists into full membership by the Lutheran Church in Liberia was defended as being permissible. The Lutheran Church in Liberia, however, does not permit polygamists who have become Christians to marry more wives after they have received the sacrament of Holy Baptism. Evangelical Lutheran missionaries in Maasai also tolerate the practice of polygamy and in Southern Sudan, some polygamists are becoming Lutheran Christians.

The 1988 Lambeth Conference of the Anglican Communion ruled that polygamy was permissible in certain circumstances:

In accordance with what Joseph Smith indicated was a revelation, the practice of plural marriage, the marriage of one man to two or more women, was instituted among members of The Church of Jesus Christ of Latter-day Saints in the early 1840s. Despite Smith's revelation, the 1835 edition of the 101st Section of the "Doctrine and Covenants", written after the doctrine of plural marriage began to be practiced, publicly condemned polygamy. This scripture was used by John Taylor in 1850 to quash Mormon polygamy rumors in Liverpool, England. Polygamy was made illegal in the state of Illinois during the 1839–44 Nauvoo era when several top Mormon leaders, including Smith, Brigham Young and Heber C. Kimball took multiple wives. Mormon elders who publicly taught that all men were commanded to enter plural marriage were subject to harsh discipline. On 7 June 1844 the "Nauvoo Expositor" criticized Smith for plural marriage.

After Joseph Smith was killed by a mob on 27 June 1844, the main body of Latter Day Saints left Nauvoo and followed Brigham Young to Utah where the practice of plural marriage continued. In 1852, Brigham Young, the second president of the LDS Church, publicly acknowledged the practice of plural marriage through a sermon he gave. Additional sermons by top Mormon leaders on the virtues of polygamy followed. Controversy followed when polygamy became a social cause, writers began to publish works condemning polygamy. The key plank of the Republican Party's 1856 platform was "to prohibit in the territories those twin relics of barbarism, polygamy and slavery". In 1862, Congress issued the Morrill Anti-Bigamy Act which clarified that the practice of polygamy was illegal in all US territories. The LDS Church believed that their religiously based practice of plural marriage was protected by the United States Constitution, however, the unanimous 1878 Supreme Court decision "Reynolds v. United States" declared that polygamy was not protected by the Constitution, based on the longstanding legal principle that "laws are made for the government of actions, and while they cannot interfere with mere religious belief and opinions, they may with practices."

Increasingly harsh anti-polygamy legislation in the US led some Mormons to emigrate to Canada and Mexico. In 1890, LDS Church president Wilford Woodruff issued a public declaration (the Manifesto) announcing that the LDS Church had discontinued new plural marriages. Anti-Mormon sentiment waned, as did opposition to statehood for Utah. The Smoot Hearings in 1904, which documented that the LDS Church was still practicing polygamy spurred the LDS Church to issue a Second Manifesto again claiming that it had ceased performing new plural marriages. By 1910 the LDS Church excommunicated those who entered into, or performed, new plural marriages. Even so, many plural husbands and wives continued to cohabit until their deaths in the 1940s and 1950s.

Enforcement of the 1890 Manifesto caused various splinter groups to leave the LDS Church in order to continue the practice of plural marriage. Polygamy among these groups persists today in Utah and neighboring states as well as in the spin-off colonies. Polygamist churches of Mormon origin are often referred to as "Mormon fundamentalist" even though they are not a part of the LDS Church. Such fundamentalists often use a purported 1886 revelation to John Taylor as the basis for their authority to continue the practice of plural marriage. "The Salt Lake Tribune" stated in 2005 there were as many as 37,000 fundamentalists with less than half of them living in polygamous households.

On 13 December 2013, US Federal Judge Clark Waddoups ruled in "Brown v. Buhman" that the portions of Utah's anti-polygamy laws which prohibit multiple cohabitation were unconstitutional, but also allowed Utah to maintain its ban on multiple marriage licenses. Unlawful cohabitation, where prosecutors did not need to prove that a marriage ceremony had taken place (only that a couple had lived together), had been the primary tool used to prosecute polygamy in Utah since the 1882 Edmunds Act.

The Council of Friends (also known as the Woolley Group and the Priesthood Council) was one of the original expressions of Mormon fundamentalism, having its origins in the teachings of Lorin C. Woolley, a dairy farmer excommunicated from the LDS Church in 1924. Several Mormon fundamentalist groups claim lineage through the Council of Friends, including but not limited to, the Fundamentalist Church of Jesus Christ of Latter Day Saints (FLDS Church), the Apostolic United Brethren, the Centennial Park group, the Latter Day Church of Christ, and the Righteous Branch of the Church of Jesus Christ of Latter-day Saints.

The Community of Christ, known as the Reorganized Church of Jesus Christ of Latter Day Saints (RLDS Church) prior to 2001, has never sanctioned polygamy since its foundation in 1860. Joseph Smith III, the first Prophet-President of the RLDS Church following the reorganization of the Church, was an ardent opponent of the practice of plural marriage throughout his life. For most of his career, Smith denied that his father had been involved in the practice and insisted that it had originated with Brigham Young. Smith served many missions to the western United States, where he met with and interviewed associates and women claiming to be widows of his father, who attempted to present him with evidence to the contrary. Smith typically responded to such accusations by saying that he was "not positive nor sure that was innocent", and that if, indeed, the elder Smith had been involved, it was still a false practice. However, many members of the Community of Christ and some of the groups that were formerly associated with it are not convinced that Joseph Smith practiced plural marriage and feel that the evidence that he did is flawed.

The Rig Veda mentions that during the Vedic period, a man could have more than one wife. The practice is attested in epics like Ramayana and Mahabharata. The Dharmashastras permit a man to marry women provided that the first wife agree to marry him. Despite its existence, it was most usually practiced by men of higher status. Common people were only allowed a second marriage if the first wife could not bear a son or have some dispute because there is no law for divorce in hinduism.

According to Vishnu Smriti, the number of wives is linked to the knowledge system:
This linkage of the number of permitted wives to the knowledge system is also supported by Baudhayana Dharmasutra and Paraskara Grihyasutra.

The Apastamba Dharmasutra and Manusmriti allow a second wife if the first one is unable to discharge her religious duties or is unable to bear a child or have any dispute because in hinduism there was no law for divorce.

For a Brahmana, only one wife could rank as the chief consort who performed the religious rites ("dharma-patni") along with the husband. The chief consort had to be of an equal knowledge. If a man married several women from the same knowledgeable, then eldest wife is the chief consort. Hindu kings commonly had more than one wife and are regularly attributed four wives by the scriptures. They were: Mahisi who was the chief consort, Parivrkti who had no son, Vaivata who is considered the favorite wife and the Palagali who was the daughter of the last of the court officials.

Traditional Hindu law allowed polygamy if the first wife could not bear a child.

The Hindu Marriage Act was enacted in 1955 by the Indian Parliament and made polygamy illegal for everyone in India except for Muslims. Prior to 1955, polygamy was permitted for Hindus. Marriage laws in India are dependent upon the religion of the parties in question.

In Islamic marital jurisprudence, under reasonable and warranted conditions, a Muslim man may have more than one wife at the same time, up to a total of four. Muslim women are not permitted to have more than one husband at the same time under any circumstances.

Based on verse 30:21 of Quran the ideal relationship is the comfort that a couple find in each other's embrace:
The polygyny that is allowed in the Quran is for special situations. There are strict requirements to marrying more than one woman, as the man must treat them equally financially and in terms of support given to each wife, according to Islamic law. However, Islam advises monogamy for a man if he fears he can't deal justly with his wives. This is based on verse 4:3 of Quran which says:

Muslim women are not allowed to marry more than one husband at once. However, in the case of a divorce or their husbands' death they can remarry after the completion of Iddah, as divorce is legal in Islamic law. A non-Muslim woman who flees from her non-Muslim husband and accepts Islam has the option to remarry without divorce from her previous husband, as her marriage with non-Muslim husband is Islamically dissolved on her fleeing. A non-Muslim woman captured during war by Muslims, can also remarry, as her marriage with her non-Muslim husband is Islamically dissolved at capture by Muslim soldiers. This permission is given to such women in verse 4:24 of Quran. The verse also emphasizes on transparency, mutual agreement and financial compensation as prerequisites for matrimonial relationship as opposed to prostitution; it says:
Muhammad was monogamously married to Khadija, his first wife, for 25 years, until she died. After her death, he married multiple women, mostly widows, for social and political reasons. He had a total of nine wives, but not all at the same time, depending on the sources in his lifetime. The Qur'an does not give preference in marrying more than one wife. One reason cited for polygyny is that it allows a man to give financial protection to multiple women, who might otherwise not have any support (e.g. widows). However, the wife can set a condition, in then marriage contract, that the husband cannot marry another woman during their marriage. In such a case, the husband cannot marry another woman as long as he is married to his wife. According to traditional Islamic law, each of those wives keeps their property and assets separate; and are paid mahar and maintenance separately by their husband. Usually the wives have little to no contact with each other and lead separate, individual lives in their own houses, and sometimes in different cities, though they all share the same husband.

In most Muslim-majority countries, polygyny is legal with Kuwait being the only one where no restrictions are imposed on it. The practice is illegal in Muslim-majority Turkey, Tunisia, Albania, Kosovo and Central Asian countries.

Countries that allow polygyny typically also require a man to obtain permission from his previous wives before marrying another, and require the man to prove that he can financially support multiple wives. In Malaysia and Morocco, a man must justify taking an additional wife at a court hearing before he is allowed to do so. In Sudan, the government encouraged polygyny in 2001 to increase the population.

The Torah contains a few specific regulations that apply to polygamy, such as Exodus 21:10: "If he take another wife for himself; her food, her clothing, and her duty of marriage, shall he not diminish". , states that a man must award the inheritance due to a first-born son to the son who was actually born first, even if he hates that son's mother and likes another wife more; and states that the king shall not have too many wives.

The Torah may distinguish concubines and "sub-standard" wives with the prefix "to" (e.g., lit. "took to wives"). Despite these nuances to the biblical perspective on polygamy, many important figures had more than one wife, such as in the instances of Esau (Gen 26:34; 28:6-9), Jacob (Gen 29:15-28), Elkanah (1 Samuel 1:1-8), David (1 Samuel 25:39-44; 2 Samuel 3:2-5; 5:13-16), and Solomon (1 Kings 11:1-3).

Multiple marriage was considered a realistic alternative in the case of famine, widowhood, or female infertility like in the practice of levirate marriage, wherein a man was required to marry and support his deceased brother's widow, as mandated by . Despite its prevalence in the Hebrew Bible, scholars do not believe that polygyny was commonly practiced in the biblical era because it required a significant amount of wealth. Michael Coogan, in contrast, states that "Polygyny continued to be practiced well into the biblical period, and it is attested among Jews as late as the second century CE".

The monogamy of the Roman Empire was the cause of two explanatory notes in the writings of Josephus describing how the polygamous marriages of Herod the Great were permitted under Jewish custom.
The rabbinical era that began with the destruction of the second temple in Jerusalem in 70 CE saw a continuation of some degree of legal acceptance for polygamy. In the Babylonian Talmud (BT), Kiddushin 7a, its states, "Raba said: 'Be thou betrothed to half of me,' she is betrothed: 'half of thee be betrothed to me,' she is not betrothed." The BT during a discussion of Levirate marriage in Yevamot 65a appears to repeat the precedent found in Exodus 21:10: "Raba said: a man may marry wives in addition to the first wife; provided only that he possesses the means to maintain them". The Jewish Codices began a process of restricting polygamy in Judaism.

Maimonides, in his Mishneh Torah maintained that polygamous unions were permissible from a legal point of view, which was contrary to his personal opinion. The Mishneh Torah, while maintaining the right to multiple spouses, and the requirement to provide fully for each as indicated in previously cited sources, went further: "He may not, however, compel his wives to live in the same courtyard. Instead, each one is entitled to her own household".

The Shulchan Aruch builds on all of the previous works by adding further nuances: "...but in any event, our sages have advised well not to marry more than four wives, in order that he can meet their conjugal needs at least once a month. And in a place where it is customary to marry only one wife, he is not permitted to take another wife on top of his present wife." As can be seen, while the tradition of the rabbinic period began with providing legal definition for the practice of polygamy (although this does not indicate the frequency with which polygamy in fact occurred) that corresponded to precedents in the Tanakh, by the time of the Codices the rabbis had greatly reduced or eliminated sanction of the practice.

Most notable in the rabbinic period on the issue of polygamy, though more specifically for Ashkenazi Jews, was the synod of Rabbeinu Gershom. About 1000 CE he called a synod which decided the following particulars: (1) prohibition of polygamy; (2) necessity of obtaining the consent of both parties to a divorce; (3) modification of the rules concerning those who became apostates under compulsion; (4) prohibition against opening correspondence addressed to another. Some sephardic Jews such as Abraham David Taroç, where known to have several wives.

In the modern day, polygamy is almost nonexistent in Rabbinic Judaism. Ashkenazi Jews have continued to follow Rabbenu Gershom's ban since the 11th century. Some Mizrahi Jewish communities (particularly Yemenite Jews and Persian Jews) discontinued polygyny more recently, after they immigrated to countries where it was forbidden or illegal. Israel prohibits polygamy by law. In practice, however, the law is loosely enforced, primarily to avoid interference with Bedouin culture, where polygyny is practiced. Pre-existing polygynous unions among Jews from Arab countries (or other countries where the practice was not prohibited by their tradition and was not illegal) are not subject to this Israeli law. But Mizrahi Jews are not permitted to enter into new polygamous marriages in Israel. However polygamy may still occur in non-European Jewish communities that exist in countries where it is not forbidden, such as Jewish communities in Iran and Morocco.

Among Karaite Jews, who do not adhere to rabbinic interpretations of the Torah, polygamy is almost non-existent today. Like other Jews, Karaites interpret to mean that a man can only take a second wife if his first wife gives her consent (Keter Torah on Leviticus, pp. 96–97) and Karaites interpret Exodus 21:10 to mean that a man can only take a second wife if he is capable of maintaining the same level of marital duties due to his first wife; the marital duties are 1) food, 2) clothing, and 3) sexual gratification. Because of these two biblical limitations and because most countries outlaw it, polygamy is considered highly impractical, and there are only a few known cases of it among Karaite Jews today.

Israel has made polygamy illegal. Provisions were instituted to allow for existing polygamous families immigrating from countries where the practice was legal. Furthermore, former chief rabbi Ovadia Yosef has come out in favor of legalizing polygamy and the practice of pilegesh (concubine) by the Israeli government.

Tzvi Zohar, a professor from the Bar-Ilan University, recently suggested that based on the opinions of leading halachic authorities, the concept of concubines may serve as a practical Halachic justification for premarital or non-marital cohabitation.

In 2000, the United Nations Human Rights Committee reported that polygamy violates the International Covenant on Civil and Political Rights (ICCPR), citing concerns that the lack of "equality of treatment with regard to the right to marry" meant that polygamy, restricted to polygyny in practice, violates the dignity of women and should be outlawed. Specifically, reports to UN Committees have noted violations of ICCPR due to these inequalities and reports to the UN General Assembly have recommended it be outlawed.

ICCPR does not apply to countries that have not signed it, which includes many Muslim countries such as Saudi Arabia, United Arab Emirates, Qatar, Malaysia, Brunei, Oman, and South Sudan.

Canada has taken a strong stand against polygamy, and the Canadian Department of Justice has argued that polygyny is a violation of International Human Rights Law, as a form of gender discrimination. In Canada, the federal Criminal Code applies throughout the country. It extends the definition of polygamy to having any kind of conjugal union with more than one person at the same time. Also anyone who assists, celebrates, or is a part to a rite, ceremony, or contract that sanctions a polygamist relationship is guilty of polygamy. Polygamy is an offence punishable by up to five years in prison. In 2017, two Canadian religious leaders were found guilty of practicing polygamy by the Supreme Court of British Columbia. Both of them are former bishops of the Mormon denomination of the Fundamentalist Church of Jesus Christ of Latter-Day Saints (FLDS).

Polygamous marriages are not recognized in Russia. The Family Code of Russia states that a marriage can only be contracted between a man and a woman, neither of whom is married to someone else. Furthermore, Russia does not recognize polygamous marriages that had been contracted in other countries. However, "de facto" polygamy or multiple cohabitation in and of itself is not a crime. Due to the shortage of men in Russia's population, it is not uncommon for men to father children with multiple women, and sometimes that results in households that are openly "de facto" polygamous.

Bigamy is illegal in the United Kingdom. "De facto" polygamy (having multiple partners at the same time) is not a criminal offence, provided the person does not register more than one marriage at the same time. In the UK, adultery is not a criminal offence (it is only a ground for divorce). In a written answer to the House of Commons, "In Great Britain, polygamy is only recognized as valid in law in circumstances where the marriage ceremony has been performed in a country whose laws permit polygamy and the parties to the marriage were domiciled there at the time. In addition, immigration rules have generally prevented the formation of polygamous households in this country since 1988."

The 2010 Government in the UK decided that Universal Credit (UC), which replaces means-tested benefits and tax credits for working-age people and will not be completely introduced until 2021, will not recognize polygamous marriages. A House of Commons Briefing Paper states "Treating second and subsequent partners in polygamous relationships as separate claimants could in some situations mean that polygamous households receive more under Universal Credit than they do under the current rules for means-tested benefits and tax credits. This is because, as explained above, the amounts which may be paid in respect of additional spouses are lower than those which generally apply to single claimants." There is currently no official statistics data on cohabiting polygamous couples who have arranged marriage in religious ceremonies.

In October 2017 there was media attention in the UK concerning website over a dating website offering Muslim men an opportunity to seek second or third wives. The website had 100 000 users of which 25 000 were in the UK. Website founder Azad Chaiwala created the website when he was seeking a second wife for himself.

Polygamy is illegal in 49 United States. Utah is the only state where it is a civil infraction rather than a crime as of 2020. Federal legislation to outlaw the practice was endorsed as constitutional in 1878 by the Supreme Court in "Reynolds v. United States," despite the religious objections of The Church of Jesus Christ of Latter-day Saints (Mormons).

On 13 December 2013, a federal judge, spurred by the American Civil Liberties Union and other groups, struck down the parts of Utah's bigamy law that criminalized cohabitation, while also acknowledging that the state may still enforce bans on having multiple marriage licenses.

Individualist feminism and advocates such as Wendy McElroy and journalist Jillian Keenan support the freedom for adults to voluntarily enter polygamous marriages.

Authors such as Alyssa Rower and Samantha Slark argue that there is a case for legalizing polygamy on the basis of regulation and monitoring of the practice, legally protecting the polygamous partners and allowing them to join mainstream society instead of forcing them to hide from it when any public situation arises.

In an October 2004 op-ed for "USA Today", George Washington University law professor Jonathan Turley argued that, as a simple matter of equal treatment under law, polygamy ought to be legal. Acknowledging that underage girls are sometimes coerced into polygamous marriages, Turley replied that "banning polygamy is no more a solution to child abuse than banning marriage would be a solution to spousal abuse".

Stanley Kurtz, a conservative fellow at the Hudson Institute, rejects the decriminalization and legalization of polygamy. He stated:

In January 2015, Pastor Neil Patrick Carrick of Detroit, Michigan, brought a case ("Carrick v. Snyder") against the State of Michigan that the state's ban of polygamy violates the Free Exercise and Equal Protection Clause of the U.S. Constitution.





</doc>
<doc id="24478" url="https://en.wikipedia.org/wiki?curid=24478" title="Postscript">
Postscript

A postscript (P.S.) is an afterthought, thought that's occurring after the letter has been written and signed. The term comes from the Latin "post scriptum", an expression meaning "written after" (which may be interpreted in the sense of "that which comes after the writing").
A postscript may be a sentence, a paragraph, or occasionally many paragraphs added, often hastily and incidentally, after the signature of a letter or (sometimes) the main body of an essay or book. In a book or essay, a more carefully composed addition (e.g., for a second edition) is called an afterword. The word "postscript" has, poetically, been used to refer to any sort of addendum to some main work, even if it is not attached to a main work, as in Søren Kierkegaard's book titled "Concluding Unscientific Postscript." 

Sometimes, when additional points are made after the first postscript, abbreviations such as PSS ("post-super-scriptum"), PPS ("postquam-post-scriptum" or "post-post-scriptum") and PPPS ("post-post-post-scriptum"), and so on, "ad infinitum" are used, though only PPS has somewhat common usage.



</doc>
<doc id="24479" url="https://en.wikipedia.org/wiki?curid=24479" title="Penectomy">
Penectomy

Penectomy is penis removal through surgery, generally for medical or personal reasons.

Cancer, for example, sometimes necessitates removal of part or all of the penis. The amount of penis removed depends on the severity of the cancer. Some men have only the tip of their penis removed. For others with more advanced cancer, the entire penis must be removed.

In rare instances, botched circumcisions have also resulted in full or partial penectomies, as with David Reimer.

Fournier gangrene can also be a reason for penectomy and/or orchiectomy.

Because of the rarity of cancers which require the partial or total removal of the penis, support from people who have had the penis removed can be difficult to find locally. Website support networks are available. For instance, the American Cancer Society's Cancer Survivors Network website provides information for finding support networks. Phalloplasty is also an option for surgical reconstruction of a penis.

Patients that have undergone a partial penectomy as a result of a penile cancer diagnosis have reported similar sexual outcomes as prior to surgery. Sexual support therapists and specialists are available nationally in the United States and can be accessed through the specialist cancer services. Many surgeons or hospitals will also provide this information post operatively. Local government health services departments may be able to provide advice, names, and contact numbers.

Genital surgical procedures for trans women undergoing genital reconstruction surgery do not usually involve the complete removal of the penis. Instead, part or all of the glans is usually kept and reshaped as a clitoris, while the skin of the penile shaft may also be inverted to form the vagina (some more recently developed procedures, such as that used by Dr. Suporn Watanyusakul use the scrotum to form the vaginal walls, and the skin of the penile shaft to form the labia majora). When procedures such as this are not possible, other procedures such as colovaginoplasty are used which may involve the removal of the penis. Some trans women have undergone penectomies, however this is much rarer.

Issues related to the removal of the penis appear in psychology, for example in the condition known as castration anxiety, which happens as a result of a man having anxiety as to whether he may at some point become castrated.

People who are third gender will sometimes want an emasculation by choosing to have their penis, testicles, or both removed.

Male members in the sect of skoptsy (Russian: скопцы, "castrated") were required to become castrated, either only the testicles ("lesser seal") or also the penis ("greater seal").



</doc>
<doc id="24481" url="https://en.wikipedia.org/wiki?curid=24481" title="Provirus">
Provirus

A provirus is a virus genome that is integrated into the DNA of a host cell. In the case of bacterial viruses (bacteriophages), proviruses are often referred to as prophages. However, it is important to note that proviruses are distinctly different from prophages and these terms should not be used interchangeably. Unlike prophages, proviruses do not excise themselves from the host genome when the host cell is stressed.

This state can be a stage of virus replication, or a state that persists over longer periods of time as either inactive viral infections or an endogenous viral element. In inactive viral infections the virus will not replicate itself except through replication of its host cell. This state can last over many host cell generations.

Endogenous retroviruses are always in the state of a provirus. When a (nonendogenous) retrovirus invades a cell, the RNA of the retrovirus is reverse-transcribed into DNA by reverse transcriptase, then inserted into the host genome by an integrase.

A provirus does not directly make new DNA copies of itself while integrated into a host genome in this way. Instead, it is passively replicated along with the host genome and passed on to the original cell's offspring; all descendants of the infected cell will also bear proviruses in their genomes. This is known as lysogenic viral reproduction. Integration can result in a latent infection or a productive infection. In a productive infection, the provirus is transcribed into messenger RNA which directly produces new virus, which in turn will infect other cells via the lytic cycle. A latent infection results when the provirus is transcriptionally silent rather than active.

A latent infection may become productive in response to changes in the host's environmental conditions or health; the provirus may be activated and begin transcription of its viral genome. This can result in the destruction of its host cell because the cell's protein synthesis machinery is hijacked to produce more viruses.

Proviruses may account for approximately 8% of the human genome in the form of inherited endogenous retroviruses.

A provirus not only refers to a retrovirus but is also used to describe other viruses that can integrate into the host chromosomes, another example being adeno-associated virus. 
Not only eukaryotic viruses integrate into the genomes of their hosts; many bacterial and archaeal viruses also employ this strategy of propagation. All families of bacterial viruses with circular (single-stranded or double-stranded) DNA genomes or replicating their genomes through a circular intermediate (e.g., tailed dsDNA viruses) have temperate members.



</doc>
<doc id="24484" url="https://en.wikipedia.org/wiki?curid=24484" title="Parade">
Parade

A parade is a procession of people, usually organized along a street, often in costume, and often accompanied by marching bands, floats, or sometimes large balloons. Parades are held for a wide range of reasons, but are usually celebrations of some kind. In British English, the term 'parade' is usually reserved for either military parades or other occasions where participants march in formation; for celebratory occasions, the word procession is more usual. In the Canadian Forces, the term also has several less formal connotations.

Protest demonstrations can also take the form of a parade, but such cases are usually referred to as a march instead.

The parade float got its name because the first floats were decorated barges that were towed along canals with ropes held by parade marchers on the shore. Floats were occasionally propelled from within by concealed oarsmen, but the practice was abandoned because of the high incidence of drowning when the lightweight and unstable frames capsized. Strikingly, among the first uses of grounded floats — towed by horses — was a ceremony in memory of recently drowned parade oarsmen. Today, parade floats are traditionally pulled by motor vehicles or are powered themselves.

Multiple grand marshals may often be designated for an iteration of the parade, and may or may not be in actual attendance due to circumstances (including death). A "community grand marshal" or other designations may be selected alongside a "grand marshal" to lead the front or other parts of the parade.

Since the advent of such technology, it became possible for aircraft and boats to parade. A flypast is an aerial parade of anything from one to dozens of aircraft, both in commercial context at airshows and also to mark, e.g., national days or significant anniversaries. They are particularly common in the United Kingdom, where they are often associated with Royal occasions. Similarly, for ships, there may be a sail-past of, e.g., tall ships (as was seen during Trafalgar 200) or other sailing vessels as during the celebrations of the 60th anniversary of World War II.
The longest parade in the world is the Hanover Schützenfest that takes place in Hanover every year during the Schützenfest. The parade is long with more than 12,000 participants from all over the world, among them more than 100 bands and around 70 floats and carriages.



At the end of hostilities in Europe in 1944-45, "victory parades" were a common feature throughout the recently liberated territories. For example, on 3 September 1944, the personnel of the 2nd Canadian Infantry Division marched six abreast to the music of massed regimental pipe and drum bands through the streets of Dieppe, France to commemorate the liberation of the city from German occupation, as well as commemorate the loss of over 900 soldiers from that formation during the Dieppe Raid two years earlier. On the Moscow Victory Parade of 1945 held in Moscow, Soviet Union in June 1945, the Red Army commemorated Victory in Europe with a parade and the ceremonial destruction of captured Wehrmacht and Waffen-SS standards.





</doc>
<doc id="24485" url="https://en.wikipedia.org/wiki?curid=24485" title="Priority queue">
Priority queue

In computer science, a priority queue is an abstract data type similar to regular queue or stack data structure in which each element additionally has a "priority" associated with it. In a priority queue, an element with high priority is served before an element with low priority. In some implementations, if two elements have the same priority, they are served according to the order in which they were enqueued, while in other implementations, ordering of elements with the same priority is undefined.

While priority queues are often implemented with heaps, they are conceptually distinct from heaps. A priority queue is a concept like "a list" or "a map"; just as a list can be implemented with a linked list or an array, a priority queue can be implemented with a heap or a variety of other methods such as an unordered array.

A priority queue must at least support the following operations:


In addition, "peek" (in this context often called "find-max" or "find-min"), which returns the highest-priority element but does not modify the queue, is very frequently implemented, and nearly always executes in "O"(1) time. This operation and its "O"(1) performance is crucial to many applications of priority queues.

More advanced implementations may support more complicated operations, such as "pull_lowest_priority_element", inspecting the first few highest- or lowest-priority elements, clearing the queue, clearing subsets of the queue, performing a batch insert, merging two or more queues into one, incrementing priority of any element, etc.

One can imagine a priority queue as a modified queue, but when one would get the next element off the queue, the highest-priority element is retrieved first.

Stacks and queues may be modeled as particular kinds of priority queues. As a reminder, here is how stacks and queues behave:


In a stack, the priority of each inserted element is monotonically increasing; thus, the last element inserted is always the first retrieved. In a queue, the priority of each inserted element is monotonically decreasing; thus, the first element inserted is always the first retrieved.

There are a variety of simple, usually inefficient, ways to implement a priority queue. They provide an analogy to help one understand what a priority queue is.

For instance, one can keep all the elements in an unsorted list ( "O"(1) insertion time ). Whenever the highest-priority element is requested, search through all elements for the one with the highest priority. ("O"("n") pull time),

In another case, one can keep all the elements in priority sorted list ("O"(n) insertion sort time ), whenever the highest-priority element is requested, the first one int the list can be returned. ( "O"(1) pull time )

To improve performance, priority queues typically use a heap as their backbone, giving "O"(log "n") performance for inserts and removals, and "O"("n") to build initially from a set of "n" elements. Variants of the basic heap data structure such as pairing heaps or Fibonacci heaps can provide better bounds for some operations.

Alternatively, when a self-balancing binary search tree is used, insertion and removal also take "O"(log "n") time, although building trees from existing sequences of elements takes "O"("n" log "n") time; this is typical where one might already have access to these data structures, such as with third-party or standard libraries.

From a computational-complexity standpoint, priority queues are congruent to sorting algorithms. The section on the equivalence of priority queues and sorting algorithms, below, describes how efficient sorting algorithms can create efficient priority queues.

There are several specialized heap data structures that either supply additional operations or outperform heap-based implementations for specific types of keys, specifically integer keys. Suppose the set of possible keys is {1, 2, ..., C}.


For applications that do many "peek" operations for every "extract-min" operation, the time complexity for peek actions can be reduced to "O"(1) in all tree and heap implementations by caching the highest priority element after every insertion and removal. For insertion, this adds at most a constant cost, since the newly inserted element is compared only to the previously cached minimum element. For deletion, this at most adds an additional "peek" cost, which is typically cheaper than the deletion cost, so overall time complexity is not significantly impacted.

Monotone priority queues are specialized queues that are optimized for the case where no item is ever inserted that has a lower priority (in the case of min-heap) than any item previously extracted. This restriction is met by several practical applications of priority queues.

The semantics of priority queues naturally suggest a sorting method: insert all the elements to be sorted into a priority queue, and sequentially remove them; they will come out in sorted order. This is actually the procedure used by several sorting algorithms, once the layer of abstraction provided by the priority queue is removed. This sorting method is equivalent to the following sorting algorithms:

A sorting algorithm can also be used to implement a priority queue. Specifically, Thorup says:
We present a general deterministic linear space reduction from priority queues to sorting implying that if we can sort up to "n" keys in "S"("n") time per key, then there is a priority queue supporting "delete" and "insert" in "O"("S"("n")) time and "find-min" in constant time.
That is, if there is a sorting algorithm which can sort in "O"("S") time per key, where "S" is some function of "n" and word size, then one can use the given procedure to create a priority queue where pulling the highest-priority element is "O"(1) time, and inserting new elements (and deleting elements) is "O"("S") time. For example, if one has an "O"("n" log "n") sort algorithm, one can create a priority queue with "O"(1) pulling and "O"(n log "n") insertion.

A priority queue is often considered to be a "container data structure".

The Standard Template Library (STL), and the C++ 1998 standard, specifies codice_1 as one of the STL container adaptor class templates. However, it does not specify how two elements with same priority should be served, and indeed, common implementations will not return them according to their order in the queue. It implements a max-priority-queue, and has three parameters: a comparison object for sorting such as a function object (defaults to less<T> if unspecified), the underlying container for storing the data structures (defaults to std::vector<T>), and two iterators to the beginning and end of a sequence. Unlike actual STL containers, it does not allow iteration of its elements (it strictly adheres to its abstract data type definition). STL also has utility functions for manipulating another random-access container as a binary max-heap. The Boost libraries also have an implementation in the library heap.

Python's heapq module implements a binary min-heap on top of a list.

Java's library contains a class, which implements a min-priority-queue.

Scala's library contains a PriorityQueue class, which implements a max-priority-queue.

Go's library contains a container/heap module, which implements a min-heap on top of any compatible data structure.

The Standard PHP Library extension contains the class SplPriorityQueue.

Apple's Core Foundation framework contains a CFBinaryHeap structure, which implements a min-heap.

Priority queuing can be used to manage limited resources such as bandwidth on a transmission line from a network router. In the event of outgoing traffic queuing due to insufficient bandwidth, all other queues can be halted to send the traffic from the highest priority queue upon arrival. This ensures that the prioritized traffic (such as real-time traffic, e.g. an RTP stream of a VoIP connection) is forwarded with the least delay and the least likelihood of being rejected due to a queue reaching its maximum capacity. All other traffic can be handled when the highest priority queue is empty. Another approach used is to send disproportionately more traffic from higher priority queues.

Many modern protocols for local area networks also include the concept of priority queues at the media access control (MAC) sub-layer to ensure that high-priority applications (such as VoIP or IPTV) experience lower latency than other applications which can be served with best effort service. Examples include IEEE 802.11e (an amendment to IEEE 802.11 which provides quality of service) and ITU-T G.hn (a standard for high-speed local area network using existing home wiring (power lines, phone lines and coaxial cables).

Usually a limitation (policer) is set to limit the bandwidth that traffic from the highest priority queue can take, in order to prevent high priority packets from choking off all other traffic. This limit is usually never reached due to high level control instances such as the Cisco Callmanager, which can be programmed to inhibit calls which would exceed the programmed bandwidth limit.

Another use of a priority queue is to manage the events in a discrete event simulation. The events are added to the queue with their simulation time used as the priority. The execution of the simulation proceeds by repeatedly pulling the top of the queue and executing the event thereon.

"See also": Scheduling (computing), queueing theory

When the graph is stored in the form of adjacency list or matrix, priority queue can be used to extract minimum efficiently when implementing Dijkstra's algorithm, although one also needs the ability to alter the priority of a particular vertex in the priority queue efficiently.

Huffman coding requires one to repeatedly obtain the two lowest-frequency trees. A priority queue is one method of doing this.

Best-first search algorithms, like the A* search algorithm, find the shortest path between two vertices or nodes of a weighted graph, trying out the most promising routes first. A priority queue (also known as the "fringe") is used to keep track of unexplored routes; the one for which the estimate (a lower bound in the case of A*) of the total path length is smallest is given highest priority. If memory limitations make best-first search impractical, variants like the SMA* algorithm can be used instead, with a double-ended priority queue to allow removal of low-priority items.

The Real-time Optimally Adapting Meshes (ROAM) algorithm computes a dynamically changing triangulation of a terrain. It works by splitting triangles where more detail is needed and merging them where less detail is needed. The algorithm assigns each triangle in the terrain a priority, usually related to the error decrease if that triangle would be split. The algorithm uses two priority queues, one for triangles that can be split and another for triangles that can be merged. In each step the triangle from the split queue with the highest priority is split, or the triangle from the merge queue with the lowest priority is merged with its neighbours.

Using min heap priority queue in Prim's algorithm to find the minimum spanning tree of a connected and undirected graph, one can achieve a good running time. This min heap priority queue uses the min heap data structure which supports operations such as "insert", "minimum", "extract-min", "decrease-key". In this implementation, the weight of the edges is used to decide the priority of the vertices. Lower the weight, higher the priority and higher the weight, lower the priority.

Parallelization can be used to speed up priority queues, but requires some changes to the priority queue interface. The reason for such changes is that a sequential update usually only has formula_2 or formula_3 cost, and there is no practical gain to parallelize such an operation. One possible change is to allow the concurrent access of multiple processors to the same priority queue. The second possible change is to allow batch operations that work on formula_4 elements, instead of just one element. For example, "extractMin" will remove the first formula_4 elements with the highest priority.

If the priority queue allows concurrent access, multiple processes can perform operations concurrently on that priority queue. However, this raises two issues. First of all, the definition of the semantics of the individual operations is no longer obvious. For example, if two processes want to extract the element with the highest priority, should they get the same element or different ones? This restricts parallelism on the level of the program using the priority queue. In addition, because multiple processes have access to the same element, this leads to contention.
The concurrent access to a priority queue can be implemented on a Concurrent Read, Concurrent Write (CRCW) PRAM model. In the following the priority queue is implemented as a skip list. In addition, an atomic synchronization primitive, CAS, is used to make the skip list lock-free. The nodes of the skip list consists of a unique key, a priority, an array of pointers, for each level, to the next nodes and a "delete" mark. The "delete" mark marks if the node is about to be deleted by a process. This ensures that other processes can react to the deletion appropriately.

If the concurrent access to a priority queue is allowed, conflicts may arise between two processes. For example, a conflict arises if one process is trying to insert a new node, but at the same time another process is about to delete the predecessor of that node. There is a risk that the new node is added to the skip list, yet it is not longer reachable. ()

In this setting, operations on a priority queue is generalized to a batch of formula_4 elements.
For instance, "k_extract-min" deletes the formula_4 smallest elements of the priority queue and returns those. 

In a shared-memory setting, the parallel priority queue can be easily implemented using parallel binary search trees and join-based tree algorithms. In particular, "k_extract-min" corresponds to a "split" on the binary search tree that has formula_3 cost and yields a tree that contains the formula_4 smallest elements. "k_insert" can be applied by a "union" of the original priority queue and the batch of insertions. If the batch is already sorted by the key, "k_insert" has formula_10 cost. Otherwise, we need to first sort the batch, so the cost will be formula_11. Other operations for priority queue can be applied similarly. For instance, "k_decrease-key" can be done by first applying "difference" and then "union", which first deletes the elements and then inserts them back with the updated keys. All these operations are highly parallel, and the theoretical and practical efficiency can be found in related research papers.

The rest of this section discusses a queue-based algorithm on distributed memory. We assume each processor has its own local memory and a local (sequential) priority queue. The elements of the global (parallel) priority queue are distributed across all processors.

A "k_insert" operation assigns the elements uniformly random to the processors which insert the elements into their local queues. Note that single elements can still be inserted into the queue. Using this strategy the global smallest elements are in the union of the local smallest elements of every processor with high probability. Thus each processor holds a representative part of the global priority queue.

This property is used when "k_extract-min" is executed, as the smallest formula_12 elements of each local queue are removed and collected in a result set. The elements in the result set are still associated with their original processor. The number of elements formula_12 that is removed from each local queue depends on formula_4 and the number of processors formula_15. 

By parallel selection the formula_4 smallest elements of the result set are determined. With high probability these are the global formula_4 smallest elements. If not, formula_12 elements are again removed from each local queue and put into the result set. This is done until the global formula_4 smallest elements are in the result set. Now these formula_4 elements can be returned. All other elements of the result set are inserted back into their local queues. The running time of "k_extract-min" is expected formula_21, where formula_22 and formula_23
is the size of the priority queue.

The priority queue can be further improved by not moving the remaining elements of the result set directly back into the local queues after a "k_extract-min" operation. This saves moving elements back and forth all the time between the result set and the local queues.

By removing several elements at once a considerable speedup can be reached. But not all algorithms can use this kind of priority queue. Dijkstra's algorithm for example can not work on several nodes at once. The algorithm takes the node with the smallest distance from the priority queue and calculates new distances for all its neighbor nodes. If you would take out formula_4 nodes, working at one node could change the distance of another one of the formula_4 nodes. So using k-element operations destroys the label setting property of Dijkstra's algorithm.




</doc>
<doc id="24487" url="https://en.wikipedia.org/wiki?curid=24487" title="Pāramitā">
Pāramitā

Pāramitā (Sanskrit, Pali) or pāramī (Pāli), is a Buddhist term often translated as "perfection". It is described in Buddhist commentaries as noble character qualities generally associated with enlightened beings. "Pāramī" and "pāramitā" are both terms in Pali but Pali literature makes greater reference to "pāramī," while Mahayana texts generally use the Sanskrit "pāramitā."

Donald S. Lopez, Jr. describes the etymology of the term:
Theravada teachings on the "pāramīs" can be found in late canonical books and post-canonical commentaries. Theravada commentator Dhammapala describes them as noble qualities usually associated with bodhisattvas. American scholar monk Thanissaro Bhikkhu, describes them as perfections "(paramī)" of character necessary to achieve enlightenment as one of the three enlightened beings, a "samma sambuddha" a "pacceka-buddha" or an "arahant".

In the Pāli Canon, the "Buddhavaṃsa" of the "Khuddaka Nikāya" lists the ten perfections ("dasa pāramiyo") as:


Two of the above virtues, mettā and "upekkhā", also are brahmavihāras.

The Theravādin teachings on the pāramīs can be found in canonical books ("Jataka tales", "Apadāna", "Buddhavaṃsa", "Cariyāpiṭaka") and post-canonical commentaries written to supplement the Pāli Canon at a later time, and thus might not be an original part of the Theravādin teachings. The oldest parts of the "Sutta Piṭaka" (for example, "Majjhima Nikāya", "Digha Nikāya", "Saṃyutta Nikāya" and the "Aṅguttara Nikāya") do not have any mention of the pāramīs as a category (though they are all mentioned individually).

Some scholars even refer to the teachings of the pāramīs as a semi-Mahāyāna teaching added to the scriptures at a later time in order to appeal to the interests and needs of the lay community and to popularize their religion. However, these views rely on the early scholarly presumption of Mahāyāna originating with religious devotion and appeal to laity. More recently, scholars have started to open up early Mahāyāna literature, which is very ascetic and expounds the ideal of the monk's life in the forest. Therefore, the practice of the pāramitās in Mahāyāna Buddhism may have been close to the ideals of the ascetic tradition of the śramaṇa.

Bhikkhu Bodhi maintains that, in the earliest Buddhist texts (which he identifies as the first four "nikāyas"), those seeking the extinction of suffering ("nibbana") pursued the noble eightfold path. As time went on, a backstory was provided for the multi-life development of the Buddha; as a result, the ten perfections were identified as part of the path for the bodhisattva (Pāli: "bodhisatta"). Over subsequent centuries, the "pāramīs" were seen as being significant for aspirants to both Buddhahood and arahantship. Bhikkhu Bodhi summarizes:

Religious studies scholar Dale S. Wright states that Mahāyāna texts refer to the "pāramitās" as "bases of training" for those looking to achieve enlightenment. Wright describes the Buddhist "pāramitās" as a set of character ideals that guide self-cultivation and provide a concrete image of the Buddhist ideal.

The Prajñapāramitā sūtras (प्रज्ञापारमिता सूत्र), and a large number of other Mahāyāna texts list six perfections:


This list is also mentioned by the Theravāda commentator Dhammapala, who describes it as a categorization of the same ten perfections of Theravada Buddhism. According to Dhammapala, "Sacca" is classified as both "Śīla" and "Prajñā", "Mettā" and "Upekkhā" are classified as "Dhyāna", and "Adhiṭṭhāna" falls under all six. Bhikkhu Bodhi states that the correlations between the two sets shows there was a shared core before the Theravada and Mahayana schools split.

In the "Ten Stages Sutra", four more pāramitās are listed:

The "Mahāratnakūṭa Sūtra" ("महारत्नकूट सूत्र, the Sutra of the Heap of Jewels") also includes these additional four pāramitās with number 8 and 9 switched.

According to the perspective of Tibetan Buddhism, Mahāyāna practitioners have the choice of two practice paths: the path of perfection (Sanskrit: "pāramitāyāna") or the path of tantra (Sanskrit: "tantrayāna"), which is the Vajrayāna.

Traleg Kyabgon Rinpoche renders "pāramitā" into English as "transcendent action" and then frames and qualifies it:
The pure illusory body is said to be endowed with the six perfections (Sanskrit: "ṣatpāramitā").

The first four perfections are skillful means practice while the last two are wisdom practice. These contain all the methods and skills required for eliminating delusion and fulfilling other's needs. Also, leading from happy to happier states.





</doc>
<doc id="24489" url="https://en.wikipedia.org/wiki?curid=24489" title="Outline of physics">
Outline of physics

The following outline is provided as an overview of and topical guide to physics:

Physics – natural science that involves the study of matter and its motion through spacetime, along with related concepts such as energy and force. More broadly, it is the general analysis of nature, conducted in order to understand how the universe behaves.

Physics can be described as all of the following:



History of physics – history of the physical science that studies matter and its motion through space-time, and related concepts such as energy and force

Physics – branch of science that studies matter and its motion through space and time, along with related concepts such as energy and force. Physics is one of the "fundamental sciences" because the other natural sciences (like biology, geology etc.) deal with systems that seem to obey the laws of physics. According to physics, the physical laws of matter, energy and the fundamental forces of nature govern the interactions between particles and physical entities (such as planets, molecules, atoms or the subatomic particles). Some of the basic pursuits of physics, which include some of the most prominent developments in modern science in the last millennium, include:

Gravity, light, physical system, physical observation, physical quantity, physical state, physical unit, physical theory, physical experiment

Theoretical concepts
Mass–energy equivalence, particle, physical field, physical interaction, physical law, fundamental force, physical constant, wave



Physics
This is a list of the primary theories in physics, major subtopics, and concepts.

Index of physics articles





</doc>
<doc id="24490" url="https://en.wikipedia.org/wiki?curid=24490" title="Outline of parapsychology">
Outline of parapsychology

Parapsychology is a field of research that studies a number of ostensible paranormal phenomena, including telepathy, precognition, clairvoyance, psychokinesis, near-death experiences, reincarnation, and apparitional experiences.









</doc>
<doc id="24493" url="https://en.wikipedia.org/wiki?curid=24493" title="Outline of public affairs">
Outline of public affairs

The following outline is provided as an overview of and topical guide to public affairs:

Public affairs – catch-all term that includes public policy as well as public administration, both of which are closely related to and draw upon the fields of political science and economics.








</doc>
<doc id="24494" url="https://en.wikipedia.org/wiki?curid=24494" title="Index of painting-related articles">
Index of painting-related articles

This is an alphabetical index of articles related to the painting.








</doc>
<doc id="24497" url="https://en.wikipedia.org/wiki?curid=24497" title="Prakrit">
Prakrit

The Prakrits (; Early Brahmi , "prākṛta"; Devanagari , ; ; ; Kannada: "pāgada") are a group of vernacular Middle Indo-Aryan languages used in India from around the 3rd century BCE to the 8th century CE. The term Prakrit is usually applied to the middle period of Middle Indo-Aryan languages, excluding earlier inscriptions and the later Pali. The Prakrits were used contemporaneously with the prestigious Classical Sanskrit of higher social classes. "Prākṛta" literally means "natural", as opposed to "saṃskṛta", which literally means "constructed" or "refined".

According to the "Prākrṭa Prakāśa", an ancient Prakrit grammar, "Saṃskṛtam is the prakṛti (source) - and the language that originates in, or comes from, that prakṛti, is therefore called prākṛtam." The same definition is also given by the Prakrit grammarian Acharya Hemachandra in his grammar of Prakrit. The dictionary of Monier Monier-Williams (1819–1899), however, interprets the word in the opposite sense: "the most frequent meanings of the term ', from which the word "prakrit" is derived, are "original, natural, normal" and the term is derived from ', "making or placing before or at first, the original or natural form or condition of anything, original or primary substance". In linguistic terms, this is used in contrast with "", "refined".

Modern scholars have used the term "Prakrit" to refer to two concepts:


Some modern scholars include all Middle Indo-Aryan languages under the rubric of 'Prakrits', while others emphasize the independent development of these languages, often separated from the history of Sanskrit by wide divisions of caste, religion, and geography.

The broadest definition uses the term "Prakrit" to describe any Middle Indo-Aryan language that deviates from Sanskrit in any manner. American scholar Andrew Ollett points out that this unsatisfactory definition makes "Prakrit" a cover term for languages that were not actually called Prakrit in ancient India, such as:


According to some scholars, such as German Indologists Richard Pischel and Oskar von Hinüber, the term "Prakrit" refers to a smaller set of languages that were used exclusively in literature:


According to Sanskrit scholar A. C. Woolner, the Ardhamagadhi (or simply Magadhi) Prakrit, which was used extensively to write the scriptures of Jainism, is often considered to be the definitive form of Prakrit, while others are considered variants of it. Prakrit grammarians would give the full grammar of Ardhamagadhi first, and then define the other grammars with relation to it. For this reason, courses teaching 'Prakrit' are often regarded as teaching Ardhamagadhi.

Medieval grammarians such as Markandeya (late 16th century) describe a highly systematized Prakrit grammar, but the surviving Prakrit texts do not adhere to this grammar. For example, according to Vishvanatha (14th century), in a Sanskrit drama, the characters should speak Maharashtri Prakrit in verse and Shauraseni Prakrit in prose. But the 10th century Sanskrit dramatist Rajashekhara doesn't abide by this rule. Markandeya, as well as later scholars such as Sten Konow find faults with the Prakrit portions of Rajashekhara's writings, but it is not clear if the rule enunciated by Vishvanatha existed during Rajashekhara's time. Rajashekhara's himself imagines Prakrit as a single language or a single kind of language, alongside Sanskrit, Apabhramsha, and Paishachi.

German Indologist Theodor Bloch (1894) dismissed the medieval Prakrit grammarians as unreliable, arguing that they were not qualified to describe the language of the texts composed centuries before them. Other scholars such as Sten Konow, Richard Pischel and Alfred Hillebrandt, disagree with Bloch. It is possible that the grammarians sought to codify only the language of the earliest classics of the Prakrit literature, such as the "Gaha Sattasai". Another explanation is that the extant Prakrit manuscripts contain scribal errors. Most of the surviving Prakrit manuscripts were produced in a variety of regional scripts, during 1300-1800 CE. It appears that the scribes who made these copies from the earlier manuscripts did not have a good command of the original language of the texts, as several of the extant Prakrit texts contain inaccuracies or are incomprehensible.

Prakrita Prakasha, a book attributed to Vararuchi, summarizes various Prakrit languages 

Prakrit literature was produced across a wide area of South Asia, from Kashmir in the north to Tamil Nadu in the south, and from Sindh in the west to Bengal in the east. Outside India, the language was also known in Cambodia and Java.

Prakrit is often wrongly assumed to have been a language (or languages) spoken by the common people, because it is different from Sanskrit, which is the predominant language of the ancient Indian literature. Several modern scholars, such as George Abraham Grierson and Richard Pischel, have asserted that the literary Prakrit does not represent the actual languages spoken by the common people of ancient India. This theory is corroborated by a market scene in Uddyotana's "Kuvalaya-mala" (779 CE), in which the narrator speaks a few words in 18 different languages: some of these languages sound similar to the languages spoken in modern India; but none of them resemble the language that Uddyotana identifies as "Prakrit" and uses for narration throughout the text.

Literary Prakrit was among the main languages of the classical Indian culture. Dandin's "Kavya-darsha" (c. 700) mentions four kinds of literary languages: Sanskrit, Prakrit, Apabhramsha, and mixed. Bhoja's "Sarasvati-Kanthabharana" (11th century) lists Prakrit among the few languages suitable for composition of literature. Mirza Khan's "Tuhfat al-hind" (1676) names Prakrit among the three kinds of literary languages native to India, the other two being Sanskrit and the vernacular languages. It describes Prakrit as a mixture of Sanskrit and vernacular languages, and adds that Prakrit was "mostly employed in the praise of kings, ministers, and chiefs".

During a large period of the first millennium, literary Prakrit was the preferred language for the fictional romance in India. Its use as a language of systematic knowledge was limited, because of Sanskrit's dominance in this area, but nevertheless, Prakrit texts exist on topics such as grammar, lexicography, metrics, alchemy, medicine, divination, and gemology. In addition, the Jains used Prakrit for religious literature, including commentaries on the Jain canonical literature, stories about Jain figures, moral stories, hymns and expositions of Jain doctrine. Prakrit is also the language of some Shaiva "tantras" and Vaishnava hymns.

Besides being the primary language of several texts, Prakrit also features as the language of low-class men and most women in the Sanskrit stage plays. American scholar Andrew Ollett traces the origin of the Sanskrit Kavya to Prakrit poems.

Some of the texts that identify their language as Prakrit include:


Some 19th-20th century European scholars, such as Hermann Jacobi and Ernst Leumann, made a distinction between Jain and non-Jain Prakrit literature. Jacobi used the term "Jain Prakrit" (or "Jain Maharashtri", as he called it) to denote the language of relatively late and relatively more Sanskrit-influenced narrative literature, as opposed to the earlier Prakrit court poetry. Later scholars used the term "Jain Prakrit" for any variety of Prakrit used by Jain authors, including the one used in early texts such as "Tarangavati" and "Vasudeva-Hindi" ("Wanderings of Vasudeva"). However, the works written by Jain authors do not necessarily belong to an exclusively Jain history, and do not show any specific literary features resulting from their belief in Jainism. Therefore, the division of Prakrit literature into Jain and non-Jain categories is no longer considered tenable.

The languages that have been labeled "Prakrit" in modern times include the following:

Not all of these languages were actually called "Prakrit" in the ancient period.

Dramatic Prakrits were those that were devised specifically for use in dramas and other literature. Whenever dialogue was written in a Prakrit, the reader would also be provided with a Sanskrit translation. None of these Prakrits came into being as vernaculars, but some ended up being used as such when Sanskrit fell out of favor.

The phrase "Dramatic Prakrits" often refers to three most prominent of them: Shauraseni, Magadhi Prakrit, and Maharashtri Prakrit. However, there were a slew of other less commonly used Prakrits that also fall into this category. These include Prachya, Bahliki, Dakshinatya, Shakari, Chandali, Shabari, Abhiri, Dramili, and Odri. There was a strict structure to the use of these different Prakrits in dramas. Characters each spoke a different Prakrit based on their role and background; for example, Dramili was the language of "forest-dwellers", Sauraseni was spoken by "the heroine and her female friends", and Avanti was spoken by "cheats and rogues".

Prakrit languages held a lower social status than Sanskrit in ancient India. In the Sanskrit stage plays, such as Kalidasa's "Shakuntala", high-class male characters typically speak Sanskrit, while the low-class male characters and most female characters typically speak Prakrit.

Mirza Khan's "Tuhfat al-hind" (1676) characterizes Prakrit as the language of "the lowest of the low", stating that the language was known as "Patal-bani" ("Language of the underground") or "Nag-bani" ("Language of the snakes").

While Prakrits were originally seen as 'lower' forms of language, the influence they had on Sanskrit - allowing it to be more easily used by the common people - as well as the converse influence of Sanskrit on the Prakrits, gave Prakrits progressively higher cultural cachet.

Among modern scholars, Prakrit literature has received less attention than Sanskrit. Few modern Prakrit texts have survived in modern times, and even fewer have been published or attracted critical scholarship. Prakrit has not been designated as a classical language by the Government of India, although the earliest Prakrit texts are older than literature of most of the languages designated as such. One of the reasons behind this neglect of Prakrit is that it is not tied to a regional, national, ethnic, or religious identity.

In 1955, government of Bihar established at Vaishali, the Research Institute of Prakrit Jainology and Ahimsa with the aim to promote research work in Prakrit.

The National Institute of Prakrit Study and Research is located in Shravanabelagola, Karnataka, India.



</doc>
<doc id="24498" url="https://en.wikipedia.org/wiki?curid=24498" title="Palestrina (disambiguation)">
Palestrina (disambiguation)

Palestrina is a city in Lazio, Italy.

Palestrina may also refer to:


</doc>
<doc id="24501" url="https://en.wikipedia.org/wiki?curid=24501" title="Progressive music">
Progressive music

Progressive music is music that attempts to expand existing stylistic boundaries associated with specific genres of music. The word comes from the basic concept of "progress", which refers to development and growth by accumulation, and is often deployed in the context of distinct genres such as progressive country, progressive folk, progressive jazz, and (most significantly) progressive rock. Music that is deemed "progressive" usually synthesizes influences from various cultural domains, such as European art music, Celtic folk, West Indian, or African. It is rooted in the idea of a cultural alternative and may also be associated with auteur-stars and concept albums, considered traditional structures of the music industry.

As an art theory, the progressive approach falls between formalism and eclecticism. "Formalism" refers to a preoccupation with established external compositional systems, structural unity, and the autonomy of individual art works. Like formalism, "eclecticism" connotates a predilection toward style synthesis or integration. However, contrary to formalist tendencies, eclecticism foregrounds discontinuities between historical and contemporary styles and electronic media, sometimes referring simultaneously to vastly different musical genres, idioms, and cultural codes. In marketing, "progressive" is used to distinguish a product from "commercial" pop music.

Progressive jazz is a form of big band that is more complex or experimental. It originated in the 1940s with arrangers who drew from modernist composers such as Igor Stravinsky and Paul Hindemith. Its "progressive" features were replete with dissonance, atonality, and brash effects. Progressive jazz was most popularized by the bandleader Stan Kenton during the 1940s. Critics were initially wary of the idiom. Dizzy Gillespie wrote in his autobiography; "They tried to make Stan Kenton a 'white hope,' called modern jazz and my music 'progressive,' then tried to tell me I played 'progressive' music. I said, 'You're full of shit!' 'Stan Kenton? There ain't nothing in my music that's cold, cold like his."

Progressive big band is a style of big band or swing music that was made for listening, with denser, more modernist arrangements and more room to improvise. The online music guide AllMusic states that, along with Kenton, musicians like Gil Evans, Toshiko Akiyoshi, Cal Massey, Frank Foster, Carla Bley, George Gruntz, David Amram, Sun Ra, and Duke Ellington were major proponents of the style.

"Progressive rock" is almost synonymous with "art rock"; the latter is more likely to have experimental or avant-garde influences. Although a unidirectional English "progressive" style emerged in the late 1960s, by 1967, progressive rock had come to constitute a diversity of loosely associated style codes. With the arrival of a "progressive" label, the music was dubbed "progressive pop" before it was called "progressive rock". "Progressive" referred to the wide range of attempts to break with the standard pop music formula. A number of additional factors contributed to the label—lyrics were more poetic, technology was harnessed for new sounds, music approached the condition of "art", some harmonic language was imported from jazz and 19th-century classical music, the album format overtook singles, and the studio, rather than the stage, became the focus of musical activity, which often involved creating music for listening, not dancing.

During the mid 1960s, pop music made repeated forays into new sounds, styles, and techniques that inspired public discourse among its listeners. The word "progressive" was frequently used, and it was thought that every song and single was to be a "progression" from the last. In 1966, the degree of social and artistic dialogue among rock musicians dramatically increased for bands such as the Beach Boys, the Beatles, and the Byrds who fused elements of composed (cultivated) music with the oral (vernacular) musical traditions of rock. Rock music started to take itself seriously, paralleling earlier attempts in jazz (as swing gave way to bop, a move which did not succeed with audiences). In this period, the popular song began signaling a new possible means of expression that went beyond the three-minute love song, leading to an intersection between the "underground" and the "establishment" for listening publics. The Beach Boys' leader Brian Wilson is credited for setting a precedent that allowed bands and artists to enter a recording studio and act as their own producers.

The music was developed immediately following a brief period in the mid 1960s where creative authenticity among musical artists and consumer marketing coincided with each other. Before the progressive pop of the late 1960s, performers were typically unable to decide on the artistic content of their music. Assisted by the mid 1960s economic boom, record labels began investing in artists, giving them freedom to experiment, and offering them limited control over their content and marketing. The growing student market serviced record labels with the word "progressive", being adopted as a marketing term to differentiate their product from "commercial" pop. Music critic Simon Reynolds writes that beginning with 1967, a divide would exist between "progressive" pop and "mass/chart" pop, a separation which was "also, broadly, one between boys and girls, middle-class and working-class." Before progressive/art rock became the most commercially successful British sound of the early 1970s, the 1960s psychedelic movement brought together art and commercialism, broaching the question of what it meant to be an artist in a mass medium. Progressive musicians thought that artistic status depended on personal autonomy, and so the strategy of "progressive" rock groups was to present themselves as performers and composers "above" normal pop practice.

"Proto-prog" is a retrospective label for the first wave of progressive rock musicians. The musicians that approached this genre harnessed modern classical and other genres usually outside of traditional rock influences, longer and more complicated compositions, interconnected songs as medley, and studio composition. Progressive rock itself evolved from psychedelic/acid rock music, specifically a strain of classical/symphonic rock led by the Nice, Procol Harum, and the Moody Blues. Critics assumed King Crimson's debut album "In the Court of the Crimson King" (1969) to be the logical extension and development of late 1960s proto-progressive rock exemplified by the Moody Blues, Procol Harum, Pink Floyd, and the Beatles. According to Macan, the album may be the most influential to progressive rock for crystallizing the music of earlier "proto-progressive bands [...] into a distinctive, immediately recognizable style". He distinguishes 1970s "classic" prog from late 1960s proto-prog by the conscious rejection of psychedelic rock elements, which proto-progressive bands continued to incorporate.

"Post-progressive" is a term invented to distinguish a type of rock music from the persistent "progressive rock" style associated with the 1970s. In the mid to late 1970s, progressive music was denigrated for its assumed pretentiousness, specifically the likes of Yes, Genesis, and Emerson, Lake & Palmer. According to musicologist John Covach, "by the early 1980s, progressive rock was thought to be all but dead as a style, an idea reinforced by the fact that some of the principal progressive groups has developed a more commercial sound. [...] What went out of the music of these now ex-progressive groups [...] was any significant evocation of art music." In the opinion of King Crimson's Robert Fripp, "progressive" music was an attitude, not a style. He believed that genuinely "progressive" music pushes stylistic and conceptual boundaries outwards through the appropriation of procedures from classical music or jazz, and that once "progressive rock" ceased to cover new ground – becoming a set of conventions to be repeated and imitated – the genre's premise had ceased to be "progressive". 

A direct reaction to prog came in the form of the punk movement, which rejected classical traditions, virtuosity, and textural complexity. Post-punk, which author Doyle Green characterizes "as a kind of progressive punk", was played by bands like Talking Heads, Pere Ubu, Public Image Ltd, and Joy Division. It differs from punk rock by balancing punk's energy and skepticism with a re-engagement with an art school consciousness, Dadaist experimentalism, and atmospheric, ambient soundscapes. It was also majorly influenced from world music, especially African and Asian traditions. In the same period, new wave music was more sophisticated in production terms than some contemporaneous progressive music, but was largely perceived as simplistic, and thus had little overt appeal to art music or art-music practice. Musicologist Bill Martin writes; "the [Talking] Heads created a kind of new-wave music that was the perfect synthesis of punk urgency and attitude and progressive-rock sophistication and creativity. A good deal of the more interesting rock since that time is clearly 'post-Talking Heads' music, but this means that it is post-progressive rock as well."

"Progressive electronic" is defined by AllMusic as a subgenre of new age music, and a style that "thrives in more unfamiliar territory" where the results are "often dictated by the technology itself." According to Allmusic, "rather than sampling or synthesizing acoustic sounds to electronically replicate them" producers of this music "tend to mutate the original timbres, sometimes to an unrecognizable state." Allmusic also states that, "true artists in the genre also create their own sounds."

In house music, a desire to define precise stylistic strands and taste markets saw the interposition of prefixes like "progressive", "tribal", and "intelligent". According to disc jockey and producer Carl Craig, the term "progressive" was used in Detroit in the early 1980s in reference to Italian disco. The music was dubbed "progressive" because it drew upon the influence of Giorgio Moroder's Euro disco rather than the disco inspired by the symphonic Philadelphia sound. By 1993, progressive house and trance music had emerged in dance clubs. "Progressive house" was an English style of house distinguished by long tracks, big riffs, mild dub inflections, and multitiered percussion. According to Simon Reynolds, the "'progressive' seemed to signify not just its anti-cheese, nongirly credentials, but its severing of house's roots from gay black disco."

In the mid 1990s, the Lowercase movement, a reductive approach towards new digital technologies, was spearheaded by a number of so-called "progressive electronica" artists.

Reynolds posited that "the truly progressive edge in electronic music involves doing things that can't be physically achieved by human beings manipulating instruments in real-time." He criticized terms like "progressive" and "intelligent", arguing that "it's usually a sign that it's gearing up the media game as a prequel to buying into traditional music industry structure of auteur-stars, concept albums, and long-term careers. Above all, it's a sign of impending musical debility, creeping self-importance, and the hemorrhaging away of fun." Reynolds also identifies links between progressive rock and other electronic music genres, and that "many post-rave genres bear an uncanny resemblance to progressive rock: conceptualism, auteur-geniuses, producers making music to impress other producers, [and] showboating virtuosity reborn as the 'science' of programming finesse."

Citations
Sources


</doc>
<doc id="24503" url="https://en.wikipedia.org/wiki?curid=24503" title="Pyotr Ilyich Tchaikovsky">
Pyotr Ilyich Tchaikovsky

Pyotr Ilyich Tchaikovsky ( ; ; 7 May 1840 – 6 November 1893) was a Russian composer of the Romantic period. He was the first Russian composer whose music made a lasting impression internationally. He was honored in 1884 by Tsar Alexander III and awarded a lifetime pension.

Although musically precocious, Tchaikovsky was educated for a career as a civil servant. There was scant opportunity for a musical career in Russia at the time and no system of public music education. When an opportunity for such an education arose, he entered the nascent Saint Petersburg Conservatory, from which he graduated in 1865. The formal Western-oriented teaching that he received there set him apart from composers of the contemporary nationalist movement embodied by the Russian composers of The Five with whom his professional relationship was mixed. 

Tchaikovsky's training set him on a path to reconcile what he had learned with the native musical practices to which he had been exposed from childhood. From that reconciliation, he forged a personal but unmistakably Russian style. The principles that governed melody, harmony and other fundamentals of Russian music ran completely counter to those that governed Western European music, which seemed to defeat the potential for using Russian music in large-scale Western composition or for forming a composite style, and it caused personal antipathies that dented Tchaikovsky's self-confidence. Russian culture exhibited a split personality, with its native and adopted elements having drifted apart increasingly since the time of Peter the Great. That resulted in uncertainty among the intelligentsia about the country's national identity, an ambiguity mirrored in Tchaikovsky's career.

Despite his many popular successes, Tchaikovsky's life was punctuated by personal crises and depression. Contributory factors included his early separation from his mother for boarding school followed by his mother's early death; the death of his close friend and colleague Nikolai Rubinstein; and the collapse of the one enduring relationship of his adult life, his 13-year association with the wealthy widow Nadezhda von Meck, who was his patron even though they never actually met each other. His homosexuality, which he kept private, has traditionally also been considered a major factor though some musicologists now downplay its importance. Tchaikovsky's sudden death at the age of 53 is generally ascribed to cholera; there is an ongoing debate as to whether cholera was indeed the cause of his death.

While his music has remained popular among audiences, critical opinions were initially mixed. Some Russians did not feel it was sufficiently representative of native musical values and expressed suspicion that Europeans accepted the music for its Western elements. In an apparent reinforcement of the latter claim, some Europeans lauded Tchaikovsky for offering music more substantive than base exoticism and said he transcended stereotypes of Russian classical music. Others dismissed Tchaikovsky's music as "lacking in elevated thought" and derided its formal workings as deficient because they did not stringently follow Western principles.

Pyotr Ilyich Tchaikovsky was born in Votkinsk, a small town in Vyatka Governorate (present-day Udmurtia) in the Russian Empire, into a family with a long history of military service. His father, Ilya Petrovich Tchaikovsky, had served as a lieutenant colonel and engineer in the Department of Mines, and would manage the Kamsko-Votkinsk Ironworks. His grandfather, Pyotr Fedorovich Tchaikovsky (né Petro Fedorovych Chaika), was born in the village of Mikolayivka, Poltava Gubernia, Russian Empire (present day Ukraine), and served first as a physician's assistant in the army and later as city governor of Glazov in Vyatka. His great-grandfather, a Cossack named Fyodor Chaika, distinguished himself under Peter the Great at the Battle of Poltava in 1709. 

Tchaikovsky's mother, Alexandra Andreyevna (née d'Assier), was the second of Ilya's three wives, 18 years her husband's junior and French and German on her father's side. Both Ilya and Alexandra were trained in the arts, including music—a necessity as a posting to a remote area of Russia also meant a need for entertainment, whether in private or at social gatherings. Of his six siblings, Tchaikovsky was close to his sister Alexandra and twin brothers Anatoly and Modest. Alexandra's marriage to Lev Davydov would produce seven children and lend Tchaikovsky the only real family life he would know as an adult, especially during his years of wandering. One of those children, Vladimir Davydov, whom the composer would nickname 'Bob', would become very close to him.

In 1844, the family hired Fanny Dürbach, a 22-year-old French governess. Four-and-a-half-year-old Tchaikovsky was initially thought too young to study alongside his older brother Nikolai and a niece of the family. His insistence convinced Dürbach otherwise. By the age of six, he had become fluent in French and German. Tchaikovsky also became attached to the young woman; her affection for him was reportedly a counter to his mother's coldness and emotional distance from him, though others assert that the mother doted on her son. Dürbach saved much of Tchaikovsky's work from this period, including his earliest known compositions, and became a source of several childhood anecdotes.

Tchaikovsky began piano lessons at age five. Precocious, within three years he had become as adept at reading sheet music as his teacher. His parents, initially supportive, hired a tutor, bought an orchestrion (a form of barrel organ that could imitate elaborate orchestral effects), and encouraged his piano study for both aesthetic and practical reasons. 

However, they decided in 1850 to send Tchaikovsky to the Imperial School of Jurisprudence in Saint Petersburg. They had both graduated from institutes in Saint Petersburg and the School of Jurisprudence, which mainly served the lesser nobility, and thought that this education would prepare Tchaikovsky for a career as a civil servant. Regardless of talent, the only musical careers available in Russia at that time—except for the affluent aristocracy—were as a teacher in an academy or as an instrumentalist in one of the Imperial Theaters. Both were considered on the lowest rank of the social ladder, with individuals in them enjoying no more rights than peasants. 

His father's income was also growing increasingly uncertain, so both parents may have wanted Tchaikovsky to become independent as soon as possible. As the minimum age for acceptance was 12 and Tchaikovsky was only 10 at the time, he was required to spend two years boarding at the Imperial School of Jurisprudence's preparatory school, from his family. Once those two years had passed, Tchaikovsky transferred to the Imperial School of Jurisprudence to begin a seven-year course of studies.

Tchaikovsky's early separation from his mother caused an emotional trauma that lasted the rest of his life and was intensified by her death from cholera in 1854, when he was fourteen. The loss of his mother also prompted Tchaikovsky to make his first serious attempt at composition, a waltz in her memory. Tchaikovsky's father, who had also contracted cholera but recovered fully, sent him back to school immediately in the hope that classwork would occupy the boy's mind. Isolated, Tchaikovsky compensated with friendships with fellow students that became lifelong; these included Aleksey Apukhtin and Vladimir Gerard. 

Music, while not an official priority at school, also bridged the gap between Tchaikovsky and his peers. They regularly attended the opera and Tchaikovsky would improvise at the school's harmonium on themes he and his friends had sung during choir practice. "We were amused," Vladimir Gerard later remembered, "but not imbued with any expectations of his future glory". Tchaikovsky also continued his piano studies through Franz Becker, an instrument manufacturer who made occasional visits to the school; however, the results, according to musicologist David Brown, were "negligible".

In 1855, Tchaikovsky's father funded private lessons with Rudolph Kündinger and questioned him about a musical career for his son. While impressed with the boy's talent, Kündinger said he saw nothing to suggest a future composer or performer. He later admitted that his assessment was also based on his own negative experiences as a musician in Russia and his unwillingness for Tchaikovsky to be treated likewise. Tchaikovsky was told to finish his course and then try for a post in the Ministry of Justice.

On 10 June 1859, the 19-year-old Tchaikovsky graduated as a titular counselor, a low rung on the civil service ladder. Appointed to the Ministry of Justice, he became a junior assistant within six months and a senior assistant two months after that. He remained a senior assistant for the rest of his three-year civil service career.

Meanwhile, the Russian Musical Society (RMS) was founded in 1859 by the Grand Duchess Elena Pavlovna (a German-born aunt of Tsar Alexander II) and her protégé, pianist and composer Anton Rubinstein. Previous tsars and the aristocracy had focused almost exclusively on importing European talent. The aim of the RMS was to fulfill Alexander II's wish to foster native talent. It hosted a regular season of public concerts (previously held only during the six weeks of Lent when the Imperial Theaters were closed) and provided basic professional training in music. In 1861, Tchaikovsky attended RMS classes in music theory taught by Nikolai Zaremba at the Mikhailovsky Palace (now the Russian Museum). These classes were a precursor to the Saint Petersburg Conservatory, which opened in 1862. Tchaikovsky enrolled at the Conservatory as part of its premiere class. He studied harmony and counterpoint with Zaremba and instrumentation and composition with Rubinstein.

The Conservatory benefited Tchaikovsky in two ways. It transformed him into a musical professional, with tools to help him thrive as a composer, and the in-depth exposure to European principles and musical forms gave him a sense that his art was not exclusively Russian or Western. This mindset became important in Tchaikovsky's reconciliation of Russian and European influences in his compositional style. He believed and attempted to show that both these aspects were "intertwined and mutually dependent". His efforts became both an inspiration and a starting point for other Russian composers to build their own individual styles.
Rubinstein was impressed by Tchaikovsky's musical talent on the whole and cited him as "a composer of genius" in his autobiography. He was less pleased with the more progressive tendencies of some of Tchaikovsky's student work. Nor did he change his opinion as Tchaikovsky's reputation grew. He and Zaremba clashed with Tchaikovsky when he submitted his First Symphony for performance by the Russian Musical Society in Saint Petersburg. Rubinstein and Zaremba refused to consider the work unless substantial changes were made. Tchaikovsky complied but they still refused to perform the symphony. Tchaikovsky, distressed that he had been treated as though he were still their student, withdrew the symphony. It was given its first complete performance, minus the changes Rubinstein and Zaremba had requested, in Moscow in February 1868.

Once Tchaikovsky graduated in 1865, Rubinstein's brother Nikolai offered him the post of Professor of Music Theory at the soon-to-open Moscow Conservatory. While the salary for his professorship was only 50 rubles a month, the offer itself boosted Tchaikovsky's morale and he accepted the post eagerly. He was further heartened by news of the first public performance of one of his works, his "Characteristic Dances", conducted by Johann Strauss II at a concert in Pavlovsk Park on 11 September 1865 (Tchaikovsky later included this work, re-titled "Dances of the Hay Maidens", in his opera "The Voyevoda").

From 1867 to 1878, Tchaikovsky combined his professorial duties with music criticism while continuing to compose. This activity exposed him to a range of contemporary music and afforded him the opportunity to travel abroad. In his reviews, he praised Beethoven, considered Brahms overrated and, despite his admiration, took Schumann to task for poor orchestration. He appreciated the staging of Wagner's "Der Ring des Nibelungen" at its inaugural performance in Bayreuth (Germany), but not the music, calling "Das Rheingold" "unlikely nonsense, through which, from time to time, sparkle unusually beautiful and astonishing details". A recurring theme he addressed was the poor state of Russian opera.

In 1856, while Tchaikovsky was still at the School of Jurisprudence and Anton Rubinstein lobbied aristocrats to form the Russian Musical Society, critic Vladimir Stasov and an 18-year-old pianist, Mily Balakirev, met and agreed upon a nationalist agenda for Russian music, one that would take the operas of Mikhail Glinka as a model and incorporate elements from folk music, reject traditional Western practices and use non-Western harmonic devices such as the whole tone and octatonic scales. They saw Western-style conservatories as unnecessary and antipathetic to fostering native talent. 

Eventually, Balakirev, César Cui, Modest Mussorgsky, Nikolai Rimsky-Korsakov and Alexander Borodin became known as the "moguchaya kuchka", translated into English as the "Mighty Handful" or "The Five". Rubinstein criticized their emphasis on amateur efforts in musical composition; Balakirev and later Mussorgsky attacked Rubinstein for his musical conservatism and his belief in professional music training. Tchaikovsky and his fellow conservatory students were caught in the middle.

While ambivalent about much of The Five's music, Tchaikovsky remained on friendly terms with most of its members. In 1869, he and Balakirev worked together on what became Tchaikovsky's first recognized masterpiece, the fantasy-overture "Romeo and Juliet", a work which The Five wholeheartedly embraced. The group also welcomed his Second Symphony, subtitled the "Little Russian". Despite their support, Tchaikovsky made considerable efforts to ensure his musical independence from the group as well as from the conservative faction at the Saint Petersburg Conservatory.

The infrequency of Tchaikovsky's musical successes, won with tremendous effort, exacerbated his lifelong sensitivity to criticism. Nikolai Rubinstein's private fits of rage critiquing his music, such as attacking the First Piano Concerto, did not help matters. His popularity grew, however, as several first-rate artists became willing to perform his compositions. Hans von Bülow premiered the First Piano Concerto and championed other Tchaikovsky works both as pianist and conductor. Other artists included Adele Aus der Ohe, Max Erdmannsdörfer, Eduard Nápravník and Sergei Taneyev.

Another factor that helped Tchaikovsky's music become popular was a shift in attitude among Russian audiences. Whereas they had previously been satisfied with flashy virtuoso performances of technically demanding but musically lightweight compositions, they gradually began listening with increasing appreciation of the music itself. Tchaikovsky's works were performed frequently, with few delays between their composition and first performances; the publication from 1867 onward of his songs and great piano music for the home market also helped boost the composer's popularity.

During the late 1860s, Tchaikovsky began to compose operas. His first, "The Voyevoda", based on a play by Alexander Ostrovsky, premiered in 1869. The composer became dissatisfied with it, however, and, having re-used parts of it in later works, destroyed the manuscript. "Undina" followed in 1870. Only excerpts were performed and it, too, was destroyed. Between these projects, Tchaikovsky started to compose an opera called "Mandragora", to a libretto by Sergei Rachinskii; the only music he completed was a short chorus of Flowers and Insects.

The first Tchaikovsky opera to survive intact, "The Oprichnik", premiered in 1874. During its composition, he lost Ostrovsky's part-finished libretto. Tchaikovsky, too embarrassed to ask for another copy, decided to write the libretto himself, modelling his dramatic technique on that of Eugène Scribe. Cui wrote a "characteristically savage press attack" on the opera. Mussorgsky, writing to Vladimir Stasov, disapproved of the opera as pandering to the public. Nevertheless, "The Oprichnik" continues to be performed from time to time in Russia.

The last of the early operas, "Vakula the Smith" (Op.14), was composed in the second half of 1874. The libretto, based on Gogol's "Christmas Eve", was to have been set to music by Alexander Serov. With Serov's death, the libretto was opened to a competition with a guarantee that the winning entry would be premiered by the Imperial Mariinsky Theatre. Tchaikovsky was declared the winner, but at the 1876 premiere, the opera enjoyed only a lukewarm reception. After Tchaikovsky's death, Rimsky-Korsakov wrote the opera "Christmas Eve", based on the same story.

Other works of this period include the "Variations on a Rococo Theme" for cello and orchestra, the Third and Fourth Symphonies, the ballet "Swan Lake", and the opera "Eugene Onegin".

Discussion of Tchaikovsky's personal life, especially his sexuality, has perhaps been the most extensive of any composer in the 19th century and certainly of any Russian composer of his time. It has also at times caused considerable confusion, from Soviet efforts to expunge all references to same-sex attraction and portray him as a heterosexual, to efforts at analysis by Western biographers. 

Biographers have generally agreed that Tchaikovsky was homosexual. He sought the company of other men in his circle for extended periods, "associating openly and establishing professional connections with them". His first love was reportedly Sergey Kireyev, a younger fellow student at the Imperial School of Jurisprudence. According to Modest Tchaikovsky, this was Pyotr Ilyich's "strongest, longest and purest love". The degree to which the composer might have felt comfortable with his sexual desires has, however, remained open to debate. It is still unknown whether Tchaikovsky, according to musicologist and biographer David Brown, "felt tainted within himself, defiled by something from which he finally realized he could never escape" or whether, according to Alexander Poznansky, he experienced "no unbearable guilt" over his sexual desires and "eventually came to see his sexual peculiarities as an insurmountable and even natural part of his personality ... without experiencing any serious psychological damage". 

Relevant portions of his brother Modest's autobiography, where he tells of the composer's same-sex attraction, have been published, as have letters previously suppressed by Soviet censors in which Tchaikovsky openly writes of it. Such censorship has persisted in the Russian government in 2013, resulting in many officials, including the former culture minister Vladimir Medinsky, denying his homosexuality outright.
Passages in Tchaikovsky's letters which reveal his homosexual desires have been censored in Russia. In one such passage
he told of a homosexual acquaintance: "Petashenka used to drop by with the criminal intention of observing the Cadet Corps, which is right opposite our windows, but I've been trying to discourage these compromising visits – and with some success." In another one he wrote "After our walk, I offered him some money, which was refused. He does it for the love of art and adores men with beards."

Tchaikovsky lived as a bachelor for most of his life. In 1868 he met Belgian soprano Désirée Artôt. They became infatuated with each other and were engaged to be married, but due to Artôt's refusal to give up the stage or settle in Russia, the relationship ended. Tchaikovsky later claimed she was the only woman he ever loved. In 1877, at the age of 37, he wed a former student, Antonina Miliukova. The marriage was a disaster. Mismatched psychologically and sexually, the couple lived together for only two and a half months before Tchaikovsky left, overwrought emotionally and suffering from acute writer's block. Tchaikovsky's family remained supportive of him during this crisis and throughout his life. 

He was also aided by Nadezhda von Meck, the widow of a railway magnate, who had begun contact with him not long before the marriage. As well as an important friend and emotional support, she became his patroness for the next 13 years, which allowed him to focus exclusively on composition. While Tchaikovsky called her his "best friend" they agreed to never meet under any circumstances. Tchaikovsky's marital debacle may have forced him to face the full truth about his sexuality; he never blamed Antonina for the failure of their marriage.

Tchaikovsky remained abroad for a year after the disintegration of his marriage. During this time, he completed "Eugene Onegin", orchestrated his Fourth Symphony, and composed the Violin Concerto. He returned briefly to the Moscow Conservatory in the autumn of 1879. For the next few years, assured of a regular income from von Meck, he traveled incessantly throughout Europe and rural Russia, mainly alone, and avoided social contact whenever possible. 

During this time, Tchaikovsky's foreign reputation grew and a positive reassessment of his music also took place in Russia, thanks in part to Russian novelist Fyodor Dostoyevsky's call for "universal unity" with the West at the unveiling of the Pushkin Monument in Moscow in 1880. Before Dostoyevsky's speech, Tchaikovsky's music had been considered "overly dependent on the West". As Dostoyevsky's message spread throughout Russia, this stigma toward Tchaikovsky's music evaporated. An unprecedented acclaim for him even drew a cult following among the young intelligentsia of Saint Petersburg, including Alexandre Benois, Léon Bakst and Sergei Diaghilev.

Two musical works from this period stand out. With the Cathedral of Christ the Saviour nearing completion in Moscow in 1880, the 25th anniversary of the coronation of Alexander II in 1881, and the 1882 Moscow Arts and Industry Exhibition in the planning stage, Nikolai Rubinstein suggested that Tchaikovsky compose a grand commemorative piece. Tchaikovsky agreed and finished it within six weeks. He wrote to Nadezhda von Meck that this piece, the "1812 Overture", would be "very loud and noisy, but I wrote it with no warm feeling of love, and therefore there will probably be no artistic merits in it". He also warned conductor Eduard Nápravník that "I shan't be at all surprised and offended if you find that it is in a style unsuitable for symphony concerts". Nevertheless, the overture became, for many, "the piece by Tchaikovsky they know best", particularly well-known for the use of cannon in the scores.

On 23 March 1881, Nikolai Rubinstein died in Paris. That December, Tchaikovsky started work on his Piano Trio in A minor, "dedicated to the memory of a great artist". First performed privately at the Moscow Conservatory on the first anniversary of Rubinstein's death, the piece became extremely popular during the composer's lifetime; in November 1893, it would become Tchaikovsky's own elegy at memorial concerts in Moscow and St. Petersburg.

In 1884, Tchaikovsky began to shed his unsociability and restlessness. That March, Tsar Alexander III conferred upon him the Order of St. Vladimir (fourth class), which included a title of hereditary nobility and a personal audience with the Tsar. This was seen as a seal of official approval which advanced Tchaikovsky's social standing and might have been cemented in the composer's mind by the success of his Orchestral Suite No. 3 at its January 1885 premiere in Saint Petersburg. 

In 1885, Alexander III requested a new production of "Eugene Onegin" at the Bolshoi Kamenny Theatre in Saint Petersburg. By having the opera staged there and not at the Mariinsky Theatre, he served notice that Tchaikovsky's music was replacing Italian opera as the official imperial art. In addition, by virtue of Ivan Vsevolozhsky, Director of the Imperial Theaters and a patron of the composer, Tchaikovsky was awarded a lifetime annual pension of 3,000 rubles from the Tsar. This made him the premier court composer, in practice if not in actual title.

Despite Tchaikovsky's disdain for public life, he now participated in it as part of his increasing celebrity and out of a duty he felt to promote Russian music. He helped support his former pupil Sergei Taneyev, who was now director of Moscow Conservatory, by attending student examinations and negotiating the sometimes sensitive relations among various members of the staff. He served as director of the Moscow branch of the Russian Musical Society during the 1889–1890 season. In this post, he invited many international celebrities to conduct, including Johannes Brahms, Antonín Dvořák and Jules Massenet.

During this period, Tchaikovsky also began promoting Russian music as a conductor, In January 1887, he substituted, on short notice, at the Bolshoi Theater in Moscow for performances of his opera "Cherevichki". Within a year, he was in considerable demand throughout Europe and Russia. These appearances helped him overcome life-long stage fright and boosted his self-assurance. In 1888, Tchaikovsky led the premiere of his Fifth Symphony in Saint Petersburg, repeating the work a week later with the first performance of his tone poem "Hamlet". Although critics proved hostile, with César Cui calling the symphony "routine" and "meretricious", both works were received with extreme enthusiasm by audiences and Tchaikovsky, undeterred, continued to conduct the symphony in Russia and Europe. Conducting brought him to the United States in 1891, where he led the New York Music Society's orchestra in his "Festival Coronation March" at the inaugural concert of Carnegie Hall.

In November 1887, Tchaikovsky arrived at Saint Petersburg in time to hear several of the Russian Symphony Concerts, devoted exclusively to the music of Russian composers. One included the first complete performance of his revised First Symphony; another featured the final version of Third Symphony of Nikolai Rimsky-Korsakov, with whose circle Tchaikovsky was already in touch. 

Rimsky-Korsakov, with Alexander Glazunov, Anatoly Lyadov and several other nationalistically minded composers and musicians, had formed a group known as the Belyayev circle, named after a merchant and amateur musician who became an influential music patron and publisher. Tchaikovsky spent much time in this circle, becoming far more at ease with them than he had been with the 'Five' and increasingly confident in showcasing his music alongside theirs. This relationship lasted until Tchaikovsky's death.

In 1892, Tchaikovsky was voted a member of the Académie des Beaux-Arts in France, only the second Russian subject to be so honored (the first was sculptor Mark Antokolski). The following year, the University of Cambridge in England awarded Tchaikovsky an honorary Doctor of Music degree.

On 16/28 October 1893, Tchaikovsky conducted the premiere of his Sixth Symphony, the "Pathétique", in Saint Petersburg. Nine days later, Tchaikovsky died there, aged 53. He was interred in Tikhvin Cemetery at the Alexander Nevsky Monastery, near the graves of fellow-composers Alexander Borodin, Mikhail Glinka, and Modest Mussorgsky; later, Nikolai Rimsky-Korsakov and Mily Balakirev were also buried nearby.

While Tchaikovsky's death has traditionally been attributed to cholera from drinking unboiled water at a local restaurant, there has been much speculation that his death was suicide. In the "New Grove Dictionary of Music", Roland John Wiley wrote that "The polemics over [Tchaikovsky's] death have reached an impasse ... Rumors attached to the famous die hard ... As for illness, problems of evidence offer little hope of satisfactory resolution: the state of diagnosis; the confusion of witnesses; disregard of long-term effects of smoking and alcohol. We do not know how Tchaikovsky died. We may never find out".

Tchaikovsky displayed a wide stylistic and emotional range, from light salon works to grand symphonies. Some of his works, such as the "Variations on a Rococo Theme", employ a "Classical" form reminiscent of 18th-century composers such as Mozart (his favorite composer). Other compositions, such as his "Little Russian" symphony and his opera "Vakula the Smith", flirt with musical practices more akin to those of the 'Five', especially in their use of folk song. Other works, such as Tchaikovsky's last three symphonies, employ a personal musical idiom that facilitated intense emotional expression.

Tchaikovsky first visited Ukraine in 1864, staying in Trostianets where he wrote his first orchestral work, "The Storm" overture. Over the next 28 years, he visited over 15 places in Ukraine, where he stayed a few months at the time. Among his most favorite places was Kamianka, Cherkasy Oblast, where his sister Alexandra lived with her family. He wrote of Kamianka: "I found a feeling of peace in my soul, which I couldn't find in Moscow and St Petersburg". Tchaikovsky wrote more than 30 compositions while in Ukraine. He also visited Ukrainian composer Mykola Lysenko and attended his "Taras Bulba" opera performance in 1890 in the Kiev Opera House. Tchaikovsky was one of the founders of the Kiev Music Conservatory, which was later renamed after him. He also performed in concerts as a conductor in Kiev, Odessa, and Kharkiv.

American music critic and journalist Harold C. Schonberg wrote of Tchaikovsky's "sweet, inexhaustible, supersensuous fund of melody", a feature that has ensured his music's continued success with audiences. Tchaikovsky's complete range of melodic styles was as wide as that of his compositions. Sometimes he used Western-style melodies, sometimes original melodies written in the style of Russian folk song; sometimes he used actual folk songs. According to "The New Grove", Tchaikovsky's melodic gift could also become his worst enemy in two ways. 

The first challenge arose from his ethnic heritage. Unlike Western themes, the melodies that Russian composers wrote tended to be self-contained: they functioned with a mindset of stasis and repetition rather than one of progress and ongoing development. On a technical level, it made modulating to a new key to introduce a contrasting second theme exceedingly difficult, as this was literally a foreign concept that did not exist in Russian music. 

The second way melody worked against Tchaikovsky was a challenge that he shared with the majority of Romantic-age composers. They did not write in the regular, symmetrical melodic shapes that worked well with sonata form, such as those favored by Classical composers such as Haydn, Mozart or Beethoven, but were complete and independent in themselves. This completeness hindered their use as structural elements in combination with one another. This challenge was why the Romantics "were never natural symphonists". All a composer like Tchaikovsky could do with them was to essentially repeat them, even when he modified them to generate tension, maintain interest and satisfy listeners.

Harmony could be a potential trap for Tchaikovsky, according to Brown, since Russian creativity tended to focus on inertia and self-enclosed tableaux, while Western harmony worked against this to propel the music onward and, on a larger scale, shape it. Modulation, the shifting from one key to another, was a driving principle in both harmony and sonata form, the primary Western large-scale musical structure since the middle of the 18th century. Modulation maintained harmonic interest over an extended time-scale, provided a clear contrast between musical themes and showed how those themes were related to each other. 

One point in Tchaikovsky's favor was "a flair for harmony" that "astonished" Rudolph Kündinger, Tchaikovsky's music tutor during his time at the School of Jurisprudence. Added to what he learned at the Saint Petersburg Conservatory studies, this talent allowed Tchaikovsky to employ a varied range of harmony in his music, from the Western harmonic and textural practices of his first two string quartets to the use of the whole tone scale in the center of the finale of the Second Symphony, a practice more typically used by The Five.

Rhythmically, Tchaikovsky sometimes experimented with unusual meters. More often, he used a firm, regular meter, a practice that served him well in dance music. At times, his rhythms became pronounced enough to become the main expressive agent of the music. They also became a means, found typically in Russian folk music, of simulating movement or progression in large-scale symphonic movements—a "synthetic propulsion", as Brown phrases it, which substituted for the momentum that would be created in strict sonata form by the interaction of melodic or motivic elements. This interaction generally does not take place in Russian music. (For more on this, please see Repetition below.)

Tchaikovsky struggled with sonata form. Its principle of organic growth through the interplay of musical themes was alien to Russian practice. The traditional argument that Tchaikovsky seemed unable to develop themes in this manner fails to consider this point; it also discounts the possibility that Tchaikovsky might have intended the development passages in his large-scale works to act as "enforced hiatuses" to build tension, rather than grow organically as smoothly progressive musical arguments.

According to Brown and musicologists Hans Keller and Daniel Zhitomirsky, Tchaikovsky found his solution to large-scale structure while composing the Fourth Symphony. He essentially sidestepped thematic interaction and kept sonata form only as an "outline", as Zhitomirsky phrases it. Within this outline, the focus centered on periodic alternation and juxtaposition. Tchaikovsky placed blocks of dissimilar tonal and thematic material alongside one another, with what Keller calls "new and violent contrasts" between musical themes, keys, and harmonies. This process, according to Brown and Keller, builds momentum and adds intense drama. While the result, Warrack charges, is still "an ingenious episodic treatment of two tunes rather than a symphonic development of them" in the Germanic sense, Brown counters that it took the listener of the period "through a succession of often highly charged sections which "added up" to a radically new kind of symphonic experience" (italics Brown), one that functioned not on the basis of summation, as Austro-German symphonies did, but on one of accumulation.

Partly due to the melodic and structural intricacies involved in this accumulation and partly due to the composer's nature, Tchaikovsky's music became intensely expressive. This intensity was entirely new to Russian music and prompted some Russians to place Tchaikovsky's name alongside that of Dostoyevsky. German musicologist Hermann Kretzschmar credits Tchaikovsky in his later symphonies with offering "full images of life, developed freely, sometimes even dramatically, around psychological contrasts ... This music has the mark of the truly lived and felt experience". Leon Botstein, in elaborating on this comment, suggests that listening to Tchaikovsky's music "became a psychological mirror connected to everyday experience, one that reflected on the dynamic nature of the listener's own emotional self". This active engagement with the music "opened for the listener a vista of emotional and psychological tension and an extremity of feeling that possessed relevance because it seemed reminiscent of one's own 'truly lived and felt experience' or one's search for intensity in a deeply personal sense".

As mentioned above, repetition was a natural part of Tchaikovsky's music, just as it is an integral part of Russian music. His use of sequences within melodies (repeating a tune at a higher or lower pitch in the same voice) could go on for extreme length. The problem with repetition is that, over a period of time, the melody being repeated remains static, even when there is a surface level of rhythmic activity added to it. Tchaikovsky kept the musical conversation flowing by treating melody, tonality, rhythm and sound color as one integrated unit, rather than as separate elements. 

By making subtle but noticeable changes in the rhythm or phrasing of a tune, modulating to another key, changing the melody itself or varying the instruments playing it, Tchaikovsky could keep a listener's interest from flagging. By extending the number of repetitions, he could increase the musical and dramatic tension of a passage, building "into an emotional experience of almost unbearable intensity", as Brown phrases it, controlling when the peak and release of that tension would take place. Musicologist Martin Cooper calls this practice a subtle form of unifying a piece of music and adds that Tchaikovsky brought it to a high point of refinement. (For more on this practice, see the next section.)

Like other late Romantic composers, Tchaikovsky relied heavily on orchestration for musical effects. Tchaikovsky, however, became noted for the "sensual opulence" and "voluptuous timbrel virtuosity" of his orchestration. Like Glinka, Tchaikovsky tended toward bright primary colors and sharply delineated contrasts of texture. However, beginning with the Third Symphony, Tchaikovsky experimented with an increased range of timbres Tchaikovsky's scoring was noted and admired by some of his peers. Rimsky-Korsakov regularly referred his students at the Saint Petersburg Conservatory to it and called it "devoid of all striving after effect, [to] give a healthy, beautiful sonority". This sonority, musicologist Richard Taruskin points out, is essentially Germanic in effect. Tchaikovsky's expert use of having two or more instruments play a melody simultaneously (a practice called doubling) and his ear for uncanny combinations of instruments resulted in "a generalized orchestral sonority in which the individual timbres of the instruments, being thoroughly mixed, would vanish".

In works like the "Serenade for Strings" and the "Variations on a Rococo Theme", Tchaikovsky showed he was highly gifted at writing in a style of 18th-century European pastiche. In the ballet "The Sleeping Beauty" and the opera "The Queen of Spades", Tchaikovsky graduated from imitation to full-scale evocation. This practice, which Alexandre Benois calls "passé-ism", lends an air of timelessness and immediacy, making the past seem as though it were the present. On a practical level, Tchaikovsky was drawn to past styles because he felt he might find the solution to certain structural problems within them. His Rococo pastiches also may have offered escape into a musical world purer than his own, into which he felt himself irresistibly drawn. (In this sense, Tchaikovsky operated in the opposite manner to Igor Stravinsky, who turned to Neoclassicism partly as a form of compositional self-discovery.) Tchaikovsky's attraction to ballet might have allowed a similar refuge into a fairy-tale world, where he could freely write dance music within a tradition of French elegance.

Of Tchaikovsky's Western contemporaries, Robert Schumann stands out as an influence in formal structure, harmonic practices and piano writing, according to Brown and musicologist Roland John Wiley. Boris Asafyev comments that Schumann left his mark on Tchaikovsky not just as a formal influence but also as an example of musical dramaturgy and self-expression. Leon Botstein claims the music of Franz Liszt and Richard Wagner also left their imprints on Tchaikovsky's orchestral style. The late-Romantic trend for writing orchestral suites, begun by Franz Lachner, Jules Massenet, and Joachim Raff after the rediscovery of Bach's works in that genre, may have influenced Tchaikovsky to try his own hand at them. 

His teacher Anton Rubinstein's opera "The Demon" became a model for the final tableau of "Eugene Onegin". So did Léo Delibes' ballets "Coppélia" and "Sylvia" for "The Sleeping Beauty" and Georges Bizet's opera "Carmen" (a work Tchaikovsky admired tremendously) for "The Queen of Spades". Otherwise, it was to composers of the past that Tchaikovsky turned—Beethoven, whose music he respected; Mozart, whose music he loved; Glinka, whose opera "A Life for the Tsar" made an indelible impression on him as a child and whose scoring he studied assiduously; and Adolphe Adam, whose ballet "Giselle" was a favorite of his from his student days and whose score he consulted while working on "The Sleeping Beauty". Beethoven's string quartets may have influenced Tchaikovsky's attempts in that medium. Other composers whose work interested Tchaikovsky included Hector Berlioz, Felix Mendelssohn, Giacomo Meyerbeer, Gioachino Rossini, Giuseppe Verdi, Vincenzo Bellini and Henry Litolff.

Maes maintains that, regardless of what he was writing, Tchaikovsky's main concern was how his music impacted his listeners on an aesthetic level, at specific moments in the piece and on a cumulative level once the music had finished. What his listeners experienced on an emotional or visceral level became an end in itself. Tchaikovsky's focus on pleasing his audience might be considered closer to that of Mendelssohn or Mozart. Considering that he lived and worked in what was probably the last 19th-century feudal nation, the statement is not actually that surprising. 

And yet, even when writing so-called 'programme' music, for example his Romeo and Juliet fantasy overture, he cast it in sonata form. His use of stylized 18th-century melodies and patriotic themes was geared toward the values of Russian aristocracy. He was aided in this by Ivan Vsevolozhsky, who commissioned "The Sleeping Beauty" from Tchaikovsky and the libretto for "The Queen of Spades" from Modest with their use of 18th century settings stipulated firmly. Tchaikovsky also used the polonaise frequently, the dance being a musical code for the Romanov dynasty and a symbol of Russian patriotism. Using it in the finale of a work could assure its success with Russian listeners.

Tchaikovsky's relationship with collaborators was mixed. Like Nikolai Rubinstein with the First Piano Concerto, virtuoso and pedagogue Leopold Auer rejected the Violin Concerto initially but changed his mind; he played it to great public success and taught it to his students, who included Jascha Heifetz and Nathan Milstein. Wilhelm Fitzenhagen "intervened considerably in shaping what he considered 'his' piece", the "Variations on a Rococo Theme", according to music critic Michael Steinberg. Tchaikovsky was angered by Fitzenhagen's license but did nothing; the Rococo Variations were published with the cellist's amendments. 

His collaboration on the three ballets went better and in Marius Petipa, who worked with him on the last two, he might have found an advocate. When "The Sleeping Beauty" was seen by its dancers as needlessly complicated, Petipa convinced them to put in the extra effort. Tchaikovsky compromised to make his music as practical as possible for the dancers and was accorded more creative freedom than ballet composers were usually accorded at the time. He responded with scores that minimized the rhythmic subtleties normally present in his work but were inventive and rich in melody, with more refined and imaginative orchestration than in the average ballet score.

Critical reception to Tchaikovsky's music was also varied but also improved over time. Even after 1880, some inside Russia held it suspect for not being nationalistic enough and thought Western European critics lauded it for exactly that reason. There might have been a grain of truth in the latter, according to musicologist and conductor Leon Botstein, as German critics especially wrote of the "indeterminacy of [Tchaikovsky's] artistic character ... being truly at home in the non-Russian". Of the foreign critics who did not care for his music, Eduard Hanslick lambasted the Violin Concerto as a musical composition "whose stink one can hear" and William Forster Abtrop wrote of the Fifth Symphony, "The furious peroration sounds like nothing so much as a horde of demons struggling in a torrent of brandy, the music growing drunker and drunker. Pandemonium, delirium tremens, raving, and above all, noise worse confounded!"

The division between Russian and Western critics remained through much of the 20th century but for a different reason. According to Brown and Wiley, the prevailing view of Western critics was that the same qualities in Tchaikovsky's music that appealed to audiences—its strong emotions, directness and eloquence and colorful orchestration—added up to compositional shallowness. The music's use in popular and film music, Brown says, lowered its esteem in their eyes still further. There was also the fact, pointed out earlier, that Tchaikovsky's music demanded active engagement from the listener and, as Botstein phrases it, "spoke to the listener's imaginative interior life, regardless of nationality". Conservative critics, he adds, may have felt threatened by the "violence and 'hysteria' " they detected and felt such emotive displays "attacked the boundaries of conventional aesthetic appreciation—the cultured reception of art as an act of formalist discernment—and the polite engagement of art as an act of amusement".

There has also been the fact that the composer did not follow sonata form strictly, relying instead on juxtaposing blocks of tonalities and thematic groups. Maes states this point has been seen at times as a weakness rather than a sign of originality. Even with what Schonberg termed "a professional reevaluation" of Tchaikovsky's work, the practice of faulting Tchaikovsky for not following in the steps of the Viennese masters has not gone away entirely, while his intent of writing music that would please his audiences is also sometimes taken to task. In a 1992 article, "New York Times" critic Allan Kozinn writes, "It is Tchaikovsky's flexibility, after all, that has given us a sense of his variability... Tchaikovsky was capable of turning out music—entertaining and widely beloved though it is—that seems superficial, manipulative and trivial when regarded in the context of the whole literature. The First Piano Concerto is a case in point. It makes a joyful noise, it swims in pretty tunes and its dramatic rhetoric allows (or even requires) a soloist to make a grand, swashbuckling impression. But it is entirely hollow".

In the 21st century, however, critics are reacting more positively to Tchaikovsky's tunefulness, originality, and craftsmanship. "Tchaikovsky is being viewed again as a composer of the first rank, writing music of depth, innovation and influence," according to cultural historian and author Joseph Horowitz. Important in this reevaluation is a shift in attitude away from the disdain for overt emotionalism that marked half of the 20th century. "We have acquired a different view of Romantic 'excess,'" Horowitz says. "Tchaikovsky is today more admired than deplored for his emotional frankness; if his music seems harried and insecure, so are we all".

Horowitz maintains that, while the standing of Tchaikovsky's music has fluctuated among critics, for the public, "it never went out of style, and his most popular works have yielded iconic sound-bytes , such as the love theme from "Romeo and Juliet"". Along with those tunes, Botstein adds, "Tchaikovsky appealed to audiences outside of Russia with an immediacy and directness that were startling even for music, an art form often associated with emotion". Tchaikovsky's melodies, stated with eloquence and matched by his inventive use of harmony and orchestration, have always ensured audience appeal. His popularity is considered secure, with his following in many countries, including Great Britain and the United States, second only to that of Beethoven. His music has also been used frequently in popular music and film.

According to Wiley, Tchaikovsky was a pioneer in several ways. "Thanks in large part to Nadezhda von Meck", Wiley writes, "he became the first full-time professional Russian composer". This, Wiley adds, allowed him the time and freedom to consolidate the Western compositional practices he had learned at the Saint Petersburg Conservatory with Russian folk song and other native musical elements to fulfill his own expressive goals and forge an original, deeply personal style. He made an impact in not only absolute works such as the symphony but also program music and, as Wiley phrases it, "transformed Liszt's and Berlioz's achievements ... into matters of Shakespearean elevation and psychological import". Wiley and Holden both note that Tchaikovsky did all this without a native school of composition upon which to fall back. They point out that only Glinka had preceded him in combining Russian and Western practices and his teachers in Saint Petersburg had been thoroughly Germanic in their musical outlook. He was, they write, for all intents and purposes alone in his artistic quest.
Maes and Taruskin write that Tchaikovsky believed that his professionalism in combining skill and high standards in his musical works separated him from his contemporaries in The Five. Maes adds that, like them, he wanted to produce music that reflected Russian national character but which did so to the highest European standards of quality. Tchaikovsky, according to Maes, came along at a time when the nation itself was deeply divided as to what that character truly was. Like his country, Maes writes, it took him time to discover how to express his Russianness in a way that was true to himself and what he had learned. Because of his professionalism, Maes says, he worked hard at this goal and succeeded. The composer's friend, music critic Hermann Laroche, wrote of "The Sleeping Beauty" that the score contained "an element deeper and more general than color, in the internal structure of the music, above all in the foundation of the element of melody. This basic element is undoubtedly Russian".

Tchaikovsky was inspired to reach beyond Russia with his music, according to Maes and Taruskin. His exposure to Western music, they write, encouraged him to think it belonged to not just Russia but also the world at large. Volkov adds that this mindset made him think seriously about Russia's place in European musical culture—the first Russian composer to do so. It steeled him to become the first Russian composer to acquaint foreign audiences personally with his own works, Warrack writes, as well as those of other Russian composers. In his biography of Tchaikovsky, Anthony Holden recalls the dearth of Russian classical music before Tchaikovsky's birth, then places the composer's achievements into historical perspective: "Twenty years after Tchaikovsky's death, in 1913, Igor Stravinsky's "The Rite of Spring" erupted onto the musical scene, signalling Russia's arrival into 20th-century music. Between these two very different worlds, Tchaikovsky's music became the sole bridge".

On 7 May 2010, Google celebrated his 170th birthday with a Google Doodle.

The following recording was made in Moscow in January 1890, by Julius Block on behalf of Thomas Edison.

According to musicologist Leonid Sabaneyev, Tchaikovsky was not comfortable with being recorded for posterity and tried to shy away from it. On an apparently separate visit from the one related above, Block asked the composer to play something on a piano or at least say something. Tchaikovsky refused. He told Block, "I am a bad pianist and my voice is raspy. Why should one eternalize it?"




</doc>
<doc id="24505" url="https://en.wikipedia.org/wiki?curid=24505" title="Phospholipid">
Phospholipid

Phospholipids (PL) are a class of lipids whose molecule has a hydrophilic "head" containing a phosphate group, and two hydrophobic "tails" derived from fatty acids, joined by an alcohol residue. The phosphate group can be modified with simple organic molecules such as choline, ethanolamine or serine.

Phospholipids are a key component of all cell membranes. They can form lipid bilayers because of their amphiphilic characteristic. In eukaryotes, cell membranes also contain another class of lipid, sterol, interspersed among the phospholipids. The combination provides fluidity in two dimensions combined with mechanical strength against rupture. Purified phospholipids are produced commercially and have found applications in nanotechnology and materials science.

The first phospholipid identified in 1847 as such in biological tissues was lecithin, or phosphatidylcholine, in the egg yolk of chickens by the French chemist and pharmacist Theodore Nicolas Gobley.

The phospholipids are amphiphilic. The hydrophilic end usually contains a negatively charged phosphate group, and the hydrophobic end usually consists of two "tails" that are long fatty acid residues. 

In aqueous solutions, phospholipids are driven by hydrophobic interactions that result in the fatty acid tails aggregating to minimize interactions with water molecules. The result is often a phospholipid bilayer: a membrane that consists of two layers of oppositely oriented phospholipid molecules, with their heads exposed to the liquid on both sides, and with the tails directed into the membrane. That is the dominant structural motif of the membranes of all cells and of some other biological structures, such as vescicles or virus coatings.
In biological membranes, the phospholipids often occur with other molecules (e.g., proteins, glycolipids, sterols) in a bilayer such as a cell membrane. Lipid bilayers occur when hydrophobic tails line up against one another, forming a membrane of hydrophilic heads on both sides facing the water.

These specific properties allow phospholipids to play an important role in the cell membrane. Their movement can be described by the fluid mosaic model, that describes the membrane as a mosaic of lipid molecules that act as a solvent for all the substances and proteins within it, so proteins and lipid molecules are then free to diffuse laterally through the lipid matrix and migrate over the membrane. Sterols contribute to membrane fluidity by hindering the packing together of phospholipids. However, this model has now been superseded, as through the study of lipid polymorphism it is now known that the behaviour of lipids under physiological (and other) conditions is not simple.


"See Sphingolipid"


Phospholipids have been widely used to prepare liposomal, ethosomal and other nanoformulations of topical, oral and parenteral drugs for differing reasons like improved bio-availability, reduced toxicity and increased permeability across membranes. Liposomes are often composed of phosphatidylcholine-enriched phospholipids and may also contain mixed phospholipid chains with surfactant properties. The ethosomal formulation of ketoconazole using phospholipids is a promising option for transdermal delivery in fungal infections.

Computational simulations of phospholipids are often performed using molecular dynamics with force fields such as GROMOS, CHARMM, or AMBER.

Phospholipids are optically highly birefringent, i.e. their refractive index is different along their axis as opposed to perpendicular to it. Measurement of birefringence can be achieved using cross polarisers in a microscope to obtain an image of e.g. vesicle walls or using techniques such as dual polarisation interferometry to quantify lipid order or disruption in supported bilayers.

There are no simple methods available for analysis of phospholipids since the close range of polarity between different phospholipid species makes detection difficult. Oil chemists often use spectroscopy to determine total Phosphorus abundance and then calculate approximate mass of phospholipids based on molecular weight of expected fatty acid species. Modern lipid profiling employs more absolute methods of analysis, with nuclear magnetic resonance spectroscopy (NMR spectroscopy), particularly P-NMR, while HPLC-ELSD provides relative values.

Phospholipid synthesis occurs in the cytosolic side of ER membrane that is studded with proteins that act in synthesis (GPAT and LPAAT acyl transferases, phosphatase and choline phosphotransferase) and allocation (flippase and floppase). Eventually a vesicle will bud off from the ER containing phospholipids destined for the cytoplasmic cellular membrane on its exterior leaflet and phospholipids destined for the exoplasmic cellular membrane on its inner leaflet.

Common sources of industrially produced phospholipids are soya, rapeseed, sunflower, chicken eggs, bovine milk, fish eggs etc. Each source has a unique profile of individual phospholipid species as well as fatty acids and consequently differing applications in food, nutrition, pharmaceuticals, cosmetics and drug delivery.

Some types of phospholipid can be split to produce products that function as second messengers in signal transduction. Examples include phosphatidylinositol (4,5)-bisphosphate (PIP), that can be split by the enzyme Phospholipase C into inositol triphosphate (IP) and diacylglycerol (DAG), which both carry out the functions of the G type of G protein in response to various stimuli and intervene in various processes from long term depression in neurons to leukocyte signal pathways started by chemokine receptors.

Phospholipids also intervene in prostaglandin signal pathways as the raw material used by lipase enzymes to produce the prostaglandin precursors. In plants they serve as the raw material to produce Jasmonic acid, a plant hormone similar in structure to prostaglandins that mediates defensive responses against pathogens.

Phospholipids can act as emulsifiers, enabling oils to form a colloid with water. Phospholipids are one of the components of lecithin which is found in egg-yolks, as well as being extracted from soybeans, and is used as a food additive in many products, and can be purchased as a dietary supplement. Lysolecithins are typically used for water-oil emulsions like margarine, due to their higher HLB ratio.




</doc>
<doc id="24507" url="https://en.wikipedia.org/wiki?curid=24507" title="Pierre Trudeau">
Pierre Trudeau

Joseph Philippe Pierre Yves Elliott Trudeau ( , ; October 18, 1919 – September 28, 2000), mostly referred to as simply Pierre Trudeau, or by the initials PET, was a Canadian politician who was the 15th prime minister of Canada and leader of the Liberal Party of Canada, between 1968 and 1984, with a brief period as Leader of the Opposition, from 1979 to 1980. His tenure of 15 years and 164 days makes him Canada's third longest-serving Prime Minister, behind William Lyon Mackenzie King and John A. Macdonald.

Trudeau rose to prominence as a lawyer, intellectual, and activist in Québec politics. He joined the Liberal Party and was elected to the Canadian Parliament in 1965, quickly being appointed as Prime Minister Lester B. Pearson's Parliamentary Secretary. In 1967, he was appointed Minister of Justice. Trudeau's outgoing personality caused a media sensation, inspiring "Trudeaumania", and helped him to win the leadership of the Liberal Party in 1968, when he was appointed Prime Minister of Canada.

From the late 1960s until the early 1980s, Trudeau's personality dominated the political scene to an extent never before seen in Canadian political life. After his appointment as Prime Minister, he won the 1968, 1972 and 1974 elections, before narrowly losing in 1979. He won a fourth election victory shortly afterwards, in 1980, and eventually retired from politics shortly before the 1984 election.

Despite his personal motto, "Reason before passion", his personality and political career aroused polarizing reactions throughout Canada during his time in office. Admirers praised what they consider to be the force of Trudeau's intellect and his political acumen, maintaining national unity over the Quebec sovereignty movement, suppressing a Québec terrorist crisis, fostering a pan-Canadian identity, and in achieving sweeping institutional reform, including the implementation of official bilingualism, patriation of the Constitution, and the establishment of the "Canadian Charter of Rights and Freedoms". Critics accused him of arrogance, of economic mismanagement, and of unduly centralizing Canadian decision-making to the detriment of the culture of Québec and the economy of the Prairies.

His eldest son, Justin Trudeau, became the 23rd and current Prime Minister, following the 2015 election and 2019 election, and is the first prime minister of Canada to be a descendant of a former prime minister.

The Trudeau family can be traced to Marcillac-Lanville in France in the 16th century and to a Robert Truteau (1544–1589). In 1659, the first Trudeau to arrive in Canada was Étienne Trudeau or Truteau (1641–1712), a carpenter and home builder from La Rochelle.

Pierre Trudeau was born at home at 5779 Durocher Avenue, Outremont, Montreal, Canada, on October 18, 1919, to Charles-Émile "Charley" Trudeau (1887–1935), a French-Canadian businessman and lawyer, and Grace Elliott, who was of mixed Scottish and French-Canadian descent. He had an older sister named Suzette and a younger brother named Charles Jr. Trudeau remained close to both siblings for his entire life. Trudeau attended the prestigious Collège Jean-de-Brébeuf (a private French Jesuit school), where he supported Québec nationalism. Trudeau's paternal grandparents were French-speaking Quebec farmers. His father, Charles-Émile Trudeau, had acquired a chain of gas stations, some "profitable mines, the Belmont amusement park in Montreal and the Montreal Royals, the city's minor-league baseball team" by the time Trudeau was fifteen. When his father died in Orlando, Florida, on April 10, 1935, Trudeau and each of his siblings inherited $5,000, a considerable sum at that time, which meant that he was financially secure and independent. His mother, Grace, "doted on Pierre" and he remained close to her throughout her long life. After her husband died, she left the management of her inheritance to others and spent a lot of her time with work for the Roman Catholic Church and other charities, travelling frequently to New York, Florida, Europe, and Maine, sometimes with her children. Already in his late teens, Trudeau was "directly involved in managing a large inheritance."

From the age of six until twelve, Trudeau attended the primary school, Académie Querbes, in Outremont, where he became immersed in the Catholic religion. The school which was for both English and French Catholics, was an exclusive school with very small classes and he excelled in mathematics and religion. From his earliest years, Trudeau was fluently bilingual which would later prove to be a "big asset for a politician in bilingual Canada." As a teenager, he attended the Jesuit French-language Collège Jean-de-Brébeuf, a prestigious secondary school known for educating elite francophone families in Québec.

In his seventh and final academic year, 1939–1940, Trudeau focused on winning a Rhodes Scholarship. In his application he wrote that he had prepared for public office by studying public speaking and publishing many articles in "Brébeuf". His letters of recommendations praised him highly. Father Boulin, who was the head of the college, said that during his seven years at the college (1933–1940), Trudeau had won a "hundred prizes and honourable mentions" and "performed with distinction in all fields". Trudeau graduated from Collège Jean-de-Brébeuf in 1940 at the age of twenty-one.

Trudeau did not win the Rhodes Scholarship. He consulted a number of people on his options including Henri Bourassa, the economist Edmond Montpetit, and Father Robert Bernier, a Franco-Manitoban. Following their advice, he chose a career in politics, and a degree in law at the Université de Montréal.

In his obituary, "The Economist" described Trudeau as "parochial as a young man", who "dismissed the second world war as a squabble between the big powers, although he later regretted "missing one of the major events of the century". In his 1993 "Memoir," Trudeau wrote that the outbreak of World War II in September 1939 and his father's death were the two "great bombshells" that marked his teenage years. In his first year at university, the prime topics of conversation were the Battle of France, the Battle of Britain, and the London blitz. He wrote that in the early 1940s, when he was in his early twenties, he thought, "So there was a war? Tough. It wouldn't stop me from concentrating on my studies so long as that was possible..[I]f you were a French Canadian in Montreal [at that time], you did not automatically believe that this was a just war. In Montreal in the early 1940s, we still knew nothing about the Holocaust and we tended to think of this war as a settling of scores among the superpowers."

Young Trudeau was opposed to overseas conscription and in 1942, he campaigned for the anti-conscription candidate Jean Drapeau (later the Mayor of Montreal) in Outremont. Trudeau described a speech he heard in Montreal by Ernest Lapointe, who was then Prime Minister William Mackenzie King's top adviser on issues relating to Quebec and French-speaking Canada. Lapointe had been a Liberal MP during the 1917 conscription crisis, in which the Canadian government had deployed up to 1,200 soldiers to suppress the Quebec City anti-conscription Easter Riots in March and April 1918. In a final and bloody conflict, soldiers fired on the crowds. At least five men were killed by gunfire and there were over 150 casualties and $300,000 in damage. In 1939, it was Lapointe who helped draft the Liberal's policy against conscription for service overseas. Lapointe was aware that a new conscription crisis would destroy national unity that Mackenzie King had been trying to build since the end of World War 1. Trudeau never forgave Lapointe for "lying" and breaking his promise. His criticisms of King's war time policies, such as "suspension of habeas corpus", the "farce of bilingualism and French-Canadian advancement in the army. The forced "voluntary enrolment", was scathing.

As a university student Trudeau joined the Canadian Officers' Training Corps (COTC). They trained at the local armoury in Montreal during the school term and undertook further training at Camp Farnham each summer. Although the "National Resources Mobilization Act" was enacted in 1940, preparing the way for conscription to serve overseas, there was no conscription until the Conscription Crisis of 1944 in response to the Invasion of Normandy in June 1944.

Trudeau continued his full-time studies in law at the Université de Montréal while in the COTC, during the war, from 1940 until his graduation in 1943.

Following his graduation in 1943, Trudeau articled for a year, and in the fall of 1944, began his master's in political economy at what is now called Harvard University's John F. Kennedy School of Government and was then known as the Graduate School of Public Administration. In his "Memoir", he admitted that it was at Harvard's "super-informed environment", that he realized the "historic importance" of the war and that he had "missed one of the major events of the century in which [he] was living. Harvard had become a major intellectual centre as fascism in Europe led to the great intellectual migration to the United States. 978-0676975222

Trudeau's Harvard dissertation was on the topic of Marxism, communism and Christianity. At Harvard, a predominantly Protestant American university, Trudeau who was French Catholic, and who for the first time was living outside the province of Quebec, felt like an outsider. As his sense of isolation deepened, in 1947, he decided to continue his work on his Harvard dissertation in Paris, France. He studied at the Institut d'Études Politiques de Paris. The Harvard dissertation remained unfinished when Trudeau entered a doctoral program to study under the socialist economist Harold Laski at the London School of Economics (LSE). This cemented Trudeau's belief that Keynesian economics and social sciences were essential to the creation of the "good life" in a democratic society. He did not finish his LSE dissertation. Over a five-week period he attended many lectures and became a follower of personalism after being influenced most notably by Emmanuel Mounier. He also was influenced by Nikolai Berdyaev, particularly his book "Slavery and Freedom". Max and Monique Nemni argue that Berdyaev's book influenced Trudeau's rejection of nationalism and separatism.

In the summer of 1948, Trudeau embarked on world travels to find a sense of purpose. At the age of twenty-eight, he travelled to Poland where he visited Auschwitz, then Czechoslovakia, Austria, Hungary, Yugoslavia, Bulgaria, Turkey, the Middle East, including Jordan and southern Iraq. Although he was very wealthy, Trudeau traveled with a back pack in "self-imposed hardship". He used his British passport instead of his Canadian passport in his travels through Pakistan, India, China, and Japan, often wearing local clothing to blend in. According to "The Economist", when Trudeau returned to Canada in 1949 after an absence of five years, his mind was "seemingly broadened" from his studying at Harvard, the Institut d'Études Politiques, and the LSE and his travels. He was "appalled at the narrow nationalism in his native French-speaking Quebec, and the authoritarianism of the province's government.

Beginning while Trudeau was travelling overseas, a number of events took place in Quebec that were precursors to the Quiet Revolution in Quebec. These include the 1948 Refus global, the publication of Les insolences du Frère Untel, the 1949 Asbestos Strike, and the 1955 Richard Riot. Artists and intellectuals in Quebec signed the Refus global on August 9, 1948 in opposition to the repressive rule of Premier of Quebec Maurice Duplessis and the decadent "social establishment" in Quebec, including the Catholic Church. When he returned to Montreal in 1949, Trudeau quickly became a leading figure opposing Duplessis' rule. Trudeau actively supported the workers in the Asbestos Strike who opposed Duplessis in 1949. Trudeau was the co-founder and editor of "Cité Libre", a dissident journal that helped provide the intellectual basis for the Quiet Revolution. In 1956 he edited an important book on the subject, "La grève de l'amiante", which argued that the asbestos miners' strike of 1949 was a seminal event in Québec's history, marking the beginning of resistance to the conservative, Francophone clerical establishment and Anglophone business class that had long ruled the province.

Because of his labour union activities in Asbestos, Trudeau was blacklisted by Premier Duplessis and he was unable to teach law at the Université de Montréal. He surprised his closest friends in Quebec when he became a civil servant in Ottawa in 1949. He worked for the federal government until 1951 in the Privy Council Office of the Liberal Prime Minister Louis St. Laurent as an economic policy advisor. He wrote in his memoirs that he found this period very useful later on, when he entered politics, and that senior civil servant Norman Robertson tried unsuccessfully to persuade him to stay on.

His progressive values and his close ties with Co-operative Commonwealth Federation (CCF) intellectuals (including F. R. Scott, Eugene Forsey, Michael Kelway Oliver and Charles Taylor) led to his support of and membership in that federal democratic socialist party throughout the 1950s.

An associate professor of law at the Université de Montréal from 1961 to 1965, Trudeau's views evolved towards a liberal position in favour of individual rights counter to the state and made him an opponent of Québec nationalism. He admired the labour unions, which were tied to the Cooperative Commonwealth Federation (CCF), and tried to infuse his Liberal party with some of their reformist zeal. By the late 1950s Trudeau began to reject social democratic and labour parties, arguing that they should put their narrow goals aside and join forces with Liberals to fight for democracy first. In economic theory he was influenced by professors Joseph Schumpeter and John Kenneth Galbraith while he was at Harvard. Trudeau criticized the Liberal Party of Lester Pearson when it supported arming Bomarc missiles in Canada with nuclear warheads.

He was offered a position at Queen's University teaching political science by James Corry, who later became principal of Queen's, but turned it down because he preferred to teach in Québec. During the 1950s he was blacklisted by the United States and prevented from entering that country because of a visit to a conference in Moscow, and because he subscribed to a number of left-wing publications. Trudeau later appealed the ban and it was rescinded.

In 1965, Trudeau joined the Liberal party, along with his friends Gérard Pelletier and Jean Marchand. These "three wise men" ran successfully for the Liberals in the 1965 election. Trudeau himself was elected in the safe Liberal riding of Mount Royal, in western Montreal. He would hold this seat until his retirement from politics in 1984, winning each election with large majorities. His decision to join the Liberal Party of Canada rather than the CCF's successor, the New Democratic Party (NDP) was partly based on his belief that the federal NDP could not achieve power. He also doubted the feasibility of the centralizing policies of the party. He felt that the party leadership tended toward a ""deux nations"" approach he could not support.

Upon arrival in Ottawa, Trudeau was appointed as Prime Minister Lester Pearson's parliamentary secretary, and spent much of the next year travelling abroad, representing Canada at international meetings and bodies, including the United Nations. In 1967 he was appointed to Pearson's cabinet as Minister of Justice.

As Minister of Justice, Trudeau was responsible for introducing the landmark "Criminal Law Amendment Act", an omnibus bill whose provisions included, among other things, the decriminalization of homosexual acts between consenting adults, the legalization of contraception, abortion and lotteries, new gun ownership restrictions as well as the authorization of breathalyzer tests on suspected drunk drivers. Trudeau famously defended the segment of the bill decriminalizing homosexual acts by telling reporters that "there's no place for the state in the bedrooms of the nation", adding that "what's done in private between adults doesn't concern the Criminal Code". Trudeau paraphrased the term from Martin O'Malley's editorial piece in "The Globe and Mail" on December 12, 1967. Trudeau also liberalized divorce laws, and clashed with Québec Premier Daniel Johnson, Sr. during constitutional negotiations.
At the end of Canada's centennial year in 1967, Prime Minister Pearson announced his intention to step down, and Trudeau entered the race for the Liberal leadership. His energetic campaign attracted massive media attention and mobilized many young people, who saw Trudeau as a symbol of generational change. Going into the leadership convention, Trudeau was the front-runner and a clear favourite with the Canadian public. However, many Liberals still had reservations given that he joined the Liberal Party in 1965 and that his views, particularly those on divorce, abortion, and homosexuality, were seen as radical and opposed by a substantial segment of the party. During the convention, prominent Cabinet Minister Judy LaMarsh was caught on television profanely stating that Trudeau wasn't a Liberal.

Nevertheless, at the April 1968 Liberal leadership convention, Trudeau was elected as the leader on the fourth ballot, with the support of 51% of the delegates. He defeated several prominent and long-serving Liberals including Paul Martin Sr., Robert Winters and Paul Hellyer. As the new leader of the governing Liberals, Trudeau was sworn in as Prime Minister two weeks later on April 20.

Trudeau soon called an election, for June 25. His election campaign benefited from an unprecedented wave of personal popularity called "Trudeaumania", which saw Trudeau mobbed by throngs of youths. Trudeau's main national opponents were PC leader Robert Stanfield and NDP leader Tommy Douglas, both popular figures who had been Premiers, respectively, of Nova Scotia and Saskatchewan (albeit in Trudeau's native Québec, the main competition to the Liberals was from the Ralliement créditiste, led by Réal Caouette). As a candidate Trudeau espoused participatory democracy as a means of making Canada a "Just Society". He defended vigorously the newly implemented universal health care and regional development programmes, as well as the recent reforms found in the Omnibus bill.

On the eve of the election, during the annual Saint-Jean-Baptiste Day parade in Montreal, rioting Québec sovereignists threw rocks and bottles at the grandstand where Trudeau was seated, chanting "Trudeau au poteau!" (Trudeau – to the stake!). Rejecting the pleas of his aides that he take cover, Trudeau stayed in his seat, facing the rioters, without any sign of fear. The image of the defiant Prime Minister impressed the public, and he handily won the 1968 election the next day.

Trudeau's first government implemented many procedural reforms to make Parliament and the Liberal caucus meetings run more efficiently, significantly expanded the size and role of the Prime Minister's office, and substantially expanded social-welfare programs.

Trudeau's first major legislative push was implementing the majority of recommendations of Pearson's Royal Commission on Bilingualism and Biculturalism via the "Official Languages Act", which made French and English the co-equal official languages of the federal government. More controversial than the declaration (which was backed by the NDP and, with some opposition in caucus, the PCs) was the implementation of the Act's principles: between 1966 and 1976, the francophone proportion of the civil service and military doubled, causing alarm in some sections of anglophone Canada that they were being disadvantaged.

Trudeau's Cabinet fulfilled Part IV of the Royal Commission on Bilingualism and Biculturalism's report by announcing a "Multiculturalism Policy" on October 8, 1971. This statement recognized that while Canada was a country of two official languages, it recognized a plurality of cultures – "a multicultural policy within a bilingual framework". This annoyed public opinion in Québec, which believed that it challenged Québec's claim of Canada as a country of two nations.

The 1999 National Film Board (NFB) documentary featuring young Canadians including the writer John Duffy, focused on how Trudeau's efforts to create a bilingual Canada affected them in the 1970s.

Trudeau's first serious test came during the October Crisis of 1970, when a Marxist group, the "Front de libération du Québec" (FLQ) kidnapped British Trade Consul James Cross at his residence on October 5. Five days later Québec Labour Minister Pierre Laporte was also kidnapped. Trudeau, with the acquiescence of Premier of Quebec Robert Bourassa, responded by invoking the "War Measures Act" which gave the government sweeping powers of arrest and detention without trial. Trudeau presented a determined public stance during the crisis, answering the question of how far he would go to stop the violence by saying "Just watch me". Laporte was found dead on October 17 in the trunk of a car. The cause of his death is still debated. Five of the FLQ members were flown to Cuba in 1970 as part of a deal in exchange for James Cross' life, although they eventually returned to Canada years later, where they served time in prison.

Although this response is still controversial and was opposed at the time as excessive by parliamentarians like Tommy Douglas and David Lewis, it was met with only limited objections from the public.

After consultations with the provincial premiers, Trudeau agreed to attend a conference called by British Columbia Premier W. A. C. Bennett to attempt to finally patriate the Canadian constitution. Negotiations with the provinces by Minister of Justice John Turner created a draft agreement, known as the Victoria Charter, that entrenched a charter of rights, bilingualism, and a guarantee of a veto of constitutional amendments for Ontario and Québec, as well as regional vetoes for Western Canada and Atlantic Canada, within the new constitution. The agreement was acceptable to the nine predominantly-English speaking provinces, while Québec's Premier Robert Bourassa requested two weeks to consult with his cabinet. After a strong backlash of popular opinion against the agreement in Québec, Bourassa stated Québec would not accept it.

In foreign affairs, Trudeau kept Canada firmly in the North Atlantic Treaty Organization (NATO), but often pursued an independent path in international relations. He established Canadian diplomatic relations with the People's Republic of China, before the United States did, and went on an official visit to Beijing. He was known as a friend of Fidel Castro, the leader of Cuba.

Trudeau was the first world leader to meet John Lennon and his wife Yoko Ono on their "tour for world peace". Lennon said, after talking with Trudeau for 50 minutes, that Trudeau was "a beautiful person" and that "if all politicians were like Pierre Trudeau, there would be world peace".

In the federal election of 1972, the Liberals won a minority government, with the New Democratic Party led by David Lewis holding the balance of power.

Requiring NDP support to continue, the government would move to the political left, including the creation of Petro-Canada.

In May 1974 the House of Commons passed a motion of no confidence in the Trudeau government, defeating its budget bill after Trudeau intentionally antagonized Stanfield and Lewis. The election of 1974 focused mainly on the current economic recession. Stanfield proposed the immediate introduction of wage and price controls to help end the increasing inflation Canada was currently facing. Trudeau mocked the proposal, saying to a newspaper reporter that it was the equivalent of a magician saying "Zap! You're frozen", and instead promoted a variety of small tax cuts to curb inflation. A campaign tour featuring Trudeau's wife and infant sons was popular, and NDP supporters scared of wage controls moved toward the Liberals.

The Liberals were re-elected with a majority government with 141 of the 264 seats, prompting Stanfield's retirement. The Liberals won no seats in Alberta, though, where Peter Lougheed was a vociferous opponent of Trudeau's 1974 budget.

While popular with the electorate, Trudeau's promised minor reforms had little effect on the growing rate of inflation, and he struggled with conflicting advice on the crisis. In September 1975 the popular Finance Minister John Turner resigned over a perceived lack of support in countervailing measures. In October 1975, in an embarrassing about-face, Trudeau and new Finance Minister Donald Macdonald introduced wage and price controls by passing the "Anti-Inflation Act". The breadth of the legislation, which touched on many powers traditionally considered the purview of the provinces, prompted a Supreme Court reference that only upheld the legislation as an emergency requiring Federal intervention under the "British North America Act". During the annual 1975 Christmas interview with CTV, Trudeau discussed the economy, citing market failures and stating that more state intervention would be necessary. However, the academic wording and hypothetical solutions posed during the complex discussion led much of the public to believe he had declared capitalism itself a failure, creating a lasting distrust among increasingly neoliberal business leaders.

Trudeau continued his attempts at increasing Canada's international profile, including joining the G7 group of major economic powers in 1976 at the behest of U.S. President Gerald Ford. On July 14, 1976, after long and emotional debate, Bill C-84 was passed by the House of Commons by a vote of 130 to 124, abolishing the death penalty completely and instituting a life sentence without parole for 25 years for first-degree murder.

Trudeau faced increasing challenges in Québec, starting with bitter relations with Bourassa and his Liberal government in Québec. After a rise in the polls after the rejection of the Victoria Charter, the Québec Liberals had taken a more confrontational approach with the Federal government on the constitution, French language laws, and the language of air traffic control in Québec. Trudeau responded with increasing anger at what he saw as nationalist provocations against the Federal government's bilingualism and constitutional initiatives, at times expressing his personal contempt for Bourassa.

Partially in an attempt to shore up his support, Bourassa called a surprise election in 1976 that resulted in René Lévesque and the Parti Québécois (PQ) winning a majority government. The PQ had chiefly campaigned on a "good government" platform, but promised a referendum on independence to be held within their first mandate. Trudeau and Lévesque had been personal rivals, with Trudeau's intellectualism contrasting with Lévesque's more working-class image. While Trudeau claimed to welcome the "clarity" provided by the PQ victory, the unexpected rise of the sovereignist movement became, in his view, his biggest challenge.

As the PQ began to take power, Trudeau faced the prolonged failure of his marriage, which was covered in lurid detail on a day-by-day basis by the English language press. Trudeau's reserve was seen as dignified by contemporaries and his poll numbers actually rose during the height of coverage, but aides felt the personal tensions left him uncharacteristically emotional and prone to outbursts.

In 1976, Trudeau, succumbing to pressure from the Chinese government, issued an order barring Taiwan from participating as China in the 1976 Montreal Olympics, although technically it was a matter for the IOC. His action strained relations with the United States – from President Ford, future President Carter and the press – and subjected Canada to international condemnation and shame.

As the 1970s wore on, growing public exhaustion towards Trudeau's personality and the country's constitutional debates caused his poll numbers to fall rapidly in the late 1970s. At the 1978 G7 summit, he discussed strategies for the upcoming election with West German Chancellor Helmut Schmidt, who advised him to announce several spending cuts to quell criticism of the large deficits his government was running.

After a series of defeats in by-elections in 1978, Trudeau waited as long as he could to call a statutory general election in 1979. He finally did so in 1979, only two months from the five-year limit provided under the "British North America Act".

Relations deteriorated on many points in the Nixon years (1969–74), including trade disputes, defence agreements, energy, fishing, the environment, cultural imperialism, and foreign policy. They changed for the better when Trudeau and President Jimmy Carter (1977–81) found a better rapport. The late 1970s saw a more sympathetic American attitude toward Canadian political and economic needs, the pardoning of draft evaders who had moved to Canada, and the passing of old sore points such as Watergate and the Vietnam War. Canada more than ever welcomed American investments during the "stagflation" (high inflation and high unemployment at the same time) that hurt both nations in the 1970s.

In the election of 1979, Trudeau and the Liberals faced declining poll numbers and the Joe Clark–led Progressive Conservatives focusing on "pocketbook" issues. Trudeau and his advisors, to contrast with the mild-mannered Clark, based their campaign on Trudeau's decisive personality and his grasp of the Constitution file, despite the general public's apparent wariness of both. The traditional Liberal rally at Maple Leaf Gardens saw Trudeau stressing the importance of major constitutional reform to general ennui, and his campaign "photo-ops" were typically surrounded by picket lines and protesters. Though polls portended disaster, Clark's struggles justifying his party's populist platform and a strong Trudeau performance in the election debate helped bring the Liberals to the point of contention.

Though winning the popular vote by four points, the Liberal vote was concentrated in Québec and faltered in industrial Ontario, allowing the PCs to win the seat-count handily and form a minority government. Trudeau soon announced his intention to resign as Liberal Party leader and favoured Donald Macdonald to be his successor.

However, before a leadership convention could be held, with Trudeau's blessing and Allan MacEachen's manoeuvring in the house, the Liberals supported an NDP subamendment to Clark's budget stating that the House had no confidence in the budget. In Canada, as in most other countries with a Westminster system, budget votes are indirectly considered to be votes of confidence in the government, and their failure automatically brings down the government. Liberal and NDP votes and Social Credit abstentions led to the subamendment passing 139–133, thereby toppling Clark's government and triggering a new election for a House less than a year old. The Liberal caucus, along with friends and advisers persuaded Trudeau to stay on as leader and fight the election, with Trudeau's main impetus being the upcoming referendum on Québec sovereignty.

Trudeau and the Liberals engaged in a new strategy for the February 1980 election: facetiously called the "low bridge", it involved dramatically underplaying Trudeau's role and avoiding media appearances, to the point of refusing a televised debate. On election day Ontario returned to the Liberal fold, and Trudeau and the Liberals defeated Clark and won a majority government.

As a result of the February 18, 1980 Canadian federal election, the 32nd Canadian Parliament was controlled by a Liberal Party majority, led by Prime Minister Trudeau and the 22nd Canadian Ministry. 

The Liberal victory in 1980 highlighted a sharp geographical divide in the country: the party had won no seats west of Manitoba. Trudeau, in an attempt to represent Western interests, offered to form a coalition government with Ed Broadbent's NDP, which had won 22 seats in the west, but was rebuffed by Broadbent out of fear the party would have no influence in a majority government.

The first challenge Trudeau faced upon re-election was the May 20, 1980 Quebec referendum on Québec sovereignty, called by the Parti Québécois government under René Lévesque. Trudeau immediately initiated federal involvement in the referendum, reversing the Clark government's policy of leaving the issue to the Québec Liberals and Claude Ryan. He appointed Jean Chrétien as the nominal spokesman for the federal government, helping to push the "Non" cause to working-class voters who tuned out the intellectual Ryan and Trudeau. Unlike Ryan and the Liberals, he refused to acknowledge the legitimacy of the referendum question, and noted that the "association" required consent from the other provinces.

In the debates in the legislature during the campaign leading up to the referendum Lévesque said that Trudeau's middle name was Scottish, and that Trudeau's aristocratic upbringing proved that he was more Scottish than French. A week prior to the referendum, Trudeau delivered one of his most well-known speeches, in which he extolled the virtues of federalism and questioned the ambiguous language of the referendum question. He described the origin of the name "Canadian". Trudeau promised a new constitutional agreement should Québec decide to stay in Canada, in which English-speaking Canadians would have to listen to valid concerns made by the Quebecois. On May 20, sixty percent of Quebecers voted to remain in Canada. Following the announcement of the results, Trudeau said that he "had never been so proud to be a Quebecer and a Canadian".

In their first budget, delivered in October 1980 by Trudeau's long-time loyalist, Finance Minister Allan MacEachen, the National Energy Program was introduced. It became one of the Liberal's most contentious policies. The NEP was fiercely protested by the Western provinces. The western provinces blamed the devastating oil bust of the 1980s on the NEB which led to what many termed "Western alienation". Peter Lougheed, then Premier of Alberta entered into tough negotiations with Trudeau and they reached a revenue-sharing agreement on energy in 1982.

This first budget, was one of a series of unpopular budgets delivered in response to the oil shock of 1979 and the ensuing severe global economic recession which began at the start of 1980. In his budget speech, MacEachen said that the global oil price shocks—in 1973 and again in 1979—had caused a "sharp renewal of inflationary forces and real income losses" in Canada and in the industrial world...They are not just Canadian problems ... they are world-wide problems." Leaders of developed countries raised their concerns at the Venice Summit, at meetings of Finance Ministers of the International Monetary Fund (IMF) and the Organisation for Economic Co-operation and Development (OECD). The Bank of Canada wrote that there was a "deeply troubling air of uncertainty and anxiety" about the economy.

Amongst the policies introduced by Trudeau's last term in office were an expansion in government support for Canada's poorest citizens.

In 1982 Trudeau succeeded in repatriating the Constitution. The British Parliament passed an act ceding to the Canadian federal government, full responsibility for amending Canada's national charter. Earlier in his tenure, he had met with opposition from the provincial governments, most notably with the Victoria Charter. Provincial premiers were united in their concerns regarding an amending formula, a court-enforced Charter of Rights, and a further devolution of powers to the provinces. In 1980, Chrétien was tasked with creating a constitutional settlement following the Québec referendum in which Quebecers voted to remain in Canada.

After chairing a series of increasingly acrimonious conferences with first ministers on the issue, Trudeau announced the intention of the federal government to proceed with a request to the British parliament to patriate the constitution, with additions to be approved by a referendum without input from provincial governments. Trudeau was backed by the NDP, Ontario Premier Bill Davis, and New Brunswick Premier Richard Hatfield and was opposed by the remaining premiers and PC leader Joe Clark. After numerous provincial governments challenged the legality of the decision using their reference power, conflicting decisions prompted a Supreme Court decision that stated unilateral patriation was legal, but was in contravention of a constitutional convention that the provinces be consulted and have general agreement to the changes.

After the court decision, which prompted some reservations in the British parliament of accepting a unilateral request, Trudeau agreed to meet with the premiers one more time before proceeding. At the meeting, Trudeau reached an agreement with nine of the premiers on patriating the constitution and implementing the Canadian Charter of Rights and Freedoms, with the caveat that provincial legislatures would have the ability to use a notwithstanding clause to protect some laws from judicial oversight. The notable exception was Lévesque, who, Trudeau believed, would never have signed an agreement. The objection of the Québec government to the new constitution became a source of continued acrimony between the federal and Québec governments, and would forever stain Trudeau's reputation amongst nationalists in the province.

The "Canada Act", which included the "Constitution Act, 1982" and the "Canadian Charter of Rights and Freedoms", was proclaimed by Queen Elizabeth II, as Queen of Canada, on April 17, 1982.

By 1984, the Progressive Conservatives held a substantial lead in opinion polls under their new leader Brian Mulroney, and polls indicated that the Liberals faced all-but-certain defeat if Trudeau led them into the next election.

On February 29, 1984, a day after what he described as a walk through the snowy streets of Ottawa, Trudeau announced he would not lead the Liberals into the next election. He was frequently known to use the term "walk in the snow" as a trope; he claimed to have taken a similar walk in December 1979 before deciding to take the Liberals into the 1980 election.

Trudeau formally retired on June 30, ending his 15-year tenure as Prime Minister. He was succeeded by John Turner, a former Cabinet minister under both Trudeau and Lester Pearson. Before handing power to Turner, Trudeau took the unusual step of appointing Liberal Senators from Western provinces to his Cabinet. He advised Governor General Jeanne Sauvé to appoint over 200 Liberals to patronage positions. He and Turner then crafted a legal agreement calling for Turner to advise an additional 70 patronage appointments. The sheer volume of appointments, combined with questions about the appointees' qualifications, led to condemnation from across the political spectrum. However, an apparent rebound in the polls prompted Turner to call an election for September 1984, almost a year before it was due. 

Turner's appointment deal with Trudeau came back to haunt the Liberals at the English-language debate, when Mulroney demanded that Turner apologize for not advising that the appointments be canceled—advice that Sauvé would have been required to follow by convention. Turner claimed that "I had no option" but to let the appointments stand, prompting Mulroney to tell him, "You had an option, sir–to say 'no'–and you chose to say 'yes' to the old attitudes and the old stories of the Liberal Party."

In the 1984 election, Mulroney won the largest majority government (by total number of seats) in Canadian history. The Liberals, with Turner as leader, lost 95 seats–at the time, the worst defeat of a sitting government at the federal level. In the 1993 Canadian federal election, the Progressive Conservatives faced a larger defeat, when cut to two seats.

Trudeau joined the Montreal law firm Heenan Blaikie as counsel and settled in the historic Maison Cormier in Montreal following his retirement from politics. Though he rarely gave speeches or spoke to the press, his interventions into public debate had a significant impact when they occurred. Trudeau wrote and spoke out against both the Meech Lake Accord and Charlottetown Accord proposals to amend the Canadian constitution, arguing that they would weaken federalism and the Charter of Rights if implemented. His opposition to both Accords was considered one of the major factors leading to the defeat of the two proposals.

He also continued to speak against the Parti Québécois and the sovereignty movement with less effect.

Trudeau also remained active in international affairs, visiting foreign leaders and participating in international associations such as the Club of Rome. He met with Soviet leader Mikhail Gorbachev and other leaders in 1985; shortly afterwards Gorbachev met President Ronald Reagan to discuss easing world tensions.

He published his memoirs in 1993. The book sold hundreds of thousands of copies in several editions, and became one of the most successful Canadian books ever published.

In his old age, he was afflicted with Parkinson's disease and prostate cancer, and became less active, although he continued to work at his law practice until a few months before his death at the age of 80. He was devastated by the death of his youngest son, Michel Trudeau, who was killed in an avalanche on November 13, 1998.

Pierre Elliott Trudeau died on September 28, 2000, and was buried in the Trudeau family crypt, St-Rémi-de-Napierville Cemetery, Saint-Rémi, Québec. His body lay in state in the Hall of Honour in Parliament Hill's Centre Block to allow Canadians to pay their last respects. Several world politicians, including former US President Jimmy Carter and Fidel Castro, attended the funeral. His son Justin delivered the eulogy during the state funeral which led to widespread speculation in the media that a career in politics was in his future. Eventually, Justin did enter politics, was elected to the House of Commons in late 2008, became the leader of the federal Liberal Party in April 2013, and led the Liberals to victory on October 19, 2015. Justin Trudeau was appointed Prime Minister on November 4, 2015, the first time a father and son had both held the position in Canada.

Trudeau was a Roman Catholic and attended church throughout his life. While mostly private about his beliefs, he made it clear that he was a believer, stating, in an interview with the "United Church Observer" in 1971: "I believe in life after death, I believe in God and I'm a Christian." Trudeau maintained, however, that he preferred to impose constraints on himself rather than have them imposed from the outside. In this sense, he believed he was more like a Protestant than a Catholic of the era in which he was schooled.

Michael W. Higgins, a former President of Catholic St. Thomas University, researched Trudeau's spirituality and finds that it incorporated elements of three Catholic traditions. The first of these was the Jesuits who provided his education up to the college level. Trudeau frequently displayed the logic and love of argument consistent with that tradition. A second great spiritual influence in Trudeau's life was Dominican. According to Michel Gourgues, professor at Dominican University College, Trudeau "considered himself a lay Dominican". He studied philosophy under Dominican Father Louis-Marie Régis and remained close to him throughout his life, regarding Régis as "spiritual director and friend". Another skein in Trudeau's spirituality was a contemplative aspect acquired from his association with the Benedictine tradition. According to Higgins, Trudeau was convinced of the centrality of meditation in a life fully lived. Trudeau meditated regularly after being initiated into Transcendental Meditation by the Maharishi Mahesh Yogi. He took retreats at Saint-Benoît-du-Lac, Québec and regularly attended Hours and the Eucharist at Montreal's Benedictine community.

Although never publicly theological in the way of Margaret Thatcher or Tony Blair, nor evangelical, in the way of Jimmy Carter or George W. Bush, Trudeau's spirituality, according to Michael W. Higgins, "suffused, anchored, and directed his inner life. In no small part, it defined him."

Described as a "swinging young bachelor" when he became prime minister, in 1968; Trudeau dated Hollywood star Barbra Streisand in 1969 and 1970. While a serious romantic relationship, there was no express marriage proposal, contrary to one contemporary published report.

On March 4, 1971, while Prime Minister, Trudeau quietly wed 22-year-old Margaret Sinclair, who was 29 years younger, at St. Stephen's Roman Catholic parish church in North Vancouver.

Contrary to his publicized exploits, Trudeau was an intense intellectual with robust work habits and little time for family or fun. As a result, Margaret felt trapped and bored in the marriage, feelings that were exacerbated by her bipolar depression, with which she was later diagnosed.

The couple had three sons: the first two, 23rd and current Prime Minister Justin (born 1971), and Alexandre (born 1973), were both born on Christmas Day two years apart. Their third son, Michel (1975–1998), died in an avalanche while skiing in Kokanee Glacier Provincial Park. They separated in 1977, and were finally divorced in 1984.

When his divorce was finalized in 1984, Trudeau became the first Canadian Prime Minister to become a single parent as the result of divorce. In 1984, Trudeau was romantically involved with Margot Kidder (a Canadian actress famous for her role as Lois Lane in "Superman: The Movie" and its sequels) in the last months of his prime-ministership and after leaving office.

In 1991, Trudeau became a father again, with Deborah Margaret Ryland Coyne, to his only daughter, Sarah. Coyne later stood for the 2013 Liberal Party of Canada leadership election and came fifth in a poll won by Justin.

Trudeau began practising the Japanese martial art judo sometime in the mid-1950s when he was in his mid-thirties, and by the end of the decade he was ranked "ikkyū" (brown belt). Later, when he travelled to Japan as Prime Minister, he was promoted to "shodan" (first-degree black belt) by the Kodokan, and then promoted to "nidan" (second-degree black belt) by Masao Takahashi in Ottawa before leaving office. Trudeau began the night of his famous "walk in the snow" before announcing his retirement in 1984 by going to judo with his sons.

Trudeau remains well regarded by many Canadians. However, the passage of time has only slightly softened the strong antipathy he inspired among his opponents. Trudeau's strong personality, contempt for his opponents and distaste for compromise on many issues have made him, as historian Michael Bliss puts it, "one of the most admired and most disliked of all Canadian prime ministers". "He haunts us still", biographers Christina McCall and Stephen Clarkson wrote in 1990. Trudeau's electoral successes were matched in the 20th century only by those of Mackenzie King.

Trudeau's most enduring legacy may lie in his contribution to Canadian nationalism, and of pride in Canada in and for itself rather than as a derivative of the British Commonwealth. His role in this effort, and his related battles with Québec on behalf of Canadian unity, cemented his political position when in office despite the controversies he faced—and remain the most remembered aspect of his tenure afterwards.

Some consider Trudeau's economic policies to have been a weak point. Inflation and unemployment marred much of his tenure as prime minister. When Trudeau took office in 1968 Canada had a debt of $18 billion (24% of GDP) which was largely left over from World War II, when he left office in 1984, that debt stood at $200 billion (46% of GDP), an increase of 83% in real terms. However, these trends were present in most western countries at the time, including the United States.

Many politicians still use the term "taking a walk in the snow", the line Trudeau used to describe how he arrived at the decision to leave office in 1984. Other popular Trudeauisms frequently used are "just watch me", the "Trudeau Salute", and "Fuddle Duddle".

Maclean's 1997 and 2011 scholarly surveys ranked him twice as the fifth best Canadian prime minister.

One of Trudeau's most enduring legacies is the 1982 patriation of the Canadian constitution—which replaced Canada's ties to Britain with its own constitution, the 1982 "1982 Constitution Act".
This included a domestic amending formula and the Charter of Rights and Freedoms. It is seen as advancing civil rights and liberties and has become a cornerstone of Canadian values for most Canadians. It also represented the final step in Trudeau's liberal vision of a fully independent Canada based on fundamental human rights and the protection of individual freedoms as well as those of linguistic and cultural minorities. Court challenges based on the Charter of Rights have been used to advance the cause of women's equality, re-establish French school boards in provinces such as Alberta and Saskatchewan, and to mandate the adoption of same-sex marriage all across Canada. Section 35 of the "Constitution Act, 1982", has clarified issues of aboriginal and equality rights, including establishing the previously denied aboriginal rights of Métis. Section 15, dealing with equality rights, has been used to remedy societal discrimination against minority groups. The coupling of the direct and indirect influences of the charter has meant that it has grown to influence every aspect of Canadian life and the override (notwithstanding clause) of the charter has been infrequently used.

Canadian conservatives claim the constitution has resulted in too much judicial activism on the part of the courts in Canada. It is also heavily criticized by Québec nationalists, who resent that the 1982 amendments to the constitution were never ratified by any Québec government and the constitution does not recognize a constitutional veto for Québec.

Bilingualism is one of Trudeau's most lasting accomplishments, having been fully integrated into the Federal government's services, documents, and broadcasting (though not, however, in provincial governments, except for Ontario, New Brunswick, and Manitoba). While official bilingualism has settled some of the grievances Francophones had towards the federal government, many Francophones had hoped that Canadians would be able to function in the official language of their choice no matter where in the country they were.

However, Trudeau's ambitions in this arena have been overstated: Trudeau once said that he regretted the use of the term "bilingualism", because it appeared to demand that all Canadians speak two languages. In fact, Trudeau's vision was to see Canada as a bilingual confederation in which "all" cultures would have a place. In this way, his conception broadened beyond simply the relationship of Québec to Canada.

On October 8, 1971, Pierre Trudeau introduced the Multiculturalism Policy in the House of Commons. It was the first of its kind in the world, and was then emulated in several provinces, such as Alberta, Saskatchewan, Manitoba, and other countries most notably Australia, which has had a similar history and immigration pattern. Beyond the specifics of the policy itself, this action signalled an openness to the world and coincided with a more open immigration policy that had been brought in by Trudeau's predecessor Lester B. Pearson.

In the last years of his tenure, he ensured both the National Gallery of Canada and the Canadian Museum of Civilization had proper homes in the national capital region. The Trudeau government also implemented programs which mandated Canadian content in film, and broadcasting, and gave substantial subsidies to develop the Canadian media and cultural industries. Though the policies remain controversial, Canadian media industries have become stronger since Trudeau's arrival.

Trudeau's posthumous reputation in the Western Provinces is notably less favourable than in the rest of English-speaking Canada, and he is sometimes regarded as the "father of Western alienation". To many westerners, Trudeau's policies seemed to favour other parts of the country, especially Ontario and Québec, at their expense. Outstanding among such policies was the National Energy Program, which was seen as unfairly depriving western provinces of the full economic benefit from their oil and gas resources, in order to pay for nationwide social programs, and make regional transfer payments to poorer parts of the country. Sentiments of this kind were especially strong in oil-rich Alberta where unemployment rose from 4% to 10% following passage of the NEP. Estimates have placed Alberta's losses between $50 billion and $100 billion because of the NEP.

More particularly, two incidents involving Trudeau are remembered as having fostered Western alienation, and as emblematic of it. During a visit to Saskatoon, Saskatchewan on July 17, 1969, Trudeau met with a group of farmers who were protesting the Canadian Wheat Board. The widely remembered perception is that Trudeau dismissed the protesters' concerns with "Why should "I" sell your wheat?" – however, he had asked the question rhetorically and then proceeded to answer it himself. Years later, on a train trip through Salmon Arm, British Columbia, he "gave the finger" to a group of protesters through the carriage window less widely remembered is that the protesters were shouting anti-French slogans at the train.

Trudeau's legacy in Québec is mixed. Many credit his actions during the October Crisis as crucial in terminating the Front de libération du Québec (FLQ) as a force in Québec, and ensuring that the campaign for Québec separatism took a democratic and peaceful route. However, his imposition of the "War Measures Act"—which received majority support at the time—is remembered by some in Québec and elsewhere as an attack on democracy. Trudeau is also credited by many for the defeat of the 1980 Quebec referendum.

At the federal level, Trudeau faced almost no strong political opposition in Québec during his time as Prime Minister. For instance, his Liberal party captured 74 out of 75 Québec seats in the 1980 federal election. Provincially, though, Québécois twice elected the pro-sovereignty Parti Québécois. Moreover, there were not at that time any pro-sovereignty federal parties such as the Bloc Québécois. Since the signing of the "Constitution Act, 1982" in 1982 and until 2015, the Liberal Party of Canada had not succeeded in winning a majority of seats in Québec. He was disliked by the Quebecois nationalists.

In 1969, Trudeau along with his then Minister of Indian Affairs Jean Chrétien, proposed the 1969 White Paper (officially entitled Statement of the Government of Canada on Indian policy). Under the legislation of the White Paper, Indian Status would be eliminated. First Nations Peoples would be incorporated fully into provincial government responsibilities as equal Canadian citizens, and reserve status would be removed imposing the laws of private property in indigenous communities. Any special programs or considerations that had been allowed to First Nations people under previous legislation would be terminated, as the special considerations were seen by the Government to act as a means to further separate Indian peoples from Canadian citizens. This proposal was seen by many as racist and an attack on Canada's aboriginal population. The Paper proposed the general assimilation of First Nations into the Canadian body politic through the elimination of the "Indian Act" and Indian status, the parcelling of reserve land to private owners, and the elimination of the Department of Indian and Northern Affairs. The White Paper prompted the first major national mobilization of Indian and Aboriginal activists against the federal government's proposal, leading to Trudeau setting aside the legislation.

Trudeau was a strong advocate for a federalist model of government in Canada, developing and promoting his ideas in response and contrast to strengthening Québec nationalist movements, for instance the social and political atmosphere created during Maurice Duplessis' time in power.

Federalism in this context can be defined as "a particular way of sharing political power among different peoples within a state...Those who believe in federalism hold that different peoples do not need states of their own in order to enjoy self-determination. Peoples ... may agree to share a single state while retaining substantial degrees of self-government over matters essential to their identity as peoples".

As a social democrat, Trudeau sought to combine and harmonize his theories on social democracy with those of federalism so that both could find effective expression in Canada. He noted the ostensible conflict between socialism, with its usually strong centralist government model, and federalism, which expounded a division and cooperation of power by both federal and provincial levels of government. In particular, Trudeau stated the following about socialists:

Trudeau pointed out that in sociological terms, Canada is inherently a federalist society, forming unique regional identities and priorities, and therefore a federalist model of spending and jurisdictional powers is most appropriate. He argues, "in the age of the mass society, it is no small advantage to foster the creation of quasi-sovereign communities at the provincial level, where power is that much less remote from the people."

Trudeau's idealistic plans for a cooperative Canadian federalist state were resisted and hindered as a result of his narrowness on ideas of identity and socio-cultural pluralism: "While the idea of a 'nation' in the sociological sense is acknowledged by Trudeau, he considers the allegiance which it generates—emotive and particularistic—to be contrary to the idea of cohesion between humans, and as such creating fertile ground for the internal fragmentation of states and a permanent state of conflict".

This position garnered significant criticism for Trudeau, in particular from Québec and First Nations peoples on the basis that his theories denied their rights to nationhood. First Nations communities raised particular concerns with the proposed 1969 White Paper, developed under Trudeau by Jean Chrétien.

Trudeau chose the following jurists to be appointed as justices of the Supreme Court of Canada by the Governor General:


According to Canadian protocol, as a former Prime Minister, he was styled "The Right Honourable" for life.

The following honours were bestowed upon him by the Governor General, or by Queen Elizabeth II herself:

Other honours include:

Trudeau received several Honorary Degrees in recognition of his political career.





Trudeau was appointed a Companion of the Order of Canada on June 24, 1985. His citation reads:
Lawyer, professor, author and defender of human rights this statesman served as Prime Minister of Canada for fifteen years. Lending substance to the phrase "the style is the man," he has imparted, both in his and on the world stage, his quintessentially personal philosophy of modern politics.
In 1990, Stephen Clarkson and Christina McCall published a major biography "Trudeau and Our Times" in two volumes. Volume 1, "The magnificent obsession" reprinted in 1997, was the winner of the Governor General's Award. The most recent reprint was in 2006.

Through hours of archival footage and interviews with Trudeau himself, the documentary "Memoirs" details the story of a man who used intelligence and charisma to bring together a country that was very nearly torn apart.

Trudeau's life was also depicted in two CBC Television mini-series. The first one, "Trudeau" (with Colm Feore in the title role), depicts his years as Prime Minister. "Trudeau II: Maverick in the Making" with Stéphane Demers as the young Pierre, and Tobie Pelletier as him in later years) portrays his earlier life.

The 1999 feature-length documentary by the National Film Board (NFB) entitled "" explores the impact of Trudeau's vision of Canadian bilingualism through interviews with eight Canadians—including John Duffy—on how Trudeau's concept of nationalism and bilingualism affected them personally in the 1970s.

In the documentary mini-series "The Champions" directed by Donald Brittain, Trudeau was the co-subject along with René Lévesque.

In 2001, the CBC produced a full-length documentary entitled "Reflections".









</doc>
<doc id="24508" url="https://en.wikipedia.org/wiki?curid=24508" title="Pencil">
Pencil

A pencil is an implement for writing or drawing, constructed of a narrow, solid pigment core in a protective casing that prevents the core from being broken and/or marking the user's hand.

Pencils create marks by physical abrasion, leaving a trail of solid core material that adheres to a sheet of paper or other surface. They are distinct from pens, which dispense liquid or gel ink onto the marked surface.

Most pencil cores are made of graphite powder mixed with a clay binder. Graphite pencils (traditionally known as "lead pencils") produce grey or black marks that are easily erased, but otherwise resistant to moisture, most chemicals, ultraviolet radiation and natural aging. Other types of pencil cores, such as those of charcoal, are mainly used for drawing and sketching. Coloured pencils are sometimes used by teachers or editors to correct submitted texts, but are typically regarded as art supplies—especially those with waxy core binders that tend to smear when erasers are applied to them. Grease pencils have a softer, crayon-like waxy core that can leave marks on smooth surfaces such as glass or porcelain.

The most common pencil casing is thin wood, usually hexagonal in section but sometimes cylindrical or triangular, permanently bonded to the core. Casings may be of other materials, such as plastic or paper. To use the pencil, the casing must be carved or peeled off to expose the working end of the core as a sharp point. Mechanical pencils have more elaborate casings which are not bonded to the core; instead, they support separate, mobile pigment cores that can be extended or retracted through the casing's tip as needed. These casings can be reloaded with new cores (usually graphite) as the previous ones are exhausted.

"Pencil", from Old French "pincel", from Latin a "little tail" (see "penis"; "pincellus" is Latin from the post-classical period) originally referred to an artist's fine brush of camel hair, also used for writing before modern lead or chalk pencils.

Though the archetypal pencil was an artist's brush, the stylus, a thin metal stick used for scratching in papyrus or wax tablets, was used extensively by the Romans and for palm-leaf manuscripts.

As a technique for drawing, the closest predecessor to the pencil was silverpoint until in 1565 (some sources say as early as 1500), a large deposit of graphite was discovered on the approach to Grey Knotts from the hamlet of Seathwaite in Borrowdale parish, Cumbria, England. This particular deposit of graphite was extremely pure and solid, and it could easily be sawn into sticks. It remains the only large-scale deposit of graphite ever found in this solid form. Chemistry was in its infancy and the substance was thought to be a form of lead. Consequently, it was called "plumbago" (Latin for "lead ore"). Because the pencil core is still referred to as "lead", or "a lead", many people have the misconception that the graphite in the pencil is lead, and the black core of pencils is still referred to as "lead", even though it never contained the element lead. The words for pencil in German ("bleistift"), Irish ("peann luaidhe"), Arabic (قلم رصاص "qalam raṣāṣ"), and some other languages literally mean "lead pen".

The value of graphite would soon be realised to be enormous, mainly because it could be used to line the moulds for cannonballs; the mines were taken over by the Crown and were guarded. When sufficient stores of graphite had been accumulated, the mines were flooded to prevent theft until more was required.

The usefulness of graphite for pencils was discovered as well, but graphite for pencils had to be smuggled. Because graphite is soft, it requires some form of encasement. Graphite sticks were initially wrapped in string or sheepskin for stability. England would enjoy a monopoly on the production of pencils until a method of reconstituting the graphite powder was found in 1662 in Italy. However, the distinctively square English pencils continued to be made with sticks cut from natural graphite into the 1860s. The town of Keswick, near the original findings of block graphite, still manufactures pencils, the factory also being the location of the Cumberland Pencil Museum. The meaning of "graphite writing implement" apparently evolved late in the 16th century.

Around 1560, an Italian couple named Simonio and Lyndiana Bernacotti made what are likely the first blueprints for the modern, wood-encased carpentry pencil. Their version was a flat, oval, more compact type of pencil. Their concept involved the hollowing out of a stick of juniper wood. Shortly thereafter, a superior technique was discovered: two wooden halves were carved, a graphite stick inserted, and the halves then glued together—essentially the same method in use to this day.

The first attempt to manufacture graphite sticks from powdered graphite was in Nuremberg, Germany, in 1662. It used a mixture of graphite, sulphur, and antimony.

English and German pencils were not available to the French during the Napoleonic Wars; France, under naval blockade imposed by Great Britain, was unable to import the pure graphite sticks from the British Grey Knotts mines – the only known source in the world. France was also unable to import the inferior German graphite pencil substitute. It took the efforts of an officer in Napoleon's army to change this. In 1795, Nicolas-Jacques Conté discovered a method of mixing powdered graphite with clay and forming the mixture into rods that were then fired in a kiln. By varying the ratio of graphite to clay, the hardness of the graphite rod could also be varied. This method of manufacture, which had been earlier discovered by the Austrian Joseph Hardtmuth, the founder of the Koh-I-Noor in 1790, remains in use. In 1802, the production of graphite leads from graphite and clay was patented by the Koh-I-Noor company in Vienna.

In England, pencils continued to be made from whole sawn graphite. Henry Bessemer's first successful invention (1838) was a method of compressing graphite powder into solid graphite thus allowing the waste from sawing to be reused.

American colonists imported pencils from Europe until after the American Revolution. Benjamin Franklin advertised pencils for sale in his "Pennsylvania Gazette" in 1729, and George Washington used a three-inch pencil when he surveyed the Ohio Country in 1762. It is said that William Munroe, a cabinetmaker in Concord, Massachusetts, made the first American wood pencils in 1812. This was not the only pencil-making occurring in Concord. According to Henry Petroski, transcendentalist philosopher Henry David Thoreau discovered how to make a good pencil out of inferior graphite using clay as the binder; this invention was prompted by his father's pencil factory in Concord, which employed graphite found in New Hampshire in 1821 by Charles Dunbar.

Munroe's method of making pencils was painstakingly slow, and in the neighbouring town of Acton, a pencil mill owner named Ebenezer Wood set out to automate the process at his own pencil mill located at Nashoba Brook. He used the first circular saw in pencil production. He constructed the first of the hexagon- and octagon-shaped wooden casings. Ebenezer did not patent his invention and shared his techniques with anyone. One of those was Eberhard Faber, which built a factory in New York and became the leader in pencil production.

Joseph Dixon, an inventor and entrepreneur involved with the Tantiusques graphite mine in Sturbridge, Massachusetts, developed a means to mass-produce pencils. By 1870, The Joseph Dixon Crucible Company was the world's largest dealer and consumer of graphite and later became the contemporary Dixon Ticonderoga pencil and art supplies company.

By the end of the 19th century, over 240,000 pencils were used each day in the US. The favoured timber for pencils was Red Cedar as it was aromatic and did not splinter when sharpened. In the early 20th century supplies of Red Cedar were dwindling so that pencil manufacturers were forced to recycle the wood from cedar fences and barns to maintain supply.

One effect of this was that "during World War II rotary pencil sharpeners were outlawed in Britain because they wasted so much scarce lead and wood, and pencils had to be sharpened in the more conservative manner – with knives."

It was soon discovered that Incense cedar, when dyed and perfumed to resemble Red Cedar, was a suitable alternative and most pencils today are made from this timber which is grown in managed forests. Over 14 billion pencils are manufactured worldwide annually. Less popular alternatives to cedar include basswood and alder.

In Southeast Asia, the wood Jelutong may be used to create pencils (though the use of this rainforest species is controversial). Environmentalists prefer the use of Pulai – another wood native to the region and used in pencil manufacturing.

On 30 March 1858, Hymen Lipman received the first patent for attaching an eraser to the end of a pencil. In 1862, Lipman sold his patent to Joseph Reckendorfer for $100,000, who went on to sue pencil manufacturer Faber-Castell for infringement. In 1875, the Supreme Court of the US ruled against Reckendorfer declaring the patent invalid.

Historian Henry Petroski notes that while ever more efficient means of mass production of pencils has driven the replacement cost of a pencil down, before this people would continue to use even the stub of a pencil. For those who "did not feel comfortable using a stub, pencil extenders were sold. These devices function something like a "porte-crayon"...the pencil stub can be inserted into the end of a shaft...Extenders were especially common among engineers and draftsmen, whose favorite pencils were priced dearly. The use of an extender also has the advantage that the pencil does not appreciably change its heft as it wears down." Artists currently use extenders to maximize the use of their colored pencils.









A standard, #2, hexagonal pencil is long.


There are also pencils which use mechanical methods to push lead through a hole at the end. These can be divided into two groups: propelling pencils use an internal mechanism to push the lead out from an internal compartment, while clutch pencils merely hold the lead in place (the lead is extended by releasing it and allowing some external force, usually gravity, to pull it out of the body). The erasers (sometimes replaced by a sharpener on pencils with larger lead sizes) are also removable (and thus replaceable), and usually cover a place to store replacement leads. Mechanical pencils are popular for their longevity and the fact that they may never need sharpening. Lead types are based on grade and size; with standard sizes being , , , , , , , , and (ISO 9175-1)—the size is available, but is not considered a standard ISO size.

Pioneered by Taiwanese stationery manufacturer Bensia Pioneer Industrial Corporation in the early 1970s, the product is also known as Bensia Pencils, stackable pencils or non-sharpening pencils. It is a type of pencil where many short pencil tips are housed in a cartridge-style plastic holder. A blunt tip is removed by pulling it from the writing end of the body and re-inserting it into the open-ended bottom of the body, thereby pushing a new tip to the top.

Invented by Harold Grossman for Empire Pencil Company in 1967 and subsequently improved upon by Arthur D. Little for Empire from 1969 through the early 1970s; the plastic pencil was commercialised by Empire as the "EPCON" Pencil. These pencils were co-extruded, extruding a plasticised graphite mix within a wood-composite core.


Residual graphite from a pencil stick is not poisonous, and graphite is harmless if consumed.

Although lead has not been used for writing since antiquity, lead poisoning from pencils was not uncommon. Until the middle of the 20th century the paint used for the outer coating could contain high concentrations of lead, and this could be ingested when the pencil was sucked or chewed.

The lead of the pencil is a mix of finely ground graphite and clay powders. Before the two substances are mixed, they are separately cleaned of foreign matter and dried in a manner that creates large square cakes. Once the cakes have fully dried, the graphite and the clay squares are mixed together using water. The amount of clay content added to the graphite depends on the intended pencil hardness (lower proportions of clay makes the core softer), and the amount of time spent on grinding the mixture determines the quality of the lead. The mixture is then shaped into long spaghetti-like strings, straightened, dried, cut, and then tempered in a kiln. The resulting strings are dipped in oil or molten wax, which seeps into the tiny holes of the material and allows for the smooth writing ability of the pencil. A juniper or incense-cedar plank with several long parallel grooves is cut to fashion a "slat," and the graphite/clay strings are inserted into the grooves. Another grooved plank is glued on top, and the whole assembly is then cut into individual pencils, which are then varnished or painted. Many pencils feature an eraser on the top and so the process is usually still considered incomplete at this point. Each pencil has a shoulder cut on one end of the pencil to allow for a metal ferrule to be secured onto the wood. A rubber plug is then inserted into the ferrule for a functioning eraser on the end of the pencil.

Graphite pencils are made of a mixture of clay and graphite and their darkness varies from light grey to black: the more clay the harder the pencil. There is a wide range of grades available, mainly for artists who are interested in creating a full range of tones from light grey to black. Engineers prefer harder pencils which allow for a greater control in the shape of the lead.

Manufacturers distinguish their pencils by grading them, but there is no common standard. Two pencils of the same grade but different manufacturers will not necessarily make a mark of identical tone nor have the same hardness.

Most manufacturers, and almost all in Europe, designate their pencils with the letters "H" (commonly interpreted as "hardness") to "B" (commonly "blackness"), as well as "F" (usually taken to mean "fineness", although F pencils are no more fine or more easily sharpened than any other grade. Also known as "firm" in Japan). The standard writing pencil is graded "HB". This designation was in use at least as early as 1844. It used "B" for black and "H" for hard; a pencil's grade was described by a sequence or successive Hs or Bs such as "BB" and "BBB" for successively softer leads, and "HH" and "HHH" for successively harder ones. The Koh-i-Noor Hardtmuth pencil manufacturers claim to have first used the HB designations, with "H" standing for Hardtmuth, "B" for the company's location of Budějovice, and "F" for Franz Hardtmuth, who was responsible for technological improvements in pencil manufacture.

As of 2009, a set of pencils ranging from a very soft, black-marking pencil to a very hard, light-marking pencil usually ranges from softest to hardest as follows:
Koh-i-noor offers twenty grades from 10H to 8B for its 1500 series. Mitsubishi Pencil offers twenty-two grades from 10H to 10B for its Hi-uni range. Derwent produces twenty grades from 9H to 9B for its graphic pencils. Staedtler produces 24 from 10H to 12B for its Mars Lumograph pencils.

Numbers as designation were first used by Conté and later by John Thoreau, father of Henry David Thoreau, in the 19th century.
Although Conté/Thoreau's equivalence table is widely accepted, not all manufacturers follow it; for example, Faber-Castell uses a different equivalence table in its "Grip 2001" pencils: 1 = 2B, 2 = B, 2½ = HB, 3 = H, 4 = 2H.

Graded pencils can be used for a rapid test that provides relative ratings for a series of coated panels but can't be used to compare the pencil hardness of different coatings. This test defines a "pencil hardness" of a coating as the grade of the hardest pencil that does not permanently mark the coating when pressed firmly against it at a 45 degree angle. For standardized measurements, there are Mohs hardness testing pencils on the market. 

The majority of pencils made in the US are painted yellow. According to Henry Petroski, this tradition began in 1890 when the L. & C. Hardtmuth Company of Austria-Hungary introduced their Koh-I-Noor brand, named after the famous diamond. It was intended to be the world's best and most expensive pencil, and at a time when most pencils were either painted in dark colours or not at all, the Koh-I-Noor was yellow.
As well as simply being distinctive, the colour may have been inspired by the Austro-Hungarian flag; it was also suggestive of the Orient at a time when the best-quality graphite came from Siberia. Other companies then copied the yellow colour so that their pencils would be associated with this high-quality brand, and chose brand names with explicit Oriental references, such as Mikado (renamed Mirado) and Mongol.

Not all countries use yellow pencils. German and Brazilian pencils, for example, are often green, blue or black, based on the trademark colours of Faber-Castell, a major German stationery company which has plants in those countries. In southern European countries, pencils tend to be dark red or black with yellow lines, while in Australia, they are red with black bands at one end. In India, the most common pencil colour scheme was dark red with black lines, and pencils with a large number of colour schemes are produced.

Pencils are commonly round, hexagonal, or sometimes triangular in section. Carpenters' pencils are typically oval or rectangular, so they cannot easily roll away during work.


The following table lists the prominent manufacturers of wood-cased (including wood-free) pencils around the world.




</doc>
<doc id="24509" url="https://en.wikipedia.org/wiki?curid=24509" title="Pierre Curie">
Pierre Curie

Pierre Curie (, ; ; 15 May 1859 – 19 April 1906) was a French physicist, a pioneer in crystallography, magnetism, piezoelectricity, and radioactivity. In 1903, he received the Nobel Prize in Physics with his wife, Marie Skłodowska-Curie, and Henri Becquerel, "in recognition of the extraordinary services they have rendered by their joint researches on the radiation phenomena discovered by Professor Henri Becquerel".

Born in Paris on 15 May 1859, Pierre Curie was the son of Eugène Curie (28 August 1827 – 25 February 1910), a doctor of French Huguenot Protestant origin from Alsace, and Sophie-Claire Depouilly Curie (15 January 1832 – 27 September 1897). He was educated by his father and in his early teens showed a strong aptitude for mathematics and geometry. When he was 16, he earned his math degree. By the age of 18, he earned a higher degree, but did not proceed immediately to a doctorate due to lack of money. Instead, he worked as a laboratory instructor. When Pierre Curie was preparing for his bachelor of science degree, he worked in the laboratory of Jean-Gustave Bourbouze in the Faculty of Science. 

In 1880, Pierre and his older brother Jacques (1856–1941) demonstrated that an electric potential was generated when crystals were compressed, i.e. piezoelectricity. To aid this work they invented the piezoelectric quartz electrometer. The following year they demonstrated the reverse effect: that crystals could be made to deform when subject to an electric field. Almost all digital electronic circuits now rely on this in the form of crystal oscillators. In subsequent work on magnetism Pierre Curie defined the Curie scale. This work also involved delicate equipment - balances, electrometers, etc.

Pierre Curie was introduced to Maria Skłodowska by their friend, physicist Józef Wierusz-Kowalski. Curie took her into his laboratory as his student. His admiration for her grew when he realized that she would not inhibit his research. He began to regard Skłodowska as his muse. She refused his initial proposal, but finally agreed to marry him on 26 July 1895.

The Curies had a happy, affectionate marriage, and they were known for their devotion to each other.

Before his famous doctoral studies on magnetism, he designed and perfected an extremely sensitive torsion balance for measuring magnetic coefficients. Variations on this equipment were commonly used by future workers in that area. Pierre Curie studied ferromagnetism, paramagnetism, and diamagnetism for his doctoral thesis, and discovered the effect of temperature on paramagnetism which is now known as Curie's law. The material constant in Curie's law is known as the Curie constant. He also discovered that ferromagnetic substances exhibited a critical temperature transition, above which the substances lost their ferromagnetic behavior. This is now known as the Curie temperature. The Curie temperature is used to study plate tectonics, treat hypothermia, measure caffeine, and to understand extraterrestrial magnetic fields.

Pierre Curie formulated what is now known as the "Curie Dissymmetry Principle": a physical effect cannot have a dissymmetry absent from its efficient cause. For example, a random mixture of sand in zero gravity has no dissymmetry (it is isotropic). Introduce a gravitational field, and there is a dissymmetry because of the direction of the field. Then the sand grains can 'self-sort' with the density increasing with depth. But this new arrangement, with the directional arrangement of sand grains, actually reflects the dissymmetry of the gravitational field that causes the separation.

Curie worked with his wife in isolating polonium and radium. They were the first to use the term "radioactivity", and were pioneers in its study. Their work, including Marie Curie's celebrated doctoral work, made use of a sensitive piezoelectric electrometer constructed by Pierre and his brother Jacques Curie. Pierre Curie's 1898 publication with his wife and M. G. Bémont for their discovery of radium and polonium was honored by a Citation for Chemical Breakthrough Award from the Division of History of Chemistry of the American Chemical Society presented to the ESPCI ParisTech (officially the École supérieure de physique et de Chimie industrielles de la Ville de Paris) in 2015.

Curie and one of his students, Albert Laborde, made the first discovery of nuclear energy, by identifying the continuous emission of heat from radium particles. Curie also investigated the radiation emissions of radioactive substances, and through the use of magnetic fields was able to show that some of the emissions were positively charged, some were negative and some were neutral. These correspond to alpha, beta and gamma radiation.

The curie is a unit of radioactivity (3.7 × 10 decays per second or 37 gigabecquerels) originally named in honor of Curie by the Radiology Congress in 1910, after his death. Subsequently, there has been some controversy over whether the naming was in honor of Pierre, Marie, or both.

In the late nineteenth century, Pierre Curie was investigating the mysteries of ordinary magnetism when he became aware of the spiritualist experiments of other European scientists, such as Charles Richet and Camille Flammarion. Pierre Curie initially thought the systematic investigation into the paranormal could help with some unanswered questions about magnetism. He wrote to his fiancée Marie: "I must admit that those spiritual phenomena intensely interest me. I think they are questions that deal with physics." Pierre Curie's notebooks from this period show he read many books on spiritualism. He did not attend séances such as those of Eusapia Palladino in Paris in 1905–06 as a mere spectator, and his goal certainly was not to communicate with spirits. He saw the séances as scientific experiments, tried to monitor different parameters, and took detailed notes of every observation. Despite studying spiritualism, Curie was an atheist.

Pierre and Marie Curie's daughter, Irène, and their son-in-law, Frédéric Joliot-Curie, were also physicists involved in the study of radioactivity, and each received Nobel prizes for their work as well. The Curies' other daughter, Ève, wrote a noted biography of her mother. She was the only member of the Curie family to not become a physicist. Ève married Henry Richardson Labouisse, Jr., who received a Nobel Peace Prize on behalf of Unicef in 1965. Pierre and Marie Curie's granddaughter, Hélène Langevin-Joliot, is a professor of nuclear physics at the University of Paris, and their grandson, Pierre Joliot, who was named after Pierre Curie, is a noted biochemist.

Pierre Curie died in a street accident in Paris on 19 April 1906. Crossing the busy Rue Dauphine in the rain at the Quai de Conti, he slipped and fell under a heavy horse-drawn cart. He died instantly when one of the wheels ran over his head, fracturing his skull. Statements made by his father and lab assistant imply that Curie's characteristic absent-minded preoccupation with his thoughts contributed to his death.

Both the Curies experienced radium burns, both accidentally and voluntarily, and were exposed to extensive doses of radiation while conducting their research. They experienced radiation sickness and Marie Curie died of aplastic anemia in 1934. Even now, all their papers from the 1890s, even her cookbooks, are too dangerous to touch. Their laboratory books are kept in special lead boxes and people who want to see them have to wear protective clothing. Had Pierre Curie not been killed as he was, it is likely that he would have eventually died of the effects of radiation, as did his wife, their daughter Irène, and her husband Frédéric Joliot.

In April 1995, Pierre and Marie Curie were moved from their original resting place, a family cemetery, and enshrined in the crypt of the Panthéon in Paris.




</doc>
<doc id="24510" url="https://en.wikipedia.org/wiki?curid=24510" title="Pushdown automaton">
Pushdown automaton

In the theory of computation, a branch of theoretical computer science, a pushdown automaton (PDA) is 
a type of automaton that employs a stack.

Pushdown automata are used in theories about what can be computed by machines. They are more capable than finite-state machines but less capable than Turing machines.
Deterministic pushdown automata can recognize all deterministic context-free languages while nondeterministic ones can recognize all context-free languages, with the former often used in parser design.

The term "pushdown" refers to the fact that the stack can be regarded as being "pushed down" like a tray dispenser at a cafeteria, since the operations never work on elements other than the top element. A stack automaton, by contrast, does allow access to and operations on deeper elements. Stack automata can recognize a strictly larger set of languages than pushdown automata.
A nested stack automaton allows full access, and also allows stacked values to be entire sub-stacks rather than just single finite symbols.

A finite-state machine just looks at the input signal and the current state: it has no stack to work with. It chooses a new state, the result of following the transition. A pushdown automaton (PDA) differs from a finite state machine in two ways:

A pushdown automaton reads a given input string from left to right. In each step, it chooses a transition by indexing a table by input symbol, current state, and the symbol at the top of the stack. A pushdown automaton can also manipulate the stack, as part of performing a transition. The manipulation can be to push a particular symbol to the top of the stack, or to pop off the top of the stack. The automaton can alternatively ignore the stack, and leave it as it is. 

Put together: Given an input symbol, current state, and stack symbol, the automaton can follow a transition to another state, and optionally manipulate (push or pop) the stack.

If, in every situation, at most one such transition action is possible, then the automaton is called a deterministic pushdown automaton (DPDA). In general, if several actions are possible, then the automaton is called a general, or nondeterministic, PDA. A given input string may drive a nondeterministic pushdown automaton to one of several configuration sequences; if one of them leads to an accepting configuration after reading the complete input string, the latter is said to belong to the "language accepted by the automaton".
We use standard formal language notation: formula_1 denotes the set of finite-length strings over alphabet formula_2 and formula_3 denotes the empty string.

A PDA is formally defined as a 7-tuple:

formula_4
where


An element formula_13 is a transition of formula_14. It has the intended meaning that formula_14, in state formula_16, on the input formula_17 and with formula_18 as topmost stack symbol, may read formula_19, change the state to formula_20, pop formula_21, replacing it by pushing formula_22. The formula_23 component of the transition relation is used to formalize that the PDA can either read a letter from the input, or proceed leaving the input untouched.
In many texts the transition relation is replaced by an (equivalent) formalization, where


Here formula_27 contains all possible actions in state formula_28 with formula_21 on the stack, while reading formula_19 on the input. One writes for example formula_31 precisely when formula_32 because formula_33. Note that "finite" in this definition is essential.

In order to formalize the semantics of the pushdown automaton a description of the current situation is introduced. Any 3-tuple formula_34 is called an instantaneous description (ID) of formula_14, which includes the current state, the part of the input tape that has not been read, and the contents of the stack (topmost symbol written first). The transition relation formula_8 defines the step-relation formula_37 of formula_14 on instantaneous descriptions. For instruction formula_13 there exists a step formula_40, for every formula_41 and every formula_42.

In general pushdown automata are nondeterministic meaning that in a given instantaneous description formula_43 there may be several possible steps. Any of these steps can be chosen in a computation.
With the above definition in each step always a single symbol (top of the stack) is popped, replacing it with as many symbols as necessary. As a consequence no step is defined when the stack is empty.

Computations of the pushdown automaton are sequences of steps. The computation starts in the initial state formula_44 with the initial stack symbol formula_45 on the stack, and a string formula_46 on the input tape, thus with initial description formula_47. 
There are two modes of accepting. The pushdown automaton either accepts by final state, which means after reading its input the automaton reaches an accepting state (in formula_48), or it accepts by empty stack (formula_3), which means after reading its input the automaton empties its stack. The first acceptance mode uses the internal memory (state), the second the external memory (stack).

Formally one defines

Here formula_55 represents the reflexive and transitive closure of the step relation formula_56 meaning any number of consecutive steps (zero, one or more).

For each single pushdown automaton these two languages need to have no relation: they may be equal but usually this is not the case. A specification of the automaton should also include the intended mode of acceptance. Taken over all pushdown automata both acceptance conditions define the same family of languages.

Theorem. For each pushdown automaton formula_14 one may construct a pushdown automaton formula_58 such that formula_59, and vice versa, for each pushdown automaton formula_14 one may construct a pushdown automaton formula_58 such that formula_62

The following is the formal description of the PDA which recognizes the language formula_63 by final state:
formula_64, where


The transition relation formula_8 consists of the following six instructions:

In words, the first two instructions say that in state any time the symbol is read, one is pushed onto the stack. Pushing symbol on top of another is formalized as replacing top by (and similarly for pushing symbol on top of a ).

The third and fourth instructions say that, at any moment the automaton may move from state to state .

The fifth instruction says that in state , for each symbol read, one is popped.

Finally, the sixth instruction says that the machine may move from state to accepting state only when the stack consists of a single .

There seems to be no generally used representation for PDA. Here we have depicted the instruction formula_77 by an edge from state to state labelled by formula_78 (read ; replace by formula_79).

The following illustrates how the above PDA computes on different input strings. The subscript from the step symbol formula_80 is here omitted.

Every context-free grammar can be transformed into an equivalent nondeterministic pushdown automaton. The derivation process of the grammar is simulated in a leftmost way. Where the grammar rewrites a nonterminal, the PDA takes the topmost nonterminal from its stack and replaces it by the right-hand part of a grammatical rule ("expand"). Where the grammar generates a terminal symbol, the PDA reads a symbol from input when it is the topmost symbol on the stack ("match"). In a sense the stack of the PDA contains the unprocessed data of the grammar, corresponding to a pre-order traversal of a derivation tree.

Technically, given a context-free grammar, the PDA has a single state, 1, and its transition relation is constructed as follows.


The PDA accepts by empty stack. Its initial stack symbol is the grammar's start symbol.

For a context-free grammar in Greibach normal form, defining (1,γ) ∈ δ(1,"a","A") for each grammar rule "A" → "a"γ also yields an equivalent nondeterministic pushdown automaton.

The converse, finding a grammar for a given PDA, is not that easy. The trick is to code two states of the PDA into the nonterminals of the grammar.

Theorem. For each pushdown automaton formula_14 one may construct a context-free grammar formula_86 such that formula_87.

The language of strings accepted by a deterministic pushdown automaton is called a deterministic context-free language. Not all context-free languages are deterministic. As a consequence, the DPDA is a strictly weaker variant of the PDA 

A finite automaton with access to two stacks is a more powerful device, equivalent in power to a Turing machine. A linear bounded automaton is a device which is more powerful than a pushdown automaton but less so than a Turing machine.

A GPDA is a PDA which writes an entire string of some known length to the stack or removes an entire string from the stack in one step.

A GPDA is formally defined as a 6-tuple: 
where formula_89, and are defined the same way as a PDA.
is the transition function.

Computation rules for a GPDA are the same as a PDA except that the formula_92's and formula_93's are now strings instead of symbols.

GPDA's and PDA's are equivalent in that if a language is recognized by a PDA, it is also recognized by a GPDA and vice versa.

One can formulate an analytic proof for the equivalence of GPDA's and PDA's using the following simulation:

Let formula_94 be a transition of the GPDA

where formula_95.

Construct the following transitions for the PDA:

As a generalization of pushdown automata, Ginsburg, Greibach, and Harrison (1967) investigated stack automata, which may additionally step left or right in the input string (surrounded by special endmarker symbols to prevent slipping out), and step up or down in the stack in read-only mode. 
A stack automaton is called "nonerasing" if it never pops from the stack. The class of languages accepted by nondeterministic, nonerasing stack automata is "NSPACE"("n"), which is a superset of the context-sensitive languages. The class of languages accepted by deterministic, nonerasing stack automata is "DSPACE"("n"⋅log("n")).

An alternating pushdown automaton (APDA) is a pushdown automaton with a state set


States in formula_99 and formula_100 are called "existential" resp. "universal". In an existential state an APDA nondeterministically chooses the next state and accepts if "at least one" of the resulting computations accepts. In a universal state APDA moves to all next states and accepts if "all" the resulting computations accept.

The model was introduced by Chandra, Kozen and Stockmeyer. Ladner, Lipton and Stockmeyer proved that this model is equivalent to EXPTIME i.e. a language is accepted by some APDA if, and only if, it can be decided by an exponential-time algorithm.

Aizikowitz and Kaminski introduced "synchronized alternating pushdown automata" (SAPDA) that are equivalent to conjunctive grammars in the same way as nondeterministic PDA are equivalent to context-free grammars.





</doc>
<doc id="24511" url="https://en.wikipedia.org/wiki?curid=24511" title="Protein primary structure">
Protein primary structure

Protein primary structure is the linear sequence of amino acids in a peptide or protein. By convention, the primary structure of a protein is reported starting from the amino-terminal (N) end to the carboxyl-terminal (C) end. Protein biosynthesis is most commonly performed by ribosomes in cells. Peptides can also be synthesized in the laboratory. Protein primary structures can be directly sequenced, or inferred from DNA sequences.

Amino acids are polymerised via peptide bonds to form a long backbone, with the different amino acid side chains protruding along it. In biological systems, proteins are produced during translation by a cell's ribosomes. Some organisms can also make short peptides by non-ribosomal peptide synthesis, which often use amino acids other than the standard 20, and may be cyclised, modified and cross-linked.

Peptides can be synthesised chemically via a range of laboratory methods. Chemical methods typically synthesise peptides in the opposite order (starting at the C-terminus) to biological protein synthesis (starting at the N-terminus).

Protein sequence is typically notated as a string of letters, listing the amino acids starting at the amino-terminal end through to the carboxyl-terminal end. Either a three letter code or single letter code can be used to represent the 20 naturally occurring amino acids, as well as mixtures or ambiguous amino acids (similar to nucleic acid notation).

Peptides can be directly sequenced, or inferred from DNA sequences. Large sequence databases now exist that collate known protein sequences.

In general, polypeptides are unbranched polymers, so their primary structure can often be specified by the sequence of amino acids along their backbone. However, proteins can become cross-linked, most commonly by disulfide bonds, and the primary structure also requires specifying the cross-linking atoms, e.g., specifying the cysteines involved in the protein's disulfide bonds. Other crosslinks include desmosine.

The chiral centers of a polypeptide chain can undergo racemization. Although it does not change the sequence, it does affect the chemical properties of the sequence. In particular, the -amino acids normally found in proteins can spontaneously isomerize at the formula_1 atom to form -amino acids, which cannot be cleaved by most proteases. Additionally, proline can form stable trans-isomers at the peptide bond.

Finally, the protein can undergo a variety of posttranslational modifications, which are briefly summarized here.

The N-terminal amino group of a polypeptide can be modified covalently, e.g.,




The C-terminal carboxylate group of a polypeptide can also be modified, e.g.,


Finally, the peptide side chains can also be modified covalently, e.g.,












Most of the polypeptide modifications listed above occur "post-translationally", i.e., after the protein has been synthesized on the ribosome, typically occurring in the endoplasmic reticulum, a subcellular organelle of the eukaryotic cell.

Many other chemical reactions (e.g., cyanylation) have been applied to proteins by chemists, although they are not found in biological systems.

In addition to those listed above, the most important modification of primary structure is peptide cleavage (by chemical hydrolysis or by proteases). Proteins are often synthesized in an inactive precursor form; typically, an N-terminal or C-terminal segment blocks the active site of the protein, inhibiting its function. The protein is activated by cleaving off the inhibitory peptide.

Some proteins even have the power to cleave themselves. Typically, the hydroxyl group of a serine (rarely, threonine) or the thiol group of a cysteine residue will attack the carbonyl carbon of the preceding peptide bond, forming a tetrahedrally bonded intermediate [classified as a hydroxyoxazolidine (Ser/Thr) or hydroxythiazolidine (Cys) intermediate]. This intermediate tends to revert to the amide form, expelling the attacking group, since the amide form is usually favored by free energy, (presumably due to the strong resonance stabilization of the peptide group). However, additional molecular interactions may render the amide form less stable; the amino group is expelled instead, resulting in an ester (Ser/Thr) or thioester (Cys) bond in place of the peptide bond. This chemical reaction is called an N-O acyl shift.

The ester/thioester bond can be resolved in several ways:


The proposal that proteins were linear chains of α-amino acids was made nearly simultaneously by two scientists at the same conference in 1902, the 74th meeting of the Society of German Scientists and Physicians, held in Karlsbad. Franz Hofmeister made the proposal in the morning, based on his observations of the biuret reaction in proteins. Hofmeister was followed a few hours later by Emil Fischer, who had amassed a wealth of chemical details supporting the peptide-bond model. For completeness, the proposal that proteins contained amide linkages was made as early as 1882 by the French chemist E. Grimaux.

Despite these data and later evidence that proteolytically digested proteins yielded only oligopeptides, the idea that proteins were linear, unbranched polymers of amino acids was not accepted immediately. Some well-respected scientists such as William Astbury doubted that covalent bonds were strong enough to hold such long molecules together; they feared that thermal agitations would shake such long molecules asunder. Hermann Staudinger faced similar prejudices in the 1920s when he argued that rubber was composed of macromolecules.

Thus, several alternative hypotheses arose. The colloidal protein hypothesis stated that proteins were colloidal assemblies of smaller molecules. This hypothesis was disproved in the 1920s by ultracentrifugation measurements by Theodor Svedberg that showed that proteins had a well-defined, reproducible molecular weight and by electrophoretic measurements by Arne Tiselius that indicated that proteins were single molecules. A second hypothesis, the cyclol hypothesis advanced by Dorothy Wrinch, proposed that the linear polypeptide underwent a chemical cyclol rearrangement C=O + HN formula_8 C(OH)-N that crosslinked its backbone amide groups, forming a two-dimensional "fabric". Other primary structures of proteins were proposed by various researchers, such as the diketopiperazine model of Emil Abderhalden and the pyrrol/piperidine model of Troensegaard in 1942. Although never given much credence, these alternative models were finally disproved when Frederick Sanger successfully sequenced insulin and by the crystallographic determination of myoglobin and hemoglobin by Max Perutz and John Kendrew.

Any linear-chain heteropolymer can be said to have a "primary structure" by analogy to the usage of the term for proteins, but this usage is rare compared to the extremely common usage in reference to proteins. In RNA, which also has extensive secondary structure, the linear chain of bases is generally just referred to as the "sequence" as it is in DNA (which usually forms a linear double helix with little secondary structure). Other biological polymers such as polysaccharides can also be considered to have a primary structure, although the usage is not standard.

The primary structure of a biological polymer to a large extent determines the three-dimensional shape (tertiary structure). Protein sequence can be used to predict local features, such as segments of secondary structure, or trans-membrane regions. However, the complexity of protein folding currently prohibits predicting the tertiary structure of a protein from its sequence alone. Knowing the structure of a similar homologous sequence (for example a member of the same protein family) allows highly accurate prediction of the tertiary structure by homology modeling. If the full-length protein sequence is available, it is possible to estimate its general biophysical properties, such as its isoelectric point.

Sequence families are often determined by sequence clustering, and structural genomics projects aim to produce a set of representative structures to cover the sequence space of possible non-redundant sequences.



</doc>
<doc id="24512" url="https://en.wikipedia.org/wiki?curid=24512" title="Peter principle">
Peter principle

The Peter principle is a concept in management developed by Laurence J. Peter, which observes that people in a hierarchy tend to rise to their "level of incompetence": employees are promoted based on their success in previous jobs until they reach a level at which they are no longer competent, as skills in one job do not necessarily translate to another. The concept was elucidated in the book "The Peter Principle" (William Morrow and Company, 1969) by Dr Peter and Raymond Hull.

Peter and Hull intended the book to be satire, but it became popular as it was seen to make a serious point about the shortcomings of how people are promoted within hierarchical organizations. Hull wrote the text, based on Peter's research. The Peter principle has been the subject of much subsequent commentary and research.

The Peter principle states that a person who is competent at a job will earn promotion to a more senior position which requires different skills. If the promoted person lacks the skills required for the new role, they will be incompetent at the new level, and will not be promoted again. If the person is competent in the new role, they will be promoted again, and will continue to be promoted until reaching a level at which they are incompetent. Being incompetent, the individual will not qualify for promotion again, and so will remain stuck at that final level (termed "Final Placement" or "Peter's Plateau"). This outcome is inevitable, given enough time and assuming that there are enough positions in the hierarchy to which competent employees may be promoted. The "Peter Principle" is therefore expressed as: "In a hierarchy every employee tends to rise to his level of incompetence." This leads to Peter's Corollary: "In time, every post tends to be occupied by an employee who is incompetent to carry out its duties." Hull calls the study of how hierarchies work "hierarchiology."

Laurence J. Peter's research led to the formulation of the Peter principle well before publishing his findings. He worked with Raymond Hull on a book that elucidated his observations about hierarchies. The principle is named for Peter because although Hull actually wrote the book, it is a summary of Peter's research. "The Peter Principle" was published by William Morrow and Company in 1969.

In chapters 1 and 2, Peter and Hull give various examples of the Peter principle in action. In each case, the higher position required skills which were not required at the level immediately below. For example, a competent school teacher may make a competent assistant principal, but then go on to be an incompetent principal. The teacher was competent at educating children, and as assistant principal he was good at dealing with parents and other teachers, but as principal he was poor at maintaining good relations with the school board and the superintendent.

In chapter 3, Peter and Hull discuss apparent exceptions to this principle and then debunk them. One of these illusory exceptions is when someone who is incompetent is still promoted anyway. They coin the phrase "percussive sublimation" for this phenomenon of being "kicked upstairs". But it is only a pseudo-promotion: a move from one unproductive position to another. This improves staff morale, as other employees believe that they too can be promoted again. Another pseudo-promotion is the ""lateral arabesque"", when a person is moved out of the way and given a longer job title.

While incompetence is merely a barrier to further promotion, "super-incompetence" is grounds for dismissal. So is super-competence. In both cases "they tend to disrupt the hierarchy." One example of a super-competent employee is a teacher of children with special needs who was so effective at educating them that after a year they exceeded all expectations at reading and arithmetic, but the teacher was still fired because he had neglected to devote enough time to bead-stringing and finger-painting.

Chapters 4 and 5 deal with the methods of achieving promotion: "Push" and "Pull." Push means the employee's own efforts, such as working hard and taking self-improvement courses. This is usually not very effective, because of the Seniority Factor: the next level up is often fully occupied, blocking the path to promotion. Pull is far more effective, and refers to accelerated promotion brought about by the efforts of an employee's mentors or patrons.

Chapter 6 explains why "good followers do not become good leaders." In chapter 7, Peter and Hull describe the effect of the Peter Principle in politics and government. Chapter 8, titled "Hints and Foreshadowings", discusses the work of earlier writers on the subject of incompetence, such as Sigmund Freud, Karl Marx and Alexander Pope.

Chapter 9 explains that once employees have reached their level of incompetence, they always lack insight into their situation. Peter and Hull go on to explain why aptitude tests do not work and are actually counter-productive. Finally, they describe "Summit Competence": when someone reaches the highest level in their organisation and yet is still competent at that level. This is only because there were not enough ranks in the hierarchy, or because they did not have time to reach a level of incompetence. Such people often seek a level of incompetence in another hierarchy. For example, Socrates was an outstanding teacher but a terrible defence attorney. This is known as "Compulsive Incompetence."

Chapter 10 explains why trying to assist an incompetent employee by promoting another employee to act as his assistant does not work. ""Incompetence plus incompetence equals incompetence.""

Chapters 11 and 12 describe the various medical and psychological manifestations of stress which may result when someone reaches his level of incompetence, as well as other symptoms such as certain characteristic habits of speech or behaviour.

Chapter 13 considers whether it is possible for an employee who has reached his level of incompetence to be happy and healthy once he gets there. The answer is no, if he realises his true situation, and yes if he does not. 

In chapter 14 various ways of avoiding promotion to the final level are described. Attempting to refuse an offered promotion is ill-advised, and is only practicable if the employee is not married and has no one else to answer to. Generally it is better to avoid being considered for promotion in the first place, by pretending to be incompetent while one is actually still employed at a level of competence. This is "Creative Incompetence," and several examples of successful techniques are given. It works best if the chosen field of incompetence does not actually impair one's work.

The concluding chapter applies Peter's Principle to the entire human species at an evolutionary level, and asks whether humanity can survive in the long run, or will become extinct upon reaching its level of incompetence as technology advances.

Other commenters made observations similar to the Peter principle long before Peter's research. Gotthold Ephraim Lessing's 1763 play "Minna von Barnhelm" features an army sergeant who shuns the opportunity to move up in the ranks, saying "I am a good sergeant; I might easily make a bad captain, and certainly an even worse general. One knows from experience." Similarly, Carl von Clausewitz (1780–1831) wrote that "there is nothing more common than to hear of men losing their energy on being raised to a higher position, to which they do not feel themselves equal." Spanish philosopher José Ortega y Gasset (1883–1955) virtually enunciated the Peter principle in 1910, "All public employees should be demoted to their immediately lower level, as they have been promoted until turning incompetent."

A number of scholars have engaged in research interpreting the Peter principle and its effects. In 2000, Edward Lazear explored two possible explanations for the phenomenon. First is the idea that employees work harder to gain a promotion, and then slack off once it is achieved. The other is that it is a statistical process: workers who are promoted have passed a particular benchmark of productivity based on factors that cannot necessarily be replicated in their new role, leading to a Peter principle situation. Lazear concluded that the former explanation only occurs under particular compensation structures, whereas the latter always holds up.

Alessandro Pluchino, Andrea Rapisarda, and Cesare Garofalo used an agent-based modelling approach to simulate the promotion of employees in a system where the Peter principle is assumed to be true. They found that the best way to improve efficiency in an enterprise is to promote people randomly, or to shortlist the best and the worst performer in a given group, from which the person to be promoted is then selected randomly. For this work, they won the 2010 edition of the parody Ig Nobel Prize in management science.

In 2018, professors Alan Benson, Danielle Li, and Kelly Shue analyzed sales workers' performance and promotion practices at 214 American businesses to test the veracity of the Peter principle. They found that these companies tended to promote employees to management position based on their performance in their previous position, rather than based on managerial potential. Consistent with the Peter principle, the researchers found that high performing sales employees were likelier to be promoted, and that they were likelier to perform poorly as managers, leading to considerable costs to the businesses.

The Peter principle inspired Scott Adams, creator of the comic strip "Dilbert", to develop a similar concept, the Dilbert principle. The Dilbert principle holds that incompetent employees are promoted to management positions to get them out of the workflow. Adams explained the idea in his 1996 business book "The Dilbert Principle", and it has since been analyzed alongside the Peter principle. João Ricardo Faria wrote that the Dilbert principle is "a sub-optimal version of the Peter principle," and leads to even lower profitability than the Peter principle.

Companies and organizations shaped their policies to contend with the Peter principle. Lazear stated that some companies expect that productivity will "regress to the mean" following promotion in their hiring and promotion practices. Other companies have adopted "up or out" strategies, such as the Cravath System, in which employees who do not advance are periodically fired. The Cravath System was developed at the law firm Cravath, Swaine & Moore, which made a practice of hiring chiefly recent law graduates, promoting internally and firing employees who do not perform at the required level. Brian Christian and Tom Griffiths have suggested the additive increase/multiplicative decrease algorithm as a solution to the Peter principle less severe than firing employees who fail to advance. They propose a dynamic hierarchy in which employees are regularly either promoted or reassigned to a lower level, so that any worker who is promoted to their point of failure is soon moved to an area where they are productive.




</doc>
<doc id="24513" url="https://en.wikipedia.org/wiki?curid=24513" title="Platonic realism">
Platonic realism

Platonic realism is the philosophical position that universals or abstract objects exist objectively and outside of human minds. It is named after the Greek philosopher Plato who applied realism to such universals, which he considered ideal forms. This stance is ambiguously also called Platonic idealism but should not be confused with idealism as presented by philosophers such as George Berkeley: as Platonic abstractions are not spatial, temporal, or mental, they are not compatible with the later idealism's emphasis on mental existence. Plato's Forms include numbers and geometrical figures, making them a theory of mathematical realism; they also include the Form of the Good, making them in addition a theory of ethical realism.

Plato expounded his own articulation of realism regarding the existence of universals in his dialogue "The Republic" and elsewhere, notably in the "Phaedo", the "Phaedrus", the "Meno" and the "Parmenides".

In Platonic realism, "universals" do not exist in the way that ordinary physical objects exist, even though Plato metaphorically referred to such objects in order to explain his concepts. More modern versions of the theory seek to avoid applying potentially misleading descriptions to universals. Instead, such versions maintain that it is meaningless (or a category mistake) to apply the categories of space and time to "universals".

Regardless of their description, Platonic realism holds that "universals" do exist in a broad, abstract sense, although not at any spatial or temporal distance from people's bodies. Thus, people cannot see or otherwise come into sensory contact with universals, but in order to conceive of universals, one must be able to conceive of these abstract forms.

Theories of universals, including Platonic realism, are challenged to satisfy certain constraints on theories of "universals".

Platonic realism satisfies one of those constraints, in that it is a theory of what general terms refer to. "Forms" are ideal in supplying meaning to referents for general terms. That is, to understand terms such as "applehood" and "redness", Platonic realism says that they refer to forms. Indeed, Platonism gets much of its plausibility because mentioning "redness", for example, could be assumed to be referring to something that is apart from space and time, but which has many specific instances.

Some contemporary linguistic philosophers construe "Platonism" to mean the proposition that universals exist independently of particulars (a universal is anything that can be predicated of a particular). Similarly, a form of modern Platonism is found in the philosophy of mathematics, especially regarding the foundations of mathematics. The Platonic interpretation of this philosophy includes the thesis that mathematics is discovered rather than created.

Plato's interpretation of universals is linked to his "Theory of Forms" in which he uses both the terms ("eidos": "form") and ("idea": "characteristic") to describe his theory. Forms are mind independent abstract objects or paradigms (παραδείγματα: "patterns in nature") of which particular objects and the properties and relations present in them are copies. Form is inherent in the particulars and these are said to "participate in" the form. Classically "idea" has been translated (or transliterated) as "idea," but secondary literature now typically employs the term "form" (or occasionally "kind," usually in discussion of Plato's "Sophist" and "Statesman") to avoid confusion with the English word connoting "thought".

Platonic form can be illustrated by contrasting a material triangle with an ideal triangle. The Platonic form is the ideal triangle — a figure with perfectly drawn lines whose angles add to 180 degrees. Any form of triangle that we experience will be an imperfect representation of the ideal triangle. Regardless of how precise your measuring and drawing tools you will never be able to recreate this perfect shape. Even drawn to the point where our senses cannot perceive a defect, in its essence the shape will still be imperfect; forever unable to match the ideal triangle.

Some versions of Platonic realism, like that of Proclus, regard Plato's forms as thoughts in the mind of God. Most consider forms not to be mental entities at all.

In Platonic realism, forms are related to "particulars" (instances of objects and properties) in that a particular is regarded as a copy of its form. For example, a particular apple is said to be a copy of the form of "applehood" and the apple's redness is an instance of the form of "Redness". "Participation" is another relationship between forms and particulars. Particulars are said to "participate" in the forms, and the forms are said to "inhere" in the particulars.

According to Plato, there are some forms that are not instantiated at all, but, he contends, that does not imply that the forms "could not" be instantiated. Forms are capable of being instantiated by many different particulars, which would result in the forms' having many copies, or inhering many particulars.

Two main criticisms with Platonic realism relate to inherence and the difficulty of creating concepts without sense perception. Despite these criticisms, realism has strong defenders. Its popularity through the centuries has been variable.

Critics claim that the terms "instantiation" and "copy" are not further defined and that "participation" and "inherence" are similarly mysterious and unenlightening.
They question what it means to say that the form of applehood "inheres" a particular apple or that the apple is a "copy" of the form of applehood. To the critic, it seems that the forms, not being spatial, cannot have a shape, so it cannot be that the apple "is the same shape as" the form. Likewise, the critic claims it is unclear what it means to say that an apple "participates" in "applehood".

Arguments refuting the inherence criticism, however, claim that a form of something spatial can lack a concrete (spatial) location and yet have "in abstracto" spatial qualities. An apple, then, can have the same shape as its form. Such arguments typically claim that the relationship between a particular and its form is very intelligible and easily grasped; that people unproblematically apply Platonic theory in everyday life; and that the inherence criticism is only created by the artificial demand to explain the normal understanding of inherence as if it were highly problematic. That is, the supporting argument claims that the criticism is with the mere illusion of a problem and thus could render suspect any philosophical concept.

A criticism of forms relates to the origin of concepts without the benefit of sense-perception. For example, to think of redness in general, according to Plato, is to think of the form of redness. Critics, however, question how one can have the concept of a form existing in a special realm of the universe, apart from space and time, since such a concept cannot come from sense-perception. Although one can see an apple and its redness, the critic argues, those things merely participate in, or are copies of, the forms. Thus, they claim, to conceive of a particular apple and its redness is not to conceive of "applehood" or "redness-in-general", so they question the source of the concept.

Plato's doctrine of recollection, however, addresses such criticism by saying that souls are "born" with the concepts of the forms, and just have to be "reminded" of those concepts from back before birth, when the souls were in close contact with the forms in the Platonic heaven. Plato is thus known as one of the very first rationalists, believing as he did that humans are born with a fund of "a priori" knowledge, to which they have access through a process of reason or intellection — a process that critics find to be rather mysterious.

A more modern response to this criticism of "concepts without sense-perception" is the claim that the universality of its qualities is an unavoidable given because one only experiences an object by means of general concepts. So, since the critic already grasps the relation between the abstract and the concrete, he is invited to stop thinking that it implies a contradiction. The response reconciles Platonism with empiricism by contending that an abstract (i.e., not concrete) object is "real" and knowable by its instantiation. Since the critic has, after all, naturally understood the abstract, the response suggests merely to abandon prejudice and accept it.





</doc>
<doc id="24514" url="https://en.wikipedia.org/wiki?curid=24514" title="Psychosis">
Psychosis

Psychosis is an abnormal condition of the mind that results in difficulties determining what is real and what is not real. Symptoms may include delusions and hallucinations. Other symptoms may include incoherent speech and behavior that is inappropriate for the situation. There may also be sleep problems, social withdrawal, lack of motivation, and difficulties carrying out daily activities. Psychosis in adolescents is a serious medical illness but is not very common.
Psychosis has many different causes. These include mental illness, such as schizophrenia or bipolar disorder, sleep deprivation, some medical conditions, certain medications, and drugs such as alcohol or cannabis. One type, known as postpartum psychosis, can occur after giving birth. The neurotransmitter dopamine is believed to play a role. Acute psychosis is considered primary if it results from a psychiatric condition and secondary if it is caused by a medical condition. The diagnosis of a mental illness requires excluding other potential causes. Testing may be done to check for central nervous system diseases, toxins, or other health problems as a cause.
Treatment may include antipsychotic medication, counselling, and social support. Early treatment appears to improve outcomes. Medications appear to have a moderate effect. Outcomes depend on the underlying cause. In the United States about 3% of people develop psychosis at some point in their lives. The condition has been described since at least the 4th century BC by Hippocrates and possibly as early as 1500 BC in the Egyptian Ebers Papyrus.

A hallucination is defined as sensory perception in the absence of external stimuli. Hallucinations are different from illusions and perceptual distortions, which are the misperception of external stimuli. Hallucinations may occur in any of the senses and take on almost any form. They may consist of simple sensations (such as lights, colors, sounds, tastes, or smells) or more detailed experiences (such as seeing and interacting with animals and people, hearing voices, and having complex tactile sensations). Hallucinations are generally characterized as being vivid and uncontrollable. Auditory hallucinations, particularly experiences of hearing voices, are the most common and often prominent feature of psychosis.

Up to 15% of the general population may experience auditory hallucinations (though not all are due to psychosis). The prevalence in schizophrenia is generally put around 70%, but may go as high as 98%. Reported prevalence in bipolar disorder ranges between 11% and 68%. During the early 20th century, auditory hallucinations were second to visual hallucinations in frequency, but they are now the most common manifestation of schizophrenia, although rates vary between cultures and regions. Auditory hallucinations are most commonly intelligible voices. When voices are present, the average number has been estimated at three. Content, like frequency, differs significantly, especially across cultures and demographics. People who experience auditory hallucinations can frequently identify the loudness, location of origin, and may settle on identities for voices. Western cultures are associated with auditory experiences concerning religious content, frequently related to sin. Hallucinations may command a person to do something potentially dangerous when combined with delusions.

Extracampine hallucinations are perceptions outside the sensory apparatus for example a sound is perceived through the knee, or a visual extracampine hallucination is seeing by sensing that somebody is near to you, that is not there.

Visual hallucinations occur in roughly a third of people with schizophrenia, although rates as high as 55% are reported. The prevalence in bipolar disorder is around 15%. Content frequently involves animate objects, although perceptual abnormalities such as changes in lighting, shading, streaks, or lines may be seen. Visual abnormalities may conflict with proprioceptive information, and visions may include experiences such as the ground tilting. Lilliputian hallucinations are less common in schizophrenia, and occur more frequently in various types of encephalopathy such as peduncular hallucinosis.

A visceral hallucination, also called a cenesthetic hallucination, is characterized by visceral sensations in the absence of stimuli. Cenesthetic hallucinations may include sensations of burning, or re-arrangement of internal organs.

Psychosis may involve delusional beliefs. A delusion is commonly defined as an unrelenting sense of certainty maintained despite strong contradictory evidence. Delusions are context- and culture-dependent: a belief which inhibits critical functioning and is widely considered delusional in one population may be common (and even adaptive) in another, or in the same population at a later time. Since normative views may themselves contradict available evidence, a belief need not contravene cultural standards in order to be considered delusional.

Prevalence in schizophrenia is generally considered at least 90%, and around 50% in bipolar disorder.

The DSM-5 characterizes certain delusions as "bizarre" if they are clearly implausible, or are incompatible with the surrounding cultural context. The concept of bizarre delusions has many criticisms, the most prominent being judging its presence is not highly reliable even among trained individuals.

A delusion may involve diverse thematic content. The most common type is a persecutory delusion, in which a person believes that some entity is attempting to harm them. Others include delusions of reference (the belief that some element of one's experience represents a deliberate and specific act by or message from some other entity), delusions of grandeur (the belief that one possesses special power or influence beyond one's actual limits), thought broadcasting (the belief that one's thoughts are audible) and thought insertion (the belief that one's thoughts are not one's own).

The subject matter of delusions seems to reflect the current culture in a particular time and location. For example in the US, during the early 1900s syphilis was a common topic, during the second world war Germany, during the cold war communists, and in recent years technology has been a focus. Some psychologists, such as those who practice the Open Dialogue method believe that the content of psychosis represent an underlying thought process that may, in part, be responsible for psychosis, though the accepted medical position is that psychosis is due to a brain disorder. 

Historically, Karl Jaspers classified psychotic delusions into "primary" and "secondary" types. Primary delusions are defined as arising suddenly and not being comprehensible in terms of normal mental processes, whereas secondary delusions are typically understood as being influenced by the person's background or current situation (e.g., ethnicity; also religious, superstitious, or political beliefs).

Disorganization is split into disorganized speech or thinking, and grossly disorganized motor behavior. Disorganized speech or thinking, also called formal thought disorder, is disorganization of thinking that is inferred from speech. Characteristics of disorganized speech include rapidly switching topics, called derailment or loose association; switching to topics that are unrelated, called tangential thinking; incomprehensible speech, called word salad or incoherence. Disorganized motor behavior includes repetitive, odd, or sometimes purposeless movement. Disorganized motor behavior rarely includes catatonia, and although it was a historically prominent symptom, it is rarely seen today. Whether this is due to historically used treatments or the lack thereof is unknown.

Catatonia describes a profoundly agitated state in which the experience of reality is generally considered impaired. There are two primary manifestations of catatonic behavior. The classic presentation is a person who does not move or interact with the world in any way while awake. This type of catatonia presents with waxy flexibility. Waxy flexibility is when someone physically moves part of a catatonic person's body and the person stays in the position even if it is bizarre and otherwise nonfunctional (such as moving a person's arm straight up in the air and the arm staying there).

The other type of catatonia is more of an outward presentation of the profoundly agitated state described above. It involves excessive and purposeless motor behaviour, as well as extreme mental preoccupation that prevents an intact experience of reality. An example is someone walking very fast in circles to the exclusion of anything else with a level of mental preoccupation (meaning not focused on anything relevant to the situation) that was not typical of the person prior to the symptom onset. In both types of catatonia there is generally no reaction to anything that happens outside of them. It is important to distinguish catatonic agitation from severe bipolar mania, although someone could have both.

Negative symptoms include reduced emotional expression, decreased motivation, and reduced spontaneous speech. Afflicted individuals lack interest and spontaneity, and have the inability to feel pleasure.

Young people who have psychosis may have trouble connecting with the world around them and may experience hallucinations and/or delusions. Adolescents with psychosis may also have cognitive deficits that may make it harder for the youth to socialize and work. Potential impairments include the speed of mental processing, ability to focus without getting distracted (attention span), and problems with their verbal memory.

The symptom of psychosis may be caused by serious medical illnesses such as schizophrenia or other psychiatric disorders, trauma, or other medical conditions. Psychosis may also be temporary in nature or transient and caused by medications or illicit substance use (substance‐induced psychosis).

Brief hallucinations are not uncommon in those without any psychiatric disease. Causes or triggers include:

Traumatic life events have been linked with an elevated risk in developing psychotic symptoms. Childhood trauma has specifically been shown to be a predictor of adolescent and adult psychosis. Approximately 65% of individuals with psychotic symptoms have experienced childhood trauma (e.g., physical or sexual abuse, physical or emotional neglect). Increased individual vulnerability toward psychosis may interact with traumatic experiences promoting an onset of future psychotic symptoms, particularly during sensitive developmental periods. Importantly, the relationship between traumatic life events and psychotic symptoms appears to be dose-dependent in which multiple traumatic life events accumulate, compounding symptom expression and severity. This suggests trauma prevention and early intervention may be an important target for decreasing the incidence of psychotic disorders and ameliorating its effects.

From a diagnostic standpoint, organic disorders were believed to be caused by physical illness affecting the brain (that is, psychiatric disorders secondary to other conditions) while functional disorders were considered disorders of the functioning of the mind in the absence of physical disorders (that is, primary psychological or psychiatric disorders). Subtle physical abnormalities have been found in illnesses traditionally considered functional, such as schizophrenia. The DSM-IV-TR avoids the functional/organic distinction, and instead lists traditional psychotic illnesses, psychosis due to general medical conditions, and substance-induced psychosis.

Primary psychiatric causes of psychosis include the following:

Psychotic symptoms may also be seen in:

Stress is known to contribute to and trigger psychotic states. A history of psychologically traumatic events, and the recent experience of a stressful event, can both contribute to the development of psychosis. Short-lived psychosis triggered by stress is known as brief reactive psychosis, and patients may spontaneously recover normal functioning within two weeks. In some rare cases, individuals may remain in a state of full-blown psychosis for many years, or perhaps have attenuated psychotic symptoms (such as low intensity hallucinations) present at most times.

Neuroticism is an independent predictor of the development of psychosis.

Subtypes of psychosis include:

Cycloid psychosis is a psychosis that progresses from normal to full-blown, usually between a few hours to days, not related to drug intake or brain injury. The cycloid psychosis has a long history in European psychiatry diagnosis. The term "cycloid psychosis" was first used by Karl Kleist in 1926. Despite the significant clinical relevance, this diagnosis is neglected both in literature and in nosology. The cycloid psychosis has attracted much interest in the international literature of the past 50 years, but the number of scientific studies have greatly decreased over the past 15 years, possibly partly explained by the misconception that the diagnosis has been incorporated in current diagnostic classification systems. The cycloid psychosis is therefore only partially described in the diagnostic classification systems used. Cycloid psychosis is nevertheless its own specific disease that is distinct from both the manic-depressive disorder, and from schizophrenia, and this despite the fact that the cycloid psychosis can include both bipolar (basic mood shifts) as well as schizophrenic symptoms. The disease is an acute, usually self-limiting, functionally psychotic state, with a very diverse clinical picture that almost consistently is characterized by the existence of some degree of confusion or distressing perplexity, but above all, of the multifaceted and diverse expressions the disease takes. The main features of the disease is thus that the onset is acute, contains the multifaceted picture of symptoms and typically reverses to a normal state and that the long-term prognosis is good. In addition, diagnostic criteria include at least four of the following symptoms:

Cycloid psychosis occurs in people of generally 15–50 years of age.

A very large number of medical conditions can cause psychosis, sometimes called "secondary psychosis". Examples include:

Various psychoactive substances (both legal and illegal) have been implicated in causing, exacerbating, or precipitating psychotic states or disorders in users, with varying levels of evidence. This may be upon intoxication for a more prolonged period after use, or upon withdrawal. Individuals who have a substance induced psychosis tend to have a greater awareness of their psychosis and tend to have higher levels of suicidal thinking compared to individuals who have a primary psychotic illness. Drugs commonly alleged to induce psychotic symptoms include alcohol, cannabis, cocaine, amphetamines, cathinones, psychedelic drugs (such as LSD and psilocybin), κ-opioid receptor agonists (such as enadoline and salvinorin A) and NMDA receptor antagonists (such as phencyclidine and ketamine). Caffeine may worsen symptoms in those with schizophrenia and cause psychosis at very high doses in people without the condition. Cannabis and other illicit recreational drugs are often associated with psychosis in adolescents and cannabis use before 15 years old may increase the risk of psychosis later in life as an adult.

Approximately three percent of people who are suffering from alcoholism experience psychosis during acute intoxication or withdrawal. Alcohol related psychosis may manifest itself through a kindling mechanism. The mechanism of alcohol-related psychosis is due to the long-term effects of alcohol consumption resulting in distortions to neuronal membranes, gene expression, as well as thiamin deficiency. It is possible in some cases that alcohol abuse via a kindling mechanism can cause the development of a chronic substance induced psychotic disorder, i.e. schizophrenia. The effects of an alcohol-related psychosis include an increased risk of depression and suicide as well as causing psychosocial impairments.

According to some studies, the more often cannabis is used the more likely a person is to develop a psychotic illness, with frequent use being correlated with twice the risk of psychosis and schizophrenia. While cannabis use is accepted as a contributory cause of schizophrenia by some, it remains controversial, with pre-existing vulnerability to psychosis emerging as the key factor that influences the link between cannabis use and psychosis. Some studies indicate that the effects of two active compounds in cannabis, tetrahydrocannabinol (THC) and cannabidiol (CBD), have opposite effects with respect to psychosis. While THC can induce psychotic symptoms in healthy individuals, CBD may reduce the symptoms caused by cannabis.

Cannabis use has increased dramatically over the past few decades whereas the rate of psychosis has not increased. Together, these findings suggest that cannabis use may hasten the onset of psychosis in those who may already be predisposed to psychosis. High-potency cannabis use indeed seems to accelerate the onset of psychosis in predisposed patients. A 2012 study concluded that cannabis plays an important role in the development of psychosis in vulnerable individuals, and that cannabis use in early adolescence should be discouraged.

Methamphetamine induces a psychosis in 26–46 percent of heavy users. Some of these people develop a long-lasting psychosis that can persist for longer than six months. Those who have had a short-lived psychosis from methamphetamine can have a relapse of the methamphetamine psychosis years later after a stressful event such as severe insomnia or a period of heavy alcohol abuse despite not relapsing back to methamphetamine. Individuals who have a long history of methamphetamine abuse and who have experienced psychosis in the past from methamphetamine abuse are highly likely to re-experience methamphetamine psychosis if drug use is recommenced. Methamphetamine-induced psychosis is likely gated by genetic vulnerability, which can produce long-term changes in brain neurochemistry following repetitive use.

Administration, or sometimes withdrawal, of a large number of medications may provoke psychotic symptoms. Drugs that can induce psychosis experimentally or in a significant proportion of people include amphetamine and other sympathomimetics, dopamine agonists, ketamine, corticosteroids (often with mood changes in addition), and some anticonvulsants such as vigabatrin. Stimulants that may cause this include lisdexamfetamine.

Meditation may induce psychological side effects, including depersonalization, derealization and psychotic symptoms like hallucinations as well as mood disturbances.

The first brain image of an individual with psychosis was completed as far back as 1935 using a technique called pneumoencephalography (a painful and now obsolete procedure where cerebrospinal fluid is drained from around the brain and replaced with air to allow the structure of the brain to show up more clearly on an X-ray picture).

Both first episode psychosis, and high risk status is associated with reductions in grey matter volume (GMV). First episode psychotic and high risk populations are associated with similar but distinct abnormalities in GMV. Reductions in the right middle temporal gyrus, right superior temporal gyrus (STG), right parahippocampus, right hippocampus, right middle frontal gyrus, and left anterior cingulate cortex (ACC) are observed in high risk populations. Reductions in first episode psychosis span a region from the right STG to the right insula, left insula, and cerebellum, and are more severe in the right ACC, right STG, insula and cerebellum. 

Another meta analysis reported bilateral reductions in insula, operculum, STG, medial frontal cortex, and ACC, but also reported increased GMV in the right lingual gyrus and left precentral gyrus. The Kraepelinian dichotomy is made questionable by grey matter abnormalities in bipolar and schizophrenia; schizophrenia is distinguishable from bipolar in that regions of grey matter reduction are generally larger in magnitude, although adjusting for gender differences reduces the difference to the left dorsomedial prefrontal cortex, and right dorsolateral prefrontal cortex.

During attentional tasks, first episode psychosis is associated with hypoactivation in the right middle frontal gyrus, a region generally described as encompassing the dorsolateral prefrontal cortex (dlPFC). In congruence with studies on grey matter volume, hypoactivity in the right insula, and right inferior parietal lobe is also reported. During cognitive tasks, hypoactivities in the right insula, dACC, and the left precuneus, as well as reduced deactivations in the right basal ganglia, right thalamus, right inferior frontal and left precentral gyri are observed. These results are highly consistent and replicable possibly except the abnormalities of the right inferior frontal gyrus. Decreased grey matter volume in conjunction with bilateral hypoactivity is observed in anterior insula, dorsal medial frontal cortex, and dorsal ACC. Decreased grey matter volume and bilateral hyperactivity is reported in posterior insula, ventral medial frontal cortex, and ventral ACC.

Studies during acute experiences of hallucinations demonstrate increased activity in primary or secondary sensory cortices. As auditory hallucinations are most common in psychosis, most robust evidence exists for increased activity in the left middle temporal gyrus, left superior temporal gyrus, and left inferior frontal gyrus (i.e. Broca's area). Activity in the ventral striatum, hippocampus, and ACC are related to the lucidity of hallucinations, and indicate that activation or involvement of emotional circuitry are key to the impact of abnormal activity in sensory cortices. Together, these findings indicate abnormal processing of internally generated sensory experiences, coupled with abnormal emotional processing, results in hallucinations. One proposed model involves a failure of feedforward networks from sensory cortices to the inferior frontal cortex, which normally cancel out sensory cortex activity during internally generated speech. The resulting disruption in expected and perceived speech is thought to produce lucid hallucinatory experiences.

The two-factor model of delusions posits that dysfunction in both belief formation systems and belief evaluation systems are necessary for delusions. Dysfunction in evaluations systems localized to the right lateral prefrontal cortex, regardless of delusion content, is supported by neuroimaging studies and is congruent with its role in conflict monitoring in healthy persons. Abnormal activation and reduced volume is seen in people with delusions, as well as in disorders associated with delusions such as frontotemporal dementia, psychosis and Lewy body dementia. Furthermore, lesions to this region are associated with "jumping to conclusions", damage to this region is associated with post-stroke delusions, and hypometabolism this region associated with caudate strokes presenting with delusions.

The aberrant salience model suggests that delusions are a result of people assigning excessive importance to irrelevant stimuli. In support of this hypothesis, regions normally associated with the salience network demonstrate reduced grey matter in people with delusions, and the neurotransmitter dopamine, which is widely implicated in salience processing, is also widely implicated in psychotic disorders.

Specific regions have been associated with specific types of delusions. The volume of the hippocampus and parahippocampus is related to paranoid delusions in Alzheimer's disease, and has been reported to be abnormal post mortem in one person with delusions. Capgras delusions have been associated with occipito-temporal damage, and may be related to failure to elicit normal emotions or memories in response to faces.

Psychosis is associated with ventral striatal hypoactivity during reward anticipation and feedback. Hypoactivity in the left ventral striatum is correlated with the severity of negative symptoms. While anhedonia is a commonly reported symptom in psychosis, hedonic experiences are actually intact in most people with schizophrenia. The impairment that may present itself as anhedonia probably actually lies in the inability to identify goals, and to identify and engage in the behaviors necessary to achieve goals. Studies support a deficiency in the neural representation of goals and goal directed behavior by demonstrating that receipt (not anticipation) of reward is associated with a robust response in the ventral striatum; reinforcement learning is intact when contingencies about stimulus-reward are implicit, but not when they require explicit neural processing; reward prediction errors (during functional neuroimaging studies), particularly positive PEs are abnormal. A positive prediction error response occurs when there is an increased activation in a brain region, typically the striatum, in response to unexpected rewards. A negative prediction error response occurs when there is a decreased activation in a region when predicted rewards do not occur. ACC response, taken as an indicator of effort allocation, does not increase with reward or reward probability increase, and is associated with negative symptoms; deficits in dlPFC activity and failure to improve performance on cognitive tasks when offered monetary incentives are present; and dopamine mediated functions are abnormal.

Psychosis has been traditionally linked to the overactivity of the neurotransmitter dopamine. In particular to its effect in the mesolimbic pathway. The two major sources of evidence given to support this theory are that dopamine receptor D2 blocking drugs (i.e., antipsychotics) tend to reduce the intensity of psychotic symptoms, and that drugs that accentuate dopamine release, or inhibit its reuptake (such as amphetamines and cocaine) can trigger psychosis in some people (see stimulant psychosis).

NMDA receptor dysfunction has been proposed as a mechanism in psychosis. This theory is reinforced by the fact that dissociative NMDA receptor antagonists such as ketamine, PCP and dextromethorphan (at large overdoses) induce a psychotic state. The symptoms of dissociative intoxication are also considered to mirror the symptoms of schizophrenia, including negative symptoms. NMDA receptor antagonism, in addition to producing symptoms reminiscent of psychosis, mimics the neurophysiological aspects, such as reduction in the amplitude of P50, P300, and MMN evoked potentials. Hierarchical Bayesian neurocomputational models of sensory feedback, in agreement with neuroimaging literature, link NMDA receptor hypofunction to delusional or hallucinatory symptoms via proposing a failure of NMDA mediated top down predictions to adequately cancel out enhanced bottom up AMPA mediated predictions errors. Excessive prediction errors in response to stimuli that would normally not produce such a response is thought to root from conferring excessive salience to otherwise mundane events. Dysfunction higher up in the hierarchy, where representation is more abstract, could result in delusions. The common finding of reduced GAD67 expression in psychotic disorders may explain enhanced AMPA mediated signaling, caused by reduced GABAergic inhibition.

The connection between dopamine and psychosis is generally believed to be complex. While dopamine receptor D2 suppresses adenylate cyclase activity, the D1 receptor increases it. If D2-blocking drugs are administered, the blocked dopamine spills over to the D1 receptors. The increased adenylate cyclase activity affects genetic expression in the nerve cell, which takes time. Hence antipsychotic drugs take a week or two to reduce the symptoms of psychosis. Moreover, newer and equally effective antipsychotic drugs actually block slightly less dopamine in the brain than older drugs whilst also blocking 5-HT2A receptors, suggesting the 'dopamine hypothesis' may be oversimplified. Soyka and colleagues found no evidence of dopaminergic dysfunction in people with alcohol-induced psychosis and Zoldan et al. reported moderately successful use of ondansetron, a 5-HT receptor antagonist, in the treatment of levodopa psychosis in Parkinson's disease patients.

A review found an association between a first-episode of psychosis and prediabetes.

Prolonged or high dose use of psychostimulants can alter normal functioning, making it similar to the manic phase of bipolar disorder. NMDA antagonists replicate some of the so-called "negative" symptoms like thought disorder in subanesthetic doses (doses insufficient to induce anesthesia), and catatonia in high doses). Psychostimulants, especially in one already prone to psychotic thinking, can cause some "positive" symptoms, such as delusional beliefs, particularly those persecutory in nature.

To make a diagnosis of a mental illness in someone with psychosis other potential causes must be excluded. An initial assessment includes a comprehensive history and physical examination by a health care provider. Tests may be done to exclude substance use, medication, toxins, surgical complications, or other medical illnesses. A person with psychosis is referred to as psychotic.

Delirium should be ruled out, which can be distinguished by visual hallucinations, acute onset and fluctuating level of consciousness, indicating other underlying factors, including medical illnesses. Excluding medical illnesses associated with psychosis is performed by using blood tests to measure:

Other investigations include:

Because psychosis may be precipitated or exacerbated by common classes of medications, medication-induced psychosis should be ruled out, particularly for first-episode psychosis. Both substance- and medication-induced psychosis can be excluded to a high level of certainty, using toxicology screening.

Because some dietary supplements may also induce psychosis or mania, but cannot be ruled out with laboratory tests, a psychotic individual's family, partner, or friends should be asked whether the patient is currently taking any dietary supplements.

Common mistakes made when diagnosing people who are psychotic include:


Only after relevant and known causes of psychosis are excluded, a mental health clinician may make a psychiatric differential diagnosis using a person's family history, incorporating information from the person with psychosis, and information from family, friends, or significant others.

Types of psychosis in psychiatric disorders may be established by formal rating scales. The Brief Psychiatric Rating Scale (BPRS) assesses the level of 18 symptom constructs of psychosis such as hostility, suspicion, hallucination, and grandiosity. It is based on the clinician's interview with the patient and observations of the patient's behavior over the previous 2–3 days. The patient's family can also answer questions on the behavior report. During the initial assessment and the follow-up, both positive and negative symptoms of psychosis can be assessed using the 30 item Positive and Negative Symptom Scale (PANSS).

The DSM-5 characterizes disorders as psychotic or on the schizophrenia spectrum if they involve hallucinations, delusions, disorganized thinking, grossly disorganized motor behavior, or negative symptoms. The DSM-5 does not include psychosis as a definition in the glossary, although it defines "psychotic features", as well as "psychoticism" with respect to personality disorder. The ICD-10 has no specific definition of psychosis.

Factor analysis of symptoms generally regarded as psychosis frequently yields a five factor solution, albeit five factors that are distinct from the five domains defined by the DSM-5 to encompass psychotic or schizophrenia spectrum disorders. The five factors are frequently labeled as hallucinations, delusions, disorganization, excitement, and emotional distress. The DSM-5 emphasizes a psychotic spectrum, wherein the low end is characterized by schizoid personality disorder, and the high end is characterized by schizophrenia.

The evidence for the effectiveness of early interventions to prevent psychosis appeared inconclusive. But psychosis caused by drugs can be prevented. Whilst early intervention in those with a psychotic episode might improve short-term outcomes, little benefit was seen from these measures after five years. However, there is evidence that cognitive behavioral therapy (CBT) may reduce the risk of becoming psychotic in those at high risk, and in 2014 the UK National Institute for Health and Care Excellence (NICE) recommended preventive CBT for people at risk of psychosis.

The treatment of psychosis depends on the specific diagnosis (such as schizophrenia, bipolar disorder or substance intoxication). The first-line treatment for many psychotic disorders is antipsychotic medication, which can reduce the positive symptoms of psychosis in about 7 to 14 days. For youth or adolescents, treatment options include medications, psychological interventions, and social interventions. 

The choice of which antipsychotic to use is based on benefits, risks, and costs. It is debatable whether, as a class, typical or atypical antipsychotics are better. Tentative evidence supports that amisulpride, olanzapine, risperidone and clozapine may be more effective for positive symptoms but result in more side effects. Typical antipsychotics have equal drop-out and symptom relapse rates to atypicals when used at low to moderate dosages. There is a good response in 40–50%, a partial response in 30–40%, and treatment resistance (failure of symptoms to respond satisfactorily after six weeks to two or three different antipsychotics) in 20% of people. Clozapine is an effective treatment for those who respond poorly to other drugs ("treatment-resistant" or "refractory" schizophrenia), but it has the potentially serious side effect of agranulocytosis (lowered white blood cell count) in less than 4% of people.

Most people on antipsychotics get side effects. People on typical antipsychotics tend to have a higher rate of extrapyramidal side effects while some atypicals are associated with considerable weight gain, diabetes and risk of metabolic syndrome; this is most pronounced with olanzapine, while risperidone and quetiapine are also associated with weight gain. Risperidone has a similar rate of extrapyramidal symptoms to haloperidol.

Psychological treatments such as acceptance and commitment therapy (ACT) are possibly useful in the treatment of psychosis, helping people to focus more on what they can do in terms of valued life directions despite challenging symptomology.

There are psychological interventions that seek to treat the symptoms of psychosis. In a 2019 review, nine classes of psychosocial interventions were identified: need adapted treatment, open dialogue, psychoanalysis/psychodynamic psychotherapy, major role therapy, soteria, psychosocial outpatient and inpatient treatment, milieu therapy, and CBT. This paper concluded that when on minimal or no medication "the overall evidence supporting the effectiveness of these interventions is generally weak".

Early intervention in psychosis is based on the observation that identifying and treating someone in the early stages of a psychosis can improve their longer term outcome. This approach advocates the use of an intensive multi-disciplinary approach during what is known as the critical period, where intervention is the most effective, and prevents the long-term morbidity associated with chronic psychotic illness.

The word "psychosis" was introduced to the psychiatric literature in 1841 by Karl Friedrich Canstatt in his work "Handbuch der Medizinischen Klinik". He used it as a shorthand for 'psychic neurosis'. At that time neurosis meant any disease of the nervous system, and Canstatt was thus referring to what was considered a psychological manifestation of brain disease. Ernst von Feuchtersleben is also widely credited as introducing the term in 1845, as an alternative to insanity and mania.

The term stems from Modern Latin "psychosis", "a giving soul or life to, animating, quickening" and that from Ancient Greek ψυχή ("psyche"), "soul" and the suffix -ωσις ("-osis"), in this case "abnormal condition".

In its adjective form "psychotic", references to psychosis can be found in both clinical and non-clinical discussions. However, in a non-clinical context, "psychotic" is generally used as a synonym for "insane".

The word was also used to distinguish a condition considered a disorder of the mind, as opposed to "neurosis", which was considered a disorder of the nervous system. The psychoses thus became the modern equivalent of the old notion of madness, and hence there was much debate on whether there was only one (unitary) or many forms of the new disease. One type of broad usage would later be narrowed down by Koch in 1891 to the 'psychopathic inferiorities'—later renamed abnormal personalities by Schneider.

The division of the major psychoses into manic depressive illness (now called bipolar disorder) and dementia praecox (now called schizophrenia) was made by Emil Kraepelin, who attempted to create a synthesis of the various mental disorders identified by 19th-century psychiatrists, by grouping diseases together based on classification of common symptoms. Kraepelin used the term 'manic depressive insanity' to describe the whole spectrum of mood disorders, in a far wider sense than it is usually used today.

In Kraepelin's classification this would include 'unipolar' clinical depression, as well as bipolar disorder and other mood disorders such as cyclothymia. These are characterised by problems with mood control and the psychotic episodes appear associated with disturbances in mood, and patients often have periods of normal functioning between psychotic episodes even without medication. Schizophrenia is characterized by psychotic episodes that appear unrelated to disturbances in mood, and most non-medicated patients show signs of disturbance between psychotic episodes.

Early civilizations considered madness a supernaturally inflicted phenomenon. Archaeologists have unearthed skulls with clearly visible drillings, some datable back to 5000 BC suggesting that trepanning was a common treatment for psychosis in ancient times. Written record of supernatural causes and resultant treatments can be traced back to the New Testament. Mark 5:8–13 describes a man displaying what would today be described as psychotic symptoms. Christ cured this "demonic madness" by casting out the demons and hurling them into a herd of swine. Exorcism is still utilized in some religious circles as a treatment for psychosis presumed to be demonic possession. A research study of out-patients in psychiatric clinics found that 30 percent of religious patients attributed the cause of their psychotic symptoms to evil spirits. Many of these patients underwent exorcistic healing rituals that, though largely regarded as positive experiences by the patients, had no effect on symptomology. Results did, however, show a significant worsening of psychotic symptoms associated with exclusion of medical treatment for coercive forms of exorcism.

The medical teachings of the fourth-century philosopher and physician Hippocrates of Cos proposed a natural, rather than supernatural, cause of human illness. In Hippocrates' work, the Hippocratic corpus, a holistic explanation for health and disease was developed to include madness and other "diseases of the mind." Hippocrates writes:

Hippocrates espoused a theory of humoralism wherein disease is resultant of a shifting balance in bodily fluids including blood, phlegm, black bile, and yellow bile. According to humoralism, each fluid or "humour" has temperamental or behavioral correlates. In the case of psychosis, symptoms are thought to be caused by an excess of both blood and yellow bile. Thus, the proposed surgical intervention for psychotic or manic behavior was bloodletting.

18th-century physician, educator, and widely considered "founder of American psychiatry", Benjamin Rush, also prescribed bloodletting as a first-line treatment for psychosis. Although not a proponent of humoralism, Rush believed that active purging and bloodletting were efficacious corrections for disruptions in the circulatory system, a complication he believed was the primary cause of "insanity". Although Rush's treatment modalities are now considered antiquated and brutish, his contributions to psychiatry, namely the biological underpinnings of psychiatric phenomenon including psychosis, have been invaluable to the field. In honor of such contributions, Benjamin Rush's image is in the official seal of the American Psychiatric Association.

Early 20th-century treatments for severe and persisting psychosis were characterized by an emphasis on shocking the nervous system. Such therapies include insulin shock therapy, cardiazol shock therapy, and electroconvulsive therapy. Despite considerable risk, shock therapy was considered highly efficacious in the treatment of psychosis including schizophrenia. The acceptance of high-risk treatments led to more invasive medical interventions including psychosurgery.

In 1888, Swiss psychiatrist Gottlieb Burckhardt performed the first medically sanctioned psychosurgery in which the cerebral cortex was excised. Although some patients showed improvement of symptoms and became more subdued, one patient died and several developed aphasia or seizure disorders. Burckhardt would go on to publish his clinical outcomes in a scholarly paper. This procedure was met with criticism from the medical community and his academic and surgical endeavors were largely ignored. In the late 1930s, Egas Moniz conceived the leucotomy (AKA prefrontal lobotomy) in which the fibers connecting the frontal lobes to the rest of the brain were severed. Moniz's primary inspiration stemmed from a demonstration by neuroscientists John Fulton and Carlyle's 1935 experiment in which two chimpanzees were given leucotomies and pre- and post-surgical behavior was compared. Prior to the leucotomy, the chimps engaged in typical behavior including throwing feces and fighting. After the procedure, both chimps were pacified and less violent. During the Q&A, Moniz asked if such a procedure could be extended to human subjects, a question that Fulton admitted was quite startling. Moniz would go on to extend the controversial practice to humans suffering from various psychotic disorders, an endeavor for which he received a Nobel Prize in 1949. Between the late 1930s and early 1970s, the leucotomy was a widely accepted practice, often performed in non-sterile environments such as small outpatient clinics and patient homes. Psychosurgery remained standard practice until the discovery of antipsychotic pharmacology in the 1950s.

The first clinical trial of antipsychotics (also commonly known as neuroleptics) for the treatment of psychosis took place in 1952. Chlorpromazine (brand name: Thorazine) passed clinical trials and became the first antipsychotic medication approved for the treatment of both acute and chronic psychosis. Although the mechanism of action was not discovered until 1963, the administration of chlorpromazine marked the advent of the dopamine antagonist, or first generation antipsychotic. While clinical trials showed a high response rate for both acute psychosis and disorders with psychotic features, the side effects were particularly harsh, which included high rates of often irreversible Parkinsonian symptoms such as tardive dyskinesia. With the advent of atypical antipsychotics (also known as second generation antipsychotics) came a dopamine antagonist with a comparable response rate but a far different, though still extensive, side-effect profile that included a lower risk of Parkinsonian symptoms but a higher risk of cardiovascular disease. Atypical antipsychotics remain the first-line treatment for psychosis associated with various psychiatric and neurological disorders including schizophrenia, bipolar disorder, major depressive disorder, anxiety disorders, dementia, and some autism spectrum disorders.

Dopamine is now one of the primary neurotransmitters implicated in psychotic symptomology. Blocking dopamine receptors (namely, the dopamine D2 receptors) and decreasing dopaminergic activity continues to be an effective but highly unrefined effect of antipsychotics, which are commonly used to treat psychosis. Recent pharmacological research suggests that the decrease in dopaminergic activity does not eradicate psychotic delusions or hallucinations, but rather attenuates the reward mechanisms involved in the development of delusional thinking; that is, connecting or finding meaningful relationships between unrelated stimuli or ideas. The author of this research paper acknowledges the importance of future investigation:
Freud's former student Wilhelm Reich explored independent insights into the physical effects of neurotic and traumatic upbringing, and published his holistic psychoanalytic treatment with a schizophrenic. With his incorporation of breathwork and insight with the patient, a young woman, she achieved sufficient self-management skills to end the therapy.

Lacan extended Freud's ideas to create a psychoanalytic model of psychosis based upon the concept of "foreclosure", the rejection of the symbolic concept of the father.

Psychiatrist David Healy has criticised pharmaceutical companies for promoting simplified biological theories of mental illness that seem to imply the primacy of pharmaceutical treatments while ignoring social and developmental factors that are known important influences in the etiology of psychosis.

Further research in the form of randomized controlled trials is needed to determine the effectiveness of treatment approaches for helping adolescents with psychosis.







</doc>
<doc id="24515" url="https://en.wikipedia.org/wiki?curid=24515" title="Paranoia">
Paranoia

Paranoia is an instinct or thought process which is believed to be heavily influenced by anxiety or fear, often to the point of delusion and irrationality. Paranoid thinking typically includes persecutory beliefs, or beliefs of conspiracy concerning a perceived threat towards oneself (i.e. the American colloquial phrase, ""Everyone is out to get me""). Paranoia is distinct from phobias, which also involve irrational fear, but usually no blame. Making false accusations and the general distrust of other people also frequently accompany paranoia. For example, a paranoid person might believe an incident was intentional when most people would view it as an accident or coincidence. Paranoia is a central symptom of psychosis.

A common symptom of paranoia is the attribution bias. These individuals typically have a biased perception of reality, often exhibiting more hostile beliefs. A paranoid person may view someone else's accidental behavior as though it is with intent or threatening.

An investigation of a non-clinical paranoid population found that feeling powerless and depressed, isolating oneself, and relinquishing activities are characteristics that could be associated with those exhibiting more frequent paranoia.
Some scientists have created different subtypes for the various symptoms of paranoia including erotic, persecutory, litigious, and exalted.

Due to the suspicious and troublesome personality traits of paranoia, it is unlikely that someone with paranoia will thrive in interpersonal relationships. Most commonly paranoid individuals tend to be of a single status. According to some research there is a hierarchy for paranoia. The least common types of paranoia at the very top of the hierarchy would be those involving more serious threats. Social anxiety is at the bottom of this hierarchy as the most frequently exhibited level of paranoia.

Social circumstances appear to be highly influential on paranoid beliefs. Based on data collected by means of a mental health survey distributed to residents of Ciudad Juárez, Chihuahua (in Mexico) and El Paso, Texas (in the United States), paranoid beliefs seem to be associated with feelings of powerlessness and victimization, enhanced by social situations. Potential causes of these effects included a sense of believing in external control, and mistrust which can be strengthened by lower socioeconomic status. Those living in a lower socioeconomic status may feel less in control of their own lives. In addition, this study explains that females have the tendency to believe in external control at a higher rate than males, potentially making females more susceptible to mistrust and the effects of socioeconomic status on paranoia.

Emanuel Messinger reports that surveys have revealed that those exhibiting paranoia can evolve from parental relationships and untrustworthy environments. These environments could include being very disciplinary, stringent, and unstable. It was even noted that, "indulging and pampering (thereby impressing the child that they are something special and warrants special privileges)," can be contributing backgrounds. Experiences likely to enhance or manifest the symptoms of paranoia include increased rates of disappointment, stress, and a hopeless state of mind.

Discrimination has also been reported as a potential predictor of paranoid delusions. Such reports that paranoia seemed to appear more in older patients who had experienced higher levels of discrimination throughout their lives. In addition to this it has been noted that immigrants are quite susceptible to forms of psychosis. This could be due to the aforementioned effects of discriminatory events and humiliation.

Many more mood-based symptoms, grandiosity and guilt, may underlie functional paranoia.

Colby (1981) defined "paranoid cognition" in terms of "persecutory delusions and false beliefs whose propositional content clusters around ideas of being harassed, threatened, harmed, subjugated, persecuted, accused, mistreated, wronged, tormented, disparaged, vilified, and so on, by malevolent others, either specific individuals or groups" (p. 518).
Three components of paranoid cognition have been identified by Robins & Post: "a) suspicions without enough basis that others are exploiting, harming, or deceiving them; b) preoccupation with unjustified doubts about the loyalty, or trustworthiness, of friends or associates; c) reluctance to confide in others because of unwarranted fear that the information will be used maliciously against them" (1997, p. 3).

Paranoid cognition has been conceptualized by clinical psychology almost exclusively in terms of psychodynamic constructs and dispositional variables. From this point of view, paranoid cognition is a manifestation of an intra-psychic conflict or disturbance. For instance, Colby (1981) suggested that the biases of blaming others for one’s problems serve to alleviate the distress produced by the feeling of being humiliated, and helps to repudiate the belief that the self is to blame for such incompetence. This intra-psychic perspective emphasizes that the cause of paranoid cognitions are inside the head of the people (social perceiver), and dismiss the fact that paranoid cognition may be related with the social context in which such cognitions are embedded. This point is extremely relevant because when origins of distrust and suspicion (two components of paranoid cognition) are studied many researchers have accentuated the importance of social interaction, particularly when social interaction has gone awry. Even more, a model of trust development pointed out that trust increases or decreases as a function of the cumulative history of interaction between two or more persons.

Another relevant difference can be discerned among "pathological and non-pathological forms of trust and distrust". According to Deutsch, the main difference is that non-pathological forms are flexible and responsive to changing circumstances. Pathological forms reflect exaggerated perceptual biases and judgmental predispositions that can arise and perpetuate them, are reflexively caused errors similar to a self-fulfilling prophecy.

It has been suggested that a "hierarchy" of paranoia exists, extending from mild social evaluative concerns, through ideas of social reference, to persecutory beliefs concerning mild, moderate, and severe threats.

A paranoid reaction may be caused from a decline in brain circulation as a result of high blood pressure or hardening of the arterial walls.

Drug-induced paranoia, associated with amphetamines, methamphetamine and similar stimulants has much in common with schizophrenic paranoia; the relationship has been under investigation since 2012. Drug-induced paranoia has a better prognosis than schizophrenic paranoia once the drug has been removed. For further information, see stimulant psychosis and substance-induced psychosis.

Based on data obtained by the Dutch NEMESIS project in 2005, there was an association between impaired hearing and the onset of symptoms of psychosis, which was based on a five-year follow up. Some older studies have actually declared that a state of paranoia can be produced in patients that were under a hypnotic state of deafness. This idea however generated much skepticism during its time.

In the DSM-IV-TR, paranoia is diagnosed in the form of:

According to clinical psychologist P. J. McKenna, "As a noun, paranoia denotes a disorder which has been argued in and out of existence, and whose clinical features, course, boundaries, and virtually every other aspect of which is controversial. Employed as an adjective, paranoid has become attached to a diverse set of presentations, from paranoid schizophrenia, through paranoid depression, to paranoid personality—not to mention a motley collection of paranoid 'psychoses', 'reactions', and 'states'—and this is to restrict discussion to functional disorders. Even when abbreviated down to the prefix para-, the term crops up causing trouble as the contentious but stubbornly persistent concept of paraphrenia".

At least 50% of the diagnosed cases of schizophrenia experience delusions of reference and delusions of persecution. Paranoia perceptions and behavior may be part of many mental illnesses, such as depression and dementia, but they are more prevalent in three mental disorders: paranoid schizophrenia, delusional disorder (persecutory type), and paranoid personality disorder.

The word "paranoia" comes from the Greek παράνοια ("paranoia"), "madness", and that from παρά ("para"), "beside, by" and νόος ("noos"), "mind". The term was used to describe a mental illness in which a delusional belief is the sole or most prominent feature. In this definition, the belief does not have to be persecutory to be classified as paranoid, so any number of delusional beliefs can be classified as paranoia. For example, a person who has the sole delusional belief that they are an important religious figure would be classified by Kraepelin as having 'pure paranoia'. The word “paranoia” is associated from the Greek word “para-noeo”. Its meaning was "derangement", or "departure from the normal". However, the word was used strictly and other words were used such as "insanity" or "crazy", as these words were introduced by Aurelius Cornelius Celsus. The term “paranoia” first made an appearance during plays of Greek tragedians, and was also used by sufficient individuals such as Plato and Hippocrates. Nevertheless, the word “paranoia” was the equivalent of “delirium” or “high fever”. Eventually, the term made its’ way out of everyday language for two millennia. “Paranoia” was soon revived as it made an appearance in the writings of the “nosologists”. It began to take appearance in France, with the writings of Rudolph August Vogel (1772) and Francois Boissier de Sauvage (1759).

According to Michael Phelan, Padraig Wright, and Julian Stern (2000), paranoia and paraphrenia are debated entities that were detached from dementia praecox by Kraepelin, who explained paranoia as a continuous systematized delusion arising much later in life with no presence of either hallucinations or a deteriorating course, paraphrenia as an identical syndrome to paranoia but with hallucinations. Even at the present time, a delusion need not be suspicious or fearful to be classified as paranoid. A person might be diagnosed with paranoid schizophrenia without delusions of persecution, simply because their delusions refer mainly to themselves.

It has generally been agreed upon that individuals with paranoid delusions will have the tendency to take action based on their beliefs. More research is needed on the particular types of actions that are pursued based on paranoid delusions. Some researchers have made attempts to distinguish the different variations of actions brought on as a result of delusions. Wessely et al. (1993) did just this by studying individuals with delusions of which more than half had reportedly taken action or behaved as a result of these delusions. However, the overall actions were not of a violent nature in most of the informants. The authors note that other studies such as one by Taylor (1985), have shown that violent behaviors were more common in certain types of paranoid individuals, mainly those considered to be offensive such as prisoners.

Other researchers have found associations between childhood abusive behaviors and the appearance of violent behaviors in psychotic individuals. This could be a result of their inability to cope with aggression as well as other people, especially when constantly attending to potential threats in their environment. The attention to threat itself has been proposed as one of the major contributors of violent actions in paranoid people, although there has been much deliberation about this as well. Other studies have shown that there may only be certain types of delusions that promote any violent behaviors, persecutory delusions seem to be one of these.

Having resentful emotions towards others and the inability to understand what other people are feeling seem to have an association with violence in paranoid individuals. This was based on a study of paranoid schizophrenics' (one of the common mental disorders that exhibit paranoid symptoms) theories of mind capabilities in relation to empathy. The results of this study revealed specifically that although the violent patients were more successful at the higher level theory of mind tasks, they were not as able to interpret others' emotions or claims.

Social psychological research has proposed a mild form of paranoid cognition, "paranoid social cognition", that has its origins in social determinants more than intra-psychic conflict. This perspective states that in milder forms, paranoid cognitions may be very common among normal individuals. For instance, it is not strange that people may exhibit in their daily life, self-centered thought such as they are being talked about, suspiciousness about other’ intentions, and assumptions of ill-will or hostility (i.e. people may feel as if everything is going against them). According to Kramer, (1998) these milder forms of paranoid cognition may be considered as an adaptive response to cope with or make sense of a disturbing and threatening social environment.

Paranoid cognition captures the idea that dysphoric self-consciousness may be related with the position that people occupy within a social system. This self-consciousness conduces to a hypervigilant and ruminative mode to process social information that finally will stimulate a variety of paranoid-like forms of social misperception and misjudgment. This model identifies four components that are essential to understanding paranoid social cognition: situational antecedents, dysphoric self-consciousness, hypervigilance and rumination, and judgmental biases.

Perceived social distinctiveness, perceived evaluative scrutiny and uncertainty about the social standing.

Refers to an aversive form of heightened 'public self-consciousness' characterized by the feelings that one is under intensive evaluation or scrutiny. Becoming self-tormenting will increase the odds of interpreting others' behaviors in a self-referential way.

Self-consciousness was characterized as an aversive psychological state. According to this model, people experiencing self-consciousness will be highly motivated to reduce it, trying to make sense of what they are experiencing. These attempts promote hypervigilance and rumination in a circular relationship: more hypervigilance generates more rumination, whereupon more rumination generates more hypervigilance. Hypervigilance can be thought of as a way to appraise threatening social information, but in contrast to adaptive vigilance, hypervigilance will produce elevated levels of arousal, fear, anxiety, and threat perception. Rumination is another possible response to threatening social information. Rumination can be related to the paranoid social cognition because it can increase negative thinking about negative events, and evoke a pessimistic explanatory style.

Three main judgmental consequences have been identified:






</doc>
<doc id="24516" url="https://en.wikipedia.org/wiki?curid=24516" title="Polybius">
Polybius

Polybius (; , "Polýbios"; –  BC) was a Greek historian of the Hellenistic period noted for his work , which covered the period of 264–146 BC in detail. The work describes the rise of the Roman Republic to the status of dominance in the ancient Mediterranean world. It includes his eyewitness account of the Sack of Carthage and Corinth in 146 BC, and the Roman annexation of mainland Greece after the Achaean War. 

Polybius is important for his analysis of the mixed constitution or the separation of powers in government, which was influential on Montesquieu's "The Spirit of the Laws" and the framers of the United States Constitution. He was also noted for witnessing the events that he recorded.

The leading expert on Polybius was F. W. Walbank (1909–2008), who for 50 years published studies related to him, including a long commentary of his "Histories" and a biography.

Polybius was born around 208 BC in Megalopolis, Arcadia, when it was an active member of the Achaean League. The town was revived, along with other Achaean states, a century before he was born. 

Polybius' father, Lycortas, was a prominent, land-owning politician and member of the governing class who became "strategos" (commanding general) of the Achaean League. Consequently, Polybius was able to observe first hand during his first 40 years the political and military affairs of Megalopolis, gaining experience as a statesman. In his early years, he accompanied his father while travelling as ambassador. He developed an interest in horse riding and hunting, diversions that later commended him to his Roman captors. 

In 182 BC, he was given quite an honor when he was chosen to carry the funeral urn of Philopoemen, one of the most eminent Achaean politicians of his generation. In either 169 BC or 170 BC, Polybius was elected hipparchus (cavalry officer) with the intention of fighting for Rome during the Third Macedonian War. This event often presaged election to the annual "strategia" (chief generalship). His early political career was devoted largely towards maintaining the independence of Megalopolis.

Polybius’ father, Lycortas, was a prominent advocate of neutrality during the Roman war against Perseus of Macedon. Lycortas attracted the suspicion of the Romans, and Polybius subsequently was one of the 1,000 Achaean nobles who were transported to Rome as hostages in 167 BC, and was detained there for 17 years. In Rome, by virtue of his high culture, Polybius was admitted to the most distinguished houses, in particular to that of Lucius Aemilius Paullus Macedonicus, the conqueror in the Third Macedonian War, who entrusted Polybius with the education of his sons, Fabius and Scipio Aemilianus (who had been adopted by the eldest son of Scipio Africanus). Polybius remained on cordial terms with his former pupil Scipio Aemilianus and was among the members of the Scipionic Circle. 

When Scipio defeated the Carthaginians in the Third Punic War, Polybius remained his counsellor. The Achaean hostages were released in 150 BC, and Polybius was granted leave to return home, but the next year he went on campaign with Scipio Aemilianus to Africa, and was present at the Sack of Carthage in 146, which he later described. Following the destruction of Carthage, Polybius likely journeyed along the Atlantic coast of Africa, as well as Spain.

After the destruction of Corinth in the same year, Polybius returned to Greece, making use of his Roman connections to lighten the conditions there. Polybius was charged with the difficult task of organizing the new form of government in the Greek cities, and in this office he gained great recognition.

In the succeeding years, Polybius resided in Rome, completing his historical work while occasionally undertaking long journeys through the Mediterranean countries in the furtherance of his history, in particular with the aim of obtaining firsthand knowledge of historical sites. He apparently interviewed veterans to clarify details of the events he was recording and was similarly given access to archival material. Little is known of Polybius' later life; he most likely accompanied Scipio to Spain, acting as his military advisor during the Numantine War. 

He later wrote about this war in a lost monograph. Polybius probably returned to Greece later in his life, as evidenced by the many existent inscriptions and statues of him there. The last event mentioned in his "Histories" seems to be the construction of the Via Domitia in southern France in 118 BC, which suggests the writings of Pseudo-Lucian may have some grounding in fact when they state, "[Polybius] fell from his horse while riding up from the country, fell ill as a result and died at the age of eighty-two".

Polybius’ "Histories" cover the period from 264 BC to 146 BC. Its main focus is the period from 220 BC to 167 BC, describing Rome's efforts in subduing its arch-enemy, Carthage, and thereby becoming the dominant Mediterranean force. Books I through V of "The Histories" are the introduction for the years during his lifetime, describing the politics in leading Mediterranean states, including ancient Greece and Egypt, and culminating in their ultimate "συμπλοκή" or interconnectedness. In Book VI, Polybius describes the political, military, and moral institutions that allowed the Romans to succeed. He describes the First and Second Punic Wars. Polybius concludes the Romans are the pre-eminent power because they have customs and institutions which promote a deep desire for noble acts, a love of virtue, piety towards parents and elders, and a fear of the gods ("deisidaimonia"). 

He also chronicled the conflicts between Hannibal and Publius Cornelius Scipio Africanus such as the Battle of Ticinus, the Battle of the Trebia, the Siege of Saguntum, the Battle of Lilybaeum, and the Battle of Rhone Crossing. In Book XII, Polybius discusses the worth of Timaeus’ account of the same period of history. He asserts Timaeus' point of view is inaccurate, invalid, and biased in favor of Rome. Therefore, Polybius's "Histories" is also useful in analyzing the different Hellenistic versions of history and of use as a credible illustration of actual events during the Hellenistic period.

In the twelfth volume of his "Histories", Polybius defines the historian's job as the analysis of documentation, the review of relevant geographical information, and political experience. Polybius held that historians should only chronicle events whose participants the historian was able to interview, and was among the first to champion the notion of factual integrity in historical writing. In Polybius' time, the profession of a historian required political experience (which aided in differentiating between fact and fiction) and familiarity with the geography surrounding one's subject matter to supply an accurate version of events. 

Polybius himself exemplified these principles as he was well travelled and possessed political and military experience. He did not neglect written sources that provided essential material for his histories of the period from 264 BC to 220 BC. When addressing events after 220 BC, he examined the writings of Greek and Roman historians to acquire credible sources of information, but rarely did he name those sources.

Polybius wrote several works, the majority of which are lost. His earliest work was a biography of the Greek statesman Philopoemen; this work was later used as a source by Plutarch when composing his "Parallel Lives", however the original Polybian text is lost. In addition, Polybius wrote an extensive treatise entitled "Tactics", which may have detailed Roman and Greek military tactics. Small parts of this work may survive in his major "Histories", but the work itself is lost, as well. Another missing work was a historical monograph on the events of the Numantine War. The largest Polybian work was, of course, his "Histories", of which only the first five books survive entirely intact, along with a large portion of the sixth book and fragments of the rest. Along with Cato the Elder (234–149 BC), he can be considered one of the founding fathers of Roman historiography.

Livy made reference to and uses Polybius' "Histories" as source material in his own narrative. Polybius was among the first historians to attempt to present history as a sequence of causes and effects, based upon a careful examination and criticism of tradition. He narrated his history based upon first-hand knowledge. "The Histories" capture the varied elements of the story of human behavior: nationalism, xenophobia, duplicitous politics, war, brutality, loyalty, valour, intelligence, reason, and resourcefulness.

Aside from the narrative of the historical events, Polybius also included three books of digressions. Book 34 was entirely devoted to questions of geography and included some trenchant criticisms of Eratosthenes, whom he accused of passing on popular preconceptions or "laodogmatika". Book 12 was a disquisition on the writing of history, citing extensive passages of lost historians, such as Callisthenes and Theopompus. Most influential was Book 6, which describes Roman political, military, and moral institutions, which he considered key to Rome's success; it presented Rome as having a mixed constitution in which monarchical, aristocratic, and popular elements existed in stable equilibrium. This enabled Rome to escape, for the time being, the cycle of eternal revolutions ("anacyclosis"). While Polybius was not the first to advance this view, his account provides the most cogent illustration of the ideal for later political theorists.

A key theme of "The Histories" is the good statesman as virtuous and composed. The character of the Polybian statesman is exemplified in that of Philip II. His beliefs about Philip's character led Polybius to reject historian Theopompus' description of Philip's private, drunken debauchery. For Polybius, it was inconceivable that such an able and effective statesman could have had an immoral and unrestrained private life as described by Theopompus.
In recounting the Roman Republic, Polybius stated that "the Senate stands in awe of the multitude, and cannot neglect the feelings of the people".

Other important themes running through "The Histories" are the role of Fortune in the affairs of nations, his insistence that history should be demonstratory, or "apodeiktike", providing lessons for statesmen, and that historians should be "men of action" ("pragmatikoi").

Polybius is considered by some to be the successor of Thucydides in terms of objectivity and critical reasoning, and the forefather of scholarly, painstaking historical research in the modern scientific sense. According to this view, his work sets forth the course of history's occurrences with clearness, penetration, sound judgment, and, among the circumstances affecting the outcomes, he lays special emphasis on geographical conditions. Modern historians are especially impressed with the manner in which Polybius used his sources, particularly documentary evidence as well as his citation and quotation of sources. Furthermore, there is some admiration of Polybius's meditation on the nature of historiography in Book 12. His work belongs, therefore, amongst the greatest productions of ancient historical writing. The writer of the "Oxford Companion to Classical Literature" (1937) praises him for his "earnest devotion to truth" and his systematic pursuit of causation.

It has long been acknowledged that Polybius's writings are prone to a certain hagiographic tone when writing of his friends, such as Scipio, and subject to a vindictive tone when detailing the exploits of his enemies, such as Callicrates, the Achaean statesman responsible for his Roman exile.

As a hostage in Rome, then as client to the Scipios, and after 146 BC, a collaborator with Roman rule, Polybius was probably in no position to freely express any negative opinions of Rome. Peter Green advises that Polybius was chronicling Roman history for a Greek audience, to justify what he believed to be the inevitability of Roman rule. Nonetheless, Green considers Polybius's "Histories" the best source for the era they cover. For Ronald Mellor, Polybius was a loyal partisan of Scipio, intent on vilifying his patron's opponents. Adrian Goldsworthy, while using Polybius as a source for Scipio's generalship, notes Polybius' underlying and overt bias in Scipio's favour. H. Ormerod considers that Polybius cannot be regarded as an 'altogether unprejudiced witness' in relation to his "betes noires"; the Aetolians, the Carthaginians, and the Cretans. Other historians perceive considerable negative bias in Polybius' account of Crete; on the other hand, Hansen notes that the same work, along with passages from Strabo and Scylax, proved a reliable guide in the eventual rediscovery of the lost city of Kydonia.

Polybius was responsible for a useful tool in telegraphy that allowed letters to be easily signaled using a numerical system (mentioned in Hist. X.45.6 ff.). This idea also lends itself to cryptographic manipulation and steganography.

This was known as the "Polybius square", where the letters of the alphabet were arranged left to right, top to bottom in a 5 x 5 square, (when used with the modern 26 letter alphabet, the letters "I" and "J" are combined). Five numbers were then aligned on the outside top of the square, and five numbers on the left side of the square vertically. Usually these numbers were arranged 1 through 5. By cross-referencing the two numbers along the grid of the square, a letter could be deduced.

In "The Histories", he specifies how this cypher could be used in fire signals, where long-range messages could be sent by means of torches raised and lowered to signify the column and row of each letter. This was a great leap forward from previous fire signaling, which could send prearranged codes only (such as, 'if we light the fire, it means that the enemy has arrived').

Other writings of scientific interest include detailed discussions of the machines Archimedes created for the defense of Syracuse against the Romans, where he praises the 'old man' and his engineering in the highest terms, and an analysis of the usefulness of astronomy to generals (both in the "Histories").

Polybius was considered a poor stylist by Dionysius of Halicarnassus, writing of Polybius' history that "no one has the endurance to reach [its] end". Nevertheless, clearly he was widely read by Romans and Greeks alike. He is quoted extensively by Strabo writing in the 1st century BC and Athenaeus in the 3rd century AD. 

His emphasis on explaining causes of events, rather than just recounting events, influenced the historian Sempronius Asellio. Polybius is mentioned by Cicero and mined for information by Diodorus, Livy, Plutarch and Arrian. Much of the text that survives today from the later books of "The Histories" was preserved in Byzantine anthologies.
His works reappeared in the West first in Renaissance Florence. Polybius gained a following in Italy, and although poor Latin translations hampered proper scholarship on his works, they contributed to the city's historical and political discourse. Niccolò Machiavelli in his "Discourses on Livy" evinces familiarity with Polybius. Vernacular translations in French, German, Italian and English first appeared during the 16th century. Consequently, in the late 16th century, Polybius's works found a greater reading audience among the learned public. Study of the correspondence of such men as Isaac Casaubon, Jacques Auguste de Thou, William Camden, and Paolo Sarpi reveals a growing interest in Polybius' works and thought during the period. Despite the existence of both printed editions in the vernacular and increased scholarly interest, however, Polybius remained an "historian's historian", not much read by the public at large. 

Printings of his work in the vernacular remained few in number — seven in French, five in English, and five in Italian.
Polybius' political analysis has influenced republican thinkers from Cicero to Charles de Montesquieu to the Founding Fathers of the United States. John Adams, for example, considered him one of the most important teachers of constitutional theory. Since the Age of Enlightenment, Polybius has in general held appeal to those interested in Hellenistic Greece and early Republican Rome, while his political and military writings have lost influence in academia. More recently, thorough work on the Greek text of Polybius, and his historical technique, has increased the academic understanding and appreciation of him as a historian.

According to Edward Tufte, he was also a major source for Charles Joseph Minard's figurative map of Hannibal's overland journey into Italy during the Second Punic War.

In his "Meditations On Hunting", Spanish philosopher José Ortega y Gasset calls Polybius "one of the few great minds that the turbid human species has managed to produce", and says the damage to the "Histories" is "without question one of the gravest losses that we have suffered in our Greco-Roman heritage".

The Italian version of his name, Polibio, was used as a male first name - for example, the composer Polibio Fumagalli - though it never became very common.

The University of Pennsylvania has an intellectual society, the Polybian Society, which is named in his honor and serves as a non-partisan forum for discussing societal issues and policy.








</doc>
