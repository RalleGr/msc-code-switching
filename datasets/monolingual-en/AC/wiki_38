<doc id="27692" url="https://en.wikipedia.org/wiki?curid=27692" title="Steam engine">
Steam engine

A steam engine is a heat engine that performs mechanical work using steam as its working fluid. The steam engine uses the force produced by steam pressure to push a piston back and forth inside a cylinder. This pushing force is transformed, by a connecting rod and flywheel, into rotational force for work. The term "steam engine" is generally applied only to reciprocating engines as just described, not to the steam turbine.

Steam engines are external combustion engines, where the working fluid is separated from the combustion products. The ideal thermodynamic cycle used to analyze this process is called the Rankine cycle.

In general usage, the term "steam engine" can refer to either complete steam plants (including boilers etc.) such as railway steam locomotives and portable engines, or may refer to the piston or turbine machinery alone, as in the beam engine and stationary steam engine.

Steam-driven devices were known as early as the aeolipile in the first century AD, with a few other uses recorded in the 16th and 17th century. Thomas Savery's dewatering pump used steam pressure operating directly on the water. The first commercially successful engine that could transmit continuous power to a machine was developed in 1712 by Thomas Newcomen. James Watt made a critical improvement by removing spent steam to a separate vessel for condensation, greatly improving the amount of work obtained per unit of fuel consumed. By the 19th century, stationary steam engines powered the factories of the Industrial Revolution. Steam engines replaced sail for ships, and steam locomotives operated on the railways.

Reciprocating piston type steam engines were the dominant source of power until the early 20th century, when advances in the design of electric motors and internal combustion engines gradually resulted in the replacement of reciprocating (piston) steam engines in commercial usage. Steam turbines replaced reciprocating engines in power generation, due to lower cost, higher operating speed, and higher efficiency.

The first recorded rudimentary steam-powered "engine" was the aeolipile described by Hero of Alexandria, a mathematician and engineer in Roman Egypt in the first century AD. In the following centuries, the few steam-powered "engines" known were, like the aeolipile, essentially experimental devices used by inventors to demonstrate the properties of steam. A rudimentary steam turbine device was described by Taqi al-Din in Ottoman Egypt in 1551 and by Giovanni Branca in Italy in 1629. Jerónimo de Ayanz y Beaumont received patents in 1606 for 50 steam-powered inventions, including a water pump for draining inundated mines. Denis Papin, a Huguenot refugee, did some useful work on the steam digester in 1679, and first used a piston to raise weights in 1690.

The first commercial steam-powered device was a water pump, developed in 1698 by Thomas Savery. It used condensing steam to create a vacuum which raised water from below and then used steam pressure to raise it higher. Small engines were effective though larger models were problematic. They had a limited lift height and were prone to boiler explosions. Savery's engine was used in mines, pumping stations and supplying water to water wheels that powered textile machinery. Savery's engine was of low cost. Bento de Moura Portugal introduced an improvement of Savery's construction "to render it capable of working itself", as described by John Smeaton in the Philosophical Transactions published in 1751. It continued to be manufactured until the late 18th century. One engine was still known to be operating in 1820.

The first commercially successful engine that could transmit continuous power to a machine was the atmospheric engine, invented by Thomas Newcomen around 1712. It improved on Savery's steam pump, using a piston as proposed by Papin. Newcomen's engine was relatively inefficient, and mostly used for pumping water. It worked by creating a partial vacuum by condensing steam under a piston within a cylinder. It was employed for draining mine workings at depths hitherto impossible, and for providing reusable water for driving waterwheels at factories sited away from a suitable "head". Water that passed over the wheel was pumped up into a storage reservoir above the wheel.
In 1780 James Pickard patented the use of a flywheel and crankshaft to provide rotative motion from an improved Newcomen engine.

In 1720, Jacob Leupold described a two-cylinder high-pressure steam engine. The invention was published in his major work "Theatri Machinarum Hydraulicarum". The engine used two heavy pistons to provide motion to a water pump. Each piston was raised by the steam pressure and returned to its original position by gravity. The two pistons shared a common four-way rotary valve connected directly to a steam boiler.
The next major step occurred when James Watt developed (1763–1775) an improved version of Newcomen's engine, with a separate condenser. Boulton and Watt's early engines used half as much coal as John Smeaton's improved version of Newcomen's. Newcomen's and Watt's early engines were "atmospheric". They were powered by air pressure pushing a piston into the partial vacuum generated by condensing steam, instead of the pressure of expanding steam. The engine cylinders had to be large because the only usable force acting on them was atmospheric pressure.

Watt developed his engine further, modifying it to provide a rotary motion suitable for driving machinery. This enabled factories to be sited away from rivers, and accelerated the pace of the Industrial Revolution.

The meaning of high pressure, together with an actual value above ambient, depends on the era in which the term was used. For early use of the term Van Reimsdijk refers to steam being at a sufficiently high pressure that it could be exhausted to atmosphere without reliance on a vacuum to enable it to perform useful work. states that Watt's condensing engines were known, at the time, as low pressure compared to high pressure, non-condensing engines of the same period.

Watt's patent prevented others from making high pressure and compound engines. Shortly after Watt's patent expired in 1800, Richard Trevithick and, separately, Oliver Evans in 1801 introduced engines using high-pressure steam; Trevithick obtained his high-pressure engine patent in 1802, and Evans had made several working models before then. These were much more powerful for a given cylinder size than previous engines and could be made small enough for transport applications. Thereafter, technological developments and improvements in manufacturing techniques (partly brought about by the adoption of the steam engine as a power source) resulted in the design of more efficient engines that could be smaller, faster, or more powerful, depending on the intended application.

The Cornish engine was developed by Trevithick and others in the 1810s. It was a compound cycle engine that used high-pressure steam expansively, then condensed the low-pressure steam, making it relatively efficient. The Cornish engine had irregular motion and torque though the cycle, limiting it mainly to pumping. Cornish engines were used in mines and for water supply until the late 19th century.

Early builders of stationary steam engines considered that horizontal cylinders would be subject to excessive wear. Their engines were therefore arranged with the piston axis vertical. In time the horizontal arrangement became more popular, allowing compact, but powerful engines to be fitted in smaller spaces.

The acme of the horizontal engine was the Corliss steam engine, patented in 1849, which was a four-valve counter flow engine with separate steam admission and exhaust valves and automatic variable steam cutoff. When Corliss was given the Rumford Medal, the committee said that "no one invention since Watt's time has so enhanced the efficiency of the steam engine". In addition to using 30% less steam, it provided more uniform speed due to variable steam cut off, making it well suited to manufacturing, especially cotton spinning.

The first experimental road-going steam-powered vehicles were built in the late 18th century, but it was not until after Richard Trevithick had developed the use of high-pressure steam, around 1800, that mobile steam engines became a practical proposition. The first half of the 19th century saw great progress in steam vehicle design, and by the 1850s it was becoming viable to produce them on a commercial basis. This progress was dampened by legislation which limited or prohibited the use of steam-powered vehicles on roads. Improvements in vehicle technology continued from the 1860s to the 1920s. Steam road vehicles were used for many applications. In the 20th century, the rapid development of internal combustion engine technology led to the demise of the steam engine as a source of propulsion of vehicles on a commercial basis, with relatively few remaining in use beyond the Second World War. Many of these vehicles were acquired by enthusiasts for preservation, and numerous examples are still in existence. In the 1960s, the air pollution problems in California gave rise to a brief period of interest in developing and studying steam-powered vehicles as a possible means of reducing the pollution. Apart from interest by steam enthusiasts, the occasional replica vehicle, and experimental technology, no steam vehicles are in production at present.

Near the end of the 19th century, compound engines came into widespread use. Compound engines exhausted steam into successively larger cylinders to accommodate the higher volumes at reduced pressures, giving improved efficiency. These stages were called expansions, with double- and triple-expansion engines being common, especially in shipping where efficiency was important to reduce the weight of coal carried. Steam engines remained the dominant source of power until the early 20th century, when advances in the design of the steam turbine, electric motors and internal combustion engines gradually resulted in the replacement of reciprocating (piston) steam engines, with shipping in the 20th-century relying upon the steam turbine.

As the development of steam engines progressed through the 18th century, various attempts were made to apply them to road and railway use. In 1784, William Murdoch, a Scottish inventor, built a model steam road locomotive. An early working model of a steam rail locomotive was designed and constructed by steamboat pioneer John Fitch in the United States probably during the 1780s or 1790s.
His steam locomotive used interior bladed wheels guided by rails or tracks.
The first full-scale working railway steam locomotive was built by Richard Trevithick in the United Kingdom and, on 21 February 1804, the world's first railway journey took place as Trevithick's unnamed steam locomotive hauled a train along the tramway from the Pen-y-darren ironworks, near Merthyr Tydfil to Abercynon in south Wales. The design incorporated a number of important innovations that included using high-pressure steam which reduced the weight of the engine and increased its efficiency. Trevithick visited the Newcastle area later in 1804 and the colliery railways in north-east England became the leading centre for experimentation and development of steam locomotives.

Trevithick continued his own experiments using a trio of locomotives, concluding with the Catch Me Who Can in 1808. Only four years later, the successful twin-cylinder locomotive "Salamanca" by Matthew Murray was used by the edge railed rack and pinion Middleton Railway. In 1825 George Stephenson built the "Locomotion" for the Stockton and Darlington Railway. This was the first public steam railway in the world and then in 1829, he built "The Rocket" which was entered in and won the Rainhill Trials. The Liverpool and Manchester Railway opened in 1830 making exclusive use of steam power for both passenger and freight trains.

Steam locomotives continued to be manufactured until the late twentieth century in places such as China and the former East Germany (where the DR Class 52.80 was produced).

The final major evolution of the steam engine design was the use of steam turbines starting in the late part of the 19th century. Steam turbines are generally more efficient than reciprocating piston type steam engines (for outputs above several hundred horsepower), have fewer moving parts, and provide rotary power directly instead of through a connecting rod system or similar means. Steam turbines virtually replaced reciprocating engines in electricity generating stations early in the 20th century, where their efficiency, higher speed appropriate to generator service, and smooth rotation were advantages. Today most electric power is provided by steam turbines. In the United States, 90% of the electric power is produced in this way using a variety of heat sources. Steam turbines were extensively applied for propulsion of large ships throughout most of the 20th century.

Although the reciprocating steam engine is no longer in widespread commercial use, various companies are exploring or exploiting the potential of the engine as an alternative to internal combustion engines. The company Energiprojekt AB in Sweden has made progress in using modern materials for harnessing the power of steam. The efficiency of Energiprojekt's steam engine reaches some 27–30% on high-pressure engines. It is a single-step, 5-cylinder engine (no compound) with superheated steam and consumes approx. of steam per kWh.

There are two fundamental components of a steam plant: the boiler or steam generator, and the "motor unit", referred to itself as a "steam engine". Stationary steam engines in fixed buildings may have the boiler and engine in separate buildings some distance apart. For portable or mobile use, such as steam locomotives, the two are mounted together.

The widely used reciprocating engine typically consisted of a cast-iron cylinder, piston, connecting rod and beam or a crank and flywheel, and miscellaneous linkages. Steam was alternately supplied and exhausted by one or more valves. Speed control was either automatic, using a governor, or by a manual valve. The cylinder casting contained steam supply and exhaust ports.

Engines equipped with a condenser are a separate type than those that exhaust to the atmosphere.

Other components are often present; pumps (such as an injector) to supply water to the boiler during operation, condensers to recirculate the water and recover the latent heat of vaporisation, and superheaters to raise the temperature of the steam above its saturated vapour point, and various mechanisms to increase the draft for fireboxes. When coal is used, a chain or screw stoking mechanism and its drive engine or motor may be included to move the fuel from a supply bin (bunker) to the firebox.

The heat required for boiling the water and raising the temperature of the steam can be derived from various sources, most commonly from burning combustible materials with an appropriate supply of air in a closed space (e.g., combustion chamber, firebox, furnace). In the case of model or toy steam engines and a few full scale cases, the heat source can be an electric heating element.

Boilers are pressure vessels that contain water to be boiled, and features that transfer the heat to the water as effectively as possible.

The two most common types are:

Fire-tube boilers were the main type used for early high-pressure steam (typical steam locomotive practice), but they were to a large extent displaced by more economical water tube boilers in the late 19th century for marine propulsion and large stationary applications.

Many boilers raise the temperature of the steam after it has left that part of the boiler where it is in contact with the water. Known as superheating it turns 'wet steam' into 'superheated steam'. It avoids the steam condensing in the engine cylinders, and gives a significantly higher efficiency.

In a steam engine, a piston or steam turbine or any other similar device for doing mechanical work takes a supply of steam at high pressure and temperature and gives out a supply of steam at lower pressure and temperature, using as much of the difference in steam energy as possible to do mechanical work.

These "motor units" are often called 'steam engines' in their own right. Engines using compressed air or other gases differ from steam engines only in details that depend on the nature of the gas although compressed air has been used in steam engines without change.

As with all heat engines, the majority of primary energy must be emitted as waste heat at relatively low temperature.

The simplest cold sink is to vent the steam to the environment. This is often used on steam locomotives to avoid the weight and bulk of condensers. Some of the released steam is vented up the chimney so as to increase the draw on the fire, which greatly increases engine power, but reduces efficiency.

Sometimes the waste heat from the engine is useful itself, and in those cases, very high overall efficiency can be obtained.

Steam engines in stationary power plants use surface condensers as a cold sink. The condensers are cooled by water flow from oceans, rivers, lakes, and often by cooling towers which evaporate water to provide cooling energy removal. The resulting condensed hot water ("condensate"), is then pumped back up to pressure and sent back to the boiler. A dry-type cooling tower is similar to an automobile radiator and is used in locations where water is costly. Waste heat can also be ejected by evaporative (wet) cooling towers, which use a secondary external water circuit that evaporates some of flow to the air.

River boats initially used a jet condenser in which cold water from the river is injected into the exhaust steam from the engine. Cooling water and condensate mix. While this was also applied for sea-going vessels, generally after only a few days of operation the boiler would become coated with deposited salt, reducing performance and increasing the risk of a boiler explosion. Starting about 1834, the use of surface condensers on ships eliminated fouling of the boilers, and improved engine efficiency.

Evaporated water cannot be used for subsequent purposes (other than rain somewhere), whereas river water can be re-used. In all cases, the steam plant boiler feed water, which must be kept pure, is kept separate from the cooling water or air.

Most steam engines have a means to supply boiler water whilst at pressure, so that they may be run continuously. Utility and industrial boilers commonly use multi-stage centrifugal pumps; however, other types are used. Another means of supplying lower-pressure boiler feed water is an injector, which uses a steam jet usually supplied from the boiler. Injectors became popular in the 1850s but are no longer widely used, except in applications such as steam locomotives. It is the pressurization of the water that circulates through the steam boiler that allows the water to be raised to temperatures well above boiling point of water at one atmospheric pressure, and by that means to increase the efficiency of the steam cycle.

For safety reasons, nearly all steam engines are equipped with mechanisms to monitor the boiler, such as a pressure gauge and a sight glass to monitor the water level.

Many engines, stationary and mobile, are also fitted with a governor to regulate the speed of the engine without the need for human interference.

The most useful instrument for analyzing the performance of steam engines is the steam engine indicator. Early versions were in use by 1851, but the most successful indicator was developed for the high speed engine inventor and manufacturer Charles Porter by Charles Richard and exhibited at London Exhibition in 1862. The steam engine indicator traces on paper the pressure in the cylinder throughout the cycle, which can be used to spot various problems and calculate developed horsepower. It was routinely used by engineers, mechanics and insurance inspectors. The engine indicator can also be used on internal combustion engines. See image of indicator diagram below (in "Types of motor units" section).

The centrifugal governor was adopted by James Watt for use on a steam engine in 1788 after Watt's partner Boulton saw one on the equipment of a flour mill Boulton & Watt were building. The governor could not actually hold a set speed, because it would assume a new constant speed in response to load changes. The governor was able to handle smaller variations such as those caused by fluctuating heat load to the boiler. Also, there was a tendency for oscillation whenever there was a speed change. As a consequence, engines equipped only with this governor were not suitable for operations requiring constant speed, such as cotton spinning. The governor was improved over time and coupled with variable steam cut off, good speed control in response to changes in load was attainable near the end of the 19th century.

In a simple engine, or "single expansion engine" the charge of steam passes through the entire expansion process in an individual cylinder, although a simple engine may have one or more individual cylinders. It is then exhausted directly into the atmosphere or into a condenser. As steam expands in passing through a high-pressure engine, its temperature drops because no heat is being added to the system; this is known as adiabatic expansion and results in steam entering the cylinder at high temperature and leaving at lower temperature. This causes a cycle of heating and cooling of the cylinder with every stroke, which is a source of inefficiency.

The dominant efficiency loss in reciprocating steam engines is cylinder condensation and re-evaporation. The steam cylinder and adjacent metal parts/ports operate at a temperature about halfway between the steam admission saturation temperature and the saturation temperature corresponding to the exhaust pressure. As high-pressure steam is admitted into the working cylinder, much of the high-temperature steam is condensed as water droplets onto the metal surfaces, significantly reducing the steam available for expansive work. When the expanding steam reaches low pressure (especially during the exhaust stroke), the previously deposited water droplets that had just been formed within the cylinder/ports now boil away (re-evaporation) and this steam does no further work in the cylinder.

There are practical limits on the expansion ratio of a steam engine cylinder, as increasing cylinder surface area tends to exacerbate the cylinder condensation and re-evaporation issues. This negates the theoretical advantages associated with a high ratio of expansion in an individual cylinder.

A method to lessen the magnitude of energy loss to a very long cylinder was invented in 1804 by British engineer Arthur Woolf, who patented his "Woolf high-pressure compound engine" in 1805. In the compound engine, high-pressure steam from the boiler expands in a high-pressure (HP) cylinder and then enters one or more subsequent lower-pressure (LP) cylinders. The complete expansion of the steam now occurs across multiple cylinders, with the overall temperature drop within each cylinder reduced considerably. By expanding the steam in steps with smaller temperature range (within each cylinder) the condensation and re-evaporation efficiency issue (described above) is reduced. This reduces the magnitude of cylinder heating and cooling, increasing the efficiency of the engine. By staging the expansion in multiple cylinders, variations of torque can be reduced. To derive equal work from lower-pressure cylinder requires a larger cylinder volume as this steam occupies a greater volume. Therefore, the bore, and in rare cases the stroke, are increased in low-pressure cylinders, resulting in larger cylinders.

Double-expansion (usually known as compound) engines expanded the steam in two stages. The pairs may be duplicated or the work of the large low-pressure cylinder can be split with one high-pressure cylinder exhausting into one or the other, giving a three-cylinder layout where cylinder and piston diameter are about the same, making the reciprocating masses easier to balance.

Two-cylinder compounds can be arranged as:

With two-cylinder compounds used in railway work, the pistons are connected to the cranks as with a two-cylinder simple at 90° out of phase with each other ("quartered"). When the double-expansion group is duplicated, producing a four-cylinder compound, the individual pistons within the group are usually balanced at 180°, the groups being set at 90° to each other. In one case (the first type of Vauclain compound), the pistons worked in the same phase driving a common crosshead and crank, again set at 90° as for a two-cylinder engine. With the three-cylinder compound arrangement, the LP cranks were either set at 90° with the HP one at 135° to the other two, or in some cases, all three cranks were set at 120°.

The adoption of compounding was common for industrial units, for road engines and almost universal for marine engines after 1880; it was not universally popular in railway locomotives where it was often perceived as complicated. This is partly due to the harsh railway operating environment and limited space afforded by the loading gauge (particularly in Britain, where compounding was never common and not employed after 1930). However, although never in the majority, it was popular in many other countries.

It is a logical extension of the compound engine (described above) to split the expansion into yet more stages to increase efficiency. The result is the multiple-expansion engine. Such engines use either three or four expansion stages and are known as "triple-" and "quadruple-expansion engines" respectively. These engines use a series of cylinders of progressively increasing diameter. These cylinders are designed to divide the work into equal shares for each expansion stage. As with the double-expansion engine, if space is at a premium, then two smaller cylinders may be used for the low-pressure stage. Multiple-expansion engines typically had the cylinders arranged inline, but various other formations were used. In the late 19th century, the Yarrow-Schlick-Tweedy balancing "system" was used on some marine triple-expansion engines. Y-S-T engines divided the low-pressure expansion stages between two cylinders, one at each end of the engine. This allowed the crankshaft to be better balanced, resulting in a smoother, faster-responding engine which ran with less vibration. This made the four-cylinder triple-expansion engine popular with large passenger liners (such as the "Olympic" class), but this was ultimately replaced by the virtually vibration-free turbine engine. It is noted, however, that triple-expansion reciprocating steam engines were used to drive the World War II Liberty ships, by far the largest number of identical ships ever built. Over 2700 ships were built, in the United States, from a British original design. 

The image in this section shows an animation of a triple-expansion engine. The steam travels through the engine from left to right. The valve chest for each of the cylinders is to the left of the corresponding cylinder.

Land-based steam engines could exhaust their steam to atmosphere, as feed water was usually readily available. Prior to and during World War I, the expansion engine dominated marine applications, where high vessel speed was not essential. It was, however, superseded by the British invention steam turbine where speed was required, for instance in warships, such as the dreadnought battleships, and ocean liners. of 1905 was the first major warship to replace the proven technology of the reciprocating engine with the then-novel steam turbine.

In most reciprocating piston engines, the steam reverses its direction of flow at each stroke (counterflow), entering and exhausting from the same end of the cylinder. The complete engine cycle occupies one rotation of the crank and two piston strokes; the cycle also comprises four "events" – admission, expansion, exhaust, compression. These events are controlled by valves often working inside a "steam chest" adjacent to the cylinder; the valves distribute the steam by opening and closing steam "ports" communicating with the cylinder end(s) and are driven by valve gear, of which there are many types.

The simplest valve gears give events of fixed length during the engine cycle and often make the engine rotate in only one direction. Many however have a reversing mechanism which additionally can provide means for saving steam as speed and momentum are gained by gradually "shortening the cutoff" or rather, shortening the admission event; this in turn proportionately lengthens the expansion period. However, as one and the same valve usually controls both steam flows, a short cutoff at admission adversely affects the exhaust and compression periods which should ideally always be kept fairly constant; if the exhaust event is too brief, the totality of the exhaust steam cannot evacuate the cylinder, choking it and giving excessive compression (""kick back"").

In the 1840s and 1850s, there were attempts to overcome this problem by means of various patent valve gears with a separate, variable cutoff expansion valve riding on the back of the main slide valve; the latter usually had fixed or limited cutoff. The combined setup gave a fair approximation of the ideal events, at the expense of increased friction and wear, and the mechanism tended to be complicated. The usual compromise solution has been to provide "lap" by lengthening rubbing surfaces of the valve in such a way as to overlap the port on the admission side, with the effect that the exhaust side remains open for a longer period after cut-off on the admission side has occurred. This expedient has since been generally considered satisfactory for most purposes and makes possible the use of the simpler Stephenson, Joy and Walschaerts motions. Corliss, and later, poppet valve gears had separate admission and exhaust valves driven by trip mechanisms or cams profiled so as to give ideal events; most of these gears never succeeded outside of the stationary marketplace due to various other issues including leakage and more delicate mechanisms.

Before the exhaust phase is quite complete, the exhaust side of the valve closes, shutting a portion of the exhaust steam inside the cylinder. This determines the compression phase where a cushion of steam is formed against which the piston does work whilst its velocity is rapidly decreasing; it moreover obviates the pressure and temperature shock, which would otherwise be caused by the sudden admission of the high-pressure steam at the beginning of the following cycle.

The above effects are further enhanced by providing "lead": as was later discovered with the internal combustion engine, it has been found advantageous since the late 1830s to advance the admission phase, giving the valve "lead" so that admission occurs a little before the end of the exhaust stroke in order to fill the "clearance volume" comprising the ports and the cylinder ends (not part of the piston-swept volume) before the steam begins to exert effort on the piston.

Uniflow engines attempt to remedy the difficulties arising from the usual counterflow cycle where, during each stroke, the port and the cylinder walls will be cooled by the passing exhaust steam, whilst the hotter incoming admission steam will waste some of its energy in restoring the working temperature. The aim of the uniflow is to remedy this defect and improve efficiency by providing an additional port uncovered by the piston at the end of each stroke making the steam flow only in one direction. By this means, the simple-expansion uniflow engine gives efficiency equivalent to that of classic compound systems with the added advantage of superior part-load performance, and comparable efficiency to turbines for smaller engines below one thousand horsepower. However, the thermal expansion gradient uniflow engines produce along the cylinder wall gives practical difficulties..

A steam turbine consists of one or more "rotors" (rotating discs) mounted on a drive shaft, alternating with a series of "stators" (static discs) fixed to the turbine casing. The rotors have a propeller-like arrangement of blades at the outer edge. Steam acts upon these blades, producing rotary motion. The stator consists of a similar, but fixed, series of blades that serve to redirect the steam flow onto the next rotor stage. A steam turbine often exhausts into a surface condenser that provides a vacuum. The stages of a steam turbine are typically arranged to extract the maximum potential work from a specific velocity and pressure of steam, giving rise to a series of variably sized high- and low-pressure stages. Turbines are only efficient if they rotate at relatively high speed, therefore they are usually connected to reduction gearing to drive lower speed applications, such as a ship's propeller. In the vast majority of large electric generating stations, turbines are directly connected to generators with no reduction gearing. Typical speeds are 3600 revolutions per minute (RPM) in the United States with 60 Hertz power, and 3000 RPM in Europe and other countries with 50 Hertz electric power systems. In nuclear power applications, the turbines typically run at half these speeds, 1800 RPM and 1500 RPM. A turbine rotor is also only capable of providing power when rotating in one direction. Therefore, a reversing stage or gearbox is usually required where power is required in the opposite direction.

Steam turbines provide direct rotational force and therefore do not require a linkage mechanism to convert reciprocating to rotary motion. Thus, they produce smoother rotational forces on the output shaft. This contributes to a lower maintenance requirement and less wear on the machinery they power than a comparable reciprocating engine.

The main use for steam turbines is in electricity generation (in the 1990s about 90% of the world's electric production was by use of steam turbines) however the recent widespread application of large gas turbine units and typical combined cycle power plants has resulted in reduction of this percentage to the 80% regime for steam turbines. In electricity production, the high speed of turbine rotation matches well with the speed of modern electric generators, which are typically direct connected to their driving turbines. In marine service, (pioneered on the "Turbinia"), steam turbines with reduction gearing (although the Turbinia has direct turbines to propellers with no reduction gearbox) dominated large ship propulsion throughout the late 20th century, being more efficient (and requiring far less maintenance) than reciprocating steam engines. In recent decades, reciprocating Diesel engines, and gas turbines, have almost entirely supplanted steam propulsion for marine applications.

Virtually all nuclear power plants generate electricity by heating water to provide steam that drives a turbine connected to an electrical generator. Nuclear-powered ships and submarines either use a steam turbine directly for main propulsion, with generators providing auxiliary power, or else employ turbo-electric transmission, where the steam drives a turbo generator set with propulsion provided by electric motors. A limited number of steam turbine railroad locomotives were manufactured. Some non-condensing direct-drive locomotives did meet with some success for long haul freight operations in Sweden and for express passenger work in Britain, but were not repeated. Elsewhere, notably in the United States, more advanced designs with electric transmission were built experimentally, but not reproduced. It was found that steam turbines were not ideally suited to the railroad environment and these locomotives failed to oust the classic reciprocating steam unit in the way that modern diesel and electric traction has done.

An oscillating cylinder steam engine is a variant of the simple expansion steam engine which does not require valves to direct steam into and out of the cylinder. Instead of valves, the entire cylinder rocks, or oscillates, such that one or more holes in the cylinder line up with holes in a fixed port face or in the pivot mounting (trunnion). These engines are mainly used in toys and models, because of their simplicity, but have also been used in full-size working engines, mainly on ships where their compactness is valued.

It is possible to use a mechanism based on a pistonless rotary engine such as the Wankel engine in place of the cylinders and valve gear of a conventional reciprocating steam engine. Many such engines have been designed, from the time of James Watt to the present day, but relatively few were actually built and even fewer went into quantity production; see link at bottom of article for more details. The major problem is the difficulty of sealing the rotors to make them steam-tight in the face of wear and thermal expansion; the resulting leakage made them very inefficient. Lack of expansive working, or any means of control of the cutoff, is also a serious problem with many such designs.

By the 1840s, it was clear that the concept had inherent problems and rotary engines were treated with some derision in the technical press. However, the arrival of electricity on the scene, and the obvious advantages of driving a dynamo directly from a high-speed engine, led to something of a revival in interest in the 1880s and 1890s, and a few designs had some limited success..

Of the few designs that were manufactured in quantity, those of the Hult Brothers Rotary Steam Engine Company of Stockholm, Sweden, and the spherical engine of Beauchamp Tower are notable. Tower's engines were used by the Great Eastern Railway to drive lighting dynamos on their locomotives, and by the Admiralty for driving dynamos on board the ships of the Royal Navy. They were eventually replaced in these niche applications by steam turbines.

The aeolipile represents the use of steam by the rocket-reaction principle, although not for direct propulsion.

In more modern times there has been limited use of steam for rocketry – particularly for rocket cars. Steam rocketry works by filling a pressure vessel with hot water at high pressure and opening a valve leading to a suitable nozzle. The drop in pressure immediately boils some of the water and the steam leaves through a nozzle, creating a propulsive force.

Ferdinand Verbiest's carriage was powered by an aeolipile in 1679.

Steam engines possess boilers and other components that are pressure vessels that contain a great deal of potential energy. Steam escapes and boiler explosions (typically BLEVEs) can and have in the past caused great loss of life. While variations in standards may exist in different countries, stringent legal, testing, training, care with manufacture, operation and certification is applied to ensure safety.

Failure modes may include:

Steam engines frequently possess two independent mechanisms for ensuring that the pressure in the boiler does not go too high; one may be adjusted by the user, the second is typically designed as an ultimate fail-safe. Such safety valves traditionally used a simple lever to restrain a plug valve in the top of a boiler. One end of the lever carried a weight or spring that restrained the valve against steam pressure. Early valves could be adjusted by engine drivers, leading to many accidents when a driver fastened the valve down to allow greater steam pressure and more power from the engine. The more recent type of safety valve uses an adjustable spring-loaded valve, which is locked such that operators may not tamper with its adjustment unless a seal is illegally broken. This arrangement is considerably safer.

Lead fusible plugs may be present in the crown of the boiler's firebox. If the water level drops, such that the temperature of the firebox crown increases significantly, the lead melts and the steam escapes, warning the operators, who may then manually suppress the fire. Except in the smallest of boilers the steam escape has little effect on dampening the fire. The plugs are also too small in area to lower steam pressure significantly, depressurizing the boiler. If they were any larger, the volume of escaping steam would itself endanger the crew.

The Rankine cycle is the fundamental thermodynamic underpinning of the steam engine. The cycle is an arrangement of components as is typically used for simple power production, and utilizes the phase change of water (boiling water producing steam, condensing exhaust steam, producing liquid water)) to provide a practical heat/power conversion system. The heat is supplied externally to a closed loop with some of the heat added being converted to work and the waste heat being removed in a condenser. The Rankine cycle is used in virtually all steam power production applications. In the 1990s, Rankine steam cycles generated about 90% of all electric power used throughout the world, including virtually all solar, biomass, coal and nuclear power plants. It is named after William John Macquorn Rankine, a Scottish polymath.

The Rankine cycle is sometimes referred to as a practical Carnot cycle because, when an efficient turbine is used, the TS diagram begins to resemble the Carnot cycle. The main difference is that heat addition (in the boiler) and rejection (in the condenser) are isobaric (constant pressure) processes in the Rankine cycle and isothermal (constant temperature) processes in the theoretical Carnot cycle. In this cycle, a pump is used to pressurize the working fluid which is received from the condenser as a liquid not as a gas. Pumping the working fluid in liquid form during the cycle requires a small fraction of the energy to transport it compared to the energy needed to compress the working fluid in gaseous form in a compressor (as in the Carnot cycle). The cycle of a reciprocating steam engine differs from that of turbines because of condensation and re-evaporation occurring in the cylinder or in the steam inlet passages.

The working fluid in a Rankine cycle can operate as a closed loop system, where the working fluid is recycled continuously, or may be an "open loop" system, where the exhaust steam is directly released to the atmosphere, and a separate source of water feeding the boiler is supplied. Normally water is the fluid of choice due to its favourable properties, such as non-toxic and unreactive chemistry, abundance, low cost, and its thermodynamic properties. Mercury is the working fluid in the mercury vapor turbine. Low boiling hydrocarbons can be used in a binary cycle.

The steam engine contributed much to the development of thermodynamic theory; however, the only applications of scientific theory that influenced the steam engine were the original concepts of harnessing the power of steam and atmospheric pressure and knowledge of properties of heat and steam. The experimental measurements made by Watt on a model steam engine led to the development of the separate condenser. Watt independently discovered latent heat, which was confirmed by the original discoverer Joseph Black, who also advised Watt on experimental procedures. Watt was also aware of the change in the boiling point of water with pressure. Otherwise, the improvements to the engine itself were more mechanical in nature. The thermodynamic concepts of the Rankine cycle did give engineers the understanding needed to calculate efficiency which aided the development of modern high-pressure and -temperature boilers and the steam turbine.

The efficiency of an engine cycle can be calculated by dividing the energy output of mechanical work that the engine produces by the energy put into the engine by the burning fuel.

The historical measure of a steam engine's energy efficiency was its "duty". The concept of duty was first introduced by Watt in order to illustrate how much more efficient his engines were over the earlier Newcomen designs. Duty is the number of foot-pounds of work delivered by burning one bushel (94 pounds) of coal. The best examples of Newcomen designs had a duty of about 7 million, but most were closer to 5 million. Watt's original low-pressure designs were able to deliver duty as high as 25 million, but averaged about 17. This was a three-fold improvement over the average Newcomen design. Early Watt engines equipped with high-pressure steam improved this to 65 million.

No heat engine can be more efficient than the Carnot cycle, in which heat is moved from a high-temperature reservoir to one at a low temperature, and the efficiency depends on the temperature difference. For the greatest efficiency, steam engines should be operated at the highest steam temperature possible (superheated steam), and release the waste heat at the lowest temperature possible.

The efficiency of a Rankine cycle is usually limited by the working fluid. Without the pressure reaching supercritical levels for the working fluid, the temperature range over which the cycle can operate is small; in steam turbines, turbine entry temperatures are typically 565 °C (the creep limit of stainless steel) and condenser temperatures are around 30 °C. This gives a theoretical Carnot efficiency of about 63% compared with an actual efficiency of 42% for a modern coal-fired power station. This low turbine entry temperature (compared with a gas turbine) is why the Rankine cycle is often used as a bottoming cycle in combined-cycle gas turbine power stations.

One principal advantage the Rankine cycle holds over others is that during the compression stage relatively little work is required to drive the pump, the working fluid being in its liquid phase at this point. By condensing the fluid, the work required by the pump consumes only 1% to 3% of the turbine (or reciprocating engine) power and contributes to a much higher efficiency for a real cycle. The benefit of this is lost somewhat due to the lower heat addition temperature. Gas turbines, for instance, have turbine entry temperatures approaching 1500 °C. Nonetheless, the efficiencies of actual large steam cycles and large modern simple cycle gas turbines are fairly well matched.

In practice, a reciprocating steam engine cycle exhausting the steam to atmosphere will typically have an efficiency (including the boiler) in the range of 1–10%, but with the addition of a condenser, Corliss valves, multiple expansion, and high steam pressure/temperature, it may be greatly improved, historically into the range of 10–20%, and very rarely slightly higher.

A modern, large electrical power station (producing several hundred megawatts of electrical output) with steam reheat, economizer etc. will achieve efficiency in the mid 40% range, with the most efficient units approaching 50% thermal efficiency.

It is also possible to capture the waste heat using cogeneration in which the waste heat is used for heating a lower boiling point working fluid or as a heat source for district heating via saturated low-pressure steam.



</doc>
<doc id="27694" url="https://en.wikipedia.org/wiki?curid=27694" title="Satan">
Satan

Satan, also known as the Devil, is an entity in the Abrahamic religions that seduces humans into sin or falsehood. In Christianity and Islam, he is usually seen as either a fallen angel or a genie, who used to possess great piety and beauty, but rebelled against God, who nevertheless allows him temporary power over the fallen world and a host of demons. In Judaism, Satan is typically regarded as a metaphor for the "yetzer hara", or "evil inclination", or as an agent subservient to God.

A figure known as "the satan" first appears in the Tanakh as a heavenly prosecutor, a member of the sons of God subordinate to Yahweh, who prosecutes the nation of Judah in the heavenly court and tests the loyalty of Yahweh's followers by forcing them to suffer. During the intertestamental period, possibly due to influence from the Zoroastrian figure of Angra Mainyu, the satan developed into a malevolent entity with abhorrent qualities in dualistic opposition to God. In the apocryphal Book of Jubilees, Yahweh grants the satan (referred to as Mastema) authority over a group of fallen angels, or their offspring, to tempt humans to sin and punish them. In the Synoptic Gospels, Satan tempts Jesus in the desert and is identified as the cause of illness and temptation. In the Book of Revelation, Satan appears as a Great Red Dragon, who is defeated by Michael the Archangel and cast down from Heaven. He is later bound for one thousand years, but is briefly set free before being ultimately defeated and cast into the Lake of Fire.

In Christianity, Satan is known as the Devil and is sometimes also called Lucifer. Although the Book of Genesis does not mention him, he is often identified as the serpent in the Garden of Eden. In the Middle Ages, Satan played a minimal role in Christian theology and was used as a comic relief figure in mystery plays. During the early modern period, Satan's significance greatly increased as beliefs such as demonic possession and witchcraft became more prevalent. During the Age of Enlightenment, belief in the existence of Satan became harshly criticized. Nonetheless, belief in Satan has persisted, particularly in the Americas. In the Quran, Shaitan, also known as Iblis, is an entity made of fire who was cast out of Heaven because he refused to bow before the newly-created Adam and incites humans to sin by infecting their minds with "waswās" ("evil suggestions"). Although Satan is generally viewed as evil, some groups have very different beliefs.

In Theistic Satanism, Satan is considered a deity who is either worshipped or revered. In LaVeyan Satanism, Satan is a symbol of virtuous characteristics and liberty. Satan's appearance is never described in the Bible, but, since the ninth century, he has often been shown in Christian art with horns, cloven hooves, unusually hairy legs, and a tail, often naked and holding a pitchfork. These are an amalgam of traits derived from various pagan deities, including Pan, Poseidon, and Bes. Satan appears frequently in Christian literature, most notably in Dante Alighieri's "Inferno", variants of the Faust legend, John Milton's "Paradise Lost" and "Paradise Regained", and the poems of William Blake. He continues to appear in film, television, and music.

The original Hebrew term "sâtan" () is a generic noun meaning "accuser" or "adversary", which is used throughout the Hebrew Bible to refer to ordinary human adversaries, as well as a specific supernatural entity. The word is derived from a verb meaning primarily "to obstruct, oppose". When it is used without the definite article (simply "satan"), the word can refer to any accuser, but when it is used with the definite article ("ha-satan"), it usually refers specifically to the heavenly accuser: the satan.

"Ha-Satan" with the definite article occurs 13 times in the Masoretic Text, in two books of the Hebrew Bible: Job ch. 1–2 (10×) and Zechariah 3:1–2 (3×). "Satan" without the definite article is used in 10 instances, of which two are translated "diabolos" in the Septuagint and "Satan" in the King James Version (KJV):

The word "satan" does not occur in the Book of Genesis, which mentions only a talking serpent and does not identify the serpent with any supernatural entity. The first occurrence of the word "satan" in the Hebrew Bible in reference to a supernatural figure comes from , which describes the Angel of Yahweh confronting Balaam on his donkey: "Balaam's departure aroused the wrath of Elohim, and the Angel of Yahweh stood in the road as a satan against him." In , Yahweh sends the "Angel of Yahweh" to inflict a plague against Israel for three days, killing 70,000 people as punishment for David having taken a census without his approval. repeats this story, but replaces the "Angel of Yahweh" with an entity referred to as "a satan".

Some passages clearly refer to the satan, without using the word itself. describes the sons of Eli as "sons of Belial"; the later usage of this word makes it clearly a synonym for "satan". In Yahweh sends a "troubling spirit" to torment King Saul as a mechanism to ingratiate David with the king. In , the prophet Micaiah describes to King Ahab a vision of Yahweh sitting on his throne surrounded by the Host of Heaven. Yahweh asks the Host which of them will lead Ahab astray. A "spirit", whose name is not specified, but who is analogous to the satan, volunteers to be "a Lying Spirit in the mouth of all his Prophets".

The satan appears in the Book of Job, a poetic dialogue set within a prose framework, which may have been written around the time of the Babylonian captivity. In the text, Job is a righteous man favored by Yahweh. describes the "sons of God" ("bənê hāʼĕlōhîm") presenting themselves before Yahweh. Yahweh asks one of them, "the satan", where he has been, to which he replies that he has been roaming around the earth. Yahweh asks, "Have you considered My servant Job?" The satan replies by urging Yahweh to let him torture Job, promising that Job will abandon his faith at the first tribulation. Yahweh consents; the satan destroys Job's servants and flocks, yet Job refuses to condemn Yahweh. The first scene repeats itself, with the satan presenting himself to Yahweh alongside the other "sons of God". Yahweh points out Job's continued faithfulness, to which the satan insists that more testing is necessary; Yahweh once again gives him permission to test Job. In the end, Job remains faithful and righteous, and it is implied that the satan is shamed in his defeat.

 contains a description of a vision dated to the middle of February of 519 BC, in which an angel shows Zechariah a scene of Joshua the High Priest dressed in filthy rags, representing the nation of Judah and its sins, on trial with Yahweh as the judge and the satan standing as the prosecutor. Yahweh rebukes the satan and orders for Joshua to be given clean clothes, representing Yahweh's forgiveness of Judah's sins.

During the Second Temple Period, when Jews were living in the Achaemenid Empire, Judaism was heavily influenced by Zoroastrianism, the religion of the Achaemenids. Jewish conceptions of Satan were impacted by Angra Mainyu, the Zoroastrian god of evil, darkness, and ignorance. In the Septuagint, the Hebrew "ha-Satan" in Job and Zechariah is translated by the Greek word "diabolos" (slanderer), the same word in the Greek New Testament from which the English word "devil" is derived. Where "satan" is used to refer to human enemies in the Hebrew Bible, such as Hadad the Edomite and Rezon the Syrian, the word is left untranslated but transliterated in the Greek as "satan", a neologism in Greek.

The idea of Satan as an opponent of God and a purely evil figure seems to have taken root in Jewish pseudepigrapha during the Second Temple Period, particularly in the "apocalypses". The Book of Enoch, which the Dead Sea Scrolls have revealed to have been nearly as popular as the Torah, describes a group of 200 angels known as the "Watchers", who are assigned to supervise the earth, but instead abandon their duties and have sexual intercourse with human women. The leader of the Watchers is Semjâzâ and another member of the group, known as Azazel, spreads sin and corruption among humankind. The Watchers are ultimately sequestered in isolated caves across the earth and are condemned to face judgement at the end of time. The Book of Jubilees, written in around 150 BC, retells the story of the Watchers' defeat, but, in deviation from the Book of Enoch, Mastema, the "Chief of Spirits", intervenes before all of their demon offspring are sealed away, requesting for Yahweh to let him keep some of them to become his workers. Yahweh acquiesces this request and Mastema uses them to tempt humans into committing more sins, so that he may punish them for their wickedness. Later, Mastema induces Yahweh to test Abraham by ordering him to sacrifice Isaac.

The Second Book of Enoch, also called the Slavonic Book of Enoch, contains references to a Watcher called Satanael. It is a pseudepigraphic text of an uncertain date and unknown authorship. The text describes Satanael as being the prince of the Grigori who was cast out of heaven and an evil spirit who knew the difference between what was "righteous" and "sinful". In the Book of Wisdom, the devil is taken to be the being who brought death into the world, but originally the culprit was recognized as Cain. The name Samael, which is used in reference to one of the fallen angels, later became a common name for Satan in Jewish Midrash and Kabbalah.

Most Jews do not believe in the existence of a supernatural omnimalevolent figure. Traditionalists and philosophers in medieval Judaism adhered to rational theology, rejecting any belief in rebel or fallen angels, and viewing evil as abstract. The rabbis usually interpreted the word "satan" as it is used in the Tanakh as referring strictly to "human" adversaries and rejected all of the Enochian writings mentioning Satan as a literal, heavenly figure from the biblical canon, making every attempt to root them out. Nonetheless, the word "satan" has occasionally been metaphorically applied to evil influences, such as the Jewish exegesis of the "yetzer hara" ("evil inclination") mentioned in .

Rabbinical scholarship on the Book of Job generally follows the Talmud and Maimonides in identifying "the satan" from the prologue as a metaphor for the "yetzer hara" and not an actual entity. Satan is rarely mentioned in Tannaitic literature, but is found in Babylonian aggadah. According to a narration, the sound of the shofar, which is primarily intended to remind Jews of the importance of "teshuva", is also intended symbolically to "confuse the accuser" (Satan) and prevent him from rendering any litigation to God against the Jews. Kabbalah presents Satan as an agent of God whose function is to tempt humans into sinning so that he may accuse them in the heavenly court. The Hasidic Jews of the eighteenth century associated ha-Satan with "Baal Davar".

Each modern sect of Judaism has its own interpretation of Satan's identity. Conservative Judaism generally rejects the Talmudic interpretation of Satan as a metaphor for the "yetzer hara", and regard him as a literal agent of God. Orthodox Judaism, on the other hand, outwardly embraces Talmudic teachings on Satan, and involves Satan in religious life far more inclusively than other sects. Satan is mentioned explicitly in some daily prayers, including during Shacharit and certain post-meal benedictions, as described in Talmud and the Jewish Code of Law. In Reform Judaism, Satan is generally seen in his Talmudic role as a metaphor for the "yetzer hara" and the symbolic representation of innate human qualities such as selfishness.

The most common English synonym for "Satan" is "devil", which descends from Middle English "devel", from Old English "dēofol," that in turn represents an early Germanic borrowing of Latin "diabolus" (also the source of "diabolical"). This in turn was borrowed from Greek "diabolos" "slanderer", from "diaballein" "to slander": "dia-" "across, through" + "ballein" "to hurl". In the New Testament, the words "Satan" and "diabolos" are used interchangeably as synonyms. Beelzebub, meaning "Lord of Flies", is the contemptuous name given in the Hebrew Bible and New Testament to a Philistine god whose original name has been reconstructed as most probably "Ba'al Zabul", meaning "Baal the Prince". The Synoptic Gospels identify Satan and Beelzebub as the same. The name Abaddon (meaning "place of destruction") is used six times in the Old Testament, mainly as a name for one the regions of Sheol. describes Abaddon, whose name is translated into Greek as "Apollyon", meaning "the destroyer", as an angel who rules the Abyss. In modern usage, Abaddon is sometimes equated with Satan.

The three Synoptic Gospels all describe the temptation of Christ by Satan in the desert (, , and ). Satan first shows Jesus a stone and tells him to turn it into bread. He also takes him to the pinnacle of the Temple in Jerusalem and commands Jesus to throw himself down so that the angels will catch him. Satan takes Jesus to the top of a tall mountain as well; there, he shows him the kingdoms of the earth and promises to give them all to him if he will bow down and worship him. Each time Jesus rebukes Satan and, after the third temptation, he is administered by the angels. Satan's promise in and to give Jesus all the kingdoms of the earth implies that all those kingdoms belong to him. The fact that Jesus does not dispute Satan's promise indicates that the authors of those gospels believed this to be true.

Satan plays a role in some of the parables of Jesus, namely the Parable of the Sower, the Parable of the Weeds, Parable of the Sheep and the Goats, and the Parable of the Strong Man. According to the Parable of the Sower, Satan "profoundly influences" those who fail to understand the gospel. The latter two parables say that Satan's followers will be punished on Judgement Day, with the Parable of the Sheep and the Goats stating that the Devil, his angels, and the people who follow him will be consigned to "eternal fire". When the Pharisees accused Jesus of exorcising demons through the power of Beelzebub, Jesus responds by telling the Parable of the Strong Man, saying: "how can someone enter a strong man's house and plunder his goods, unless he first binds the strong man? Then indeed he may plunder his house" (). The strong man in this parable represents Satan.

The Synoptic Gospels identify Satan and his demons as the causes of illness, including fever (), leprosy (), and arthritis (), while the Epistle to the Hebrews describes the Devil as "him who holds the power of death" (). The author of Luke-Acts attributes more power to Satan than both Matthew and Mark. In , Jesus grants Satan the authority to test Peter and the other apostles. states that Judas Iscariot betrayed Jesus because "Satan entered" him and, in , Peter describes Satan as "filling" Ananias's heart and causing him to sin. The Gospel of John only uses the name "Satan" three times. In , Jesus says that his Jewish or Judean enemies are the children of the Devil rather than the children of Abraham. The same verse describes the Devil as "a man-killer from the beginning" and "a liar and the father of lying." describes the Devil as inspiring Judas to betray Jesus and identifies Satan as "the Archon of this Cosmos", who is destined to be overthrown through Jesus's death and resurrection. promises that the Holy Spirit will "accuse the World concerning sin, justice, and judgement", a role resembling that of the satan in the Old Testament.

The Book of Revelation represents Satan as the supernatural ruler of the Roman Empire and the ultimate cause of all evil in the world. In , as part of the letter to the church at Smyrna, John of Patmos refers to the Jews of Smyrna as "a synagogue of Satan" and warns that "the Devil is about to cast some of you into prison as a test ["peirasmos"], and for ten days you will have affliction." In , in the letter to the church of Pergamum, John warns that Satan lives among the members of the congregation and declares that "Satan's throne" is in their midst. Pergamum was the capital of the Roman Province of Asia and "Satan's throne" may be referring to the monumental Pergamon Altar in the city, which was dedicated to the Greek god Zeus, or to a temple dedicated to the Roman emperor Augustus.

In , Satan is bound with a chain and hurled into the Abyss, where he is imprisoned for one thousand years. In , he is set free and gathers his armies along with Gog and Magog to wage war against the righteous, but is defeated with fire from Heaven, and cast into the lake of fire. Some Christians associate Satan with the number 666, which describes as the Number of the Beast. However, the beast mentioned in Revelation 13 is not Satan, and the use of 666 in the Book of Revelation has been interpreted as a reference to the Roman Emperor Nero, as 666 is the numeric value of his name in Hebrew.

Despite the fact that the Book of Genesis never mentions Satan, Christians have traditionally interpreted the serpent in the Garden of Eden as Satan due to , which calls Satan "that ancient serpent". This verse, however, is probably intended to identify Satan with the Leviathan, a monstrous sea-serpent whose destruction by Yahweh is prophesied in . The first recorded individual to identify Satan with the serpent from the Garden of Eden was the second-century AD Christian apologist Justin Martyr, in chapters 45 and 79 of his "Dialogue with Trypho". Other early church fathers to mention this identification include Theophilus and Tertullian. The early Christian Church, however, encountered opposition from pagans such as Celsus, who claimed in his treatise "The True Word" that "it is blasphemy... to say that the greatest God... has an adversary who constrains his capacity to do good" and said that Christians "impiously divide the kingdom of God, creating a rebellion in it, as if there were opposing factions within the divine, including one that is hostile to God".
The name "Heylel", meaning "morning star" (or, in Latin, "Lucifer"), was a name for Attar, the god of the planet Venus in Canaanite mythology, who attempted to scale the walls of the heavenly city, but was vanquished by the god of the sun. The name is used in in metaphorical reference to the king of Babylon. uses a description of a cherub in Eden as a polemic against Ithobaal II, the king of Tyre. The Church Father Origen of Alexandria ( 184 – 253), who was only aware of the actual text of these passages and not the original myths to which they refer, concluded in his treatise "On the First Principles", which is preserved in a Latin translation by Tyrannius Rufinus, that neither of these verses could literally refer to a human being and must therefore be alluding to "a certain Angel who had received the office of governing the nation of the Tyrians," but was hurled down to Earth after he was found to be corrupt.

In his apologetic treatise "Contra Celsum", however, Origen changed his interpretations of Isaiah 14:12 and Ezekiel 28:12–15, now interpreting both of them as referring to Satan. According to Henry Ansgar Kelly, Origen seems to have adopted this new interpretation to refute unnamed persons who, perhaps under the influence of Zoroastrian radical dualism, believed "that Satan's original nature was Darkness." The later Church Father Jerome ( 347 – 420), translator of the Latin Vulgate, accepted Origen's theory of Satan as a fallen angel and wrote about it in his commentary on the Book of Isaiah. In Christian tradition ever since, both Isaiah 14:12 and Ezekiel 28:12–15 have been understood as allegorically referring to Satan. For most Christians, Satan has been regarded as an angel who rebelled against God.

According to the ransom theory of atonement, which was popular among early Christian theologians, Satan gained power over humanity through Adam and Eve's sin and Christ's death on the cross was a ransom to Satan in exchange for humanity's liberation. This theory holds that Satan was tricked by God because Christ was not only free of sin, but also the incarnate Deity, whom Satan lacked the ability to enslave. Irenaeus of Lyons described a prototypical form of the ransom theory, but Origen was the first to propose it in its fully developed form. The theory was later expanded by theologians such as Gregory of Nyssa and Rufinus of Aquileia. In the eleventh century, Anselm of Canterbury criticized the ransom theory, along with the associated Christus Victor theory, resulting in the theory's decline in western Europe. The theory has nonetheless retained some of its popularity in the Eastern Orthodox Church.

Most early Christians firmly believed that Satan and his demons had the power to possess humans and exorcisms were widely practiced by Jews, Christians, and pagans alike. Belief in demonic possession continued through the Middle Ages into the early modern period. Exorcisms were seen as a display of God's power over Satan. The vast majority of people who thought they were possessed by the Devil did not suffer from hallucinations or other "spectacular symptoms", but "complained of anxiety, religious fears, and evil thoughts."

Satan had minimal role in medieval Christian theology, but he frequently appeared as a recurring comedic stock character in late medieval mystery plays, in which he was portrayed as a comic relief figure who "frolicked, fell, and farted in the background". Jeffrey Burton Russell describes the medieval conception of Satan as "more pathetic and repulsive than terrifying" and he was seen as little more than a nuisance to God's overarching plan. The "Golden Legend", a collection of saints' lives compiled in around 1260 by the Dominican Friar Jacobus da Varagine, contains numerous stories about encounters between saints and Satan, in which Satan is constantly duped by the saints' cleverness and by the power of God. Henry Ansgar Kelly remarks that Satan "comes across as the opposite of fearsome." The "Golden Legend" was the most popular book during the High and Late Middle Ages and more manuscripts of it have survived from the period than for any other book, including even the Bible itself.

The "Canon Episcopi", written in the eleventh century AD, condemns belief in witchcraft as heretical, but also documents that many people at the time apparently believed in it. Witches were believed to fly through the air on broomsticks, consort with demons, perform in "lurid sexual rituals" in the forests, murder human infants and eat them as part of Satanic rites, and engage in conjugal relations with demons. In 1326, Pope John XXII issued the papal bull "Super illius Specula", which condemned folk divination practices as consultation with Satan. By the 1430s, the Catholic Church began to regard witchcraft as part of a vast conspiracy led by Satan himself.

During the Early Modern Period, Christians gradually began to regard Satan as increasingly powerful and the fear of Satan's power became a dominant aspect of the worldview of Christians across Europe. During the Protestant Reformation, Martin Luther taught that, rather than trying to argue with Satan, Christians should avoid temptation altogether by seeking out pleasant company; Luther especially recommended music as a safeguard against temptation, since the Devil "cannot endure gaiety." John Calvin repeated a maxim from Saint Augustine that "Man is like a horse, with either God or the devil as rider."

In the late fifteenth century, a series of witchcraft panics erupted in France and Germany. The German Inquisitors Heinrich Kramer and Jacob Sprenger argued in their book "Malleus Maleficarum", published in 1487, that all "maleficia" ("sorcery") was rooted in the work of Satan. In the mid-sixteenth century, the panic spread to England and Switzerland. Both Protestants and Catholics alike firmly believed in witchcraft as a real phenomenon and supported its prosecution. In the late 1500s, the Dutch demonologist Johann Weyer argued in his treatise "De praestigiis daemonum" that witchcraft did not exist, but that Satan promoted belief in it to lead Christians astray. The panic over witchcraft intensified in the 1620s and continued until the end of the 1600s. Brian Levack estimates that around 60,000 people were executed for witchcraft during the entire span of the witchcraft hysteria.

The early English settlers of North America, especially the Puritans of New England, believed that Satan "visibly and palpably" reigned in the New World. John Winthrop claimed that the Devil made rebellious Puritan women give birth to stillborn monsters with claws, sharp horns, and "on each foot three claws, like a young fowl." Cotton Mather wrote that devils swarmed around Puritan settlements "like the frogs of Egypt". The Puritans believed that the Native Americans were worshippers of Satan and described them as "children of the Devil". Some settlers claimed to have seen Satan himself appear in the flesh at native ceremonies. During the First Great Awakening, the "new light" preachers portrayed their "old light" critics as ministers of Satan. By the time of the Second Great Awakening, Satan's primary role in American evangelicalism was as the opponent of the evangelical movement itself, who spent most of his time trying to hinder the ministries of evangelical preachers, a role he has largely retained among present-day American fundamentalists.

By the early 1600s, skeptics in Europe, including the English author Reginald Scot and the Anglican bishop John Bancroft, had begun to criticize the belief that demons still had the power to possess people. This skepticism was bolstered by the belief that miracles only occurred during the Apostolic Age, which had long since ended. Later, Enlightenment thinkers, such as David Hume, Denis Diderot, and Voltaire, attacked the notion of Satan's existence altogether. Voltaire labelled John Milton's "Paradise Lost" a "disgusting fantasy" and declared that belief in Hell and Satan were among the many lies propagated by the Catholic Church to keep humanity enslaved. By the eighteenth century, trials for witchcraft had ceased in most western countries, with the notable exceptions of Poland and Hungary, where they continued. Belief in the power of Satan, however, remained strong among traditional Christians.

Mormonism developed its own views on Satan. According to the Book of Moses, the Devil offered to be the redeemer of mankind for the sake of his own glory. Conversely, Jesus offered to be the redeemer of mankind so that his father's will would be done. After his offer was rejected, Satan became rebellious and was subsequently cast out of heaven. In the Book of Moses, Cain is said to have "loved Satan more than God" and conspired with Satan to kill Abel. It was through this pact that Cain became a Master Mahan. The Book of Moses also says that Moses was tempted by Satan before calling upon the name of the "Only Begotten", which caused Satan to depart. Douglas Davies asserts that this text "reflects" the temptation of Jesus in the Bible.

Belief in Satan and demonic possession remains strong among Christians in the United States and Latin America. According to a 2013 poll conducted by YouGov, fifty-seven percent of people in the United States believe in a literal Devil, compared to eighteen percent of people in Britain. Fifty-one percent of Americans believe that Satan has the power to possess people. W. Scott Poole, author of "Satan in America: The Devil We Know", has opined that "In the United States over the last forty to fifty years, a composite image of Satan has emerged that borrows from both popular culture and theological sources" and that most American Christians do not "separate what they know [about Satan] from the movies from what they know from various ecclesiastical and theological traditions." The Catholic Church generally played down Satan and exorcism during late twentieth and early twenty-first centuries, but Pope Francis brought renewed focus on the Devil in the early 2010s, stating, among many other pronouncements, that "The devil is intelligent, he knows more theology than all the theologians together." According to the "Encyclopædia Britannica", liberal Christianity tends to view Satan "as a [figurative] mythological attempt to express the reality and extent of evil in the universe, existing outside and apart from humanity but profoundly influencing the human sphere."

Bernard McGinn describes multiple traditions detailing the relationship between the Antichrist and Satan. In the dualist approach, Satan will become incarnate in the Antichrist, just as God became incarnate in Jesus. However, in Orthodox Christian thought, this view is problematic because it is too similar to Christ's incarnation. Instead, the "indwelling" view has become more accepted, which stipulates that the Antichrist is a human figure inhabited by Satan, since the latter's power is not to be seen as equivalent to God's.

The Arabic equivalent of the word "Satan" is "Shaitan" (شيطان, from the root šṭn شط⁬ن). The word itself is an adjective (meaning "astray" or "distant", sometimes translated as "devil") that can be applied to both man ("al-ins", الإنس) and "al-jinn" (الجن), but it is also used in reference to Satan in particular. In the Quran, Satan's name is Iblis (), probably a derivative of the Greek word "diabolos". Muslims do not regard Satan as the cause of evil, but as a tempter, who takes advantage of humans' inclinations toward self-centeredness.

Seven suras in the Quran describe how God ordered all the angels and Iblis to bow before the newly-created Adam. All the angels bowed, but Iblis refused, claiming to be superior to Adam because he was made from fire; whereas Adam was made from clay (). Consequently, God expelled him from Paradise and condemned him to Jahannam. Iblis thereafter became a "kafir", "an ungrateful disbeliever", whose sole mission is to lead humanity astray. God allows Iblis to do this, because he knows that the righteous will be able to resist Iblis's attempts to misguide them. On Judgement Day, while the lot of Satan remains in question, those who followed him will be thrown into the fires of Jahannam. After his banishment from Paradise, Iblis, who thereafter became known as "Al-Shaitan" ("the Demon"), lured Adam and Eve into eating the fruit from the forbidden tree.

The primary characteristic of Satan, aside from his hubris and despair, is his ability to cast evil suggestions ("waswās") into men and women. states that Satan has no influence over the righteous, but that those who fall in error are under his power. implies that those who obey God's laws are immune to the temptations of Satan. warns that Satan tries to keep Muslims from reading the Quran and recommends reciting the Quran as an antidote against Satan. refers to Satan as the enemy of humanity and forbids humans from worshipping him. In the Quranic retelling of the story of Job, Job knows that Satan is the one tormenting him.

In the Quran, Satan is apparently an angel, but, in , he is described as "from the jinns". This, combined with the fact that he describes himself as having been made from fire, posed a major problem for Muslims exegetes of the Quran, who disagree on whether Satan is a fallen angel or the leader of a group of evil jinn. According to a hadith from Ibn Abbas, Iblis was actually an angel whom God created out of fire. Ibn Abbas asserts that the word "jinn" could be applied to earthly jinn, but also to "fiery angels" like Satan.

Hasan of Basra, an eminent Muslim theologian who lived in the seventh century AD, was quoted as saying: "Iblis was not an angel even for the time of an eye wink. He is the origin of Jinn as Adam is of Mankind." The medieval Persian scholar Abu Al-Zamakhshari states that the words "angels" and "jinn" are synonyms. Another Persian scholar, Al-Baydawi, instead argues that Satan "hoped" to be an angel, but that his actions made him a jinn. Other Islamic scholars argue that Satan was a jinn who was admitted into Paradise as a reward for his righteousness and, unlike the angels, was given the choice to obey or disobey God. When he was expelled from Paradise, Satan blamed humanity for his punishment. Concerning the fiery origin of Iblis, Zakariya al-Qazwini and Muḥammad ibn Aḥmad Ibshīhī state that all supernatural creatures originated from fire but the angels from its light and the jinn from its blaze, thus fire denotes a disembodiment origin of all spiritual entities. Abd al-Ghani al-Maqdisi argued that only the angels of mercy are created from light, but angels of punishment have been created from fire.

The Muslim historian Al-Tabari, who died in around 923 AD, writes that, before Adam was created, earthly jinn made of smokeless fire roamed the earth and spread corruption. He further relates that Iblis was originally an angel named "Azazil" or "Al-Harith", from a group of angels, in contrast to the jinn, created from the "fires of simoom", who was sent by God to confront the earthly jinn. Azazil defeated the jinn in battle and drove them into the mountains, but he became convinced that he was superior to humans and all the other angels, leading to his downfall. In this account, Azazil's group of angels were called "jinn" because they guarded Jannah (Paradise). In another tradition recorded by Al-Tabari, Satan was one of the earthly jinn, who was taken captive by the angels and brought to Heaven as a prisoner. God appointed him as judge over the other jinn and he became known as "Al-Hakam". He fulfilled his duty for a thousand years before growing negligent, but was rehabilitated again and resumed his position until his refusal to bow before Adam.

During the first two centuries of Islam, Muslims almost unanimously accepted the traditional story known as the Satanic Verses as true. According to this narrative, Muhammad was told by Satan to add words to the Quran which would allow Muslims to pray for the intercession of pagan goddesses. He mistook the words of Satan for divine inspiration. Modern Muslims almost universally reject this story as heretical, as it calls the integrity of the Quran into question.

On the third day of the Hajj, Muslim pilgrims to Mecca throw seven stones at a pillar known as the "Jamrah al-’Aqabah", symbolizing the stoning of the Devil. This ritual is based on the Islamic tradition that, when God ordered Abraham to sacrifice his son Ishmael, Satan tempted him three times not to do it, and, each time, Abraham responded by throwing seven stones at him.

The hadith teach that newborn babies cry because Satan touches them while they are being born, and that this touch causes people to have an aptitude for sin. This doctrine bears some similarities to the doctrine of original sin. Muslim tradition holds that only Jesus and Mary were not touched by Satan at birth. However, when he was a boy, Muhammad's heart was literally opened by an angel, who removed a black clot that symbolized sin.

Muslim tradition preserves a number of stories involving dialogues between Jesus and Iblis, all of which are intended to demonstrate Jesus's virtue and Satan's depravity. Ahmad ibn Hanbal records an Islamic retelling of Jesus's temptation by Satan in the desert from the Synoptic Gospels. Ahmad quotes Jesus as saying, "The greatest sin is love of the world. Women are the ropes of Satan. Wine is the key to every evil." Abu Uthman al-Jahiz credits Jesus with saying, "The world is Satan's farm, and its people are his plowmen." Al-Ghazali tells an anecdote about how Jesus went out one day and saw Satan carrying ashes and honey; when he asked what they were for, Satan replied, "The honey I put on the lips of backbiters so that they achieve their aim. The ashes I put on the faces of orphans, so that people come to dislike them." The thirteenth-century scholar Sibt ibn al-Jawzi states that, when Jesus asked him what truly broke his back, Satan replied, "The neighing of horses in the cause of Allah."

According to Sufi mysticism, Iblis refused to bow to Adam because he was fully devoted to God alone and refused to bow to anyone else. For this reason, Sufi masters regard Satan and Muhammad as the two most perfect monotheists. Sufis reject the concept of dualism and instead believe in the unity of existence. In the same way that Muhammad was the instrument of God's mercy, Sufis regard Satan as the instrument of God's wrath.

Muslims believe that Satan is also the cause of deceptions originating from the mind and desires for evil. He is regarded as a cosmic force for separation, despair and spiritual envelopment. Muslims do distinguish between the satanic temptations and the murmurings of the bodily lower self (Nafs). The lower self commands the person to do a specific task or to fulfill a specific desire; whereas the inspirations of Satan tempt the person to do evil in general and, after a person successfully resists his first suggestion, Satan returns with new ones. If a Muslim feels that Satan is inciting him to sin, he is advised to seek refuge with God by reciting: "In the name of Allah, I seek refuge in you, from Satan the outcast." Muslims are also obliged to "seek refuge" before reciting the Quran.

In the Bahá'í Faith, Satan is not regarded as an independent evil power as he is in some faiths, but signifies the "lower nature" of humans. `Abdu'l-Bahá explains: "This lower nature in man is symbolized as Satan—the evil ego within us, not an evil personality outside." All other evil spirits described in various faith traditions—such as fallen angels, demons, and jinns—are also metaphors for the base character traits a human being may acquire and manifest when he turns away from God. Actions, that are described as "satanic" in some Bahá'í writings, denote humans deeds caused by selfish desires.

Theistic Satanism, commonly referred to as "devil worship", views Satan as a deity, whom individuals may supplicate to. It consists of loosely affiliated or independent groups and cabals, which all agree that Satan is a real entity.

Atheistic Satanism, as practiced by the Satanic Temple and by followers of LaVeyan Satanism, holds that Satan does not exist as a literal anthropomorphic entity, but rather as a symbol of a cosmos which Satanists perceive to be permeated and motivated by a force that has been given many names by humans over the course of time. In this religion, "Satan" is not viewed or depicted as a hubristic, irrational, and fraudulent creature, but rather is revered with Prometheus-like attributes, symbolizing liberty and individual empowerment. To adherents, he also serves as a conceptual framework and an external metaphorical projection of the Satanist's highest personal potential. In his essay "Satanism: The Feared Religion", the current High Priest of the Church of Satan, Peter H. Gilmore, further expounds that "...Satan is a symbol of Man living as his prideful, carnal nature dictates. The reality behind Satan is simply the dark evolutionary force of entropy that permeates all of nature and provides the drive for survival and propagation inherent in all living things. Satan is not a conscious entity to be worshiped, rather a reservoir of power inside each human to be tapped at will".

LaVeyan Satanists embrace the original etymological meaning of the word "Satan" (Hebrew: שָּׂטָן "satan", meaning "adversary"). According to Peter H. Gilmore, "The Church of Satan has chosen Satan as its primary symbol because in Hebrew it means adversary, opposer, one to accuse or question. We see ourselves as being these Satans; the adversaries, opposers and accusers of all spiritual belief systems that would try to hamper enjoyment of our life as a human being."

Post-LaVeyan Satanists, like the adherents of The Satanic Temple, argue that the human animal has a natural altruistic and communal tendency, and frame Satan as a figure of struggle against injustice and activism. They also believe in bodily autonomy, that personal beliefs should conform to science and inspire nobility, and that people should atone for their mistakes.

The main deity in the tentatively Indo-European pantheon of the Yazidis, Melek Taus, is similar to the devil in Christian and Islamic traditions, as he refused to bow down before humanity. Therefore, Christians and Muslims often consider Melek Taus to be Satan. However, rather than being Satanic, Yazidism can be understood as a remnant of a pre-Islamic Middle Eastern Indo-European religion, and/or a ghulat Sufi movement founded by Shaykh Adi. In fact, there is no entity in Yazidism which represents evil in opposition to God; such dualism is rejected by Yazidis.

In the Middle Ages, the Cathars, practitioners of a dualistic religion, were accused of worshipping Satan by the Catholic Church. Pope Gregory IX stated in his work "Vox in Rama" that the Cathars believed that God had erred in casting Lucifer out of heaven and that Lucifer would return to reward his faithful. On the other hand, according to Catharism, the creator god of the material world worshipped by the Catholic Church is actually Satan.

Wicca is a modern, syncretic Neopagan religion, whose practitioners many Christians have incorrectly assumed to worship Satan. In actuality, Wiccans do not believe in the existence of Satan or any analogous figure and have repeatedly and emphatically rejected the notion that they venerate such an entity. The cult of the skeletal figure of Santa Muerte, which has grown exponentially in Mexico, has been denounced by the Catholic Church as Devil-worship. However, devotees of Santa Muerte view her as an angel of death created by God, and many of them identify as Catholic.

Much modern folklore about Satanism does not originate from the actual beliefs or practices of theistic or atheistic Satanists, but rather from a mixture of medieval Christian folk beliefs, political or sociological conspiracy theories, and contemporary urban legends. An example is the Satanic ritual abuse scare of the 1980s—beginning with the memoir "Michelle Remembers"—which depicted Satanism as a vast conspiracy of elites with a predilection for child abuse and human sacrifice. This genre frequently describes Satan as physically incarnating in order to receive worship.

In Dante Alighieri's "Inferno", Satan appears as a giant demon, frozen mid-breast in ice at the center of the Ninth Circle of Hell. Satan has three faces and a pair of bat-like wings affixed under each chin. In his three mouths, Satan gnaws on Brutus, Judas Iscariot, and Cassius, whom Dante regarded as having betrayed the "two greatest heroes of the human race": Julius Caesar, the founder of the new order of government, and Jesus, the founder of the new order of religion. As Satan beats his wings, he creates a cold wind that continues to freeze the ice surrounding him and the other sinners in the Ninth Circle. Dante and Virgil climb up Satan's shaggy legs until gravity is reversed and they fall through the earth into the southern hemisphere.

Satan appears in several stories from "The Canterbury Tales" by Geoffrey Chaucer, including "The Summoner's Prologue", in which a friar arrives in Hell and sees no other friars, but is told there are millions. Then Satan lifts his tail to reveal that all of the friars live inside his anus. Chaucer's description of Satan's appearance is clearly based on Dante's. The legend of Faust, recorded in the 1589 chapbook "The History of the Damnable Life and the Deserved Death of Doctor John Faustus", concerns a pact allegedly made by the German scholar Johann Georg Faust with a demon named Mephistopheles agreeing to sell his soul to Satan in exchange for twenty-four years of earthly pleasure. This chapbook became the source for Christopher Marlowe's "The Tragical History of the Life and Death of Doctor Faustus".

John Milton's epic poem "Paradise Lost" features Satan as its main protagonist. Milton portrays Satan as a tragic antihero destroyed by his own hubris. The poem, which draws extensive inspiration from Greek tragedy, recreates Satan as a complex literary character, who dares to rebel against the "tyranny" of God, in spite of God's own omnipotence. The English poet and painter William Blake famously quipped that "The reason Milton wrote in fetters when he wrote of Angels & God, and at liberty when of Devils & Hell, is because he was a true poet and of the Devils party without knowing it." "Paradise Regained", the sequel to "Paradise Lost", is a retelling of Satan's temptation of Jesus in the desert.

William Blake regarded Satan as a model of rebellion against unjust authority and features him in many of his poems and illustrations, including his 1780 book "The Marriage of Heaven and Hell", in which Satan is celebrated as the ultimate rebel, the incarnation of human emotion and the epitome of freedom from all forms of reason and orthodoxy. Based on the Biblical passages portraying Satan as the accuser of sin, Blake interpreted Satan as "a promulgator of moral laws."

Satan's appearance does not appear in the Bible or in early Christian writings, though Paul the Apostle does write that "Satan disguises himself as an angel of light" (). The Devil was never shown in early Christian artwork and may have first appeared in the sixth century in one of the mosaics of the Basilica of Sant'Apollinare Nuovo. The mosaic "Christ the Good Sheppard" features a blue-violet angel at the left hand side of Christ behind three goats; opposite to a red angel on the right hand side and in front of sheep. Depictions of the devil became more common in the ninth century, where he is shown with cloven hooves, hairy legs, the tail of a goat, pointed ears, a beard, a flat nose, and a set of horns. Satan may have first become associated with goats through the Parable of the Sheep and the Goats, recorded in , in which Jesus separates sheep (representing the saved) from goats (representing the damned); the damned are thrown into a hell along with "the devil and his angels."

Medieval Christians were known to adapt previously existing pagan iconography to suit depictions of Christian figures. Much of Satan's traditional iconography in Christianity appears to be derived from Pan, a rustic, goat-legged fertility god in ancient Greek religion. Early Christian writers such as Saint Jerome equated the Greek satyrs and the Roman fauns, whom Pan resembled, with demons. The Devil's pitchfork appears to have been adapted from the trident wielded by the Greek god Poseidon and Satan's flame-like hair seems to have originated from the Egyptian god Bes. By the High Middle Ages, Satan and devils appear in all works of Christian art: in paintings, sculptures, and on cathedrals. Satan is usually depicted naked, but his genitals are rarely shown and are often covered by animal furs. The goat-like portrayal of Satan was especially closely associated with him in his role as the object of worship by sorcerers and as the incubus, a demon believed to rape human women in their sleep.

Italian frescoes from the late Middle Ages onward frequently show Satan chained in Hell, feeding on the bodies of the perpetually damned. These frescoes are early enough to have inspired Dante's portrayal in his "Inferno". As the serpent in the Garden of Eden, Satan is often shown as a snake with arms and legs as well the head and full-breasted upper torso of a woman. Satan and his demons could take any form in medieval art, but, when appearing in their true form, they were often shown as short, hairy, black-skinned humanoids with clawed and bird feet and extra faces on their chests, bellies, genitals, buttocks, and tails. The modern popular culture image of Satan as a well-dressed gentleman with small horns and a tail originates from portrayals of Mephistopheles in the operas "La damnation de Faust" (1846) by Hector Berlioz, "Mefistofele" (1868) by Arrigo Boito, and "Faust" by Charles Gounod.

The Devil is depicted as a vampire bat in Georges Méliès' "The Haunted Castle" (1896), which is often considered the first horror film. So-called "Black Masses" have been portrayed in sensationalist B-movies since the 1960s. One of the first films to portray such a ritual was the 1965 film "Eye of the Devil", also known as "13". Alex Sanders, a former black magician, served as a consultant on the film to ensure that the rituals portrayed in it were depicted accurately. Over the next thirty years, the novels of Dennis Wheatley and the films of Hammer Film Productions both played a major role in shaping the popular image of Satanism.

The film version of Ira Levin's "Rosemary's Baby" established made Satanic themes a staple of mainstream horror fiction. Later films such as "The Exorcist" (1973), "The Omen" (1976) and "Angel Heart" (1987) feature Satan as an antagonist.

References to Satan in music can be dated back to the Middle Ages. During the fifth century, a musical interval called the tritone became known as "the devil in Music" and was banned by the Catholic Church. Giuseppe Tartini was inspired to write his most famous work, the Violin Sonata in G minor, also known as "The Devil's Trill", after dreaming of the Devil playing the violin. Tartini claimed that the sonata was a lesser imitation of what the Devil had played in his dream. Niccolò Paganini was believed to have derived his musical talent from a deal with the Devil. Charles Gounod's "Faust" features a narrative that involves Satan.

In the early 1900s, jazz and blues became known as the "Devil's Music" as they were considered "dangerous and unholy". According to legend, blues musician Tommy Johnson was a terrible guitarist before exchanging his soul to the Devil for a guitar. Later, Robert Johnson claimed that he had sold his soul in return for becoming a great blues guitarist. Satanic symbolism appears in rock music from the 1960s. Mick Jagger assumes the role of Lucifer in the Rolling Stones' "Sympathy for the Devil" (1968), while Black Sabbath portrayed the Devil in numerous songs, including "War Pigs" (1970) and "N.I.B." (1970).





</doc>
<doc id="27695" url="https://en.wikipedia.org/wiki?curid=27695" title="Structured programming">
Structured programming

Structured programming is a programming paradigm aimed at improving the clarity, quality, and development time of a computer program by making extensive use of the structured control flow constructs of selection (if/then/else) and repetition (while and for), block structures, and subroutines.

It emerged in the late 1950s with the appearance of the ALGOL 58 and ALGOL 60 programming languages, with the latter including support for block structures. Contributing factors to its popularity and widespread acceptance, at first in academia and later among practitioners, include the discovery of what is now known as the structured program theorem in 1966, and the publication of the influential "Go To Statement Considered Harmful" open letter in 1968 by Dutch computer scientist Edsger W. Dijkstra, who coined the term "structured programming".

Structured programming is most frequently used with deviations that allow for clearer programs in some particular cases, such as when exception handling has to be performed.

Following the structured program theorem, all programs are seen as composed of control structures:

Subroutines; callable units such as procedures, functions, methods, or subprograms are used to allow a sequence to be referred to by a single statement.

Blocks are used to enable groups of statements to be treated as if they were one statement. "Block-structured" languages have a syntax for enclosing structures in some formal way, such as an if-statement bracketed by codice_6 as in ALGOL 68, or a code section bracketed by codice_7, as in PL/I and Pascal, whitespace indentation as in Python - or the curly braces codice_8 of C and many later languages.

It is possible to do structured programming in any programming language, though it is preferable to use something like a procedural programming language. Some of the languages initially used for structured programming include: ALGOL, Pascal, PL/I and Ada, but most new procedural programming languages since that time have included features to encourage structured programming, and sometimes deliberately left out features – notably GOTO – in an effort to make unstructured programming more difficult.
"Structured programming" (sometimes known as modular programming) enforces a logical structure on the program being written to make it more efficient and easier to understand and modify.

The structured program theorem provides the theoretical basis of structured programming. It states that three ways of combining programs—sequencing, selection, and iteration—are sufficient to express any computable function. This observation did not originate with the structured programming movement; these structures are sufficient to describe the instruction cycle of a central processing unit, as well as the operation of a Turing machine. Therefore, a processor is always executing a "structured program" in this sense, even if the instructions it reads from memory are not part of a structured program. However, authors usually credit the result to a 1966 paper by Böhm and Jacopini, possibly because Dijkstra cited this paper himself. The structured program theorem does not address how to write and analyze a usefully structured program. These issues were addressed during the late 1960s and early 1970s, with major contributions by Dijkstra, Robert W. Floyd, Tony Hoare, Ole-Johan Dahl, and David Gries.

P. J. Plauger, an early adopter of structured programming, described his reaction to the structured program theorem:

Donald Knuth accepted the principle that programs must be written with provability in mind, but he disagreed (and still disagrees) with abolishing the GOTO statement. In his 1974 paper, "Structured Programming with Goto Statements", he gave examples where he believed that a direct jump leads to clearer and more efficient code without sacrificing provability. Knuth proposed a looser structural constraint: It should be possible to draw a program's flow chart with all forward branches on the left, all backward branches on the right, and no branches crossing each other. Many of those knowledgeable in compilers and graph theory have advocated allowing only reducible flow graphs.

Structured programming theorists gained a major ally in the 1970s after IBM researcher Harlan Mills applied his interpretation of structured programming theory to the development of an indexing system for "The New York Times" research file. The project was a great engineering success, and managers at other companies cited it in support of adopting structured programming, although Dijkstra criticized the ways that Mills's interpretation differed from the published work.

As late as 1987 it was still possible to raise the question of structured programming in a computer science journal. Frank Rubin did so in that year with an open letter titled ""GOTO considered harmful" considered harmful". Numerous objections followed, including a response from Dijkstra that sharply criticized both Rubin and the concessions other writers made when responding to him.

By the end of the 20th century nearly all computer scientists were convinced that it is useful to learn and apply the concepts of structured programming. High-level programming languages that originally lacked programming structures, such as FORTRAN, COBOL, and BASIC, now have them.

While goto has now largely been replaced by the structured constructs of selection (if/then/else) and repetition (while and for), few languages are purely structured. The most common deviation, found in many languages, is the use of a return statement for early exit from a subroutine. This results in multiple exit points, instead of the single exit point required by structured programming. There are other constructions to handle cases that are awkward in purely structured programming.

The most common deviation from structured programming is early exit from a function or loop. At the level of functions, this is a codice_9 statement. At the level of loops, this is a codice_10 statement (terminate the loop) or codice_11 statement (terminate the current iteration, proceed with next iteration). In structured programming, these can be replicated by adding additional branches or tests, but for returns from nested code this can add significant complexity. C is an early and prominent example of these constructs. Some newer languages also have "labeled breaks", which allow breaking out of more than just the innermost loop. Exceptions also allow early exit, but have further consequences, and thus are treated below.

Multiple exits can arise for a variety of reasons, most often either that the subroutine has no more work to do (if returning a value, it has completed the calculation), or has encountered "exceptional" circumstances that prevent it from continuing, hence needing exception handling.

The most common problem in early exit is that cleanup or final statements are not executed – for example, allocated memory is not deallocated, or open files are not closed, causing memory leaks or resource leaks. These must be done at each return site, which is brittle and can easily result in bugs. For instance, in later development, a return statement could be overlooked by a developer, and an action which should be performed at the end of a subroutine (e.g., a trace statement) might not be performed in all cases. Languages without a return statement, such as standard Pascal and Seed7, do not have this problem.

Most modern languages provide language-level support to prevent such leaks; see detailed discussion at resource management. Most commonly this is done via unwind protection, which ensures that certain code is guaranteed to be run when execution exits a block; this is a structured alternative to having a cleanup block and a codice_12. This is most often known as codice_13 and considered a part of exception handling. In case of multiple codice_9 statements introducing codice_13 without exceptions might look strange. Various techniques exist to encapsulate resource management. An alternative approach, found primarily in C++, is Resource Acquisition Is Initialization, which uses normal stack unwinding (variable deallocation) at function exit to call destructors on local variables to deallocate resources.

Kent Beck, Martin Fowler and co-authors have argued in their refactoring books that nested conditionals may be harder to understand than a certain type of flatter structure using multiple exits predicated by guard clauses. Their 2009 book flatly states that "one exit point is really not a useful rule. Clarity is the key principle: If the method is clearer with one exit point, use one exit point; otherwise don’t". They offer a cookbook solution for transforming a function consisting only of nested conditionals into a sequence of guarded return (or throw) statements, followed by a single unguarded block, which is intended to contain the code for the common case, while the guarded statements are supposed to deal with the less common ones (or with errors). Herb Sutter and Andrei Alexandrescu also argue in their 2004 C++ tips book that the single-exit point is an obsolete requirement.

In his 2004 textbook, David Watt writes that "single-entry multi-exit control flows are often desirable". Using Tennent's framework notion of sequencer, Watt uniformly describes the control flow constructs found in contemporary programming languages and attempts to explain why certain types of sequencers are preferable to others in the context of multi-exit control flows. Watt writes that unrestricted gotos (jump sequencers) are bad because the destination of the jump is not self-explanatory to the reader of a program until the reader finds and examines the actual label or address that is the target of the jump. In contrast, Watt argues that the conceptual intent of a return sequencer is clear from its own context, without having to examine its destination. Watt writes that a class of sequencers known as "escape sequencers", defined as a "sequencer that terminates execution of a textually enclosing command or procedure", encompasses both breaks from loops (including multi-level breaks) and return statements. Watt also notes that while jump sequencers (gotos) have been somewhat restricted in languages like C, where the target must be an inside the local block or an encompassing outer block, that restriction alone is not sufficient to make the intent of gotos in C self-describing and so they can still produce "spaghetti code". Watt also examines how exception sequencers differ from escape and jump sequencers; this is explained in the next section of this article.

In contrast to the above, Bertrand Meyer wrote in his 2009 textbook that instructions like codice_10 and codice_11 "are just the old codice_12 in sheep's clothing" and strongly advised against their use.

Based on the coding error from the Ariane 501 disaster, software developer Jim Bonang argues that any exceptions thrown from a function violate the single-exit paradigm, and proposes that all inter-procedural exceptions should be forbidden. In C++ syntax, this is done by declaring all function signatures as codice_19 (since C++11) or codice_20. Bonang proposes that all single-exit conforming C++ should be written along the lines of:
bool MyCheck1() throw() {

Peter Ritchie also notes that, in principle, even a single codice_21 right before the codice_9 in a function constitutes a violation of the single-exit principle, but argues that Dijkstra's rules were written in a time before exception handling became a paradigm in programming languages, so he proposes to allow any number of throw points in addition to a single return point. He notes that solutions which wrap exceptions for the sake of creating a single-exit have higher nesting depth and thus are more difficult to comprehend, and even accuses those who propose to apply such solutions to programming languages which support exceptions of engaging in cargo cult thinking.

David Watt also analyzes exception handling in the framework of sequencers (introduced in this article in the previous section on early exits.) Watt notes that an abnormal situation (generally exemplified with arithmetic 
overflows or input/output failures like file not found) is a kind of error that "is detected in some low-level program unit, but [for which] a handler is more naturally located in a high-level program unit". For example, a program might contain several calls to read files, but the action to perform when a file is not found depends on the meaning (purpose) of the file in question to the program and thus a handling routine for this abnormal situation cannot be located in low-level system code. Watts further notes that introducing status flags testing in the caller, as single-exit structured programming or even (multi-exit) return sequencers would entail, results in a situation where "the application code tends to get cluttered by tests of status flags" and that "the programmer might forgetfully or lazily omit to test a status flag. In fact, abnormal situations represented by status flags are by default ignored!" He notes that in contrast to status flags testing, exceptions have the opposite default behavior, causing the program to terminate unless the programmer explicitly deals with the exception in some way, possibly by adding code to willfully ignore it. Based on these arguments, Watt concludes that jump sequencers or escape sequencers (discussed in the previous section) aren't as suitable as a dedicated exception sequencer with the semantics discussed above.

The textbook by Louden and Lambert emphasizes that exception handling differs from structured programming constructs like codice_2 loops because the transfer of control "is set up at a different point in the program than that where the actual transfer takes place. At the point where the transfer actually occurs, there may be no syntactic indication that control will in fact be transferred." Computer science professor Arvind Kumar Bansal also notes that in languages which implement exception handling, even control structures like codice_4, which have the single-exit property in absence of exceptions, no longer have it in presence of exceptions, because an exception can prematurely cause an early exit in any part of the control structure; for instance if codice_25 throws an exception in codice_26, then the usual exit point after check() is not reached. Citing multiple prior studies by others (1999-2004) and their own results, Westley Weimer and George Necula wrote that a significant problem with exceptions is that they "create hidden control-flow paths that are difficult for programmers to reason about".

The necessity to limit code to single-exit points appears in some contemporary programming environments focused on parallel computing, such as OpenMP. The various parallel constructs from OpenMP, like codice_27, do not allow early exits from inside to the outside of the parallel construct; this restriction includes all manner of exits, from codice_10 to C++ exceptions, but all of these are permitted inside the parallel construct if the jump target is also inside it.

More rarely, subprograms allow multiple "entry." This is most commonly only "re"-entry into a coroutine (or generator/semicoroutine), where a subprogram yields control (and possibly a value), but can then be resumed where it left off. There are a number of common uses of such programming, notably for streams (particularly input/output), state machines, and concurrency. From a code execution point of view, yielding from a coroutine is closer to structured programming than returning from a subroutine, as the subprogram has not actually terminated, and will continue when called again – it is not an early exit. However, coroutines mean that multiple subprograms have execution state – rather than a single call stack of subroutines – and thus introduce a different form of complexity.

It is very rare for subprograms to allow entry to an arbitrary position in the subprogram, as in this case the program state (such as variable values) is uninitialized or ambiguous, and this is very similar to a goto.

Some programs, particularly parsers and communications protocols, have a number of states that follow each other in a way that is not easily reduced to the basic structures, and some programmers implement the state-changes with a jump to the new state. This type of state-switching is often used in the Linux kernel.

However, it is possible to structure these systems by making each state-change a separate subprogram and using a variable to indicate the active state (see trampoline). Alternatively, these can be implemented via coroutines, which dispense with the trampoline.





</doc>
<doc id="27696" url="https://en.wikipedia.org/wiki?curid=27696" title="Semiconductor device fabrication">
Semiconductor device fabrication

Semiconductor device fabrication is the process used to manufacture semiconductor devices, typically the metal–oxide–semiconductor (MOS) devices used in the integrated circuit (IC) chips that are present in everyday electrical and electronic devices. It is a multiple-step sequence of photolithographic and chemical processing steps (such as surface passivation, thermal oxidation, planar diffusion and junction isolation) during which electronic circuits are gradually created on a wafer made of pure semiconducting material. Silicon is almost always used, but various compound semiconductors are used for specialized applications.

The entire manufacturing process, from start to packaged chips ready for shipment, takes six to eight weeks and is performed in highly specialized semiconductor fabrication plants, also called foundries or fabs. In more advanced semiconductor devices, such as modern 14/10/7 nm nodes, fabrication can take up to 15 weeks, with 11–13 weeks being the industry average. Production in advanced fabrication facilities is completely automated and carried out in a hermetically sealed nitrogen environment to improve yield (the percent of microchips that function correctly in a wafer), with automated material handling systems taking care of the transport of wafers from machine to machine. All machinery contains an internal nitrogen atmosphere. The air inside the machinery is usually kept cleaner than the surrounding air in the cleanroom. This internal atmosphere is known as a mini-environment. Fabrication plants need large amounts of liquid nitrogen to maintain the atmosphere inside production machinery, which is constantly purged with nitrogen.

A specific semiconductor process has specific rules on the minimum size and spacing for features on each layer of the chip.
Often a newer semiconductor processes allows a simple die shrink to reduce costs and improve performance.
Early semiconductor processes had arbitrary names such as HMOS III, CHMOS V;
later ones are referred to by size such as 90 nm process.

By industry standard, each generation of the semiconductor manufacturing process, also known as technology node, is designated by the process’ minimum feature size. Technology nodes, also known as "process technologies" or simply "nodes", are typically indicated by the size in nanometers (or historically micrometers) of the process' transistor gate length.

The first metal–oxide–silicon field-effect transistors (MOSFETs) were fabricated by Egyptian engineer Mohamed M. Atalla and Korean engineer Dawon Kahng at Bell Labs between 1959 and 1960. There were originally two types of MOSFET technology, PMOS (p-type MOS) and NMOS (n-type MOS). Both types were developed by Atalla and Kahng when they originally invented the MOSFET, fabricating both PMOS and NMOS devices at 20µm and 10µm scales.

An improved type of MOSFET technology, CMOS, was developed by Chih-Tang Sah and Frank Wanlass at Fairchild Semiconductor in 1963. CMOS was commercialised by RCA in the late 1960s. RCA commercially used CMOS for its 4000-series integrated circuits in 1968, starting with a 20µm process before gradually scaling to a 10 µm process over the next several years.

Semiconductor device manufacturing has since spread from Texas and California in the 1960s to the rest of the world, including Asia, Europe, and the Middle East.

The semiconductor industry is a global business today. The leading semiconductor manufacturers typically have facilities all over the world. Samsung Electronics, the world's largest manufacturer of semiconductors, has facilities in South Korea and the US. Intel, the second-largest manufacturer, has facilities in Europe and Asia as well as the US. TSMC, the world's largest pure play foundry, has facilities in Taiwan, China, Singapore, and the US. Qualcomm and Broadcom are among the biggest fabless semiconductor companies, outsourcing their production to companies like TSMC. They also have facilities spread in different countries.

Since 2009, "node" has become a commercial name for marketing purposes that indicates new generations of process technologies, without any relation to gate length, metal pitch or gate pitch. For example, GlobalFoundries' 7 nm process is similar to Intel's 10 nm process, thus the conventional notion of a process node has become blurred. Additionally, TSMC and Samsung's 10 nm processes are only slightly denser than Intel's 14 nm in transistor density. They are actually much closer to Intel's 14 nm process than they are to Intel's 10 nm process (e.g. Samsung's 10 nm processes' fin pitch is the exact same as that of Intel's 14 nm process: 42 nm).

As of 2019, 14 nanometer and 10 nanometer chips are in mass production by Intel, UMC, TSMC, Samsung, Micron, SK Hynix, Toshiba Memory and GlobalFoundries, with 7 nanometer process chips in mass production by TSMC and Samsung, although their 7nanometer node definition is similar to Intel's 10 nanometer process. The 5 nanometer process began being produced by Samsung in 2018. As of 2019, the node with the highest transistor density is TSMC's 5nanometer N5 node, with a density of 171.3million transistors per square millimeter. In 2019, Samsung and TSMC announced plans to produce 3 nanometer nodes. GlobalFoundries has decided to stop the development of new nodes beyond 12 nanometers in order to save resources, as it has determined that setting up a new fab to handle sub-12nm orders would be beyond the company's financial abilities. , Samsung is the industry leader in advanced semiconductor scaling, followed by TSMC and then Intel.

This is a list of processing techniques that are employed numerous times throughout the construction of a modern electronic device; this list does not necessarily imply a specific order. Equipment for carrying out these processes is made by a handful of companies. All equipment needs to be tested before a semiconductor fabrication plant is started. 

When feature widths were far greater than about 10 micrometres, semiconductor purity was not as big of an issue as it is today in device manufacturing. As devices become more integrated, cleanrooms must become even cleaner. Today, fabrication plants are pressurized with filtered air to remove even the smallest particles, which could come to rest on the wafers and contribute to defects. The workers in a semiconductor fabrication facility are required to wear cleanroom suits to protect the devices from human contamination. To prevent oxidation and to increase yield, FOUPs and semiconductor capital equipment may have a pure nitrogen environment with ISO class 1 level of dust.

A typical wafer is made out of extremely pure silicon that is grown into mono-crystalline cylindrical ingots (boules) up to 300 mm (slightly less than 12 inches) in diameter using the Czochralski process. These ingots are then sliced into wafers about 0.75 mm thick and polished to obtain a very regular and flat surface.

In semiconductor device fabrication, the various processing steps fall into four general categories: deposition, removal, patterning, and modification of electrical properties.
Modern chips have up to eleven metal levels produced in over 300 sequenced processing steps.

FEOL processing refers to the formation of the transistors directly in the silicon. The raw wafer is engineered by the growth of an ultrapure, virtually defect-free silicon layer through epitaxy. In the most advanced logic devices, "prior" to the silicon epitaxy step, tricks are performed to improve the performance of the transistors to be built. One method involves introducing a "straining step" wherein a silicon variant such as silicon-germanium (SiGe) is deposited. Once the epitaxial silicon is deposited, the crystal lattice becomes stretched somewhat, resulting in improved electronic mobility. Another method, called "silicon on insulator" technology involves the insertion of an insulating layer between the raw silicon wafer and the thin layer of subsequent silicon epitaxy. This method results in the creation of transistors with reduced parasitic effects.

Front-end surface engineering is followed by growth of the gate dielectric (traditionally silicon dioxide), patterning of the gate, patterning of the source and drain regions, and subsequent implantation or diffusion of dopants to obtain the desired complementary electrical properties. In dynamic random-access memory (DRAM) devices, storage capacitors are also fabricated at this time, typically stacked above the access transistor (the now defunct DRAM manufacturer Qimonda implemented these capacitors with trenches etched deep into the silicon surface).

Once the various semiconductor devices have been created, they must be interconnected to form the desired electrical circuits. This occurs in a series of wafer processing steps collectively referred to as BEOL (not to be confused with "back end" of chip fabrication, which refers to the packaging and testing stages). BEOL processing involves creating metal interconnecting wires that are isolated by dielectric layers. The insulating material has traditionally been a form of SiO or a silicate glass, but recently new low dielectric constant materials are being used (such as silicon oxycarbide), typically providing dielectric constants around 2.7 (compared to 3.82 for SiO), although materials with constants as low as 2.2 are being offered to chipmakers.

Historically, the metal wires have been composed of aluminum. In this approach to wiring (often called "subtractive aluminum"), blanket films of aluminum are deposited first, patterned, and then etched, leaving isolated wires. Dielectric material is then deposited over the exposed wires. The various metal layers are interconnected by etching holes (called ""vias")" in the insulating material and then depositing tungsten in them with a CVD technique; this approach is still used in the fabrication of many memory chips such as dynamic random-access memory (DRAM), because the number of interconnect levels is small (currently no more than four).

More recently, as the number of interconnect levels for logic has substantially increased due to the large number of transistors that are now interconnected in a modern microprocessor, the timing delay in the wiring has become so significant as to prompt a change in wiring material (from aluminum to copper interconnect layer) and a change in dielectric material (from silicon dioxides to newer low-K insulators). This performance enhancement also comes at a reduced cost via damascene processing, which eliminates processing steps. As the number of interconnect levels increases, planarization of the previous layers is required to ensure a flat surface prior to subsequent lithography. Without it, the levels would become increasingly crooked, extending outside the depth of focus of available lithography, and thus interfering with the ability to pattern. CMP (chemical-mechanical planarization) is the primary processing method to achieve such planarization, although dry "etch back" is still sometimes employed when the number of interconnect levels is no more than three.

The highly serialized nature of wafer processing has increased the demand for metrology in between the various processing steps. For example, thin film metrology based on ellipsometry or reflectometry is used to tightly control the thickness of gate oxide, as well as the thickness, refractive index and extinction coefficient of photoresist and other coatings. Wafer test metrology equipment is used to verify that the wafers haven't been damaged by previous processing steps up until testing; if too many dies on one wafer have failed, the entire wafer is scrapped to avoid the costs of further processing. Virtual metrology has been used to predict wafer properties based on statistical methods without performing the physical measurement itself.

Once the front-end process has been completed, the semiconductor devices are subjected to a variety of electrical tests to determine if they function properly. The percent of devices on the wafer found to perform properly is referred to as the yield. Manufacturers are typically secretive about their yields, but it can be as low as 30%, meaning that only 30% of the chips on the wafer work as intended. Process variation is one among many reasons for low yield.

The number of defects is often but not necessarily proportional to device (die) size. As an example, In December 2019, TSMC announced an average yield of ~80%, with a peak yield per wafer of >90% for their 5nm test chips with a die size of 17.92 mm. The yield went down to 32.0% with an increase in die size to 100 mm. 

The fab tests the chips on the wafer with an electronic tester that presses tiny probes against the chip. The machine marks each bad chip with a drop of dye. Currently, electronic dye marking is possible if wafer test data is logged into a central computer database and chips are "binned" (i.e. sorted into virtual bins) according to the predetermined test limits. The resulting binning data can be graphed, or logged, on a wafer map to trace manufacturing defects and mark bad chips. This map can also be used during wafer assembly and packaging.

Chips are also tested again after packaging, as the bond wires may be missing, or analog performance may be altered by the package. This is referred to as the "final test".

Usually, the fab charges for testing time, with prices in the order of cents per second. Testing times vary from a few milliseconds to a couple of seconds, and the test software is optimized for reduced testing time. Multiple chip (multi-site) testing is also possible because many testers have the resources to perform most or all of the tests in parallel.

Chips are often designed with "testability features" such as scan chains or a "built-in self-test" to speed testing and reduce testing costs. In certain designs that use specialized analog fab processes, wafers are also laser-trimmed during testing, in order to achieve tightly-distributed resistance values as specified by the design.

Good designs try to test and statistically manage "corners" (extremes of silicon behavior caused by a high operating temperature combined with the extremes of fab processing steps). Most designs cope with at least 64 corners.

Once tested, a wafer is typically reduced in thickness in a process also known as "backlap", "backfinish" or "wafer thinning" before the wafer is scored and then broken into individual dice, a process known as wafer dicing. Only the good, unmarked chips are packaged.

Plastic or ceramic packaging involves mounting the die, connecting the die pads to the pins on the package, and sealing the die. Tiny bondwires are used to connect the pads to the pins. In the old days, wires were attached by hand, but now specialized machines perform the task. Traditionally, these wires have been composed of gold, leading to a lead frame (pronounced "leed frame") of solder-plated copper; lead is poisonous, so lead-free "lead frames" are now mandated by RoHS.

Chip scale package (CSP) is another packaging technology. A plastic dual in-line package, like most packages, is many times larger than the actual die hidden inside, whereas CSP chips are nearly the size of the die; a CSP can be constructed for each die "before" the wafer is diced.

The packaged chips are retested to ensure that they were not damaged during packaging and that the die-to-pin interconnect operation was performed correctly. A laser then etches the chip's name and numbers on the package.

Many toxic materials are used in the fabrication process. These include:

It is vital that workers should not be directly exposed to these dangerous substances. The high degree of automation common in the IC fabrication industry helps to reduce the risks of exposure. Most fabrication facilities employ exhaust management systems, such as wet scrubbers, combustors, heated absorber cartridges, etc., to control the risk to workers and to the environment.




</doc>
<doc id="27698" url="https://en.wikipedia.org/wiki?curid=27698" title="Sanskrit">
Sanskrit

Sanskrit (; , ) is an Indo-Aryan or Indic language of the ancient Indian subcontinent with a 3,500-year history. It is the primary liturgical language of Hinduism and the predominant language of most works of Hindu philosophy as well as some of the principal texts of Buddhism and Jainism. Sanskrit, in its variants and numerous dialects, was the "lingua franca" of ancient and medieval India. In the early 1st millennium CE, along with Buddhism and Hinduism, Sanskrit migrated to Southeast Asia, parts of East Asia and Central Asia, emerging as a language of high culture and of local ruling elites in these regions.

Sanskrit is an Old Indo-Aryan language. As one of the oldest documented members of the Indo-European family of languages, Sanskrit holds a prominent position in Indo-European studies. It is related to Greek and Latin, as well as Hittite, Luwian, Old Avestan and many other living and extinct languages with historical significance to Europe, West Asia, Central Asia and South Asia. It traces its linguistic ancestry to the Proto-Indo-Aryan, Proto-Indo-Iranian and Proto-Indo-European languages.

Sanskrit is traceable to the 2nd millennium BCE in a form known as Vedic Sanskrit, with the "Rigveda" as the earliest-known composition. A more refined and standardized grammatical form called Classical Sanskrit emerged in the mid-1st millennium BCE with the "Aṣṭādhyāyī" treatise of Pāṇini. The broader definition of Sanskrit refers to the whole range of mutually intelligible Old Indo-Aryan dialects spoken in North-western India at the time of the composition of the Vedas and thus can be treated as the ancestor of the Prakrits and Pali, and consequently, of all Modern Indo-Aryan languages like Hindi, Marathi, Bengali, Punjabi, Gujarati, Sindhi, Kashmiri, Kumaoni, Garhwali, Urdu, Dogri, Maithili, Konkani, Assamese, Odia, and Nepali. On the other hand, the standardized Classical and Epic Sanskrit was a literary language and not the ancestor of any Indo-Aryan language.

The body of Sanskrit literature encompasses a rich tradition of philosophical and religious texts, as well as poetry, music, drama, scientific, technical and other texts. In the ancient era, Sanskrit compositions were orally transmitted by methods of memorisation of exceptional complexity, rigour and fidelity. The earliest known inscriptions in Sanskrit are from the 1st century BCE, such as the few discovered in Ayodhya and Ghosundi-Hathibada (Chittorgarh). Sanskrit texts dated to the 1st millennium CE were written in the Brahmi script, the Nāgarī script, the historic South Indian scripts and their derivative scripts. Sanskrit is one of the 22 languages listed in the Eighth Schedule of the Constitution of India. More than 3,000 Sanskrit works have been composed since India's independence in 1947. Sanskrit is a living language and spoken as a primary language in some villages in India. It is taught in a large number of schools in India. It also continues to be widely used as a ceremonial and ritual language in Hinduism and some Buddhist practices such as recitation of hymns and chants.

In Sanskrit verbal adjective "" is a compound word consisting of "sam" (together, good, well, perfected) and "krta-" (made, formed, work). It connotes a work that has been "well prepared, pure and perfect, polished, sacred". According to Biderman, the perfection contextually being referred to in the etymological origins of the word is its tonal—rather than semantic—qualities. Sound and oral transmission were highly valued qualities in ancient India, and its sages refined the alphabet, the structure of words and its exacting grammar into a "collection of sounds, a kind of sublime musical mold", states Biderman, as an integral language they called "Sanskrit". From the late Vedic period onwards, state Annette Wilke and Oliver Moebus, resonating sound and its musical foundations attracted an "exceptionally large amount of linguistic, philosophical and religious literature" in India. Sound was visualized as "pervading all creation", another representation of the world itself; the "mysterious magnum" of Hindu thought. The search for perfection in thought and the goal of liberation were among the dimensions of sacred sound, and the common thread that weaved all ideas and inspirations became the quest for what the ancient Indians believed to be a perfect language, the "phonocentric episteme" of Sanskrit.

Sanskrit as a language competed with numerous, less exact vernacular Indian languages called "Prakritic languages" (""). The term "prakrta" literally means "original, natural, normal, artless", states Franklin Southworth. The relationship between Prakrit and Sanskrit is found in Indian texts dated to the 1st millennium CE. Patañjali acknowledged that Prakrit is the first language, one instinctively adopted by every child with all its imperfections and later leads to the problems of interpretation and misunderstanding. The purifying structure of the Sanskrit language removes these imperfections. The early Sanskrit grammarian Daṇḍin states, for example, that much in the Prakrit languages is etymologically rooted in Sanskrit, but involve "loss of sounds" and corruptions that result from a "disregard of the grammar". Daṇḍin acknowledged that there are words and confusing structures in Prakrit that thrive independent of Sanskrit. This view is found in the writing of Bharata Muni, the author of the ancient "Nāṭyaśāstra" text. The early Jain scholar Namisādhu acknowledged the difference, but disagreed that the Prakrit language was a corruption of Sanskrit. Namisādhu stated that the Prakrit language was the "pūrvam" (came before, origin) and that it came naturally to children, while Sanskrit was a refinement of Prakrit through "purification by grammar".

Sanskrit belongs to the Indo-European family of languages. It is one of three ancient documented languages that arose from a common root language now referred to as Proto-Indo-European language:

Other Indo-European languages related to Sanskrit include archaic and classical Latin (c. 600 BCE – 100 CE, old Italian), Gothic (archaic Germanic language, c. 350 CE), Old Norse (c. 200 CE and after), Old Avestan (c. late 2nd millennium BCE) and Younger Avestan (c. 900 BCE). The closest ancient relatives of Vedic Sanskrit in the Indo-European languages are the Nuristani languages found in the remote Hindu Kush region of the northeastern Afghanistan and northwestern Himalayas, as well as the extinct Avestan and Old Persian—both Iranian languages. Sanskrit belongs to the satem group of the Indo-European languages.

Colonial era scholars familiar with Latin and Greek were struck by the resemblance of the Sanskrit language, both in its vocabulary and grammar, to the classical languages of Europe. It suggested a common root and historical links between some of the major distant ancient languages of the world.

In order to explain the common features shared by Sanskrit and other Indo-European languages, the Indo-Aryan migration theory states that the original speakers of what became Sanskrit arrived in the Indian subcontinent from the north-west sometime during the early second millennium BCE. Evidence for such a theory includes the close relationship between the Indo-Iranian tongues and the Baltic and Slavic languages, vocabulary exchange with the non-Indo-European Uralic languages, and the nature of the attested Indo-European words for flora and fauna. The pre-history of Indo-Aryan languages which preceded Vedic Sanskrit is unclear and various hypotheses place it over a fairly wide limit. According to Thomas Burrow, based on the relationship between various Indo-European languages, the origin of all these languages may possibly be in what is now Central or Eastern Europe, while the Indo-Iranian group possibly arose in Central Russia. The Iranian and Indo-Aryan branches separated quite early. It is the Indo-Aryan branch that moved into eastern Iran and then south into the Indian subcontinent in the first half of the 2nd millennium BCE. Once in ancient India, the Indo-Aryan language underwent rapid linguistic change and morphed into the Vedic Sanskrit language.

The pre-Classical form of Sanskrit is known as Vedic Sanskrit. The earliest attested Sanskrit text is the Rigveda, a Hindu scripture, from the mid-to-late second millennium BCE. No written records from such an early period survive if they ever existed. However, scholars are confident that the oral transmission of the texts is reliable: they were ceremonial literature where the exact phonetic expression and its preservation were a part of the historic tradition.

The "Rigveda" is a collection of books, created by multiple authors from distant parts of ancient India. These authors represented different generations, and the mandalas 2 to 7 are the oldest while the mandalas 1 and 10 are relatively the youngest. Yet, the Vedic Sanskrit in these books of the "Rigveda" "hardly presents any dialectical diversity", states Louis Renou—an Indologist known for his scholarship of the Sanskrit literature and the "Rigveda" in particular. According to Renou, this implies that the Vedic Sanskrit language had a "set linguistic pattern" by the second half of the 2nd millennium BCE. Beyond the "Rigveda", the ancient literature in Vedic Sanskrit that has survived into the modern age include the "Samaveda", "Yajurveda", "Atharvaveda" along with the embedded and layered Vedic texts such as the Brahmanas, Aranyakas and the early Upanishads. These Vedic documents reflect the dialects of Sanskrit found in the various parts of the northwestern, northern and eastern Indian subcontinent.

Vedic Sanskrit was both a spoken and literary language of ancient India. According to Michael Witzel, Vedic Sanskrit was a spoken language of the semi-nomadic Aryas who temporarily settled in one place, maintained cattle herds, practiced limited agriculture and after some time moved by wagon trains they called "grama". The Vedic Sanskrit language or a closely related Indo-European variant was recognized beyond ancient India as evidenced by the "Mitanni Treaty" between the ancient Hittite and Mitanni people, carved into a rock, in a region that are now parts of Syria and Turkey. Parts of this treaty such as the names of the Mitanni princes and technical terms related to horse training, for reasons not understood, are in early forms of Vedic Sanskrit. The treaty also invokes the gods Varuna, Mitra, Indra and Nasatya found in the earliest layers of the Vedic literature.
The Vedic Sanskrit found in the "Rigveda" is distinctly more archaic than other Vedic texts, and in many respects, the Rigvedic language is notably more similar to those found in the archaic texts of Old Avestan Zoroastrian "Gathas" and Homer's "Iliad" and "Odyssey". According to Stephanie W. Jamison and Joel P. Brereton—Indologists known for their translation of the "Rigveda"—the Vedic Sanskrit literature "clearly inherited" from Indo-Iranian and Indo-European times the social structures such as the role of the poet and the priests, the patronage economy, the phrasal equations, and some of the poetic meters. While there are similarities, state Jamison and Brereton, there are also differences between Vedic Sanskrit, the Old Avestan, and the Mycenaean Greek literature. For example, unlike the Sanskrit similes in the "Rigveda", the Old Avestan "Gathas" lack simile entirely, and it is rare in the later version of the language. The Homerian Greek, like Rigvedic Sanskrit, deploys simile extensively, but they are structurally very different.

The early Vedic form of the Sanskrit language was far less homogenous, and it evolved over time into a more structured and homogeneous language, ultimately into the Classical Sanskrit by about the mid-1st millennium BCE. According to Richard Gombrich—an Indologist and a scholar of Sanskrit, Pāli and Buddhist Studies—the archaic Vedic Sanskrit found in the "Rigveda" had already evolved in the Vedic period, as evidenced in the later Vedic literature. The language in the early Upanishads of Hinduism and the late Vedic literature approaches Classical Sanskrit, while the archaic Vedic Sanskrit had by the Buddha's time become unintelligible to all except ancient Indian sages, states Gombrich.

The formalization of the Sanskrit language is credited to , along with Patanjali's "Mahabhasya" and Katyayana's commentary that preceded Patanjali's work. Panini composed "" ("Eight-Chapter Grammar"). The century in which he lived is unclear and debated, but his work is generally accepted to be from sometime between 6th and 4th centuries BCE.

The was not the first description of Sanskrit grammar, but it is the earliest that has survived in full. Pāṇini cites ten scholars on the phonological and grammatical aspects of the Sanskrit language before him, as well as the variants in the usage of Sanskrit in different regions of India. The ten Vedic scholars he quotes are Apisali, Kashyapa, Gargya, Galava, Cakravarmana, Bharadvaja, Sakatayana, Sakalya, Senaka and Sphotayana. The of Panini became the foundation of Vyākaraṇa, a Vedanga. In the , language is observed in a manner that has no parallel among Greek or Latin grammarians. Pāṇini's grammar, according to Renou and Filliozat, defines the linguistic expression and a classic that set the standard for the Sanskrit language. Pāṇini made use of a technical metalanguage consisting of a syntax, morphology and lexicon. This metalanguage is organised according to a series of meta-rules, some of which are explicitly stated while others can be deduced.

Pāṇini's comprehensive and scientific theory of grammar is conventionally taken to mark the start of Classical Sanskrit. His systematic treatise inspired and made Sanskrit the preeminent Indian language of learning and literature for two millennia. It is unclear whether Pāṇini wrote his treatise on Sanskrit language or he orally created the detailed and sophisticated treatise then transmitted it through his students. Modern scholarship generally accepts that he knew of a form of writing, based on references to words such as "lipi" ("script") and "lipikara" ("scribe") in section 3.2 of the "Aṣṭādhyāyī".

The Classical Sanskrit language formalized by Pāṇini, states Renou, is "not an impoverished language", rather it is "a controlled and a restrained language from which archaisms and unnecessary formal alternatives were excluded". The Classical form of the language simplified the "sandhi" rules but retained various aspects of the Vedic language, while adding rigor and flexibilities, so that it had sufficient means to express thoughts as well as being "capable of responding to the future increasing demands of an infinitely diversified literature", according to Renou. Pāṇini included numerous "optional rules" beyond the Vedic Sanskrit's "bahulam" framework, to respect liberty and creativity so that individual writers separated by geography or time would have the choice to express facts and their views in their own way, where tradition followed competitive forms of the Sanskrit language.

The phonetic differences between Vedic Sanskrit and Classical Sanskrit are negligible when compared to the intense change that must have occurred in the pre-Vedic period between Indo-Aryan language and the Vedic Sanskrit. The noticeable differences between the Vedic and the Classical Sanskrit include the much-expanded grammar and grammatical categories as well as the differences in the accent, the semantics and the syntax. There are also some differences between how some of the nouns and verbs end, as well as the "sandhi" rules, both internal and external. Quite many words found in the early Vedic Sanskrit language are never found in late Vedic Sanskrit or Classical Sanskrit literature, while some words have different and new meanings in Classical Sanskrit when contextually compared to the early Vedic Sanskrit literature.

Arthur Macdonell was among the early colonial era scholars who summarized some of the differences between the Vedic and Classical Sanskrit. Louis Renou published in 1956, in French, a more extensive discussion of the similarities, the differences and the evolution of the Vedic Sanskrit within the Vedic period and then to the Classical Sanskrit along with his views on the history. This work has been translated by Jagbans Balbir.

The earliest known use of the word "Saṃskṛta" (Sanskrit), in the context of a language, is found in verses 3.16.14 and 5.28.17–19 of the "Ramayana".. Outside the learned sphere of written Classical Sanskrit, vernacular colloquial dialects (Prakrits) continued to evolve. Sanskrit co-existed with numerous other Prakrit languages of ancient India. The Prakrit languages of India also have ancient roots and some Sanskrit scholars have called these "Apabhramsa", literally "spoiled". The Vedic literature includes words whose phonetic equivalent are not found in other Indo-European languages but which are found in the regional Prakrit languages, which makes it likely that the interaction, the sharing of words and ideas began early in the Indian history. As the Indian thought diversified and challenged earlier beliefs of Hinduism, particularly in the form of Buddhism and Jainism, the Prakrit languages such as Pali in Theravada Buddhism and Ardhamagadhi in Jainism competed with Sanskrit in the ancient times. However, states Paul Dundas, a scholar of Jainism, these ancient Prakrit languages had "roughly the same relationship to Sanskrit as medieval Italian does to Latin." The Indian tradition states that the Buddha and the Mahavira preferred the Prakrit language so that everyone could understand it. However, scholars such as Dundas have questioned this hypothesis. They state that there is no evidence for this and whatever evidence is available suggests that by the start of the common era, hardly anybody other than learned monks had the capacity to understand the old Prakrit languages such as Ardhamagadhi.

Colonial era scholars questioned whether Sanskrit was ever a spoken language, or just a literary language. Scholars disagree in their answers. A section of Western scholars state that Sanskrit was never a spoken language, while others and particularly most Indian scholars state the opposite. Those who affirm Sanskrit to have been a vernacular language point to the necessity of Sanskrit being a spoken language for the oral tradition that preserved the vast number of Sanskrit manuscripts from ancient India. Secondly, they state that the textual evidence in the works of Yaksa, Panini and Patanajali affirms that the Classical Sanskrit in their era was a language that is spoken ("bhasha") by the cultured and educated. Some "sutras" expound upon the variant forms of spoken Sanskrit versus written Sanskrit. The 7th-century Chinese Buddhist pilgrim Xuanzang mentioned in his memoir that official philosophical debates in India were held in Sanskrit, not in the vernacular language of that region.
According to Sanskrit linguist Madhav Deshpande, Sanskrit was a spoken language in a colloquial form by the mid-1st millennium BCE which coexisted with a more formal, grammatically correct form of literary Sanskrit. This, states Deshpande, is true for modern languages where colloquial incorrect approximations and dialects of a language are spoken and understood, along with more "refined, sophisticated and grammatically accurate" forms of the same language being found in the literary works. The Indian tradition, states Moriz Winternitz, has favored the learning and the usage of multiple languages from the ancient times. Sanskrit was a spoken language in the educated and the elite classes, but it was also a language that must have been understood in a wider circle of society because the widely popular folk epics and stories such as the "Ramayana", the "Mahabharata", the "Bhagavata Purana", the "Panchatantra" and many other texts are all in the Sanskrit language. The Classical Sanskrit with its exacting grammar was thus the language of the Indian scholars and the educated classes, while others communicated with approximate or ungrammatical variants of it as well as other natural Indian languages. Sanskrit, as the learned language of Ancient India, thus existed alongside the vernacular Prakrits. Many Sanskrit dramas indicate that the language coexisted with the vernacular Prakrits. Centres in Varanasi, Paithan, Pune and Kanchipuram were centers of classical Sanskrit learning and public debates until the arrival of the colonial era.

According to Étienne Lamotte—an Indologist and Buddhism scholar, Sanskrit became the dominant literary and inscriptional language because of its precision in communication. It was, states Lamotte, an ideal instrument for presenting ideas and as knowledge in Sanskrit multiplied so did its spread and influence. Sanskrit was adopted voluntarily as a vehicle of high culture, arts, and profound ideas. Pollock disagrees with Lamotte, but concurs that Sanskrit's influence grew into what he terms as "Sanskrit Cosmopolis" over a region that included all of South Asia and much of southeast Asia. The Sanskrit language cosmopolis thrived beyond India between 300 and 1300 CE.

Reinöhl mentions that not only have the Dravidian languages borrowed from Sanskrit vocabulary but they have also impacted Sanskrit on deeper levels of structure “for instance in the domain of phonology where Indo-Aryan retroflexes have been attributed to Dravidian Influence”. Hans Henrich et al. quoting George Hart state that, there was influence of Old Tamil on Sanskrit. Hart compared Old Tamil and Classical Sanskrit to arrive at a conclusion that there was a common language Prakrit from which both derived – “that both Tamil and Sanskrit derived their shared conventions, metres and techniques from a common source, for it is clear that neither borrowed directly from the other.”. 

Reinöhl  further states that there is a symmetric relationship between Dravidian language like Kannada or Tamil with Indo-Aryan language like Bengali or Hindi whereas the same is not found in Persian or English sentence into Non-Indo Aryan language. To quote from Reinöhl --  “A sentence in a Dravidian language like Tamil or Kannada becomes ordinarily good Bengali or Hindi by substituting Bengali or Hindi equivalents for the Dravidian words and forms, without modifying the word order, but the same thing is not possible in rendering a Persian or English sentence into Non-Indo-Aryan language”. 

Shulman mentions that "Dravidian nonfinite verbal forms (called "vinaiyeccam" in Tamil) shaped the usage of the Sanskrit nonfinite verbs (originally derived from inflected forms of action nouns in Vedic). This particularly salient case of possible influence of Dravidian on Sanskrit is only one of many items of syntactic assimilation, not least among them the large repertoire of morphological modality and aspect that, once one knows to look for it, can be found everywhere in classical and postclassical Sanskrit". 

Sanskrit has been the predominant language of Hindu texts encompassing a rich tradition of philosophical and religious texts, as well as poetry, music, drama, scientific, technical and others. It is the predominant language of one of the largest collection of historic manuscripts. The earliest known inscriptions in Sanskrit are from the 1st century BCE, such as the Ayodhya Inscription of Dhana and Ghosundi-Hathibada (Chittorgarh).

Though developed and nurtured by scholars of orthodox schools of Hinduism, Sanskrit has been the language for some of the key literary works and theology of heterodox schools of Indian philosophies such as Buddhism and Jainism. The structure and capabilities of the Classical Sanskrit language launched ancient Indian speculations about "the nature and function of language", what is the relationship between words and their meanings in the context of a community of speakers, whether this relationship is objective or subjective, discovered or is created, how individuals learn and relate to the world around them through language, and about the limits of language? They speculated on the role of language, the ontological status of painting word-images through sound, and the need for rules so that it can serve as a means for a community of speakers, separated by geography or time, to share and understand profound ideas from each other. These speculations became particularly important to the Mīmāṃsā and the Nyaya schools of Hindu philosophy, and later to Vedanta and Mahayana Buddhism, states Frits Staal—a scholar of Linguistics with a focus on Indian philosophies and Sanskrit. Though written in a number of different scripts, the dominant language of Hindu texts has been Sanskrit. It or a hybrid form of Sanskrit became the preferred language of Mahayana Buddhism scholarship; for example, one of the early and influential Buddhist philosophers, Nagarjuna (~200 CE), used Classical Sanskrit as the language for his texts. According to Renou, Sanskrit had a limited role in the Theravada tradition (formerly known as the Hinayana) but the Prakrit works that have survived are of doubtful authenticity. Some of the canonical fragments of the early Buddhist traditions, discovered in the 20th century, suggest the early Buddhist traditions used an imperfect and reasonably good Sanskrit, sometimes with a Pali syntax, states Renou. The Mahāsāṃghika and Mahavastu, in their late Hinayana forms, used hybrid Sanskrit for their literature. Sanskrit was also the language of some of the oldest surviving, authoritative and much followed philosophical works of Jainism such as the "Tattvartha Sutra" by Umaswati.
The Sanskrit language has been one of the major means for the transmission of knowledge and ideas in Asian history. Indian texts in Sanskrit were already in China by 402 CE, carried by the influential Buddhist pilgrim Faxian who translated them into Chinese by 418 CE. Xuanzang, another Chinese Buddhist pilgrim, learnt Sanskrit in India and carried 657 Sanskrit texts to China in the 7th century where he established a major center of learning and language translation under the patronage of Emperor Taizong. By the early 1st millennium CE, Sanskrit had spread Buddhist and Hindu ideas to Southeast Asia, parts of the East Asia and the Central Asia. It was accepted as a language of high culture and the preferred language by some of the local ruling elites in these regions. According to the Dalai Lama, the Sanskrit language is a parent language that is at the foundation of many modern languages of India and the one that promoted Indian thought to other distant countries. In Tibetan Buddhism, states the Dalai Lama, Sanskrit language has been a revered one and called "legjar lhai-ka" or "elegant language of the gods". It has been the means of transmitting the "profound wisdom of Buddhist philosophy" to Tibet.
The Sanskrit language created a pan-Indic accessibility to information and knowledge in the ancient and medieval times, in contrast to the Prakrit languages which were understood just regionally. It created a cultural bond across the subcontinent. As local languages and dialects evolved and diversified, Sanskrit served as the common language. It connected scholars from distant parts of the Indian subcontinent such as Tamil Nadu and Kashmir, states Deshpande, as well as those from different fields of studies, though there must have been differences in its pronunciation given the first language of the respective speakers. The Sanskrit language brought Indic people together, particularly its elite scholars. Some of these scholars of Indian history regionally produced vernacularized Sanskrit to reach wider audiences, as evidenced by texts discovered in Rajasthan, Gujarat, and Maharashtra. Once the audience became familiar with the easier to understand vernacularized version of Sanskrit, those interested could graduate from colloquial Sanskrit to the more advanced Classical Sanskrit. Rituals and the rites-of-passage ceremonies have been and continue to be the other occasions where a wide spectrum of people hear Sanskrit, and occasionally join in to speak some Sanskrit words such as ""namah"".

Classical Sanskrit is the standard register as laid out in the grammar of , around the fourth century BCE. Its position in the cultures of Greater India is akin to that of Latin and Ancient Greek in Europe. Sanskrit has significantly influenced most modern languages of the Indian subcontinent, particularly the languages of the northern, western, central and eastern Indian subcontinent.

Sanskrit declined starting about and after the 13th century. This coincides with the beginning of Islamic invasions of the Indian subcontinent to create, and thereafter expand the Muslim rule in the form of Sultanates, and later the Mughal Empire. With the fall of Kashmir around the 13th century, a premier center of Sanskrit literary creativity, Sanskrit literature there disappeared, perhaps in the "fires that periodically engulfed the capital of Kashmir" or the "Mongol invasion of 1320" states Sheldon Pollock. The Sanskrit literature which was once widely disseminated out of the northwest regions of the subcontinent, stopped after the 12th century. As Hindu kingdoms fell in the eastern and the South India, such as the great Vijayanagara Empire, so did Sanskrit. There were exceptions and short periods of imperial support for Sanskrit, mostly concentrated during the reign of the tolerant Mughal emperor Akbar. Muslim rulers patronized the Middle Eastern language and scripts found in Persia and Arabia, and the Indians linguistically adapted to this Persianization to gain employment with the Muslim rulers. Hindu rulers such as Shivaji of the Maratha Empire, reversed the process, by re-adopting Sanskrit and re-asserting their socio-linguistic identity. After Islamic rule disintegrated in the Indian subcontinent and the colonial rule era began, Sanskrit re-emerged but in the form of a "ghostly existence" in regions such as Bengal. This decline was the result of "political institutions and civic ethos" that did not support the historic Sanskrit literary culture.

Scholars are divided on whether or when Sanskrit died. Western authors such as John Snelling state that Sanskrit and Pali are both dead Indian languages. Indian authors such as M Ramakrishnan Nair state that Sanskrit was a dead language by the 1st millennium BCE. Sheldon Pollock states that in some crucial way, "Sanskrit is dead". After the 12th century, the Sanskrit literary works were reduced to "reinscription and restatements" of ideas already explored, and any creativity was restricted to hymns and verses. This contrasted with the previous 1,500 years when "great experiments in moral and aesthetic imagination" marked the Indian scholarship using Classical Sanskrit, states Pollock.

Other scholars state that the Sanskrit language did not die, only declined. Hanneder disagrees with Pollock, finding his arguments elegant but "often arbitrary". According to Hanneder, a decline or regional absence of creative and innovative literature constitutes a negative evidence to Pollock's hypothesis, but it is not positive evidence. A closer look at Sanskrit in the Indian history after the 12th century suggests that Sanskrit survived despite the odds. According to Hanneder,

The Sanskrit language scholar Moriz Winternitz states, Sanskrit was never a dead language and it is still alive though its prevalence is lesser than ancient and medieval times. Sanskrit remains an integral part of Hindu journals, festivals, Ramlila plays, drama, rituals and the rites-of-passage. Similarly, Brian Hatcher states that the "metaphors of historical rupture" by Pollock are not valid, that there is ample proof that Sanskrit was very much alive in the narrow confines of surviving Hindu kingdoms between the 13th and 18th centuries, and its reverence and tradition continues.

Hanneder states that modern works in Sanskrit are either ignored or their "modernity" contested.

According to Robert Goldman and Sally Sutherland, Sanskrit is neither "dead" nor "living" in the conventional sense. It is a special, timeless language that lives in the numerous manuscripts, daily chants and ceremonial recitations, a heritage language that Indians contextually prize and some practice.

When the British introduced English to India in the 19th century, knowledge of Sanskrit and ancient literature continued to flourish as the study of Sanskrit changed from a more traditional style into a form of analytical and comparative scholarship mirroring that of Europe.

The relationship of Sanskrit to the Prakrit languages, particularly the modern form of Indian languages, is complex and spans about 3,500 years, states Colin Masica—a linguist specializing in South Asian languages. A part of the difficulty is the lack of sufficient textual, archaeological and epigraphical evidence for the ancient Prakrit languages with rare exceptions such as Pali, leading to a tendency of anachronistic errors. Sanskrit and Prakrit languages may be divided into Old Indo-Aryan (1500 BCE–600 BCE), Middle Indo-Aryan (600 BCE–1000 CE) and New Indo-Aryan (1000 CE–current), each can further be subdivided in early, middle or second, and late evolutionary substages.

Vedic Sanskrit belongs to the early Old Indo-Aryan while Classical Sanskrit to the later Old Indo-Aryan stage. The evidence for Prakrits such as Pali (Theravada Buddhism) and Ardhamagadhi (Jainism), along with Magadhi, Maharashtri, Sinhala, Sauraseni and Niya (Gandhari), emerge in the Middle Indo-Aryan stage in two versions—archaic and more formalized—that may be placed in early and middle substages of the 600 BCE–1000 CE period. Two literary Indic languages can be traced to the late Middle Indo-Aryan stage and these are Apabhramsa and Elu (a form of literary Sinhalese). Numerous North, Central, Eastern and Western Indian languages, such as Hindi, Gujarati, Sindhi, Punjabi, Kashmiri, Nepali, Braj, Awadhi, Bengali, Assamese, Oriya, Marathi, and others belong to the New Indo-Aryan stage.

There is an extensive overlap in the vocabulary, phonetics and other aspects of these New Indo-Aryan languages with Sanskrit, but it is neither universal nor identical across the languages. They likely emerged from a synthesis of the ancient Sanskrit language traditions and an admixture of various regional dialects. Each language has some unique and regionally creative aspects, with unclear origins. Prakrit languages do have a grammatical structure, but like the Vedic Sanskrit, it is far less rigorous than Classical Sanskrit. The roots of all Prakrit languages may be in the Vedic Sanskrit and ultimately the Indo-Aryan language, their structural details vary from the Classical Sanskrit. It is generally accepted by scholars and widely believed in India that the modern Indic languages, such as Bengali, Gujarati, Hindi and Punjabi are descendants of the Sanskrit language. Sanskrit, states Burjor Avari, can be described as "the mother language of almost all the languages of north India".

The Sanskrit language's historic presence is attested across a wide geography beyond the Indian subcontinent. Inscriptions and literary evidence suggests that Sanskrit language was already being adopted in Southeast Asia and Central Asia in the 1st millennium CE, through monks, religious pilgrims and merchants.

The Indian subcontinent has been the geographic range of the largest collection of the ancient and pre-18th-century Sanskrit manuscripts and inscriptions. Beyond ancient India, significant collections of Sanskrit manuscripts and inscriptions have been found in China (particularly the Tibetan monasteries), Myanmar, Indonesia, Cambodia, Laos, Vietnam, Thailand, and Malaysia. Sanskrit inscriptions, manuscripts or its remnants, including some of the oldest known Sanskrit written texts, have been discovered in dry high deserts and mountainous terrains such as in Nepal, Tibet, Afghanistan, Mongolia, Uzbekistan, Turkmenistan, Tajikistan, and Kazakhstan. Some Sanskrit texts and inscriptions have also been discovered in Korea and Japan.

In India, Sanskrit is among the 22 official languages of India in the Eighth Schedule to the Constitution. In 2010, Uttarakhand became the first state in India to make Sanskrit its second official language. In 2019, Himachal Pradesh made Sanskrit its second official language, becoming the second state in India to do so.

Sanskrit shares many Proto-Indo-European phonological features, although it features a larger inventory of distinct phonemes. The consonantal system is the same, though it systematically enlarged the inventory of distinct sounds. For example, Sanskrit added a voiceless aspirated "tʰ", to the voiceless "t", voiced "d" and voiced aspirated "dʰ" found in PIE languages.

The most significant and distinctive phonological development in Sanskrit is vowel-merger, states Stephanie Jamison—an Indo-European linguist specializing in Sanskrit literature. The short "*e", "*o" and "*a", all merge as "a" (अ) in Sanskrit, while long "*ē", "*ō" and "*ā", all merge as long "ā" (आ). These mergers occurred very early and significantly impacted Sanskrit's morphological system. Some phonological developments in it mirror those in other PIE languages. For example, the labiovelars merged with the plain velars as in other satem languages. The secondary palatalization of the resulting segments is more thorough and systematic within Sanskrit, states Jamison. A series of retroflex dental stops were innovated in Sanskrit to more thoroughly articulate sounds for clarity. For example, unlike the loss of the morphological clarity from vowel contraction that is found in early Greek and related southeast European languages, Sanskrit deployed "*y", "*w", and "*s" intervocalically to provide morphological clarity.

The cardinal vowels ("svaras") "i" (इ), "u" (उ), "a" (अ) distinguish length in Sanskrit, states Jamison. The short "a" (अ) in Sanskrit is a closer vowel than ā, equivalent to schwa. The mid-vowels ē (ए) and ō (ओ) in Sanskrit are monophthongizations of the Indo-Iranian diphthongs "*ai" and "*au". The Old Iranian language preserved "*ai" and "*au". The Sanskrit vowels are inherently long, though often transcribed "e" and "o" without the diacritic. The vocalic liquid "r̥" in Sanskrit is a merger of PIE "*r̥" and "*l̥". The long "r̥" is an innovation and it is used in a few analogically generated morphological categories.

According to Masica, Sanskrit has four traditional semivowels, with which were classed, "for morphophonemic reasons, the liquids: y, r, l, and v; that is, as y and v were the non-syllabics corresponding to i, u, so were r, l in relation to r̥ and l̥". The northwestern, the central and the eastern Sanskrit dialects have had a historic confusion between "r" and "l". The Paninian system that followed the central dialect preserved the distinction, likely out of reverence for the Vedic Sanskrit that distinguished the "r" and "l". However, the northwestern dialect only had "r", while the eastern dialect probably only had "l", states Masica. Thus literary works from different parts of ancient India appear inconsistent in their use of "r" and "l", resulting in doublets that is occasionally semantically differentiated.

Sanskrit possesses a symmetric consonantal phoneme structure based on how the sound is articulated, though the actual usage of these sounds conceals the lack of parallelism in the apparent symmetry possibly from historical changes within the language.

Sanskrit had a series of retroflex stops. All the retroflexes in Sanskrit are in "origin conditioned alternants of dentals, though from the beginning of the language they have a qualified independence", states Jamison.

Regarding the palatal plosives, the pronunciation is a matter of debate. In contemporary attestation, the palatal plosives are a regular series of palatal stops, supported by most Sanskrit sandhi rules. However, the reflexes in descendant languages, as well as a few of the sandhi rules regarding "ch", could suggest an affricate pronunciation.

"jh" was a marginal phoneme in Sanskrit, hence its phonology is more difficult to reconstruct; it was more commonly employed in the Middle Indo-Aryan languages as a result of phonological processes resulting in the phoneme.

The palatal nasal is a conditioned variant of n occurring next to palatal obstruents. The "anusvara" that Sanskrit deploys is a conditioned alternant of postvocalic nasals, under certain sandhi conditions. Its "visarga" is a word-final or morpheme-final conditioned alternant of s and r under certain sandhi conditions.
The voiceless aspirated series is also an innovation in Sanskrit but is significantly rarer than the other three series.

While the Sanskrit language organizes sounds for expression beyond those found in the PIE language, it retained many features found in the Iranian and Balto-Slavic languages. An example of a similar process in all three, states Jamison, is the retroflex sibilant ʂ being the automatic product of dental s following i, u, r, and k (mnemonically "ruki").

Sanskrit deploys extensive phonological alternations on different linguistic levels through "sandhi" rules (literally, the rules of "putting together, union, connection, alliance"). This is similar to the English alteration of "going to" as "gonna", states Jamison. The Sanskrit language accepts such alterations within it, but offers formal rules for the "sandhi" of any two words next to each other in the same sentence or linking two sentences. The external "sandhi" rules state that similar short vowels coalesce into a single long vowel, while dissimilar vowels form glides or undergo diphthongization. Among the consonants, most external "sandhi" rules recommend regressive assimilation for clarity when they are voiced. According to Jamison, these rules ordinarily apply at compound seams and morpheme boundaries. In Vedic Sanskrit, the external "sandhi" rules are more variable than in Classical Sanskrit.

The internal "sandhi" rules are more intricate and account for the root and the canonical structure of the Sanskrit word. These rules anticipate what are now known as the Bartholomae's law and Grassmann's law. For example, states Jamison, the "voiceless, voiced, and voiced aspirated obstruents of a positional series regularly alternate with each other (p ≈ b ≈ bʰ; t ≈ d ≈ dʰ, etc.; note, however, c ≈ j ≈ h), such that, for example, a morpheme with an underlying voiced aspirate final may show alternants with all three stops under differing internal sandhi conditions". The velar series (k, g, gʰ) alternate with the palatal series (c, j, h), while the structural position of the palatal series is modified into a retroflex cluster when followed by dental. This rule create two morphophonemically distinct series from a single palatal series.

Vocalic alternations in the Sanskrit morphological system is termed "strengthening", and called "guna" and "vriddhi" in the preconsonantal versions. There is an equivalence to terms deployed in Indo-European descriptive grammars, wherein Sanskrit's unstrengthened state is same as the zero-grade, "guna" corresponds to normal-grade, while "vriddhi" is same as the lengthened-state. The qualitative ablaut is not found in Sanskrit just like it is absent in Iranian, but Sanskrit retains quantitative ablaut through vowel strengthening. The transformations between unstrengthened to "guna" is prominent in the morphological system, states Jamison, while "vriddhi" is a particularly significant rule when adjectives of origin and appurtenance are derived. The manner in which this is done slightly differs between the Vedic and the Classical Sanskrit.

Sanskrit grants a very flexible syllable structure, where they may begin or end with vowels, be single consonants or clusters. Similarly, the syllable may have an internal vowel of any weight. The Vedic Sanskrit shows traces of following the Sievers-Edgerton Law, but Classical Sanskrit doesn't. Vedic Sanskrit has a pitch accent system, states Jamison, which were acknowledged by Panini, but in his Classical Sanskrit the accents disappear. Most Vedic Sanskrit words have one accent. However, this accent is not phonologically predictable, states Jamison. It can fall anywhere in the word and its position often conveys morphological and syntactic information. According to Masica, the presence of an accent system in Vedic Sanskrit is evidenced from the markings in the Vedic texts. This is important because of Sanskrit's connection to the PIE languages and comparative Indo-European linguistics.

Sanskrit, like most early Indo-European languages, lost the so-called "laryngeal consonants (cover-symbol "*H") present in the Proto-Indo-European", states Jamison. This significantly impacted the evolutionary path of the Sanskrit phonology and morphology, particularly in the variant forms of roots.

Because Sanskrit is not anyone's native language, it does not have a fixed pronunciation. People tend to pronounce it as they do their native language. The articles on Hindustani, Marathi, Nepali, Oriya and Bengali phonology will give some indication of the variation that is encountered. When Sanskrit was a spoken language, its pronunciation varied regionally and also over time. Nonetheless, Panini described the sound system of Sanskrit well enough that people have a fairly good idea of what he intended.

The basis of Sanskrit morphology is the root, states Jamison, "a morpheme bearing lexical meaning". The verbal and nominal stems of Sanskrit words are derived from this root through the phonological vowel-gradation processes, the addition of affixes, verbal and nominal stems. It then adds an ending to establish the grammatical and syntactic identity of the stem. According to Jamison, the "three major formal elements of the morphology are (i) root, (ii) affix, and (iii) ending; and they are roughly responsible for (i) lexical meaning, (ii) derivation, and (iii) inflection respectively".

A Sanskrit word has the following canonical structure:

The root structure has certain phonological constraints. Two of the most important constraints of a "root" is that it does not end in a short "a" (अ) and that it is monosyllabic. In contrast, the affixes and endings commonly do. The affixes in Sanskrit are almost always suffixes, with exceptions such as the augment "a-" added as prefix to past tense verb forms and the "-na/n-" infix in single verbal present class, states Jamison.

A verb in Sanskrit has the following canonical structure:

According to Ruppel, verbs in Sanskrit express the same information as other Indo-European languages such as English. Sanskrit verbs describe an action or occurrence or state, its embedded morphology informs as to "who is doing it" (person or persons), "when it is done" (tense) and "how it is done" (mood, voice). The Indo-European languages differ in the detail. For example, the Sanskrit language attaches the affixes and ending to the verb root, while the English language adds small independent words before the verb. In Sanskrit, these elements co-exist within the word.

Both verbs and nouns in Sanskrit are either thematic or athematic, states Jamison. "Guna" (strengthened) forms in the active singular regularly alternate in athematic verbs. The finite verbs of Classical Sanskrit have the following grammatical categories: person, number, voice, tense-aspect, and mood. According to Jamison, a portmanteau morpheme generally expresses the person-number-voice in Sanskrit, and sometimes also the ending or only the ending. The mood of the word is embedded in the affix.

These elements of word architecture are the typical building blocks in Classical Sanskrit, but in Vedic Sanskrit these elements fluctuate and are unclear. For example, in the "Rigveda" preverbs regularly occur in tmesis, states Jamison, which means they are "separated from the finite verb". This indecisiveness is likely linked to Vedic Sanskrit's attempt to incorporate accent. With nonfinite forms of the verb and with nominal derivatives thereof, states Jamison, "preverbs show much clearer univerbation in Vedic, both by position and by accent, and by Classical Sanskrit, tmesis is no longer possible even with finite forms".

While roots are typical in Sanskrit, some words do not follow the canonical structure. A few forms lack both inflection and root. Many words are inflected (and can enter into derivation) but lack a recognizable root. Examples from the basic vocabulary include kinship terms such as "mātar-" (mother), "nas-" (nose), "śvan-" (dog). According to Jamison, pronouns and some words outside the semantic categories also lack roots, as do the numerals. Similarly, the Sanskrit language is flexible enough to not mandate inflection.

The Sanskrit words can contain more than one affix that interact with each other. Affixes in Sanskrit can be athematic as well as thematic, according to Jamison. Athematic affixes can be alternating. Sanskrit deploys eight cases, namely nominative, accusative, instrumental, dative, ablative, genitive, locative, vocative.

Stems, that is "root + affix", appear in two categories in Sanskrit: vowel stems and consonant stems. Unlike some Indo-European languages such as Latin or Greek, according to Jamison, "Sanskrit has no closed set of conventionally denoted noun declensions". Sanskrit includes a fairly large set of stem-types. The linguistic interaction of the roots, the phonological segments, lexical items and the grammar for the Classical Sanskrit consist of four Paninian components. These, states Paul Kiparsky, are the "Astadhyaayi", a comprehensive system of 4,000 grammatical rules, of which a small set are frequently used; "Sivasutras", an inventory of "anubandhas" (markers) that partition phonological segments for efficient abbreviations through the "pratyharas" technique; "Dhatupatha", a list of 2,000 verbal roots classified by their morphology and syntactic properties using diacritic markers, a structure that guides its writing systems; and, the "Ganapatha", an inventory of word groups, classes of lexical systems. There are peripheral adjuncts to these four, such as the "Unadisutras", which focus on irregularly formed derivatives from the roots.

Sanskrit morphology is generally studied in two broad fundamental categories: the nominal forms and the verbal forms. These differ in the types of endings and what these endings mark in the grammatical context. Pronouns and nouns share the same grammatical categories, though they may differ in inflection. Verb-based adjectives and participles are not formally distinct from nouns. Adverbs are typically frozen case forms of adjectives, states Jamison, and "nonfinite verbal forms such as infinitives and gerunds also clearly show frozen nominal case endings".

The Sanskrit language includes five tenses: present, future, past imperfect, past aorist and past perfect. It outlines three types of voices: active, passive and the middle. The middle is also referred to as the mediopassive, or more formally in Sanskrit as "parasmaipada" (word for another) and "atmanepada" (word for oneself).

The paradigm for the tense-aspect system in Sanskrit is the three-way contrast between the "present", the "aorist" and the "perfect" architecture. Vedic Sanskrit is more elaborate and had several additional tenses. For example, the "Rigveda" includes perfect and a marginal pluperfect. Classical Sanskrit simplifies the "present" system down to two tenses, the perfect and the imperfect, while the "aorist" stems retain the aorist tense and the "perfect" stems retain the perfect and marginal pluperfect. The classical version of the language has elaborate rules for both voice and the tense-aspect system to emphasize clarity, and this is more elaborate than in other Indo-European languages. The evolution of these systems can be seen from the earliest layers of the Vedic literature to the late Vedic literature.

Sanskrit recognizes three numbers—singular, dual, and plural. The dual is a fully functioning category, used beyond naturally paired objects such as hands or eyes, extending to any collection of two. The elliptical dual is notable in the Vedic Sanskrit, according to Jamison, where a noun in the dual signals a paired opposition. Illustrations include "dyāvā" (literally, "the two heavens" for heaven-and-earth), "mātarā" (literally, "the two mothers" for mother-and-father). A verb may be singular, dual or plural, while the person recognized in the language are forms of "I", "you", "he/she/it", "we" and "they".

There are three persons in Sanskrit: first, second and third. Sanskrit uses the 3×3 grid formed by the three numbers and the three persons parameters as the paradigm and the basic building block of its verbal system.

The Sanskrit language incorporates three genders: feminine, masculine and neuter. All nouns have inherent gender, but with some exceptions, personal pronouns have no gender. Exceptions include demonstrative and anaphoric pronouns. Derivation of a word is used to express the feminine. Two most common derivations come from feminine-forming suffixes, the "-ā-" (आ, Rādhā) and "-ī-" (ई, Rukmīnī). The masculine and neuter are much simpler, and the difference between them is primarily inflectional. Similar affixes for the feminine are found in many Indo-European languages, states Burrow, suggesting links of the Sanskrit to its PIE heritage.

Pronouns in Sanskrit include the personal pronouns of the first and second persons, unmarked for gender, and a larger number of gender-distinguishing pronouns and adjectives. Examples of the former include "ahám" (first singular), "vayám" (first plural) and "yūyám" (second plural). The latter can be demonstrative, deictic or anaphoric. Both the Vedic and Classical Sanskrit share the "sá/tám" pronominal stem, and this is the closest element to a third person pronoun and an article in the Sanskrit language, states Jamison.

Indicative, potential and imperative are the three mood forms in Sanskrit.

The Sanskrit language formally incorporates poetic metres. By the late Vedic era, this developed into a field of study and it was central to the composition of the Hindu literature including the later Vedic texts. This study of Sanskrit prosody is called "chandas" and considered as one of the six Vedangas, or limbs of Vedic studies.

Sanskrit prosody includes linear and non-linear systems. The system started off with seven major metres, according to Annette Wilke and Oliver Moebus, called the "seven birds" or "seven mouths of Brihaspati", and each had its own rhythm, movements and aesthetics wherein a non-linear structure (aperiodicity) was mapped into a four verse polymorphic linear sequence. A syllable in Sanskrit is classified as either "laghu" (light) or "guru" (heavy). This classification is based on a "matra" (literally, "count, measure, duration"), and typically a syllable that ends in a short vowel is a light syllable, while those that end in consonant, "anusvara" or "visarga" are heavy. The classical Sanskrit found in Hindu scriptures such as the "Bhagavad Gita" and many texts are so arranged that the light and heavy syllables in them follow a rhythm, though not necessarily a rhyme.

Sanskrit metres include those based on a fixed number of syllables per verse, and those based on fixed number of morae per verse. The Vedic Sanskrit employs fifteen metres, of which seven are common, and the most frequent are three (8-, 11- and 12-syllable lines). The Classical Sanskrit deploys both linear and non-linear metres, many of which are based on syllables and others based on diligently crafted verses based on repeating numbers of morae (matra per foot).
Meter and rhythm is an important part of the Sanskrit language. It may have played a role in helping preserve the integrity of the message and Sanskrit texts. The verse perfection in the Vedic texts such as the verse Upanishads and post-Vedic Smriti texts are rich in prosody. This feature of the Sanskrit language led some Indologists from the 19th century onwards to identify suspected portions of texts where a line or sections are off the expected metre.

The meter-feature of the Sanskrit language embeds another layer of communication to the listener or reader. A change in metres has been a tool of literary architecture and an embedded code to inform the reciter and audience that it marks the end of a section or chapter. Each section or chapter of these texts uses identical metres, rhythmically presenting their ideas and making it easier to remember, recall and check for accuracy. Authors coded a hymn's end by frequently using a verse of a metre different than that used in the hymn's body. However, Hindu tradition does not use the Gayatri metre to end a hymn or composition, possibly because it has enjoyed a special level of reverence in Hinduism.

The early history of writing Sanskrit and other languages in ancient India is a problematic topic despite a century of scholarship, states Richard Salomon—an epigraphist and Indologist specializing in Sanskrit and Pali literature. The earliest possible script from the Indian subcontinent is from the Indus Valley Civilization (3rd/2nd millennium BCE), but this script – if it is a script – remains undeciphered. If any scripts existed in the Vedic period, they have not survived. Scholars generally accept that Sanskrit was spoken in an oral society, and that an oral tradition preserved the extensive Vedic and Classical Sanskrit literature. Other scholars such as Jack Goody state that the Vedic Sanskrit texts are not the product of an oral society, basing this view by comparing inconsistencies in the transmitted versions of literature from various oral societies such as the Greek, Serbian, and other cultures, then noting that the Vedic literature is too consistent and vast to have been composed and transmitted orally across generations, without being written down.

"Lipi" is the term in Sanskrit which means "writing, letters, alphabet". It contextually refers to scripts, the art or any manner of writing or drawing. The term, in the sense of a writing system, appears in some of the earliest Buddhist, Hindu, and Jaina texts. Pāṇini's "Astadhyayi", composed sometime around the 5th or 4th century BCE, for example, mentions "lipi" in the context of a writing script and education system in his times, but he does not name the script. Several early Buddhist and Jaina texts, such as the "Lalitavistara Sūtra" and "Pannavana Sutta" include lists of numerous writing scripts in ancient India. The Buddhist texts list the sixty four "lipi" that the Buddha knew as a child, with the Brahmi script topping the list. "The historical value of this list is however limited by several factors", states Salomon. The list may be a later interpolation. The Jain canonical texts such as the "Pannavana Sutta"—probably older than the Buddhist texts—list eighteen writing systems, with the Brahmi topping the list and Kharotthi (Kharoshthi) listed as fourth. The Jaina text elsewhere states that the "Brahmi is written in 18 different forms", but the details are lacking. However, the reliability of these lists has been questioned and the empirical evidence of writing systems in the form of Sanskrit or Prakrit inscriptions dated prior to the 3rd century BCE has not been found. If the ancient surface for writing Sanskrit was palm leaves, tree bark and cloth—the same as those in later times, these have not survived. According to Salomon, many find it difficult to explain the "evidently high level of political organization and cultural complexity" of ancient India without a writing system for Sanskrit and other languages.

The oldest datable writing systems for Sanskrit are the Brāhmī script, the related Kharoṣṭhī script and the Brahmi derivatives. The Kharosthi was used in the northwestern part of the Indian subcontinent and it became extinct, while the Brahmi was used in all over the subcontinent along with regional scripts such as Old Tamil. Of these, the earliest records in the Sanskrit language are in Brahmi, a script that later evolved into numerous related Indic scripts for Sanskrit, along with Southeast Asian scripts (Burmese, Thai, Lao, Khmer, others) and many extinct Central Asian scripts such as those discovered along with the Kharosthi in the Tarim Basin of western China and in Uzbekistan. The most extensive inscriptions that have survived into the modern era are the rock edicts and pillar inscriptions of the 3rd-century BCE Mauryan emperor Ashoka, but these are not in Sanskrit.

Over the centuries, and across countries, a number of scripts have been used to write Sanskrit.

The Brahmi script for writing Sanskrit is a "modified consonant-syllabic" script. The graphic syllable is its basic unit, and this consists of a consonant with or without diacritic modifications. Since the vowel is an integral part of the consonants, and given the efficiently compacted, fused consonant cluster morphology for Sanskrit words and grammar, the Brahmi and its derivative writing systems deploy ligatures, diacritics and relative positioning of the vowel to inform the reader how the vowel is related to the consonant and how it is expected to be pronounced for clarity. This feature of Brahmi and its modern Indic script derivatives makes it difficult to classify it under the main script types used for the writing systems for most of the world's languages, namely logographic, syllabic and alphabetic.

The Brahmi script evolved into "a vast number of forms and derivatives", states Richard Salomon, and in theory, Sanskrit "can be represented in virtually any of the main Brahmi-based scripts and in practice it often is". Sanskrit does not have a native script. Being a phonetic language, it can be written in any precise script that efficiently maps unique human sounds to unique symbols. From the ancient times, it has been written in numerous regional scripts in South and Southeast Asia. Most of these are descendants of the Brahmi script. The earliest datable "varnamala" Brahmi alphabet system, found in later Sanskrit texts, is from the 2nd century BCE, found in Sughana, Haryana. It shows a "schoolboy's writing lessons", states Salomon.

Many modern era manuscripts are written and available in the Nagari script, whose form is attestable to the 1st millennium CE. The Nagari script is the ancestor of Devanagari (north India), Nandinagari (south India) and other variants. The Nāgarī script was in regular use by 7th century CE, and had fully evolved into Devanagari and Nandinagari scripts by about the end of the first millennium of the common era. The Devanagari script, states Banerji, became more popular for Sanskrit in India since about the 18th century. However, Sanskrit does have special historical connection to the Nagari script as attested by the epigraphical evidence.
The Nagari script has been thought as a north Indian script for Sanskrit as well as the regional languages such as Hindi, Marathi and Nepali. However, it has had a "supra-local" status as evidenced by 1st-millennium CE epigraphy and manuscripts discovered all over India and as far as Sri Lanka, Burma, Indonesia and in its parent form called the Siddhamatrka script found in manuscripts of East Asia. The Sanskrit and Balinese languages Sanur inscription on Belanjong pillar of Bali (Indonesia), dated to about 914 CE, is in part in the Nagari script.

The Nagari script used for Classical Sanskrit has the fullest repertoire of characters consisting of fourteen vowels and thirty three consonants. For the Vedic Sanskrit, it has two more allophonic consonantal characters (the intervocalic ळ "ḷa", and ळ्ह "ḷha"). To communicate phonetic accuracy, it also includes several modifiers such as the "anusvara" dot and the "visarga" double dot, punctuation symbols and others such as the "halanta" sign.

Other scripts such as Gujarati, Bangla, Odia and major south Indian scripts, states Salomon, "have been and often still are used in their proper territories for writing Sanskrit". These and many Indian scripts look different to the untrained eye, but the differences between Indic scripts is "mostly superficial and they share the same phonetic repertoire and systemic features", states Salomon. They all have essentially the same set of eleven to fourteen vowels and thirty-three consonants as established by the Sanskrit language and attestable in the Brahmi script. Further, a closer examination reveals that they all have the similar basic graphic principles, the same "varnamala" (literally, "garland of letters") alphabetic ordering following the same logical phonetic order, easing the work of historic skilled scribes writing or reproducing Sanskrit works across the Indian subcontinent. The Sanskrit language written in some Indic scripts exaggerate angles or round shapes, but this serves only to mask the underlying similarities. Nagari script favours symmetry set with squared outlines and right angles. In contrast, Sanskrit written in the Bangla script emphasizes the acute angles while the neighbouring Odia script emphasizes rounded shapes and uses cosmetically appealing "umbrella-like curves" above the script symbols.
In the south, where Dravidian languages predominate, scripts used for Sanskrit include the Kannada, Telugu, Malayalam and Grantha alphabets.

Since the late 18th century, Sanskrit has been transliterated using the Latin alphabet. The system most commonly used today is the IAST (International Alphabet of Sanskrit Transliteration), which has been the academic standard since 1888. ASCII-based transliteration schemes have also evolved because of difficulties representing Sanskrit characters in computer systems. These include Harvard-Kyoto and ITRANS, a transliteration scheme that is used widely on the Internet, especially in Usenet and in email, for considerations of speed of entry as well as rendering issues. With the wide availability of Unicode-aware web browsers, IAST has become common online. It is also possible to type using an alphanumeric keyboard and transliterate to Devanagari using software like Mac OS X's international support.

European scholars in the 19th century generally preferred Devanagari for the transcription and reproduction of whole texts and lengthy excerpts. However, references to individual words and names in texts composed in European Languages were usually represented with Roman transliteration. From the 20th century onwards, because of production costs, textual editions edited by Western scholars have mostly been in Romanised transliteration.

The earliest known stone inscriptions in Sanskrit are in the Brahmi script from the first century BCE. These include the Ayodhyā (Uttar Pradesh) and Hāthībādā-Ghosuṇḍī (near Chittorgarh, Rajasthan) inscriptions. Both of these, states Salomon, are "essentially standard" and "correct Sanskrit", with a few exceptions reflecting an "informal Sanskrit usage". Other important Hindu inscriptions dated to the 1st century BCE, in relatively accurate classical Sanskrit and Brahmi script are the Yavanarajya inscription on a red sandstone slab and the long Naneghat inscription on the wall of a cave rest stop in the Western Ghats.

Besides these few examples from the 1st century BCE, the earliest Sanskrit and hybrid dialect inscriptions are found in Mathura (Uttar Pradesh). These date to the 1st and 2nd century CE, states Salomon, from the time of the Indo-Scythian Northern Satraps and the subsequent Kushan Empire. These are also in the Brahmi script. The earliest of these, states Salomon, are attributed to Ksatrapa Sodasa from the early years of 1st century CE. Of the Mathura inscriptions, the most significant is the Mora Well Inscription. In a manner similar to the Hathibada inscription, the Mora well inscription is a dedicatory inscription and is linked to the cult of the Vrishni heroes: it mentions a stone shrine (temple), pratima (murti, images) and calls the five Vrishnis as "bhagavatam". There are many other Mathura Sanskrit inscriptions in Brahmi script overlapping the era of Indo-Scythian Northern Satraps and early Kushanas. Other significant 1st-century inscriptions in reasonably good classical Sanskrit in the Brahmi script include the Vasu Doorjamb Inscription and the Mountain Temple inscription. The early ones are related to the Brahmanical, except for the inscription from Kankali Tila which may be Jaina, but none are Buddhist. A few of the later inscriptions from the 2nd century CE include Buddhist Sanskrit, while others are in "more or less" standard Sanskrit and related to the Brahmanical tradition.
In Maharashtra and Gujarat, Brahmi script Sanskrit inscriptions from the early centuries of the common era exist at the Nasik Caves site, near the Girnar mountain of Junagadh and elsewhere such as at Kanakhera, Kanheri, and Gunda. The Nasik inscription dates to the mid-1st century CE, is a fair approximation of standard Sanskrit and has hybrid features. The Junagadh rock inscription of Western Satraps ruler Rudradaman I (c. 150 CE, Gujarat) is the first long poetic-style inscription in "more or less" standard Sanskrit that has survived into the modern era. It represents a turning point in the history of Sanskrit epigraphy, states Salomon. Though no similar inscriptions are found for about two hundred years after the Rudradaman reign, it is important because its style is the prototype of the eulogy-style Sanskrit inscriptions found in the Gupta Empire era. These inscriptions are also in the Brahmi script.

The Nagarjunakonda inscriptions are the earliest known substantial South Indian Sanskrit inscriptions, probably from the late 3rd century or early 4th century CE, or both. These inscriptions are related to Buddhism and the Shaivism tradition of Hinduism. A few of these inscriptions from both traditions are verse-style in the classical Sanskrit language, while some such as the pillar inscription is written in prose and a hybridized Sanskrit language. An earlier hybrid Sanskrit inscription found on Amaravati slab is dated to the late 2nd century, while a few later ones include Sanskrit inscriptions along with Prakrit inscriptions related to Hinduism and Buddhism. After the 3rd century CE, Sanskrit inscriptions dominate and many have survived. Between the 4th and 7th centuries CE, south Indian inscriptions are exclusively in the Sanskrit language. In the eastern regions of the Indian subcontinent, scholars report minor Sanskrit inscriptions from the 2nd century, these being fragments and scattered. The earliest substantial true Sanskrit language inscription of Susuniya (West Bengal) is dated to the 4th century. Elsewhere, such as Dehradun (Uttarakhand), inscriptions in more or less correct classical Sanskrit inscriptions are dated to the 3rd century.

According to Salomon, the 4th-century reign of Samudragupta was the turning point when the classical Sanskrit language became established as the "epigraphic language par excellence" of the Indian world. These Sanskrit language inscriptions are either "donative" or "panegyric" records. Generally in accurate classical Sanskrit, they deploy a wide range of regional Indic writing systems extant at the time. They record the donation of a temple or stupa, images, land, monasteries, pilgrim's travel record, public infrastructure such as water reservoir and irrigation measures to prevent famine. Others praise the king or the donor in lofty poetic terms. The Sanskrit language of these inscriptions is written on stone, various metals, terracotta, wood, crystal, ivory, shell and cloth.

The evidence of the use of the Sanskrit language in Indic writing systems appears in southeast Asia in the first half of the 1st millennium CE. A few of these in Vietnam are bilingual where both the Sanskrit and the local language is written in the Indian alphabet. Early Sanskrit language inscriptions in Indic writing systems are dated to the 4th century in Malaysia, 5th to 6th centuries in Thailand near Si Thep and the Sak River, early 5th century in Kutai (east Borneo) and mid-5th century in west Java (Indonesia). Both major writing systems for Sanskrit, the North Indian and South Indian scripts, have been discovered in southeast Asia, but the Southern variety with its rounded shapes are far more common. The Indic scripts, particularly the Pallava script prototype, spread and ultimately evolved into Mon-Burmese, Khmer, Thai, Laos, Sumatran, Celebes, Javanese and Balinese scripts. From about the 5th century, Sanskrit inscriptions become common in many parts of South Asia and Southeast Asia, with significant discoveries in Nepal, Vietnam and Cambodia.

Sanskrit has been written in various scripts on a variety of media such as palm leaves, cloth, paper, rock and metal sheets, from ancient times.

For nearly 2,000 years, Sanskrit was the language of a cultural order that exerted influence across South Asia, Inner Asia, Southeast Asia, and to a certain extent East Asia. A significant form of post-Vedic Sanskrit is found in the Sanskrit of Indian epic poetry—the "Ramayana" and "Mahabharata". The deviations from in the epics are generally considered to be on account of interference from Prakrits, or innovations, and not because they are pre-Paninian. Traditional Sanskrit scholars call such deviations "ārṣa" (आर्ष), meaning 'of the ṛṣis', the traditional title for the ancient authors. In some contexts, there are also more "prakritisms" (borrowings from common speech) than in Classical Sanskrit proper. Buddhist Hybrid Sanskrit is a literary language heavily influenced by the Middle Indo-Aryan languages, based on early Buddhist Prakrit texts which subsequently assimilated to the Classical Sanskrit standard in varying degrees.

Sanskrit has greatly influenced the languages of India that grew from its vocabulary and grammatical base; for instance, Hindi is a "Sanskritised register" of Hindustani. All modern Indo-Aryan languages, as well as Munda and Dravidian languages have borrowed many words either directly from Sanskrit ("tatsama" words), or indirectly via middle Indo-Aryan languages ("tadbhava" words). Words originating in Sanskrit are estimated at roughly fifty percent of the vocabulary of modern Indo-Aryan languages, as well as the literary forms of Malayalam and Kannada. Literary texts in Telugu are lexically Sanskrit or Sanskritised to an enormous extent, perhaps seventy percent or more. Marathi is another prominent language in Western India, that derives most of its words and Marathi grammar from Sanskrit. Sanskrit words are often preferred in the literary texts in Marathi over corresponding colloquial Marathi word.

There has been a profound influence of Sanskrit on the lexical and grammatical systems of Dravidian languages.  As per Dalby, India has been a single cultural area for about two millennia which has helped Sanskrit influence on all the Indic languages. Emeneau and Burrow mention the tendency “for all four of the Dravidian literary languages in South to make literary use of total Sanskrit lexicon indiscriminately”. There are a large number of loanwords found in the vocabulary of the three major Dravidian languages Malayalam, Kannada and Telugu. Tamil also has significant loanwords from Sanskrit. Krishnamurthi mentions that although it is not clear when the Sanskrit influence happened on the Dravidian languages, it can perhaps be around 5th century BCE at the time of separation of Tamil and Kannada from a proto-dravidian language. ‌The borrowed words are classified into two types based on phonological integration – "tadbhava" – those words derived from Prakrit and "tatsama" – unassimilated loanwords from Sanskrit.

Strazny mentions that “so massive has been the influence that it is hard to utter Sanskrit words have influenced Kannada from the early times”. The very first document in Kannada, the Halmidi inscription has a large number of Sanskrit words. As per Kachru, the influence has not only been on single lexical items in Kannada but also on “long nominal compounds and complicated syntactic expressions”. New words have been created in Kannada using Sanskrit derivational prefixes and suffixes like "vike:ndri:karaNa, anili:karaNa, bahi:skruTa". Similar stratification is found in verb morphology. Sanskrit words readily undergo verbalization in Kannada, verbalizing suffixes as in: "cha:pisu, dowDa:yisu, rava:nisu." 

George mentions that “no other Dravidian language has been so deeply influenced by Sanskrit as Malayalam”. Loanwords have been integrated into Malayalam by “prosodic phonological” changes as per Grant. These phonological changes are either by replacement of a vowel as in "Sant-"am coming from Sanskrit "Santa"-h, "Sagar"-am from "Sagara"-h, or addition of prothetic vowel as in "aracan" from "rajan", "uruvam" from "rupa", "codyam" from "sodhya". 

Hans Henrich et al. note that, the language of the pre-modern Telugu literature was also highly influenced by Sanskrit and was standardized between 11th and 14th centuries. Aiyar has shown that in a class of "tadbhavas" in Telugu the first and second letters are often replaced by the third and fourth letters and fourth again replaced often by h. Examples of the same are: Sanskrit "arthah" becomes "ardhama", "vithi" becomes "vidhi", "putrah" becomes "bidda", "mukham" becomes "muhamu".

Tamil language also has been influenced from Sanskrit. Hans Henrich et al. mention that propagation of Jainism and Buddhism into south India had its influence on Old Tamil Cankam Anthologies, Sanskrit poetical literature influenced Old Tamil literature Cilappatikaram and Maniemakalai. Middle Tamil has shown a significantly higher influence of Sanskrit into the Bhakti poems. Shulman mentions that although contrary to the views held by Tamil purists, modern Tamil has been significantly influenced from Sanskrit, further states that "Indeed there may well be more Sanskrit in Tamil than in the Sanskrit derived north-Indian vernaculars". Sanskrit words have been Tamilized through the "Tamil phonematic grid". 

Buddhist Sanskrit has had a considerable influence on East Asian languages such as Chinese, state William Wang and Chaofen Sun. Many words have been adopted from Sanskrit into the Chinese, both in its historic religious discourse and everyday use. This process likely started about 200 CE and continued through about 1400 CE, with the efforts of monks such as Yuezhi, Anxi, Kangju, Tianzhu, Yan Fodiao, Faxian, Xuanzang and Yijing. Further, as the Chinese language and culture influenced the rest of East Asia, the ideas in Sanskrit texts and some of its linguistic elements migrated further.

Sanskrit has also influenced Sino-Tibetan languages, mostly through translations of Buddhist Hybrid Sanskrit. Many terms were transliterated directly and added to the Chinese vocabulary. Chinese words like "chànà" (Devanagari: क्षण "" 'instantaneous period') were borrowed from Sanskrit. Many Sanskrit texts survive only in Tibetan collections of commentaries to the Buddhist teachings, the Tengyur.

Sanskrit was a language for religious purposes and for the political elite in parts of medieval era Southeast Asia, Central Asia and East Asia. In Southeast Asia, languages such as Thai and Lao contain many loanwords from Sanskrit, as does Khmer. Many Sanskrit loanwords are also found in Austronesian languages, such as Javanese, particularly the older form in which nearly half the vocabulary is borrowed. Other Austronesian languages, such as traditional Malay and modern Indonesian, also derive much of their vocabulary from Sanskrit. Similarly, Philippine languages such as Tagalog have some Sanskrit loanwords, although more are derived from Spanish. A Sanskrit loanword encountered in many Southeast Asian languages is the word "bhāṣā", or spoken language, which is used to refer to the names of many languages. English also has words of Sanskrit origin.

Sanskrit has also influenced the religious register of Japanese mostly through transliterations. These were borrowed from Chinese transliterations. In particular, the Shingon () sect of esoteric Buddhism has been relying on Sanskrit and original Sanskrit mantras and writings, as a means of realizing Buddhahood.

Sanskrit is the sacred language of various Hindu, Buddhist, and Jain traditions. It is used during worship in Hindu temples. In Newar Buddhism, it is used in all monasteries, while Mahayana and Tibetan Buddhist religious texts and sutras are in Sanskrit as well as vernacular languages. Some of the revered texts of Jainism including the Tattvartha sutra, Ratnakaranda śrāvakācāra, the Bhaktamara Stotra and the Agamas are in Sanskrit. Further, states Paul Dundas, Sanskrit mantras and Sanskrit as a ritual language was commonplace among Jains throughout their medieval history.

Many Hindu rituals and rites-of-passage such as the "giving away the bride" and mutual vows at weddings, a baby's naming or first solid food ceremony and the goodbye during a cremation invoke and chant Sanskrit hymns. Major festivals such as the "Durga Puja" ritually recite entire Sanskrit texts such as the "Devi Mahatmya" every year particularly amongst the numerous communities of eastern India. In the south, Sanskrit texts are recited at many major Hindu temples such as the Meenakshi Temple. According to Richard H. Davis, a scholar of Religion and South Asian studies, the breadth and variety of oral recitations of the Sanskrit text "Bhagavad Gita" is remarkable. In India and beyond, its recitations include "simple private household readings, to family and neighborhood recitation sessions, to holy men reciting in temples or at pilgrimage places for passersby, to public Gita discourses held almost nightly at halls and auditoriums in every Indian city".

More than 3,000 Sanskrit works have been composed since India's independence in 1947. Much of this work has been judged of high quality, in comparison to both classical Sanskrit literature and modern literature in other Indian languages.

The Sahitya Akademi has given an award for the best creative work in Sanskrit every year since 1967. In 2009, Satya Vrat Shastri became the first Sanskrit author to win the Jnanpith Award, India's highest literary award.

Sanskrit is used extensively in the Carnatic and Hindustani branches of classical music. Kirtanas, bhajans, stotras, and shlokas of Sanskrit are popular throughout India. The samaveda uses musical notations in several of its recessions.

In Mainland China, musicians such as Sa Dingding have written pop songs in Sanskrit.

Numerous loan Sanskrit words are found in other major Asian languages. For example, Filipino, Cebuano, Lao, Khmer Thai and its alphabets, Malay, Indonesian (old Javanese-English dictionary by P.J. Zoetmulder contains over 25,500 entries), and even in English.

Since 1974, there has been a short daily news broadcast on state-run All India Radio. These broadcasts are also made available on the internet on AIR's website. Sanskrit news is broadcast on TV and on the internet through the DD National channel at 6:55 AM IST.

Over 90 weeklies, fortnightlies and quarterlies are published in Sanskrit. "Sudharma", a daily printed newspaper in Sanskrit, has been published out of Mysore, India, since 1970. It was started by K.N. Varadaraja Iyengar, a sanskrit scholar from Mysore. Sanskrit Vartman Patram and Vishwasya Vrittantam started in Gujarat during the last five years.

Sanskrit has been taught in schools from time immemorial in India. In modern times, the first Sanskrit University was Sampurnanand Sanskrit University, established in 1791 in the Indian city of Varanasi. Sanskrit is taught in 5,000 traditional schools (Pathashalas), and 14,000 schools in India, where there are also 22 colleges and universities dedicated to the exclusive study of the language. Sanskrit is one the 22 scheduled languages of India. Despite it being a studied school subject in contemporary India, Sanskrit is scarce as a first language. In the 2001 Census of India, 14,135 Indians reported Sanskrit to be their mother tongue, while in the 2011 census, 24,821 people out of about 1.21 billion reported this to be the case.
According to the 2011 national census of Nepal, 1,669 people use Sanskrit as their first language.

The Central Board of Secondary Education of India (CBSE), along with several other state education boards, has made Sanskrit an alternative option to the state's own official language as a second or third language choice in the schools it governs. In such schools, learning Sanskrit is an option for grades 5 to 8 (Classes V to VIII). This is true of most schools affiliated with the Indian Certificate of Secondary Education (ICSE) board, especially in states where the official language is Hindi. Sanskrit is also taught in traditional gurukulas throughout India.

A number of colleges and universities in India have dedicated departments for Sanskrit studies.

St James Junior School in London, England, offers Sanskrit as part of the curriculum. Since September 2009, US high school students have been able to receive credits as Independent Study or toward Foreign Language requirements by studying Sanskrit as part of the "SAFL: Samskritam as a Foreign Language" program coordinated by Samskrita Bharati. In Australia, the private boys' high school Sydney Grammar School offers Sanskrit from years 7 through to 12, including for the Higher School Certificate. Other schools that offer Sanskrit include the Ficino School in Auckland, New Zealand; St James Preparatory Schools in Cape Town, Durban and Johannesburg, South Africa; John Colet School, Sydney, Australia; Erasmus School, Melbourne, Australia.

European scholarship in Sanskrit, begun by Heinrich Roth (1620–1668) and Johann Ernst Hanxleden (1681–1731), is considered responsible for the discovery of an Indo-European language family by Sir William Jones (1746–1794). This research played an important role in the development of Western philology, or historical linguistics.

The 18th- and 19th-century speculations about the possible links of Sanskrit to ancient Egyptian language were later proven to be wrong, but it fed an orientalist discourse both in the form Indophobia and Indophilia, states Trautmann. Sanskrit writings, when first discovered, were imagined by Indophiles to potentially be "repositories of the primitive experiences and religion of the human race, and as such confirmatory of the truth of Christian scripture", as well as a key to "universal ethnological narrative". The Indophobes imagined the opposite, making the counterclaim that there is little of any value in Sanskrit, portraying it as "a language fabricated by artful [Brahmin] priests", with little original thought, possibly copied from the Greeks who came with Alexander or perhaps the Persians.

Scholars such as William Jones and his colleagues felt the need for systematic studies of Sanskrit language and literature. This launched the Asiatic Society, an idea that was soon transplanted to Europe starting with the efforts of Henry Thomas Colebrooke in Britain, then Alexander Hamilton who helped expand its studies to Paris and thereafter his student Friedrich Schlegel who introduced Sanskrit to the universities of Germany. Schlegel nurtured his own students into influential European Sanskrit scholars, particularly through Franz Bopp and Friedrich Max Muller. As these scholars translated the Sanskrit manuscripts, the enthusiasm for Sanskrit grew rapidly among European scholars, states Trautmann, and chairs for Sanskrit "were established in the universities of nearly every German statelet" creating a competition for Sanskrit experts.

In India, Indonesia, Nepal, Bangladesh, Sri Lanka, and Southeast Asia, Sanskrit phrases are widely used as mottoes for various national, educational and social organisations:

The song "My Sweet Lord" by George Harrison includes The Hare Krishna mantra, also referred to reverentially as the Maha Mantra, is a 16-word Vaishnava mantra which is mentioned in the Kali-Santarana Upanishad."Satyagraha", an opera by Philip Glass, uses texts from the "Bhagavad Gita", sung in Sanskrit. The closing credits of "The Matrix Revolutions" has a prayer from the "Brihadaranyaka Upanishad". The song "Cyber-raga" from Madonna's album "Music" includes Sanskrit chants, and "Shanti/Ashtangi" from her 1998 album "Ray of Light", which won a Grammy, is the ashtanga vinyasa yoga chant. The lyrics include the mantra Om shanti. Composer John Williams featured choirs singing in Sanskrit for "Indiana Jones and the Temple of Doom" and in "". The theme song of "Battlestar Galactica 2004" is the Gayatri Mantra, taken from the Rigveda. The lyrics of "The Child in Us" by Enigma also contains Sanskrit verses. In 2006, Mexican singer Paulina Rubio was influenced in Sanskrit for her concept album "Ananda".





</doc>
<doc id="27699" url="https://en.wikipedia.org/wiki?curid=27699" title="Sign language">
Sign language

Sign languages (also known as signed languages) are languages that use the visual-manual modality to convey meaning. Sign languages are expressed through manual articulations in combination with non-manual elements. Sign languages are full-fledged natural languages with their own grammar and lexicon. Sign languages are not universal and they are not mutually intelligible with each other, although there are also striking similarities among sign languages.

Linguists consider both spoken and signed communication to be types of natural language, meaning that both emerged through an abstract, protracted aging process and evolved over time without meticulous planning. Sign language should not be confused with body language, a type of nonverbal communication.

Wherever communities of deaf people exist, sign languages have developed as handy means of communication and they form the core of local deaf cultures. Although signing is used primarily by the deaf and hard of hearing, it is also used by hearing individuals, such as those unable to physically speak, those who have trouble with spoken language due to a disability or condition (augmentative and alternative communication), or those with deaf family members, such as children of deaf adults.

It is unclear how many sign languages currently exist worldwide. Each country generally has its own native sign language, and some have more than one. The 2013 edition of "Ethnologue" lists 137 sign languages. Some sign languages have obtained some form of legal recognition, while others have no status at all.

Linguists distinguish natural sign languages from other systems that are precursors to them or derived from them, such as invented manual codes for spoken languages, home sign, "baby sign", and signs learned by non-human primates.

Groups of deaf people have used sign languages throughout history. One of the earliest written records of a sign language is from the fifth century BC, in Plato's "Cratylus", where Socrates says: "If we hadn't a voice or a tongue, and wanted to express things to one another, wouldn't we try to make signs by moving our hands, head, and the rest of our body, just as dumb people do at present?"

Until the 19th century, most of what is known about historical sign languages is limited to the manual alphabets (fingerspelling systems) that were invented to facilitate transfer of words from a spoken language to a sign language, rather than documentation of the language itself. Pedro Ponce de León (1520–1584) is said to have developed the first manual alphabet.

In 1620, Juan Pablo Bonet published (‘Reduction of letters and art for teaching mute people to speak’) in Madrid. It is considered the first modern treatise of sign language phonetics, setting out a method of oral education for deaf people and a manual alphabet.
In Britain, manual alphabets were also in use for a number of purposes, such as secret communication, public speaking, or communication by deaf people. In 1648, John Bulwer described "Master Babington", a deaf man proficient in the use of a manual alphabet, "contryved on the joynts of his fingers", whose wife could converse with him easily, even in the dark through the use of tactile signing.

In 1680, George Dalgarno published "Didascalocophus, or, The deaf and dumb mans tutor", in which he presented his own method of deaf education, including an "arthrological" alphabet, where letters are indicated by pointing to different joints of the fingers and palm of the left hand. Arthrological systems had been in use by hearing people for some time; some have speculated that they can be traced to early Ogham manual alphabets.

The vowels of this alphabet have survived in the contemporary alphabets used in British Sign Language, Auslan and New Zealand Sign Language. The earliest known printed pictures of consonants of the modern two-handed alphabet appeared in 1698 with "Digiti Lingua" (Latin for "Language" [or "Tongue"] "of the Finger"), a pamphlet by an anonymous author who was himself unable to speak. He suggested that the manual alphabet could also be used by mutes, for silence and secrecy, or purely for entertainment. Nine of its letters can be traced to earlier alphabets, and 17 letters of the modern two-handed alphabet can be found among the two sets of 26 handshapes depicted.

Charles de La Fin published a book in 1692 describing an alphabetic system where pointing to a body part represented the first letter of the part (e.g. Brow=B), and vowels were located on the fingertips as with the other British systems. He described such codes for both English and Latin.

By 1720, the British manual alphabet had found more or less its present form. Descendants of this alphabet have been used by deaf communities (or at least in classrooms) in former British colonies India, Australia, New Zealand, Uganda and South Africa, as well as the republics and provinces of the former Yugoslavia, Grand Cayman Island in the Caribbean, Indonesia, Norway, Germany and the United States.

Frenchman Charles-Michel de l'Épée published his manual alphabet in the 18th century, which has survived largely unchanged in France and North America until the present time. In 1755, Abbé de l'Épée founded the first school for deaf children in Paris; Laurent Clerc was arguably its most famous graduate. Clerc went to the United States with Thomas Hopkins Gallaudet to found the American School for the Deaf in Hartford, Connecticut, in 1817. Gallaudet's son, Edward Miner Gallaudet, founded a school for the deaf in 1857 in Washington, D.C., which in 1864 became the National Deaf-Mute College. Now called Gallaudet University, it is still the only liberal arts university for deaf people in the world.

International Sign, formerly known as Gestuno, is used mainly at international deaf events such as the Deaflympics and meetings of the World Federation of the Deaf. While recent studies claim that International Sign is a kind of a pidgin, they conclude that it is more complex than a typical pidgin and indeed is more like a full sign language.
While the more commonly used term is International Sign, it is sometimes referred to as Gestuno, or International Sign Pidgin and International Gesture (IG). International Sign is a term used by the World Federation of the Deaf and other international organisations.

In linguistic terms, sign languages are as rich and complex as any spoken language, despite the common misconception that they are not "real languages". Professional linguists have studied many sign languages and found that they exhibit the fundamental properties that exist in all languages.

Sign languages are not mime—in other words, signs are conventional, often arbitrary and do not necessarily have a visual relationship to their referent, much as most spoken language is not onomatopoeic. While iconicity is more systematic and widespread in sign languages than in spoken ones, the difference is not categorical. The visual modality allows the human preference for close connections between form and meaning, present but suppressed in spoken languages, to be more fully expressed. This does not mean that sign languages are a visual rendition of a spoken language. They have complex grammars of their own and can be used to discuss any topic, from the simple and concrete to the lofty and abstract.

Sign languages, like spoken languages, organize elementary, meaningless units called phonemes into meaningful semantic units. (These were once called cheremes, from the Greek word for "hand", in the case of sign languages, by analogy to the phonemes, from Greek for "voice", of spoken languages, but now also called phonemes, since the function is the same.) This is often called duality of patterning. As in spoken languages, these meaningless units are represented as (combinations of) features, although crude distinctions are often also made in terms of handshape (or "handform"), orientation, location (or "place of articulation"), movement, and non-manual expression. More generally, both sign and spoken languages share the characteristics that linguists have found in all natural human languages, such as transitoriness, semanticity, arbitrariness, productivity, and cultural transmission.

Common linguistic features of many sign languages are the occurrence of classifier constructions, a high degree of inflection by means of changes of movement, and a topic-comment syntax. More than spoken languages, sign languages can convey meaning by simultaneous means, e.g. by the use of space, two manual articulators, and the signer's face and body. Though there is still much discussion on the topic of iconicity in sign languages, classifiers are generally considered to be highly iconic, as these complex constructions "function as predicates that may express any or all of the following: motion, position, stative-descriptive, or handling information". It needs to be noted that the term classifier is not used by everyone working on these constructions. Across the field of sign language linguistics the same constructions are also referred with other terms.

Today, linguists study sign languages as true languages, part of the field of linguistics. However, the category "sign languages" was not added to the "Linguistic Bibliography / Bibliographie Linguistique" until the 1988 volume, when it appeared with 39 entries.

There is a common misconception that sign languages are somehow dependent on spoken languages: that they are spoken language expressed in signs, or that they were invented by hearing people. Similarities in language processing in the brain between signed and spoken languages further perpetuated this misconception. Hearing teachers in deaf schools, such as Charles-Michel de l'Épée or Thomas Hopkins Gallaudet, are often incorrectly referred to as "inventors" of sign language. Instead, sign languages, like all natural languages, are developed by the people who use them, in this case, deaf people, who may have little or no knowledge of any spoken language.

As a sign language develops, it sometimes borrows elements from spoken languages, just as all languages borrow from other languages that they are in contact with. Sign languages vary in how and how much they borrow from spoken languages. In many sign languages, a manual alphabet (fingerspelling) may be used in signed communication to borrow a word from a spoken language, by spelling out the letters. This is most commonly used for proper names of people and places; it is also used in some languages for concepts for which no sign is available at that moment, particularly if the people involved are to some extent bilingual in the spoken language. Fingerspelling can sometimes be a source of new signs, such as initialized signs, in which the handshape represents the first letter of a spoken word with the same meaning.

On the whole, though, sign languages are independent of spoken languages and follow their own paths of development. For example, British Sign Language (BSL) and American Sign Language (ASL) are quite different and mutually unintelligible, even though the hearing people of the United Kingdom and the United States share the same spoken language. The grammars of sign languages do not usually resemble those of spoken languages used in the same geographical area; in fact, in terms of syntax, ASL shares more with spoken Japanese than it does with English.

Similarly, countries which use a single spoken language throughout may have two or more sign languages, or an area that contains more than one spoken language might use only one sign language. South Africa, which has 11 official spoken languages and a similar number of other widely used spoken languages, is a good example of this. It has only one sign language with two variants due to its history of having two major educational institutions for the deaf which have served different geographic areas of the country.

Sign languages exploit the unique features of the visual medium (sight), but may also exploit tactile features (tactile sign languages). Spoken language is by and large linear; only one sound can be made or received at a time. Sign language, on the other hand, is visual and, hence, can use a simultaneous expression, although this is limited articulatorily and linguistically. Visual perception allows processing of simultaneous information.

One way in which many sign languages take advantage of the spatial nature of the language is through the use of classifiers. Classifiers allow a signer to spatially show a referent's type, size, shape, movement, or extent.

The large focus on the possibility of simultaneity in sign languages in contrast to spoken languages is sometimes exaggerated, though. The use of two manual articulators is subject to motor constraints, resulting in a large extent of symmetry or signing with one articulator only. Further, sign languages, just like spoken languages, depend on linear sequencing of signs to form sentences; the greater use of simultaneity is mostly seen in the morphology (internal structure of individual signs).

Sign languages convey much of their prosody through non-manual elements. Postures or movements of the body, head, eyebrows, eyes, cheeks, and mouth are used in various combinations to show several categories of information, including lexical distinction, grammatical structure, adjectival or adverbial content, and discourse functions.

At the lexical level, signs can be lexically specified for non-manual elements in addition to the manual articulation. For instance, facial expressions may accompany verbs of emotion, as in the sign for "angry" in Czech Sign Language. Non-manual elements may also be lexically contrastive. For example, in ASL (American Sign Language), facial components distinguish some signs from other signs. An example is the sign translated as "not yet", which requires that the tongue touch the lower lip and that the head rotate from side to side, in addition to the manual part of the sign. Without these features the sign would be interpreted as "late". Mouthings, which are (parts of) spoken words accompanying lexical signs, can also be contrastive, as in the manually identical signs for "doctor" and "battery" in Sign Language of the Netherlands.

While the content of a signed sentence is produced manually, many grammatical functions are produced non-manually (i.e., with the face and the torso). Such functions include questions, negation, relative clauses and topicalization. ASL and BSL use similar non-manual marking for yes/no questions, for example. They are shown through raised eyebrows and a forward head tilt.

Some adjectival and adverbial information is conveyed through non-manual elements, but what these elements are varies from language to language. For instance, in ASL a slightly open mouth with the tongue relaxed and visible in the corner of the mouth means 'carelessly', but a similar non-manual in BSL means 'boring' or 'unpleasant'.

Discourse functions such as turn taking are largely regulated through head movement and eye gaze. Since the addressee in a signed conversation must be watching the signer, a signer can avoid letting the other person have a turn by not looking at them, or can indicate that the other person may have a turn by making eye contact.

Iconicity is similarity or analogy between the form of a sign (linguistic or otherwise) and its meaning, as opposed to arbitrariness. The first studies on iconicity in ASL were published in the late 1970s, and early 1980s. Many early sign language linguists rejected the notion that iconicity was an important aspect of the language. Though they recognized that certain aspects of the language seemed iconic, they considered this to be merely extralinguistic, a property which did not influence the language. However, mimetic aspects of sign language (signs that imitate, mimic, or represent) are found in abundance across a wide variety of sign languages. For example, when deaf children learning sign language try to express something but do not know the associated sign, they will often invent an iconic sign that displays mimetic properties. Though it never disappears from a particular sign language, iconicity is gradually weakened as forms of sign languages become more customary and are subsequently grammaticized. As a form becomes more conventional, it becomes disseminated in a methodical way phonologically to the rest of the sign language community. Frishberg (1975) wrote a very influential paper addressing the relationship between arbitrariness and iconicity in ASL. She concluded that though originally present in many signs, iconicity is degraded over time through the application of grammatical processes. In other words, over time, the natural processes of regularization in the language obscures any iconically motivated features of the sign.

In 1978, Psychologist Roger Brown was one of the first to suggest that the properties of ASL give it a clear advantage in terms of learning and memory. In his study, Brown found that when a group of six hearing children were taught signs that had high levels of iconic mapping they were significantly more likely to recall the signs in a later memory task than another group of six children that were taught signs that had little or no iconic properties. In contrast to Brown, linguists Elissa Newport and Richard Meier found that iconicity "appears to have virtually no impact on the acquisition of American Sign Language".

A central task for the pioneers of sign language linguistics was trying to prove that ASL was a real language and not merely a collection of gestures or "English on the hands." One of the prevailing beliefs at this time was that 'real languages' must consist of an arbitrary relationship between form and meaning. Thus, if ASL consisted of signs that had iconic form-meaning relationship, it could not be considered a real language. As a result, iconicity as a whole was largely neglected in research of sign languages.

The cognitive linguistics perspective rejects a more traditional definition of iconicity as a relationship between linguistic form and a concrete, real-world referent. Rather it is a set of selected correspondences between the form and meaning of a sign. In this view, iconicity is grounded in a language user's mental representation ("construal" in cognitive grammar). It is defined as a fully grammatical and central aspect of a sign language rather than a peripheral phenomenon.

The cognitive linguistics perspective allows for some signs to be fully iconic or partially iconic given the number of correspondences between the possible parameters of form and meaning. In this way, the Israeli Sign Language (ISL) sign for "ask" has parts of its form that are iconic ("movement away from the mouth" means "something coming from the mouth"), and parts that are arbitrary (the handshape, and the orientation).

Many signs have metaphoric mappings as well as iconic or metonymic ones. For these signs there are three way correspondences between a form, a concrete source and an abstract target meaning. The ASL sign LEARN has this three way correspondence. The abstract target meaning is "learning". The concrete source is putting objects into the head from books. The form is a grasping hand moving from an open palm to the forehead. The iconic correspondence is between form and concrete source. The metaphorical correspondence is between concrete source and abstract target meaning. Because the concrete source is connected to two correspondences linguistics refer to metaphorical signs as "double mapped".
Although sign languages have emerged naturally in deaf communities alongside or among spoken languages, they are unrelated to spoken languages and have different grammatical structures at their core.

Sign languages may be classified by how they arise.

In non-signing communities, home sign is not a full language, but closer to a pidgin. Home sign is amorphous and generally idiosyncratic to a particular family, where a deaf child does not have contact with other deaf children and is not educated in sign. Such systems are not generally passed on from one generation to the next. Where they are passed on, creolization would be expected to occur, resulting in a full language. However, home sign may also be closer to full language in communities where the hearing population has a gestural mode of language; examples include various Australian Aboriginal sign languages and gestural systems across West Africa, such as Mofu-Gudur in Cameroon.

A village sign language is a local indigenous language that typically arises over several generations in a relatively insular community with a high incidence of deafness, and is used both by the deaf and by a significant portion of the hearing community, who have deaf family and friends. The most famous of these is probably the extinct Martha's Vineyard Sign Language of the US, but there are also numerous village languages scattered throughout Africa, Asia, and America.

Deaf-community sign languages, on the other hand, arise where deaf people come together to form their own communities. These include school sign, such as Nicaraguan Sign Language, which develop in the student bodies of deaf schools which do not use sign as a language of instruction, as well as community languages such as Bamako Sign Language, which arise where generally uneducated deaf people congregate in urban centers for employment. At first, Deaf-community sign languages are not generally known by the hearing population, in many cases not even by close family members. However, they may grow, in some cases becoming a language of instruction and receiving official recognition, as in the case of ASL.

Both contrast with speech-taboo languages such as the various Aboriginal Australian sign languages, which are developed by the hearing community and only used secondarily by the deaf. It is doubtful whether most of these are languages in their own right, rather than manual codes of spoken languages, though a few such as Yolngu Sign Language are independent of any particular spoken language. Hearing people may also develop sign to communicate with users of other languages, as in Plains Indian Sign Language; this was a contact signing system or pidgin that was evidently not used by deaf people in the Plains nations, though it presumably influenced home sign.

Language contact and creolization is common in the development of sign languages, making clear family classifications difficult – it is often unclear whether lexical similarity is due to borrowing or a common parent language, or whether there was one or several parent languages, such as several village languages merging into a Deaf-community language. Contact occurs between sign languages, between sign and spoken languages (contact sign, a kind of pidgin), and between sign languages and gestural systems used by the broader community. One author has speculated that Adamorobe Sign Language, a village sign language of Ghana, may be related to the "gestural trade jargon used in the markets throughout West Africa", in vocabulary and areal features including prosody and phonetics.

The only comprehensive classification along these lines going beyond a simple listing of languages dates back to 1991. The classification is based on the 69 sign languages from the 1988 edition of Ethnologue that were known at the time of the 1989 conference on sign languages in Montreal and 11 more languages the author added after the conference.

In his classification, the author distinguishes between primary and auxiliary sign languages as well as between single languages and names that are thought to refer to more than one language. The prototype-A class of languages includes all those sign languages that seemingly cannot be derived from any other language. Prototype-R languages are languages that are remotely modelled on a prototype-A language (in many cases thought to have been French Sign Language) by a process Kroeber (1940) called "stimulus diffusion". The families of BSL, DGS, JSL, LSF (and possibly LSG) were the products of creolization and relexification of prototype languages. Creolization is seen as enriching overt morphology in sign languages, as compared to reducing overt morphology in spoken languages.

Linguistic typology (going back to Edward Sapir) is based on word structure and distinguishes morphological classes such as agglutinating/concatenating, inflectional, polysynthetic, incorporating, and isolating ones.

Sign languages vary in word-order typology. For example, Austrian Sign Language, Japanese Sign Language and Indo-Pakistani Sign Language are Subject-object-verb while ASL is Subject-verb-object. Influence from the surrounding spoken languages is not improbable.

Sign languages tend to be incorporating classifier languages, where a classifier handshape representing the object is incorporated into those transitive verbs which allow such modification. For a similar group of intransitive verbs (especially motion verbs), it is the subject which is incorporated. Only in a very few sign languages (for instance Japanese Sign Language) are agents ever incorporated. in this way, since subjects of intransitives are treated similarly to objects of transitives, incorporation in sign languages can be said to follow an ergative pattern.

Brentari classifies sign languages as a whole group determined by the medium of communication (visual instead of auditory) as one group with the features monosyllabic and polymorphemic. That means, that one syllable (i.e. one word, one sign) can express several morphemes, e.g., subject and object of a verb determine the direction of the verb's movement (inflection).

Another aspect of typology that has been studied in sign languages is their systems for cardinal numbers. Typologically significant differences have been found between sign languages.

Children who are exposed to a sign language from birth will acquire it, just as hearing children acquire their native spoken language.

The Critical Period hypothesis suggests that language, spoken or signed, is more easily acquired as a child at a young age versus an adult because of the plasticity of the child's brain. In a study done at the University of McGill, they found that American Sign Language users who acquired the language natively (from birth) performed better when asked to copy videos of ASL sentences than ASL users who acquired the language later in life. They also found that there are differences in the grammatical morphology of ASL sentences between the two groups, all suggesting that there is a very important critical period in learning signed languages.

The acquisition of non-manual features follows an interesting pattern: When a word that always has a particular non-manual feature associated with it (such as a wh- question word) is learned, the non-manual aspects are attached to the word but don't have the flexibility associated with adult use. At a certain point, the non-manual features are dropped and the word is produced with no facial expression. After a few months, the non-manuals reappear, this time being used the way adult signers would use them.

Sign languages do not have a traditional or formal written form. Many deaf people do not see a need to write their own language.

Several ways to represent sign languages in written form have been developed.


So far, there is no consensus regarding the written form of sign language. Except for SignWriting, none are widely used. Maria Galea writes that SignWriting "is becoming widespread, uncontainable and untraceable. In the same way that works written in and about a well developed writing system such as the Latin script, the time has arrived where SW is so widespread, that it is impossible in the same way to list all works that have been produced using this writing system and that have been written about this writing system." In 2015, the Federal University of Santa Catarina accepted a dissertation written in Brazilian Sign Language using Sutton SignWriting for a master's degree in linguistics. The dissertation "The Writing of Grammatical Non-Manual Expressions in Sentences in LIBRAS Using the SignWriting System" by João Paulo Ampessan states that "the data indicate the need for [non-manual expressions] usage in writing sign language".

For a native signer, sign perception influences how the mind makes sense of their visual language experience. For example, a handshape may vary based on the other signs made before or after it, but these variations are arranged in perceptual categories during its development. The mind detects handshape contrasts but groups similar handshapes together in one category. Different handshapes are stored in other categories. The mind ignores some of the similarities between different perceptual categories, at the same time preserving the visual information within each perceptual category of handshape variation.

When Deaf people constitute a relatively small proportion of the general population, Deaf communities often develop that are distinct from the surrounding hearing community.
These Deaf communities are very widespread in the world, associated especially with sign languages used in urban areas and throughout a nation, and the cultures they have developed are very rich.

One example of sign language variation in the Deaf community is Black ASL. This sign language was developed in the Black Deaf community as a variant during the American era of segregation and racism, where young Black Deaf students were forced to attend separate schools than their white Deaf peers.

On occasion, where the prevalence of deaf people is high enough, a deaf sign language has been taken up by an entire local community, forming what is sometimes called a "village sign language" or "shared signing community". Typically this happens in small, tightly integrated communities with a closed gene pool. Famous examples include:
In such communities deaf people are generally well integrated in the general community and not socially disadvantaged, 
so much so that it is difficult to speak of a separate "Deaf" community.

Many Australian Aboriginal sign languages arose in a context of extensive speech taboos, such as during mourning and initiation rites. They are or were especially highly developed among the Warlpiri, Warumungu, Dieri, Kaytetye, Arrernte, and Warlmanpa, and are based on their respective spoken languages.

A pidgin sign language arose among tribes of American Indians in the Great Plains region of North America (see Plains Indian Sign Language). It was used by hearing people to communicate among tribes with different spoken languages, as well as by deaf people. There are especially users today among the Crow, Cheyenne, and Arapaho. Unlike Australian Aboriginal sign languages, it shares the spatial grammar of deaf sign languages.
In the 1500s, a Spanish expeditionary, Cabeza de Vaca, observed natives in the western part of modern-day Florida using sign language, and in the mid-16th century Coronado mentioned that communication with the Tonkawa using signs was possible without a translator. Whether or not these gesture systems reached the stage at which they could properly be called languages is still up for debate. There are estimates indicating that as many as 2% of Native Americans are seriously or completely deaf, a rate more than twice the national average.

Sign language is also used by some people as a form of alternative or augmentative communication by people who can hear but cannot use their voices to speak.

Signs may also be used by hearing people for manual communication in secret situations, such as hunting, in noisy environments, underwater, through windows or at a distance.

Some sign languages have obtained some form of legal recognition, while others have no status at all. Sarah Batterbury has argued that sign languages should be recognized and supported not merely as an accommodation for the disabled, but as the communication medium of language communities.

One of the first demonstrations of the ability for telecommunications to help sign language users communicate with each other occurred when AT&T's videophone (trademarked as the "Picturephone") was introduced to the public at the 1964 New York World's Fair – two deaf users were able to freely communicate with each other between the fair and another city. However, video communication did not become widely available until sufficient bandwidth for the high volume of video data became available in the early 2000s.

The Internet now allows deaf people to talk via a video link, either with a special-purpose videophone designed for use with sign language or with "off-the-shelf" video services designed for use with broadband and an ordinary computer webcam. The special videophones that are designed for sign language communication may provide better quality than 'off-the-shelf' services and may use data compression methods specifically designed to maximize the intelligibility of sign languages. Some advanced equipment enables a person to remotely control the other person's video camera, in order to zoom in and out or to point the camera better to understand the signing.

In order to facilitate communication between deaf and hearing people, sign language interpreters are often used. Such activities involve considerable effort on the part of the interpreter, since sign languages are distinct natural languages with their own syntax, different from any spoken language.

The interpretation flow is normally between a sign language and a spoken language that are customarily used in the same country, such as French Sign Language (LSF) and spoken French in France, Spanish Sign Language (LSE) to spoken Spanish in Spain, British Sign Language (BSL) and spoken English in the U.K., and American Sign Language (ASL) and spoken English in the US and most of anglophone Canada (since BSL and ASL are distinct sign languages both used in English-speaking countries), etc. Sign language interpreters who can translate between signed and spoken languages that are not normally paired (such as between LSE and English), are also available, albeit less frequently.

With recent developments in artificial intelligence in computer science, some recent deep learning based machine translation algorithms have been developed which automatically translate short videos containing sign language sentences (often simple sentence consists of only one clause) directly to written language. 

Interpreters may be physically present with both parties to the conversation but, since the technological advancements in the early 2000s, provision of interpreters in remote locations has become available. In video remote interpreting (VRI), the two clients (a sign language user and a hearing person who wish to communicate with each other) are in one location, and the interpreter is in another. The interpreter communicates with the sign language user via a video telecommunications link, and with the hearing person by an audio link. VRI can be used for situations in which no on-site interpreters are available.

However, VRI cannot be used for situations in which all parties are speaking via telephone alone. With video relay service (VRS), the sign language user, the interpreter, and the hearing person are in three separate locations, thus allowing the two clients to talk to each other on the phone through the interpreter.
Sign language is sometimes provided for television programmes that include speech. The signer usually appears in the bottom corner of the screen, with the programme being broadcast full size or slightly shrunk away from that corner. Typically for press conferences such as those given by the Mayor of New York City, the signer appears to stage left or right of the public official to allow both the speaker and signer to be in frame at the same time.

Paddy Ladd initiated deaf programming on British television in the 1980s and is credited with getting sign language on television and enabling deaf children to be educated in sign.

In traditional analogue broadcasting, many programmes are repeated, often in the early hours of the morning, with the signer present rather than have them appear at the main broadcast time. This is due to the distraction they cause to those not wishing to see the signer. On the BBC, many programmes that broadcast late at night or early in the morning are signed. Some emerging television technologies allow the viewer to turn the signer on and off in a similar manner to subtitles and closed captioning.

Legal requirements covering sign language on television vary from country to country. In the United Kingdom, the Broadcasting Act 1996 addressed the requirements for blind and deaf viewers, but has since been replaced by the Communications Act 2003.

As with any spoken language, sign languages are also vulnerable to becoming endangered. For example, a sign language used by a small community may be endangered and even abandoned as users shift to a sign language used by a larger community, as has happened with Hawai'i Sign Language, which is almost extinct except for a few elderly signers. Even nationally recognised sign languages can be endangered; for example, New Zealand Sign Language is losing users. Methods are being developed to assess the language vitality of sign languages.

There are a number of communication systems that are similar in some respects to sign languages, while not having all the characteristics of a full sign language, particularly its grammatical structure. Many of these are either precursors to natural sign languages or are derived from them.

When Deaf and Hearing people interact, signing systems may be developed that use signs drawn from a natural sign language but used according to the grammar of the spoken language. In particular, when people devise one-for-one sign-for-word correspondences between spoken words (or even morphemes) and signs that represent them, the system that results is a manual code for a spoken language, rather than a natural sign language. Such systems may be invented in an attempt to help teach Deaf children the spoken language, and generally are not used outside an educational context.

Some hearing parents teach signs to young hearing children. Since the muscles in babies' hands grow and develop quicker than their mouths, signs are seen as a beneficial option for better communication. Babies can usually produce signs before they can speak. This reduces the confusion between parents when trying to figure out what their child wants. When the child begins to speak, signing is usually abandoned, so the child does not progress to acquiring the grammar of the sign language.

This is in contrast to hearing children who grow up with Deaf parents, who generally acquire the full sign language natively, the same as Deaf children of Deaf parents.

Informal, rudimentary sign systems are sometimes developed within a single family. For instance, when hearing parents with no sign language skills have a deaf child, the child may develop a system of signs naturally, unless repressed by the parents. The term for these mini-languages is home sign (sometimes "kitchen sign").

Home sign arises due to the absence of any other way to communicate. Within the span of a single lifetime and without the support or feedback of a community, the child naturally invents signs to help meet his or her communication needs, and may even develop a few grammatical rules for combining short sequences of signs. Still, this kind of system is inadequate for the intellectual development of a child and it comes nowhere near meeting the standards linguists use to describe a complete language. No type of home sign is recognized as a full language.

There have been several notable examples of scientists teaching signs to non-human primates in order to communicate with humans, such as 
chimpanzees, gorillas and 
orangutans. However, linguists generally point out that this does not constitute knowledge of a human "language" as a complete system, rather than simply signs / words. Notable examples of animals who have learned signs include:

One theory of the evolution of human language states that it developed first as a gestural system, which later shifted to speech. An important question for this gestural theory is what caused the shift to vocalization.




"Note: the articles for specific sign languages (e.g. ASL or BSL) may contain further external links, e.g. for learning those languages."


</doc>
<doc id="27701" url="https://en.wikipedia.org/wiki?curid=27701" title="String (computer science)">
String (computer science)

In computer programming, a string is traditionally a sequence of characters, either as a literal constant or as some kind of variable. The latter may allow its elements to be mutated and the length changed, or it may be fixed (after creation). A string is generally considered as a data type and is often implemented as an array data structure of bytes (or words) that stores a sequence of elements, typically characters, using some character encoding. "String" may also denote more general arrays or other sequence (or list) data types and structures.

Depending on the programming language and precise data type used, a variable declared to be a string may either cause storage in memory to be statically allocated for a predetermined maximum length or employ dynamic allocation to allow it to hold a variable number of elements.

When a string appears literally in source code, it is known as a string literal or an anonymous string.

In formal languages, which are used in mathematical logic and theoretical computer science, a string is a finite sequence of symbols that are chosen from a set called an alphabet.

A string datatype is a datatype modeled on the idea of a formal string. Strings are such an important and useful datatype that they are implemented in nearly every programming language. In some languages they are available as primitive types and in others as composite types. The syntax of most high-level programming languages allows for a string, usually quoted in some way, to represent an instance of a string datatype; such a meta-string is called a "literal" or "string literal".

Although formal strings can have an arbitrary finite length, the length of strings in real languages is often constrained to an artificial maximum. In general, there are two types of string datatypes: "fixed-length strings", which have a fixed maximum length to be determined at compile time and which use the same amount of memory whether this maximum is needed or not, and "variable-length strings", whose length is not arbitrarily fixed and which can use varying amounts of memory depending on the actual requirements at run time (see Memory management). Most strings in modern programming languages are variable-length strings. Of course, even variable-length strings are limited in length – by the size of available computer memory. The string length can be stored as a separate integer (which may put another artificial limit on the length) or implicitly through a termination character, usually a character value with all bits zero such as in C programming language. See also "Null-terminated" below.

String datatypes have historically allocated one byte per character, and, although the exact character set varied by region, character encodings were similar enough that programmers could often get away with ignoring this, since characters a program treated specially (such as period and space and comma) were in the same place in all the encodings a program would encounter. These character sets were typically based on ASCII or EBCDIC. If text in one encoding was displayed on a system using a different encoding, text was often mangled, though often somewhat readable and some computer users learned to read the mangled text.

Logographic languages such as Chinese, Japanese, and Korean (known collectively as CJK) need far more than 256 characters (the limit of a one 8-bit byte per-character encoding) for reasonable representation. The normal solutions involved keeping single-byte representations for ASCII and using two-byte representations for CJK ideographs. Use of these with existing code led to problems with matching and cutting of strings, the severity of which depended on how the character encoding was designed. Some encodings such as the EUC family guarantee that a byte value in the ASCII range will represent only that ASCII character, making the encoding safe for systems that use those characters as field separators. Other encodings such as ISO-2022 and Shift-JIS do not make such guarantees, making matching on byte codes unsafe. These encodings also were not "self-synchronizing", so that locating character boundaries required backing up to the start of a string, and pasting two strings together could result in corruption of the second string.

Unicode has simplified the picture somewhat. Most programming languages now have a datatype for Unicode strings. Unicode's preferred byte stream format UTF-8 is designed not to have the problems described above for older multibyte encodings. UTF-8, UTF-16 and UTF-32 require the programmer to know that the fixed-size code units are different than the "characters", the main difficulty currently is incorrectly designed APIs that attempt to hide this difference (UTF-32 does make "code points" fixed-sized, but these are not "characters" due to composing codes).

Some languages, such as C++ and Ruby, normally allow the contents of a string to be changed after it has been created; these are termed "mutable" strings. In other languages, such as Java and Python, the value is fixed and a new string must be created if any alteration is to be made; these are termed "immutable" strings (some of these languages also provide another type that is mutable, such as Java and .NET , the thread-safe Java , and the Cocoa codice_1).

Strings are typically implemented as arrays of bytes, characters, or code units, in order to allow fast access to individual units or substrings—including characters when they have a fixed length. A few languages such as Haskell implement them as linked lists instead.

Some languages, such as Prolog and Erlang, avoid implementing a dedicated string datatype at all, instead adopting the convention of representing strings as lists of character codes.

Representations of strings depend heavily on the choice of character repertoire and the method of character encoding. Older string implementations were designed to work with repertoire and encoding defined by ASCII, or more recent extensions like the ISO 8859 series. Modern implementations often use the extensive repertoire defined by Unicode along with a variety of complex encodings such as UTF-8 and UTF-16.

The term "byte string" usually indicates a general-purpose string of bytes, rather than strings of only (readable) characters, strings of bits, or such. Byte strings often imply that bytes can take any value and any data can be stored as-is, meaning that there should be no value interpreted as a termination value.

Most string implementations are very similar to variable-length arrays with the entries storing the character codes of corresponding characters. The principal difference is that, with certain encodings, a single logical character may take up more than one entry in the array. This happens for example with UTF-8, where single codes (UCS code points) can take anywhere from one to four bytes, and single characters can take an arbitrary number of codes. In these cases, the logical length of the string (number of characters) differs from the physical length of the array (number of bytes in use). UTF-32 avoids the first part of the problem.

The length of a string can be stored implicitly by using a special terminating character; often this is the null character (NUL), which has all bits zero, a convention used and perpetuated by the popular C programming language. Hence, this representation is commonly referred to as a C string. This representation of an "n"-character string takes "n" + 1 space (1 for the terminator), and is thus an implicit data structure.

In terminated strings, the terminating code is not an allowable character in any string. Strings with "length" field do not have this limitation and can also store arbitrary binary data.

An example of a "null-terminated string" stored in a 10-byte buffer, along with its ASCII (or more modern UTF-8) representation as 8-bit hexadecimal numbers is:

The length of the string in the above example, "codice_2", is 5 characters, but it occupies 6 bytes. Characters after the terminator do not form part of the representation; they may be either part of other data or just garbage. (Strings of this form are sometimes called "ASCIZ strings", after the original assembly language directive used to declare them.)

Using a special byte other than null for terminating strings has historically appeared in both hardware and software, though sometimes with a value that was also a printing character. codice_3 was used by many assembler systems, codice_4 used by CDC systems (this character had a value of zero), and the ZX80 used codice_5 since this was the string delimiter in its BASIC language.

Somewhat similar, "data processing" machines like the IBM 1401 used a special word mark bit to delimit strings at the left, where the operation would start at the right. This bit had to be clear in all other parts of the string. This meant that, while the IBM 1401 had a seven-bit word, almost no-one ever thought to use this as a feature, and override the assignment of the seventh bit to (for example) handle ASCII codes.

Early microcomputer software relied upon the fact that ASCII codes do not use the high-order bit, and set it to indicate the end of a string. It must be reset to 0 prior to output.

The length of a string can also be stored explicitly, for example by prefixing the string with the length as a byte value. This convention is used in many Pascal dialects; as a consequence, some people call such a string a Pascal string or P-string. Storing the string length as byte limits the maximum string length to 255. To avoid such limitations, improved implementations of P-strings use 16-, 32-, or 64-bit words to store the string length. When the "length" field covers the address space, strings are limited only by the available memory.

If the length is bounded, then it can be encoded in constant space, typically a machine word, thus leading to an implicit data structure, taking "n" + "k" space, where "k" is the number of characters in a word (8 for 8-bit ASCII on a 64-bit machine, 1 for 32-bit UTF-32/UCS-4 on a 32-bit machine, etc.).
If the length is not bounded, encoding a length "n" takes log("n") space (see fixed-length code), so length-prefixed strings are a succinct data structure, encoding a string of length "n" in log("n") + "n" space.

In the latter case, the length-prefix field itself doesn't have fixed length, therefore the actual string data needs to be moved when the string grows such that the length field needs to be increased.

Here is a Pascal string stored in a 10-byte buffer, along with its ASCII / UTF-8 representation:

Many languages, including object-oriented ones, implement strings as records with an internal structure like:
class string {

However, since the implementation is usually hidden, the string must be accessed and modified through member functions. codice_6 is a pointer to a dynamically allocated memory area, which might be expanded as needed. See also string (C++).

Both character termination and length codes limit strings: For example, C character arrays that contain null (NUL) characters cannot be handled directly by C string library functions: Strings using a length code are limited to the maximum value of the length code.

Both of these limitations can be overcome by clever programming.

It is possible to create data structures and functions that manipulate them that do not have the problems associated with character termination and can in principle overcome length code bounds. It is also possible to optimize the string represented using techniques from run length encoding (replacing repeated characters by the character value and a length) and Hamming encoding.

While these representations are common, others are possible. Using ropes makes certain string operations, such as insertions, deletions, and concatenations more efficient.

The core data structure in a text editor is the one that manages the string (sequence of characters) that represents the current state of the file being edited.
While that state could be stored in a single long consecutive array of characters, a typical text editor instead uses an alternative representation as its sequence data structure—a gap buffer, a linked list of lines, a piece table, or a rope—which makes certain string operations, such as insertions, deletions, and undoing previous edits, more efficient.

The differing memory layout and storage requirements of strings can affect the security of the program accessing the string data. String representations requiring a terminating character are commonly susceptible to buffer overflow problems if the terminating character is not present, caused by a coding error or an attacker deliberately altering the data. String representations adopting a separate length field are also susceptible if the length can be manipulated. In such cases, program code accessing the string data requires bounds checking to ensure that it does not inadvertently access or change data outside of the string memory limits.

String data is frequently obtained from user input to a program. As such, it is the responsibility of the program to validate the string to ensure that it represents the expected format. Performing limited or no validation of user input can cause a program to be vulnerable to code injection attacks.

Sometimes, strings need to be embedded inside a text file that is both human-readable and intended for consumption by a machine. This is needed in, for example, source code of programming languages, or in configuration files. In this case, the NUL character doesn't work well as a terminator since it is normally invisible (non-printable) and is difficult to input via a keyboard. Storing the string length would also be inconvenient as manual computation and tracking of the length is tedious and error-prone.

Two common representations are:

While character strings are very common uses of strings, a string in computer science may refer generically to any sequence of homogeneously typed data. A bit string or byte string, for example, may be used to represent non-textual binary data retrieved from a communications medium. This data may or may not be represented by a string-specific datatype, depending on the needs of the application, the desire of the programmer, and the capabilities of the programming language being used. If the programming language's string implementation is not 8-bit clean, data corruption may ensue.

C programmers draw a sharp distinction between a "string", aka a "string of characters", which by definition is always null terminated, vs. a "byte string" or "pseudo string" which may be stored in the same array but is often not null terminated.
Using C string handling functions on such a "byte string" often seems to work, but later leads to security problems.

There are many algorithms for processing strings, each with various trade-offs. Competing algorithms can be analyzed with respect to run time, storage requirements, and so forth.

Some categories of algorithms include:

Advanced string algorithms often employ complex mechanisms and data structures, among them suffix trees and finite-state machines.

The name "stringology" was coined in 1984 by computer scientist Zvi Galil for the issue of algorithms and data structures used for string processing.

Character strings are such a useful datatype that several languages have been designed in order to make string processing applications easy to write. Examples include the following languages:

Many Unix utilities perform simple string manipulations and can be used to easily program some powerful string processing algorithms. Files and finite streams may be viewed as strings.

Some APIs like Multimedia Control Interface, embedded SQL or printf use strings to hold commands that will be interpreted.

Recent scripting programming languages, including Perl, Python, Ruby, and Tcl employ regular expressions to facilitate text operations. Perl is particularly noted for its regular expression use, and many other languages and applications implement Perl compatible regular expressions.

Some languages such as Perl and Ruby support string interpolation, which permits arbitrary expressions to be evaluated and included in string literals.

String functions are used to create strings or change the contents of a mutable string. They also are used to query information about a string. The set of functions and their names varies depending on the computer programming language.

The most basic example of a string function is the string length function – the function that returns the length of a string (not counting any terminator characters or any of the string's internal structural information) and does not modify the string. This function is often named codice_7 or codice_8. For example, codice_9 would return 11. Another common function is concatenation, where a new string is created by appending two strings, often this is the + addition operator.

Some microprocessor's instruction set architectures contain direct support for string operations, such as block copy (e.g. In intel x86m codice_10).

Let Σ be a finite set of symbols (alternatively called characters), called the alphabet. No assumption is made about the nature of the symbols. A string (or word) over Σ is any finite sequence of symbols from Σ. For example, if Σ = {0, 1}, then "01011" is a string over Σ.

The "length" of a string "s" is the number of symbols in "s" (the length of the sequence) and can be any non-negative integer; it is often denoted as |"s"|. The "empty string" is the unique string over Σ of length 0, and is denoted "ε" or "λ".

The set of all strings over Σ of length "n" is denoted Σ. For example, if Σ = {0, 1}, then Σ = {00, 01, 10, 11}. Note that Σ = {ε} for any alphabet Σ.

The set of all strings over Σ of any length is the Kleene closure of Σ and is denoted Σ. In terms of Σ,
For example, if Σ = {0, 1}, then Σ = {ε, 0, 1, 00, 01, 10, 11, 000, 001, 010, 011, ...}. Although the set Σ itself is countably infinite, each element of Σ is a string of finite length.

A set of strings over Σ (i.e. any subset of Σ) is called a "formal language" over Σ. For example, if Σ = {0, 1}, the set of strings with an even number of zeros, {ε, 1, 00, 11, 001, 010, 100, 111, 0000, 0011, 0101, 0110, 1001, 1010, 1100, 1111, ...}, is a formal language over Σ.

"Concatenation" is an important binary operation on Σ. For any two strings "s" and "t" in Σ, their concatenation is defined as the sequence of symbols in "s" followed by the sequence of characters in "t", and is denoted "st". For example, if Σ = {a, b, ..., z}, "s" = bear, and "t" = hug, then "st" = bearhug and "ts" = hugbear.

String concatenation is an associative, but non-commutative operation. The empty string ε serves as the identity element; for any string "s", ε"s" = "s"ε = "s". Therefore, the set Σ and the concatenation operation form a monoid, the free monoid generated by Σ. In addition, the length function defines a monoid homomorphism from Σ to the non-negative integers (that is, a function formula_2, such that formula_3).

A string "s" is said to be a "substring" or "factor" of "t" if there exist (possibly empty) strings "u" and "v" such that "t" = "usv". The relation "is a substring of" defines a partial order on Σ, the least element of which is the empty string.

A string "s" is said to be a prefix of "t" if there exists a string "u" such that "t" = "su". If "u" is nonempty, "s" is said to be a "proper" prefix of "t". Symmetrically, a string "s" is said to be a suffix of "t" if there exists a string "u" such that "t" = "us". If "u" is nonempty, "s" is said to be a "proper" suffix of "t". Suffixes and prefixes are substrings of "t". Both the relations "is a prefix of" and "is a suffix of" are prefix orders.

The reverse of a string is a string with the same symbols but in reverse order. For example, if "s" = abc (where a, b, and c are symbols of the alphabet), then the reverse of "s" is cba. A string that is the reverse of itself (e.g., "s" = madam) is called a palindrome, which also includes the empty string and all strings of length 1.

A string "s" = "uv" is said to be a rotation of "t" if "t" = "vu". For example, if Σ = {0, 1} the string 0011001 is a rotation of 0100110, where "u" = 00110 and "v" = 01. As another example, the string abc has three different rotations, viz. abc itself (with "u"=abc, "v"=ε), bca (with "u"=bc, "v"=a), and cab (with "u"=c, "v"=ab).

It is often useful to define an ordering on a set of strings. If the alphabet Σ has a total order (cf. alphabetical order) one can define a total order on Σ called lexicographical order. For example, if Σ = {0, 1} and 0 < 1, then the lexicographical order on Σ includes the relationships ε < 0 < 00 < 000 < ... < 0001 < 001 < 01 < 010 < 011 < 0110 < 01111 < 1 < 10 < 100 < 101 < 111 < 1111 < 11111 ... The lexicographical order is total if the alphabetical order is, but isn't well-founded for any nontrivial alphabet, even if the alphabetical order is.

See Shortlex for an alternative string ordering that preserves well-foundedness.

A number of additional operations on strings commonly occur in the formal theory. These are given in the article on string operations.

Strings admit the following interpretation as nodes on a graph, where "k" is the number of symbols in Σ:

The natural topology on the set of fixed-length strings or variable-length strings is the discrete topology, but the natural topology on the set of infinite strings is the limit topology, viewing the set of infinite strings as the inverse limit of the sets of finite strings. This is the construction used for the "p"-adic numbers and some constructions of the Cantor set, and yields the same topology.

Isomorphisms between string representations of topologies can be found by normalizing according to the lexicographically minimal string rotation.



</doc>
<doc id="27706" url="https://en.wikipedia.org/wiki?curid=27706" title="Satanism">
Satanism

Satanism is a group of ideological and philosophical beliefs based on Satan. Contemporary religious practice of Satanism began with the founding of the Church of Satan in 1966, although a few historical precedents exist. Prior to the public practice, Satanism existed primarily as an accusation by various Christian groups toward perceived ideological opponents, rather than a self-identity. Satanism, and the concept of Satan, has also been used by artists and entertainers for symbolic expression.

Accusations that various groups have been practicing Satanism have been made throughout much of Christian history. During the Middle Ages, the Inquisition attached to the Catholic Church alleged that various heretical Christian sects and groups, such as the Knights Templar and the Cathars, performed secret Satanic rituals. In the subsequent Early Modern period, belief in a widespread Satanic conspiracy of witches resulted in mass trials of alleged witches across Europe and the North American colonies. Accusations that Satanic conspiracies were active, and behind events such as Protestantism (and conversely, the Protestant claim that the Pope was the Antichrist) and the French Revolution continued to be made in Christendom during the eighteenth to the twentieth century. The idea of a vast Satanic conspiracy reached new heights with the influential Taxil hoax of France in the 1890s, which claimed that Freemasonry worshipped Satan, Lucifer, and Baphomet in their rituals. In the 1980s and 1990s, the Satanic ritual abuse hysteria spread through the United States and the United Kingdom, amid fears that groups of Satanists were regularly sexually abusing and murdering children in their rites. In most of these cases, there is no corroborating evidence that any of those accused of Satanism were actually practitioners of a Satanic religion or guilty of the allegations leveled at them.

Since the 19th century, various small religious groups have emerged that identify as Satanists or use Satanic iconography. Satanist groups that appeared after the 1960s are widely diverse, but two major trends are theistic Satanism and atheistic Satanism. Theistic Satanists venerate Satan as a supernatural deity, viewing him not as omnipotent but rather as a patriarch. In contrast, atheistic Satanists regard Satan as merely a symbol of certain human traits.
Contemporary religious Satanism is predominantly an American phenomenon, the ideas spreading elsewhere with the effects of globalization and the Internet. The Internet spreads awareness of other Satanists, and is also the main battleground for Satanist disputes. Satanism started to reach Central and Eastern Europe in the 1990s, in time with the fall of the Soviet Union, and most noticeably in Poland and Lithuania, predominantly Roman Catholic countries.

In their study of Satanism, the religious studies scholars Asbjørn Dyrendal, James R. Lewis, and Jesper Aa. Petersen stated that the term "Satanism" "has a history of being a designation made by people against those whom they dislike; it is a term used for 'othering'". The concept of Satanism is an invention of Christianity, for it relies upon the figure of Satan, a character deriving from Christian mythology.

Elsewhere, Petersen noted that "Satanism as something others do is very different from Satanism as a self-designation".
Eugene Gallagher noted that, as commonly used, "Satanism" was usually "a polemical, not a descriptive term".

In 1994, the Italian sociologist Massimo Introvigne suggested defining Satanism with the simultaneous presence of "1) the worship of the character identified with the name of Satan or Lucifer in the Bible, 2) by organized groups with at least a minimal organization and hierarchy, 3) through ritual or liturgical practices." The definition applies regardless the way in which "each group perceives Satan, as personal or impersonal, real or symbolical.

The word "Satan" was not originally a proper name but rather an ordinary noun meaning "the adversary"; in this context, it appears at several points in the Old Testament. For instance, in the Book of Samuel, David is presented as the satan ("adversary") of the Philistines, while in the Book of Numbers the term appears as a verb, when God sent an angel to "satan" ("to oppose") Balaam. Prior to the composition of the New Testament, the idea developed within Jewish communities that Satan was the name of an angel who had rebelled against God and had been cast out of Heaven along with his followers; this account would be incorporated into contemporary texts like the Book of Enoch. This Satan was then featured in parts of the New Testament, where he was presented as a figure who tempted humans to commit sin; in the Book of Matthew and the Book of Luke, he attempted to tempt Jesus of Nazareth as the latter fasted in the wilderness.

The word "Satanism" was adopted into English from the French "satanisme". The terms "Satanism" and "Satanist" are first recorded as appearing in the English and French languages during the sixteenth century, when they were used by Christian groups to attack other, rival Christian groups. In a Roman Catholic tract of 1565, the author condemns the "heresies, blasphemies, and sathanismes [sic]" of the Protestants. In an Anglican work of 1559, Anabaptists and other Protestant sects are condemned as "swarmes of Satanistes [sic]". As used in this manner, the term "Satanism" was not used to claim that people literally worshipped Satan, but rather presented the view that through deviating from what the speaker or writer regarded as the true variant of Christianity, they were regarded as being essentially in league with the devil. During the nineteenth century, the term "Satanism" began to be used to describe those considered to lead a broadly immoral lifestyle, and it was only in the late nineteenth century that it came to be applied in English to individuals who were believed to consciously and deliberately venerate Satan. This latter meaning had appeared earlier in the Swedish language; the Lutheran Bishop Laurentius Paulinus Gothus had described devil-worshipping sorcerers as "Sathanister" in his "Ethica Christiana", produced between 1615 and 1630.

Historical and anthropological research suggests that nearly all societies have developed the idea of a sinister and anti-human force that can hide itself within society. This commonly involves a belief in witches, a group of individuals who invert the norms of their society and seek to harm their community, for instance by engaging in incest, murder, and cannibalism. Allegations of witchcraft may have different causes and serve different functions within a society. For instance, they may serve to uphold social norms, to heighten the tension in existing conflicts between individuals, or to scapegoat certain individuals for various social problems.

Another contributing factor to the idea of Satanism is the concept that there is an agent of misfortune and evil who operates on a cosmic scale, something usually associated with a strong form of ethical dualism that divides the world clearly into forces of good and forces of evil. The earliest such entity known is Angra Mainyu, a figure that appears in the Persian religion of Zoroastrianism. This concept was also embraced by Judaism and early Christianity, and although it was soon marginalized within Jewish thought, it gained increasing importance within early Christian understandings of the cosmos. While the early Christian idea of the Devil was not well developed, it gradually adapted and expanded through the creation of folklore, art, theological treatises, and morality tales, thus providing the character with a range of extra-Biblical associations.

As Christianity expanded throughout the Middle East, North Africa, and Europe, it came into contact with a variety of other religions, which it regarded as "pagan". Christian theologians claimed that the gods and goddesses venerated by these "pagans" were not genuine divinities, but were actually demons. However, they did not believe that "pagans" were deliberately devil-worshippers, instead claiming that they were simply misguided. In Christian iconography, the Devil and demons were given the physical traits of figures from Classical mythology such as the god Pan, fauns, and satyrs.

Those Christian groups regarded as heretics by the Roman Catholic Church were treated differently, with theologians arguing that they were deliberately worshipping the Devil. This was accompanied by claims that such individuals engaged in incestuous sexual orgies, murdered infants, and committed acts of cannibalism, all stock accusations that had previously been leveled at Christians themselves in the Roman Empire.
The first recorded example of such an accusation being made within Western Christianity took place in Toulouse in 1022, when two clerics were tried for allegedly venerating a demon. Throughout the Middle Ages, this accusation would be applied to a wide range of Christian heretical groups, including the Paulicians, Bogomils, Cathars, Waldensians, and the Hussites. The Knights Templar were accused of worshipping an idol known as Baphomet, with Lucifer having appeared at their meetings in the form of a cat. As well as these Christian groups, these claims were also made about Europe's Jewish community. In the thirteenth century, there were also references made to a group of "Luciferians" led by a woman named Lucardis which hoped to see Satan rule in Heaven. References to this group continued into the fourteenth century, although historians studying the allegations concur that these Luciferians were likely a fictitious invention.

Within Christian thought, the idea developed that certain individuals could make a pact with Satan. This may have emerged after observing that pacts with gods and goddesses played a role in various pre-Christian belief systems, or that such pacts were also made as part of the Christian cult of saints. Another possibility is that it derives from a misunderstanding of Augustine of Hippo's condemnation of augury in his "On the Christian Doctrine", written in the late fourth century. Here, he stated that people who consulted augurs were entering ""quasi pacts"" (covenants) with demons. The idea of the diabolical pact made with demons was popularized across Europe in the story of Faust, likely based in part on the real life Johann Georg Faust.

As the late medieval gave way to the early modern period, European Christendom experienced a schism between the established Roman Catholic Church and the breakaway Protestant movement. In the ensuing Reformation and Counter-Reformation, both Catholics and Protestants accused each other of deliberately being in league with Satan. It was in this context that the terms "Satanist" and "Satanism" emerged.

The early modern period also saw fear of Satanists reach its "historical apogee" in the form of the witch trials of the fifteenth to the eighteenth centuries. This came about as the accusations which had been leveled at medieval heretics, among them that of devil-worship, were applied to the pre-existing idea of the witch, or practitioner of malevolent magic. The idea of a conspiracy of Satanic witches was developed by educated elites, although the concept of malevolent witchcraft was a widespread part of popular belief and folkloric ideas about the night witch, the wild hunt, and the dance of the fairies were incorporated into it. The earliest trials took place in Northern Italy and France, before spreading it out to other areas of Europe and to Britain's North American colonies, being carried out by the legal authorities in both Catholic and Protestant regions.
Between 30,000 and 50,000 individuals were executed as accused Satanic witches.
Most historians agree that the majority of those persecuted in these witch trials were innocent of any involvement in Devil worship. However, in their summary of the evidence for the trials, the historians Geoffrey Scarre and John Callow thought it "without doubt" that some of those accused in the trials had been guilty of employing magic in an attempt to harm their enemies, and were thus genuinely guilty of witchcraft.

In seventeenth-century Sweden, a number of highway robbers and other outlaws living in the forests informed judges that they venerated Satan because he provided more practical assistance than God.
The historian of religion Massimo Introvigne regarded these practices as "folkloric Satanism".

During the eighteenth century, gentleman's social clubs became increasingly prominent in Britain and Ireland, among the most secretive of which were the Hellfire Clubs, which were first reported in the 1720s. The most famous of these groups was the Order of the Knights of Saints Francis, which was founded circa 1750 by the aristocrat Sir Francis Dashwood and which assembled first at his estate at West Wycombe and later in Medmenham Abbey. A number of contemporary press sources portrayed these as gatherings of atheist rakes where Christianity was mocked and toasts were made to the Devil. Beyond these sensationalist accounts, which may not be accurate portrayals of actual events, little is known about the activities of the Hellfire Clubs. Introvigne suggested that they may have engaged in a form of "playful Satanism" in which Satan was invoked "to show a daring contempt for conventional morality" by individuals who neither believed in his literal existence nor wanted to pay homage to him.

The French Revolution of 1789 dealt a blow to the hegemony of the Roman Catholic Church in parts of Europe, and soon a number of Catholic authors began making claims that it had been masterminded by a conspiratorial group of Satanists. Among the first to do so was French Catholic priest Jean-Baptiste Fiard, who publicly claimed that a wide range of individuals, from the Jacobins to tarot card readers, were part of a Satanic conspiracy. Fiard's ideas were furthered by Alexis-Vincent-Charles Berbiguier, who devoted a lengthy book to this conspiracy theory; he claimed that Satanists had supernatural powers allowing them to curse people and to shapeshift into both cats and fleas. Although most of his contemporaries regarded Berbiguier as mad, his ideas gained credence among many occultists, including Stanislas de Guaita, a Cabalist who used them for the basis of his book, "The Temple of Satan".

In the early 20th century, the British novelist Dennis Wheatley produced a range of influential novels in which his protagonists battled Satanic groups. At the same time, non-fiction authors like Montague Summers and Rollo Ahmed published books claiming that Satanic groups practicing black magic were still active across the world, although they provided no evidence that this was the case. During the 1950s, various British tabloid newspapers repeated such claims, largely basing their accounts on the allegations of one woman, Sarah Jackson, who claimed to have been a member of such a group. In 1973, the British Christian Doreen Irvine published "From Witchcraft to Christ", in which she claimed to have been a member of a Satanic group that gave her supernatural powers, such as the ability to levitate, before she escaped and embraced Christianity.
In the United States during the 1960s and 1970s, various Christian preachers—the most famous being Mike Warnke in his 1972 book "The Satan-Seller"—claimed that they had been members of Satanic groups who carried out sex rituals and animal sacrifices before discovering Christianity. According to Gareth Medway in his historical examination of Satanism, these stories were "a series of inventions by insecure people and hack writers, each one based on a previous story, exaggerated a little more each time".

Other publications made allegations of Satanism against historical figures. The 1970s saw the publication of the Romanian Protestant preacher Richard Wurmbrand's book in which he argued—without corroborating evidence—that the socio-political theorist Karl Marx had been a Satanist.

At the end of the twentieth century, a moral panic developed around claims regarding a Devil-worshipping cult that made use of sexual abuse, murder, and cannibalism in its rituals, with children being among its victims. Initially, the alleged perpetrators of such crimes were labeled "witches", although the term "Satanist" was soon adopted as a favored alternative, and the phenomenon itself came to be called "the Satanism Scare". Promoters of the claims alleged that there was a conspiracy of organized Satanists who occupied prominent positions throughout society, from the police to politicians, and that they had been powerful enough to cover up their crimes.

One of the primary sources for the scare was "Michelle Remembers", a 1980 book by the Canadian psychiatrist Lawrence Pazder in which he detailed what he claimed were the repressed memories of his patient (and wife) Michelle Smith. Smith had claimed that as a child she had been abused by her family in Satanic rituals in which babies were sacrificed and Satan himself appeared. In 1983, allegations were made that the McMartin family—owners of a preschool in California—were guilty of sexually abusing the children in their care during Satanic rituals. The allegations resulted in a lengthy and expensive trial, in which all of the accused would eventually be cleared. The publicity generated by the case resulted in similar allegations being made in various other parts of the United States.

A prominent aspect of the Satanic Scare was the claim by those in the developing "anti-Satanism" movement that any child's claim about Satanic ritual abuse must be true, because children would not lie. Although some involved in the anti-Satanism movement were from Jewish and secular backgrounds, a central part was played by fundamentalist and evangelical forms of Christianity, in particular Pentecostalism, with Christian groups holding conferences and producing books and videotapes to promote belief in the conspiracy. Various figures in law enforcement also came to be promoters of the conspiracy theory, with such "cult cops" holding various conferences to promote it. The scare was later imported to the United Kingdom through visiting evangelicals and became popular among some of the country's social workers, resulting in a range of accusations and trials across Britain.

The Satanic ritual abuse hysteria died down between 1990 and 1994. In the late 1980s, the Satanic Scare had lost its impetus following increasing skepticism about such allegations, and a number of those who had been convicted of perpetrating Satanic ritual abuse saw their convictions overturned.
In 1990, an agent of the U.S. Federal Bureau of Investigation, Ken Lanning, revealed that he had investigated 300 allegations of Satanic ritual abuse and found no evidence for Satanism or ritualistic activity in any of them. In the UK, the Department of Health commissioned the anthropologist Jean La Fontaine to examine the allegations of SRA. She noted that while approximately half did reveal evidence of genuine sexual abuse of children, none revealed any evidence that Satanist groups had been involved or that any murders had taken place. She noted three examples in which lone individuals engaged in child molestation had created a ritual performance to facilitate their sexual acts, with the intent of frightening their victims and justifying their actions, but that none of these child molesters were involved in wider Satanist groups. By the 21st century, hysteria about Satanism has waned in most Western countries, although allegations of Satanic ritual abuse continued to surface in parts of continental Europe and Latin America.

From the late seventeenth through to the nineteenth century, the character of Satan was increasingly rendered unimportant in Western philosophy and ignored in Christian theology, while in folklore he came to be seen as a foolish rather than a menacing figure. The development of new values in the Age of Enlightenment—in particular those of reason and individualism—contributed to a shift in how many Europeans viewed Satan. In this context, a number of individuals took Satan out of the traditional Christian narrative and reread and reinterpreted him in light of their own time and their own interests, in turn generating new and different portraits of Satan.

The shifting view of Satan owes many of its origins to John Milton's epic poem "Paradise Lost" (1667), in which Satan features as the protagonist. Milton was a Puritan and had never intended for his depiction of Satan to be a sympathetic one. However, in portraying Satan as a victim of his own pride who rebeled against God he humanized him and also allowed him to be interpreted as a rebel against tyranny. This was how Milton's Satan was understood by later readers like the publisher Joseph Johnson, and the anarchist philosopher William Godwin, who reflected it in his 1793 book "Enquiry Concerning Political Justice". "Paradise Lost" gained a wide readership in the eighteenth century, both in Britain and in continental Europe, where it had been translated into French by Voltaire. Milton thus became "a central character in rewriting Satanism" and would be viewed by many later religious Satanists as a ""de facto" Satanist".

The nineteenth century saw the emergence of what has been termed "literary Satanism" or "romantic Satanism". According to Van Luijk, this cannot be seen as a "coherent movement with a single voice, but rather as a "post factum" identified group of sometimes widely divergent authors among whom a similar theme is found". For the literary Satanists, Satan was depicted as a benevolent and sometimes heroic figure, with these more sympathetic portrayals proliferating in the art and poetry of many romanticist and decadent figures. For these individuals, Satanism was not a religious belief or ritual activity, but rather a "strategic use of a symbol and a character as part of artistic and political expression".

Among the romanticist poets to adopt this view of Satan was the English poet Percy Bysshe Shelley, who had been influenced by Milton. In his poem "Laon and Cythna", Shelley praised the "Serpent", a reference to Satan, as a force for good in the universe.
Another was Shelley's fellow British poet Lord Byron, who included Satanic themes in his 1821 play "Cain", which was a dramatization of the Biblical story of Cain and Abel. These more positive portrayals also developed in France; one example was the 1823 work "Eloa" by Alfred de Vigny. Satan was also adopted by the French poet Victor Hugo, who made the character's fall from Heaven a central aspect of his "La Fin de Satan", in which he outlined his own cosmogony.
Although the likes of Shelley and Byron promoted a positive image of Satan in their work, there is no evidence that any of them performed religious rites to venerate him, and thus it is problematic to regard them as religious Satanists.

Radical left-wing political ideas had been spread by the American Revolution of 1765–83 and the French Revolution of 1789–99, and the figure of Satan, who was interpreted as having rebelled against the tyranny imposed by God, was an appealing one for many of the radical leftists of the period. For them, Satan was "a symbol for the struggle against tyranny, injustice, and oppression... a mythical figure of rebellion for an age of revolutions, a larger-than-life individual for an age of individualism, a free thinker in an age struggling for free thought". The French anarchist Pierre-Joseph Proudhon, who was a staunch critic of Christianity, embraced Satan as a symbol of liberty in several of his writings. Another prominent 19th century anarchist, the Russian Mikhail Bakunin, similarly described the figure of Satan as "the eternal rebel, the first freethinker and the emancipator of worlds" in his book "God and the State". These ideas likely inspired the American feminist activist Moses Harman to name his anarchist periodical "Lucifer the Lightbearer". The idea of this "Leftist Satan" declined during the twentieth century, although it was used on occasion by authorities within the Soviet Union, who portrayed Satan as a symbol of freedom and equality.

During the 1960s and 1970s, several rock bands—namely the American Coven and the British Black Widow—employed the imagery of Satanism and witchcraft in their work. References to Satan also appeared in the work of those rock bands which were pioneering the heavy metal genre in Britain during the 1970s. Black Sabbath for instance made mention of Satan in their lyrics, although several of the band's members were practicing Christians and other lyrics affirmed the power of the Christian God over Satan. In the 1980s, greater use of Satanic imagery was made by heavy metal bands like Slayer, Kreator, Sodom, and Destruction. Bands active in the subgenre of death metal—among them Deicide, Morbid Angel, and Entombed—also adopted Satanic imagery, combining it with other morbid and dark imagery, such as that of zombies and serial killers.

Satanism would come to be more closely associated with the subgenre of black metal, in which it was foregrounded over the other themes that had been used in death metal. A number of black metal performers incorporated self-injury into their act, framing this as a manifestation of Satanic devotion. The first black metal band, Venom, proclaimed themselves to be Satanists, although this was more an act of provocation than an expression of genuine devotion to the Devil. Satanic themes were also used by the black metal bands Bathory and Hellhammer. However, the first black metal act to more seriously adopt Satanism was Mercyful Fate, whose vocalist, King Diamond, joined the Church of Satan. More often than not musicians associating themselves with black metal say they do not believe in legitimate Satanic ideology and often profess to being atheists, agnostics, or religious skeptics.

In contrast to King Diamond, various black metal Satanists sought to distance themselves from LaVeyan Satanism, for instance by referring to their beliefs as "devil worship". These individuals regarded Satan as a literal entity, and in contrast to LaVey's views, they associated Satanism with criminality, suicide, and terror. For them, Christianity was regarded as a plague which required eradication. Many of these individuals—such as Varg Vikernes and Euronymous—were Norwegian, and influenced by the strong anti-Christian views of this milieu, between 1992 and 1996 around fifty Norwegian churches were destroyed in arson attacks. Within the black metal scene, a number of musicians later replaced Satanic themes with those deriving from Heathenry, a form of modern Paganism.

Religious Satanism does not exist in a single form, as there are multiple different religious Satanisms, each with different ideas about what being a Satanist entails. The historian of religion Ruben van Luijk used a "working definition" in which Satanism was regarded as "the intentional, religiously motivated veneration of Satan".

Dyrendal, Lewis, and Petersen believed that it was not a single movement, but rather a milieu. They and others have nevertheless referred to it as a new religious movement. They believed that there was a family resemblance that united all of the varying groups in this milieu, and that most of them were self religions. They argued that there were a set of features that were common to the groups in this Satanic milieu: these were the positive use of the term "Satanist" as a designation, an emphasis on individualism, a genealogy that connects them to other Satanic groups, a transgressive and antinomian stance, a self-perception as an elite, and an embrace of values such as pride, self-reliance, and productive non-conformity.

Dyrendal, Lewis, and Petersen argued that the groups within the Satanic milieu could be divided into three groups: reactive Satanists, rationalist Satanists, and esoteric Satanists. They saw reactive Satanism as encompassing "popular Satanism, inverted Christianity, and symbolic rebellion" and noted that it situates itself in opposition to society while at the same time conforming to society's perspective of evil. Rationalist Satanism is used to describe the trend in the Satanic milieu which is atheistic, skeptical, materialistic, and epicurean. Esoteric Satanism instead applied to those forms which are theistic and draw upon ideas from other forms of Western esotericism, Modern Paganism, Buddhism, and Hinduism.

The first person to promote a Satanic philosophy was the Pole Stanislaw Przybyszewski, who promoted a Social Darwinian ideology.

The use of the term "Lucifer" was also taken up by the French ceremonial magician Eliphas Levi, who has been described as a "Romantic Satanist". During his younger days, Levi used "Lucifer" in much the same manner as the literary romantics. As he moved toward a more politically conservative outlook in later life, he retained the use of the term, but instead applied it as to what he believed was a morally neutral facet of the Absolute.

Levi was not the only occultist who wanted to use the term "Lucifer" without adopting the term "Satan" in a similar way. The early Theosophical Society held to the view that "Lucifer" was a force that aided humanity's awakening to its own spiritual nature. In keeping with this view, the Society began production of a journal titled "Lucifer".

"Satan" was also used within the esoteric system propounded by the Danish occultist Carl William Hansen, who used the pen name "Ben Kadosh". Hansen was involved in a variety of esoteric groups, including Martinism, Freemasonry, and the Ordo Templi Orientis, drawing on ideas from various groups to establish his own philosophy. In one pamphlet, he provided a "Luciferian" interpretation of Freemasonry. Kadosh's work left little influence outside of Denmark.

Both during his life and after it, the British occultist Aleister Crowley has been widely described as a Satanist, usually by detractors. Crowley stated he did not consider himself a Satanist, nor did he worship Satan, as he did not accept the Christian world view in which Satan was believed to exist. He nevertheless used imagery considered satanic, for instance by describing himself as "the Beast 666" and referring to the Whore of Babylon in his work, while in later life he sent "Antichristmas cards" to his friends. Dyrendel, Lewis, and Petersen noted that despite the fact that Crowley was not a Satanist, he "in many ways embodies the pre-Satanist esoteric discourse on Satan and Satanism through his lifestyle and his philosophy", with his "image and thought" becoming an "important influence" on the later development of religious Satanism.

In 1928, the Fraternitas Saturni (FS) was established in Germany; its founder, Eugen Grosche, published "Satanische Magie" ("Satanic Magic") that same year. The group connected Satan to Saturn, claiming that the planet related to the Sun in the same manner that Lucifer relates to the human world.

In 1932, an esoteric group known as the Brotherhood of the Golden Arrow was established in Paris, France by Maria de Naglowska, a Russian occultist who had fled to France following the Russian Revolution. She promoted a theology centered on what she called the Third Term of the Trinity consisting of Father, Son, and Sex, the latter of which she deemed to be most important. Her early disciples, who underwent what she called "Satanic Initiations", included models and art students recruited from bohemian circles. The Golden Arrow disbanded after Naglowska abandoned it in 1936. According to Introvigne, hers was "a quite complicated Satanism, built on a complex philosophical vision of the world, of which little would survive its initiator".

In 1969, a Satanic group based in Toledo, Ohio, part of the United States, came to public attention. Called the Our Lady of Endor Coven, it was led by a man named Herbert Sloane, who described his Satanic tradition as the Ophite Cultus Sathanas and alleged that it had been established in the 1940s. The group offered a Gnostic interpretation of the world in which the creator God was regarded as evil and the Biblical Serpent presented as a force for good who had delivered salvation to humanity in the Garden of Eden. Sloane's claims that his group had a 1940s origin remain unproven; it may be that he falsely claimed older origins for his group to make it appear older than Anton LaVey's Church of Satan, which had been established in 1966.

None of these groups had any real impact on the emergence of the later Satanic milieu in the 1960s.

Anton LaVey, who has been referred to as "The Father of Satanism", synthesized his religion through the establishment of the Church of Satan in 1966 and the publication of "The Satanic Bible" in 1969. LaVey's teachings promoted "indulgence", "vital existence", "undefiled wisdom", "kindness to those who deserve it", "responsibility to the responsible" and an "eye for an eye" code of ethics, while shunning "abstinence" based on guilt, "spirituality", "unconditional love", "pacifism", "equality", "herd mentality" and "scapegoating". In LaVey's view, the Satanist is a carnal, physical and pragmatic being—and enjoyment of physical existence and an undiluted view of this-worldly truth are promoted as the core values of Satanism, propagating a naturalistic worldview that sees mankind as animals existing in an amoral universe.

LaVey believed that the ideal Satanist should be individualistic and non-conformist, rejecting what he called the "colorless existence" that mainstream society sought to impose on those living within it. He praised the human ego for encouraging an individual's pride, self-respect, and self-realization and accordingly believed in satisfying the ego's desires. He expressed the view that self-indulgence was a desirable trait, and that hate and aggression were not wrong or undesirable emotions but that they were necessary and advantageous for survival. Accordingly, he praised the seven deadly sins as virtues which were beneficial for the individual. The anthropologist Jean La Fontaine highlighted an article that appeared in "The Black Flame", in which one writer described "a true Satanic society" as one in which the population consists of "free-spirited, well-armed, fully-conscious, self-disciplined individuals, who will neither need nor tolerate any external entity 'protecting' them or telling them what they can and cannot do."

The sociologist James R. Lewis noted that "LaVey was directly responsible for the genesis of Satanism as a serious religious (as opposed to a purely literary) movement". Scholars agree that there is no reliably documented case of Satanic continuity prior to the founding of the Church of Satan. It was the first organized church in modern times to be devoted to the figure of Satan, and according to Faxneld and Petersen, the Church represented "the first public, highly visible, and long-lasting organization which propounded a coherent satanic discourse". LaVey's book, "The Satanic Bible", has been described as the most important document to influence contemporary Satanism. The book contains the core principles of Satanism, and is considered the foundation of its philosophy and dogma. Petersen noted that it is "in many ways "the" central text of the Satanic milieu", with Lap similarly testifying to its dominant position within the wider Satanic movement. David G. Bromley calls it "iconoclastic" and "the best-known and most influential statement of Satanic theology." Eugene V. Gallagher says that Satanists use LaVey's writings "as lenses through which they view themselves, their group, and the cosmos." He also states: "With a clear-eyed appreciation of true human nature, a love of ritual and pageantry, and a flair for mockery, LaVey's "Satanic Bible" promulgated a gospel of self-indulgence that, he argued, anyone who dispassionately considered the facts would embrace."

A number of religious studies scholars have described LaVey's Satanism as a form of "self-religion" or "self-spirituality", with religious studies scholar Amina Olander Lap arguing that it should be seen as being both part of the "prosperity wing" of the self-spirituality New Age movement and a form of the Human Potential Movement. The anthropologist Jean La Fontaine described it as having "both elitist and anarchist elements", also citing one occult bookshop owner who referred to the Church's approach as "anarchistic hedonism". In "The Invention of Satanism", Dyrendal and Petersen theorized that LaVey viewed his religion as "an antinomian self-religion for productive misfits, with a cynically carnivalesque take on life, and no supernaturalism". The sociologist of religion James R. Lewis even described LaVeyan Satanism as "a blend of Epicureanism and Ayn Rand's philosophy, flavored with a pinch of ritual magic." The historian of religion Mattias Gardell described LaVey's as "a rational ideology of egoistic hedonism and self-preservation", while Nevill Drury characterized LaVeyan Satanism as "a religion of self-indulgence". It has also been described as an "institutionalism of Machiavellian self-interest".

Prominent Church leader Blanche Barton described Satanism as "an alignment, a lifestyle". LaVey and the Church espoused the view that "Satanists are born, not made"; that they are outsiders by their nature, living as they see fit, who are self-realized in a religion which appeals to the would-be Satanist's nature, leading them to realize they are Satanists through finding a belief system that is in line with their own perspective and lifestyle. Adherents to the philosophy have described Satanism as a non-spiritual religion of the flesh, or "...the world's first religion". LaVey used Christianity as a negative mirror for his new faith, with LaVeyan Satanism rejecting the basic principles and theology of Christian belief. It views Christianity – alongside other major religions, and philosophies such as humanism and liberal democracy – as a largely negative force on humanity; LaVeyan Satanists perceive Christianity as a lie which promotes idealism, self-denigration, herd behavior, and irrationality. LaVeyans view their religion as a force for redressing this balance by encouraging materialism, egoism, stratification, carnality, atheism, and social Darwinism. LaVey's Satanism was particularly critical of what it understands as Christianity's denial of humanity's animal nature, and it instead calls for the celebration of, and indulgence in, these desires. In doing so, it places an emphasis on the carnal rather than the spiritual.

Practitioners do not believe that Satan literally exists and do not worship him. Instead, Satan is viewed as a positive archetype embracing the Hebrew root of the word "Satan" as "adversary", who represents pride, carnality, and enlightenment, and of a cosmos which Satanists perceive to be motivated by a "dark evolutionary force of entropy that permeates all of nature and provides the drive for survival and propagation inherent in all living things". The Devil is embraced as a symbol of defiance against the Abrahamic faiths which LaVey criticized for what he saw as the suppression of humanity's natural instincts. Moreover, Satan also serves as a metaphorical external projection of the individual's godhood. LaVey espoused the view that "god" is a creation of man, rather than man being a creation of "god". In his book, "The Satanic Bible", the Satanist's view of god is described as the Satanist's true "self"—a projection of his or her own personality—not an external deity. Satan is used as a representation of personal liberty and individualism.

LaVey explained that the gods worshipped by other religions are also projections of man's true self. He argues that man's unwillingness to accept his own ego has caused him to externalize these gods so as to avoid the feeling of narcissism that would accompany self-worship. The current High Priest of the Church of Satan, Peter H. Gilmore, further expounds that "...Satan is a symbol of Man living as his prideful, carnal nature dictates [...] Satan is not a conscious entity to be worshipped, rather a reservoir of power inside each human to be tapped at will. The Church of Satan has chosen Satan as its primary symbol because in Hebrew it means adversary, opposer, one to accuse or question. We see ourselves as being these Satans; the adversaries, opposers and accusers of all spiritual belief systems that would try to hamper enjoyment of our life as a human being." The term "Theistic Satanism" has been described as "oxymoronic" by the church and its High Priest. The Church of Satan rejects the legitimacy of any other organizations who claim to be Satanists, dubbing them reverse-Christians, pseudo-Satanists or Devil worshipers, atheistic or otherwise, and maintains a purist approach to Satanism as expounded by LaVey.

After LaVey's death in 1997, the Church of Satan was taken over by a new administration and its headquarters were moved to New York. LaVey's daughter, the High Priestess Karla LaVey, felt this to be a disservice to her father's legacy. The First Satanic Church was re-founded on October 31, 1999 by Karla LaVey to carry on the legacy of her father. She continues to run it out of San Francisco, California.

The Satanic Temple is an American religious and political activist organization based in Salem, Massachusetts. The organization actively participates in public affairs that have manifested in several public political actions and efforts at lobbying, with a focus on the separation of church and state and using satire against Christian groups that it believes interfere with personal freedom. According to Dyrendal, Lewis, and Petersen, the group were "rationalist, political pranksters". Their pranks are designed to highlight religious hypocrisy and advance the cause of secularism. In one of their actions, they performed a "Pink Mass" over the grave of the mother of the evangelical Christian and prominent anti-LGBT preacher Fred Phelps; the Temple claimed that the mass converted the spirit of Phelps' mother into a lesbian.

The Satanic Temple does not believe in a supernatural Satan, as they believe that this encourages superstition that would keep them from being "malleable to the best current scientific understandings of the material world". The Temple uses the literary Satan as metaphor to construct a cultural narrative which promotes pragmatic skepticism, rational reciprocity, personal autonomy, and curiosity. Satan is thus used as a symbol representing "the eternal rebel" against arbitrary authority and social norms.

Theistic Satanism (also known as traditional Satanism, Spiritual Satanism or Devil worship) is a form of Satanism with the primary belief that Satan is an actual deity or force to revere or worship. Other characteristics of theistic Satanism may include a belief in magic, which is manipulated through ritual, although that is not a defining criterion, and theistic Satanists may focus solely on devotion.

Luciferianism can be understood best as a belief system or intellectual creed that venerates the essential and inherent characteristics that are affixed and commonly given to Lucifer. Luciferianism is often identified as an auxiliary creed or movement of Satanism, due to the common identification of Lucifer with Satan. Some Luciferians accept this identification and/or consider Lucifer as the "light bearer" and illuminated aspect of Satan, giving them the name of Satanists and the right to bear the title. Others reject it, giving the argument that Lucifer is a more positive and easy-going ideal than Satan. They are inspired by the ancient myths of Egypt, Rome and Greece, Gnosticism and traditional Western occultism.

According to the group's own claims, the Order of Nine Angles was established in Shropshire, Western England during the late 1960s, when a Grand Mistress united a number of ancient pagan groups active in the area.
This account states that when the Order's Grand Mistress migrated to Australia, a man known as "Anton Long" took over as the new Grand Master. From 1976 onward he authored an array of texts for the tradition, codifying and extending its teachings, mythos, and structure.
Various academics have argued that Long is the pseudonym of British neo-Nazi activist David Myatt, an allegation that Myatt has denied.
The ONA arose to public attention in the early 1980s, spreading its message through magazine articles over the following two decades. In 2000, it established a presence on the internet, later adopting social media to promote its message.

The ONA is a secretive organization, and lacks any central administration, instead operating as a network of allied Satanic practitioners, which it terms the "kollective". It consists largely of autonomous cells known as "nexions". The majority of these are located in Britain, Ireland, and Germany, although others are located elsewhere in Europe, and in Russia, Egypt, South Africa, Brazil, Australia, and the United States.

The ONA describe their occultism as "Traditional Satanism". The ONA's writings encourage human sacrifice, referring to their victims as "opfers". According to the Order's teachings, such opfers must demonstrate character faults that mark them out as being worthy of death, and accordingly the ONA insists that children must never be victims. No ONA cell have admitted to carrying out a sacrifice in a ritualized manner, but rather Order members have joined the police and military in order to carry out such killings. Faxneld described the Order as "a dangerous and extreme form of Satanism", while religious studies scholar Graham Harvey claimed that the ONA fit the stereotype of the Satanist "better than other groups" by embracing "deeply shocking" and illegal acts.

The Temple of Set is an initiatory occult society claiming to be the world's leading left-hand path religious organization. It was established in 1975 by Michael A. Aquino and certain members of the priesthood of the Church of Satan, who left because of administrative and philosophical disagreements. ToS deliberately self-differentiates from CoS in several ways, most significantly in theology and sociology. The philosophy of the Temple of Set may be summed up as "enlightened individualism"—enhancement and improvement of oneself by personal education, experiment and initiation. This process is necessarily different and distinctive for each individual. The members do not agree on whether Set is "real" or not, and they're not expected to.

The Temple presents the view that the name "Satan" was originally a corruption of the name "Set". The Temple teaches that Set is a real entity, the only real god in existence, with all others created by the human imagination. Set is described as having given humanity—through the means of non-natural evolution—the "Black Flame" or the "Gift of Set", a questioning intellect which sets the species apart from other animals. While Setians are expected to revere Set, they do not worship him. Central to Setian philosophy is the human individual, with self-deification presented as the ultimate goal.

In 2005 Petersen noted that academic estimates for the Temple's membership varied from between 300 and 500, and Granholm suggested that in 2007 the Temple contained circa 200 members.

Dyrendal, Lewis, and Petersen used the term "reactive Satanism" to describe one form of modern religious Satanism. They described this as an adolescent and anti-social means of rebelling in a Christian society, by which an individual transgresses cultural boundaries. They believed that there were two tendencies within reactive Satanism: one, "Satanic tourism", was characterized by the brief period of time in which an individual was involved, while the other, the "Satanic quest", was typified by a longer and deeper involvement.

The researcher Gareth Medway noted that in 1995 he encountered a British woman who stated that she had been a practicing Satanist during her teenage years. She had grown up in a small mining village, and had come to believe that she had psychic powers. After hearing about Satanism in some library books, she declared herself a Satanist and formulated a belief that Satan was the true god. After her teenage years she abandoned Satanism and became a chaos magickian.

Some reactive Satanists are teenagers or mentally disturbed individuals who have engaged in criminal activities. During the 1980s and 1990s, several groups of teenagers were apprehended after sacrificing animals and vandalizing both churches and graveyards with Satanic imagery. Introvigne expressed the view that these incidents were "more a product of juvenile deviance and marginalization than Satanism". In a few cases the crimes of these reactive Satanists have included murder. In 1970, two separate groups of teenagers—one led by Stanley Baker in Big Sur and the other by Steven Hurd in Los Angeles—killed a total of three people and consumed parts of their corpses in what they later claimed were sacrifices devoted to Satan. In 1984, a U.S. group called the Knights of the Black Circle killed one of its own members, Gary Lauwers, over a disagreement regarding the group's illegal drug dealing; group members later related that Lauwers' death was a sacrifice to Satan.
The American serial killer Richard Ramirez for instance claimed that he was a Satanist; during his 1980s killing spree he left an inverted pentagram at the scene of each murder and at his trial called out "Hail Satan!"

Dyrendal, Lewis, and Petersen observed that from surveys of Satanists conducted in the early 21st century, it was clear that the Satanic milieu was "heavily dominated by young males". They nevertheless noted that census data from New Zealand suggested that there may be a growing proportion of women becoming Satanists. In comprising more men than women, Satanism differs from most other religious communities, including most new religious communities. Most Satanists came to their religion through reading, either online or books, rather than through being introduced to it through personal contacts. Many practitioners do not claim that they converted to Satanism, but rather state that they were born that way, and only later in life confirmed that Satanism served as an appropriate label for their pre-existing worldviews. Others have stated that they had experiences with supernatural phenomena that led them to embracing Satanism. A number reported feelings of anger in respect of a set of practicing Christians and expressed the view that the monotheistic Gods of Christianity and other religions are unethical, citing issues such as the problem of evil. For some practitioners, Satanism gave a sense of hope, even for those who had been physically and sexually abused.

The surveys revealed that atheistic Satanists appeared to be in the majority, although the numbers of theistic Satanists appeared to grow over time. Beliefs in the afterlife varied, although the most popular afterlife views were reincarnation and the idea that consciousness survives bodily death. The surveys also demonstrated that most recorded Satanists practiced magic, although there were differing opinions as to whether magical acts operated according to etheric laws or whether the effect of magic was purely psychological. A number described performing cursing, in most cases as a form of vigilante justice.
Most practitioners conduct their religious observances in a solitary manner, and never or rarely meet fellow Satanists for rituals. Rather, the primary interaction that takes place between Satanists is online, on websites or via email.
From their survey data, Dyrendal, Lewis, and Petersen noted that the average length of involvement in the Satanic milieu was seven years. A Satanist's involvement in the movement tends to peak in the early twenties and drops off sharply in their thirties. A small proportion retain their allegiance to the religion into their elder years. When asked about their political views, the largest proportion of Satanists identified as apolitical or non-aligned, while only a small percentage identified as conservative despite the conservative views of prominent Satanists like LaVey and Marilyn Manson. A small minority of Satanists expressed support for the far right; conversely, over two-thirds expressed negative or extremely negative views about Nazism and neo-Nazism.

Satanism is not illegal by itself, but there are criminal religious movements which say they are Satanism. What is illegal therein is not worshiping Lucifer/Satan, but real, objective felonies.

In 2004, it was claimed that Satanism was allowed in the Royal Navy of the British Armed Forces, despite opposition from Christians. In 2016, under a Freedom of Information request, the Navy Command Headquarters stated that "we do not recognise satanism as a formal religion, and will not grant facilities or make specific time available for individual 'worship'."

In 2005, the Supreme Court of the United States debated in the case of Cutter v. Wilkinson over protecting minority religious rights of prison inmates after a lawsuit challenging the issue was filed to them. The court ruled that facilities that accept federal funds cannot deny prisoners accommodations that are necessary to engage in activities for the practice of their own religious beliefs.





</doc>
<doc id="27707" url="https://en.wikipedia.org/wiki?curid=27707" title="Socialist law">
Socialist law

Socialist law or Soviet law denotes a general type of legal system which has been (and continues to be) used in socialist and formerly socialist states. It is based on the civil law system, with major modifications and additions from Marxist-Leninist ideology. There is controversy as to whether socialist law ever constituted a separate legal system or not. If so, prior to the end of the Cold War, "socialist law" would be ranked among the major legal systems of the world.

While civil law systems have traditionally put great pains in defining the notion of private property, how it may be acquired, transferred, or lost, socialist law systems provide for most property to be owned by the Government or by agricultural co-operatives, and having special courts and laws for state enterprises.

Many scholars argue that socialist law was not a separate legal classification. Although the command economy approach of the communist states meant that most types of property could not be owned, the Soviet Union always had a civil code, courts that interpreted this civil code, and a civil law approach to legal reasoning (thus, both legal process and legal reasoning were largely analogous to the French or German civil code system). Legal systems in all socialist states preserved formal criteria of the Romano-Germanic civil law; for this reason, law theorists in post-socialist states usually consider the socialist law as a particular case of the Romano-Germanic civil law. Cases of development of common law into socialist law are unknown because of incompatibility of basic principles of these two systems (common law presumes influential rule-making role of courts while courts in socialist states play a dependent role).

Recent work, however, suggests that socialist law -- at least from the perspective of public law and constitutional design -- is a useful category. William Partlett and Eric Ip (in the NYU Journal of International Law and Policy) argue that socialist law helps to understand the "Russo-Leninist transplants" that currently operate in China's socialist law system. This helps to understand the "distinctive public law institutions and approaches in China that have been ignored by many scholars." 

Soviet law displayed many special characteristics that derived from the socialist nature of the Soviet state and reflected Marxist-Leninist ideology. Vladimir Lenin accepted the Marxist conception of the law and the state as instruments of coercion in the hands of the bourgeoisie and postulated the creation of popular, informal tribunals to administer revolutionary justice. One of the main theoreticians of Soviet socialist legality in this early phase was Pēteris Stučka.

Alongside this utopian trend was one more critical of the concept of "proletarian justice", represented by Evgeny Pashukanis. A dictatorial trend developed that advocated the use of law and legal institutions to suppress all opposition to the regime. This trend reached its zenith under Joseph Stalin with the ascendancy of Andrey Vyshinsky, when the administration of justice was carried out mainly by the security police in special tribunals.

During the de-Stalinization of the Nikita Khrushchev era, a new trend developed, based on socialist legality, that stressed the need to protect the procedural and statutory rights of citizens, while still calling for obedience to the state. New legal codes, introduced in 1960, were part of the effort to establish legal norms in administering laws. Although socialist legality remained in force after 1960, the dictatorial and utopian trends continued to influence the legal process. Persecution of political and religious dissenters continued, but at the same time there was a tendency to decriminalize lesser offenses by handing them over to people's courts and administrative agencies and dealing with them by education rather than by incarceration. By late 1986, the Mikhail Gorbachev era was stressing anew the importance of individual rights in relation to the state and criticizing those who violated procedural law in implementing Soviet justice. This signaled a resurgence of socialist legality as the dominant trend. Socialist legality itself still lacked features associated with Western jurisprudence.

Socialist law is similar to the civil law but with a greatly increased public law sector and decreased private law sector.


A specific institution characteristic to Socialist law was the so-called burlaw court (or, verbally, "court of comrades", Russian товарищеский суд) which decided on minor offences.




</doc>
<doc id="27709" url="https://en.wikipedia.org/wiki?curid=27709" title="Semiconductor">
Semiconductor

A semiconductor material has an electrical conductivity value falling between that of a conductor, such as metallic copper, and an insulator, such as glass. Its resistivity falls as its temperature rises; metals are the opposite. Its conducting properties may be altered in useful ways by introducing impurities ("doping") into the crystal structure. When two differently-doped regions exist in the same crystal, a semiconductor junction is created. The behavior of charge carriers, which include electrons, ions and electron holes, at these junctions is the basis of diodes, transistors and all modern electronics. Some examples of semiconductors are silicon, germanium, gallium arsenide, and elements near the so-called "metalloid staircase" on the periodic table. After silicon, gallium arsenide is the second most common semiconductor and is used in laser diodes, solar cells, microwave-frequency integrated circuits and others. Silicon is a critical element for fabricating most electronic circuits.

Semiconductor devices can display a range of useful properties, such as passing current more easily in one direction than the other, showing variable resistance, and sensitivity to light or heat. Because the electrical properties of a semiconductor material can be modified by doping, or by the application of electrical fields or light, devices made from semiconductors can be used for amplification, switching, and energy conversion.

The conductivity of silicon is increased by adding a small amount (of the order of 1 in 10) of pentavalent (antimony, phosphorus, or arsenic) or trivalent (boron, gallium, indium) atoms. This process is known as doping and resulting semiconductors are known as doped or extrinsic semiconductors. Apart from doping, the conductivity of a semiconductor can equally be improved by increasing its temperature. This is contrary to the behaviour of a metal in which conductivity decreases with increase in temperature.

The modern understanding of the properties of a semiconductor relies on quantum physics to explain the movement of charge carriers in a crystal lattice. Doping greatly increases the number of charge carriers within the crystal. When a doped semiconductor contains mostly free holes it is called "p-type", and when it contains mostly free electrons it is known as "n-type". The semiconductor materials used in electronic devices are doped under precise conditions to control the concentration and regions of p- and n-type dopants. A single semiconductor crystal can have many p- and n-type regions; the p–n junctions between these regions are responsible for the useful electronic behavior.

Some of the properties of semiconductor materials were observed throughout the mid 19th and first decades of the 20th century. The first practical application of semiconductors in electronics was the 1904 development of the cat's-whisker detector, a primitive semiconductor diode used in early radio receivers. Developments in quantum physics in turn led to the development of the transistor in 1947, the integrated circuit in 1958, and the MOSFET (metal–oxide–semiconductor field-effect transistor) in 1959.



A large number of elements and compounds have semiconducting properties, including:

Most common semiconducting materials are crystalline solids, but amorphous and liquid semiconductors are also known. These include hydrogenated amorphous silicon and mixtures of arsenic, selenium and tellurium in a variety of proportions. These compounds share with better known semiconductors the properties of intermediate conductivity and a rapid variation of conductivity with temperature, as well as occasional negative resistance. Such disordered materials lack the rigid crystalline structure of conventional semiconductors such as silicon. They are generally used in thin film structures, which do not require material of higher electronic quality, being relatively insensitive to impurities and radiation damage.

Almost all of today's electronic technology involves the use of semiconductors, with the most important aspect being the integrated circuit (IC), which are found in laptops, scanners, cell-phones, etc. Semiconductors for ICs are mass-produced. To create an ideal semiconducting material, chemical purity is paramount. Any small imperfection can have a drastic effect on how the semiconducting material behaves due to the scale at which the materials are used.

A high degree of crystalline perfection is also required, since faults in crystal structure (such as dislocations, twins, and stacking faults) interfere with the semiconducting properties of the material. Crystalline faults are a major cause of defective semiconductor devices. The larger the crystal, the more difficult it is to achieve the necessary perfection. Current mass production processes use crystal ingots between in diameter which are grown as cylinders and sliced into wafers.

There is a combination of processes that is used to prepare semiconducting materials for ICs. One process is called thermal oxidation, which forms silicon dioxide on the surface of the silicon. This is used as a gate insulator and field oxide. Other processes are called photomasks and photolithography. This process is what creates the patterns on the circuity in the integrated circuit. Ultraviolet light is used along with a photoresist layer to create a chemical change that generates the patterns for the circuit.

Etching is the next process that is required. The part of the silicon that was not covered by the photoresist layer from the previous step can now be etched. The main process typically used today is called plasma etching. Plasma etching usually involves an etch gas pumped in a low-pressure chamber to create plasma. A common etch gas is chlorofluorocarbon, or more commonly known Freon. A high radio-frequency voltage between the cathode and anode is what creates the plasma in the chamber. The silicon wafer is located on the cathode, which causes it to be hit by the positively charged ions that are released from the plasma. The end result is silicon that is etched anisotropically.

The last process is called diffusion. This is the process that gives the semiconducting material its desired semiconducting properties. It is also known as doping. The process introduces an impure atom to the system, which creates the p-n junction. In order to get the impure atoms embedded in the silicon wafer, the wafer is first put in a 1,100 degree Celsius chamber. The atoms are injected in and eventually diffuse with the silicon. After the process is completed and the silicon has reached room temperature, the doping process is done and the semiconducting material is ready to be used in an integrated circuit.

Semiconductors are defined by their unique electric conductive behavior, somewhere between that of a conductor and an insulator.
The differences between these materials can be understood in terms of the quantum states for electrons, each of which may contain zero or one electron (by the Pauli exclusion principle). These states are associated with the electronic band structure of the material.
Electrical conductivity arises due to the presence of electrons in states that are delocalized (extending through the material), however in order to transport electrons a state must be "partially filled", containing an electron only part of the time. If the state is always occupied with an electron, then it is inert, blocking the passage of other electrons via that state.
The energies of these quantum states are critical, since a state is partially filled only if its energy is near the Fermi level (see Fermi–Dirac statistics).

High conductivity in a material comes from it having many partially filled states and much state delocalization.
Metals are good electrical conductors and have many partially filled states with energies near their Fermi level.
Insulators, by contrast, have few partially filled states, their Fermi levels sit within band gaps with few energy states to occupy.
Importantly, an insulator can be made to conduct by increasing its temperature: heating provides energy to promote some electrons across the band gap, inducing partially filled states in both the band of states beneath the band gap (valence band) and the band of states above the band gap (conduction band).
An (intrinsic) semiconductor has a band gap that is smaller than that of an insulator and at room temperature significant numbers of electrons can be excited to cross the band gap.

A pure semiconductor, however, is not very useful, as it is neither a very good insulator nor a very good conductor.
However, one important feature of semiconductors (and some insulators, known as "semi-insulators") is that their conductivity can be increased and controlled by doping with impurities and gating with electric fields. Doping and gating move either the conduction or valence band much closer to the Fermi level, and greatly increase the number of partially filled states.

Some wider-band gap semiconductor materials are sometimes referred to as semi-insulators. When undoped, these have electrical conductivity nearer to that of electrical insulators, however they can be doped (making them as useful as semiconductors). Semi-insulators find niche applications in micro-electronics, such as substrates for HEMT. An example of a common semi-insulator is gallium arsenide. Some materials, such as titanium dioxide, can even be used as insulating materials for some applications, while being treated as wide-gap semiconductors for other applications.

The partial filling of the states at the bottom of the conduction band can be understood as adding electrons to that band.
The electrons do not stay indefinitely (due to the natural thermal recombination) but they can move around for some time.
The actual concentration of electrons is typically very dilute, and so (unlike in metals) it is possible to think of the electrons in the conduction band of a semiconductor as a sort of classical ideal gas, where the electrons fly around freely without being subject to the Pauli exclusion principle. In most semiconductors the conduction bands have a parabolic dispersion relation, and so these electrons respond to forces (electric field, magnetic field, etc.) much like they would in a vacuum, though with a different effective mass.
Because the electrons behave like an ideal gas, one may also think about conduction in very simplistic terms such as the Drude model, and introduce concepts such as electron mobility.

For partial filling at the top of the valence band, it is helpful to introduce the concept of an electron hole.
Although the electrons in the valence band are always moving around, a completely full valence band is inert, not conducting any current.
If an electron is taken out of the valence band, then the trajectory that the electron would normally have taken is now missing its charge.
For the purposes of electric current, this combination of the full valence band, minus the electron, can be converted into a picture of a completely empty band containing a positively charged particle that moves in the same way as the electron.
Combined with the "negative" effective mass of the electrons at the top of the valence band, we arrive at a picture of a positively charged particle that responds to electric and magnetic fields just as a normal positively charged particle would do in vacuum, again with some positive effective mass.
This particle is called a hole, and the collection of holes in the valence band can again be understood in simple classical terms (as with the electrons in the conduction band).

When ionizing radiation strikes a semiconductor, it may excite an electron out of its energy level and consequently leave a hole. This process is known as "electron–hole pair generation". Electron-hole pairs are constantly generated from thermal energy as well, in the absence of any external energy source.

Electron-hole pairs are also apt to recombine. Conservation of energy demands that these recombination events, in which an electron loses an amount of energy larger than the band gap, be accompanied by the emission of thermal energy (in the form of phonons) or radiation (in the form of photons).

In some states, the generation and recombination of electron–hole pairs are in equipoise. The number of electron-hole pairs in the steady state at a given temperature is determined by quantum statistical mechanics. The precise quantum mechanical mechanisms of generation and recombination are governed by conservation of energy and conservation of momentum.

As the probability that electrons and holes meet together is proportional to the product of their numbers, the product is in steady state nearly constant at a given temperature, providing that there is no significant electric field (which might "flush" carriers of both types, or move them from neighbour regions containing more of them to meet together) or externally driven pair generation. The product is a function of the temperature, as the probability of getting enough thermal energy to produce a pair increases with temperature, being approximately exp(−"E"/"kT"), where "k" is Boltzmann's constant, "T" is absolute temperature and "E" is band gap.

The probability of meeting is increased by carrier traps – impurities or dislocations which can trap an electron or hole and hold it until a pair is completed. Such carrier traps are sometimes purposely added to reduce the time needed to reach the steady state.

The conductivity of semiconductors may easily be modified by introducing impurities into their crystal lattice. The process of adding controlled impurities to a semiconductor is known as "doping". The amount of impurity, or dopant, added to an "intrinsic" (pure) semiconductor varies its level of conductivity. Doped semiconductors are referred to as "extrinsic". By adding impurity to the pure semiconductors, the electrical conductivity may be varied by factors of thousands or millions.

A 1 cm specimen of a metal or semiconductor has of the order of 10 atoms. In a metal, every atom donates at least one free electron for conduction, thus 1 cm of metal contains on the order of 10 free electrons, whereas a 1 cm sample of pure germanium at 20 °C contains about atoms, but only free electrons and holes. The addition of 0.001% of arsenic (an impurity) donates an extra 10 free electrons in the same volume and the electrical conductivity is increased by a factor of 10,000.

The materials chosen as suitable dopants depend on the atomic properties of both the dopant and the material to be doped. In general, dopants that produce the desired controlled changes are classified as either electron acceptors or donors. Semiconductors doped with "donor" impurities are called "n-type", while those doped with "acceptor" impurities are known as "p-type". The n and p type designations indicate which charge carrier acts as the material's majority carrier. The opposite carrier is called the minority carrier, which exists due to thermal excitation at a much lower concentration compared to the majority carrier.

For example, the pure semiconductor silicon has four valence electrons which bond each silicon atom to its neighbors. In silicon, the most common dopants are "group III" and "group V" elements. Group III elements all contain three valence electrons, causing them to function as acceptors when used to dope silicon. When an acceptor atom replaces a silicon atom in the crystal, a vacant state (an electron "hole") is created, which can move around the lattice and functions as a charge carrier. Group V elements have five valence electrons, which allows them to act as a donor; substitution of these atoms for silicon creates an extra free electron. Therefore, a silicon crystal doped with boron creates a p-type semiconductor whereas one doped with phosphorus results in an n-type material.

During manufacture, dopants can be diffused into the semiconductor body by contact with gaseous compounds of the desired element, or ion implantation can be used to accurately position the doped regions.

Some materials, when rapidly cooled to a glassy amorphous state, have semiconducting properties. These include B, Si, Ge, Se, and Te, and there are multiple theories to explain them.

The history of the understanding of semiconductors begins with experiments on the electrical properties of materials. The properties of negative temperature coefficient of resistance, rectification, and light-sensitivity were observed starting in the early 19th century.

Thomas Johann Seebeck was the first to notice an effect due to semiconductors, in 1821. In 1833, Michael Faraday reported that the resistance of specimens of silver sulfide decreases when they are heated. This is contrary to the behavior of metallic substances such as copper. In 1839, Alexandre Edmond Becquerel reported observation of a voltage between a solid and a liquid electrolyte when struck by light, the photovoltaic effect. In 1873 Willoughby Smith observed that selenium resistors exhibit decreasing resistance when light falls on them. In 1874, Karl Ferdinand Braun observed conduction and rectification in metallic sulfides, although this effect had been discovered much earlier by Peter Munck af Rosenschold () writing for the Annalen der Physik und Chemie in 1835, and Arthur Schuster found that a copper oxide layer on wires has rectification properties that ceases when the wires are cleaned. William Grylls Adams and Richard Evans Day observed the photovoltaic effect in selenium in 1876.

A unified explanation of these phenomena required a theory of solid-state physics which developed greatly in the first half of the 20th Century. In 1878 Edwin Herbert Hall demonstrated the deflection of flowing charge carriers by an applied magnetic field, the Hall effect. The discovery of the electron by J.J. Thomson in 1897 prompted theories of electron-based conduction in solids. Karl Baedeker, by observing a Hall effect with the reverse sign to that in metals, theorized that copper iodide had positive charge carriers. Johan Koenigsberger classified solid materials as metals, insulators and "variable conductors" in 1914 although his student Josef Weiss already introduced the term "Halbleiter" (semiconductor in modern meaning) in PhD thesis in 1910. Felix Bloch published a theory of the movement of electrons through atomic lattices in 1928. In 1930, B. Gudden stated that conductivity in semiconductors was due to minor concentrations of impurities. By 1931, the band theory of conduction had been established by Alan Herries Wilson and the concept of band gaps had been developed. Walter H. Schottky and Nevill Francis Mott developed models of the potential barrier and of the characteristics of a metal–semiconductor junction. By 1938, Boris Davydov had developed a theory of the copper-oxide rectifier, identifying the effect of the p–n junction and the importance of minority carriers and surface states.

Agreement between theoretical predictions (based on developing quantum mechanics) and experimental results was sometimes poor. This was later explained by John Bardeen as due to the extreme "structure sensitive" behavior of semiconductors, whose properties change dramatically based on tiny amounts of impurities. Commercially pure materials of the 1920s containing varying proportions of trace contaminants produced differing experimental results. This spurred the development of improved material refining techniques, culminating in modern semiconductor refineries producing materials with parts-per-trillion purity.

Devices using semiconductors were at first constructed based on empirical knowledge, before semiconductor theory provided a guide to construction of more capable and reliable devices.

Alexander Graham Bell used the light-sensitive property of selenium to transmit sound over a beam of light in 1880. A working solar cell, of low efficiency, was constructed by Charles Fritts in 1883 using a metal plate coated with selenium and a thin layer of gold; the device became commercially useful in photographic light meters in the 1930s. Point-contact microwave detector rectifiers made of lead sulfide were used by Jagadish Chandra Bose in 1904; the cat's-whisker detector using natural galena or other materials became a common device in the development of radio. However, it was somewhat unpredictable in operation and required manual adjustment for best performance. In 1906 H.J. Round observed light emission when electric current passed through silicon carbide crystals, the principle behind the light-emitting diode. Oleg Losev observed similar light emission in 1922 but at the time the effect had no practical use. Power rectifiers, using copper oxide and selenium, were developed in the 1920s and became commercially important as an alternative to vacuum tube rectifiers.

The first semiconductor devices used galena, including German physicist Ferdinand Braun's crystal detector in 1874 and Bengali physicist Jagadish Chandra Bose's radio crystal detector in 1901.

In the years preceding World War II, infrared detection and communications devices prompted research into lead-sulfide and lead-selenide materials. These devices were used for detecting ships and aircraft, for infrared rangefinders, and for voice communication systems. The point-contact crystal detector became vital for microwave radio systems, since available vacuum tube devices could not serve as detectors above about 4000 MHz; advanced radar systems relied on the fast response of crystal detectors. Considerable research and development of silicon materials occurred during the war to develop detectors of consistent quality.

Detector and power rectifiers could not amplify a signal. Many efforts were made to develop a solid-state amplifier and were successful in developing a device called the point contact transistor which could amplify 20db or more. In 1922, Oleg Losev developed two-terminal, negative resistance amplifiers for radio, but he perished in the Siege of Leningrad after successful completion. In 1926, Julius Edgar Lilienfeld patented a device resembling a field-effect transistor, but it was not practical. R. Hilsch and R. W. Pohl in 1938 demonstrated a solid-state amplifier using a structure resembling the control grid of a vacuum tube; although the device displayed power gain, it had a cut-off frequency of one cycle per second, too low for any practical applications, but an effective application of the available theory. At Bell Labs, William Shockley and A. Holden started investigating solid-state amplifiers in 1938. The first p–n junction in silicon was observed by Russell Ohl about 1941, when a specimen was found to be light-sensitive, with a sharp boundary between p-type impurity at one end and n-type at the other. A slice cut from the specimen at the p–n boundary developed a voltage when exposed to light.

The first working transistor was a point-contact transistor invented by John Bardeen, Walter Houser Brattain and William Shockley at Bell Labs in 1947. Shockley had earlier theorized a field-effect amplifier made from germanium and silicon, but he failed to build such a working device, before eventually using germanium to invent the point-contact transistor. In France, during the war, Herbert Mataré had observed amplification between adjacent point contacts on a germanium base. After the war, Mataré's group announced their "Transistron" amplifier only shortly after Bell Labs announced the "transistor".

In 1954, physical chemist Morris Tanenbaum fabricated the first silicon junction transistor at Bell Labs. However, early junction transistors were relatively bulky devices that were difficult to manufacture on a mass-production basis, which limited them to a number of specialised applications.

The first silicon semiconductor device was a silicon radio crystal detector, developed by American engineer Greenleaf Whittier Pickard in 1906. In 1940, Russell Ohl discovered the p-n junction and photovoltaic effects in silicon. In 1941, techniques for producing high-purity germanium and silicon crystals were developed for radar microwave detectors during World War II. In 1955, Carl Frosch and Lincoln Derick at Bell Labs accidentally discovered that silicon dioxide (SiO) could be grown on silicon, and they later proposed this could mask silicon surfaces during diffusion processes in 1958.

In the early years of the semiconductor industry, up until the late 1950s, germanium was the dominant semiconductor material for transistors and other semiconductor devices, rather than silicon. Germanium was initially considered the more effective semiconductor material, as it was able to demonstrate better performance due to higher carrier mobility. The relative lack of performance in early silicon semiconductors was due to electrical conductivity being limited by unstable quantum surface states, where electrons are trapped at the surface, due to dangling bonds that occur because unsaturated bonds are present at the surface. This prevented electricity from reliably penetrating the surface to reach the semiconducting silicon layer.

A breakthrough in silicon semiconductor technology came with the work of Egyptian engineer Mohamed Atalla, who developed the process of surface passivation by thermal oxidation at Bell Labs in the late 1950s. He discovered that the formation of a thermally grown silicon dioxide layer greatly reduced the concentration of electronic states at the silicon surface, and that silicon oxide layers could be used to electrically stabilize silicon surfaces. Atalla first published his findings in Bell memos during 1957, and then demonstrated it in 1958. This was the first demonstration to show that high-quality silicon dioxide insulator films could be grown thermally on the silicon surface to protect the underlying silicon p-n junction diodes and transistors. Atalla's surface passivation process enabled silicon to surpass the conductivity and performance of germanium, and led to silicon replacing germanium as the dominant semiconductor material. Atalla's surface passivation process is considered the most important advance in silicon semiconductor technology, paving the way for the mass-production of silicon semiconductor devices. By the mid-1960s, Atalla's process for oxidized silicon surfaces was used to fabricate virtually all integrated circuits and silicon devices.

In the late 1950s, Mohamed Atalla utilized his surface passivation and thermal oxidation methods to develop the metal–oxide–semiconductor (MOS) process, which he proposed could be used to build the first working silicon field-effect transistor. This led to the invention of the MOSFET (MOS field-effect transistor) by Mohamed Atalla and Dawon Kahng in 1959. It was the first truly compact transistor that could be miniaturised and mass-produced for a wide range of uses, With its scalability, and much lower power consumption and higher density than bipolar junction transistors, the MOSFET became the most common type of transistor in computers, electronics, and communications technology such as smartphones. The US Patent and Trademark Office calls the MOSFET a "groundbreaking invention that transformed life and culture around the world".

The CMOS (complementary MOS) process was developed by Chih-Tang Sah and Frank Wanlass at Fairchild Semiconductor in 1963. The first report of a floating-gate MOSFET was made by Dawon Kahng and Simon Sze in 1967. FinFET (fin field-effect transistor), a type of 3D multi-gate MOSFET, was developed by Digh Hisamoto and his team of researchers at Hitachi Central Research Laboratory in 1989.





</doc>
<doc id="27711" url="https://en.wikipedia.org/wiki?curid=27711" title="Starch">
Starch

Starch or amylum is a polymeric carbohydrate consisting of numerous glucose units joined by glycosidic bonds. This polysaccharide is produced by most green plants as energy storage. It is the most common carbohydrate in human diets and is contained in large amounts in staple foods like potatoes, maize (corn), rice, wheat and cassava.

Pure starch is a white, tasteless and odorless powder that is insoluble in cold water or alcohol. It consists of two types of molecules: the linear and helical amylose and the branched amylopectin. Depending on the plant, starch generally contains 20 to 25% amylose and 75 to 80% amylopectin by weight. Glycogen, the glucose store of animals, is a more highly branched version of amylopectin.

In industry, starch is converted into sugars, for example by malting, and fermented to produce ethanol in the manufacture of beer, whisky and biofuel. It is processed to produce many of the sugars used in processed foods. Mixing most starches in warm water produces a paste, such as wheatpaste, which can be used as a thickening, stiffening or gluing agent. The biggest industrial non-food use of starch is as an adhesive in the papermaking process. Starch can be applied to parts of some garments before ironing, to stiffen them.

The word "starch" is from its Germanic root with the meanings "strong, stiff, strengthen, stiffen". Modern German "Stärke" (strength) is related and referring for centuries main application, the use in textile: sizing yarn for weaving and starching linen. The Greek term for starch, "amylon" (ἄμυλον), which means "not milled", is also related. It provides the root amyl, which is used as a prefix for several 5-carbon compounds related to or derived from starch (e.g. amyl alcohol).

Starch grains from the rhizomes of "Typha" (cattails, bullrushes) as flour have been identified from grinding stones in Europe dating back to 30,000 years ago. Starch grains from sorghum were found on grind stones in caves in Ngalue, Mozambique dating up to 100,000 years ago.

Pure extracted wheat starch paste was used in Ancient Egypt possibly to glue papyrus. The extraction of starch is first described in the "Natural History" of Pliny the Elder around AD 77–79. Romans used it also in cosmetic creams, to powder the hair and to thicken sauces. Persians and Indians used it to make dishes similar to gothumai wheat halva. Rice starch as surface treatment of paper has been used in paper production in China since 700 CE.

In addition to starchy plants consumed directly, by 2008 66 million tonnes of starch were being produced per year worldwide. In 2011, production was increased to 73 million ton. 

In the EU the starch industry produced about 8.5 million tonnes in 2008, with around 40% being used for industrial applications and 60% for food uses, most of the latter as glucose syrups. In 2017 EU production was 11 million ton of which 9,4 million ton was consumed in the EU and of which 54% were starch sweeteners.

The US produced about 27.5 million tons of starch in 2017, of which about 8.2 million tons was high fructose syrup, 6.2 million tons was glucose syrups, and 2.5 million tons were starch products. The rest of the starch was used for producing ethanol (1.6 billion gallons). 

Most green plants store energy as starch, which is packed into semicrystalline granules. The extra glucose is changed into starch which is more complex than glucose (by plants). Young plants live on this stored energy in their roots, seeds, and fruits until it can find suitable soil in which to grow. An exception is the family Asteraceae (asters, daisies and sunflowers), where starch is replaced by the fructan inulin. Inulin-like fructans are also present in grasses such as wheat, in onions and garlic, bananas, and asparagus.

In photosynthesis, plants use light energy to produce glucose from carbon dioxide. The glucose is used to generate the chemical energy required for general metabolism, to make organic compounds such as nucleic acids, lipids, proteins and structural polysaccharides such as cellulose, or is stored in the form of starch granules, in amyloplasts. Toward the end of the growing season, starch accumulates in twigs of trees near the buds. Fruit, seeds, rhizomes, and tubers store starch to prepare for the next growing season.

Glucose is soluble in water, hydrophilic, binds with water and then takes up much space and is osmotically active; glucose in the form of starch, on the other hand, is not soluble, therefore osmotically inactive and can be stored much more compactly. The semicrystalline granules generally consist of concentric layers of amylose and amylopectin which can be made bioavailable upon cellular demand in the plant. 

Glucose molecules are bound in starch by the easily hydrolyzed alpha bonds. The same type of bond is found in the animal reserve polysaccharide glycogen. This is in contrast to many structural polysaccharides such as chitin, cellulose and peptidoglycan, which are bound by beta bonds and are much more resistant to hydrolysis.

Plants produce starch by first converting glucose 1-phosphate to ADP-glucose using the enzyme glucose-1-phosphate adenylyltransferase. This step requires energy in the form of ATP. The enzyme starch synthase then adds the ADP-glucose via a 1,4-alpha glycosidic bond to a growing chain of glucose residues, liberating ADP and creating amylose. The ADP-glucose is almost certainly added to the non-reducing end of the amylose polymer, as the UDP-glucose is added to the non-reducing end of glycogen during glycogen synthesis.

Starch branching enzyme introduces 1,6-alpha glycosidic bonds between the amylose chains, creating the branched amylopectin. The starch debranching enzyme isoamylase removes some of these branches. Several isoforms of these enzymes exist, leading to a highly complex synthesis process.

Glycogen and amylopectin have similar structure, but the former has about one branch point per ten 1,4-alpha bonds, compared to about one branch point per thirty 1,4-alpha bonds in amylopectin. Amylopectin is synthesized from ADP-glucose while mammals and fungi synthesize glycogen from UDP-glucose; for most cases, bacteria synthesize glycogen from ADP-glucose (analogous to starch).

In addition to starch synthesis in plants, starch can be synthesized from non-food starch mediated by an enzyme cocktail. In this cell-free biosystem, beta-1,4-glycosidic bond-linked cellulose is partially hydrolyzed to cellobiose. Cellobiose phosphorylase cleaves to glucose 1-phosphate and glucose; the other enzyme—potato alpha-glucan phosphorylase can add a glucose unit from glucose 1-phosphorylase to the non-reducing ends of starch. In it, phosphate is internally recycled. The other product, glucose, can be assimilated by a yeast. This cell-free bioprocessing does not need any costly chemical and energy input, can be conducted in aqueous solution, and does not have sugar losses.

Starch is synthesized in plant leaves during the day and stored as granules; it serves as an energy source at night. The insoluble, highly branched starch chains have to be phosphorylated in order to be accessible for degrading enzymes. The enzyme glucan, water dikinase (GWD) phosphorylates at the C-6 position of a glucose molecule, close to the chains 1,6-alpha branching bonds. A second enzyme, phosphoglucan, water dikinase (PWD) phosphorylates the glucose molecule at the C-3 position. A loss of these enzymes, for example a loss of the GWD, leads to a starch excess (sex) phenotype, and because starch cannot be phosphorylated, it accumulates in the plastids.

After the phosphorylation, the first degrading enzyme, beta-amylase (BAM) can attack the glucose chain at its non-reducing end. Maltose is released as the main product of starch degradation. If the glucose chain consists of three or fewer molecules, BAM cannot release maltose. A second enzyme, disproportionating enzyme-1 (DPE1), combines two maltotriose molecules. From this chain, a glucose molecule is released. Now, BAM can release another maltose molecule from the remaining chain. This cycle repeats until starch is degraded completely. If BAM comes close to the phosphorylated branching point of the glucose chain, it can no longer release maltose. In order for the phosphorylated chain to be degraded, the enzyme isoamylase (ISA) is required.

The products of starch degradation are predominantly maltose and smaller amounts of glucose. These molecules are exported from the plastid to the cytosol, maltose via the maltose transporter, which if mutated (MEX1-mutant) results in maltose accumulation in the plastid. Glucose is exported via the plastidic glucose translocator (pGlcT). These two sugars act as a precursor for sucrose synthesis. Sucrose can then be used in the oxidative pentose phosphate pathway in the mitochondria, to generate ATP at night.

While amylose was thought to be completely unbranched, it is now known that some of its molecules contain a few branch points.
Amylose is a much smaller molecule than amylopectin. About one quarter of the mass of starch granules in plants consist of amylose, although there are about 150 times more amylose than amylopectin molecules.

Starch molecules arrange themselves in the plant in semi-crystalline granules. Each plant species has a unique starch granular size: rice starch is relatively small (about 2 μm) while potato starches have larger granules (up to 100 μm).

Starch becomes soluble in water when heated. The granules swell and burst, the semi-crystalline structure is lost and the smaller amylose molecules start leaching out of the granule, forming a network that holds water and increasing the mixture's viscosity. This process is called starch gelatinization. During cooking, the starch becomes a paste and increases further in viscosity. During cooling or prolonged storage of the paste, the semi-crystalline structure partially recovers and the starch paste thickens, expelling water. This is mainly caused by retrogradation of the amylose. This process is responsible for the hardening of bread or staling, and for the water layer on top of a starch gel (syneresis).

Some cultivated plant varieties have pure amylopectin starch without amylose, known as "waxy starches". The most used is waxy maize, others are glutinous rice and waxy potato starch. Waxy starches have less retrogradation, resulting in a more stable paste. High amylose starch, amylomaize, is cultivated for the use of its gel strength and for use as a resistant starch (a starch that resists digestion) in food products.

Synthetic amylose made from cellulose has a well-controlled degree of polymerization. Therefore, it can be used as a potential drug deliver carrier.

Certain starches, when mixed with water, will produce a non-newtonian fluid sometimes nicknamed "oobleck".

The enzymes that break down or hydrolyze starch into the constituent sugars are known as amylases.

Alpha-amylases are found in plants and in animals. Human saliva is rich in amylase, and the pancreas also secretes the enzyme. Individuals from populations with a high-starch diet tend to have more amylase genes than those with low-starch diets;

Beta-amylase cuts starch into maltose units. This process is important in the digestion of starch and is also used in brewing, where amylase from the skin of seed grains is responsible for converting starch to maltose (Malting, Mashing).

Given a heat of combustion of glucose of whereas that of starch is per mole of glucose monomer, hydrolysis releases about per mole, or per gram of glucose product.

If starch is subjected to dry heat, it breaks down to form dextrins, also called "pyrodextrins" in this context. This break down process is known as dextrinization. (Pyro)dextrins are mainly yellow to brown in color and dextrinization is partially responsible for the browning of toasted bread.

A triiodide (I) solution formed by mixing iodine and iodide (usually from potassium iodide) is used to test for starch; a dark blue color indicates the presence of starch. The details of this reaction are not fully known, but recent scientific work using single crystal x-ray crystallography and comparative Raman spectroscopy suggests that the final starch-iodine structure is similar to an infinite polyiodide chain like one found in a pyrroloperylene-iodine complex. The strength of the resulting blue color depends on the amount of amylose present. Waxy starches with little or no amylose present will color red. Benedict's test and Fehling's test is also done to indicate the presence of starch.

Starch indicator solution consisting of water, starch and iodide is often used in redox titrations: in the presence of an oxidizing agent the solution turns blue, in the presence of reducing agent the blue color disappears because triiodide (I) ions break up into three iodide ions, disassembling the starch-iodine complex. Starch solution was used as indicator for visualizing the periodic formation and consumption of triiodide intermediate in the Briggs-Rauscher oscillating reaction. The starch, however, changes the kinetics of the reaction steps involving triiodide ion. 
A 0.3% w/w solution is the standard concentration for a starch indicator. It is made by adding 3 grams of soluble starch to 1 liter of heated water; the solution is cooled before use (starch-iodine complex becomes unstable at temperatures above 35 °C).

Each species of plant has a unique type of starch granules in granular size, shape and crystallization pattern. Under the microscope, starch grains stained with iodine illuminated from behind with polarized light show a distinctive Maltese cross effect (also known as extinction cross and birefringence).

Starch is the most common carbohydrate in the human diet and is contained in many staple foods. The major sources of starch intake worldwide are the cereals (rice, wheat, and maize) and the root vegetables (potatoes and cassava). Many other starchy foods are grown, some only in specific climates, including acorns, arrowroot, arracacha, bananas, barley, breadfruit, buckwheat, canna, colocasia, katakuri, kudzu, malanga, millet, oats, oca, polynesian arrowroot, sago, sorghum, sweet potatoes, rye, taro, chestnuts, water chestnuts and yams, and many kinds of beans, such as favas, lentils, mung beans, peas, and chickpeas.

Widely used prepared foods containing starch are bread, pancakes, cereals, noodles, pasta, porridge and tortilla.

Digestive enzymes have problems digesting crystalline structures. Raw starch is digested poorly in the duodenum and small intestine, while bacterial degradation takes place mainly in the colon. When starch is cooked, the digestibility is increased.

Starch gelatinization during cake baking can be impaired by sugar competing for water, preventing gelatinization and improving texture.

Before the advent of processed foods, people consumed large amounts of uncooked and unprocessed starch-containing plants, which contained high amounts of resistant starch. Microbes within the large intestine fermented the starch, produced short-chain fatty acids, which are used as energy, and support the maintenance and growth of the microbes. More highly processed foods are more easily digested and release more glucose in the small intestine—less starch reaches the large intestine and more energy is absorbed by the body. It is thought that this shift in energy delivery (as a result of eating more processed foods) may be one of the contributing factors to the development of metabolic disorders of modern life, including obesity and diabetes.

The amylose/amylopectin ratio, molecular weight and molecular fine structure influences the physicochemical properties as well as energy release of different types of starches. In addition, cooking and food processing significantly impacts starch digestibility and energy release. Starch can be classified as rapidly digestible, slowly digestible and resistant starch. Raw starch granules resist digestion by human enzymes and do not break down into glucose in the small intestine - they reach the large intestine instead and function as prebiotic dietary fiber. When starch granules are fully gelatinized and cooked, the starch becomes easily digestible and releases glucose quickly within the small intestine. When starchy foods are cooked and cooled, some of the glucose chains re-crystallize and become resistant to digestion again. Slowly digestible starch can be found in raw cereals, where digestion is slow but relatively complete within the small intestine.

The starch industry extracts and refines starches from seeds, roots and tubers, by wet grinding, washing, sieving and drying. Today, the main commercial refined starches are cornstarch, tapioca, arrowroot, and wheat, rice, and potato starches. To a lesser extent, sources of refined starch are sweet potato, sago and mung bean. To this day, starch is extracted from more than 50 types of plants.

Untreated starch requires heat to thicken or gelatinize. When a starch is pre-cooked, it can then be used to thicken instantly in cold water. This is referred to as a pregelatinized starch.

Starch can be hydrolyzed into simpler carbohydrates by acids, various enzymes, or a combination of the two. The resulting fragments are known as dextrins. The extent of conversion is typically quantified by dextrose equivalent (DE), which is roughly the fraction of the glycosidic bonds in starch that have been broken.

These starch sugars are by far the most common starch based food ingredient and are used as sweeteners in many drinks and foods. They include:


A modified starch is a starch that has been chemically modified to allow the starch to function properly under conditions frequently encountered during processing or storage, such as high heat, high shear, low pH, freeze/thaw and cooling.

The modified food starches are E coded according to the International Numbering System for Food Additives (INS):

INS 1400, 1401, 1402, 1403 and 1405 are in the EU food ingredients without an E-number. Typical modified starches for technical applications are cationic starches, hydroxyethyl starch and carboxymethylated starches.

As an additive for food processing, food starches are typically used as thickeners and stabilizers in foods such as puddings, custards, soups, sauces, gravies, pie fillings, and salad dressings, and to make noodles and pastas. They function as thickeners, extenders, emulsion stabilizers and are exceptional binders in processed meats.

Gummed sweets such as jelly beans and wine gums are not manufactured using a mold in the conventional sense. A tray is filled with native starch and leveled. A positive mold is then pressed into the starch leaving an impression of 1,000 or so jelly beans. The jelly mix is then poured into the impressions and put onto a stove to set. This method greatly reduces the number of molds that must be manufactured.

In the pharmaceutical industry, starch is also used as an excipient, as tablet disintegrant, and as binder.

Resistant starch is starch that escapes digestion in the small intestine of healthy individuals.
High amylose starch from corn has a higher gelatinization temperature than other types of starch and retains its resistant starch content through baking, mild extrusion and other food processing techniques. It is used as an insoluble dietary fiber in processed foods such as bread, pasta, cookies, crackers, pretzels and other low moisture foods. It is also utilized as a dietary supplement for its health benefits. Published studies have shown that resistant starch helps to improve insulin sensitivity, increases satiety, reduces pro-inflammatory biomarkers interleukin 6 and tumor necrosis factor alpha and improves markers of colonic function.
It has been suggested that resistant starch contributes to the health benefits of intact whole grains.

Papermaking is the largest non-food application for starches globally, consuming many millions of metric tons annually. In a typical sheet of copy paper for instance, the starch content may be as high as 8%. Both chemically modified and unmodified starches are used in papermaking. In the wet part of the papermaking process, generally called the "wet-end", the starches used are cationic and have a positive charge bound to the starch polymer. These starch derivatives associate with the anionic or negatively charged paper fibers / cellulose and inorganic fillers. Cationic starches together with other retention and internal sizing agents help to give the necessary strength properties to the paper web formed in the papermaking process (wet strength), and to provide strength to the final paper sheet (dry strength).

In the dry end of the papermaking process, the paper web is rewetted with a starch based solution. The process is called surface sizing. Starches used have been chemically, or enzymatically depolymerized at the paper mill or by the starch industry (oxidized starch). The size/starch solutions are applied to the paper web by means of various mechanical presses (size presses). Together with surface sizing agents the surface starches impart additional strength to the paper web and additionally provide water hold out or "size" for superior printing properties. Starch is also used in paper coatings as one of the binders for the coating formulations which include a mixture of pigments, binders and thickeners. Coated paper has improved smoothness, hardness, whiteness and gloss and thus improves printing characteristics.

Corrugated board adhesives are the next largest application of non-food starches globally. Starch glues are mostly based on unmodified native starches, plus some additive such as borax and caustic soda. Part of the starch is gelatinized to carry the slurry of uncooked starches and prevent sedimentation. This opaque glue is called a SteinHall adhesive. The glue is applied on tips of the fluting. The fluted paper is pressed to paper called liner. This is then dried under high heat, which causes the rest of the uncooked starch in glue to swell/gelatinize. This gelatinizing makes the glue a fast and strong adhesive for corrugated board production.

Clothing or laundry starch is a liquid prepared by mixing a vegetable starch in water (earlier preparations also had to be boiled), and is used in the laundering of clothes. Starch was widely used in Europe in the 16th and 17th centuries to stiffen the wide collars and ruffs of fine linen which surrounded the necks of the well-to-do. During the 19th and early 20th century it was stylish to stiffen the collars and sleeves of men's shirts and the ruffles of women's petticoats by applying starch to them as the clean clothes were being ironed. Starch gave clothing smooth, crisp edges, and had an additional practical purpose: dirt and sweat from a person's neck and wrists would stick to the starch rather than to the fibers of the clothing. The dirt would wash away along with the starch; after laundering, the starch would be reapplied. Today, in many cultures, starch is sold in aerosol cans for home use, but in others it remain available in granular form for mixing with water.
Another large non-food starch application is in the construction industry, where starch is used in the gypsum wall board manufacturing process. Chemically modified or unmodified starches are added to the stucco containing primarily gypsum. Top and bottom heavyweight sheets of paper are applied to the formulation, and the process is allowed to heat and cure to form the eventual rigid wall board. The starches act as a glue for the cured gypsum rock with the paper covering, and also provide rigidity to the board.

Starch is used in the manufacture of various adhesives or glues for book-binding, wallpaper adhesives, paper sack production, tube winding, gummed paper, envelope adhesives, school glues and bottle labeling. Starch derivatives, such as yellow dextrins, can be modified by addition of some chemicals to form a hard glue for paper work; some of those forms use borax or soda ash, which are mixed with the starch solution at to create a very good adhesive. Sodium silicate can be added to reinforce these formula.


The Occupational Safety and Health Administration (OSHA) has set the legal limit (Permissible exposure limit) for starch exposure in the workplace as 15 mg/m total exposure and 5 mg/m respiratory exposure over an 8-hour workday. The National Institute for Occupational Safety and Health (NIOSH) has set a Recommended exposure limit (REL) of 10 mg/m total exposure and 5 mg/m respiratory exposure over an 8-hour workday.




</doc>
<doc id="27712" url="https://en.wikipedia.org/wiki?curid=27712" title="Sugar">
Sugar

Sugar is the generic name for sweet-tasting, soluble carbohydrates, many of which are used in food. Table sugar, granulated sugar, or regular sugar, refers to sucrose, a disaccharide composed of glucose and fructose.

Simple sugars, also called monosaccharides, include glucose, fructose, and galactose. Compound sugars, also called disaccharides or double sugars, are molecules composed of two monosaccharides joined by a glycosidic bond. Common examples are sucrose (table sugar) (glucose + fructose), lactose (glucose + galactose), and maltose (two molecules of glucose). In the body, compound sugars are hydrolysed into simple sugars.

Longer chains of monosaccharides are not regarded as sugars, and are called oligosaccharides or polysaccharides. Some other chemical substances, such as glycerol and sugar alcohols, may have a sweet taste, but are not classified as sugar. 

Sugars are found in the tissues of most plants. Honey and fruit are abundant natural sources of unbounded simple sugars. Sucrose is especially concentrated in sugarcane and sugar beet, making them ideal for efficient commercial extraction to make refined sugar. In 2016, the combined world production of those two crops was about two billion tonnes. Maltose may be produced by malting grain. Lactose is the only sugar that cannot be extracted from plants. It can only be found in milk, including human breast milk, and in some dairy products. A cheap source of sugar is corn syrup, industrially produced by converting corn starch into sugars, such as maltose, fructose and glucose.
Sucrose is used in prepared foods (e.g. cookies and cakes), is sometimes added to commercially available processed food and beverages, and may be used by people as a sweetener for foods (e.g. toast and cereal) and beverages (e.g. coffee and tea). The average person consumes about of sugar each year, or in developed countries, equivalent to over 260 food calories per day. As sugar consumption grew in the latter part of the 20th century, researchers began to examine whether a diet high in sugar, especially refined sugar, was damaging to human health. Excessive consumption of sugar has been implicated in the onset of obesity, diabetes, cardiovascular disease, dementia, and tooth decay. Numerous studies have tried to clarify those implications, but with varying results, mainly because of the difficulty of finding populations for use as controls that consume little or no sugar. In 2015, the World Health Organization recommended that adults and children reduce their intake of free sugars to less than 10%, and encouraged a reduction to below 5%, of their total energy intake.

The etymology reflects the spread of the commodity. From Sanskrit ("śarkarā"), meaning "ground or candied sugar", came Persian "shakar", then to 12th century French "sucre" and the English "sugar".

The English word "jaggery", a coarse brown sugar made from date palm sap or sugarcane juice, has a similar etymological origin: Portuguese "jágara" from the Malayalam "cakkarā", which is from the Sanskrit "śarkarā".

Sugar has been produced in the Indian subcontinent since ancient times and its cultivation spread from there into modern-day Afghanistan through the Khyber Pass. It was not plentiful nor cheap in early times, and in most parts of the world, honey was more often used for sweetening. Originally, people chewed raw sugarcane to extract its sweetness. Sugarcane was a native of tropical Indian subcontinent (South Asia) and Southeast Asia.

Different species seem to have originated from different locations with "Saccharum barberi" originating in India and "S. edule" and "S. officinarum" coming from New Guinea. One of the earliest historical references to sugarcane is in Chinese manuscripts dating to 8th century BCE, which state that the use of sugarcane originated in India.

In the tradition of Indian medicine (āyurveda), the sugarcane is known by the name "Ikṣu" and the sugarcane juice is known as "Phāṇita". Its varieties, synonyms and characteristics are defined in nighaṇṭus such as the Bhāvaprakāśa (1.6.23, group of sugarcanes).
Sugar remained relatively unimportant until the Indians discovered methods of turning sugarcane juice into granulated crystals that were easier to store and to transport. Crystallized sugar was discovered by the time of the Imperial Guptas, around the 5th century CE. In the local Indian language, these crystals were called "khanda" (Devanagari: खण्ड, ), which is the source of the word "candy". Indian sailors, who carried clarified butter and sugar as supplies, introduced knowledge of sugar along the various trade routes they travelled. Traveling Buddhist monks took sugar crystallization methods to China. During the reign of Harsha (r. 606–647) in North India, Indian envoys in Tang China taught methods of cultivating sugarcane after Emperor Taizong of Tang (r. 626–649) made known his interest in sugar. China established its first sugarcane plantations in the seventh century. Chinese documents confirm at least two missions to India, initiated in 647 CE, to obtain technology for sugar refining. In the Indian subcontinent, the Middle East and China, sugar became a staple of cooking and desserts.

Nearchus, admiral of Alexander of Macedonia, knew of sugar during the year 325 B.C., because of his participation in the campaign of India led by Alexander ("Arrian, Anabasis"). The Greek physician Pedanius Dioscorides in the 1st century CE described sugar in his medical treatise De Materia Medica, and Pliny the Elder, a 1st-century CE Roman, described sugar in his Natural History: "Sugar is made in Arabia as well, but Indian sugar is better. It is a kind of honey found in cane, white as gum, and it crunches between the teeth. It comes in lumps the size of a hazelnut. Sugar is used only for medical purposes." Crusaders brought sugar back to Europe after their campaigns in the Holy Land, where they encountered caravans carrying "sweet salt". Early in the 12th century, Venice acquired some villages near Tyre and set up estates to produce sugar for export to Europe. It supplemented the use of honey, which had previously been the only available sweetener. Crusade chronicler William of Tyre, writing in the late 12th century, described sugar as "very necessary for the use and health of mankind". In the 15th century, Venice was the chief sugar refining and distribution center in Europe.

There was a drastic change in the mid-15th century, when São Tomé, Madeira, and the Canary Islands were settled from Europe, and sugar grown there. After this an "all-consuming passion for sugar ... swept through society" as it became far more easily available, though initially still very expensive. By 1492, Madeira was producing over three million pounds weight of sugar annually. Genoa, one of the centers of distribution, became known for candied fruit, while Venice specialized in pastries, sweets (candies), and sugar sculptures. Sugar was considered to have "valuable medicinal properties" as a "warm" food under prevailing categories, being "helpful to the stomach, to cure cold diseases, and sooth lung complaints".

A feast given in Tours in 1457 by Gaston de Foix, which is "probably the best and most complete account we have of a late medieval banquet" includes the first mention of sugar sculptures, as the final food brought in was "a heraldic menagerie sculpted in sugar: lions, stags, monkeys ... each holding in paw or beak the arms of the Hungarian king". Other recorded grand feasts in the decades following included similar pieces. Originally the sculptures seem to have been eaten in the meal, but later they become merely table decorations, the most elaborate called "triomfi". Several significant sculptors are known to have produced them; in some cases their preliminary drawings survive. Early ones were in brown sugar, partly cast in molds, with the final touches carved. They continued to be used until at least the Coronation Banquet for Edward VII of the United Kingdom in 1903; among other sculptures every guest was given a sugar crown to take away.

In August 1492 Christopher Columbus picked up sugar cane in La Gomera in the Canary Islands, and introduced it to the New World. The cuttings were planted and the first sugar-cane harvest in Hispaniola took place in 1501. Many sugar mills had been constructed in Cuba and Jamaica by the 1520s. The Portuguese took sugar cane to Brazil. By 1540, there were 800 cane-sugar mills in Santa Catarina Island and another 2,000 on the north coast of Brazil, Demarara, and Surinam. It took until 1600 for Brazilian sugar production to exceed that of São Tomé, which was the main center of sugar production in sixteenth century.
Sugar was a luxury in Europe until the early 19th century, when it became more widely available, due to the rise of beet sugar in Prussia, and later in France under Napoleon. Beet sugar was a German invention, since, in 1747, Andreas Sigismund Marggraf announced the discovery of sugar in beets and devised a method using alcohol to extract it. Marggraf's student, Franz Karl Achard, devised an economical industrial method to extract the sugar in its pure form in the late 18th century. Achard first produced beet sugar in 1783 in Kaulsdorf, and in 1801, the world's first beet sugar production facility was established in Cunern, Silesia (then part of Prussia). The works of Marggraf and Achard were the starting point for the sugar industry in Europe, and for the modern sugar industry in general, since sugar was no longer a luxury product and a product almost only produced in warmer climates.

Sugar became highly popular and by the 19th century, sugar came to be considered a necessity. This evolution of taste and demand for sugar as an essential food ingredient resulted in major economic and social changes. Demand drove, in part, the colonization of tropical islands and areas where labor-intensive sugarcane plantations and sugar manufacturing could be successful. The demand for cheap labor to perform the labor-intensive cultivation and processing increased the demand for the slave trade from Africa (in particular West Africa).

After slavery was abolished, the demand for workers in the British Caribbean colonies was filled by indentured laborers from Indian subcontinent (in particular India). Millions of slave and indentured laborers were brought into the Caribbean and the Americas, Indian Ocean colonies, southeast Asia, Pacific Islands, and East Africa and Natal. Thus the modern ethnic mix of many nations that have been settled in the last two centuries has been influenced by the demand for sugar.

Sugar also led to some industrialization of areas where sugar cane was grown. For example, in the 1790s Lieutenant J. Paterson, of the Bengal establishment, promoted to the British Government the idea that sugar cane could grow in British India, where it had started, with many advantages and at less expense than in the West Indies. As a result, sugar factories were established in Bihar in eastern India.
During the Napoleonic Wars, sugar-beet production increased in continental Europe because of the difficulty of importing sugar when shipping was subject to blockade. By 1880 the sugar beet was the main source of sugar in Europe. It was also cultivated in Lincolnshire and other parts of England, although the United Kingdom continued to import the main part of its sugar from its colonies.

Until the late nineteenth century, sugar was purchased in loaves, which had to be cut using implements called sugar nips. In later years, granulated sugar was more usually sold in bags. Sugar cubes were produced in the nineteenth century. The first inventor of a process to produce sugar in cube form was Moravian Jakub Kryštof Rad, director of a sugar company in Dačice. He began sugar-cube production after being granted a five-year patent for the process on January 23, 1843. Henry Tate of Tate & Lyle was another early manufacturer of sugar cubes at his refineries in Liverpool and London. Tate purchased a patent for sugar-cube manufacture from German Eugen Langen, who in 1872 had invented a different method of processing of sugar cubes.

Sugar was rationed during World War I and more sharply during World War II. This led to the development and use of various artificial sweeteners.

Scientifically, "sugar" loosely refers to a number of carbohydrates, such as monosaccharides, disaccharides, or oligosaccharides. Monosaccharides are also called "simple sugars", the most important being glucose. Most monosaccharides have a formula that conforms to with n between 3 and 7 (deoxyribose being an exception). Glucose has the molecular formula . The names of typical sugars end with -"ose", as in "glucose" and "fructose". Sometimes such words may also refer to any types of carbohydrates soluble in water. The acyclic mono- and disaccharides contain either aldehyde groups or ketone groups. These carbon-oxygen double bonds (C=O) are the reactive centers. All saccharides with more than one ring in their structure result from two or more monosaccharides joined by glycosidic bonds with the resultant loss of a molecule of water () per bond.

Monosaccharides in a closed-chain form can form glycosidic bonds with other monosaccharides, creating disaccharides (such as sucrose) and polysaccharides (such as starch). Enzymes must hydrolyze or otherwise break these glycosidic bonds before such compounds become metabolized. After digestion and absorption the principal monosaccharides present in the blood and internal tissues include glucose, fructose, and galactose. Many pentoses and hexoses can form ring structures. In these closed-chain forms, the aldehyde or ketone group remains non-free, so many of the reactions typical of these groups cannot occur. Glucose in solution exists mostly in the ring form at equilibrium, with less than 0.1% of the molecules in the open-chain form.

Biopolymers of sugars are common in nature. Through photosynthesis, plants produce glyceraldehyde-3-phosphate (G3P), a phosphated 3-carbon sugar that is used by the cell to make monosaccharides such as glucose () or (as in cane and beet) sucrose (). Monosaccharides may be further converted into structural polysaccharides such as cellulose and pectin for cell wall construction or into energy reserves in the form of storage polysaccharides such as starch or inulin. Starch, consisting of two different polymers of glucose, is a readily degradable form of chemical energy stored by cells, and can be converted to other types of energy. Another polymer of glucose is cellulose, which is a linear chain composed of several hundred or thousand glucose units. It is used by plants as a structural component in their cell walls. Humans can digest cellulose only to a very limited extent, though ruminants can do so with the help of symbiotic bacteria in their gut. DNA and RNA are built up of the monosaccharides deoxyribose and ribose, respectively. Deoxyribose has the formula and ribose the formula .

Because sugars burn easily when exposed to flame, the handling of sugars risks dust explosion. The risk of explosion is higher when the sugar has been milled to superfine texture, such as for use in chewing gum. The 2008 Georgia sugar refinery explosion, which killed 14 people and injured 40, and destroyed most of the refinery, was caused by the ignition of sugar dust.

In its culinary use, exposing sugar to heat causes caramelization. As the process occurs, volatile chemicals such as diacetyl are released, producing the characteristic caramel flavor.

Fructose, galactose, and glucose are all simple sugars, monosaccharides, with the general formula CHO. They have five hydroxyl groups (−OH) and a carbonyl group (C=O) and are cyclic when dissolved in water. They each exist as several isomers with dextro- and laevo-rotatory forms that cause polarized light to diverge to the right or the left.

Lactose, maltose, and sucrose are all compound sugars, disaccharides, with the general formula CHO. They are formed by the combination of two monosaccharide molecules with the exclusion of a molecule of water.

The sugar contents of common fruits and vegetables are presented in Table 1.
The fructose to fructose plus glucose ratio is calculated by including the fructose and glucose coming from the sucrose.

In November 2019, scientists reported detecting, for the first time, sugar molecules, including ribose, in meteorites, suggesting that chemical processes on asteroids can produce some fundamentally essential bio-ingredients important to life, and supporting the notion of an RNA World prior to a DNA-based origin of life on Earth, and possibly, as well, the notion of panspermia.

Due to rising demand, sugar production in general increased some 14% over the period 2009 to 2018. The largest importers were China, Indonesia, and the United States.

Global production of sugarcane in 2016 was 1.9 billion tonnes, with Brazil producing 41% of the world total and India 18% (table).

Sugarcane refers to any of several species, or their hybrids, of giant grasses in the genus "Saccharum" in the family Poaceae. They have been cultivated in tropical climates in the Indian subcontinent and Southeast Asia over centuries for the sucrose found in their stems. A great expansion in sugarcane production took place in the 18th century with the establishment of slave plantations in the Americas. The use of slavery for the labor-intensive process resulted in sugar production, enabling prices cheap enough for most people to buy. Mechanization reduced some labor needs, but in the 21st century, cultivation and production relied on low-wage laborers.

Sugar cane requires a frost-free climate with sufficient rainfall during the growing season to make full use of the plant's substantial growth potential. The crop is harvested mechanically or by hand, chopped into lengths and conveyed rapidly to the processing plant (commonly known as a sugar mill) where it is either milled and the juice extracted with water or extracted by diffusion. The juice is clarified with lime and heated to destroy enzymes. The resulting thin syrup is concentrated in a series of evaporators, after which further water is removed. The resulting supersaturated solution is seeded with sugar crystals, facilitating crystal formation and drying. Molasses is a by-product of the process and the fiber from the stems, known as bagasse, is burned to provide energy for the sugar extraction process. The crystals of raw sugar have a sticky brown coating and either can be used as they are, can be bleached by sulfur dioxide, or can be treated in a carbonatation process to produce a whiter product. About of irrigation water is needed for every one kilogram (2.2 pounds) of sugar produced.

In 2016, global production of sugar beets was 277 million tonnes, led by Russia with 19% of the world total (table).

The sugar beet became a major source of sugar in the 19th century when methods for extracting the sugar became available. It is a biennial plant, a cultivated variety of "Beta vulgaris" in the family Amaranthaceae, the tuberous root of which contains a high proportion of sucrose. It is cultivated as a root crop in temperate regions with adequate rainfall and requires a fertile soil. The crop is harvested mechanically in the autumn and the crown of leaves and excess soil removed. The roots do not deteriorate rapidly and may be left in the field for some weeks before being transported to the processing plant where the crop is washed and sliced, and the sugar extracted by diffusion. Milk of lime is added to the raw juice with calcium carbonate. After water is evaporated by boiling the syrup under a vacuum, the syrup is cooled and seeded with sugar crystals. The white sugar that crystallizes can be separated in a centrifuge and dried, requiring no further refining.

Refined sugar is made from raw sugar that has undergone a refining process to remove the molasses. Raw sugar is sucrose which is extracted from sugarcane or sugar beet. While raw sugar can be consumed, the refining process removes unwanted tastes and results in refined sugar or white sugar.

The sugar may be transported in bulk to the country where it will be used and the refining process often takes place there. The first stage is known as affination and involves immersing the sugar crystals in a concentrated syrup that softens and removes the sticky brown coating without dissolving them. The crystals are then separated from the liquor and dissolved in water. The resulting syrup is treated either by a carbonatation or by a phosphatation process. Both involve the precipitation of a fine solid in the syrup and when this is filtered out, many of the impurities are removed at the same time. Removal of color is achieved by using either a granular activated carbon or an ion-exchange resin. The sugar syrup is concentrated by boiling and then cooled and seeded with sugar crystals, causing the sugar to crystallize out. The liquor is spun off in a centrifuge and the white crystals are dried in hot air and ready to be packaged or used. The surplus liquor is made into refiners' molasses.

The International Commission for Uniform Methods of Sugar Analysis sets standards for the measurement of the purity of refined sugar, known as ICUMSA numbers; lower numbers indicate a higher level of purity in the refined sugar.

Refined sugar is widely used for industrial needs for higher quality. Refined sugar is purer (ICUMSA below 300) than raw sugar (ICUMSA over 1,500). The level of purity associated with the colors of sugar, expressed by standard number ICUMSA, the smaller ICUMSA numbers indicate the higher purity of sugar.



Brown sugars are granulated sugars, either containing residual molasses, or with the grains deliberately coated with molasses to produce a light- or dark-colored sugar. They are used in baked goods, confectionery, and toffees. Their darkness is due to the amount of molasses they contain. They may be classified based on their darkness or country of origin. For instance:



In most parts of the world, sugar is an important part of the human diet, making food more palatable and providing food energy. After cereals and vegetable oils, sugar derived from sugarcane and beet provided more kilocalories per capita per day on average than other food groups. According to one source, per capita consumption of sugar in 2016 was highest in the United States, followed by Germany and the Netherlands.

Brown and white granulated sugar are 97% to nearly 100% carbohydrates, respectively, with less than 2% water, and no dietary fiber, protein or fat (table). Brown sugar contains a moderate amount of iron (15% of the Reference Daily Intake in a 100 gram amount, see table), but a typical serving of 4 grams (one teaspoon), would provide 15 calories and a negligible amount of iron or any other nutrient. Because brown sugar contains 5–10% molasses reintroduced during processing, its value to some consumers is a richer flavor than white sugar.

Sugar refiners and manufacturers of sugary foods and drinks have sought to influence medical research and public health recommendations, with substantial and largely clandestine spending documented from the 1960s to 2016. The results of research on the health effects of sugary food and drink differ significantly, depending on whether the researcher has financial ties to the food and drink industry. A 2013 medical review concluded that "unhealthy commodity industries should have no role in the formation of national or international NCD <nowiki>[</nowiki>non-communicable disease<nowiki>]</nowiki> policy".

There have been similar efforts to steer coverage of sugar-related health information in popular media, including news media and social media.

A 2003 World Health Organization technical report provided evidence that high intake of sugary drinks (including fruit juice) increased the risk of obesity by adding to overall energy intake. By itself, sugar is not a factor causing obesity and metabolic syndrome, but rather – when over-consumed – is a component of unhealthy dietary behavior. Meta-analyses showed that excessive consumption of sugar-sweetened beverages increased the risk of developing type 2 diabetes and metabolic syndrome – including weight gain and obesity – in adults and children.

A 2019 meta-analysis found that sugar consumption does not improve mood, but can lower alertness and increase fatigue within an hour of consumption. Some studies report evidence of causality between high consumption of refined sugar and hyperactivity. One review of low-quality studies of children consuming high amounts of energy drinks showed association with higher rates of unhealthy behaviors, including smoking and alcohol abuse, and with hyperactivity and insomnia.

The 2003 WHO report stated that "Sugars are undoubtedly the most important dietary factor in the development of dental caries". A review of human studies showed that the incidence of caries is lower when sugar intake is less than 10% of total energy consumed.

The "empty calories" argument states that a diet high in added sugar will reduce consumption of foods that contain essential nutrients. This nutrient displacement occurs if sugar makes up more than 25% of daily energy intake, a proportion associated with poor diet quality and risk of obesity. Displacement may occur at lower levels of consumption.

Claims have been made of a sugar–Alzheimer's disease connection, but there is inconclusive evidence that cognitive decline is related to dietary fructose or overall energy intake.

The World Health Organization recommends that both adults and children reduce the intake of free sugars to less than 10% of total energy intake, and suggests a reduction to below 5%. "Free sugars" include monosaccharides and disaccharides added to foods, and sugars found in fruit juice and concentrates, as well as in honey and syrups. According to the WHO, "[t]hese recommendations were based on the totality of available evidence reviewed regarding the relationship between free sugars intake and body weight (low and moderate quality evidence) and dental caries (very low and moderate quality evidence)."

On May 20, 2016, the U.S. Food and Drug Administration announced changes to the Nutrition Facts panel displayed on all foods, to be effective by July 2018. New to the panel is a requirement to list "Added sugars" by weight and as a percent of Daily Value (DV). For vitamins and minerals, the intent of DVs is to indicate how much should be consumed. For added sugars, the guidance is that 100% DV should not be exceeded. 100% DV is defined as 50 grams. For a person consuming 2000 calories a day, 50 grams is equal to 200 calories and thus 10% of total calories—the same guidance as the World Health Organization. To put this in context, most 355 mL (12 US fl oz) cans of soda contain 39 grams of sugar. In the United States, a government survey on food consumption in 2013–2014 reported that, for men and women aged 20 and older, the average total sugar intakes—naturally occurring in foods and added—were, respectively, 125 and 99 g/day.

Various culinary sugars have different densities due to differences in particle size and inclusion of moisture.

Domino Sugar gives the following weight to volume conversions (in United States customary units):

The "Engineering Resources – Bulk Density Chart" published in "Powder and Bulk" gives different values for the bulk densities:
Manufacturers of sugary products, such as soft drinks and candy, and the Sugar Research Foundation have been accused of trying to influence consumers and medical associations in the 1960s and 1970s by creating doubt about the potential health hazards of sucrose overconsumption, while promoting saturated fat as the main dietary risk factor in cardiovascular diseases. In 2016, the criticism led to recommendations that diet policymakers emphasize the need for high-quality research that accounts for multiple biomarkers on development of cardiovascular diseases.




</doc>
<doc id="27715" url="https://en.wikipedia.org/wiki?curid=27715" title="Saint Louis">
Saint Louis

Saint Louis, Saint-Louis or St. Louis may refer to a number of things, many of them named after Saint Louis IX (1214–1270), including St. Louis, Missouri.

















</doc>
<doc id="27717" url="https://en.wikipedia.org/wiki?curid=27717" title="Salma Hayek">
Salma Hayek

Salma Hayek Pinault (, ; born Salma Valgarma Hayek Jiménez; September 2, 1966) is a Mexican and American film actress and producer. She began her career in Mexico starring in the telenovela "Teresa" and starred in the film "El Callejón de los Milagros" ("Miracle Alley") for which she was nominated for an Ariel Award. In 1991, Hayek moved to Hollywood and came to prominence with roles in films such as "Desperado" (1995), "From Dusk till Dawn" (1996), "Wild Wild West", and "Dogma" (both 1999).

Her breakthrough role was in the 2002 film "Frida", as Mexican painter Frida Kahlo, for which she was nominated for Best Actress for the Academy Award, BAFTA Award, Golden Globe Award, and Screen Actors Guild Award, and which she also produced. This movie received widespread attention and was a critical and commercial success.

She won the Daytime Emmy Award for Outstanding Directing in a Children/Youth/Family Special for "The Maldonado Miracle" in 2004, and received a Primetime Emmy Award nomination for Outstanding Guest Actress in a Comedy Series after guest-starring in the ABC television comedy-drama "Ugly Betty" in 2007. She also guest-starred on the NBC comedy series "30 Rock" from 2009 to 2013. In 2017, she was nominated for an Independent Spirit Award for her role in "Beatriz at Dinner".

Hayek's recent films include "Grown Ups" (2010), "Puss in Boots" (2011), "Grown Ups 2" (2013), "Tale of Tales" (2015), "The Hitman's Bodyguard" (2017) and "Like a Boss" (2020).

Salma Hayek Jiménez was born in Coatzacoalcos, Veracruz, Mexico. Her father, Sami Hayek Domínguez, is Lebanese Mexican, hailing from the city of Baabdat, Lebanon, a city Salma and her father visited in 2015 to promote her movie "Kahlil Gibran's The Prophet". He owns an industrial-equipment firm and is an oil company executive in Mexico, who once ran for mayor of Coatzacoalcos. Her mother, Diana Jiménez Medina, is an opera singer and talent scout, and is Mexican of Spanish descent. In an interview in 2015 with "Un Nuevo Día" while visiting Madrid, Hayek described herself as fifty-percent Lebanese and fifty-percent Spanish, stating that her grandmother/maternal great-grandparents were from Spain. Her younger brother, Sami (born 1972), is a furniture designer.

Hayek was raised in a wealthy, devout Roman Catholic family, and at age 12 was sent to the Academy of the Sacred Heart in Grand Coteau, Louisiana. In school, she was diagnosed with dyslexia. She attended university at the Universidad Iberoamericana where she studied International Relations. In a 2011 interview with "V" magazine, Hayek mentioned that she was once an illegal immigrant in the United States, although it was not for a long period of time.

At the age of 23, Hayek landed the title role in "Teresa" (1989), a successful Mexican telenovela that made her a star in Mexico. In 1994, Hayek starred in the film "El Callejón de los Milagros" ("Miracle Alley"), which has won more awards than any other movie in the history of Mexican cinema. For her performance, Hayek was nominated for an Ariel Award.

Hayek moved to Los Angeles, California, in 1991 to study acting under Stella Adler. She had limited fluency in English, and dyslexia. Robert Rodriguez, and his producer and then-wife, Elizabeth Avellan, soon gave Hayek a starring role opposite Antonio Banderas in 1995's "Desperado". She followed her role in "Desperado" with a brief role as a vampire queen in "From Dusk till Dawn", in which she performed an erotic table-top snake dance.

Hayek had a starring role opposite Matthew Perry in the 1997 romantic comedy "Fools Rush In". In 1999, she co-starred in Will Smith's "Wild Wild West". She also played a supporting role in Kevin Smith's "Dogma" film as Serendipity. In 2000, Hayek had an uncredited acting part opposite Benicio del Toro in "Traffic". In 2003, she reprised her role from "Desperado" by appearing in "Once Upon a Time in Mexico", the final film of the "Mariachi Trilogy".

Around 2000, Hayek founded film production company Ventanarosa, through which she produces film and television projects. Her first feature as a producer was 1999's "El Coronel No Tiene Quien Le Escriba", Mexico's official selection for submission for Best Foreign Film at the Oscars.

"Frida", co-produced by Hayek, was released in 2002. Starring Hayek as Frida Kahlo, and Alfred Molina as her unfaithful husband, Diego Rivera, the film was directed by Julie Taymor and featured an entourage of stars in supporting and minor roles (Valeria Golino, Ashley Judd, Edward Norton, Geoffrey Rush) and cameos (Antonio Banderas). She earned a Best Actress Academy Award nomination for her performance.

"In the Time of the Butterflies" is a 2001 feature film based on the Julia Álvarez book of the same name, covering the lives of the Mirabal sisters. In the movie, Salma Hayek plays one of the sisters, Minerva, and Edward James Olmos plays the Dominican dictator Rafael Leónidas Trujillo whom the sisters opposed.
In 2003, Hayek produced and directed "The Maldonado Miracle", a Showtime movie based on the book of the same name, winning her a Daytime Emmy Award for Outstanding Directing in a Children/Youth/Family Special. In December 2005, she directed a music video for Prince, titled "Te Amo Corazon" ("I love you, sweetheart") that featured Mía Maestro.

Hayek was an executive producer of "Ugly Betty", a television series that aired around the world from 2006 to 2010. Hayek adapted the series for American television with Ben Silverman, who acquired the rights and scripts from the Colombian telenovela "Yo Soy Betty La Fea" in 2001. Originally intended as a half-hour sitcom for NBC in 2004, the project would later be picked up by ABC for the 2006–2007 season with Silvio Horta also producing. Hayek guest-starred on "Ugly Betty" as Sofia Reyes, a magazine editor. She also had a cameo playing an actress in the telenovela within the show. The show won a Golden Globe Award for Best Comedy Series in 2007. Hayek's performance as Sofia resulted in a nomination for Outstanding Guest Actress in a Comedy Series at the 59th Primetime Emmy Awards.

In April 2007, Hayek finalized negotiations with MGM to become the CEO of her own Latin-themed film production company, Ventanarosa. The following month, she signed a two-year deal with ABC for Ventanarosa to develop projects for the network.

Hayek played the wife of Adam Sandler's character in the buddy comedy "Grown Ups", which also co-starred Chris Rock and Kevin James. At his insistence, Hayek co-starred with Antonio Banderas in the "Shrek" spin-off film "Puss in Boots" as the voice of the character Kitty Softpaws, who serves as Puss in Boots's female counterpart and love interest. In 2012, Hayek directed Jada Pinkett Smith in the music video "Nada Se Compara." She reprised her role in "Grown Ups 2", which was released in July 2013.

At the 2019 San Diego Comic-Con it was announced that she will star as Ajak in the Marvel Cinematic Universe film "The Eternals", directed by Chloé Zhao and scheduled to be theatrically released in the United States on February 12, 2021.

Hayek has been a spokeswoman for Avon cosmetics since February 2004. She was a spokeswoman for Revlon in 1998. In 2001, she modeled for Chopard and was featured in 2006 Campari ads, photographed by Mario Testino. On April 3, 2009, she helped introduce La Doña, a watch by Cartier inspired by fellow Mexican actress María Félix.

Hayek has worked with the Procter & Gamble Company and UNICEF to promote the funding (through disposable diaper sales) of vaccines against maternal and neonatal tetanus. She is a global spokesperson for the Pampers/UNICEF "partnership" 1 Pack = 1 Vaccine to help raise awareness of the program. This "partnership" involves Procter & Gamble donating the cost of one tetanus vaccination (approximately 24 cents) for every pack of Pampers sold.

In 2008, Hayek co-founded Juice Generation's juice delivery program Cooler Cleanse. In 2017, she and Juice Generation founder Eric Helms launched the beauty subscription delivery service Blend It Yourself, based on Hayek's personal beauty elixirs. It supplies subscribers with the prepared organic frozen smoothie and acai bowl ingredients, some of which can also be applied as face masks. She also wrote the foreword to Helms' 2014 book "The Juice Generation: 100 Recipes for Fresh Juices and Superfood Smoothies".

In 2011, Hayek launched her own line of cosmetics, skincare, and haircare products called Nuance by Salma Hayek, to be sold at CVS stores in North America. Hayek was also featured in a series of Spanish language commercials for Lincoln cars.

In spring 2006, the Blue Star Contemporary Art Center in San Antonio, Texas displayed sixteen portrait paintings by muralist George Yepes and filmmaker Robert Rodriguez of Hayek as Aztec goddess Itzpapalotl.

Hayek is a naturalized United States citizen. She studied at Ramtha's School of Enlightenment and is a practitioner of yoga. Hayek, who was raised Catholic, has said she is not very devout anymore and does not believe in the institution (Church), but still believes in Jesus Christ and God.

On March 9, 2007, Hayek confirmed her engagement to French billionaire and Kering CEO François-Henri Pinault as well as her pregnancy. She gave birth to her daughter Valentina Paloma Pinault in September 2007 at Cedars-Sinai Medical Center in Los Angeles, California. They were married on Valentine's Day 2009 in Paris. On April 25, 2009, they had a second ceremony in Venice.

On December 13, 2017, Hayek published an op-ed in "The New York Times" stating that she had been harassed and abused by Harvey Weinstein during the production of "Frida".<ref name="Hayek_12/13/2017"></ref>

In 2019, the Pinault family pledged US$113 million to support the reconstruction efforts of the burned Notre Dame Cathedral in Paris, France.

Hayek's charitable work includes increasing awareness on violence against women and discrimination against immigrants. On July 19, 2005, Hayek testified before the U.S. Senate Committee on the Judiciary supporting reauthorizing the Violence Against Women Act. In February 2006, she donated $25,000 to a Coatzacoalcos, Mexico, shelter for battered women and another $50,000 to Monterrey based anti-domestic violence groups. Hayek is a board member of V-Day, the charity founded by playwright Eve Ensler. Nonetheless, Hayek has stated that she is not a feminist. She later revised her stance on this, stating: "I am a feminist because a lot of amazing women have made me who I am today. (...) But – it should not be just because I am a woman".

Hayek also advocates breastfeeding. During a UNICEF fact-finding trip to Sierra Leone, she breastfed a hungry week-old baby whose mother could not produce milk. She said she did it to reduce the stigma associated with breastfeeding and to encourage infant nutrition.

In 2010, Hayek's humanitarian work earned her a nomination for the VH1 Do Something Awards. In 2013, Hayek launched with Beyoncé and Frida Giannini a Gucci campaign, "Chime for Change", that aims to spread female empowerment.

For International Women's Day 2014 Hayek was one of the artist signatories of Amnesty International's letter, to then British Prime Minister David Cameron, campaigning for women's rights in Afghanistan. Following her visit to Lebanon in 2015, Hayek criticized the discrimination against women there.

In 2020, Hayek raised awareness for the disappearance of Vanessa Guillen through her Instagram.


In July 2007, "The Hollywood Reporter" ranked Hayek fourth in their inaugural Latino Power 50, a list of the most powerful members of the Hollywood Latino community. That same month, a poll found Hayek to be the "sexiest celebrity" out of a field of 3,000 celebrities (male and female); according to the poll, "65 percent of the U.S. population would use the term 'sexy' to describe her". In 2008, she was awarded the Women in Film Lucy Award in recognition of her excellence and innovation in her creative works that have enhanced the perception of women through the medium of television. In December of that year, "Entertainment Weekly" ranked Hayek number 17 in their list of the "25 Smartest People in TV."

She was one of fifteen women selected to appear on the cover of the September 2019 issue of "British Vogue", by guest editor Meghan, Duchess of Sussex.


 


</doc>
<doc id="27718" url="https://en.wikipedia.org/wiki?curid=27718" title="Super Bowl">
Super Bowl

The Super Bowl is the annual championship game of the National Football League (NFL) played in early February. It is the culmination of a regular season that begins in the late summer of the previous year.

The game was created as part of the merger agreement between the NFL and its rival the American Football League (AFL). It was agreed that the two champion teams would begin playing in an annual AFL–NFL World Championship Game until the merger officially began in 1970. The first game was played on January 15, 1967 after both leagues had completed their respective 1966 seasons. After the merger, each league was re-designated as a "conference", and the game has since been played between the conference champions to determine the NFL's league champion. The NFL restricts the use of its "Super Bowl" trademark, and it is frequently referred to as the "big game" or other generic terms by non-sponsoring corporations.

The New England Patriots and the Pittsburgh Steelers have the most Super Bowl championship titles with six each. The Patriots have the most Super Bowl appearances with 11. The National Football Conference (NFC) and the American Football Conference (AFC) are tied with 27 Super Bowl wins each.

The Super Bowl is the second-largest day for U.S. food consumption, after Thanksgiving Day. In addition, the Super Bowl has frequently been the most-watched American television broadcast of the year; the seven most-watched broadcasts in American television history are Super Bowls. In 2015, Super Bowl XLIX became the most-watched American television program in history with an audience of 114.4 million viewers, the fifth time in six years that the game had set a record. The Super Bowl is also among the most-watched sporting events in the world, and is second to the UEFA Champions League final as the most watched annual sporting event worldwide.

Commercial airtime during the Super Bowl broadcast is the most expensive of the year because of the high viewership, leading to companies regularly developing their most expensive advertisements for this broadcast. Watching and discussing the broadcast's commercials has become a significant aspect of the event. In addition, popular singers and musicians have performed during the event's pre-game and halftime ceremonies.

For four decades after its 1920 inception, the NFL successfully fended off several rival leagues. In 1960, the NFL encountered its most serious competitor when the American Football League (AFL) was formed. The AFL vied with the NFL for players and fans. The original "bowl game" was the Rose Bowl Game in Pasadena, California, which was first played in 1902 as the "Tournament East–West football game" as part of the Pasadena Tournament of Roses and moved to the new Rose Bowl Stadium in 1923. The stadium got its name from the fact that the game played there was part of the Tournament of Roses and that it was shaped like a bowl, much like the Yale Bowl in New Haven, Connecticut. The Tournament of Roses football game eventually came to be known as the Rose Bowl Game. Exploiting the Rose Bowl Game's popularity, post-season college football contests were created for Miami (the Orange Bowl), New Orleans (the Sugar Bowl), and El Paso (the Sun Bowl) in 1935, and for Dallas (the Cotton Bowl) in 1937. By the time the first Super Bowl was played, the term "bowl" for any major American football game was well established.
After the American Football League's inaugural season, AFL commissioner Joe Foss sent an invitation to the NFL on January 14, 1961 to schedule a "World Playoff" game between the two leagues' champions, beginning with the upcoming 1961 season. The first World Playoff game would have, if actually played, matched up the Houston Oilers vs. the Green Bay Packers. It took a half-dozen more seasons for this idea to become a reality.

In the mid-1960s, Lamar Hunt, owner of the AFL's Kansas City Chiefs, first used the term "Super Bowl" to refer to the AFL–NFL championship game in the merger meetings. Hunt later said the name was likely in his head because his children had been playing with a Super Ball toy; a vintage example of the ball is on display at the Pro Football Hall of Fame in Canton, Ohio. In a July 25, 1966, letter to NFL commissioner Pete Rozelle, Hunt wrote, "I have kiddingly called it the 'Super Bowl,' which obviously can be improved upon."

The leagues' owners chose the name "AFL–NFL Championship Game", but in July 1966 the "Kansas City Star" quoted Hunt in discussing "the Super Bowl—that's my term for the championship game between the two leagues", and the media immediately began using the term. Although the league stated in 1967 that "not many people like it", asking for suggestions and considering alternatives such as "Merger Bowl" and "The Game", the Associated Press reported that "Super Bowl" "grew and grew and grew—until it reached the point that there was Super Week, Super Sunday, Super Teams, Super Players, ad infinitum". "Super Bowl" became official beginning with the third annual game.

Roman numerals are used to identify each Super Bowl, rather than the year in which it is held, since the fifth edition, in January 1971. The sole exception to this naming convention tradition occurred with Super Bowl 50, which was played on February 7, 2016, following the 2015 regular season, and the following year, the nomenclature returned to Roman numerals for Super Bowl LI, following the 2016 regular season.
After the NFL's Green Bay Packers won the first two Super Bowls, some team owners feared for the future of the merger. At the time, many doubted the competitiveness of AFL teams compared with their NFL counterparts, though that perception changed when the AFL's New York Jets defeated the NFL's Baltimore Colts in Super Bowl III in Miami. One year later, the AFL's Kansas City Chiefs defeated the NFL's Minnesota Vikings 23–7 in Super Bowl IV in New Orleans, which was the final AFL–NFL World Championship Game played before the merger. Beginning with the 1970 season, the NFL realigned into two conferences; the former AFL teams plus three NFL teams (the Baltimore Colts, Pittsburgh Steelers, and Cleveland Browns) would constitute the American Football Conference (AFC), while the remaining NFL clubs would form the National Football Conference (NFC). The champions of the two conferences would play each other in the Super Bowl.

The winning team receives the Vince Lombardi Trophy, named after the coach of the Green Bay Packers, who won the first two Super Bowl games and three of the five preceding NFL championships in 1961, 1962, and 1965. Following Lombardi's death in September 1970, the trophy was named the Vince Lombardi Trophy. The first trophy awarded under the new name was presented to the Baltimore Colts following their win in Super Bowl V in Miami.

Since 2002, the Super Bowl is currently played on the first Sunday in February. This is due to the current NFL schedule which consists of the opening weekend of the season being held immediately after Labor Day (the first Monday in September), the 17-week regular season (where teams each play 16 games and have one bye), the first three rounds of the playoffs, and the Super Bowl two weeks after the two Conference Championship Games. The Conference Championship Games are the third round of the playoffs. The week after the third round of the playoffs is when the Pro Bowl is played. The week after that, the Super Bowl is played. This schedule has been in effect since Super Bowl XXXVIII in February 2004. The date of the Super Bowl can thus be determined from the date of the preceding Labor Day. For example, Labor Day in 2015 occurred on September 7; therefore the next Super Bowl was scheduled exactly five months later on February 7, 2016.

Originally, the game took place in early to mid-January. For Super Bowl I there was only one round of playoffs: the pre-merger NFL and AFL Championship Games. The addition of two playoff rounds (first in 1967 and then in 1978), an increase in regular season games from 14 to 16 (1978), and the establishment of one bye-week per team (1990) have caused the Super Bowl to be played later. Partially offsetting these season-lengthening effects, simultaneous with the addition of two regular season games in 1978, the season was started earlier. Prior to 1978, the season started as late as September 21. Now, since Labor Day is always the first Monday of September, September 13 is the latest possible date for the first full Sunday set of games (since 2002, the regular season has started with the Kickoff Game on the first Thursday after Labor Day). The earliest possible season start date is September 7.

The Pittsburgh Steelers and the New England Patriots are tied with six Super Bowl wins; the Dallas Cowboys and San Francisco 49ers have five victories each, while the Green Bay Packers and New York Giants have four Super Bowl championships. Fourteen other NFL franchises have won at least one Super Bowl.

The Patriots own the record for most Super Bowl appearances overall (eleven) and tied for the most won (six).
The Dallas Cowboys, Pittsburgh Steelers, and Denver Broncos are tied for second with eight appearances apiece, achieving reaching that milestone in this respective order. Belichick owns the record for most Super Bowl wins (eight) and participation in any capacity (twelve, nine times as head coach, once as assistant head coach, and twice as defensive coordinator). Dan Reeves previously held the Super Bowl participation record in any capacity (nine, twice as a player, three times as assistant coach, and four times as head coach). Brady has the most Super Bowl starts (nine) and wins as a player (six), while Charles Haley has the second-most wins among players (five).

Eight teams have appeared in Super Bowl games without a win. The Minnesota Vikings won the last NFL Championship before the merger but lost to the AFL champion Kansas City Chiefs in Super Bowl IV and became the first team to have appeared a record four times without a win. The Buffalo Bills played in a record four Super Bowls in a row but lost every one. The Patriots and Denver Broncos are tied for the most Super Bowl losses (five).

Four teams (the Cleveland Browns, Detroit Lions, Jacksonville Jaguars, and Houston Texans) have never appeared in a Super Bowl. The Browns and Lions both won NFL Championships prior to the creation of the Super Bowl, while the Jaguars (1995) and Texans (2002) are both recent NFL expansion teams.

The Green Bay Packers won the first two Super Bowls (known as the AFL–NFL World Championship Game for these first two contests), defeating the Kansas City Chiefs and Oakland Raiders following the and seasons, respectively. The Packers were led by quarterback, Bart Starr, who was named the Most Valuable Player (MVP) for both games. These two championships, coupled with the Packers' NFL championships in , , and , amount to the most successful stretch in NFL History; five championships in seven years, and the only threepeat in NFL history (1965, 1966, and 1967).

In Super Bowl III, the AFL's New York Jets defeated the eighteen-point favorite Baltimore Colts of the NFL, 16–7. The Jets were led by quarterback Joe Namath, who had famously guaranteed a Jets win prior to the game, and former Colts head coach Weeb Ewbank, and their victory proved that the AFL was the NFL's competitive equal. This was reinforced the following year when the AFL's Kansas City Chiefs defeated the NFL's Minnesota Vikings 23–7 in Super Bowl IV.

After the AFL–NFL merger was completed in 1970, three franchises—the Dallas Cowboys, Miami Dolphins, and Pittsburgh Steelers—would go on to dominate the 1970s, winning a combined eight Super Bowls in the decade.

The Baltimore Colts, now a member of the AFC, would start the decade by defeating the Cowboys in Super Bowl V, a game which is notable as being the only Super Bowl to date in which a player from the losing team won the Super Bowl MVP (Cowboys' linebacker Chuck Howley). Beginning with this Super Bowl, all Super Bowls have served as the NFL's championship game.
The Cowboys, coming back from a loss the previous season, won Super Bowl VI over the Dolphins. However, this would be the Dolphins' final loss for over a year, as the next year, the Dolphins would go 14–0 in the regular season and eventually win all their playoff games, capped off with a 14–7 victory in Super Bowl VII, becoming the first and only team to finish an entire perfect regular and postseason. The Dolphins would repeat as league champions by winning Super Bowl VIII a year later.

In the late 1970s, the Steelers became the first NFL dynasty of the post-merger era by winning four Super Bowls (IX, X, XIII, and XIV) in six years. They were led by head coach Chuck Noll, the play of offensive stars Terry Bradshaw, Franco Harris, Lynn Swann, John Stallworth, and Mike Webster, and their dominant "Steel Curtain" defense, led by "Mean" Joe Greene, L. C. Greenwood, Ernie Holmes, Mel Blount, Jack Ham, and Jack Lambert. The coaches and administrators also were part of the dynasty's greatness as evidenced by the team's "final pieces" being part of the famous 1974 draft. The selections in that class have been considered the best by any pro franchise ever, as Pittsburgh selected four future Hall of Famers, the most for any team in any sport in a single draft. The Steelers were the first team to win three and then four Super Bowls and appeared in six AFC Championship Games during the decade, making the playoffs in eight straight seasons. Nine players and three coaches and administrators on the team have been inducted into the Pro Football Hall of Fame. Pittsburgh still remains the only team to win back-to-back Super Bowls twice and four Super Bowls in a six-year period.

The Steelers' dynasty was interrupted only by the Oakland Raiders' Super Bowl XI win and the Cowboys winning their second Super Bowl of the decade.

Conversely, the Minnesota Vikings, with quarterback Fran Tarkenton and their Purple People Eaters defense, were the only other team to appear in multiple Super Bowls (IV, VIII, IX and XI) this decade but failed to win each one.

In the 1980s and 1990s, the tables turned for the AFC, as the NFC dominated the Super Bowls of the new decade and most of those in the 1990s. The NFC won 16 of the 20 Super Bowls during these two decades, including 13 straight from Super Bowl XIX to Super Bowl XXXI.
The most successful team of the 1980s was the San Francisco 49ers, which featured the West Coast offense of Hall of Fame head coach Bill Walsh. This offense was led by three-time Super Bowl MVP and Hall of Fame quarterback Joe Montana, Super Bowl MVP and Hall of Fame wide receiver Jerry Rice, running back Roger Craig, and defensive safety/cornerback Ronnie Lott. Under their leadership, the 49ers won four Super Bowls in the decade (XVI, XIX, XXIII, and XXIV) and made nine playoff appearances between 1981 and 1990, including eight division championships, becoming the second dynasty of the post-merger NFL.

The 1980s also produced the 1985 Chicago Bears, who posted an 18–1 record under head coach Mike Ditka; quarterback Jim McMahon; and Hall of Fame running back Walter Payton. Their team won Super Bowl XX in dominant fashion. The Washington Redskins and New York Giants were also top teams of this period; the Redskins won Super Bowls XVII, XXII, and XXVI. The Giants claimed Super Bowls XXI and XXV. Both teams won multiple Super Bowls with different starting quarterbacks; the Redskins won with Joe Theismann (XVII), Doug Williams (XXII) and Mark Rypien (XXVI), and the Giants with Phil Simms (XXI) and Jeff Hostetler (XXV). As in the 1970s, the Oakland Raiders were the only team to interrupt the Super Bowl dominance of other teams; they won Super Bowls XV and XVIII (the latter as the Los Angeles Raiders).

Conversely, the Cincinnati Bengals (XVI and XXIII), Miami Dolphins (XVII and XIX) and Denver Broncos (XXI, XXII and XXIV) made multiple Super Bowls in the 1980s without winning one.

Following several seasons with poor records in the 1980s, the Dallas Cowboys rose back to prominence in the 1990s. During this decade, the Cowboys made post-season appearances every year except for the seasons of 1990 and 1997. From 1992 to 1996, the Cowboys won their division championship each year. In this same period, the Buffalo Bills had made their mark reaching the Super Bowl for a record four consecutive years, only to lose all four. After Super Bowl championships by division rivals New York (1990) and Washington (1991), the Cowboys won three of the next four Super Bowls (XXVII, XXVIII, and XXX) led by quarterback Troy Aikman, running back Emmitt Smith, and wide receiver Michael Irvin. All three of these players went to the Hall of Fame. The Cowboys' streak was interrupted by the 49ers, who won their league-leading fifth title overall with Super Bowl XXIX with a dominant performance featuring the Super Bowl MVP and Hall of Fame quarterback Steve Young, Hall of Fame wide receiver Jerry Rice, and Hall of Fame cornerback Deion Sanders; however, the Cowboys' victory in Super Bowl XXX the next year also gave them five titles overall and they did so with Sanders after he won the Super Bowl the previous year with the 49ers. The NFC's winning streak was continued by the Green Bay Packers led by Hall of Fame quarterback Brett Favre, won Super Bowl XXXI, their first championship since Super Bowl II in 1968.

Super Bowl XXXII saw quarterback John Elway and running back Terrell Davis lead the Denver Broncos to an upset victory over the defending champion Packers, snapping the NFC's thirteen-year winning streak. The following year, the Broncos defeated the Atlanta Falcons in Super Bowl XXXIII, Elway's fifth Super Bowl appearance, his second NFL championship, and his final NFL game. The back-to-back victories heralded a change in momentum in which AFC teams would win nine out of 12 Super Bowls. In the years between 1995 and 2018, five teams—the Steelers, New England Patriots, Broncos, Baltimore Ravens, and Indianapolis Colts—accounted for 22 of the 24 AFC Super Bowl appearances (including the last 16), with those same teams often meeting each other earlier in the playoffs. In contrast, the NFC saw a different representative in the Super Bowl every season from 2001 through 2010.
The New England Patriots became the dominant team throughout the early 2000s, winning the championship three out of four years early in the decade. They would become only the second team in the history of the NFL to do so (after the 1990s Dallas Cowboys). In Super Bowl XXXVI, first-year starting quarterback Tom Brady led his team to a 20–17 upset victory over the St. Louis Rams, who two seasons earlier won Super Bowl XXXIV. Brady would go on to win the MVP award for this game. The Patriots also won Super Bowls XXXVIII and XXXIX defeating the Carolina Panthers and the Philadelphia Eagles respectively. This four-year stretch of Patriot dominance was interrupted by the Tampa Bay Buccaneers' 48–21 Super Bowl XXXVII victory over the Oakland Raiders.

The Pittsburgh Steelers and Indianapolis Colts continued the era of AFC dominance by winning Super Bowls XL and XLI in 2005–06 and 2006–07, respectively defeating the Seattle Seahawks and Chicago Bears.

In the 2007 season, the Patriots became the fourth team in NFL history to have a perfect unbeaten and untied regular season record, the second in the Super Bowl era after the 1972 Miami Dolphins, and the first to finish 16–0. They easily marched through the AFC playoffs and were heavy favorites in Super Bowl XLII. However, they lost that game to Eli Manning and the New York Giants 17–14, leaving the Patriots' 2007 record at 18–1.

The following season, the Steelers logged their record sixth Super Bowl title (XLIII) in a 27–23, final-minute victory against the Arizona Cardinals.

The 2009 season saw the New Orleans Saints defeat the Indianapolis Colts in Super Bowl XLIV by a score of 31–17 to take home their first Championship. With this victory, the Saints joined the Tampa Bay Buccaneers and New York Jets as the only teams to have won in their sole Super Bowl appearance, a distinction the Baltimore Ravens also enjoyed in winning Super Bowl XXXV after the 2000 season.

In the AFC, this era was dominated by the New England Patriots, with the only three other teams to represent the conference being the Pittsburgh Steelers, Denver Broncos and Baltimore Ravens. The Super Bowls of the late 2000s and 2010s are notable for the performances (and the pedigrees) of several of the participating quarterbacks, especially on the AFC side in repeated appearances by the same teams and players. In particular, Tom Brady, Ben Roethlisberger, or Peyton Manning appeared as the AFC team's quarterback in all but two of the Super Bowls from 2002 through 2019. Conversely, the only NFC teams to make the Super Bowl twice in this era were the Seattle Seahawks, led by quarterback Russell Wilson, and the New York Giants, led by quarterback Eli Manning.

One of these teams was featured in the culmination of the 2010 season, Super Bowl XLV, which brought the Green Bay Packers their fourth Super Bowl victory and record thirteenth NFL championship overall with the defeat of the Pittsburgh Steelers in February 2011. This became Aaron Rodgers' only Super Bowl victory. The following year, in Super Bowl XLVI, the Patriots made their first appearance of the decade, a position where they would become a mainstay. The Patriots, however, lost to the Eli Manning led New York Giants, 21–17, who had beaten the Patriots four years before. This was the Giants 4th Super Bowl victory.

In Super Bowl XLVII, the NFC's San Francisco 49ers were defeated by the Baltimore Ravens 34–31. The game had been dubbed as the 'Harbaugh Bowl' in the weeks leading up to the game, due to the fact that the coaches of the two teams, John Harbaugh and Jim Harbaugh, are brothers. During the 3rd quarter, the Ravens had a commanding 28–6 lead. However, there was a blackout in New Orleans, where the game was being played. The game was delayed for 34 minutes, and after play resumed, San Francisco stormed back with 17 straight points, but still lost. Super Bowl XLVIII, played at New Jersey's MetLife Stadium in February 2014, was the first Super Bowl held outdoors in a cold weather environment. The Seattle Seahawks won their first NFL title with a 43–8 defeat of the Denver Broncos, in a highly touted matchup that pitted Seattle's top-ranked defense against a Peyton Manning-led Denver offense that had broken the NFL's single-season scoring record.

In Super Bowl XLIX, the Patriots beat the defending Super Bowl champions, the Seahawks, by a score of 28–24. Down by 10, the Patriots hosted a late 4th quarter comeback to win the game with Tom Brady scoring two touchdowns in the 4th quarter. In a key play in the final seconds of the game, then rookie free agent Malcolm Butler would intercept a pass by Russell Wilson at the one yard line, allowing the Patriots to run out the clock and end the game. 
Tom Brady was awarded his 3rd Super Bowl MVP, tying Joe Montana for the most Super Bowl MVP awards.

In Super Bowl 50, the first Super Bowl to be branded with Arabic numerals, the Broncos, led by the league's top-ranked defense, defeated the Carolina Panthers, who had the league's top-ranked offense, in what became the final game of quarterback Peyton Manning's career. Von Miller dominated, totaling 2.5 sacks and forcing two Cam Newton fumbles; both fumbles leading to Broncos touchdowns.

In Super Bowl LI, the first Super Bowl to end in overtime, the Atlanta Falcons led 28–3 late in the third quarter; however, they squandered the lead as the Patriots would tie the game 28–28 on back to back touchdowns and two point conversions. The Atlanta Falcons lost to the Patriots 34–28 in overtime. This 25 point deficit would be the largest comeback win for any team in a Super Bowl, breaking the previous of a 10 point deficit to comeback and win. The Patriots never held the lead until the game winning touchdown in overtime. Tom Brady was awarded his record fourth Super Bowl MVP and 5th win as a Super Bowl Champion, throwing a then record 466 yards for 43 completions.

In Super Bowl LII, the Philadelphia Eagles defeated the defending champion Patriots 41–33, ending a 57-year championship drought for the franchise. Nick Foles won the Super Bowl MVP. The Patriots totaled 613 yards in defeat, with Tom Brady breaking his previous Super Bowl record of 466 passing yards with an all time playoff record 505 passing yards in the high scoring game; while the Eagles would gain 538 yards in victory. The Patriots' 33 points was the highest losing score in Super Bowl history. The combined total of 1,151 yards of offense for both teams broke an NFL record (for any game) that had stood for nearly seven decades. It was the Eagles' third Super Bowl appearance, and their first win in franchise history. 

While Super Bowl LII produced the second highest-scoring Super Bowl, the following year's Super Bowl LIII became the lowest-scoring Super Bowl. The Patriots defeated the Los Angeles Rams, 13–3. Tom Brady would receive a record sixth Super Bowl championship, the most of any player in NFL history, surpassing his tie with Charles Haley for five wins. Brady would also become the oldest player to ever win a Super Bowl at age 41, while Bill Belichick would be the oldest coach to ever win a Super Bowl at age 66. Wide receiver Julian Edelman was named Super Bowl MVP.

In Super Bowl LIV the Kansas City Chiefs defeated the San Francisco 49ers in an end-game comeback, 31–20, for their first Super Bowl title in 50 years. This victory marked the first time since 1991 that the NFC did not have more Super Bowl victories than the AFC.

The Super Bowl is one of the most watched annual sporting events in the world, with viewership overwhelmingly domestic. The only other annual event that gathers more viewers is the UEFA Champions League final. For many years, the Super Bowl has possessed a large US and global television viewership, and it is often the most watched United States originating television program of the year. The game tends to have high Nielsen television ratings, which is usually around a 40 rating and 60 shares. This means that on average, more than 100 million people from the United States alone are tuned into the Super Bowl at any given moment.

In press releases preceding each year's event, the NFL typically claims that this year's Super Bowl will have a potential worldwide audience of around one billion people in over 200 countries. This figure refers to the number of people "able" to watch the game, not the number of people "actually" watching. However, the statements have been frequently misinterpreted in various media as referring to the latter figure, leading to a common misperception about the game's actual global audience. The New York-based media research firm Initiative measured the global audience for the 2005 Super Bowl at 93 million people, with 98 percent of that figure being viewers in North America, which meant roughly two million people outside North America watched the Super Bowl that year.

The 2015 Super Bowl XLIX holds the record for average number of U.S. viewers, with a final number of 114.4 million, making the game the most-viewed television broadcast of any kind in American history. The halftime show followed with 118.5 million viewers tuning in, and an all-time high of 168 million viewers in the United States had watched several portions of the Super Bowl 2015 broadcast. The game set a record for total viewers for the fifth time in six years.

The highest-rated game according to Nielsen was Super Bowl XVI in 1982, which was watched in 49.1% of households (73 shares), or 40,020,000 households at the time. Ratings for that game, a San Francisco victory over Cincinnati, may have been aided by a large blizzard that had affected much of the northeastern United States on game day, leaving residents to stay at home more than usual. Super Bowl XVI still ranks fourth on Nielsen's list of top-rated programs of all time, and three other Super Bowls, XII, XVII, and XX, made the top ten.

Famous commercial campaigns include the Budweiser "Bud Bowl" campaign, the 1984 introduction of Apple's Macintosh computer, and the 1999 and 2000 dot-com ads. As the television ratings of the Super Bowl have steadily increased over the years, prices have also increased every year, with advertisers paying as much as $3.5 million for a thirty-second spot during Super Bowl XLVI in 2012. A segment of the audience tunes into the Super Bowl solely to view commercials. In 2010, Nielsen reported that 51 percent of Super Bowl viewers tune in for the commercials. The Super Bowl halftime show has spawned another set of alternative entertainment such as the Lingerie Bowl, the Beer Bottle Bowl, and others.

Since 1991, the Super Bowl has begun between 6:19 and 6:40 PM EST so that most of the game is played during the primetime hours on the East Coast.

<br>

Super Bowls I–VI were blacked out in the television markets of the host cities, due to league restrictions then in place. Super Bowl VII was telecast in Los Angeles on an experimental basis after all tickets were sold ten days prior to the game.

The Super Bowl provides an extremely strong lead-in to programming following it on the same channel, the effects of which can last for several hours. For instance, in discussing the ratings of a local TV station, Buffalo television critic Alan Pergament noted on the coattails from Super Bowl XLVII, which aired on CBS: "A paid program that ran on CBS4 (WIVB-TV) at 2:30 in the morning had a 1.3 rating. That's higher than some CW prime time shows get on WNLO-TV, Channel 4's sister station."

Because of this strong coattail effect, the network that airs the Super Bowl typically takes advantage of the large audience to air an episode of a hit series, or to premiere the pilot of a promising new one in the lead-out slot, which immediately follows the Super Bowl and post-game coverage.

Early Super Bowls featured a halftime show consisting of marching bands from local colleges or high schools; but as the popularity of the game increased, a trend where popular singers and musicians performed during its pre-game ceremonies and the halftime show, or simply sang the national anthem of the United States or America the Beautiful emerged. Unlike regular season or playoff games, thirty minutes are allocated for the Super Bowl halftime. After a special live episode of the Fox sketch comedy series "In Living Color" caused a drop in viewership for the Super Bowl XXVI halftime show, the NFL sought to increase the Super Bowl's audience by hiring A-list talent to perform. They approached Michael Jackson, whose performance the following year drew higher figures than the game itself. Another notable performance came during Super Bowl XXXVI in 2002, when U2 performed; during their third song, "Where the Streets Have No Name", the band played under a large projection screen which scrolled through names of the victims of the September 11 attacks.

For many years, Whitney Houston's performance of the national anthem at Super Bowl XXV in 1991, during the Gulf War, had long been regarded as one of the best renditions of the anthem in history. Prior to Super Bowl XLVIII, soprano Renee Fleming became the first opera singer to perform the anthem.

The halftime show of Super Bowl XXXVIII attracted controversy, following an incident in which Justin Timberlake removed a piece of Janet Jackson's top, briefly exposing one of her breasts before the broadcast quickly cut away from the shot. The incident led to fines being issued by the FCC (and a larger crackdown over "indecent" content broadcast on television), and MTV (then a sister to the game's broadcaster that year, CBS, under Viacom) being banned by the NFL from producing the Super Bowl halftime show in the future. In an effort to prevent a repeat of the incident, the NFL held a moratorium on Super Bowl halftime shows featuring pop performers, and instead invited a single, headlining veteran act, such as Paul McCartney, The Rolling Stones, The Who, Prince, and Bruce Springsteen. This practice ended at Super Bowl XLV, which returned to using current pop acts such as The Black Eyed Peas and Lady Gaga.

Excluding Super Bowl XXXIX, the famous "I'm going to Disney World!" advertising campaign took place in every Super Bowl since Super Bowl XXI when quarterback Phil Simms from the New York Giants became the first player to say the tagline.

As of Super Bowl LIV, 28 of 54 Super Bowls have been played in three metropolitan areas: the Greater Miami area (eleven times), New Orleans (ten times), and the Greater Los Angeles area (seven times). No market or region without an active NFL franchise has ever hosted a Super Bowl, and the presence of an NFL team in a market or region is now a "de jure" requirement for bidding on the game. For instance while Los Angeles had been a seven time host city with its most recent being Super Bowl XXVII in 1993, it has not hosted one since due to the . The Louisiana Superdome has hosted seven Super Bowls, the most of any venue. The Orange Bowl was the only AFL stadium to host a Super Bowl and the only stadium to host consecutive Super Bowls, hosting Super Bowls II and III.

Seven Super Bowls have been held in a stadium other than the one the NFL team in that city was using at the time, a situation that has not arisen after Super Bowl XXVII's host stadium was selected on March 19, 1991. This was as the winning market was previously not required to host the Super Bowl in the same stadium that its NFL team used, if the stadium in which the Super Bowl was held was perceived to be a better stadium for a large high-profile event than the existing NFL home stadium in the same city; for example Los Angeles's last five Super Bowls were all played at the Rose Bowl, which has never been used by any NFL franchise outside of the Super Bowl. Besides the Rose Bowl, the only other Super Bowl venues that were not the home stadium to NFL teams at the time were Rice Stadium (the Houston Oilers had played in Rice Stadium previously but moved to the Astrodome several years prior to Super Bowl VIII) and Stanford Stadium. Starting with the selection of the Super Bowl XXVIII venue on May 23, 1990, the league has given preference in awarding the Super Bowl to brand new or recently renovated NFL stadiums, alongside a trend of teams demanding public money or relocating to play in new stadiums.

No team has ever played the Super Bowl in its home stadium. The closest any team has come was the 2017 Minnesota Vikings, who were within one win of playing Super Bowl LII in U.S. Bank Stadium, but lost the NFC Championship game to the Philadelphia Eagles. In that instance, U.S. Bank Stadium became the first Super Bowl host stadium (selected on May 20, 2014) to also host a Divisional Playoff Game in the same season (which the Vikings won); all previous times that the Super Bowl host stadium hosted another playoff game in the same postseason were all Wild Card games. Two teams have played the Super Bowl in their home market: the San Francisco 49ers, who won Super Bowl XIX in Stanford Stadium instead of Candlestick Park; and the Los Angeles Rams, who lost Super Bowl XIV in the Rose Bowl instead of Los Angeles Memorial Coliseum, during the time when league often picked a stadium that was not home to an NFL team to host the Super Bowl (see above).

Traditionally, the NFL does not award Super Bowls to stadiums that are located in climates with an expected average daily temperature less than 50 °F (10 °C) on game day unless the field can be completely covered by a fixed or retractable roof. Six Super Bowls have been played in northern cities: two in the Detroit area—Super Bowl XVI at Pontiac Silverdome in Pontiac, Michigan and Super Bowl XL at Ford Field in Detroit, two in Minneapolis—Super Bowl XXVI at the Hubert H. Humphrey Metrodome and Super Bowl LII at the U.S. Bank Stadium, one in Indianapolis at Lucas Oil Stadium for Super Bowl XLVI, and one in the New York area—Super Bowl XLVIII at MetLife Stadium. Only MetLife Stadium did not have a roof (be it fixed or retractable) but it was still picked as the host stadium for Super Bowl XLVIII in an apparent waiver of the warm-climate rule, with a contingency plan to reschedule the game in the event of heavy snowfall. MetLife Stadium's selection over Sun Life Stadium generated controversy as the league requested a roof to be added to Sun Life Stadium (in the event of rainstorms) in order to considered for future Super Bowls.

There have been a few instances where the league has rescinded the Super Bowl from cities. Super Bowl XXVII in 1993 was originally awarded to Sun Devil Stadium in Tempe, Arizona, but after Arizona voters elected not to recognize Martin Luther King, Jr. Day as a paid state employees' holiday in 1990, the NFL moved the game to the Rose Bowl in Pasadena, California. When voters in Arizona opted to create such a legal holiday in 1992, Super Bowl XXX in 1996 was awarded to Tempe. Super Bowl XXXIII was awarded first to Candlestick Park in San Francisco, but when plans to renovate the stadium fell through, the game was moved to Pro Player Stadium in greater Miami. Super Bowl XXXVII was awarded to a new stadium not yet built in San Francisco, when that stadium failed to be built, the game was moved to Qualcomm Stadium in San Diego. Super Bowl XLIV, slated for February 7, 2010, was withdrawn from New York City's proposed West Side Stadium, because the city, state, and proposed tenants New York Jets could not agree on funding. Super Bowl XLIV was then eventually awarded to Hard Rock Stadium in Miami Gardens, Florida. Super Bowl XLIX in 2015 was originally given to Arrowhead Stadium in Kansas City, Missouri, but after two sales taxes failed to pass at the ballot box (a renovation proposal had passed successfully, but a second ballot question to add a rolling roof structure to be shared with Kaufmann Stadium critical for the game to be hosted was rejected), and opposition by local business leaders and politicians increased, Kansas City eventually withdrew its request to host the game. Super Bowl XLIX was then eventually awarded to State Farm Stadium in Glendale, Arizona.

The location of the Super Bowl is chosen at a meeting of all NFL team owners, usually three to five years prior to the event. The game has never been played in a metropolitan area that lacked an NFL franchise at the time the game was played, although in 2007 NFL commissioner Roger Goodell suggested that a Super Bowl might be played in London, perhaps at Wembley Stadium.

Through Super Bowl LVI, teams were allowed to bid for the rights to host Super Bowls, where cities submitted proposals to host a Super Bowl and were evaluated in terms of stadium renovation and their ability to host, but this competition was rescinded in 2018. The league will make all decisions regarding hosting sites from Super Bowl LVII onward; the league will choose a potential venue unilaterally, the chosen team will put together a hosting proposal, and the league will vote upon it to determine if it is acceptable.

In 2014, a document listing the specific requirements of Super Bowl hosts was leaked, giving a clear list of what was required for a Super Bowl host. Some of the host requirements include:

Much of the cost of a Super Bowl is to be assumed by the host community, although some costs are enumerated within the requirements to be assumed by the NFL. 
New Orleans, the site of the Super Bowl XLVII in 2013, invested more than $1 billion in infrastructure improvements in the years leading up to the game.

The designated "home team" alternates between the NFC team in odd-numbered games and the AFC team in even-numbered games. This alternation was initiated with the first Super Bowl, when the Green Bay Packers were the designated home team. Regardless of being the home or away team of record, each team has their team logo and wordmark painted in one of the end zones. Designated away teams have won 30 of 54 Super Bowls to date (approximately 56%).

Since Super Bowl XIII in January 1979, the home team is given the choice of wearing their colored or white jerseys. Originally, the designated home team had to wear their colored jerseys, which resulted in Dallas donning their less exposed dark blue jerseys for Super Bowl V. While most of the home teams in the Super Bowl have chosen to wear their colored jerseys, there have been six (6) exceptions: the Dallas Cowboys during Super Bowl XIII and XXVII, the Washington Redskins during Super Bowl XVII, the Pittsburgh Steelers during Super Bowl XL, the Denver Broncos during Super Bowl 50, and the New England Patriots in Super Bowl LII. The Cowboys, since , have worn white jerseys at home. The Redskins wore white at home under coach Joe Gibbs starting in through , continued by Richie Petitbon and Norv Turner through , then again when Gibbs returned from through . Meanwhile, the Steelers, who have always worn their black jerseys at home since the AFL–NFL merger in , opted for the white jerseys after winning three consecutive playoff games on the road, wearing white. The Steelers' decision was compared with the New England Patriots in Super Bowl XX; the Patriots had worn white jerseys at home during the season, but after winning road playoff games against the New York Jets and Miami Dolphins wearing red jerseys, New England opted to switch to crimson for the Super Bowl as the designated home team. For the Broncos in Super Bowl 50, Denver general manager John Elway simply stated, "We've had Super Bowl success in our white uniforms"; they previously had been in Super Bowls when wearing their orange jerseys. The Broncos' decision is also perceived to be made out of superstition, losing all Super Bowl games with the orange jerseys in terrible fashion. It is unclear why the Patriots chose to wear their white jerseys for Super Bowl LII. During the pairing of Bill Belichick and Tom Brady, New England has mostly worn their blue jerseys for home games, but have worn white for a home game in the , , and seasons. The New England Patriots were 3–0 in their white uniforms in Super Bowls prior to Super Bowl LII with Belichick and Brady, and they may have been going on recent trends of teams who wear white for the Super Bowl game. White-shirted teams have won 34 of 54 Super Bowls to date (63%). The only teams to win in their dark-colored uniform in more recent years are the Green Bay Packers against the Pittsburgh Steelers in Super Bowl XLV, the Philadelphia Eagles against the New England Patriots in Super Bowl LII, and the Kansas City Chiefs against the San Francisco 49ers in Super Bowl LIV with teams in white winning 13 of the last 16 Super Bowls.

The 49ers, as part of the league's 75th Anniversary celebration, used their 1955 throwback uniform in Super Bowl XXIX, which for that year was their regular home jersey. The Los Angeles Rams in Super Bowl LIII wore their royal blue and yellow throwback uniforms, which they have previously worn for six home games including a home playoff game. No team has yet worn a third jersey or Color Rush uniform for the Super Bowl. The 49ers reportedly requested to wear an all-white third jersey ensemble for Super Bowl LIV, which the "San Francisco Chronicle" noted they could do with special permission from the league; the league never granted such permission, and the 49ers instead opted for their standard uniform of white jerseys with gold pants.

Fifteen different regions have hosted Super Bowls.

A total of 26 different stadiums, six of which no longer exist and one of which does not yet exist, either have hosted or are scheduled to host Super Bowls. The years listed in the table below are the years the game was actually played ("will be played") rather than what NFL season it is considered to have been.

Future venues:

The game has never been played in a region that lacked an NFL or AFL franchise at the time the game was played. San Diego is the only metropolitan area that has hosted past Super Bowls but does not currently have an NFL franchise; what is now SDCCU Stadium hosted three Super Bowls prior to that city losing its NFL franchise to relocation. Also, London, England, has occasionally been mentioned as a host city for a Super Bowl in the near future. Wembley Stadium has hosted several NFL games as part of the NFL International Series and is specifically designed for large, individual events. NFL Commissioner Roger Goodell has openly discussed the possibility on different occasions. Time zone complications are a significant obstacle to a Super Bowl in London; a typical 6:30 p.m. EST start would result in the game beginning at 11:30 p.m. local time in London, an unusually late hour to be holding spectator sports (the NFL has never in its history started a game later than 9:15 p.m. local time). As bids have been submitted for all Super Bowls through Super Bowl LVIII, the soonest that any stadium outside the NFL's footprint could serve as host would be Super Bowl LIX in 2025.

Seven stadiums that hosted a Super Bowl game no longer exist. Tulane Stadium, on the Tulane University campus, which hosted three Super Bowls, was demolished in November 1979; Tampa Stadium, which hosted two Super Bowls, was demolished in April 1999; Stanford Stadium, which hosted one Super Bowl, was demolished and redeveloped in 2005–06; the Orange Bowl, which hosted five Super Bowls, was demolished in May 2008; the Hubert H. Humphrey Metrodome in Minneapolis, which hosted the 1992 Super Bowl, was demolished in March 2014; the Georgia Dome in Atlanta, which hosted two Super Bowls, was demolished in November 2017; and the Pontiac Silverdome in suburban Detroit, which hosted the 1982 Super Bowl, was demolished in March 2018, five months following the demolition of the Georgia Dome.

The NFL is very active on stopping what it says is unauthorized commercial use of its trademarked terms "NFL", "Super Bowl", and "Super Sunday". As a result, many events and promotions tied to the game, but not sanctioned by the NFL, are asked to refer to it with euphemisms such as "The Big Game", or other generic descriptions. A radio spot for Planters nuts parodied this, by saying "it would be "super"... to have a "bowl"... of Planters nuts while watching the big game!" and comedian Stephen Colbert began referring to the game in 2014 as the "Superb Owl". In 2015, the NFL filed opposition with the USPTO Trademark Trial and Appeal Board to a trademark application submitted by an Arizona-based nonprofit for "Superb Owl". The NFL claims that the use of the phrase "Super Bowl" implies an NFL affiliation, and on this basis the league asserts broad rights to restrict how the game may be shown publicly; for example, the league says Super Bowl showings are prohibited in churches or at other events that "promote a message", while venues that do not regularly show sporting events cannot show the Super Bowl on any television screen larger than 55 inches. Some critics say the NFL is exaggerating its ownership rights by stating that "any use is prohibited", as this contradicts the broad doctrine of fair use in the United States. Legislation was proposed by Utah Senator Orrin Hatch in 2008 "to provide an exemption from exclusive rights in copyright for certain nonprofit organizations to display live football games", and "for other purposes".

In 2004, the NFL started issuing Cease and Desist letters to casinos in Las Vegas that were hosting Super Bowl parties. "Super Bowl" is a registered trademark, owned by the NFL, and any other business using that name for profit-making ventures is in violation of federal law, according to the letters. In reaction to the letters, many Vegas resorts, rather than discontinue the popular and lucrative parties, started referring to them as "Big Game Parties".

In 2006, the NFL made an attempt to trademark "The Big Game" as well; however, it withdrew the application in 2007 due to growing commercial and public relations opposition to the move, mostly from Stanford University and the University of California, Berkeley and their fans, as the Stanford Cardinal football and California Golden Bears football teams compete in the "Big Game", which has been played since 1892 (28 years before the formation of the NFL and 75 years before Super Bowl I). Additionally, the Mega Millions lottery game was known as "The Big Game" (then "The Big Game Mega Millions") from 1996 to 2002.

Like the other major professional leagues in the United States, the winner of the Super Bowl is usually declared de facto "World Champion." The origin of U.S. major professional sports using the term "World Champion" originates from the World Series of Major League Baseball, and it was later used during the first three Super Bowls when they were referred to as AFL–NFL World Championship Games. While controversial, most players in the league endorse the use of the word, due to the fact that there aren't any teams from other countries that have challenged for the title. The phrase is still engraved on the Super Bowl rings.


Notes
Further reading



</doc>
<doc id="27725" url="https://en.wikipedia.org/wiki?curid=27725" title="Surface area">
Surface area

The surface area of a solid object is a measure of the total area that the surface of the object occupies. The mathematical definition of surface area in the presence of curved surfaces is considerably more involved than the definition of arc length of one-dimensional curves, or of the surface area for polyhedra (i.e., objects with flat polygonal faces), for which the surface area is the sum of the areas of its faces. Smooth surfaces, such as a sphere, are assigned surface area using their representation as parametric surfaces. This definition of surface area is based on methods of infinitesimal calculus and involves partial derivatives and double integration.

A general definition of surface area was sought by Henri Lebesgue and Hermann Minkowski at the turn of the twentieth century. Their work led to the development of geometric measure theory, which studies various notions of surface area for irregular objects of any dimension. An important example is the Minkowski content of a surface.

While the areas of many simple surfaces have been known since antiquity, a rigorous mathematical "definition" of area requires a great deal of care.
This should provide a function

which assigns a positive real number to a certain class of surfaces that satisfies several natural requirements. The most fundamental property of the surface area is its additivity: "the area of the whole is the sum of the areas of the parts". More rigorously, if a surface "S" is a union of finitely many pieces "S", …, "S" which do not overlap except at their boundaries, then 

Surface areas of flat polygonal shapes must agree with their geometrically defined area. Since surface area is a geometric notion, areas of congruent surfaces must be the same and the area must depend only on the shape of the surface, but not on its position and orientation in space. This means that surface area is invariant under the group of Euclidean motions. These properties uniquely characterize surface area for a wide class of geometric surfaces called "piecewise smooth". Such surfaces consist of finitely many pieces that can be represented in the parametric form

with a continuously differentiable function formula_4 The area of an individual piece is defined by the formula

Thus the area of "S" is obtained by integrating the length of the normal vector formula_6 to the surface over the appropriate region "D" in the parametric "uv" plane. The area of the whole surface is then obtained by adding together the areas of the pieces, using additivity of surface area. The main formula can be specialized to different classes of surfaces, giving, in particular, formulas for areas of graphs "z" = "f"("x","y") and surfaces of revolution.

One of the subtleties of surface area, as compared to arc length of curves, is that surface area cannot be defined simply as the limit of areas of polyhedral shapes approximating a given smooth surface. It was demonstrated by Hermann Schwarz that already for the cylinder, different choices of approximating flat surfaces can lead to different limiting values of the area; this example is known as the Schwarz lantern.

Various approaches to a general definition of surface area were developed in the late nineteenth and the early twentieth century by Henri Lebesgue and Hermann Minkowski. While for piecewise smooth surfaces there is a unique natural notion of surface area, if a surface is very irregular, or rough, then it may not be possible to assign an area to it at all. A typical example is given by a surface with spikes spread throughout in a dense fashion. Many surfaces of this type occur in the study of fractals. Extensions of the notion of area which partially fulfill its function and may be defined even for very badly irregular surfaces are studied in geometric measure theory. A specific example of such an extension is the Minkowski content of the surface.

The below given formulas can be used to show that the surface area of a sphere and cylinder of the same radius and height are in the ratio 2 : 3, as follows.

Let the radius be "r" and the height be "h" (which is 2"r" for the sphere).

formula_7

The discovery of this ratio is credited to Archimedes.
Surface area is important in chemical kinetics. Increasing the surface area of a substance generally increases the rate of a chemical reaction. For example, iron in a fine powder will combust, while in solid blocks it is stable enough to use in structures. For different applications a minimal or maximal surface area may be desired.
The surface area of an organism is important in several considerations, such as regulation of body temperature and digestion. Animals use their teeth to grind food down into smaller particles, increasing the surface area available for digestion. The epithelial tissue lining the digestive tract contains microvilli, greatly increasing the area available for absorption. Elephants have large ears, allowing them to regulate their own body temperature. In other instances, animals will need to minimize surface area; for example, people will fold their arms over their chest when cold to minimize heat loss.

The surface area to volume ratio (SA:V) of a cell imposes upper limits on size, as the volume increases much faster than does the surface area, thus limiting the rate at which substances diffuse from the interior across the cell membrane to interstitial spaces or to other cells. Indeed, representing a cell as an idealized sphere of radius "r", the volume and surface area are, respectively, "V" = 4/3 π "r"; "SA" = 4 π "r". The resulting surface area to volume ratio is therefore 3/"r". Thus, if a cell has a radius of 1 μm, the SA:V ratio is 3; whereas if the radius of the cell is instead 10 μm, then the SA:V ratio becomes 0.3. With a cell radius of 100, SA:V ratio is 0.03. Thus, the surface area falls off steeply with increasing volume.




</doc>
<doc id="27727" url="https://en.wikipedia.org/wiki?curid=27727" title="Solid state">
Solid state

Solid state, or solid matter, is one of the four fundamental states of matter.

Solid state may also refer to:






</doc>
<doc id="27730" url="https://en.wikipedia.org/wiki?curid=27730" title="Serbo-Croatian">
Serbo-Croatian

Serbo-Croatian – also called Serbo-Croat , Serbo-Croat-Bosnian (SCB), Bosnian-Croatian-Serbian (BCS), and Bosnian-Croatian-Montenegrin-Serbian (BCMS) – is a South Slavic language and the primary language of Serbia, Croatia, Bosnia and Herzegovina, and Montenegro. It is a pluricentric language with four mutually intelligible standard varieties, namely Serbian, Croatian, Bosnian and Montenegrin.

South Slavic languages historically formed a continuum. The turbulent history of the area, particularly due to expansion of the Ottoman Empire, resulted in a patchwork of dialectal and religious differences. Due to population migrations, Shtokavian became the most widespread language in the western Balkans, intruding westwards into the area previously occupied by Chakavian and Kajkavian (which further blend into Slovenian in the northwest). Bosniaks, Croats and Serbs differ in religion and were historically often part of different cultural circles, although a large part of the nations have lived side by side under foreign overlords. During that period, the language was referred to under a variety of names, such as "Slavic" in general or "Serbian", "Croatian" or "Bosnian" in particular. In a classicizing manner, it was also referred to as "Illyrian".

The process of linguistic standardization of Serbo-Croatian was originally initiated in the mid-19th-century Vienna Literary Agreement by Croatian and Serbian writers and philologists, decades before a Yugoslav state was established. From the very beginning, there were slightly different literary Serbian and Croatian standards, although both were based on the same dialect of Shtokavian, Eastern Herzegovinian. In the 20th century, Serbo-Croatian served as the official language of the Kingdom of Yugoslavia (when it was called "Serbo-Croato-Slovenian"), and later as one of the official languages of the Socialist Federal Republic of Yugoslavia. The breakup of Yugoslavia affected language attitudes, so that social conceptions of the language separated on ethnic and political lines. Since the breakup of Yugoslavia, Bosnian has likewise been established as an official standard in Bosnia and Herzegovina, and there is an ongoing movement to codify a separate Montenegrin standard. 

Like other South Slavic languages, Serbo-Croatian has a simple phonology, with the common five-vowel system and twenty-five consonants. Its grammar evolved from Common Slavic, with complex inflection, preserving seven grammatical cases in nouns, pronouns, and adjectives. Verbs exhibit imperfective or perfective aspect, with a moderately complex tense system. Serbo-Croatian is a pro-drop language with flexible word order, subject–verb–object being the default. It can be written in Serbian Cyrillic or Gaj's Latin alphabet, whose thirty letters mutually map one-to-one, and the orthography is highly phonemic in all standards.

Serbo-Croatian generally goes by the individual names Serbian, Croatian, Bosnian, and sometimes Montenegrin and Bunjevac.

In the language itself, it is typically known as / "Serbo-Croatian", / "Croato-Serbian", or simply / "ours".

Throughout the history of the South Slavs, the vernacular, literary, and written languages (e.g. Chakavian, Kajkavian, Shtokavian) of the various regions and ethnicities developed and diverged independently. Prior to the 19th century, they were collectively called "Illyric", "Slavic", "Slavonian", "Bosnian", "Dalmatian", "Serbian" or "Croatian". Since the nineteenth century the term "Illyrian" or "Illyric" was used quite often (thus creating confusion with the Illyrian language). Although the word "Illyrian" was used on a few occasions before, its widespread usage began after Ljudevit Gaj and several other prominent linguists met at Ljudevit Vukotinović's house to discuss the issue in 1832. The term "Serbo-Croatian" was first used by Jacob Grimm in 1824, popularized by the Viennese philologist Jernej Kopitar in the following decades, and accepted by Croatian Zagreb grammarians in 1854 and 1859. At that time, Serb and Croat lands were still part of the Ottoman and Austrian Empires. Officially, the language was called variously "Serbo-Croat, Croato-Serbian, Serbian and Croatian, Croatian and Serbian, Serbian or Croatian, Croatian or Serbian." Unofficially, Serbs and Croats typically called the language "Serbian" or "Croatian", respectively, without implying a distinction between the two, and again in independent Bosnia and Herzegovina, "Bosnian", "Croatian", and "Serbian" were considered to be three names of a single official language. Croatian linguist Dalibor Brozović advocated the term "Serbo-Croatian" as late as 1988, claiming that in an analogy with Indo-European, Serbo-Croatian does not only name the two components of the same language, but simply charts the limits of the region in which it is spoken and includes everything between the limits (‘Bosnian’ and ‘Montenegrin’). Today, use of the term "Serbo-Croatian" is controversial due to the prejudice that nation and language must match. It is still used for lack of a succinct alternative, though alternative names have emerged, such as "Bosnian/Croatian/Serbian" (BCS), which is often seen in political contexts such as the International Criminal Tribunal for the former Yugoslavia.

In 9th Century, Old Church Slavonic was adopted as the language of the liturgy in churches serving various Slavic nations. This language was gradually adapted to non-liturgical purposes and became known as the Croatian version of Old Slavonic. The two variants of the language, liturgical and non-liturgical, continued to be a part of the Glagolitic service as late as the middle of the 19th century. The earliest known Croatian Church Slavonic Glagolitic manuscripts are the "Glagolita Clozianus" and the "Vienna Folia" from the 11th century.

The beginning of written Serbo-Croatian can be traced from the 10th century and on when Serbo-Croatian medieval texts were written in five scripts: Latin, Glagolitic, Early Cyrillic, Bosnian Cyrillic ("bosančica/bosanica"), and Arebica, the last principally by Bosniak nobility. Serbo-Croatian competed with the more established literary languages of Latin and Old Slavonic in the west and Persian and Arabic in the east.

Old Slavonic developed into the Serbo-Croatian variant of Church Slavonic between the 12th and 16th centuries.

Among the earliest attestations of Serbo-Croatian are the Humac tablet, dating from the 10th or 11th century, written in Bosnian Cyrillic and Glagolitic; the Plomin tablet, dating from the same era, written in Glagolitic; the Valun tablet, dated to the 11th century, written in Glagolitic and Latin; and the Inscription of Župa Dubrovačka, a Glagolitic tablet dated to the 11th century.

The Baška tablet from the late 11th century was written in Glagolitic. It is a large stone tablet found in the small Church of St. Lucy, Jurandvor on the Croatian island of Krk that contains text written mostly in Chakavian in the Croatian script. It is also important in the history of the nation as it mentions Zvonimir, the king of Croatia at the time.

The Charter of Ban Kulin of 1189, written by Ban Kulin of Bosnia, was an early Shtokavian text, written in Bosnian Cyrillic.

The luxurious and ornate representative texts of Serbo-Croatian Church Slavonic belong to the later era, when they coexisted with the Serbo-Croatian vernacular literature. The most notable are the "Missal of Duke Novak" from the Lika region in northwestern Croatia (1368), "Evangel from Reims" (1395, named after the town of its final destination), Hrvoje's Missal from Bosnia and Split in Dalmatia (1404), and the first printed book in Serbo-Croatian, the Glagolitic Missale Romanum Glagolitice (1483).

During the 13th century Serbo-Croatian vernacular texts began to appear, the most important among them being the "Istrian land survey" of 1275 and the "Vinodol Codex" of 1288, both written in the Chakavian dialect.

The Shtokavian dialect literature, based almost exclusively on Chakavian original texts of religious provenance (missals, breviaries, prayer books) appeared almost a century later. The most important purely Shtokavian vernacular text is the Vatican Croatian Prayer Book (c. 1400).

Both the language used in legal texts and that used in Glagolitic literature gradually came under the influence of the vernacular, which considerably affected its phonological, morphological, and lexical systems. From the 14th and the 15th centuries, both secular and religious songs at church festivals were composed in the vernacular.

Writers of early Serbo-Croatian religious poetry ("začinjavci") gradually introduced the vernacular into their works. These "začinjavci" were the forerunners of the rich literary production of the 16th-century literature, which, depending on the area, was Chakavian-, Kajkavian-, or Shtokavian-based. The language of religious poems, translations, miracle and morality plays contributed to the popular character of medieval Serbo-Croatian literature.

One of the earliest dictionaries, also in the Slavic languages as a whole, was the "Bosnian–Turkish Dictionary" of 1631 authored by Muhamed Hevaji Uskufi and was written in the Arebica script.

In the mid-19th century, Serbian (led by self-taught writer and folklorist Vuk Stefanović Karadžić) and most Croatian writers and linguists (represented by the Illyrian movement and led by Ljudevit Gaj and Đuro Daničić), proposed the use of the most widespread dialect, Shtokavian, as the base for their common standard language. Karadžić standardised the Serbian Cyrillic alphabet, and Gaj and Daničić standardized the Croatian Latin alphabet, on the basis of vernacular speech phonemes and the principle of phonological spelling. In 1850 Serbian and Croatian writers and linguists signed the Vienna Literary Agreement, declaring their intention to create a unified standard. Thus a complex bi-variant language appeared, which the Serbs officially called "Serbo-Croatian" or "Serbian or Croatian" and the Croats "Croato-Serbian", or "Croatian or Serbian". Yet, in practice, the variants of the conceived common literary language served as different literary variants, chiefly differing in lexical inventory and stylistic devices. The common phrase describing this situation was that Serbo-Croatian or "Croatian or Serbian" was a single language. During the Austro-Hungarian occupation of Bosnia and Herzegovina, the language of all three nations was called "Bosnian" until the death of administrator von Kállay in 1907, at which point the name was changed to "Serbo-Croatian".

With unification of the first the Kingdom of the Serbs, Croats, and Slovenes – the approach of Karadžić and the Illyrians became dominant. The official language was called "Serbo-Croato-Slovenian" ("srpsko-hrvatsko-slovenački") in the 1921 constitution. In 1929, the constitution was suspended, and the country was renamed the Kingdom of Yugoslavia, while the official language of Serbo-Croato-Slovene was reinstated in the 1931 constitution.

In June 1941, the Nazi puppet Independent State of Croatia began to rid the language of "Eastern" (Serbian) words, and shut down Serbian schools.

On January 15, 1944, the Anti-Fascist Council of the People's Liberation of Yugoslavia (AVNOJ) declared Croatian, Serbian, Slovene, and Macedonian to be equal in the entire territory of Yugoslavia. In 1945 the decision to recognize Croatian and Serbian as separate languages was reversed in favor of a single Serbo-Croatian or Croato-Serbian language. In the Communist-dominated second Yugoslavia, ethnic issues eased to an extent, but the matter of language remained blurred and unresolved.

In 1954, major Serbian and Croatian writers, linguists and literary critics, backed by Matica srpska and Matica hrvatska signed the Novi Sad Agreement, which in its first conclusion stated: "Serbs, Croats and Montenegrins share a single language with two equal variants that have developed around Zagreb (western) and Belgrade (eastern)". The agreement insisted on the equal status of Cyrillic and Latin scripts, and of Ekavian and Ijekavian pronunciations. It also specified that "Serbo-Croatian" should be the name of the language in official contexts, while in unofficial use the traditional "Serbian" and "Croatian" were to be retained. Matica hrvatska and Matica srpska were to work together on a dictionary, and a committee of Serbian and Croatian linguists was asked to prepare a "pravopis". During the sixties both books were published simultaneously in Ijekavian Latin in Zagreb and Ekavian Cyrillic in Novi Sad. Yet Croatian linguists claim that it was an act of unitarianism. The evidence supporting this claim is patchy: Croatian linguist Stjepan Babić complained that the television transmission from Belgrade always used the Latin alphabet— which was true, but was not proof of unequal rights, but of frequency of use and prestige. Babić further complained that the Novi Sad Dictionary (1967) listed side by side words from both the Croatian and Serbian variants wherever they differed, which one can view as proof of careful respect for both variants, and not of unitarism. Moreover, Croatian linguists criticized those parts of the Dictionary for being unitaristic that were written by Croatian linguists. And finally, Croatian linguists ignored the fact that the material for the "Pravopisni rječnik" came from the Croatian Philological Society. Regardless of these facts, Croatian intellectuals brought the Declaration on the Status and Name of the Croatian Literary Language in 1967. On occasion of the publication's 45th anniversary, the Croatian weekly journal "Forum" published the Declaration again in 2012, accompanied by a critical analysis.

West European scientists judge the Yugoslav language policy as an exemplary one: although three-quarters of the population spoke one language, no single language was official on a federal level. Official languages were declared only at the level of constituent republics and provinces, and very generously: Vojvodina had five (among them Slovak and Romanian, spoken by 0.5 per cent of the population), and Kosovo four (Albanian, Turkish, Romany and Serbo-Croatian). Newspapers, radio and television studios used sixteen languages, fourteen were used as languages of tuition in schools, and nine at universities. Only the Yugoslav Army used Serbo-Croatian as the sole language of command, with all other languages represented in the army's other activities—however, this is not different from other armies of multilingual states, or in other specific institutions, such as international air traffic control where English is used worldwide. All variants of Serbo-Croatian were used in state administration and republican and federal institutions. Both Serbian and Croatian variants were represented in respectively different grammar books, dictionaries, school textbooks and in books known as pravopis (which detail spelling rules). Serbo-Croatian was a kind of soft standardisation. However, legal equality could not dampen the prestige Serbo-Croatian had: since it was the language of three quarters of the population, it functioned as an unofficial lingua franca. And within Serbo-Croatian, the Serbian variant, with twice as many speakers as the Croatian, enjoyed greater prestige, reinforced by the fact that Slovene and Macedonian speakers preferred it to the Croatian variant because their languages are also Ekavian. This is a common situation in other pluricentric languages, e.g. the variants of German differ according to their prestige, the variants of Portuguese too. Moreover, all languages differ in terms of prestige: "the fact is that languages (in terms of prestige, learnability etc.) are not equal, and the law cannot make them equal".

In 2017, the "Declaration on the Common Language" ("Deklaracija o zajedničkom jeziku"), signed by a group of NGOs and linguists from former Yugoslavia, argues that all variants belong to a common polycentric language.

The total number of persons who declared their native language as either 'Bosnian', 'Croatian', 'Serbian', 'Montenegrin', or 'Serbo-Croatian' in countries of the region is about 16 million.

Serbian is spoken by about 9.5 million, mostly in Serbia (6.7m), Bosnia and Herzegovina (1.4m), and Montenegro (0.4m). Serbian minorities are found in North Macedonia and in Romania. In Serbia, there are about 760,000 second-language speakers of Serbian, including Hungarians in Vojvodina and the 400,000 estimated Roma. Familiarity of Kosovo Albanians with Serbian in Kosovo varies depending on age and education, and exact numbers are not available.

Croatian is spoken by roughly 4.8 million, including some 575,000 in Bosnia and Herzegovina. A small Croatian minority that lives in Italy, known as Molise Croats, have somewhat preserved traces of the Croatian language. In Croatia, 170,000, mostly Italians and Hungarians, use it as a second language.

Bosnian is spoken by 2.2 million people, chiefly Bosniaks, including about 220,000 in Serbia and Montenegro.

The notion of Montenegrin as a separate standard from Serbian is relatively recent. In the 2003 census, around 150,000 Montenegrins, of the country's 620,000, declared Montenegrin as their native language. That figure is likely to increase, due to the country's independence and strong institutional backing of the Montenegrin language.

Serbo-Croatian is also a second language of many Slovenians and Macedonians, especially those born during the time of Yugoslavia. According to the 2002 Census, Serbo-Croatian and its variants have the largest number of speakers of the minority languages in Slovenia.

Outside the Balkans, there are over 2 million native speakers of the language(s), especially in countries which are frequent targets of immigration, such as Australia, Austria, Brazil, Canada, Chile, Germany, Hungary, Italy, Sweden, and the United States.

Serbo-Croatian is a highly inflected language. Traditional grammars list seven cases for nouns and adjectives: nominative, genitive, dative, accusative, vocative, locative, and instrumental, reflecting the original seven cases of Proto-Slavic, and indeed older forms of Serbo-Croatian itself. However, in modern Shtokavian the locative has almost merged into dative (the only difference is based on accent in some cases), and the other cases can be shown declining; namely:

Like most Slavic languages, there are mostly three genders for nouns: masculine, feminine, and neuter, a distinction which is still present even in the plural (unlike Russian and, in part, the Čakavian dialect). They also have two numbers: singular and plural. However, some consider there to be three numbers (paucal or "dual," too), since (still preserved in closely related Slovene) after two ("dva", "dvije"/"dve"), three ("tri") and four ("četiri"), and all numbers ending in them (e.g. twenty-two, ninety-three, one hundred four) the genitive singular is used, and after all other numbers five ("pet") and up, the genitive plural is used. (The number one ["jedan"] is treated as an adjective.) Adjectives are placed in front of the noun they modify and must agree in both case and number with it.

There are seven tenses for verbs: past, present, future, exact future, aorist, imperfect, and pluperfect; and three moods: indicative, imperative, and conditional. However, the latter three tenses are typically used only in Shtokavian writing, and the time sequence of the exact future is more commonly formed through an alternative construction.

In addition, like most Slavic languages, the Shtokavian verb also has one of two aspects: perfective or imperfective. Most verbs come in pairs, with the perfective verb being created out of the imperfective by adding a prefix or making a stem change. The imperfective aspect typically indicates that the action is unfinished, in progress, or repetitive; while the perfective aspect typically denotes that the action was completed, instantaneous, or of limited duration. Some Štokavian tenses (namely, aorist and imperfect) favor a particular aspect (but they are rarer or absent in Čakavian and Kajkavian). Actually, aspects "compensate" for the relative lack of tenses, because aspect of the verb determines whether the act is completed or in progress in the referred time.

The Serbo-Croatian vowel system is simple, with only five vowels in Shtokavian. All vowels are monophthongs. The oral vowels are as follows:

The vowels can be short or long, but the phonetic quality does not change depending on the length. In a word, vowels can be long in the stressed syllable and the syllables following it, never in the ones preceding it.

The consonant system is more complicated, and its characteristic features are series of affricate and palatal consonants. As in English, voice is phonemic, but aspiration is not.

In consonant clusters all consonants are either voiced or voiceless. All the consonants are voiced if the last consonant is normally voiced or voiceless if the last consonant is normally voiceless. This rule does not apply to approximantsa consonant cluster may contain voiced approximants and voiceless consonants; as well as to foreign words ("Washington" would be transcribed as "VašinGton"), personal names and when consonants are not inside of one syllable.

Apart from Slovene, Serbo-Croatian is the only Slavic language with a pitch accent (simple tone) system. This feature is present in some other Indo-European languages, such as Swedish, Norwegian, and Ancient Greek. Neo-Shtokavian Serbo-Croatian, which is used as the basis for standard Bosnian, Croatian, Montenegrin, and Serbian, has four "accents", which involve either a rising or falling tone on either long or short vowels, with optional post-tonic lengths:

The tone stressed vowels can be approximated in English with "set" vs. "setting?" said in isolation for a short tonic "e," or "leave" vs. "leaving?" for a long tonic "i," due to the prosody of final stressed syllables in English.

General accent rules in the standard language:


There are no other rules for accent placement, thus the accent of every word must be learned individually; furthermore, in inflection, accent shifts are common, both in type and position (the so-called "mobile paradigms"). The second rule is not strictly obeyed, especially in borrowed words.

Comparative and historical linguistics offers some clues for memorising the accent position: If one compares many standard Serbo-Croatian words to e.g. cognate Russian words, the accent in the Serbo-Croatian word will be one syllable before the one in the Russian word, with the rising tone. Historically, the rising tone appeared when the place of the accent shifted to the preceding syllable (the so-called "Neoshtokavian retraction"), but the quality of this new accent was different – its melody still "gravitated" towards the original syllable. Most Shtokavian dialects (Neoshtokavian) dialects underwent this shift, but Chakavian, Kajkavian and the Old Shtokavian dialects did not.

Accent diacritics are not used in the ordinary orthography, but only in the linguistic or language-learning literature (e.g. dictionaries, orthography and grammar books). However, there are very few minimal pairs where an error in accent can lead to misunderstanding.

Serbo-Croatian orthography is almost entirely phonetic. Thus, most words should be spelled as they are pronounced. In practice, the writing system does not take into account allophones which occur as a result of interaction between words:

Also, there are some exceptions, mostly applied to foreign words and compounds, that favor morphological/etymological over phonetic spelling:

One systemic exception is that the consonant clusters ds and dš are not respelled as ts and tš (although "d" tends to be unvoiced in normal speech in such clusters):

Only a few words are intentionally "misspelled", mostly in order to resolve ambiguity:

Through history, this language has been written in a number of writing systems:

The oldest texts since the 11th century are in Glagolitic, and the oldest preserved text written completely in the Latin alphabet is , from 1345. The Arabic alphabet had been used by Bosniaks; Greek writing is out of use there, and Arabic and Glagolitic persisted so far partly in religious liturgies.

Today, it is written in both the Latin and Cyrillic scripts. Serbian and Bosnian variants use both alphabets, while Croatian uses the Latin only.

Latin script has become more and more popular in Serbia, as it is easy to input on phones and computers.

The Serbian Cyrillic alphabet was revised by Vuk Stefanović Karadžić in the 19th century.

The Croatian Latin alphabet () followed suit shortly afterwards, when Ljudevit Gaj defined it as standard Latin with five extra letters that had diacritics, apparently borrowing much from Czech, but also from Polish, and inventing the unique digraphs , and . These digraphs are represented as , and respectively in the , published by the former Yugoslav Academy of Sciences and Arts in Zagreb. The latter digraphs, however, are unused in the literary standard of the language. All in all, this makes Serbo-Croatian the only Slavic language to officially use both the Latin and Cyrillic scripts, albeit the Latin version is more commonly used.

In both cases, spelling is phonetic and spellings in the two alphabets map to each other one-to-one:

The digraphs "Lj", "Nj" and "Dž" represent distinct phonemes and are considered to be single letters. In crosswords, they are put into a single square, and in sorting, lj follows l and nj follows n, except in a few words where the individual letters are pronounced separately. For instance, "nadživ(j)eti" "to outlive" is composed of the prefix "out, over" and the verb "to live". The Cyrillic alphabet avoids such ambiguity by providing a single letter for each phoneme: .

"Đ" used to be commonly written as "Dj" on typewriters, but that practice led to too many ambiguities. It is also used on car license plates. Today "Dj" is often used again in place of "Đ" on the Internet as a replacement due to the lack of installed Serbo-Croat keyboard layouts.

Unicode has separate characters for the digraphs lj (Ǉ, ǈ, ǉ), nj (Ǌ, ǋ, ǌ) and dž (Ǆ, ǅ, ǆ).

South Slavic historically formed a dialect continuum, i.e. each dialect has some similarities with the neighboring one, and differences grow with distance. However, migrations from the 16th to 18th centuries resulting from the spread of Ottoman Empire on the Balkans have caused large-scale population displacement that broke the dialect continuum into many geographical pockets. Migrations in the 20th century, primarily caused by urbanization and wars, also contributed to the reduction of dialectal differences.

The primary dialects are named after the most common question word for "what": Shtokavian uses the pronoun "što" or "šta", Chakavian uses "ča" or "ca", Kajkavian ("kajkavski"), "kaj" or "kej". In native terminology they are referred to as "nar(j)ečje", which would be equivalent of "group of dialects", whereas their many subdialects are referred to as "dijalekti ""dialects" or "govori ""speeches".

The pluricentric Serbo-Croatian standard language and all four contemporary standard variants are based on the Eastern Herzegovinian subdialect of Neo-Shtokavian. Other dialects are not taught in schools or used by the state media. The Torlakian dialect is often added to the list, though sources usually note that it is a transitional dialect between Shtokavian and the Bulgaro-Macedonian dialects.

The Serbo-Croatian dialects differ not only in the question word they are named after, but also heavily in phonology, accentuation and intonation, case endings and tense system (morphology) and basic vocabulary. In the past, Chakavian and Kajkavian dialects were spoken on a much larger territory, but have been replaced by Štokavian during the period of migrations caused by Ottoman Turkish conquest of the Balkans in the 15th and the 16th centuries. These migrations caused the koinéisation of the Shtokavian dialects, that used to form the West Shtokavian (more closer and transitional towards the neighbouring Chakavian and Kajkavian dialects) and East Shtokavian (transitional towards the Torlakian and the whole Bulgaro-Macedonian area) dialect bundles, and their subsequent spread at the expense of Chakavian and Kajkavian. As a result, Štokavian now covers an area larger than all the other dialects combined, and continues to make its progress in the enclaves where non-literary dialects are still being spoken.

The differences among the dialects can be illustrated on the example of Schleicher's fable. Diacritic signs are used to show the difference in accents and prosody, which are often quite significant, but which are not reflected in the usual orthography.

A basic distinction among the dialects is in the reflex of the long Common Slavic vowel "jat", usually transcribed as *ě. Depending on the reflex, the dialects are divided into Ikavian, Ekavian, and Ijekavian, with the reflects of "jat" being /i/, /e/, and /ije/ or /je/ respectively. The long and short "jat" is reflected as long or short */i/ and /e/ in Ikavian and Ekavian, but Ijekavian dialects introduce a "ije"/"je" alternation to retain a distinction.

Standard Croatian and Bosnian are based on Ijekavian, whereas Serbian uses both Ekavian and Ijekavian forms (Ijekavian for Bosnian Serbs, Ekavian for most of Serbia). Influence of standard language through state media and education has caused non-standard varieties to lose ground to the literary forms.

The jat-reflex rules are not without exception. For example, when short "jat" is preceded by "r", in most Ijekavian dialects developed into /re/ or, occasionally, /ri/. The prefix "prě-" ("trans-, over-") when long became "pre-" in eastern Ijekavian dialects but to "prije-" in western dialects; in Ikavian pronunciation, it also evolved into "pre-" or "prije-" due to potential ambiguity with "pri-" ("approach, come close to"). For verbs that had "-ěti " in their infinitive, the past participle ending "-ěl" evolved into "-io" in Ijekavian Neoštokavian.

The following are some examples:

The nature and classification of Serbo-Croatian has been the subject of long-standing sociolinguistic debate. The question is whether Serbo-Croatian should be called a single language or a cluster of closely related languages.

Enisa Kafadar argues that there is only one Serbo-Croatian language with several varieties. This has made it possible to include all four varieties in new grammars of the language. Daniel Bunčić concludes that it is a pluricentric language, with four standard variants spoken in Serbia, Croatia, Montenegro, and Bosnia-Herzegovina. The mutual intelligibility between their speakers "exceeds that between the standard variants of English, French, German, or Spanish". "There is no doubt of the near 100% mutual intelligibility of (standard) Croatian and (standard) Serbian, as is obvious from the ability of all groups to enjoy each others’ films, TV and sports broadcasts, newspapers, rock lyrics etc." Other linguists have argued that the differences between the variants of Serbo-Croatian are less significant than those between the variants of English, German, Dutch, and Hindustani.

Among pluricentric languages, Serbo-Croatian was the only one with a pluricentric standardisation within one state. The dissolution of Yugoslavia has made Serbo-Croatian even more of a typical pluricentric language, since the variants of other pluricentric languages are also spoken in different states.

As in other pluricentric languages, all Serbo-Croatian standard varieties are based on the same dialect (the Eastern Herzegovinian subdialect of the Shtokavian dialect) and consequently, according to the sociolinguistic definitions, constitute a single pluricentric language (and not, for example, several Ausbau languages). "An examination of all the major 'levels' of language shows that BCS is clearly a single language with a single grammatical system."

In 2017, numerous prominent writers, scientists, journalists, activists and other public figures from Croatia, Bosnia-Herzegovina, Montenegro and Serbia signed the Declaration on the Common Language, which states that in Croatia, Serbia, Bosnia-Herzegovina and Montenegro a common polycentric standard language is used, consisting of several standard varieties, such as German, English or Spanish.

The use of "Serbo-Croatian" as a linguistic label has been the subject of long-standing controversy. Wayles Browne calls it a "term of convenience" and notes the difference of opinion as to whether it comprises a single language or a cluster of languages. Ronelle Alexander refers to the national standards as three separate languages, but also notes that the reasons for this are complex and generally non-linguistic. She calls BCS (her term for Serbo-Croatian) a single language for communicative linguistic purposes, but three separate languages for symbolic non-linguistic purposes.

The current Serbian constitution of 2006 refers to the official language as "Serbian", while the Montenegrin constitution of 2007 proclaimed "Montenegrin" as the primary official language, but also grants other languages the right of official use.

The International Organization for Standardization (ISO) has specified different Universal Decimal Classification (UDC) numbers for Croatian "(UDC 862," abbreviation hr) and Serbian "(UDC 861", abbreviation sr), while the cover term "Serbo-Croatian" is used to refer to the combination of original signs ("UDC 861/862," abbreviation sh). Furthermore, the "ISO 639" standard designates the Bosnian language with the abbreviations bos and bs.

While it operated, the International Criminal Tribunal for the former Yugoslavia, which had English and French as official languages, translated court proceedings and documents into what it referred to as "Bosnian/Croatian/Serbian", usually abbreviated as BCS. Translators were employed from all regions of the former Yugoslavia and all national and regional variations were accepted, regardless of the nationality of the person on trial (sometimes against a defendant's objections), on the grounds of mutual intelligibility.

For utilitarian purposes, the Serbo-Croatian language is often called ""naš jezik" ("our language") or ""naški" (sic. "Ourish" or "Ourian") by native speakers. This term is frequently used to describe the Serbo-Croatian language by those who wish to avoid nationalistic and linguistic discussions. Native speakers traditionally describe their language as ""jedan ali ne jedinstven"—"one but not uniform".

The majority of mainstream Serbian linguists consider Serbian and Croatian to be one language, that is called Serbo-Croatian ("srpskohrvatski") or Croato-Serbian ("hrvatskosrpski"). A minority of Serbian linguists are of the opinion that Serbo-Croatian did exist, but has, in the meantime, dissolved.

The opinion of the majority of Croatian linguists is that there has never been a Serbo-Croatian language, but two different standard languages that overlapped sometime in the course of history. However, Croatian linguist Snježana Kordić has been leading an academic discussion on this issue in the Croatian journal "Književna republika" from 2001 to 2010. In the discussion, she shows that linguistic criteria such as mutual intelligibility, the huge overlap in the linguistic system, and the same dialect basis of the standard language are evidence that Croatian, Serbian, Bosnian and Montenegrin are four national variants of the pluricentric Serbo-Croatian language. Igor Mandić states: "During the last ten years, it has been the longest, the most serious and most acrid discussion (…) in 21st-century Croatian culture". Inspired by that discussion, a monograph on language and nationalism has been published.

The view of the majority of Croatian linguists that there is no single Serbo-Croatian language but several different standard languages has been sharply criticized by German linguist Bernhard Gröschel in his monograph "Serbo-Croatian Between Linguistics and Politics".

A more detailed overview, incorporating arguments from Croatian philology and contemporary linguistics, would be as follows:

The linguistic debate in this region is more about politics than about linguistics per se.

The topic of language for writers from Dalmatia and Dubrovnik prior to the 19th century made a distinction only between speakers of Italian or Slavic, since those were the two main groups that inhabited Dalmatian city-states at that time. Whether someone spoke Croatian or Serbian was not an important distinction then, as the two languages were not distinguished by most speakers.

However, most intellectuals and writers from Dalmatia who used the Štokavian dialect and practiced the Catholic faith saw themselves as part of a Croatian nation as far back as the mid-16th to 17th centuries, some 300 years before Serbo-Croatian ideology appeared. Their loyalty was first and foremost to Catholic Christendom, but when they professed an ethnic identity, they referred to themselves as "Slovin" and "Illyrian" (a sort of forerunner of Catholic baroque pan-Slavism) and Croatthese 30-odd writers over the span of c. 350 years always saw themselves as Croats first and never as part of a Serbian nation. It should also be noted that, in the pre-national era, Catholic religious orientation did not necessarily equate with Croat ethnic identity in Dalmatia. A Croatian follower of Vuk Karadžić, Ivan Broz, noted that for a Dalmatian to identify oneself as a Serb was seen as foreign as identifying oneself as Macedonian or Greek. Vatroslav Jagić pointed out in 1864:

On the other hand, the opinion of Jagić from 1864 is argued not to have firm grounds. When Jagić says "Croatian", he refers to a few cases referring to the Dubrovnik vernacular as "ilirski" (Illyrian). This was a common name for all Slavic vernaculars in Dalmatian cities among the Roman inhabitants. In the meantime, other written monuments are found that mention "srpski", "lingua serviana" (= Serbian), and some that mention Croatian. By far the most competent Serbian scientist on the Dubrovnik language issue, Milan Rešetar, who was born in Dubrovnik himself, wrote behalf of language characteristics: "The one who thinks that Croatian and Serbian are two separate languages must confess that Dubrovnik always (linguistically) used to be Serbian."

Finally, the former "medieval" texts from Dubrovnik and Montenegro dating before the 16th century were neither true Štokavian nor Serbian, but mostly specific a Jekavian-Čakavian that was nearer to actual Adriatic islanders in Croatia.

Nationalists have conflicting views about the language(s). The nationalists among the Croats conflictingly claim either that they speak an entirely separate language from Serbs and Bosniaks or that these two peoples have, due to the longer lexicographic tradition among Croats, somehow "borrowed" their standard languages from them. Bosniak nationalists claim that both Croats and Serbs have "appropriated" the Bosnian language, since Ljudevit Gaj and Vuk Karadžić preferred the Neoštokavian-Ijekavian dialect, widely spoken in Bosnia and Herzegovina, as the basis for language standardization, whereas the nationalists among the Serbs claim either that any divergence in the language is artificial, or claim that the Štokavian dialect is theirs and the Čakavian Croats'— in more extreme formulations Croats have "taken" or "stolen" their language from the Serbs. 

Proponents of unity among Southern Slavs claim that there is a single language with normal dialectal variations. The term "Serbo-Croatian" (or synonyms) is not officially used in any of the successor countries of former Yugoslavia.

In Serbia, the Serbian standard has an official status countrywide, while both Serbian and Croatian are official in the province of Vojvodina. A large Bosniak minority is present in the southwest region of Sandžak, but the "official recognition" of Bosnian is moot. Bosnian is an optional course in 1st and 2nd grade of the elementary school, while it is also in official use in the municipality of Novi Pazar. However, its nomenclature is controversial, as there is incentive that it is referred to as "Bosniak" ("bošnjački") rather than "Bosnian" ("bosanski") (see Bosnian language#Controversy and recognition for details).

Croatian is the official language of Croatia, while Serbian is also official in municipalities with significant Serb population.

In Bosnia and Herzegovina, all three standard languages are recorded as official. Confrontations have on occasion been absurd. The academic Muhamed Filipović, in an interview to Slovenian television, told of a local court in a Croatian district requesting a paid translator to translate from Bosnian to Croatian before the trial could proceed.

The International Criminal Tribunal for the former Yugoslavia referred to the language as "Bosnian/Croatian/Serbian", usually abbreviated as BCS. Translators were employed from all regions of the former Yugoslavia and all national and regional variations were accepted, regardless of the nationality of the person on trial (sometimes against a defendant's objections), on the grounds of mutual intelligibility.

Since the year 2000, the ISO classification only recognizes "Serbo-Croatian" as a 'macrolanguage', since the original codes were removed from the ISO 639-1 and ISO 639-2 standards. That left the ISO 639-3 'macrolanguage' (a book-keeping device in the ISO 639-3 standard to keep track of which ISO 639-3 codes correspond with which ISO 639-2 codes) stranded without a corresponding ISO 639-2 code.





</doc>
<doc id="27737" url="https://en.wikipedia.org/wiki?curid=27737" title="Saint Kitts">
Saint Kitts

Saint Kitts, also known more formally as Saint Christopher Island, is an island in the West Indies. The west side of the island borders the Caribbean Sea, and the eastern coast faces the Atlantic Ocean. Saint Kitts and the neighbouring island of Nevis constitute one country: the Federation of Saint Kitts and Nevis. Saint Kitts and Nevis are separated by a shallow channel known as "The Narrows".

Saint Kitts became home to the first Caribbean British and French colonies in the mid-1620s. Along with the island of Nevis, Saint Kitts was a member of the British West Indies until gaining independence on 19 September 1983.

The island is one of the Leeward Islands in the Lesser Antilles. It is situated about southeast of Miami, Florida, US. The land area of Saint Kitts is about , being approximately long and on average about across.

Saint Kitts has a population of around 40,000, the majority of whom are of African descent. The primary language is English, with a literacy rate of approximately 98%. Residents call themselves Kittitians.

Brimstone Hill Fortress National Park, a UNESCO World Heritage Site, is the largest fortress ever built in the Western Caribbean. The island of Saint Kitts is home to the Warner Park Cricket Stadium, which was used to host 2007 Cricket World Cup matches. This made Saint Kitts and Nevis the smallest nation to ever host a World Cup event. Saint Kitts is also home to several institutions of higher education, including Ross University School of Veterinary Medicine, Windsor University School of Medicine, and the University of Medicine and Health Sciences.

The capital of the two-island nation, and also its largest port, is the town of Basseterre on Saint Kitts. There is a modern facility for handling large cruise ships there. A ring road goes around the perimeter of the island with smaller roads branching off it; the interior of the island is too steep for habitation.

Saint Kitts is away from Sint Eustatius to the north and from Nevis to the south. St. Kitts has three distinct groups of volcanic peaks: the North West or Mount Misery Range; the Middle or Verchilds Range and the South East or Olivees Range. The highest peak is Mount Liamuiga, formerly Mount Misery, a dormant volcano 1,156 m high.

The youngest volcanic center is Mt. Liamuiga, 5 km in diameter and rising to an elevation of 1155 m. Its last eruption was 1620 years ago, corresponding with the Steel Dust series of pyroclastic deposits on the western flank. The Mansion Series of pyroclastic deposits and andesite with basalt layers occur on the northern flank, along with mudflows. This volcano has a crater 900 m wide and 244 m deep, plus two distinct parasitic domes consisting primarily of andesite, Brimstone Hill and Sandy Point Hill which is coalesced with Farm Flat. Brimstone Hill is noted for having limestone on its flanks, which was dragged upward with the formation of the dome 44,400 years ago. Mt. Liamuiga partially overlays the Middle Range to the southeast. This Middle Range is another stratovolcano 976 m in height with a small summit crater containing a lake. Next in line is the 900 m South East Range, 1 Myr in age, consisting of four peaks. Ottley's dome and Monkey Hill dome are on the flanks, while the older volcanoes represented by Canada Hills, and Conaree Hills lie past the airport and Bassaterre on the southeast flank. The Salt Dome Peninsula contains the oldest volcanic deposits, 2.3-2.77 Myr in age, consisting of at least nine Pelean domes rising up to 319 m in height, which includes Williams Hill and St. Anthony's Peak.

During the last Ice Age, the sea level was up to lower and St. Kitts and Nevis were one island along with Saba and Sint Eustatius (also known as Statia).

St. Kitts was originally settled by pre-agricultural, pre-ceramic "Archaic people", who migrated south down the archipelago from Florida. In a few hundred years they disappeared, to be replaced by the ceramic-using and agriculturalist Saladoid people around 100 BC, who migrated to St. Kitts north up the archipelago from the banks of the Orinoco River in Venezuela. Around 800 AD, they were replaced by the Igneri people, members of the Arawak group.

Around 1300, the Kalinago, or Carib people arrived on the islands. These agriculturalists quickly dispersed the Igneri, and forced them northwards to the Greater Antilles. They named Saint Kitts "Liamuiga" meaning "fertile island", and would likely have expanded further north if not for the arrival of Europeans.

A Spanish expedition under Christopher Columbus arrived and claimed the island for Spain in 1493.

The first English colony was established in 1623, followed by a French colony in 1625. The English and French briefly united to massacre the local Kalinago, and then partitioned the island, with the English colonists in the middle and the French on either end. In 1629, a Spanish force sent to clear the islands of foreign settlement seized St. Kitts. The English settlement was rebuilt following the 1630 peace between England and Spain.

The island alternated repeatedly between English (then British) and French control during the 17th and 18th centuries, as one power took the whole island, only to have it switch hands due to treaties or military action. Parts of the island were heavily fortified, as exemplified by the UNESCO World Heritage Site at Brimstone Hill and the now-crumbling Fort Charles.

Since 1783, Saint Kitts has been affiliated with the Kingdom of Great Britain, which became the United Kingdom.

The island originally produced tobacco, but it changed to sugar cane in 1640 due to stiff competition from the colony of Virginia. The labour-intensive cultivation of sugar cane was the reason for the large-scale importation of African slaves. The importation began almost immediately upon the arrival of Europeans to the region.

The purchasing of enslaved Africans was outlawed in the British Empire by an Act of Parliament in 1807. Slavery was abolished by an Act of Parliament which became law on 1 August 1834. This emancipation was followed by four years of forced enslavement, put in place to protect the "planters" (plantation owners) from losing their free labour force.

1 August is now celebrated as a public holiday and is called Emancipation Day. In 1883, Saint Kitts, Nevis, and Anguilla were all linked under one presidency, located on Saint Kitts, to the dismay of the Nevisians and Anguillans. Anguilla eventually separated out of this arrangement, in 1971, after an armed raid on Saint Kitts in 1967.

Sugar production continued to dominate the local economy until 2005, when, after 365 years of having a mono-culture, the government closed the sugar industry. This was due to huge losses and European Union plans to greatly cut sugar prices.

For purposes of governing, the island is divided into nine parishes:

Saint Kitts & Nevis uses the Eastern Caribbean dollar, which maintains a fixed exchange rate of 2.7-to-one with the United States dollar. The US dollar is almost as widely accepted as the Eastern Caribbean dollar.

For hundreds of years, Saint Kitts operated as a sugar monoculture, but due to decreasing profitability, the government closed the industry in 2005. Tourism is a major and growing source of income to the island, although the number and density of resorts is less than on many other Caribbean islands. Transportation, non-sugar agriculture, manufacturing and construction are the other growing sectors of the economy.

Saint Kitts is dependent on tourism to drive its economy. Tourism has been increasing since 1978. In 2009, there were 587,479 arrivals to Saint Kitts compared to 379,473 in 2007, which represents an increase of just under 40% growth in a two-year period. As tourism grows, the demand for vacation property increases in conjunction.

Saint Kitts & Nevis also acquires foreign direct investment from their unique citizenship by investment programme, outlined in their Citizenship Act of 1984. Interested parties can acquire citizenship if they pass the government's strict background checks and make an investment into an approved real estate development. Purchasers who pass government due diligence and make a minimum investment of US$400,000, into qualifying government approved real estate, are entitled to apply for citizenship of the Federation of Saint Kitts and Nevis. Many projects are approved under the citizenship by investment programme, and the main qualifying projects of interest can be found within the Henley Estates market overview .

The country hosts an annual St. Kitts Music Festival.

Robert L. Bradshaw International Airport serves Saint Kitts. 
Daily connections from Charlotte, Miami and New York are available.

The Basseterre Ferry Terminal facilitates travel between Saint Kitts and sister island Nevis.

The narrow-gauge (30 inches) St. Kitts Scenic Railway circles the island and offers passenger service from its headquarters near the airport, although the service is geared more for tourists than as day-to-day transportation for residents. Built between 1912 and 1926 to haul sugar cane from farms to the sugar factory in Basseterre, since 2003 the railway has offered a 3.5-hour, 30-mile circle tour of the island on specially designed double-decker open-air coaches, with 12 miles of the trip being by bus.

Saint Kitts is or was the residence of:





</doc>
<doc id="27739" url="https://en.wikipedia.org/wiki?curid=27739" title="Shogi">
Shogi

Shogi was the earliest chess variant to allow captured pieces to be returned to the board by the capturing player. This drop rule is speculated to have been invented in the 15th century and possibly connected to the practice of 15th century mercenaries switching loyalties when captured instead of being killed.

The earliest predecessor of the game, chaturanga, originated in India in the 6th century, and the game was likely transmitted to Japan via China or Korea sometime after the Nara period. Shogi in its present form was played as early as the 16th century, while a direct ancestor without the drop rule was recorded from 1210 in a historical document "Nichūreki", which is an edited copy of "Shōchūreki" and "Kaichūreki" from the late Heian period (c. 1120).

Two players face each other across a board composed of rectangles in a grid of 9 "ranks" (rows, ) by 9 "files" (columns, ) yielding an 81 square board. In Japanese they are called "Sente" (first player) and "Gote" (second player), but in English are conventionally referred to as Black and White, with Black the first player.
The board is nearly always rectangular, and the rectangles are undifferentiated by marking or color. Pairs of dots mark the players' promotion zones.

Each player has a set of 20 flat wedge-shaped pentagonal pieces of slightly different sizes. Except for the kings, opposing pieces are undifferentiated by marking or color. Pieces face "forward" by having the pointed side of each piece oriented toward the opponent's side – this shows who controls the piece during play. The pieces from largest (most important) to smallest (least important) are:


Several of these names were chosen to correspond to their rough equivalents in international chess, and not as literal translations of the Japanese names.

Each piece has its name written on its surface in the form of two "kanji" (Chinese characters used in Japanese), usually in black ink. On the reverse side of each piece, other than the king and gold general, are one or two other characters, in amateur sets often in a different color (usually red); this side is turned face up during play to indicate that the piece has been promoted.

Following is a table of the pieces with their Japanese representations and English equivalents. The abbreviations are used for game notation and often when referring to the pieces in speech in Japanese.

English speakers sometimes refer to promoted bishops as "horses" and promoted rooks as "dragons", after their Japanese names, and generally use the Japanese term "tokin" for promoted pawns. Silver generals and gold generals are commonly referred to simply as "silvers" and "golds".

The characters inscribed on the reverse sides of the pieces to indicate promotion may be in red ink, and are usually cursive. The characters on the backs of the pieces that promote to gold generals are cursive variants of 'gold', becoming more cursive (more abbreviated) as the value of the original piece decreases. These cursive forms have these equivalents in print: for promoted silver, for promoted knight, for promoted lance, and for promoted pawn (tokin). Another typographic convention has abbreviated versions of the original values, with a reduced number of strokes: for a promoted knight , for a promoted lance , and the as above for a promoted silver, but (a hiragana symbol for the syllable "to") for "tokin".

The suggestion that the Japanese characters have deterred Western players from learning shogi has led to "Westernized" or "international" pieces which use iconic symbols instead of characters. Most players soon learn to recognize the characters, however, partially because the traditional pieces are already iconic by size, with more powerful pieces being larger. As a result, Westernized pieces have never become popular. Bilingual pieces with both Japanese characters and English captions have been developed as have pieces with animal cartoons.

Each player sets up friendly pieces facing forward (toward the opponent).


A "furigoma" 振り駒 'piece toss' is used to decide who moves first. One of the players tosses five pawns. If the number of tokins (promoted pawns, と) facing up is higher than unpromoted pawns (歩), then the player who tossed the pawns plays "gote" 後手 'white' (that is, getting the second move).

After the piece toss "furigoma," the game proceeds. If multiple games are played, then players alternate turns for who goes first in subsequent games. (The terms "Black" and "White" are used to differentiate sides although there is no difference in the color of the pieces.) For each turn, a player may either move a piece that is currently on the board (and potentially promote it, capture an opposing piece, or both) or else drop a piece that has been previously captured onto a square of the board. These options are explained below.

The usual goal of a game is for one player to checkmate the other player's king, winning the game.

Most shogi pieces can move only to an adjacent square. A few may move across the board, and one jumps over intervening pieces.

The lance, bishop, and rook are "ranging" pieces: They can move any number of squares along a straight line limited only by intervening pieces and the edge of the board. If an opposing piece intervenes, it may be captured by removing it from the board and replacing it with the moving piece. If a friendly piece intervenes, the moving piece must stop short of that square; if the friendly piece is adjacent, the moving piece may not move in that direction at all.

A king (玉/王) moves one square in any direction, orthogonal or diagonal.

A rook (飛) moves any number of squares in an orthogonal direction.

A bishop (角) moves any number of squares in a diagonal direction. Because they cannot move orthogonally, the players' unpromoted bishops can reach only half the squares of the board, unless one is captured and then dropped.

A gold general (金) moves one square orthogonally, or one square diagonally forward, giving it six possible destinations. It cannot move diagonally backwards.

A silver general (銀) moves one square diagonally, or one square straight forward, giving it five possible destinations. Because an unpromoted silver can retreat more easily than a promoted one, it is common to leave a silver unpromoted at the far side of the board. (See Promotion).

A knight (桂) jumps at an angle intermediate to orthogonal and diagonal, amounting to one square straight forward plus one square diagonally forward, in a single move. Thus the knight has two possible forward destinations. Unlike international chess knights, shogi knights cannot move to the sides or in a backwards direction. The knight is the only piece that ignores intervening pieces on the way to its destination. It is not blocked from moving if the square in front of it is occupied, but neither can it capture a piece on that square. It is often useful to leave a knight unpromoted at the far side of the board. A knight "must" promote, however, if it reaches either of the two furthest ranks. (See Promotion.)

A lance (香) moves just like the rook except it cannot move backwards or to the sides. It is often useful to leave a lance unpromoted at the far side of the board. A lance "must" promote, however, if it reaches the furthest rank. (See Promotion.)

A pawn (歩) moves one square straight forward. It cannot retreat. Unlike international chess pawns, shogi pawns capture the same as they move. A pawn "must" promote if it arrives at the furthest rank. (See Promotion.) In practice, however, a pawn is usually promoted whenever possible. There are two restrictions on where a pawn may be dropped. (See Drops.)

All pieces but the knight move either horizontally, vertically, or diagonally. These directions cannot be combined in a single move; one direction must be chosen.

Every piece blocks the movement of all other non-jumping pieces through the square it occupies.

If a piece occupies a legal destination for an opposing piece, it may be "captured" by removing it from the board and replacing it with the opposing piece. The capturing piece may not continue beyond that square on that turn. Shogi pieces capture the same as they move.

Normally when moving a piece, a player snaps it to the board with the ends of the fingers of the same hand. This makes a sudden sound effect, bringing the piece to the attention of the opponent. This is also true for capturing and dropping pieces. On a traditional "shogi-ban", the pitch of the snap is deeper, delivering a subtler effect.
A player's "promotion zone" consists of the furthest one-third of the board – the three ranks occupied by the opponent's pieces at setup. The zone is typically delineated on shogi boards by two inscribed dots. When a piece is moved, if part of the piece's path lies within the promotion zone (that is, if the piece moves into, out of, or wholly within the zone; but "not" if it is dropped into the zone – see Drops), then the player has the option to "promote" the piece at the end of the turn. Promotion is indicated by turning the piece over after it moves, revealing the character of the promoted piece.

If a pawn or lance is moved to the furthest rank, or a knight is moved to either of the two furthest ranks, that piece "must" promote (otherwise, it would have no legal move on subsequent turns). A silver general is never required to promote, and it is often advantageous to keep a silver general unpromoted. (It is easier, for example, to extract an unpromoted silver from behind enemy lines; whereas a promoted silver, with only one line of retreat, can be easily blocked.) A rook, bishop and a pawn is almost always promoted, unless there is a problem due to "mate with a dropped pawn".

Promoting a piece changes the way it moves. The various pieces promote as follows:

When captured, a piece loses its promoted status. Otherwise promotion is permanent.

A promoted rook ("dragon king", 龍王 "ryūō") moves as a rook and as a king. It is also called a dragon. Alternate forms: 龍, 竜.

A promoted bishop ("dragon horse", 龍馬 "ryūma") moves as a bishop and as a king. It is also known as a horse. Alternate form: 馬.

A promoted silver (成銀 "narigin") moves the same as a gold general. Alternate forms: 全, cursive 金.

A promoted knight (成桂 "narikei") moves the same as a gold general. Alternate forms: 圭, 今, cursive 金.

A promoted lance (成香 "narikyō") moves the same as a gold general. Alternate forms: 杏, 仝, cursive 金.

A promoted pawn (と金 "tokin") moves the same as a gold general. Alternate forms: と, 个.

Captured pieces are retained in hand and can be brought back into play under the capturing player's control. The Japanese term for "piece(s) in hand" is either 持ち駒 "mochigoma" or 手駒 "tegoma." On any turn, instead of moving a piece on the board, a player may select a piece in hand and place it – unpromoted side up and facing the opposing side – on any empty square. The piece is then one of that player's active pieces on the board and can be moved accordingly. This is called "dropping" the piece, or simply, a "drop". A drop counts as a complete move.

A drop cannot capture a piece, nor does dropping within the promotion zone result in immediate promotion. Capture and/or promotion may occur normally, however, on subsequent moves of the piece.

Restrictions. There are three restrictions on dropping pieces. The last two restrictions apply only to pawns.


It is common to keep captured pieces on a wooden stand (駒台 "komadai)" which is traditionally placed so that its bottom left corner aligns with the bottom right corner of the board from the perspective of each player. It is not permissible to hide pieces from full view.

It is common for players to swap bishops, which oppose each other across the board, early in the game. This leaves each player with a bishop in hand to be dropped later. The ability for drops in shogi give the game tactical richness and complexity. The fact that no piece ever goes entirely out of play accounts for the rarity of draws.

When a player's move threatens to capture the opposing king on the next turn, the move is said to "give check" to the king and the king is said to be "in check." If a player's king is in check, that player's responding move must remove the check if possible. Ways to remove a check include moving the king away from the threat, capturing the threatening piece, or placing another interposing piece between the king and the threatening piece.

To announce check in Japanese, one can say "ōte" (). However, this is an influence of international chess and is not required, even as a courtesy. Announcing a check vocally is unheard of in serious play.

The usual way for shogi games to end is for one side to checkmate the other side's king, after which the losing player will be given the opportunity to admit defeat. Unlike western chess or xiangqi, checkmate is almost always the end result in shogi since pieces never retire from play which gives the players a sufficient number of pieces to deliver checkmate. However, there are three other possible ways for a game to end: "repetition" ( "sennichite"), "impasse" ( "jishōgi"), and an "illegal move" (反則手). The first two – repetition and impasse – are particularly uncommon. Illegal moves are also uncommon in professional games although this may not be true with amateur players (especially beginners).

Unlike western chess, there is no tradition of offering a mutual draw by agreement.

If the king is in check and there is no possible move which could protect the king, the move is said to "checkmate" ("tsumi" 詰み) the king. Checkmate effectively means that the opponent wins the game as the player would have no remaining legal moves. (See also: tsumeshogi, hisshi.)

The losing player will usually resign when the situation is thought to be hopeless and may declare the resignation at any time during their turn. Although a player may resign just after they are checkmated, playing up to the checkmate point rarely occurs in practice as players normally resign as soon as a loss is deemed inevitable – such as when a "tsume" (forced mate sequence) is realized by the losing player. Similarly, if a player were to lose in an Entering King situation (see section below) by having less than 24 points (or by any of the other Impasse rules used by amateurs), then the player will usually resign before that point.

In traditional tournament play, a formal resignation is required – that is, a checkmate is not a sufficient condition for winning. The resignation is indicated by bowing and/or saying 'I lost' (負けました "makemashita") and/or placing the right hand over the piece stands. Placing the hand over the piece stand is a vestige of an older practice of gently dropping one's pieces in hand over the board in order to indicate resignation. In western practice, a handshake may be used.

In professional and serious (tournament) amateur games, a player who makes an illegal move loses immediately. The loss stands even if play continued and the move was discovered later in game. However, if neither the opponent nor a third party points out the illegal move and the opponent later resigned, the resignation stands as the result.

Illegal moves include:


In friendly amateur games, this rule is sometimes relaxed, and the player may be able to take back the illegal move and replay a new legal move.

In particular, the Two Pawn violation is most common illegal move played by professional players. The Two Pawn violation played by Takahiro Toyokawa (against Kōsuke Tamura) in the 2004 NHK Cup is infamous since it was broadcast on television. On the 109th move, Toyokawa (playing as Black) dropped a pawn to the 29 square while he already had a pawn in play on the board on the 23 square and, thus, lost the game.

If the same game position occurs four times with the same player to move and the same pieces in hand for each player, then the game ends in a repetition draw (千日手 "sennichite," lit. "moves for a thousand days"), as long as the positions are not due to perpetual check. Perpetual check (連続王手の千日手) is an illegal move (see above), which ends the game in a loss in tournament play.

In professional shogi, a repetition draw outcome is not a final result as draws essentially do not count. There can be only one victorious through wins. In the case of a repetition draw, professional shogi players will have to immediately play a subsequent game (or as many games as necessary) with sides reversed in order to obtain a true win outcome. (That is, the player who was White becomes Black, and vice versa.) Also, depending on the tournament, professional players play the subsequent game in the remainder of the allowed game time.

Thus, aiming for a repetition draw may be a possible professional strategy for the White player in order to play the second replay game as Black, which has a slight statistical advantage and/or greater initiative. For instance, Bishop Exchange Fourth File Rook is a passive strategy for White with the goal of a repetition draw (as it requires two tempo losses – swinging the rook and trading the bishops) while it is a very aggressive strategy if played by Black.

Repetition draws are rare in professional shogi occurring in about 1–2% of games and even rarer in amateur games. In professional shogi, repetition draws usually occur in the opening as certain positions are reached that are theoretically disadvantaged for both sides (reciprocal zugzwang). In amateur shogi, repetition draws tend to occur in the middle or endgame as a result of player errors.

The game reaches an Impasse or Deadlock (持将棋 "jishōgi") if both kings have advanced into their respective promotion zones – a situation known as 相入玉 ("ai-nyū gyoku" "double entering kings") – and neither player can hope to mate the other or to gain any further material. An Impasse can result in either a win or a draw. If an Impasse happens, the winner is decided as follows: each player agrees to an Impasse, then each rook or bishop, promoted or not, scores 5 points for the owning player, and all other pieces except kings score 1 point each. A player scoring fewer than 24 points loses. (Note that in the start position, both players have 27 points each.) If neither player has fewer than 24, the game is no contest — a draw. In professional shogi, an Impasse result is always a draw since a player that cannot obtain the 24 points will simply resign. "Jishōgi" is considered an outcome in its own right rather than no contest, but there is no practical difference. As an Impasse needs to be agreed on for the rule to be invoked, a player may refuse to do so and attempt to win the game in future moves. If that happens, there is no official rule about the verdict of the game.

However, in amateur shogi, there are different practices most of which force a win resolution to the Impasse in order to avoid a draw result.

The first draw by Impasse occurred in 1731 in a bishop handicap game between the seventh Lifetime Meijin, , and his brother, Sōkei Ōhashi.

As a practical matter, when an opponent's king has entered a player's own territory especially with supporting defending pieces, the opponent's king is often very difficult to mate given the forward attacking nature of most shogi pieces. This state is referred to as entering king (入玉 "nyū gyoku"). If both players' kings are in entering king states, the game becomes more likely to result in an impasse.

In the adjacent diagram example, although White's king is in a strong Bear-in-the-hole castle, Black's king has entered White's territory making it very difficult to mate. Therefore, this position favors Black.

An example of Entering King occurred in the fourth game of the 60th Ōi title match between Masayuki Toyoshima and Kazuki Kimura held on August 2021, 2019. After being unsuccessful in attacking Kimura and also in defending his own king within his camp, Toyoshima (playing as White) moved his king away from Kimura's attacking pieces by fleeing up the second file, ultimately entering his king into Kimura's camp by move 150. Although Toyoshima had achieved Entering King, he still had only 23 pointsone point shy of the required 24 points for an Impasse drawwhile Kimura (Black) had 31 points. Toyoshima then spent the next 134 moves trying to bring his point total, which fluctuated between 17 and 23, up to the necessary 24. By the 231st move, the game had reached a Double Entering Kings state, and by move 285 Kimura had successfully kept Toyoshima's point total at bay. Here, Toyoshima with 20 points (and Kimura at 34 points) resigned. Incidentally, this game broke the record of longest game in a title match.

For amateur games, there are various guidances with little standardization. Fairbairn reports a practice in the 1980s (considered a rule by the now defunct Shogi Association for The West) where the dispute is resolved by either player moving all friendly pieces into the promotion zone and then the game ends with points tallied.

Another resolution is the 27-Point (27点法) rule used for some amateur tournaments. One version of this is simply the player who has 27 or more points is the winner of the Impasse. Another version is a 27-Point Declaration rule. For instance, the Declaration rule on the online shogi site, 81Dojo, is that the player who wants to declare an Impasse win must (i) declare an intention win via Impasse, (ii) have the king in the enemy camp (the promotion zone for that player), (iii) 10 other pieces must be in the promotion zone, (iv) not be in check, (v) have time remaining, and (vi) must have 28 points if Black or 27 points if White. If all of these conditions are met, then the Impasse declarer will win the game regardless of whether the opponent objects.

Yet another resolution to Impasse is the so-called Try Rule (トライルール "torairūru"). In this case, after both kings have entered their corresponding promotion zones, then the player who first moves the king to the opponent's king's start square (51 for Black, 59 for White) first will be the winner. As an example, the popular (Shogi Wars) app by HEROZ Inc. used the Try Rule up until 2014. (Now the app uses a variant of the 27-Point Declaration Rule – although it differs from the variant used on the 81Dojo site.) The idea of "Try Rule" was taken from rugby football (see Try (rugby)).

In professional tournaments, the rules typically require drawn games to be replayed with sides reversed, possibly with reduced time limits. This is rare compared to chess and xiangqi, occurring at a rate of 1–2% even in amateur games.

The 1982 "Meijin" title match between Makoto Nakahara and Hifumi Katoh was unusual in this regard with an impasse draw in the first (Double Fortress) game on April 13–14 (only the fifth draw in the then 40-year history of the tournament). This game (with Katoh as Black) lasted for 223 moves with 114 minutes spent pondering a single move. One of the reasons for the length of this game was that White (Nakahara) was very close to falling below the minimum of 24 points required for a draw. Thus, the end of the endgame was strategically about trying to keep White's points above the 24-point threshold. In this match, "sennichite" occurred in the sixth and eighth games. Thus, this best-of-seven match lasted eight games and took over three months to finish; Black did not lose a single game and the eventual victor was Katoh at 4–3.

Professional games are timed as in international chess, but professional shogi players are almost never expected to keep time in their games. Instead a timekeeper is assigned, typically an apprentice professional. Time limits are much longer than in international chess (9 hours a side plus extra time in the prestigious "Meijin" title match), and in addition "byōyomi" (literally "second counting") is employed. This means that when the ordinary time has run out, the player will from that point on have a certain amount of time to complete every move (a "byōyomi" period), typically upwards of one minute. The final ten seconds are counted down, and if the time expires the player to move loses the game immediately. Amateurs often play with electronic clocks that beep out the final ten seconds of a "byōyomi" period, with a prolonged beep for the last five.

Amateur players are ranked from 15 "kyū" to 1 kyū and then from 1 "dan" to 8 dan. Amateur 8 dan was previously only honorarily given to famous people. While it is now possible to win amateur 8 dan by actual strength (winning amateur Ryu-oh 3 times), this has yet to be achieved.

Professional players operate with their own scale, from 6 kyū to 3 dan for pro-aspiring players and professional 4 dan to 9 dan for formal professional players. Amateur and professional ranks are offset (with amateur 4 dan being equivalent to professional 6 kyū).

Shogi has a handicap system (like go) in which games between players of disparate strengths are adjusted so that the stronger player is put in a more disadvantageous position in order to compensate for the difference in playing levels. In a handicap game, one or more of White's pieces are removed from the setup, and instead White plays first.

There are two common systems used to notate piece movements in shogi game records. One is used in Japanese language texts while a second was created for western players by George Hodges and Glyndon Townhill in the English language. This system was updated by Hosking to be closer to the Japanese standard (two numerals). Other systems are used to notate shogi board positions. Unlike chess, the origin (11 square) is at the top right of a printed position rather than the bottom left.

In western piece movement notation, the format is the piece initial followed by the type of movement and finally the file and rank where the piece moved to. The piece initials are K (King), R (Rook), B (Bishop), G (Gold), S (Silver), N (Knight), L (Lance), and P (Pawn). Simple movement is indicated with -, captures with x, and piece drops with *. The files are indicated with numerals 1–9. The older Hodges standard used letters a–i for ranks, and the newer Hosking standard also uses numerals 1–9 for the ranks. Thus, Rx24 indicates 'rook captures on 24'. Promoted pieces are notated with + prefixed to the piece initial (e.g. +Rx24). Piece promotion is also indicated with + (e.g. S-21+) while unpromotion is indicated with = (e.g. S-21=). Piece ambiguity is resolved by notating which square a piece is moving from (e.g. N65-53+ means 'knight from 65 moves to 53 and promotes,' which distinguishes it from N45-53+).

The Japanese notation system uses Japanese characters for pieces and promotion indication and uses Japanese numerals instead of letters for ranks. Movement type aside from drops is not indicated, and the conventions for resolving ambiguity are quite different from the western system. As examples, the western Rx24 would be ' in Japanese notation, +Rx24 would be ', S-21+ would be ', S-21= would be ', and N65-53+ would be ' showing that the leftmost knight jumped (implicitly from the 65 square), which distinguishes it from ' in which the rightmost knight jumped.

Although not strictly part of the notational calculus for games, game results are indicated in Japanese newspapers, websites, etc. with wins indicated by a white circle and losses indicated by a black circle.

Shogi is similar to chess but has a much larger game tree complexity because of the use of drops, greater number of pieces, and larger board size. In comparison, shogi games average about 140 (half-)moves per game (or 70 chess move-pairs) whereas chess games average about 80 moves per game (or 40 chess move-pairs) and minishogi averages about 40 moves per game (or 20 chess move-pairs).

Like chess, however, the game can be divided into the opening, middle game and endgame, each requiring a different strategy. The opening consists of arranging one's defenses usually in a castle and positioning for attack, the mid game consists of attempting to break through the opposing defenses while maintaining one's own, and the endgame starts when one side's defenses have been compromised.

In the adjacent diagram, Black has chosen a Ranging Rook position (specifically Fourth File Rook) where the rook has been moved leftward away from its starting position. Additionally, Black is utilizing a Silver Crown castle, which is a type of fortification structure constructed with one silver and two gold pieces and the king moved inside of the fortification – the "silver crown" name comes from the silver being positioned directly above the king's head on the 27 square as if it were a crown. In the diagram, White has chosen a Static Rook position, in which the rook remains on its starting square. This Static Rook position is specifically a type of Counter-Ranging Rook position known as Bear-in-the-hole Static Rook that uses an Bear-in-the-hole castle. The Bear-in-the-hole fortification has the king moved all the way into very edge corner of the board on the 11 square as if it were a badger in a hole with a silver moved to the 22 square in order to close up the hole and additional reinforcing golds on 31 and 32 squares. This board position required 33 moves (or 12 move pairs as counted in western chess) to construct.

Shogi players are expected to follow etiquette in addition to rules explicitly described. Commonly accepted etiquette include following:

Shogi piece sets may contain two types of king pieces, (king) and (jewel). In this case, the higher classed player, in either social or genuine shogi player rank, may take the king piece. For example, in titleholder system games, the current titleholder takes the king piece as the higher.

The higher-ranked (or older) player also sits facing the door of the room and is the person who takes the pieces out of the piece box.

Shogi does not have a touch-move rule as in western chess tournament play or chu shogi. However, in professional games, a piece is considered to be moved when the piece has been let go of. In both amateur and professional play, any piece may be touched in order to adjust its centralization within its square (to look tidy).

Taking back moves (待った "matta") in professional games is prohibited. However, in friendly amateur games in Japan, it is often permitted.

Professional players are required to follow several ritualistic etiquette prescriptions such as kneeling exactly 15 centimeters from the shogi board, sitting in the formal seiza position, etc.

Traditionally, the order of placing the pieces on the board is determined. There are two commonly used orders, the "Ōhashi" order 大橋流 and the "Itō" order 伊藤流. Placement sets pieces with multiples (generals, knights, lances) from left to right in all cases, and follows the order:


Among amateur tournaments, the higher-ranked player or defending champion performs the piece toss. In professional games, the furigoma is done on the behalf of the higher-ranked player/champion by the timekeeper who kneels by the side of the higher-ranked player and tosses the pawn pieces onto a silk cloth. In friendly amateur games, a player will ask the opponent to toss the pawns out of politeness. Otherwise, the person who tosses the pawns can be determined by Rock–paper–scissors.

From "The Chess Variant Pages":
It is not clear when chess was brought to Japan. The earliest generally accepted mention of shogi is (1058–1064) by Fujiwara Akihira. The oldest archaeological evidence is a group of 16 shogi pieces excavated from the grounds of Kōfuku-ji in Nara Prefecture. As it was physically associated with a wooden tablet written on in the sixth year of Tenki (1058), the pieces are thought to date from that period. These simple pieces were cut from a writing plaque in the same five-sided shape as modern pieces, with the names of the pieces written on them.

The dictionary of common folk culture, (c. 1210–1221), a collection based on the two works and , describes two forms of shogi, large "(dai)" shogi and small "(shō)" shogi. These are now called Heian shogi (or Heian small shogi) and Heian dai shogi. Heian small shogi is the version on which modern shogi is based, but the "Nichūreki" states that one wins if one's opponent is reduced to a single king, indicating that drops had not yet been introduced. According to Kōji Shimizu, chief researcher at the Archaeological Institute of Kashihara, Nara Prefecture, the names of the Heian shogi pieces keep those of chaturanga (general, elephant, horse, chariot and soldier), and add to them the five treasures of Buddhism (jade, gold, silver, katsura tree, and incense).

Around the 13th century the game of dai shogi developed, created by increasing the number of pieces in Heian shogi, as was sho shogi, which added the rook, bishop, and drunken elephant from dai shogi to Heian shogi. The drunken elephant steps one square in any direction except directly backward, and promotes to the prince, which acts as a second king and must also be captured along with the original king for the other player to win. Around the 15th century, the rules of dai shogi were simplified, creating the game of chu shogi. Chu shogi, like its parent dai shogi, contains many distinct pieces, such as the queen (identical with Western chess) and the lion (which moves like a king, but twice per turn, potentially being able to capture twice, among other idiosyncrasies). The popularity of dai shogi soon waned in favour of chu shogi, until it stopped being played commonly. Chu shogi rivalled sho shogi in popularity until the introduction of drops in the latter, upon which standard shogi became ascendant, although chu shogi was still commonly played until about World War II, especially in Kyoto. Dai shogi was much less often played, but must have been remembered somewhat, as it is depicted in a woodcut by Kobayashi Kiyochika from around 1904 or 1905.

It is thought that the rules of standard shogi were fixed in the 16th century, when the drunken elephant was removed from the set of pieces present in sho shogi. There is no clear record of when drops were introduced, however.

In the Edo period, shogi variants were greatly expanded: tenjiku shogi, dai dai shogi, maka dai dai shogi, tai shogi, and taikyoku shogi were all invented. It is thought that these were played to only a very limited extent, however. Both standard shogi and Go were promoted by the Tokugawa shogunate. In 1612, the shogunate passed a law giving endowments to top shogi players (). During the reign of the eighth shōgun, Tokugawa Yoshimune, castle shogi tournaments were held once a year on the 17th day of Kannazuki, corresponding to November 17, which is Shogi Day on the modern calendar.

The title of "meijin" became hereditary in the Ōhashi and Itō families until the fall of the shogunate, when it came to be passed by recommendation. Today the title is used for the winner of the Meijin-sen competition, the first modern title match. From around 1899, newspapers began to publish records of shogi matches, and high-ranking players formed alliances with the aim of having their games published. In 1909, the was formed, and in 1924, the was formed. This was an early incarnation of the modern , or JSA, and 1924 is considered by the JSA to be the date it was founded.

In 1935, "meijin" Kinjirō Sekine stepped down, and the rank of meijin came to be awarded to the winner of a . became the first Meijin under this system in 1937. This was the start of the (see titleholder system). After the war other tournaments were promoted to title matches, culminating with the in 1988 for the modern line-up of seven. About 200 professional shogi players compete. Each year, the title holder defends the title against a challenger chosen from knockout or round matches.

After the Second World War, SCAP (occupational government mainly led by US) tried to eliminate all "feudal" factors from Japanese society and shogi was included in the possible list of items to be banned along with Bushido (philosophy of samurai) and other things. The reason for banning shogi for SCAP was its exceptional character as a board game seen in the usage of captured pieces. SCAP insisted that this could lead to the idea of prisoner abuse. But Kozo Masuda, then one of the top professional shogi players, when summoned to the SCAP headquarters for an investigation, criticized such understanding of shogi and insisted that it is not shogi but western chess that potentially contains the idea of prisoner abuse because it just kills the pieces of the opponent while shogi is rather democratic for giving prisoners the chance to get back into the game. Masuda also said that chess contradicts the ideal of gender equality in western society because the king shields itself behind the queen and runs away. Masuda’s assertion is said to have eventually led to the exemption of shogi from the list of items to be banned.

There are two organizations for shogi professional players in Japan: the JSA, and the , or LPSA. The JSA is the primary organization for men and women's professional shogi while the LPSA is a group of women professionals who broke away from the JSA in 2007 to establish their own independent organization. Both organize tournaments for their members and have reached an agreement to cooperate with each other to promote shogi through events and other activities. Top professional players are fairly well-paid from tournament earnings. In 2016, the highest tournament earners were Yoshiharu Habu and Akira Watanabe who earned ¥91,500,000 and ¥73,900,000. (The tenth highest earner, Kouichi Fukaura, won ¥18,490,000.)

The JSA recognizes two categories of shogi professionals: , and . Sometimes "kishi" are addressed as , a term from Go used to distinguish "kishi" from other classes of players. JSA professional ranks and female professional ranks are not equivalent and each has their own promotion criteria and ranking system. In 2006, the JSA officially granted women "professional status". This is not equivalent, however, to the more traditional way of "gaining professional status", i.e., being promoted from the : leagues of strong amateur players aspiring to become a professional. Rather, it is a separate system especially designed for female professionals. Qualified amateurs, regardless of gender, may apply for the "Shoreikai System" and all those who successfully "graduate" are granted "kishi" status; however, no woman has yet to accomplish this feat (the highest women have reached is "Shoreikai 3 "dan" league" by Kana Satomi and Tomoka Nishiyama), so "kishi" is de facto only used to refer to male shogi professionals.

The JSA is the only body which can organize tournaments for professionals, e.g., the eight major tournaments in the titleholder system and other professional tournaments. In 1996, Yoshiharu Habu became the only "kishi" to hold seven major titles at the same time. For female professionals, both the JSA and LPSA organize tournaments, either jointly or separately. Tournaments for amateurs may be organized by the JSA and LPSA as well as local clubs, newspapers, private corporations, educational institutions or municipal governments for cities or prefectures under the guidance of the JSA or LPSA.

Since the 1990s, shogi has grown in popularity outside Japan, particularly in the People's Republic of China, and especially in Shanghai. The January 2006 edition of stated that there were 120,000 shogi players in Shanghai. The spread of the game to countries where Chinese characters are not in common use, however, has been slower.

, in Europe there are currently over 1,200 active players.

Shogi has the highest game complexity of all popular chess variants. Computers have steadily improved in playing shogi since the 1970s. In 2007, champion Yoshiharu Habu estimated the strength of the 2006 world computer shogi champion Bonanza at the level of two-dan shoreikai.

The JSA prohibits its professionals from playing computers in public without prior permission, with the reason of promoting shogi and monetizing the computer–human events.

On October 12, 2010, after some 35 years of development, a computer finally beat a professional player, when the top ranked female champion Ichiyo Shimizu was beaten by the Akara2010 system in a game lasting just over 6 hours.

On July 24, 2011, computer shogi programs Bonanza and Akara crushed the amateur team of Kosaku and Shinoda in two games. The allotted time for the amateurs was one hour and then three minutes per move. The allotted time for the computer was 25 minutes and then 10 seconds per move.

On April 20, 2013, GPS Shogi defeated 8-dan professional shogi player Hiroyuki Miura in a 102-move game which lasted over 8 hours.

On December 13, 2015, the highest rated player on Shogi Club 24 was computer program Ponanza, rated 3455.

On April 10, 2016, Ponanza defeated Takayuki Yamasaki, 8-dan in 85 moves. Takayuki used 7 hours 9 minutes.

In October 2017, DeepMind claimed that its program AlphaZero, after a full nine hours of training, defeated elmo in a 100-game match, winning 90, losing 8, and drawing two.

From a computational complexity point of view, generalized shogi is EXPTIME-complete.

Hundreds of video games were released exclusively in Japan for several consoles.

According to professional player Yoshiharu Habu, in Japan shogi is viewed as not merely a game as entertainment or a mind sport but is instead an art that is a part of traditional Japanese culture along with haiku, tanka, noh, ikebana, and the Japanese tea ceremony. Its elevated status was established by the "iemoto" system supported by the historical shogunate.
The backwards "uma" (shogi horse symbol) is often featured on merchandise (such as on large decorative shogi piece sculptures, keychains, and other keepsakes) available for sale in Tendō. It also serves as a symbol of good luck. (Cf. Rabbit's foot.) There are multiple theories on its origin. One is that "uma" (うま ) spelled in the Japanese syllabary backwards is まう "mau" (舞う), which means "(to) dance" and dancing horses are a good luck omen.

In the manga series "Naruto", shogi plays an essential part in Shikamaru Nara's character development. He often plays it with his sensei, Asuma Sarutobi, apparently always beating him. When Asuma is fatally injured in battle, he reminds Shikamaru that the shogi king must always be protected, and draws a parallel between the king in shogi and his yet-unborn daughter, Mirai, whom he wanted Shikamaru to guide.

Shogi has been a central plot point in the manga and anime "Shion no Ō", the manga and anime "March Comes in Like a Lion", and the manga and television drama "81diver".

In the manga and anime "Durarara!!", the information broker Izaya Orihara plays a twisted version of chess, go and shogi, where he mixes all three games into one as a representation of the battles in Ikebukuro.

In the video game "Persona 5", the Star confidant, a girl named Hifumi Togo, is a high school shogi player looking to break into the ranks of the professionals. The player character will gain knowledge stat when spending time with the confidant, supposedly from learning to play shogi. The abilities learned from ranking up the confidant comes from Japanese shogi terms.

In the light novel, manga, and anime "The Ryuo's Work is Never Done!", protagonist Yaichi Kuzuryū is a prodigy shogi player who won the title of Ryūō at the age of 16. He is approached by Ai Hinatsuru, a 9-year-old girl who begs him to make her his disciple. Astonished by Ai's potential, Yaichi agrees to become her master, and the two then brave themselves together in the world of shogi with their friends and rivals.

In the anime "Asobi Asobase", Hanako's butler Maeda tells her shogi is a sport where you fire a beam from your butt, because he does not know the rules, so he cannot teach her how to actually play shogi. He follows this by demonstrating the sport and destroying the roof with a laser beam fired from behind.



Rules

Online play

Online tools


</doc>
<doc id="27743" url="https://en.wikipedia.org/wiki?curid=27743" title="Solar energy">
Solar energy

Solar energy is radiant light and heat from the Sun that is harnessed using a range of ever-evolving technologies such as solar heating, photovoltaics, solar thermal energy, solar architecture, molten salt power plants and artificial photosynthesis.

It is an essential source of renewable energy, and its technologies are broadly characterized as either passive solar or active solar depending on how they capture and distribute solar energy or convert it into solar power. Active solar techniques include the use of photovoltaic systems, concentrated solar power, and solar water heating to harness the energy. Passive solar techniques include orienting a building to the Sun, selecting materials with favorable thermal mass or light-dispersing properties, and designing spaces that naturally circulate air.

The large magnitude of solar energy available makes it a highly appealing source of electricity. The United Nations Development Programme in its 2000 World Energy Assessment found that the annual potential of solar energy was 1,575–49,837 exajoules (EJ). This is several times larger than the total world energy consumption, which was 559.8 EJ in 2012.

In 2011, the International Energy Agency said that "the development of affordable, inexhaustible and clean solar energy technologies will have huge longer-term benefits. It will increase countries' energy security through reliance on an indigenous, inexhaustible, and mostly import-independent resource, enhance sustainability, reduce pollution, lower the costs of mitigating global warming, and keep fossil fuel prices lower than otherwise. These advantages are global. Hence the additional costs of the incentives for early deployment should be considered learning investments; they must be wisely spent and need to be widely shared".

The Earth receives 174 petawatts (PW) of incoming solar radiation (insolation) at the upper atmosphere. Approximately 30% is reflected back to space while the rest is absorbed by clouds, oceans and land masses. The spectrum of solar light at the Earth's surface is mostly spread across the visible and near-infrared ranges with a small part in the near-ultraviolet. Most of the world's population live in areas with insolation levels of 150–300 watts/m², or 3.5–7.0 kWh/m² per day.

Solar radiation is absorbed by the Earth's land surface, oceans – which cover about 71% of the globe – and atmosphere. Warm air containing evaporated water from the oceans rises, causing atmospheric circulation or convection. When the air reaches a high altitude, where the temperature is low, water vapor condenses into clouds, which rain onto the Earth's surface, completing the water cycle. The latent heat of water condensation amplifies convection, producing atmospheric phenomena such as wind, cyclones and anti-cyclones. Sunlight absorbed by the oceans and land masses keeps the surface at an average temperature of 14 °C. By photosynthesis, green plants convert solar energy into chemically stored energy, which produces food, wood and the biomass from which fossil fuels are derived.

The total solar energy absorbed by Earth's atmosphere, oceans and land masses is approximately 3,850,000 exajoules (EJ) per year. In 2002, this was more energy in one hour than the world used in one year. Photosynthesis captures approximately 3,000 EJ per year in biomass. The amount of solar energy reaching the surface of the planet is so vast that in one year it is about twice as much as will ever be obtained from all of the Earth's non-renewable resources of coal, oil, natural gas, and mined uranium combined,

The potential solar energy that could be used by humans differs from the amount of solar energy present near the surface of the planet because factors such as geography, time variation, cloud cover, and the land available to humans limit the amount of solar energy that we can acquire.

Geography affects solar energy potential because areas that are closer to the equator have a higher amount of solar radiation. However, the use of photovoltaics that can follow the position of the Sun can significantly increase the solar energy potential in areas that are farther from the equator. Time variation effects the potential of solar energy because during the nighttime, there is little solar radiation on the surface of the Earth for solar panels to absorb. This limits the amount of energy that solar panels can absorb in one day. Cloud cover can affect the potential of solar panels because clouds block incoming light from the Sun and reduce the light available for solar cells.

Besides, land availability has a large effect on the available solar energy because solar panels can only be set up on land that is otherwise unused and suitable for solar panels. Roofs are a suitable place for solar cells, as many people have discovered that they can collect energy directly from their homes this way. Other areas that are suitable for solar cells are lands that are not being used for businesses where solar plants can be established.

Solar technologies are characterized as either passive or active depending on the way they capture, convert and distribute sunlight and enable solar energy to be harnessed at different levels around the world, mostly depending on the distance from the equator. Although solar energy refers primarily to the use of solar radiation for practical ends, all renewable energies, other than Geothermal power and Tidal power, derive their energy either directly or indirectly from the Sun.

Active solar techniques use photovoltaics, concentrated solar power, solar thermal collectors, pumps, and fans to convert sunlight into useful outputs. Passive solar techniques include selecting materials with favorable thermal properties, designing spaces that naturally circulate air, and referencing the position of a building to the Sun. Active solar technologies increase the supply of energy and are considered supply side technologies, while passive solar technologies reduce the need for alternate resources and are generally considered demand-side technologies.

In 2000, the United Nations Development Programme, UN Department of Economic and Social Affairs, and World Energy Council published an estimate of the potential solar energy that could be used by humans each year that took into account factors such as insolation, cloud cover, and the land that is usable by humans. The estimate found that solar energy has a global potential of per year "(see table below)".

Solar thermal technologies can be used for water heating, space heating, space cooling and process heat generation.

In 1878, at the Universal Exposition in Paris, Augustin Mouchot successfully demonstrated a solar steam engine, but couldn't continue development because of cheap coal and other factors.

In 1897, Frank Shuman, a US inventor, engineer and solar energy pioneer built a small demonstration solar engine that worked by reflecting solar energy onto square boxes filled with ether, which has a lower boiling point than water and were fitted internally with black pipes which in turn powered a steam engine. In 1908 Shuman formed the Sun Power Company with the intent of building larger solar power plants. He, along with his technical advisor A.S.E. Ackermann and British physicist Sir Charles Vernon Boys, developed an improved system using mirrors to reflect solar energy upon collector boxes, increasing heating capacity to the extent that water could now be used instead of ether. Shuman then constructed a full-scale steam engine powered by low-pressure water, enabling him to patent the entire solar engine system by 1912.

Shuman built the world's first solar thermal power station in Maadi, Egypt, between 1912 and 1913. His plant used parabolic troughs to power a engine that pumped more than of water per minute from the Nile River to adjacent cotton fields. Although the outbreak of World War I and the discovery of cheap oil in the 1930s discouraged the advancement of solar energy, Shuman's vision, and basic design were resurrected in the 1970s with a new wave of interest in solar thermal energy. In 1916 Shuman was quoted in the media advocating solar energy's utilization, saying:
Solar hot water systems use sunlight to heat water. In middle geographical latitudes (between 40 degrees north and 40 degrees south), 60 to 70% of the domestic hot water use, with water temperatures up to , can be provided by solar heating systems. The most common types of solar water heaters are evacuated tube collectors (44%) and glazed flat plate collectors (34%) generally used for domestic hot water; and unglazed plastic collectors (21%) used mainly to heat swimming pools.

As of 2007, the total installed capacity of solar hot water systems was approximately 154 thermal gigawatt (GW). China is the world leader in their deployment with 70 GW installed as of 2006 and a long-term goal of 210 GW by 2020. Israel and Cyprus are the per capita leaders in the use of solar hot water systems with over 90% of homes using them. In the United States, Canada, and Australia, heating swimming pools is the dominant application of solar hot water with an installed capacity of 18 GW as of 2005.

In the United States, heating, ventilation and air conditioning (HVAC) systems account for 30% (4.65 EJ/yr) of the energy used in commercial buildings and nearly 50% (10.1 EJ/yr) of the energy used in residential buildings. Solar heating, cooling and ventilation technologies can be used to offset a portion of this energy.
Thermal mass is any material that can be used to store heat—heat from the Sun in the case of solar energy. Common thermal mass materials include stone, cement, and water. Historically they have been used in arid climates or warm temperate regions to keep buildings cool by absorbing solar energy during the day and radiating stored heat to the cooler atmosphere at night. However, they can be used in cold temperate areas to maintain warmth as well. The size and placement of thermal mass depend on several factors such as climate, daylighting, and shading conditions. When duly incorporated, thermal mass maintains space temperatures in a comfortable range and reduces the need for auxiliary heating and cooling equipment.

A solar chimney (or thermal chimney, in this context) is a passive solar ventilation system composed of a vertical shaft connecting the interior and exterior of a building. As the chimney warms, the air inside is heated, causing an updraft that pulls air through the building. Performance can be improved by using glazing and thermal mass materials in a way that mimics greenhouses.

Deciduous trees and plants have been promoted as a means of controlling solar heating and cooling. When planted on the southern side of a building in the northern hemisphere or the northern side in the southern hemisphere, their leaves provide shade during the summer, while the bare limbs allow light to pass during the winter. Since bare, leafless trees shade 1/3 to 1/2 of incident solar radiation, there is a balance between the benefits of summer shading and the corresponding loss of winter heating. In climates with significant heating loads, deciduous trees should not be planted on the Equator-facing side of a building because they will interfere with winter solar availability. They can, however, be used on the east and west sides to provide a degree of summer shading without appreciably affecting winter solar gain.

Solar cookers use sunlight for cooking, drying, and pasteurization. They can be grouped into three broad categories: box cookers, panel cookers, and reflector cookers. The simplest solar cooker is the box cooker first built by Horace de Saussure in 1767. A basic box cooker consists of an insulated container with a transparent lid. It can be used effectively with partially overcast skies and will typically reach temperatures of . Panel cookers use a reflective panel to direct sunlight onto an insulated container and reach temperatures comparable to box cookers. Reflector cookers use various concentrating geometries (dish, trough, Fresnel mirrors) to focus light on a cooking container. These cookers reach temperatures of and above but require direct light to function properly and must be repositioned to track the Sun.

Solar concentrating technologies such as parabolic dish, trough and Scheffler reflectors can provide process heat for commercial and industrial applications. The first commercial system was the Solar Total Energy Project (STEP) in Shenandoah, Georgia, US where a field of 114 parabolic dishes provided 50% of the process heating, air conditioning and electrical requirements for a clothing factory. This grid-connected cogeneration system provided 400 kW of electricity plus thermal energy in the form of 401 kW steam and 468 kW chilled water, and had a one-hour peak load thermal storage. Evaporation ponds are shallow pools that concentrate dissolved solids through evaporation. The use of evaporation ponds to obtain salt from seawater is one of the oldest applications of solar energy. Modern uses include concentrating brine solutions used in leach mining and removing dissolved solids from waste streams. Clothes lines, clotheshorses, and clothes racks dry clothes through evaporation by wind and sunlight without consuming electricity or gas. In some states of the United States legislation protects the "right to dry" clothes. Unglazed transpired collectors (UTC) are perforated sun-facing walls used for preheating ventilation air. UTCs can raise the incoming air temperature up to and deliver outlet temperatures of . The short payback period of transpired collectors (3 to 12 years) makes them a more cost-effective alternative than glazed collection systems. As of 2003, over 80 systems with a combined collector area of had been installed worldwide, including an collector in Costa Rica used for drying coffee beans and a collector in Coimbatore, India, used for drying marigolds.

Solar distillation can be used to make saline or brackish water potable. The first recorded instance of this was by 16th-century Arab alchemists. A large-scale solar distillation project was first constructed in 1872 in the Chilean mining town of Las Salinas. The plant, which had solar collection area of , could produce up to per day and operate for 40 years. Individual still designs include single-slope, double-slope (or greenhouse type), vertical, conical, inverted absorber, multi-wick, and multiple effect. These stills can operate in passive, active, or hybrid modes. Double-slope stills are the most economical for decentralized domestic purposes, while active multiple effect units are more suitable for large-scale applications.

Solar water disinfection (SODIS) involves exposing water-filled plastic polyethylene terephthalate (PET) bottles to sunlight for several hours. Exposure times vary depending on weather and climate from a minimum of six hours to two days during fully overcast conditions. It is recommended by the World Health Organization as a viable method for household water treatment and safe storage. Over two million people in developing countries use this method for their daily drinking water.

Solar energy may be used in a water stabilization pond to treat waste water without chemicals or electricity. A further environmental advantage is that algae grow in such ponds and consume carbon dioxide in photosynthesis, although algae may produce toxic chemicals that make the water unusable.

Molten salt can be employed as a thermal energy storage method to retain thermal energy collected by a solar tower or solar trough of a concentrated solar power plant so that it can be used to generate electricity in bad weather or at night. It was demonstrated in the Solar Two project from 1995–1999. The system is predicted to have an annual efficiency of 99%, a reference to the energy retained by storing heat before turning it into electricity, versus converting heat directly into electricity. The molten salt mixtures vary. The most extended mixture contains sodium nitrate, potassium nitrate and calcium nitrate. It is non-flammable and non-toxic, and has already been used in the chemical and metals industries as a heat-transport fluid. Hence, experience with such systems exists in non-solar applications.

The salt melts at . It is kept liquid at in an insulated "cold" storage tank. The liquid salt is pumped through panels in a solar collector where the focused irradiance heats it to . It is then sent to a hot storage tank. This is so well insulated that the thermal energy can be usefully stored for up to a week.

When electricity is needed, the hot salt is pumped to a conventional steam-generator to produce superheated steam for a turbine/generator as used in any conventional coal, oil, or nuclear power plant. A 100-megawatt turbine would need a tank about tall and in diameter to drive it for four hours by this design.

Several parabolic trough power plants in Spain and solar power tower developer SolarReserve use this thermal energy storage concept. The Solana Generating Station in the U.S. has six hours of storage by molten salt. The María Elena plant is a 400 MW thermo-solar complex in the northern Chilean region of Antofagasta employing molten salt technology.

Solar power is the conversion of sunlight into electricity, either directly using photovoltaics (PV), or indirectly using concentrated solar power (CSP). CSP systems use lenses or mirrors and tracking systems to focus a large area of sunlight into a small beam. PV converts light into electric current using the photoelectric effect.

Solar power is anticipated to become the world's largest source of electricity by 2050, with solar photovoltaics and concentrated solar power contributing 16 and 11 percent to the global overall consumption, respectively. In 2016, after another year of rapid growth, solar generated 1.3% of global power.

Commercial concentrated solar power plants were first developed in the 1980s. The 392 MW Ivanpah Solar Power Facility, in the Mojave Desert of California, is the largest solar power plant in the world. Other large concentrated solar power plants include the 150 MW Solnova Solar Power Station and the 100 MW Andasol solar power station, both in Spain. The 250 MW Agua Caliente Solar Project, in the United States, and the 221 MW Charanka Solar Park in India, are the world's largest photovoltaic plants. Solar projects exceeding 1 GW are being developed, but most of the deployed photovoltaics are in small rooftop arrays of less than 5 kW, which are connected to the grid using net metering or a feed-in tariff.

In the last two decades, photovoltaics (PV), also known as solar PV, has evolved from a pure niche market of small scale applications towards becoming a mainstream electricity source. A solar cell is a device that converts light directly into electricity using the photoelectric effect. The first solar cell was constructed by Charles Fritts in the 1880s. In 1931 a German engineer, Dr Bruno Lange, developed a photo cell using silver selenide in place of copper oxide. Although the prototype selenium cells converted less than 1% of incident light into electricity, both Ernst Werner von Siemens and James Clerk Maxwell recognized the importance of this discovery. Following the work of Russell Ohl in the 1940s, researchers Gerald Pearson, Calvin Fuller and Daryl Chapin created the crystalline silicon solar cell in 1954. These early solar cells cost US$286/watt and reached efficiencies of 4.5–6%. By 2012 available efficiencies exceeded 20%, and the maximum efficiency of research photovoltaics was in excess of 40%.

Concentrating Solar Power (CSP) systems use lenses or mirrors and tracking systems to focus a large area of sunlight into a small beam. The concentrated heat is then used as a heat source for a conventional power plant. A wide range of concentrating technologies exists; the most developed are the parabolic trough, the concentrating linear fresnel reflector, the Stirling dish, and the solar power tower. Various techniques are used to track the Sun and focus light. In all of these systems a working fluid is heated by the concentrated sunlight, and is then used for power generation or energy storage.

Sunlight has influenced building design since the beginning of architectural history. Advanced solar architecture and urban planning methods were first employed by the Greeks and Chinese, who oriented their buildings toward the south to provide light and warmth.

The common features of passive solar architecture are orientation relative to the Sun, compact proportion (a low surface area to volume ratio), selective shading (overhangs) and thermal mass. When these features are tailored to the local climate and environment, they can produce well-lit spaces that stay in a comfortable temperature range. Socrates' Megaron House is a classic example of passive solar design. The most recent approaches to solar design use computer modeling tying together solar lighting, heating and ventilation systems in an integrated solar design package. Active solar equipment such as pumps, fans, and switchable windows can complement passive design and improve system performance.

Urban heat islands (UHI) are metropolitan areas with higher temperatures than that of the surrounding environment. The higher temperatures result from increased absorption of solar energy by urban materials such as asphalt and concrete, which have lower albedos and higher heat capacities than those in the natural environment. A straightforward method of counteracting the UHI effect is to paint buildings and roads white and to plant trees in the area. Using these methods, a hypothetical "cool communities" program in Los Angeles has projected that urban temperatures could be reduced by approximately 3 °C at an estimated cost of US$1  billion, giving estimated total annual benefits of US$530  million from reduced air-conditioning costs and healthcare savings.

Agriculture and horticulture seek to optimize the capture of solar energy to optimize the productivity of plants. Techniques such as timed planting cycles, tailored row orientation, staggered heights between rows and the mixing of plant varieties can improve crop yields. While sunlight is generally considered a plentiful resource, the exceptions highlight the importance of solar energy to agriculture. During the short growing seasons of the Little Ice Age, French and English farmers employed fruit walls to maximize the collection of solar energy. These walls acted as thermal masses and accelerated ripening by keeping plants warm. Early fruit walls were built perpendicular to the ground and facing south, but over time, sloping walls were developed to make better use of sunlight. In 1699, Nicolas Fatio de Duillier even suggested using a tracking mechanism which could pivot to follow the Sun. Applications of solar energy in agriculture aside from growing crops include pumping water, drying crops, brooding chicks and drying chicken manure. More recently the technology has been embraced by vintners, who use the energy generated by solar panels to power grape presses.

Greenhouses convert solar light to heat, enabling year-round production and the growth (in enclosed environments) of specialty crops and other plants not naturally suited to the local climate. Primitive greenhouses were first used during Roman times to produce cucumbers year-round for the Roman emperor Tiberius. The first modern greenhouses were built in Europe in the 16th century to keep exotic plants brought back from explorations abroad. Greenhouses remain an important part of horticulture today. Plastic transparent materials have also been used to similar effect in polytunnels and row covers.

Development of a solar-powered car has been an engineering goal since the 1980s. The World Solar Challenge is a biannual solar-powered car race, where teams from universities and enterprises compete over across central Australia from Darwin to Adelaide. In 1987, when it was founded, the winner's average speed was and by 2007 the winner's average speed had improved to .
The North American Solar Challenge and the planned South African Solar Challenge are comparable competitions that reflect an international interest in the engineering and development of solar powered vehicles.

Some vehicles use solar panels for auxiliary power, such as for air conditioning, to keep the interior cool, thus reducing fuel consumption.

In 1975, the first practical solar boat was constructed in England. By 1995, passenger boats incorporating PV panels began appearing and are now used extensively. In 1996, Kenichi Horie made the first solar-powered crossing of the Pacific Ocean, and the "Sun21" catamaran made the first solar-powered crossing of the Atlantic Ocean in the winter of 2006–2007. There were plans to circumnavigate the globe in 2010.

In 1974, the unmanned AstroFlight Sunrise airplane made the first solar flight. On 29 April 1979, the "Solar Riser" made the first flight in a solar-powered, fully controlled, man-carrying flying machine, reaching an altitude of . In 1980, the "Gossamer Penguin" made the first piloted flights powered solely by photovoltaics. This was quickly followed by the "Solar Challenger" which crossed the English Channel in July 1981. In 1990 Eric Scott Raymond in 21 hops flew from California to North Carolina using solar power. Developments then turned back to unmanned aerial vehicles (UAV) with the "Pathfinder" (1997) and subsequent designs, culminating in the "Helios" which set the altitude record for a non-rocket-propelled aircraft at in 2001. The "Zephyr", developed by BAE Systems, is the latest in a line of record-breaking solar aircraft, making a 54-hour flight in 2007, and month-long flights were envisioned by 2010. As of 2016, Solar Impulse, an electric aircraft, is currently circumnavigating the globe. It is a single-seat plane powered by solar cells and capable of taking off under its own power. The design allows the aircraft to remain airborne for several days.

A solar balloon is a black balloon that is filled with ordinary air. As sunlight shines on the balloon, the air inside is heated and expands, causing an upward buoyancy force, much like an artificially heated hot air balloon. Some solar balloons are large enough for human flight, but usage is generally limited to the toy market as the surface-area to payload-weight ratio is relatively high.

Solar chemical processes use solar energy to drive chemical reactions. These processes offset energy that would otherwise come from a fossil fuel source and can also convert solar energy into storable and transportable fuels. Solar induced chemical reactions can be divided into thermochemical or photochemical. A variety of fuels can be produced by artificial photosynthesis. The multielectron catalytic chemistry involved in making carbon-based fuels (such as methanol) from reduction of carbon dioxide is challenging; a feasible alternative is hydrogen production from protons, though use of water as the source of electrons (as plants do) requires mastering the multielectron oxidation of two water molecules to molecular oxygen. Some have envisaged working solar fuel plants in coastal metropolitan areas by 2050 the splitting of seawater providing hydrogen to be run through adjacent fuel-cell electric power plants and the pure water by-product going directly into the municipal water system. Another vision involves all human structures covering the Earth's surface (i.e., roads, vehicles and buildings) doing photosynthesis more efficiently than plants.

Hydrogen production technologies have been a significant area of solar chemical research since the 1970s. Aside from electrolysis driven by photovoltaic or photochemical cells, several thermochemical processes have also been explored. One such route uses concentrators to split water into oxygen and hydrogen at high temperatures (). Another approach uses the heat from solar concentrators to drive the steam reformation of natural gas thereby increasing the overall hydrogen yield compared to conventional reforming methods. Thermochemical cycles characterized by the decomposition and regeneration of reactants present another avenue for hydrogen production. The Solzinc process under development at the Weizmann Institute of Science uses a 1 MW solar furnace to decompose zinc oxide (ZnO) at temperatures above . This initial reaction produces pure zinc, which can subsequently be reacted with water to produce hydrogen.

Thermal mass systems can store solar energy in the form of heat at domestically useful temperatures for daily or interseasonal durations. Thermal storage systems generally use readily available materials with high specific heat capacities such as water, earth and stone. Well-designed systems can lower peak demand, shift time-of-use to off-peak hours and reduce overall heating and cooling requirements.

Phase change materials such as paraffin wax and Glauber's salt are another thermal storage medium. These materials are inexpensive, readily available, and can deliver domestically useful temperatures (approximately ). The "Dover House" (in Dover, Massachusetts) was the first to use a Glauber's salt heating system, in 1948. Solar energy can also be stored at high temperatures using molten salts. Salts are an effective storage medium because they are low-cost, have a high specific heat capacity, and can deliver heat at temperatures compatible with conventional power systems. The Solar Two project used this method of energy storage, allowing it to store in its 68 m³ storage tank with an annual storage efficiency of about 99%.

Off-grid PV systems have traditionally used rechargeable batteries to store excess electricity. With grid-tied systems, excess electricity can be sent to the transmission grid, while standard grid electricity can be used to meet shortfalls. Net metering programs give household systems credit for any electricity they deliver to the grid. This is handled by 'rolling back' the meter whenever the home produces more electricity than it consumes. If the net electricity use is below zero, the utility then rolls over the kilowatt-hour credit to the next month. Other approaches involve the use of two meters, to measure electricity consumed vs. electricity produced. This is less common due to the increased installation cost of the second meter. Most standard meters accurately measure in both directions, making a second meter unnecessary.

Pumped-storage hydroelectricity stores energy in the form of water pumped when energy is available from a lower elevation reservoir to a higher elevation one. The energy is recovered when demand is high by releasing the water, with the pump becoming a hydroelectric power generator.

Beginning with the surge in coal use, which accompanied the Industrial Revolution, energy consumption has steadily transitioned from wood and biomass to fossil fuels. The early development of solar technologies starting in the 1860s was driven by an expectation that coal would soon become scarce. However, development of solar technologies stagnated in the early 20th  century in the face of the increasing availability, economy, and utility of coal and petroleum.

The 1973 oil embargo and 1979 energy crisis caused a reorganization of energy policies around the world. It brought renewed attention to developing solar technologies. Deployment strategies focused on incentive programs such as the Federal Photovoltaic Utilization Program in the US and the Sunshine Program in Japan. Other efforts included the formation of research facilities in the US (SERI, now NREL), Japan (NEDO), and Germany (Fraunhofer Institute for Solar Energy Systems ISE).

Commercial solar water heaters began appearing in the United States in the 1890s. These systems saw increasing use until the 1920s but were gradually replaced by cheaper and more reliable heating fuels. As with photovoltaics, solar water heating attracted renewed attention as a result of the oil crises in the 1970s, but interest subsided in the 1980s due to falling petroleum prices. Development in the solar water heating sector progressed steadily throughout the 1990s, and annual growth rates have averaged 20% since 1999. Although generally underestimated, solar water heating and cooling is by far the most widely deployed solar technology with an estimated capacity of 154  GW as of 2007.

The International Energy Agency has said that solar energy can make considerable contributions to solving some of the most urgent problems the world now faces:

The development of affordable, inexhaustible, and clean solar energy technologies will have huge longer-term benefits. It will increase countries' energy security through reliance on an indigenous, inexhaustible, and mostly import-independent resource, enhance sustainability, reduce pollution, lower the costs of mitigating climate change, and keep fossil fuel prices lower than otherwise. These advantages are global. Hence the additional costs of the incentives for early deployment should be considered learning investments; they must be wisely spent and need to be widely shared.
In 2011, a report by the International Energy Agency found that solar energy technologies such as photovoltaics, solar hot water, and concentrated solar power could provide a third of the world's energy by 2060 if politicians commit to limiting climate change and transitioning to renewable energy. The energy from the Sun could play a key role in de-carbonizing the global economy alongside improvements in energy efficiency and imposing costs on greenhouse gas emitters. "The strength of solar is the incredible variety and flexibility of applications, from small scale to big scale".

The International Organization for Standardization has established several standards relating to solar energy equipment. For example, ISO 9050 relates to glass in the building, while ISO 10217 relates to the materials used in solar water heaters.




</doc>
<doc id="27745" url="https://en.wikipedia.org/wiki?curid=27745" title="Standard conditions for temperature and pressure">
Standard conditions for temperature and pressure

Standard temperature and pressure are standard sets of conditions for experimental measurements to be established to allow comparisons to be made between different sets of data. The most used standards are those of the International Union of Pure and Applied Chemistry (IUPAC) and the National Institute of Standards and Technology (NIST), although these are not universally accepted standards. Other organizations have established a variety of alternative definitions for their standard reference conditions.

In chemistry, IUPAC changed the definition of standard temperature and pressure (STP) in 1982: 

STP should not be confused with the standard state commonly used in thermodynamic evaluations of the Gibbs energy of a reaction.

NIST uses a temperature of 20 °C (293.15 K, 68 °F) and an absolute pressure of 1 atm (14.696 psi, 101.325 kPa). This standard is also called normal temperature and pressure (abbreviated as NTP).

The International Standard Metric Conditions for natural gas and similar fluids are and 101.325 kPa.

In industry and commerce, standard conditions for temperature and pressure are often necessary to define the standard reference conditions to express the volumes of gases and liquids and related quantities such as the rate of volumetric flow (the volumes of gases vary significantly with temperature and pressure): standard cubic meters per second (sm/s), and normal cubic meters per second (nm/s).

However, many technical publications (books, journals, advertisements for equipment and machinery) simply state "standard conditions" without specifying them; often substituting the term with older "normal conditions", or "NC". In special cases this can lead to confusion and errors. Good practice always incorporates the reference conditions of temperature and pressure. If not stated, some room environment conditions are supposed, close to 1 atm pressure, 293 К (20 °C), and 0% humidity.

Before 1918, many professionals and scientists using the metric system of units defined the standard reference conditions of temperature and pressure for expressing gas volumes as being and . During those same years, the most commonly used standard reference conditions for people using the imperial or U.S. customary systems was and 14.696 psi (1 atm) because it was almost universally used by the oil and gas industries worldwide. The above definitions are no longer the most commonly used in either system of units.

Many different definitions of standard reference conditions are currently being used by organizations all over the world. The table below lists a few of them, but there are more. Some of these organizations used other standards in the past. For example, IUPAC has, since 1982, defined standard reference conditions as being 0 °C and 100 kPa (1 bar), in contrast to its old standard of 0 °C and 101.325 kPa (1 atm). The new value is the mean atmospheric pressure at an altitude of about 112 metres, which is closer to the worldwide median altitude of human habitation (194 m).

Natural gas companies in Europe, Australia, and South America have adopted 15 °C (59 °F) and 101.325 kPa (14.696 psi) as their standard gas volume reference conditions, used as the base values for defining the standard cubic meter. Also, the International Organization for Standardization (ISO), the United States Environmental Protection Agency (EPA) and National Institute of Standards and Technology (NIST) each have more than one definition of standard reference conditions in their various standards and regulations.
Note: This table needs careful checking. For example the American Association of Physicists in Medicine paper quotes a temperature of 22°C. It does not quote a Fahrenheit equivalent. The correct Fahrenheit equivalent is 71.6°F, not 72°F as stated in the table.

Abbreviations:

In aeronautics and fluid dynamics the "International Standard Atmosphere" (ISA) is a specification of pressure, temperature, density, and speed of sound at each altitude. The International Standard Atmosphere is representative of atmospheric conditions at mid latitudes. In the USA this information is specified the U.S. Standard Atmosphere which is identical to the "International Standard Atmosphere" at all altitudes up to 65,000 feet above sea level.

Because many definitions of standard temperature and pressure differ in temperature significantly from standard laboratory temperatures (e.g. 0 °C vs. ~25 °C), reference is often made to "standard laboratory conditions" (a term deliberately chosen to be different from the term "standard conditions for temperature and pressure", despite its semantic near identity when interpreted literally). However, what is a "standard" laboratory temperature and pressure is inevitably geography-bound, given that different parts of the world differ in climate, altitude and the degree of use of heat/cooling in the workplace. For example, schools in New South Wales, Australia use 25 °C at 100 kPa for standard laboratory conditions.
ASTM International has published Standard ASTM E41- Terminology Relating to Conditioning and hundreds of special conditions for particular materials and test methods. Other standards organizations also have specialized standard test conditions.

It is equally as important to indicate the applicable reference conditions of temperature and pressure when stating the molar volume of a gas as it is when expressing a gas volume or volumetric flow rate. Stating the molar volume of a gas without indicating the reference conditions of temperature and pressure has very little meaning and can cause confusion.

The molar volume of gases around STP and at atmospheric pressure can be calculated with an accuracy that is usually sufficient by using the ideal gas law. The molar volume of any ideal gas may be calculated at various standard reference conditions as shown below:

Technical literature can be confusing because many authors fail to explain whether they are using the ideal gas constant "R", or the specific gas constant "R". The relationship between the two constants is "R" = "R" / "m", where "m" is the molecular mass of the gas.

The US Standard Atmosphere (USSA) uses 8.31432 m·Pa/(mol·K) as the value of "R". However, the USSA,1976 does recognize that this value is not consistent with the values of the Avogadro constant and the Boltzmann constant.




</doc>
<doc id="27750" url="https://en.wikipedia.org/wiki?curid=27750" title="Script kiddie">
Script kiddie

In programming and hacking cultures, a script kiddie, skiddie, or skid is an unskilled individual who uses scripts or programs, such as a web shell, developed by others to attack computer systems and networks and deface websites. It is generally assumed that most script kiddies are juveniles who lack the ability to write sophisticated programs or exploits on their own and that their objective is to try to impress their friends or gain credit in computer-enthusiast communities. However, the term does not relate to the actual age of the participant. The term is considered to be derogatory.

In a Carnegie Mellon report prepared for the U.S. Department of Defense in 2005, script kiddies are defined as The more immature but unfortunately often just as dangerous exploiter of security lapses on the Internet. The typical script kiddy uses existing and frequently well known and easy-to-find techniques and programs or scripts to search for and exploit weaknesses in other computers on the Internet—often randomly and with little regard or perhaps even understanding of the potentially harmful consequences.

Script kiddies have at their disposal a large number of effective, easily downloadable programs capable of breaching computers and networks. Such programs have included remote denial-of-service WinNuke, trojans, Back Orifice, NetBus and Sub7 vulnerability scanner/injector kit Metasploit and often software intended for legitimate security auditing.

Script kiddies vandalize websites both for the thrill of it and to increase their reputation among their peers. Some more malicious script kiddies have used virus toolkits to create and propagate the Anna Kournikova and Love Bug viruses.
Script kiddies lack, or are only developing, programming skills sufficient to understand the effects and side effects of their actions. As a result, they leave significant traces which lead to their detection, or directly attack companies which have detection and countermeasures already in place, or in some cases, leave automatic crash reporting turned on.

One of the most common types of attack utilized by script kiddies involves a form of social engineering, whereby the attacker somehow manipulates or tricks a user into sharing their information. This is often done through the creation of fake websites where users will input their login (a form of phishing), thus allowing the script kiddie access to the account. 

An elitist subculture of hacking and programming communities, cheat developers, are responsible for the development and maintenance of clients. These individuals must circumvent the target program's security features to become undetected by the anti-cheat. Script kiddies are known to download, slightly modify, then take credit for the entire development of something that a cheat developer may have spent countless hours creating.





</doc>
<doc id="27751" url="https://en.wikipedia.org/wiki?curid=27751" title="Scalable Vector Graphics">
Scalable Vector Graphics

Scalable Vector Graphics (SVG) is an Extensible Markup Language (XML)-based vector image format for two-dimensional graphics with support for interactivity and animation. The SVG specification is an open standard developed by the World Wide Web Consortium (W3C) since 1999.

SVG images and their behaviors are defined in XML text files. This means that they can be searched, indexed, scripted, and compressed. As XML files, SVG images can be created and edited with any text editor, as well as with drawing software.

All major modern web browsers—including Mozilla Firefox, Internet Explorer, Google Chrome, Opera, Safari, and Microsoft Edge—have SVG rendering support.

SVG has been in development within the World Wide Web Consortium (W3C) since 1999 after six competing proposals for vector graphics languages had been submitted to the consortium during 1998. The early SVG Working Group decided not to develop any of the commercial submissions, but to create a new markup language that was informed by but not really based on any of them.

SVG allows three types of graphic objects: vector graphic shapes such as paths and outlines consisting of straight lines and curves, bitmap images, and text. Graphical objects can be grouped, styled, transformed and composited into previously rendered objects. The feature set includes nested transformations, clipping paths, alpha masks, filter effects and template objects. SVG drawings can be interactive and can include animation, defined in the SVG XML elements or via scripting that accesses the SVG Document Object Model (DOM). SVG uses CSS for styling and JavaScript for scripting. Text, including internationalization and localization, appearing in plain text within the SVG DOM, enhances the accessibility of SVG graphics.

The SVG specification was updated to version 1.1 in 2011. There are two 'Mobile SVG Profiles,' SVG Tiny and SVG Basic, meant for mobile devices with reduced computational and display capabilities. Scalable Vector Graphics 2 became a W3C Candidate Recommendation on 15 September 2016. SVG 2 incorporates several new features in addition to those of SVG 1.1 and SVG Tiny 1.2.

Though the SVG Specification primarily focuses on vector graphics markup language, its design includes the basic capabilities of a page description language like Adobe's PDF. It contains provisions for rich graphics, and is compatible with CSS for styling purposes. SVG has the information needed to place each glyph and image in a chosen location on a printed page.

SVG drawings can be dynamic and interactive. Time-based modifications to the elements can be described in SMIL, or can be programmed in a scripting language (e.g. ECMAScript or JavaScript). The W3C explicitly recommends SMIL as the standard for animation in SVG.

A rich set of event handlers such as ""onmouseover"" and ""onclick"" can be assigned to any SVG graphical object to apply actions and events.

SVG images, being XML, contain many repeated fragments of text, so they are well suited for lossless data compression algorithms. When an SVG image has been compressed with the gzip algorithm, it is referred to as an "SVGZ" image and uses the corresponding codice_1 filename extension. Conforming SVG 1.1 viewers will display compressed images. An SVGZ file is typically 20 to 50 percent of the original size. W3C provides SVGZ files to test for conformance.

SVG was developed by the W3C SVG Working Group starting in 1998, after six competing vector graphics submissions were received that year:
The working group was chaired at the time by Chris Lilley of the W3C.


SVG 2.0 removes or deprecates some features of SVG 1.1 and incorporates new features from HTML5 and Web Open Font Format:

It reached Candidate Recommendation stage on 15 September 2016. The latest draft was released on 16 February 2020.

Because of industry demand, two mobile profiles were introduced with SVG 1.1: "SVG Tiny" (SVGT) and "SVG Basic" (SVGB).

These are subsets of the full SVG standard, mainly intended for user agents with limited capabilities. In particular, SVG Tiny was defined for highly restricted mobile devices such as cellphones; it does not support styling or scripting. SVG Basic was defined for higher-level mobile devices, such as smartphones.

In 2003, the 3GPP, an international telecommunications standards group, adopted SVG Tiny as the mandatory vector graphics media format for next-generation phones. SVGT is the required vector graphics format and support of SVGB is optional for Multimedia Messaging Service (MMS) and Packet-switched Streaming Service. It was later added as required format for vector graphics in 3GPP IP Multimedia Subsystem (IMS).

Neither mobile profile includes support for the full Document Object Model (DOM), while only SVG Basic has optional support for scripting, but because they are fully compatible subsets of the full standard, most SVG graphics can still be rendered by devices which only support the mobile profiles.

SVGT 1.2 adds a microDOM (μDOM), styling and scripting.

The MPEG-4 Part 20 standard - "Lightweight Application Scene Representation (LASeR) and Simple Aggregation Format (SAF)" is based on SVG Tiny. It was developed by MPEG (ISO/IEC JTC1/SC29/WG11) and published as ISO/IEC 14496-20:2006. SVG capabilities are enhanced in MPEG-4 Part 20 with key features for mobile services, such as dynamic updates, binary encoding, state-of-art font representation. SVG was also accommodated in MPEG-4 Part 11, in the Extensible MPEG-4 Textual (XMT) format - a textual representation of the MPEG-4 multimedia content using XML.

The SVG 1.1 specification defines 14 functional areas or feature sets:


An SVG document can define components including shapes, gradients etc., and use them repeatedly. SVG images can also contain raster graphics, such as PNG and JPEG images, and further SVG images.

This code will produce the colored shapes shown in the image, excluding the grid and labels:

<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<svg width="391" height="391" viewBox="-70.5 -70.5 391 391" xmlns="http://www.w3.org/2000/svg">
<rect fill="#fff" stroke="#000" x="-70" y="-70" width="390" height="390"/>
<g opacity="0.8">
</g>
</svg>
The use of SVG on the web was limited by the lack of support in older versions of Internet Explorer (IE). Many web sites that serve SVG images, such as Wikipedia, also provide the images in a raster format, either automatically by HTTP content negotiation or by allowing the user directly to choose the file.

Google announced on 31 August 2010 that it had started to index SVG content on the web, whether it is in standalone files or embedded in HTML, and that users would begin to see such content listed among their search results.
It was announced on 8 December 2010 that Google Image Search would also begin indexing SVG files. The site announced an option to restrict image searches to SVG files on 11 February 2011. Web search engine can parse and navigate this format.

Konqueror was the first browser to support SVG in release version 3.2 in February 2004. As of 2011, all major desktop browsers, and many minor ones, have some level of SVG support. Other browsers' implementations are not yet complete; see comparison of layout engines for further details.

Some earlier versions of Firefox (e.g. versions between 1.5 and 3.6), as well as a smattering of other now-outdated web browsers capable of displaying SVG graphics, needed them embedded in codice_35 or codice_36 elements to display them integrated as parts of an HTML webpage instead of using the standard way of integrating images with codice_37. However, SVG images may be included in XHTML pages using XML namespaces.

Tim Berners-Lee, the inventor of the World Wide Web, was critical of early versions of Internet Explorer for its failure to support SVG.

There are several advantages to native and full support: plugins are not needed, SVG can be freely mixed with other content in a single document, and rendering and scripting become considerably more reliable.

SVG Tiny (SVGT) 1.1 and 1.2 are mobile profiles for SVG. SVGT 1.2 includes some features not found in SVG 1.1, including non-scaling strokes, which are supported by some SVG 1.1 implementations, such as Opera, Firefox and WebKit. As shared code bases between desktop and mobile browsers increased, the use of SVG 1.1 over SVGT 1.2 also increased.

Support for SVG may be limited to SVGT on older or more limited smart phones or may be primarily limited by their respective operating system. Adobe Flash Lite has optionally supported SVG Tiny since version 1.1. At the SVG Open 2005 conference, Sun demonstrated a mobile implementation of SVG Tiny 1.1 for the Connected Limited Device Configuration (CLDC) platform.

Mobiles that use Opera Mobile, as well as the iPhone's built in browser, also include SVG support. However, even though it used the WebKit engine, the Android built-in browser did not support SVG prior to v3.0 (Honeycomb). Prior to v3.0, Firefox Mobile 4.0b2 (beta) for Android was the first browser running under Android to support SVG by default.

The level of SVG Tiny support available varies from mobile to mobile, depending on the SVG engine installed. Many newer mobile products support additional features beyond SVG Tiny 1.1, like gradient and opacity; this is sometimes referred to as "SVGT 1.1+", though there is no such standard.

RIM's BlackBerry has built-in support for SVG Tiny 1.1 since version 5.0. Support continues for WebKit-based BlackBerry Torch browser in OS 6 and 7.

Nokia's S60 platform has built-in support for SVG. For example, icons are generally rendered using the platform's SVG engine. Nokia has also led the JSR 226: Scalable 2D Vector Graphics API expert group that defines Java ME API for SVG presentation and manipulation. This API has been implemented in S60 Platform 3rd Edition Feature Pack 1 and onward. Some Series 40 phones also support SVG (such as Nokia 6280).

Most Sony Ericsson phones beginning with K700 (by release date) support SVG Tiny 1.1. Phones beginning with K750 also support such features as opacity and gradients. Phones with Sony Ericsson Java Platform-8 have support for JSR 226.

Windows Phone has supported SVG since version 7.5.

SVG is also supported on various mobile devices from Motorola, Samsung, LG, and Siemens mobile/BenQ-Siemens. eSVG, an SVG rendering library mainly written for embedded devices, is available on some mobile platforms.

SVG images can be produced by the use of a vector graphics editor, such as Inkscape, Adobe Illustrator, Adobe Flash Professional, or CorelDRAW, and rendered to common raster image formats such as PNG using the same software. Additionally, editors like Inkscape and Boxy SVG provide tools to trace raster images to Bézier curves typically using back-ends like potrace, autotrace, and imagetracerjs.

Software can be programmed to render SVG images by using a library such as librsvg used by GNOME since 2000, or Batik. SVG images can also be rendered to any desired popular image format by using ImageMagick, a free command-line utility (which also uses librsvg under the hood).

Other uses for SVG include embedding for use in word processing (e.g. with LibreOffice) and desktop publishing (e.g. Scribus), plotting graphs (e.g. gnuplot), and importing paths (e.g. for use in GIMP or Blender). Microsoft 365 and Microsoft Office 2019 offer support for importing and editing SVG images. The Uniform Type Identifier for SVG used by Apple is codice_38 and conforms to codice_39 and codice_40.




</doc>
<doc id="27752" url="https://en.wikipedia.org/wiki?curid=27752" title="Spectroscopy">
Spectroscopy

Spectroscopy, (or spectrometry), is the study of the electromagnetic spectrum of a source of electromagnetic radiation. Generally the spectrum is the analysis, the decomposition of a physical quantity in the energy domain (or scale), or in the frequency, the wavelength, etc.).

Spectroscopy is the study of the interaction between matter and electromagnetic radiation as a function of the wavelength or frequency of the radiation. Historically, spectroscopy originated as the study of the wavelength dependence of the absorption by gas phase matter of visible light dispersed by a prism. An elementary description of absorption, emission and scattering spectroscopy is given in Chapter 1 of a student level text book. We can also consider matter waves and acoustic waves as forms of radiative energy, and recently gravitational waves have been associated with a spectral signature in the context of the Laser Interferometer Gravitational-Wave Observatory (LIGO).
Spectroscopy, primarily in the electromagnetic spectrum, is a fundamental exploratory tool in the fields of physics, chemistry, and astronomy, allowing the composition, physical structure and electronic structure of matter to be investigated at the atomic, molecular and macro scale, and over astronomical distances. Important applications arise from biomedical spectroscopy in the areas of tissue analysis and medical imaging. 

Spectroscopy and spectrography are terms used to refer to the measurement of radiation intensity as a function of wavelength and are often used to describe experimental spectroscopic methods. Spectral measurement devices are referred to as spectrometers, spectrophotometers, spectrographs or spectral analyzers.

Daily observations of color can be related to spectroscopy. Neon lighting is a direct application of atomic spectroscopy. Neon and other noble gases have characteristic emission frequencies (colors). Neon lamps use collision of electrons with the gas to excite these emissions. Inks, dyes and paints include chemical compounds selected for their spectral characteristics in order to generate specific colors and hues. A commonly encountered molecular spectrum is that of nitrogen dioxide. Gaseous nitrogen dioxide has a characteristic red absorption feature, and this gives air polluted with nitrogen dioxide a reddish-brown color. Rayleigh scattering is a spectroscopic scattering phenomenon that accounts for the color of the sky.

Spectroscopic studies were central to the development of quantum mechanics and included Max Planck's explanation of blackbody radiation, Albert Einstein's explanation of the photoelectric effect and Niels Bohr's explanation of atomic structure and spectra. Spectroscopy is used in physical and analytical chemistry because atoms and molecules have unique spectra. As a result, these spectra can be used to detect, identify and quantify information about the atoms and molecules. Spectroscopy is also used in astronomy and remote sensing on Earth. Most research telescopes have spectrographs. The measured spectra are used to determine the chemical composition and physical properties of astronomical objects (such as their temperature and velocity).

One of the central concepts in spectroscopy is a resonance and its corresponding resonant frequency. Resonances were first characterized in mechanical systems such as pendulums. Mechanical systems that vibrate or oscillate will experience large amplitude oscillations when they are driven at their resonant frequency. A plot of amplitude vs. excitation frequency will have a peak centered at the resonance frequency. This plot is one type of spectrum, with the peak often referred to as a spectral line, and most spectral lines have a similar appearance.

In quantum mechanical systems, the analogous resonance is a coupling of two quantum mechanical stationary states of one system, such as an atom, via an oscillatory source of energy such as a photon. The coupling of the two states is strongest when the energy of the source matches the energy difference between the two states. The energy formula_1 of a photon is related to its frequency formula_2 by formula_3 where formula_4 is Planck's constant, and so a spectrum of the system response vs. photon frequency will peak at the resonant frequency or energy. Particles such as electrons and neutrons have a comparable relationship, the de Broglie relations, between their kinetic energy and their wavelength and frequency and therefore can also excite resonant interactions.

Spectra of atoms and molecules often consist of a series of spectral lines, each one representing a resonance between two different quantum states. The explanation of these series, and the spectral patterns associated with them, were one of the experimental enigmas that drove the development and acceptance of quantum mechanics. The hydrogen spectral series in particular was first successfully explained by the Rutherford-Bohr quantum model of the hydrogen atom. In some cases spectral lines are well separated and distinguishable, but spectral lines can also overlap and appear to be a single transition if the density of energy states is high enough. Named series of lines include the principal, sharp, diffuse and fundamental series.

Spectroscopy is a sufficiently broad field that many sub-disciplines exist, each with numerous implementations of specific spectroscopic techniques. The various implementations and techniques can be classified in several ways.

The types of spectroscopy are distinguished by the type of radiative energy involved in the interaction. In many applications, the spectrum is determined by measuring changes in the intensity or frequency of this energy. The types of radiative energy studied include:

The types of spectroscopy also can be distinguished by the nature of the interaction between the energy and the material. These interactions include:

Spectroscopic studies are designed so that the radiant energy interacts with specific types of matter.

Atomic spectroscopy was the first application of spectroscopy developed. Atomic absorption spectroscopy and atomic emission spectroscopy involve visible and ultraviolet light. These absorptions and emissions, often referred to as atomic spectral lines, are due to electronic transitions of outer shell electrons as they rise and fall from one electron orbit to another. Atoms also have distinct x-ray spectra that are attributable to the excitation of inner shell electrons to excited states.

Atoms of different elements have distinct spectra and therefore atomic spectroscopy allows for the identification and quantitation of a sample's elemental composition. After inventing the spectroscope, Robert Bunsen and Gustav Kirchhoff discovered new elements by observing their emission spectra. Atomic absorption lines are observed in the solar spectrum and referred to as Fraunhofer lines after their discoverer. A comprehensive explanation of the hydrogen spectrum was an early success of quantum mechanics and explained the Lamb shift observed in the hydrogen spectrum, which further led to the development of quantum electrodynamics.

Modern implementations of atomic spectroscopy for studying visible and ultraviolet transitions include flame emission spectroscopy, inductively coupled plasma atomic emission spectroscopy, glow discharge spectroscopy, microwave induced plasma spectroscopy, and spark or arc emission spectroscopy. Techniques for studying x-ray spectra include X-ray spectroscopy and X-ray fluorescence.

The combination of atoms into molecules leads to the creation of unique types of energetic states and therefore unique spectra of the transitions between these states. Molecular spectra can be obtained due to electron spin states (electron paramagnetic resonance), molecular rotations, molecular vibration, and electronic states. Rotations are collective motions of the atomic nuclei and typically lead to spectra in the microwave and millimeter-wave spectral regions. Rotational spectroscopy and microwave spectroscopy are synonymous. Vibrations are relative motions of the atomic nuclei and are studied by both infrared and Raman spectroscopy. Electronic excitations are studied using visible and ultraviolet spectroscopy as well as fluorescence spectroscopy.

Studies in molecular spectroscopy led to the development of the first maser and contributed to the subsequent development of the laser.

The combination of atoms or molecules into crystals or other extended forms leads to the creation of additional energetic states. These states are numerous and therefore have a high density of states. This high density often makes the spectra weaker and less distinct, i.e., broader. For instance, blackbody radiation is due to the thermal motions of atoms and molecules within a material. Acoustic and mechanical responses are due to collective motions as well.
Pure crystals, though, can have distinct spectral transitions, and the crystal arrangement also has an effect on the observed molecular spectra. The regular lattice structure of crystals also scatters x-rays, electrons or neutrons allowing for crystallographic studies.

Nuclei also have distinct energy states that are widely separated and lead to gamma ray spectra. Distinct nuclear spin states can have their energy separated by a magnetic field, and this allows for nuclear magnetic resonance spectroscopy.

Other types of spectroscopy are distinguished by specific applications or implementations:


The history of spectroscopy began with Isaac Newton's optics experiments (1666–1672). Newton applied the word "spectrum" to describe the rainbow of colors that combine to form white light and that are revealed when the white light is passed through a prism. During the early 1800s, Joseph von Fraunhofer made experimental advances with dispersive spectrometers that enabled spectroscopy to become a more precise and quantitative scientific technique. Since then, spectroscopy has played and continues to play a significant role in chemistry, physics, and astronomy.







</doc>
<doc id="27753" url="https://en.wikipedia.org/wiki?curid=27753" title="List of science fiction themes">
List of science fiction themes

The following is a list of articles about recurring themes in science fiction.










</doc>
<doc id="27754" url="https://en.wikipedia.org/wiki?curid=27754" title="Samaritanism">
Samaritanism

The Samaritan religion, also known as Samaritanism, is the national religion of the Samaritans. The Samaritans adhere to the Samaritan Torah, which they believe is the original, unchanged Torah, as opposed to the Torah used by Jews. In addition to the Samaritan Torah, Samaritans also revere their version of the Book of Joshua and recognize some Biblical figures, such as Eli.

Samaritanism is internally described as the religion that began with Moses, unchanged over the millennia that have since passed. Samaritans believe Judaism and the Jewish Torah have been corrupted by time and no longer serve the duties God mandated on Mount Sinai. Jews view the Temple Mount as the most sacred location in their faith, but Samaritans regard Mount Gerizim as their holiest site.

Samaritanism holds that the summit of Mount Gerizim is the true location of God's Holy Place, as opposed to the Foundation Stone on the Temple Mount as Judaism teaches. As such, Samaritans trace their history as a separate entity from the Jews back to the time of Moses, where they believe Joshua laid the foundation for their temple. Samaritan historiography traces the schism itself to the High Priest Eli abandoning Moses' Tabernacle in favor of Mount Gerizim following Joshua's death.

Abu l-Fath, who in the 14th century wrote a major work of Samaritan history, comments on Samaritan origins as follows:

A terrible civil war broke out between Eli son of Yafni, of the line of Ithamar, and the sons of Pincus (Phinehas), because Eli son of Yafni resolved to usurp the High Priesthood from the descendants of Pincus. He used to offer sacrifices on an altar of stones. He was 50 years old, endowed with wealth and in charge of the treasury of the Children of Israel. ...

He offered a sacrifice on the altar, but without salt, as if he were inattentive. When the Great High Priest Ozzi learned of this, and found the sacrifice was not accepted, he thoroughly disowned him; and it is (even) said that he rebuked him.

Thereupon he and the group that sympathized with him, rose in revolt and at once he and his followers and his beasts set off for Shiloh. Thus Israel split in factions. He sent to their leaders saying to them, "Anyone who would like to see wonderful things, let him come to me." Then he assembled a large group around him in Shiloh, and built a Temple for himself there; he constructed a place like the Temple (on Mount Gerizim). He built an altar, omitting no detail—it all corresponded to the original, piece by piece.

At this time the Children of Israel split into three factions. A loyal faction on Mount Gerizim; a heretical faction that followed false gods; and the faction that followed Eli son of Yafni in Shiloh.

Further, the "Samaritan Chronicle Adler", or New Chronicle, believed to have been composed in the 18th century using earlier chronicles as sources, states:

Samaritanism emerged as an independent ethnic culture following its survival of the Assyrian captivity in the 8th century BC. The traditional Jewish narrative of 2 Kings and Josephus, has it that the people of Israel were removed by the king of the Assyrians (Sargon II) to Halah, to Gozan on the Khabur River and to the towns of the Medes. The king of the Assyrians, we are then told, then brought people from Babylon, Kutha, Avah, Emath, and Sepharvaim to settle in Samaria. Because God sent lions among them to kill them, the king of the Assyrians sent one of the priests from Bethel to teach the new settlers about God's ordinances. The eventual result was that the new settlers worshiped both the God of the land and their own gods from the countries from which they came. 

Modern genetic studies (2004) suggest that Samaritans' lineages trace back to a common ancestor with Jews in the paternally-inherited Jewish high priesthood (Cohanim) temporally proximate to the period of the Assyrian conquest of the kingdom of Israel, and are probably descendants of the historical Israelite population, albeit isolated given the people's reclusive history. This casts doubt into, if not totally disproves, this historical theory that Samaritans originated from Assyria.

Furthermore, the Dead Sea scroll 4Q372, which recounts the hope that the northern tribes will return to the land of Joseph, remarks that the current dwellers in the north are fools, an enemy people, but does not explicitly refer to them as foreigners. It goes on to say that these people, the Samaritans, mocked Jerusalem and built a temple on a high place (Gerizim) to provoke Israel.

Conflicts between the Samaritans and the Jews were numerous between the end of the Assyrian diaspora and the Bar Kokhba revolt. The Tanakh describes multiple instigations from the Samaritan population against the Jews and disparages them, and Jesus' Parable of the Good Samaritan also gives evidence of conflict. The destruction of Mount Gerizim's Samaritan temple is attributed to the High Priest John Hyrcanus.

Following the failed revolts, Mount Gerizim was rededicated with a new temple, which was ultimately again destroyed during the Samaritan Revolts. Persecution of Samaritans was common in the following centuries.

The principal beliefs of Samaritanism are as follows:

The Samaritan preserve the proto-Hebraic script, conserve the institution of a High Priesthood, and the practice of slaughtering and eating lambs on Passover eve. They celebrate Pesach, Shavuot, Sukkot but use a different mode from that employed in Judaism in order to determine the dates annually.Yom Teru'ah (the Biblical name for "Rosh Hashanah"), at the beginning of Tishrei, is not considered a New Year as it is in Rabbinic Judaism.

Passover is particularly important in the Samaritan community, climaxing with the sacrifice of up to 40 sheep. The Counting of the Omer remains largely unchanged; however, the week before Shavuot is a unique festival celebrating the continued commitment Samaritanism has maintained since the time of Moses. Shavuot is characterized by nearly day-long services of continuous prayer, especially over the stones on Gerizim traditionally attributed to Joshua. 

During Sukkot, the sukkah is built inside houses, as opposed to outdoor settings that are traditional among Jews. Samaritan historian Benyamim Tsedaka traces the indoor-sukkah tradition to persecution of Samaritans during the Byzantine Empire. The roof of the Samaritan sukkah is decorated with citrus fruits and the branches of palm, myrtle, and willow trees, according to the Samaritan interpretation of the four species designated in the Torah for the holiday.

The restrictions of Yom Kippur are more universal in Samaritanism, with even breastfeeding and the feeding of children being disallowed.

Samaritan law differs from Halakha (Rabbinic Jewish law) and other Jewish movements. The Samaritans have several groups of religious texts, which correspond to Jewish Halakha. A few examples of such texts are:




</doc>
<doc id="27760" url="https://en.wikipedia.org/wiki?curid=27760" title="Statute of Anne">
Statute of Anne

The Statute of Anne, also known as the Copyright Act 1710 (cited either as 8 Ann. c. 21 or as 8 Ann. c. 19), is an act of the Parliament of Great Britain passed in 1710, which was the first statute to provide for copyright regulated by the government and courts, rather than by private parties.

Prior to the statute's enactment in 1710, copying restrictions were authorized by the Licensing of the Press Act 1662. These restrictions were enforced by the Stationers' Company, a guild of printers given the exclusive power to print—and the responsibility to censor—literary works. The censorship administered under the Licensing Act led to public protest; as the act had to be renewed at two-year intervals, authors and others sought to prevent its reauthorisation. In 1694, Parliament refused to renew the Licensing Act, ending the Stationers' monopoly and press restrictions.

Over the next 10 years the Stationers repeatedly advocated bills to re-authorize the old licensing system, but Parliament declined to enact them. Faced with this failure, the Stationers decided to emphasise the benefits of licensing to authors rather than publishers, and the Stationers succeeded in getting Parliament to consider a new bill. This bill, which after substantial amendments was granted Royal Assent on 5 April 1710, became known as the Statute of Anne owing to its passage during the reign of Queen Anne. The new law prescribed a copyright term of 14 years, with a provision for renewal for a similar term, during which only the author and the printers to whom they chose to license their works could publish the author's creations. Following this, the work's copyright would expire, with the material falling into the public domain. Despite a period of instability known as the Battle of the Booksellers when the initial copyright terms under the Statute began to expire, the Statute of Anne remained in force until the Copyright Act 1842 replaced it.

The statute is considered a "watershed event in Anglo-American copyright history ... transforming what had been the publishers' private law copyright into a public law grant". Under the statute, copyright was for the first time vested in authors rather than publishers; it also included provisions for the public interest, such as a legal deposit scheme. The Statute was an influence on copyright law in several other nations, including the United States, and even in the 21st century is "frequently invoked by modern judges and academics as embodying the utilitarian underpinnings of copyright law".

With the introduction of the printing press to England by William Caxton in 1476, printed works became both more common and more economically important. As early as 1483, Richard III recognised the value of literary works by specifically exempting them from the government's protectionist legislation. Over the next fifty years, the government moved further towards economic regulation, abolishing the provision with the Printers and Binders Act 1534, which also banned the import of foreign works and empowered the Lord Chancellor to set maximum pricing for English books. This was followed by increasing degrees of censorship. A further proclamation of 1538, aiming to stop the spread of Lutheran doctrine, saw Henry VIII note that "sondry contentious and sinyster opiniones, have by wrong teachynge and naughtye bokes increaced and growen within this his realme of England", and declare that all authors and printers must allow the Privy Council or their agents to read and censor books before publication.

This censorship peaked on 4 May 1557, when Mary I issued a royal warrant formally incorporating the Stationers' Company. The old method of censorship had been limited by the Second Statute of Repeal, and with Mary's increasing unpopularity the existing system was unable to cope with the number of critical works being printed. Instead, the royal warrant devolved this power to the Company. This was done by decreeing that only the Company's publishers could print and distribute books. Their Wardens were given the power to enter any printing premises, destroy illegal works and imprison anyone found manufacturing them. In this way the government "harnessed the self interest of the publishers to the yoke of royal incentive", guaranteeing that the Company would follow the rules due to the economic monopoly it gave their members. With the abolition of the Star Chamber and Court of High Commission by the Long Parliament, the legal basis for this warrant was removed, but the Long Parliament chose to replace it with the Licensing Act 1662. This provided that the Company would retain their original powers, and imposed additional restrictions on printing; King's Messengers were permitted to enter any home or business in search of illegal presses. The legislation required renewal every two years, and was regularly reapproved.

This was not "copyright" as is normally understood; although there was a monopoly on the right to copy, this was available to publishers, not authors, and did not exist by default; it only applied to books which had been accepted and published by the Company. A member of the Company would register the book, and would then have a perpetual copyright over its printing, copying and publication, which could be leased, transferred to others or given to heirs upon the member's death. The only exception to this was that, if a book was out of print for more than 6 months and the publisher ignored a warning to make it available, the copyright would be released and other publishers would be permitted to copy it. Authors themselves were not particularly respected until the 18th century, and were not permitted to be members of the Company, playing no role in the development or use of its licences despite the Company's sovereign authority to decide what was published. There is evidence that some authors were recognised by the Company itself to have the right to copy and the right to alter their works; these authors were uniformly the writers of uneconomical books who were underwriting their publication.

The Company's monopoly, censorship and failure to protect authors made the system highly unpopular; John Milton wrote "Areopagitica" as a result of his experiences with the Company, accusing Parliament of being deceived by "the fraud of some old patentees and monopolisers in the trade of bookselling". He was not the first writer to criticise the system, with John Locke writing a formal memorandum to the MP Edward Clarke in 1693 while the Licensing Act was being renewed, complaining that the existing system restricted the free exchange of ideas and education while providing an unfair monopoly for Company members. Academic Mark Rose attributes the efforts of Milton to promote the "bourgeois public sphere", along with the Glorious Revolution's alterations to the political system and the rise of public coffee houses, as the source of growing public unhappiness with the system. At the same time, this was a period in which clearly defined political parties were taking shape, and with the promise of regular elections, an environment where the public were of increasing importance to the political process. The result was a "developing public sphere [which] provided the context that enabled the collapse of traditional press controls".

The result of this environment was the lapse of the Licensing Act. In November 1694, a committee was appointed by the Commons to see what laws were "lately expired and expiring [and] fit to be revived and continued". The Committee reported in January 1695, and suggested the renewal of the Licensing Act; this was included in the "Continuation Bill", but rejected by the House of Commons on 11 February. When it reached the House of Lords, the Lords re-included the Licensing Act, and returned the bill to the Commons. In response, a second committee was appointed – this one to produce a report indicating why the Commons disagreed with the inclusion of the Licensing Act, and chaired by Edward Clarke. This committee soon reported to the Commons, and Clarke was ordered to carry a message to the Lords requesting a conference over the Act. On 18 April 1695, Clarke met with representatives of the Lords, and they agreed to allow the Continuation Bill to pass without the renewal of the Licensing Act. With this, "the Lords' decision heralded an end to a relationship that had developed throughout the sixteenth and seventeenth centuries between the State and the Company of Stationers", ending both nascent publishers' copyright and the existing system of censorship.

John Locke's close relationship with Clarke, along with the respect he commanded, is seen by academics as what led to this decision. Locke had spent the early 1690s campaigning against the statute, considering it "ridiculous" that the works of dead authors were held perpetually in copyright. In letters to Clarke he wrote of the absurdity of the existing system, complaining primarily about the unfairness of it to authors, and "[t]he parallels between Locke's commentary and those reasons presented by the Commons to the Lords for refusing to renew the 1662 Act are striking". He was assisted by a number of independent printers and booksellers, who opposed the monopolistic aspects of the Act, and introduced a petition in February 1693 that the Act prevented them from conducting their business. The "developing public sphere", along with the harm the existing system had caused to both major political parties, is also seen as a factor.

The failure to renew the Licensing Act led to confusion and both positive and negative outcomes; while the government no longer played a part in censoring publications, and the monopoly of the Company over printing was broken, there was uncertainty as to whether or not copyright was a binding legal concept without the legislation. Economic chaos also resulted; with the Company now unable to enforce any monopoly, provincial towns began establishing printing presses, producing cheaper books than the London booksellers. The absence of the censorship provisions also opened Britain up as a market for internationally printed books, which were similarly cheaper than those British printers could produce.

The rejection of the existing system was not done with universal approval, and there were ultimately twelve unsuccessful attempts to replace it. The first was introduced to the House of Commons on 11 February 1695. A committee, again led by Clarke, was to write a "Bill for the Better Regulating of Printing and the Printing Presses". This bill was essentially a copy of the Licensing Act, but with a narrower jurisdiction; only books covering religion, history, the affairs of the state or the law would require official authorisation. Four days after its introduction, the Stationers' held an emergency meeting to agree to petition the Commons – this was because the bill did not contain any reference to books as property, eliminating their monopoly on copying. Clarke also had issues with the provisions, and the debate went on until the end of the Parliamentary session, with the bill failing to pass.

With the end of the Parliamentary session came the first general election under the Triennial Act 1694, which required the Monarch to dissolve Parliament every 3 years, causing a general election. This led to the "golden age" of the English electorate, and allowed for the forming of two major political parties – the Whigs and Tories. At the same time, with the failure to renew the Licensing Act, a political press developed. While the Act had been in force only one official newspaper existed; the "London Gazette", published by the government. After its demise, a string of newspapers sprang into being, including the "Flying Post", the "Evening Post" and the "Daily Courant". Newspapers had a strong bias towards particular parties, with the "Courant" and the "Flying Post" supporting the Whigs and the "Evening Post" in favour of the Tories, leading to politicians from both parties realising the importance of an efficient propaganda machine in influencing the electorate. This added a new dimension to the Commons' decision to reject two new renewals of the Licensing Act in the new Parliamentary session.

Authors, as well as Stationers, then joined the demand for a new system of licensing. Jonathan Swift was a strong advocate for licensing, and Daniel Defoe wrote on 8 November 1705 that with the absence of licensing, "One Man Studies Seven Year, to bring a finish'd Peice into the World, and a Pyrate Printer, Reprints his Copy immediately, and Sells it for a quarter of the Price ... these things call for an Act of Parliament". Seeing this, the Company took the opportunity to experiment with a change to their approach and argument. Instead of lobbying because of the effect the absence of legislation was having on their trade, they lobbied on behalf of the authors, but seeking the same things. The first indication of this change in approach comes from the 1706 pamphlet by John How, a stationer, titled "Reasons humbly Offer'd for a Bill for the Encouragement of Learning and the Improvement of Printing". This argued for a return to licensing, not with reference to the printers, but because without something to protect authors and guarantee them an income, "Learned men will be wholly discouraged from Propagating the most useful Parts of Knowledge and Literature". Using these new tactics and the support of authors, the Company petitioned Parliament again in both 1707 and 1709 to introduce a bill providing for copyright.

Although both bills failed, they led to media pressure that was exacerbated by both Defoe and How. Defoe's "A Review", published on 3 December 1709 and demanding "a Law in the present Parliament ... for the Encouragement of Learning, Arts, and Industry, by securing the Property of Books to the Authors or Editors of them", was followed by How's "Some Thoughts on the Present State of Printing and Bookselling", which hoped that Parliament "might think fit to secure Property in Books by a Law". This was followed by another review by Defoe on 6 December, in which he even went so far as to provide a draft text for the bill. On 12 December, the Stationers submitted yet another petition asking for legislation on the issue, and the House of Commons gave three MPs – Spencer Compton, Craven Peyton and Edward Wortley – permission to form a drafting committee. On 11 January 1710, Wortley introduced this bill, titling it "A Bill for the Encouragement of Learning and for Securing the Property of Copies of Books to the rightful Owners thereof".

The bill imposed fines on anyone who imported or traded in unlicensed or foreign books, required every book for which copyright protection was sought to be entered into the Stationers' Register, provided a legal deposit system centred around the King's Library, the University of Oxford and the University of Cambridge, but said nothing about limiting the term of copyright. It also specified that books were property; an emphasis on the idea that authors deserved copyright simply due to their efforts. The Stationers were enthusiastic, urging Parliament to pass the bill, and it received its second reading on 9 February. A Committee of the Whole met to amend it on 21 February, with further alterations made when it was passed back to the House of Commons on 25 February. Alterations during this period included minor changes, such as extending the legal deposit system to cover Sion College and the Faculty of Advocates, but also major ones, including the introduction of a limit on the length of time for which copyright would be granted.

Linguistic amendments were also included; the line in the preamble emphasising that authors possessed books as they would any other piece of property was dropped, and the bill moved from something designed "for Securing the Property of Copies of Books to the rightful Owners thereof" to a bill "for the Encouragement of Learning, by Vesting the Copies of Printed Books in the Authors or Purchasers of such Copies". Another amendment allowed anyone to own and trade in copies of books, undermining the Stationers. Other changes were made when the bill went to the House of Lords, and it was finally returned to the Commons on 5 April. The aims of the resulting statute are debated; Ronan Deazley suggests that the intent was to balance the rights of the author, publisher and public in such a way as to ensure the maximum dissemination of works, while other academics argue that the bill was intended to protect the Company's monopoly or, conversely, to weaken it. Oren Bracha, writing in the "Berkeley Technology Law Journal", says that when considering which of these options are correct, "the most probable answer [is] all of them". Whatever the motivations, the bill was passed on 5 April 1710, and is commonly known simply as the Statute of Anne due its passage during the reign of Queen Anne.

Consisting of 11 sections, the Statute of Anne is formally titled "An Act for the Encouragement of Learning, by Vesting the Copies of Printed Books in the Authors or Purchasers of Copies, during the Times therein mentioned". The preamble for the Statute indicates the purpose of the legislation – to bring order to the book trade – saying: 
The Statute then continued by stating the nature of copyright. The right granted was the right to copy; to have sole control over the printing and reprinting of books, with no provision to benefit the owner of this right after the sale. This right, previously held by the Stationers' Company's members, would automatically be given to the author as soon as it was published, although they had the ability to license these rights to another person. The copyright could be gained through two stages; first, the registration of the book's publication with the Company, to prevent unintentional infringement, and second, the deposit of copies of the book at the Stationers' Company, the royal library and various universities. One restriction on copyright was a "cumbersome system" designed to prohibit unreasonably high prices for books, which limited how much authors could charge for copies. There was also a prohibition on importing foreign works, with exceptions made for Latin and Greek classics.

Once registration had been completed and the deposits were made, the author was granted an exclusive right to control the copying of the book. Penalties for infringing this right were severe, with all infringing copies to be destroyed and large fines to be paid to both the copyright holder and the government; there was only a three-month statute of limitations on bringing a case, however. This exclusive right's length was dependent on when the book had been published. If it was published after 10 April 1710, the length of copyright was 14 years; if published before that date, 21 years. An author who survived until the copyright expired would be granted an additional 14-year term, and when that ran out, the works would enter the public domain. Copyright under the Statute applied to Scotland and England, as well as Ireland when that country joined the union in 1800.

The Statute was initially welcomed, ushering in "stability to an insecure book trade" while providing for a "pragmatic bargain" between the rights of the author, publisher and public intended to boost public learning and the availability of knowledge. The clause requiring book deposits, however, was not seen as a success. If the books were not deposited, the penalties would be severe, with a fine of £5. The number of deposits required, however, meant that it was a substantial burden; a print run might only be of 250 copies, and if they were particularly expensive to print, it could be cheaper to ignore the law. Some booksellers argued that the deposit provision only applied to registered books, and so deliberately avoided registration just to be able to minimise their liability. This was further undermined by the ruling in "Beckford v Hood", where the Court of King's Bench confirmed that, even without registration, copyright could be enforced against infringers.

Another failure, identified by Bracha, is not found in what the Statute covered, but in what it did not. The Statute did not provide any means for identifying authors, did not identify what constituted authored works, and covered only "books", even while discussing "property" as a whole. Moreover, the right provided was merely that of "making and selling ... exact reprints. To a large extent, the new regime was the old stationer's privilege, except it was universalised, capped in time, and formally conferred upon authors rather than publishers". The effect of the Statute on authors was also minimal. Previously, publishers would have bought the original manuscript from writers for a lump sum; with the passage of the Statute, they simply did the same thing, but with the manuscript's copyright as well. The remaining economic power of the Company also allowed them to pressure booksellers and distributors into continuing their past arrangements, meaning that even theoretically "public domain" works were, in practise, still treated as copyrighted.

When the copyrights granted to works published before the Statute began to expire in 1731, the Stationers' Company and their publishers again began to fight to preserve the status quo. Their first port of call was Parliament, where they lobbied for new legislation to extend the length of copyright, and when this failed, they turned to the courts. Their principal argument was that copyright had not been created by the Statute of Anne; it existed beforehand, in the common law, and was perpetual. As such, even though the Statute provided for a limited term, all works remained in copyright under the common law regardless of when statutory copyright expired. Starting in 1743, this began a thirty-year campaign known as the "Battle of the Booksellers". They first tried going to the Court of Chancery and applying for injunctions prohibiting other publishers from printing their works, and this was initially successful. A series of legal setbacks over the next few years, however, left the law ambiguous.

The first major action taken to clarify the situation was "Millar v Taylor". Andrew Millar, a British publisher, purchased the rights to James Thomson's "The Seasons" in 1729, and when the copyright term expired, a competing publisher named Robert Taylor began issuing his own reprints of the work. Millar sued, and went to the Court of King's Bench to obtain an injunction and advocate perpetual copyright at common law. The jury found that the facts submitted by Millar were accurate, and asked the judges to clarify whether common law copyright existed. The first arguments were delivered on 30 June 1767, with John Dunning representing Millar and Edward Thurlow representing Taylor. A second set of arguments were submitted for Millar by William Blackstone on 7 June, and judgment was given on 20 April 1769. The final decision, written by Lord Mansfield and endorsed by Aston and Willes JJ, confirmed that there existed copyright at common law that turned "upon Principles before and independent" of the Statute of Anne, something justified because it was right "that an Author should reap the pecuniary Profits of his own Ingenuity and Labour". In other words, regardless of the Statute, there existed a perpetual copyright under the common law. Yates J dissented, on the grounds that the focus on the author obscured the effect this decision would have on "the rest of mankind", which he felt would be to create a virtual monopoly, something that would harm the public and should certainly not be considered "an encouragement of the propagation of learning".

Although this decision was a boon to the Stationers, it was short-lived. Following "Millar", the right to print "The Seasons" was sold to a coalition of publishers including Thomas Becket. Two Scottish printers, Alexander and John Donaldson, began publishing an unlicensed edition, and Becket successfully obtained an injunction to stop them. This decision was appealed in "Donaldson v Beckett", and eventually went to the House of Lords. After consulting with the judges of the King's Bench, Common Pleas and Exchequer of Pleas, the Lords concluded that copyright was not perpetual, and that the term permitted by the Statute of Anne was the maximum length of legal protection for publishers and authors alike.

Until its repeal, most extensions to copyright law were based around provisions found in the Statute of Anne. The one successful bill from the lobbying in the 1730s, which came into force on 29 September 1739, extended the provision prohibiting the import of foreign books to also prohibit the import of books that, while originally published in Britain, were being reprinted in foreign nations and then shipped to England and Wales. This was intended to stop the influx of cheap books from Ireland, and also repealed the price restrictions in the Statute of Anne. Another alteration was over the legal deposit provisions of the Statute, which many booksellers found unfair. Despite an initial period of compliance, the principle of donating copies of books to certain libraries lapsed, partly due to the unwieldiness of the statute's provisions and partly because of a lack of cooperation by the publishers. In 1775 Lord North, who was Chancellor of the University of Oxford, succeeded in passing a bill that reiterated the legal deposit provisions and granted the universities perpetual copyright on their works.

Another range of extensions came in relation to what could be copyrighted. The Statute only referred to books, and being an Act of Parliament, it was necessary to pass further legislation to include various other types of intellectual property. The Engraving Copyright Act 1734 extended copyright to cover engravings, statutes in 1789 and 1792 involved cloth, sculptures were copyrighted in 1814 and the performance of plays and music were covered by copyright in 1833 and 1842 respectively. The length of copyright was also altered; the Copyright Act 1814 set a copyright term of either 28 years, or the natural life of the author if this was longer. Despite these expansions, some still felt copyright was not a strong enough regime. In 1837, Thomas Noon Talfourd introduced a bill into Parliament to expand the scope of copyright. A friend of many men of letters, Talfourd aimed to provide adequate rewards for authors and artists. He campaigned for copyright to exist for the life of the author, with an additional 60 years after that. He also proposed that existing statutes be codified under the bill, so that the case law that had arisen around the Statute of Anne was clarified.

Talfourd's proposals led to opposition, and he reintroduced modified versions of them year on year. Printers, publishers and booksellers were concerned about the cost implications for original works, and for reprinting works that had fallen out of copyright. Many within Parliament argued that the bill failed to take into account the public interest, including Lord Macaulay, who succeeded in defeating one of Talfourd's bills in 1841. The Copyright Act 1842 passed, but "fell far short of Talfourd's dream of a uniform, consistent, codified law of copyright". It extended copyright to life plus seven years, and, as part of the codification clauses, repealed the Statute of Anne.

The Statute of Anne is traditionally seen as "a historic moment in the development of copyright", and the first statute in the world to provide for copyright. Craig Joyce and Lyman Ray Patterson, writing in the "Emory Law Journal", call this a "too simple understanding [that] ignores the statute's source", arguing that it is at best a derivative of the Licensing Act. Even considering this, however, the Statute of Anne was "the watershed event in Anglo-American copyright history ... transforming what had been the publishers' private law copyright into a public law grant". Patterson, writing separately, does note the differences between the Licensing Act and the Statute of Anne; the question of censorship was, by 1710, out of the question, and in that regard the Statute is distinct, not providing for censorship.

It also marked the first time that copyright had been vested primarily in the author, rather than the publisher, and also the first time that the injurious treatment of authors by publishers was recognised; regardless of what authors signed away, the second 14-year term of copyright would automatically return to them. Even in the 21st century, the Statute of Anne is "frequently invoked by modern judges and academics as embodying the utilitarian underpinnings of copyright law". In "IceTV v Nine Network", for example, the High Court of Australia noted that the title of the Statute "echoed explicitly the emphasis on the practical or utilitarian importance that certain seventeenth-century philosophers attached to knowledge and its encouragement in the scheme of human progress". Despite "widely recognised flaws", the Act became a model copyright statute, both within the United Kingdom and internationally. Christophe Geiger notes that it is "a difficult, almost impossible task" to analyse the relationship between the Statute of Anne and early French copyright law, both because it is difficult to make a direct connection, and because the ongoing debate over both has led to radically different interpretations of each nation's law.

Similarly, Belgium took no direct influence from the Statute or English copyright theory, but Joris Deene of the University of Ghent identifies an indirect influence "at two levels"; the criteria for what constitutes copyrightable material, which comes from the work of English theorists such as Locke and Edward Young, and the underlying justification of copyright law. In Belgium, this justification is both that copyright serves the public interest, and that copyright is a "private right" that serves the interests of individual authors. Both theories were taken into account in "Donaldson v Beckett", as well as in the drafting of the Statute of Anne, and Deene infers that they subsequently affected the Belgian debates over their first copyright statute. In the United States, the Copyright Clause of the United States Constitution and the first Federal copyright statute, the Copyright Act of 1790, both draw on the Statute of Anne. The 1790 Act contains provisions for a 14-year term of copyright and sections that provide for authors who published their works before 1790, both of which mirror the protection offered by the Statute 80 years previously.





</doc>
<doc id="27761" url="https://en.wikipedia.org/wiki?curid=27761" title="School choice">
School choice

"School choice" is a term for pre-college public education options, describing a wide array of programs offering students and their families voluntary alternatives to publicly provided schools, to which students are generally assigned by the location of their family residence. In the United States, the most common—both by number of programs and by number of participating students—school choice programs are scholarship tax credit programs, which allow individuals or corporations to receive tax credits toward their state taxes in exchange for donations made to non-profit organizations that grant private school scholarships. In other cases, a similar subsidy may be provided by the state through a school voucher program. Other school choice options include open enrollment laws (which allow students to attend public schools outside the district in which the students live), charter schools, magnet schools, virtual schools, homeschooling, education savings accounts (ESAs), and individual tax credits or deductions for educational expenses. School choice is supported by international human rights law including the Universal Declaration of Human Rights art. 26, and the Convention Against Discrimination in Education.

Economist and Nobel laureate Milton Friedman proposed in 1955 using free market principles to improve the United States public school system. The practice had been that children were assigned a public school based on where their parents live, which public schools were funded by state and local taxes. Friedman proposed that parents should be able to receive those education funds in the form of vouchers, which would allow them to choose their children's schools, including both public and private, religious and non-religious options. In 1996, Friedman and his wife economist Rose Director Friedman founded the Friedman Foundation for Educational Choice, (now EdChoice). This American education reform organization headquartered in Indianapolis, Indiana seeks to advance “school choice for all children” nationwide.

The first use of school vouchers in the United States came in the form of state tuition grants provided by Virginia's 1956 Stanley Plan, which financed white-only private schools known as segregation academies. Other states followed until the practice was disallowed by "Griffin v. County School Board of Prince Edward County" (1964). While "school choice" has always implied school improvement, debates have regularly followed about the motivation and implementation.

States with scholarship tax credit programs grant individuals and/or businesses a credit, whether full or partial, toward their taxes for donations made to scholarship granting organizations (also called school tuition organizations). SGOs/STOs use the donations to create scholarships that are then given to help pay for the cost of tuition for students. These scholarships allow students to attend private schools or out-of-district public schools that would otherwise be prohibitively expensive for many families. These programs currently exist in fourteen states: Alabama, Arizona, Florida, Georgia, Illinois, Iowa, Kansas, Louisiana, Minnesota, New Hampshire, Oklahoma, Pennsylvania, Rhode Island, and Virginia in the United States.

Vouchers give students the opportunity to attend a private school of their choosing, secular or religious. This would be paid for by accessing all or part of the public funding set aside for their children’s education.

Charter schools are independent public schools which are exempt from many of the state and local regulations which govern most public schools. These exemptions grant charter schools some autonomy and flexibility with decision-making, such as teacher union contracts, hiring, and curriculum. In return, charter schools are subject to stricter accountability on spending and academic performance. The majority of states (and the District of Columbia) have charter school laws, though they vary in how charter schools are approved. Minnesota was the first state to have a charter school law and the first charter school in the United States, City Academy High School, opened in St. Paul, Minnesota in 1992. The prevalence of charter schools has increased with the support of the Obama Administration. Under the Administration, the Department of Education has provided funding incentives to states and school districts that increase the number of charter schools.

Somewhere between 22 and 26% of Dayton, Ohio children are in charter schools. This is the highest percentage in the nation. Other hotbeds for charter schools are Kansas City (24%), Washington, D.C. (20-24%), and Arizona. Almost one in four public schools in Arizona are charter schools, comprising about 8% of total enrollment.

Charter schools can also come in the form of cyber charters. Cyber charter schools deliver the majority of their instruction over the internet instead of in a school building. And, like all charter schools, cyber charters are public schools, but they are free from some of the rules and regulations that conventional public schools must follow.

Magnet schools are public schools that often have a specialized function like science, technology, or art. These magnet schools, unlike charter schools, are not open to all children. Much like many private schools, some (but not all) magnet schools require a test to get in. Magnet schools are an example of open enrollment programs. Open enrollment refers to district or statewide programs that allow families to choose public schools other than the ones they are assigned. Intradistrict open enrollment programs allow school choice within a district. Interdistrict open enrollment allows families to choose schools outside the district in other districts.

"Home education" or "home schooling" is instruction in a child's home, or provided primarily by a parent, or under direct parental control. Informal home education has always taken place, and formal instruction in the home has at times also been very popular. As public education grew in popularity during the 1900s, however, the number of people educated at home using a planned curriculum dropped. In the last 20 years, in contrast, the number of children being formally educated at home has grown tremendously, in particular in the United States. The laws relevant to home education differ throughout the country. In some states the parent simply needs to notify the state that the child will be educated at home. In other states the parents are not free to educate at home unless at least one parent is a certified teacher and yearly progress reports are reviewed by the state. Such laws are not always enforced however. According to the federal government, about 1.1 million children were home educated in 2003.

District of Choice is a program in California created during 1993, allowing any California public school district to enroll students outside district lines. To participate in the program, district governing boards only need to declare themselves a District of Choice and set a quota for how many transfer students to accept to participate in the program. School districts cannot discriminate among students to enroll, but can limit them through an unbiased lottery system. The program was created in response to several parent's concerns over the lack of choice of schools to enroll their children in. Currently 47 school districts and 10,000 students participate in the program, serving 5 percent of school districts and 0.2 percent of students in California.

This variant of school choice allows the parent to withdraw their child out of the public or charter school, and receive a direct deposit of public funds into a government-authorized savings account. These funds are often distributed in the form of a debit card that can be used to pay for various services, such as private school tuition and fees, online programs, private tutoring, community college costs, higher education services, and other approved learning materials and services. ESA’s also acquire the ability to pay for a combination of public school courses and private services.

Certain states allow parents to claim a tax credit or deduction as a means to provide relief for certain educational expenses. These can include private school tuition, textbooks, school supplies and equipment, tutoring, and transportation. Currently, Alabama, Illinois, Indiana, Iowa, Louisiana, Minnesota, and Wisconsin have such programs.

This form of choice abates the income tax for parents, so approved educational expenses can be more economical. Approved educational expenses include private school tuition, supplies, computers, books, tutors, and transportation.

Online learning permits students to work with teachers and their courses over the internet. This can be used in cooperation with, or in place of traditional classroom instruction. The online learning can be also paid for by accessing ESA’s and vouchers.

This form of tutelage is a student-tailored form of education. This form of instruction can have various combinations. For example, course choice programs, public school courses, and special education therapies can all be integrated into a students curriculum. There are a myriad of possibilities, especially as learning innovations continue to occur.

The goal of school choice programs is to give parents more control over their child's education and to allow parents to pursue the most appropriate learning environments for children. For example, school choice may enable parents to choose a school that provides religious instruction, stronger discipline, better foundational skills (including reading, writing, mathematics, and science), everyday skills (from handling money to farming), or other desirable foci.

Supporters of voucher models of school choice argue that choice creates competition between schools for students. Schools that fail to attract students can be closed. Advocates of school choice argue that this competition for students (and the dollars that come with them) create a catalyst for schools to create innovative programs, become more responsive to parental demands, and to increase student achievement. Caroline Hoxby suggests that this competition increases the productivity of a school. Hoxby describes a productive school as being one that produces high student achievement for each dollar spent. Others suggest that this competition gives parents more power to influence their child's school in the school marketplace. Parents and students become the consumers and schools must work to attract new students with new programs. Parents also have the ability to punish schools that they judge to be inferior by leaving the 'bad' school for a better, more highly ranked school. Parents look for schools that will advocate for the needs of their child and if the school does not meet the needs required for that child, parents have the choice to find a school that will be more suitable. This freedom to choose puts the consequences of good or bad choosing on the parents instead of the government.

Another argument in favor of school choice is based on cost-effectiveness. Studies undertaken by the Cato Institute and other libertarian and conservative think tanks conclude that privately run education both costs less and produces superior outcomes compared to public education.

Others argue that since children from impoverished families almost exclusively attend D or F ranked public schools, school choice programs would give parents the power to opt their children out of poorly-performing schools assigned by zip code and seek better education elsewhere. Supporters say this would level the playing field by broadening opportunities for low-income students—particularly minorities—to attend high-quality schools that would otherwise be accessible only to higher-income families.

The Organisation Internationale pour le Droit à l'Education et la Liberté d'Enseignement (OIDEL), an international non-profit organization for the development of freedom of education, maintains that the right to education is a fundamental human right which cannot exist without the presence of State benefits and the protection of individual liberties. According to the organization, freedom of education notably implies the freedom for parents to choose a school for their children without discrimination on the basis of finances. To advance freedom of education, OIDEL promotes a greater parity between public and private schooling systems.

The The Walton Foundation has also held charter school investment conferences featuring Standard & Poor's, Piper Jaffray, Bank of America, and Wells Capital Management.

Teachers' unions in the United States are very opposed to school choice. School choice measures are criticized as profiteering in an under-regulated environment. Charter authorization organizations have non-profit status; and contract with related for-profit entities with public funding. Reports indicate that charters create organizational arms that profit by charging high rent, and that while the facilities are used as schools, there are no property taxes.

Public school entities are chiefly concerned that these school choice measures are taking funding away from public schools and therefore depleting their already strained resources. Other opponents of certain school choice policies (particularly vouchers) have cited the Establishment Clause and individual state Blaine amendments, which forbid, to one degree or another, the use of direct government aid to religiously affiliated entities. This is of particular concern in the voucher debate because voucher dollars are often spent at parochial schools.

Some school choice measures are criticized by public school entities, organizations opposed to church-state entanglement, and self-identified liberal advocacy groups. Known plaintiffs who have filed suit to challenge the constitutionality of state sponsored school choice laws are as follows: School Boards Associations, Public School Districts, Federations for Teachers, Associations of School Business Officials, Education Associations/Associations of Educators (unions for public school teachers), the American Civil Liberties Union, Freedom From Religion Foundation, and People for the American Way.

There is evidence that school choice programs reduce housing prices, and that they do so in high-performing districts more than in low-performing districts.

The basic compulsory educational system in Finland is the nine-year comprehensive school (Finnish "peruskoulu", Swedish "grundskola", "basic school"), for which school attendance is mandatory (homeschooling is allowed, but extremely rare). There are no so-called "gifted" programs. The more able children are expected to help those who are slower to catch on.

The French government subsidizes most private primary and secondary schools, including those affiliated with religious denominations, under contracts stipulating that education must follow the same curriculum as public schools and that schools cannot discriminate on grounds of religion or force pupils to attend religion classes.

This system of "école libre" (Free Schooling) is mostly used not for religious reasons, but for practical reasons (private schools may offer more services, such as after-class tutoring) as well as the desire of parents living in disenfranchised areas to send their children away from the local schools, where they perceive that the youth are too prone to delinquency or have too many difficulties keeping up with schooling requirements that the educational content is bound to suffer. The threatened repealing of that status in the 1980s triggered mass street demonstrations () in favor of the status.

Sweden reformed its school system in 1992. Its system of school choice is one of the freest in the world, allowing students to use public funds for the publicly or privately run school of their choice, including religious and for-profit schools. Fifteen years after the reform, private school enrollment had increased from 1% to 10% of the student population.

In Chile, there is an extensive voucher system in which the state pays private and municipal schools directly, based on average attendance (90% of the country students utilize such a system). The result has been a steady increase in the number and recruitment of private schools that show consistently better results in standardized testing than municipal schools. The reduction of students in municipal schools has gone from 78% of all students in 1981, to 57% in 1990, and to less than 50% in 2005.

Regarding vouchers in Chile, researchers have found that when controls for the student's background (parental income and education) are introduced, the difference in performance between public and private subsectors is not significant. There is also greater variation within each subsector than between the two systems.

A variety of forms of school choice exist in the United States. It is a highly debatable subject because some people wish to use taxpayer dollars in order to allow low-income students the choice of a private schools by way of vouchers. 

Scholarship tax credit programs currently exist in Alabama, Arizona, Florida, Georgia, Illinois, Iowa, Kansas, Louisiana, Minnesota, New Hampshire, Oklahoma, Pennsylvania, Rhode Island, and Virginia.

Arizona has a well-known and fast-growing tax credit program. In the Arizona Individual Private School Tuition Tax Credit Program, in accordance with A.R.S. §43-1089 and §1089.03, individuals can claim up to $1,053 and couples filing joint returns can claim up to $2106 (for 2014, amounts are indexed annually). Nearly 24,000 children received scholarships in the 2011-2012 school year. Since the program has started in 1998, over 77,500 taxpayers have participated in the program, providing over $500 million in scholarship money for children at private schools across the state.

The Arizona program was challenged in court in "ACSTO v Winn" by a group of state taxpayers on the grounds that the tax credit violated the First Amendment because the tuition grants could go to students who attend private schools with religious affiliations. The suit was initially brought against the state until the Arizona Christian School Tuition Organization (ACSTO), one of the largest School Tuition Organizations in the state, voluntarily stepped in to represent the defense with the help of the Alliance Defending Freedom (formerly Alliance Defense Fund). Typically, taxpayers are not allowed to bring suit against the government regarding how taxes are spent because injury would be purely speculative. In addition, insomuch as a donation to a School Tuition Organization is still a charitable act, just like any donation to a charity, there would be no standing unless all charitable deduction programs nationwide were brought under scrutiny. The Court ruled 5-4 to let the tax credit program stand. In April 2011, a Fairleigh Dickinson University PublicMind poll found that a majority of American voters (60%) felt that the tax credits support school choice for parents whereas 26% felt as it the tax credits support religion.

In Iowa, the Educational Opportunities Act was signed into law in 2006, creating a pool of tax credits for eligible donors to student tuition organizations (STOs). At first, these tax caps were $5 million but in 2007, Governor Chet Culver increased the total amount to $7.5 million. The Iowa Alliance for Choice in Education (Iowa ACE) oversees the STOs and advocates for school choice in Iowa.

Greater Opportunities for Access to Learning (GOAL) is the Georgia program which offers a state income tax credit to donors of scholarships to private schools. Representative David Casas was responsible for passing the Georgia version of the school choice legislation.

Vouchers currently exist in Wisconsin, Ohio, Florida, Indiana and, most recently, the District of Columbia and Georgia.

The largest and oldest Voucher program is in Milwaukee. Started in 1990, and expanded in 1995, it currently allows no more than 15% of the district's public school enrollment to use vouchers. As of 2005 over 14,000 students use vouchers and they are nearing the 15% cap.

School vouchers are legally controversial in some states. In 2014 a lawsuit sought to challenge the legality of the Florida voucher program.

In the U.S., the legal and moral precedents for vouchers may have been set by the G.I. bill, which includes a voucher program for university-level education of veterans. The G.I. bill permits veterans to take their educational benefits at religious schools, an extremely divisive issue when applied to primary and secondary schools.

In "Zelman v. Simmons-Harris", 536 U.S. 639 (2002), the Supreme Court of the United States held that school vouchers could be used to pay for education in sectarian schools without violating the Establishment Clause of the First Amendment. As a result, states are basically free to enact voucher programs that provide funding for any school of the parent's choosing.

The Supreme Court has not decided, however, whether states can provide vouchers for secular schools only, excluding sectarian schools. Proponents of funding for parochial schools argue that such an exclusion would violate the free exercise clause. However, in "Locke v. Davey", 540 U.S. 712 (2004), the Court held that states could exclude majors in "devotional theology" from an otherwise generally available college scholarship. The Court has not indicated, however, whether this holding extends to the public school context, and it may well be limited to the context of individuals training to enter the ministry.

The majority of states (and the District of Columbia) have charter school laws. Minnesota was the first state to have a charter school law and the first charter school in the United States, City Academy, opened in St. Paul, Minnesota in 1992.

Dayton, Ohio has between 22–26% of all children in charter schools. This is the highest percentage in the nation. Other hotbeds for charter schools are Kansas City (24%), Washington, D.C. (20-24%) and the State of Arizona. Almost 1 in 4 public schools in Arizona are charter schools, comprising about 8% of total enrollment.

Charter schools can also come in the form of Cyber Charters. Cyber charter schools deliver the majority of their instruction over the internet instead of in a school building. And, like charter schools, they are public schools, but free of many of the rules and regulations that public schools must follow.

Magnet schools are public schools that often have a specialized function like science, technology or art. These magnet schools, unlike charter schools, are not open to all children. Much like many private schools, the students must test into the school.

The laws relevant to homeschooling differ between US states. In some states the parent simply needs to notify the state that the child will be educated at home. In other states the parents are not free to educate at home unless at least one parent is a certified teacher and yearly progress reports are reviewed by the state. Such laws are not always enforced however. According to the Federal Government, about 1.1 million children were Home Educated in 2003.

The United States has school choice at the university level. College students can get subsidized tuition by attending "any" public college or university within their state of residence. Furthermore, the U.S. federal government provides tuition assistance for both public and private colleges via the G.I. Bill and federally guaranteed student loans.




</doc>
<doc id="27762" url="https://en.wikipedia.org/wiki?curid=27762" title="Star Frontiers">
Star Frontiers

Star Frontiers is a science fiction role-playing game produced by TSR beginning in 1982. The game offered a space opera action-adventure setting.

"Star Frontiers" takes place near the center of a spiral galaxy (the setting does not specify whether the galaxy is our own Milky Way). 
A previously undiscovered quirk of the laws of physics allows starships to jump to "The Void", a hyperspatial realm that greatly shortens the travel times between inhabited worlds, once they reach 1% of the speed of light (3,000 km/s).

The basic game setting was an area known as "The Frontier Sector" where four sentient races (Dralasite, Humans, Vrusk, and Yazirian) had met and formed the United Planetary Federation (UPF). The original homeworlds of the Dralasites, Humans, and Vrusk were never detailed in the setting and it is possible that they no longer existed. A large number of the star systems shown on the map of the Frontier sector in the basic rulebook were unexplored and undetailed, allowing the Gamemaster (called the "referee" in the game) to put whatever they wished there.

Players could take on any number of possible roles in the setting but the default was to act as hired agents of the Pan Galactic corporation in exploring the Frontier and fighting the aggressive incursions of the alien and mysterious worm-like race known as the Sathar. Most published modules for the game followed these themes.


These races were altered heavily and reused in TSR's "Spelljammer", and were later loosely republished for "d20 Future" by Wizards of the Coast.


The game was a percentile-based system and used only 10-sided dice (d10). Characters had attributes rated from 1-100 (usually in the 25-75 range) which could be rolled against for raw-attribute actions such as lifting items or getting out of the way of falling rocks. There were eight attributes that were paired together (and shared the same rating to begin with)—Strength/Stamina, Dexterity/Reaction Speed, Intuition/Logic, and Personality/Leadership.

Characters also each had a Primary Skill Area (PSA—Military, Technological, or Biosocial) which allowed them to buy skills that fell into their PSA at a discount. Skills were rated from 1–6 and usually consisted of a set of subskills that gave a chance for accomplishing a particular action as a base percentage plus a 10% bonus for each skill level the character had in the skill. Weapon skills were based on the character's relevant attribute (Dexterity or Strength) but other skills had a base chance of success independent of the character's attributes. Many of the technological skills were penalized by the complexity of the robot, security system, or computer the character was attempting to manipulate (also rated from 1 to 6).

Characters were usually quite durable in combat—it would take several hits from normal weapons to kill an average character. Medical technology was also advanced—characters could recover quickly from wounds with appropriate medical attention and a dead character could be "frozen" and revived later.

Vehicle and robot rules were included in the "Alpha Dawn" basic set. A beneficial feature of the game was its seamless integration of personal, vehicle and aerial combat simulation. The "Knight Hawks" rules expansion set included detailed rules for starships.
The basic set also included a short "bestiary" of creatures native to the world of Volturnus (the setting for the introductory module included with the basic boxed set), along with rules for creating new creatures.

Character advancement consisted of spending experience points on improving skills and attributes.

The basic boxed set was renamed "Alpha Dawn" after the expansions began publication. It included two ten-sided dice, a large set of cardboard counters, and a folding map with a futuristic city on one side and various wilderness areas on the other for use with the included adventure, SF-0: "Crash on Volturnus".

A second boxed set called "Knight Hawks" followed shortly. It provided rules for using starships in the setting and also a set of wargame rules for fighting space battles between the UPF and Sathar. Included were counters for starships, two-ten sided dice, a large folding map with open space on one side and on the other a space station and starship (for use with the included adventure), and the adventure SFKH-0: "Warriors of White Light". This set was designed by Douglas Niles (who also designed the D&D wargame "Battlesystem", released two years later).

Adventures printed separately for the game included two more adventures set on Volturnus (SF-1: "Volturnus, Planet of Mystery" and SF-2: "Starspawn of Volturnus" continuing the adventure included in the basic set), SF-3: "Sundown on Starmist", SF-4: "Mission to Alcazzar", SF-5: "Bugs in the System" and SF-6: "Dark Side of the Moon". The last two modules (SF-5 and SF-6) were written by authors from TSR's UK division, and are distinctly different from the others in the series in tone and production style.

Adventures using the "Knight Hawks" rules included SFKH-1: "Dramune Run" and a trilogy set "Beyond the Frontier" in which the players learn more about the Sathar and foil their latest plot (SFKH-2: "Mutiny on the Eleanor Moraes", SFKH-3: "Face of the Enemy", and SFKH-4: "The War Machine").

Two modules also re-created the plot and setting of the movies "" and "".

A late addition to the line was "Zebulon's Guide to Frontier Space" which introduced several additional races and radical changes to the game's mechanics. Of the three planned volumes of the Guide, only the first was ever published (in 1985), leaving the game in a partially-overhauled state. Gamers were given little to no practical advice on how to convert their existing characters to the new rules, and TSR never published any further products using the "Zebulon's" concepts.

Wizards of the Coast published many of the races originally found in "Star Frontiers" in their "d20 Future" supplement for d20 Modern.

A version of the setting called "Star Law", which uses the d20 system rules was published as an alternate campaign setting in the d20 Future book. It uses the species names of Vrusk, Dralasite, Sathar, and Yazirian, but is not actually the "Star Frontiers" setting.

Andy Slack reviewed "Star Frontiers" for "White Dwarf" #37, giving it an overall rating of 7 out of 10, and stated that "Unfortunately, I can't say the system struck me as especially realistic; but if you like action adventure, thinking with your fists, and "Star Wars" (and who doesn't from time to time) you can have a lot of fun with this game."

William A. Barton reviewed "Star Frontiers" in "The Space Gamer" No. 60. Barton commented that ""Star Frontiers" probably isn't going to lose TSR any money. But I wish there were a lot more to commend it than that."

Jim Bambra reviewed "Star Frontiers" for "Imagine" magazine, and said that "In summary, the "Starfrontiers" game is an excellent introduction to Sci Fi gaming, a game I heartily recommend to beginners and experienced gamers. A lot of expertise has gone into the designing of this product and the result is a very enjoyable and easy to learn game."




</doc>
<doc id="27763" url="https://en.wikipedia.org/wiki?curid=27763" title="Structuralism">
Structuralism

In sociology, anthropology, and linguistics, structuralism is a general theory of culture and methodology that implies that elements of human culture must be understood by way of their relationship to a broader system. It works to uncover the structures that underlie all the things that humans do, think, perceive, and feel. 

Alternatively, as summarized by philosopher Simon Blackburn, structuralism is:[T]he belief that phenomena of human life are not intelligible except through their interrelations. These relations constitute a structure, and behind local variations in the surface phenomena there are constant laws of abstract structure.Structuralism in Europe developed in the early 20th century, mainly in France and the Russian Empire, in the structural linguistics of Ferdinand de Saussure and the subsequent Prague, Moscow, and Copenhagen schools of linguistics. As an intellectual movement, structuralism was initially presumed to be the heir apparent to existentialism. In the late 1950s and early 1960s, when structural linguistics came to be challenged by thinkers as Noam Chomsky and faded in importance, an array of scholars in the humanities borrowed Saussure's concepts for use in their respective fields. French anthropologist Claude Lévi-Strauss was arguably the first such scholar, sparking a widespread interest in structuralism.

The structuralist mode of reasoning has since been applied in a range of fields, including anthropology, sociology, psychology, literary criticism, economics, and architecture. Along with Lévi-Strauss, the most prominent thinkers associated with structuralism include linguist Roman Jakobson and psychoanalyst Jacques Lacan.

By the late 1960s, many of structuralism's basic tenets came under attack from a new wave of predominantly French intellectuals/philosophers such as historian Michel Foucault, Jacques Derrida, Marxist philosopher Louis Althusser, and literary critic Roland Barthes. Though elements of their work necessarily relate to structuralism and are informed by it, these theorists have generally been referred to as post-structuralists. In the 1970s, structuralism was criticized for its rigidity and ahistoricism. Despite this, many proponents of structuralism, such as Lacan, continue to influence continental philosophy and many of the fundamental assumptions of some of structuralism's post-structuralist critics are a continuation of structuralism.

Throughout the 1940s and 1950s, existentialism, such as that propounded by Jean-Paul Sartre, was the dominant European intellectual movement. Structuralism rose to prominence in France in the wake of existentialism, particularly in the 1960s. The initial popularity of structuralism in France led to its spread across the globe. By the early 1960s, structuralism as a movement was coming into its own and some believed that it offered a single unified approach to human life that would embrace all disciplines.

The term "structuralism" in reference to social science first appeared in the works of French anthropologist Claude Lévi-Strauss, who gave rise to the structuralist movement in France, influencing the thinking of other writers, most of whom disavowed themselves as being a part of this movement. This included such writers as Louis Althusser and psychoanalyst Jacques Lacan, as well as the structural Marxism of Nicos Poulantzas. Roland Barthes and Jacques Derrida focused on how structuralism could be applied to literature.

Accordingly, the so-called "Gang of Four" of structuralism is considered to be Lévi-Strauss, Lacan, Barthes, and Michel Foucault.

The origins of structuralism are connected with the work of Ferdinand de Saussure on linguistics, which has its roots in Pāṇini's grammar, along with the linguistics of the Prague and Moscow schools. In brief, Saussure's structural linguistics propounded three related concepts.

Structuralism rejected the concept of human freedom and choice, focusing instead on the way that human experience and behaviour is determined by various structures. The most important initial work on this score was Lévi-Strauss's 1949 volume "The Elementary Structures of Kinship". Lévi-Strauss had known Roman Jakobson during their time together at the New School in New York during WWII and was influenced by both Jakobson's structuralism, as well as the American anthropological tradition. 

In "Elementary Structures", he examined kinship systems from a structural point of view and demonstrated how apparently different social organizations were different permutations of a few basic kinship structures. In the late 1950s, he published "Structural Anthropology", a collection of essays outlining his program for structuralism.

Blending Freud and Saussure, French (post)structuralist Jacques Lacan applied structuralism to psychoanalysis. Similarly, Jean Piaget applied structuralism to the study of psychology, though in a different way. Piaget, who would better define himself as constructivist, considered structuralism as "a method and not a doctrine," because, for him, "there exists no structure without a construction, abstract or genetic."

Proponents of structuralism argue that a specific domain of culture may be understood by means of a structure that is modelled on language and is distinct both from the organizations of reality and those of ideas, or the imagination—the "third order." In Lacan's psychoanalytic theory, for example, the structural order of "the Symbolic" is distinguished both from "the Real" and "the Imaginary;" similarly, in Althusser's Marxist theory, the structural order of the capitalist mode of production is distinct both from the actual, real agents involved in its relations and from the ideological forms in which those relations are understood.

Although French theorist Louis Althusser is often associated with structural social analysis, which helped give rise to "structural Marxism," such association was contested by Althusser himself in the Italian foreword to the second edition of "Reading Capital". In this foreword Althusser states the following: 

Despite the precautions we took to distinguish ourselves from the 'structuralist' ideology…, despite the decisive intervention of categories foreign to 'structuralism'…, the terminology we employed was too close in many respects to the 'structuralist' terminology not to give rise to an ambiguity. With a very few exceptions…our interpretation of Marx has generally been recognized and judged, in homage to the current fashion, as 'structuralist'.… We believe that despite the terminological ambiguity, the profound tendency of our texts was not attached to the 'structuralist' ideology.
In a later development, feminist theorist Alison Assiter enumerated four ideas common to the various forms of structuralism:


In Ferdinand de Saussure's "Course in General Linguistics", the analysis focuses not on the use of language ("parole", 'speech'), but rather on the underlying system of language ("langue"). This approach examines how the elements of language relate to each other in the present, synchronically rather than diachronically. Saussure argued that linguistic signs were composed of two parts:


This differed from previous approaches that focused on the relationship between words and the things in the world that they designate.

Although not fully developed by Saussure, other key notions in structural linguistics can be found in structural "idealism." A structural idealism is a class of linguistic units (lexemes, morphemes, or even constructions) that are possible in a certain position in a given "syntagm", or linguistic environment (such as a given sentence). The different functional role of each of these members of the paradigm is called 'value' (French: "").

Saussure's "Course" influenced many linguists between World War I and World War II. In the United States, Leonard Bloomfield developed his own version of structural linguistics, as did Louis Hjelmslev, in Denmark, and Alf Sommerfelt, in Norway. 

In France, Antoine Meillet and Émile Benveniste continued Saussure's project, and members of the Prague school of linguistics such as Roman Jakobson and Nikolai Trubetzkoy conducted influential research. However, by the 1950s, Saussure's linguistic concepts were under heavy criticism and were largely abandoned by practicing linguists: 

Saussure's views are not held, so far as I know, by modern linguists, only by literary critics and the occasional philosopher. [Strict adherence to Saussure] has elicited wrong film and literary theory on a grand scale. One can find dozens of books of literary theory bogged down in signifiers and signifieds, but only a handful that refer to Chomsky.
The clearest and most important example of Prague school structuralism lies in phonemics. Rather than simply compiling a list of which sounds occur in a language, the Prague school examined how they were related. They determined that the inventory of sounds in a language could be analysed as a series of contrasts. Thus, in English, the sounds /p/ and /b/ represent distinct phonemes because there are cases ("minimal pairs") where the contrast between the two is the only difference between two distinct words (e.g. 'pat' and 'bat'). Analyzing sounds in terms of contrastive features also opens up comparative scope—for instance, it makes clear the difficulty Japanese speakers have differentiating /r/ and /l/ in English and other languages is because these sounds are not contrastive in Japanese. Phonology would become the paradigmatic basis for structuralism in a number of different fields.

According to structural theory in anthropology and social anthropology, "meaning" is produced and reproduced within a culture through various practices, phenomena, and activities that serve as systems of signification. 

A structuralist approach may study activities as diverse as food-preparation and serving rituals, religious rites, games, literary and non-literary texts, and other forms of entertainment to discover the deep structures by which meaning is produced and reproduced within the culture. For example, Lévi-Strauss analysed in the 1950s cultural phenomena including mythology, kinship (the alliance theory and the incest taboo), and food preparation. In addition to these studies, he produced more linguistically-focused writings in which he applied Saussure's distinction between "langue" and "parole" in his search for the fundamental structures of the human mind, arguing that the structures that form the "deep grammar" of society originate in the mind and operate in people unconsciously. Lévi-Strauss took inspiration from mathematics.

Another concept used in structural anthropology came from the Prague school of linguistics, where Roman Jakobson and others analysed sounds based on the presence or absence of certain features (e.g., voiceless vs. voiced). Lévi-Strauss included this in his conceptualization of the universal structures of the mind, which he held to operate based on pairs of binary oppositions such as hot-cold, male-female, culture-nature, cooked-raw, or marriageable vs. tabooed women.

A third influence came from Marcel Mauss (1872–1950), who had written on gift-exchange systems. Based on Mauss, for instance, Lévi-Strauss argued an "alliance" theory—that kinship systems are based on the exchange of women between groups—as opposed to the "'descent'-based" theory described by Edward Evans-Pritchard and Meyer Fortes. While replacing Mauss at his "Ecole Pratique des Hautes Etudes" chair, the writings of Lévi-Strauss became widely popular in the 1960s and 1970s and gave rise to the term "structuralism" itself.

In Britain, authors such as Rodney Needham and Edmund Leach were highly influenced by structuralism. Authors such as Maurice Godelier and Emmanuel Terray combined Marxism with structural anthropology in France. In the United States, authors such as Marshall Sahlins and James Boon built on structuralism to provide their own analysis of human society. Structural anthropology fell out of favour in the early 1980s for a number of reasons. D'Andrade suggests that this was because it made unverifiable assumptions about the universal structures of the human mind. Authors such as Eric Wolf argued that political economy and colonialism should be at the forefront of anthropology. More generally, criticisms of structuralism by Pierre Bourdieu led to a concern with how cultural and social structures were changed by human agency and practice, a trend which Sherry Ortner has referred to as 'practice theory'.

One example is Douglas E. Foley's "Learning Capitalist Culture" (2010), in which he applied a mixture of structural and Marxist theories to his ethnographic fieldwork among high school students in Texas. Foley analyzed how they reach a shared goal through the lens of social solidarity when he observed "Mexicanos" and "Anglo-Americans" come together on the same football team to defeat the school's rivals. However, he also continually applies a marxist lens and states that he," wanted to wow peers with a new cultural marxist theory of schooling."

Some anthropological theorists, however, while finding considerable fault with Lévi-Strauss's version of structuralism, did not turn away from a fundamental structural basis for human culture. The Biogenetic Structuralism group for instance argued that some kind of structural foundation for culture must exist because all humans inherit the same system of brain structures. They proposed a kind of neuroanthropology which would lay the foundations for a more complete scientific account of cultural similarity and variation by requiring an integration of cultural anthropology and neuroscience—a program that theorists such as Victor Turner also embraced.

In literary theory, structuralist criticism relates literary texts to a larger structure, which may be a particular genre, a range of intertextual connections, a model of a universal narrative structure, or a system of recurrent patterns or motifs.

The field of structuralist semiotics argues that there must be a structure in every text, which explains why it is easier for experienced readers than for non-experienced readers to interpret a text. Hence, everything that is written seems to be governed by specific rules, or a "grammar of literature", that one learns in educational institutions and that are to be unmasked.

A potential problem for a structuralist interpretation is that it can be highly reductive; as scholar Catherine Belsey puts it: "the structuralist danger of collapsing all difference." An example of such a reading might be if a student concludes the authors of "West Side Story" did not write anything "really" new, because their work has the same structure as Shakespeare's "Romeo and Juliet". In both texts a girl and a boy fall in love (a "formula" with a symbolic operator between them would be "Boy + Girl") despite the fact that they belong to two groups that hate each other ("Boy's Group - Girl's Group" or "Opposing forces") and conflict is resolved by their deaths. Structuralist readings focus on how the structures of the single text resolve inherent narrative tensions. If a structuralist reading focuses on multiple texts, there must be some way in which those texts unify themselves into a coherent system. The versatility of structuralism is such that a literary critic could make the same claim about a story of two "friendly" families ("Boy's Family + Girl's Family") that arrange a marriage between their children despite the fact that the children hate each other ("Boy - Girl") and then the children commit suicide to escape the arranged marriage; the justification is that the second story's structure is an 'inversion' of the first story's structure: the relationship between the values of love and the two pairs of parties involved have been reversed.

Structuralist literary criticism argues that the "literary banter of a text" can lie only in new structure, rather than in the specifics of character development and voice in which that structure is expressed. Literary structuralism often follows the lead of Vladimir Propp, Algirdas Julien Greimas, and Claude Lévi-Strauss in seeking out basic deep elements in stories, myths, and more recently, anecdotes, which are combined in various ways to produce the many versions of the ur-story or ur-myth.

There is considerable similarity between structural literary theory and Northrop Frye's archetypal criticism, which is also indebted to the anthropological study of myths. Some critics have also tried to apply the theory to individual works, but the effort to find unique structures in individual literary works runs counter to the structuralist program and has an affinity with New Criticism.

Structuralism is less popular today than other approaches, such as post-structuralism and deconstruction. Structuralism has often been criticized for being ahistorical and for favouring deterministic structural forces over the ability of people to act. As the political turbulence of the 1960s and 1970s (particularly the student uprisings of May 1968) began affecting academia, issues of power and political struggle moved to the center of public attention.

In the 1980s, deconstruction—and its emphasis on the fundamental ambiguity of language rather than its logical structure—became popular. By the end of the century, structuralism was seen as a historically important school of thought, but the movements that it spawned, rather than structuralism itself, commanded attention.

Several social theorists and academics have strongly criticized structuralism or even dismissed it. French hermeneutic philosopher Paul Ricœur (1969) criticized Lévi-Strauss for overstepping the limits of validity of the structuralist approach, ending up in what Ricœur described as "a Kantianism without a transcendental subject." 

Anthropologist Adam Kuper (1973) argued that:'Structuralism' came to have something of the momentum of a millennial movement and some of its adherents felt that they formed a secret society of the seeing in a world of the blind. Conversion was not just a matter of accepting a new paradigm. It was, almost, a question of salvation. Philip Noel Pettit (1975) called for an abandoning of "the positivist dream which Lévi-Strauss dreamed for semiology," arguing that semiology is not to be placed among the natural sciences. Cornelius Castoriadis (1975) criticized structuralism as failing to explain symbolic mediation in the social world; he viewed structuralism as a variation on the "logicist" theme, arguing that, contrary to what structuralists advocate, language—and symbolic systems in general—cannot be reduced to logical organizations on the basis of the binary logic of oppositions. 

Critical theorist Jürgen Habermas (1985) accused structuralists like Foucault of being positivists; Foucault, while not an ordinary positivist per se, paradoxically uses the tools of science to criticize science, according to Habermas. (See "Performative contradiction" and "Foucault–Habermas debate".) Sociologist Anthony Giddens (1993) is another notable critic; while Giddens draws on a range of structuralist themes in his theorizing, he dismisses the structuralist view that the reproduction of social systems is merely "a mechanical outcome."





</doc>
<doc id="27764" url="https://en.wikipedia.org/wiki?curid=27764" title="Systems engineering">
Systems engineering

Systems engineering is an interdisciplinary field of engineering and engineering management that focuses on how to design, integrate, and manage complex systems over their life cycles. At its core, systems engineering utilizes systems thinking principles to organize this body of knowledge. The individual outcome of such efforts, an engineered system, can be defined as a combination of components that work in synergy to collectively perform a useful function.

Issues such as requirements engineering, reliability, logistics, coordination of different teams, testing and evaluation, maintainability and many other disciplines necessary for successful system design, development, implementation, and ultimate decommission become more difficult when dealing with large or complex projects. Systems engineering deals with work-processes, optimization methods, and risk management tools in such projects. It overlaps technical and human-centered disciplines such as industrial engineering, process systems engineering, mechanical engineering, manufacturing engineering, production engineering, control engineering, software engineering, electrical engineering, cybernetics, aerospace engineering, organizational studies, civil engineering and project management. Systems engineering ensures that all likely aspects of a project or system are considered and integrated into a whole.

The systems engineering process is a discovery process that is quite unlike a manufacturing process. A manufacturing process is focused on repetitive activities that achieve high quality outputs with minimum cost and time. The systems engineering process must begin by discovering the real problems that need to be resolved, and identifying the most probable or highest impact failures that can occur – systems engineering involves finding solutions to these problems.

The term "systems engineering" can be traced back to Bell Telephone Laboratories in the 1940s. The need to identify and manipulate the properties of a system as a whole, which in complex engineering projects may greatly differ from the sum of the parts' properties, motivated various industries, especially those developing systems for the U.S. Military, to apply the discipline.

When it was no longer possible to rely on design evolution to improve upon a system and the existing tools were not sufficient to meet growing demands, new methods began to be developed that addressed the complexity directly. The continuing evolution of systems engineering comprises the development and identification of new methods and modeling techniques. These methods aid in a better comprehension of the design and developmental control of engineering systems as they grow more complex. Popular tools that are often used in the systems engineering context were developed during these times, including USL, UML, QFD, and IDEF0.

In 1990, a professional society for systems engineering, the "National Council on Systems Engineering" (NCOSE), was founded by representatives from a number of U.S. corporations and organizations. NCOSE was created to address the need for improvements in systems engineering practices and education. As a result of growing involvement from systems engineers outside of the U.S., the name of the organization was changed to the International Council on Systems Engineering (INCOSE) in 1995. Schools in several countries offer graduate programs in systems engineering, and continuing education options are also available for practicing engineers.

Systems engineering signifies only an approach and, more recently, a discipline in engineering. The aim of education in systems engineering is to formalize various approaches simply and in doing so, identify new methods and research opportunities similar to that which occurs in other fields of engineering. As an approach, systems engineering is holistic and interdisciplinary in flavour.

The traditional scope of engineering embraces the conception, design, development, production and operation of physical systems. Systems engineering, as originally conceived, falls within this scope. "Systems engineering", in this sense of the term, refers to the building of engineering concepts.

The use of the term "systems engineer" has evolved over time to embrace a wider, more holistic concept of "systems" and of engineering processes. This evolution of the definition has been a subject of ongoing controversy, and the term continues to apply to both the narrower and broader scope.

Traditional systems engineering was seen as a branch of engineering in the classical sense, that is, as applied only to physical systems, such as spacecraft and aircraft. More recently, systems engineering has evolved to a take on a broader meaning especially when humans were seen as an essential component of a system. Checkland, for example, captures the broader meaning of systems engineering by stating that 'engineering' "can be read in its general sense; you can engineer a meeting or a political agreement."

Consistent with the broader scope of systems engineering, the Systems Engineering Body of Knowledge (SEBoK) has defined three types of systems engineering: (1) Product Systems Engineering (PSE) is the traditional systems engineering focused on the design of physical systems consisting of hardware and software. (2) Enterprise Systems Engineering (ESE) pertains to the view of enterprises, that is, organizations or combinations of organizations, as systems. (3) Service Systems Engineering (SSE) has to do with the engineering of service systems. Checkland defines a service system as a system which is conceived as serving another system. Most civil infrastructure systems are service systems.

Systems engineering focuses on analyzing and eliciting customer needs and required functionality early in the development cycle, documenting requirements, then proceeding with design synthesis and system validation while considering the complete problem, the system lifecycle. This includes fully understanding all of the stakeholders involved. Oliver et al. claim that the systems engineering process can be decomposed into
Within Oliver's model, the goal of the Management Process is to organize the technical effort in the lifecycle, while the Technical Process includes "assessing available information", "defining effectiveness measures", to "create a behavior model", "create a structure model", "perform trade-off analysis", and "create sequential build & test plan".

Depending on their application, although there are several models that are used in the industry, all of them aim to identify the relation between the various stages mentioned above and incorporate feedback. Examples of such models include the Waterfall model and the VEE model.

System development often requires contribution from diverse technical disciplines. By providing a systems (holistic) view of the development effort, systems engineering helps mold all the technical contributors into a unified team effort, forming a structured development process that proceeds from concept to production to operation and, in some cases, to termination and disposal. In an acquisition, the holistic integrative discipline combines contributions and balances tradeoffs among cost, schedule, and performance while maintaining an acceptable level of risk covering the entire life cycle of the item.

This perspective is often replicated in educational programs, in that systems engineering courses are taught by faculty from other engineering departments, which helps create an interdisciplinary environment.

The need for systems engineering arose with the increase in complexity of systems and projects, in turn exponentially increasing the possibility of component friction, and therefore the unreliability of the design. When speaking in this context, complexity incorporates not only engineering systems, but also the logical human organization of data. At the same time, a system can become more complex due to an increase in size as well as with an increase in the amount of data, variables, or the number of fields that are involved in the design. The International Space Station is an example of such a system.

The development of smarter control algorithms, microprocessor design, and analysis of environmental systems also come within the purview of systems engineering. Systems engineering encourages the use of tools and methods to better comprehend and manage complexity in systems. Some examples of these tools can be seen here:

Taking an interdisciplinary approach to engineering systems is inherently complex since the behavior of and interaction among system components is not always immediately well defined or understood. Defining and characterizing such systems and subsystems and the interactions among them is one of the goals of systems engineering. In doing so, the gap that exists between informal requirements from users, operators, marketing organizations, and technical specifications is successfully bridged.

One way to understand the motivation behind systems engineering is to see it as a method, or practice, to identify and improve common rules that exist within a wide variety of systems. Keeping this in mind, the principles of systems engineering – holism, emergent behavior, boundary, et al. – can be applied to any system, complex or otherwise, provided systems thinking is employed at all levels. Besides defense and aerospace, many information and technology based companies, software development firms, and industries in the field of electronics & communications require systems engineers as part of their team.

An analysis by the INCOSE Systems Engineering center of excellence (SECOE) indicates that optimal effort spent on systems engineering is about 15–20% of the total project effort. At the same time, studies have shown that systems engineering essentially leads to reduction in costs among other benefits. However, no quantitative survey at a larger scale encompassing a wide variety of industries has been conducted until recently. Such studies are underway to determine the effectiveness and quantify the benefits of systems engineering.

Systems engineering encourages the use of modeling and simulation to validate assumptions or theories on systems and the interactions within them.

Use of methods that allow early detection of possible failures, in safety engineering, are integrated into the design process. At the same time, decisions made at the beginning of a project whose consequences are not clearly understood can have enormous implications later in the life of a system, and it is the task of the modern systems engineer to explore these issues and make critical decisions. No method guarantees today's decisions will still be valid when a system goes into service years or decades after first conceived. However, there are techniques that support the process of systems engineering. Examples include soft systems methodology, Jay Wright Forrester's System dynamics method, and the Unified Modeling Language (UML)—all currently being explored, evaluated, and developed to support the engineering decision process.

Education in systems engineering is often seen as an extension to the regular engineering courses, reflecting the industry attitude that engineering students need a foundational background in one of the traditional engineering disciplines (e.g., aerospace engineering, civil engineering, electrical engineering, mechanical engineering, manufacturing engineering, industrial engineering, chemical engineering)—plus practical, real-world experience to be effective as systems engineers. Undergraduate university programs explicitly in systems engineering are growing in number but remain uncommon, the degrees including such material most often presented as a BS in Industrial Engineering. Typically programs (either by themselves or in combination with interdisciplinary study) are offered beginning at the graduate level in both academic and professional tracks, resulting in the grant of either a MS/MEng or Ph.D./EngD degree.

INCOSE, in collaboration with the Systems Engineering Research Center at Stevens Institute of Technology maintains a regularly updated directory of worldwide academic programs at suitably accredited institutions. As of 2017, it lists over 140 universities in North America offering more than 400 undergraduate and graduate programs in systems engineering. Widespread institutional acknowledgment of the field as a distinct subdiscipline is quite recent; the 2009 edition of the same publication reported the number of such schools and programs at only 80 and 165, respectively.

Education in systems engineering can be taken as "Systems-centric" or "Domain-centric":
Both of these patterns strive to educate the systems engineer who is able to oversee interdisciplinary projects with the depth required of a core-engineer.

Systems engineering tools are strategies, procedures, and techniques that aid in performing systems engineering on a project or product. The purpose of these tools vary from database management, graphical browsing, simulation, and reasoning, to document production, neutral import/export and more.

There are many definitions of what a system is in the field of systems engineering. Below are a few authoritative definitions:


Systems engineering processes encompass all creative, manual and technical activities necessary to define the product and which need to be carried out to convert a system definition to a sufficiently detailed system design specification for product manufacture and deployment. Design and development of a system can be divided into four stages, each with different definitions: 

Depending on their application, tools are used for various stages of the systems engineering process:

Models play important and diverse roles in systems engineering. A model can be defined in several
ways, including:
Together, these definitions are broad enough to encompass physical engineering models used in the verification of a system design, as well as schematic models like a functional flow block diagram and mathematical (i.e., quantitative) models used in the trade study process. This section focuses on the last.

The main reason for using mathematical models and diagrams in trade studies is to provide estimates of system effectiveness, performance or technical attributes, and cost from a set of known or estimable quantities. Typically, a collection of separate models is needed to provide all of these outcome variables. The heart of any mathematical model is a set of meaningful quantitative relationships among its inputs and outputs. These relationships can be as simple as adding up constituent quantities to obtain a total, or as complex as a set of differential equations describing the trajectory of a spacecraft in a gravitational field. Ideally, the relationships express causality, not just correlation. Furthermore, key to successful systems engineering activities are also the methods with which these models are efficiently and effectively managed and used to simulate the systems. However, diverse domains often present recurring problems of modeling and simulation for systems engineering, and new advancements are aiming to crossfertilize methods among distinct scientific and engineering communities, under the title of 'Modeling & Simulation-based Systems Engineering'.

Initially, when the primary purpose of a systems engineer is to comprehend a complex problem, graphic representations of a system are used to communicate a system's functional and data requirements. Common graphical representations include:

A graphical representation relates the various subsystems or parts of a system through functions, data, or interfaces. Any or each of the above methods are used in an industry based on its requirements. For instance, the N2 chart may be used where interfaces between systems is important. Part of the design phase is to create structural and behavioral models of the system.

Once the requirements are understood, it is now the responsibility of a systems engineer to refine them, and to determine, along with other engineers, the best technology for a job. At this point starting with a trade study, systems engineering encourages the use of weighted choices to determine the best option. A decision matrix, or Pugh method, is one way (QFD is another) to make this choice while considering all criteria that are important. The trade study in turn informs the design, which again affects graphic representations of the system (without changing the requirements). In an SE process, this stage represents the iterative step that is carried out until a feasible solution is found. A decision matrix is often populated using techniques such as statistical analysis, reliability analysis, system dynamics (feedback control), and optimization methods.

Systems Modeling Language (SysML), a modeling language used for systems engineering applications, supports the specification, analysis, design, verification and validation of a broad range of complex systems.

Lifecycle Modeling Language (LML), is an open-standard modeling language designed for systems engineering that supports the full lifecycle: conceptual, utilization, support and retirement stages.

Many related fields may be considered tightly coupled to systems engineering. The following areas have contributed to the development of systems engineering as a distinct entity:





</doc>
<doc id="27765" url="https://en.wikipedia.org/wiki?curid=27765" title="September 4">
September 4






</doc>
<doc id="27766" url="https://en.wikipedia.org/wiki?curid=27766" title="Sam &amp; Max">
Sam &amp; Max

Sam & Max is a media franchise focusing on the titular fictional characters, the Freelance Police. The characters, who occupy a universe that parodies American popular culture, were created by Steve Purcell in his youth, and later debuted in a 1987 comic book series. The characters have since been the subject of a graphic adventure video game developed by LucasArts, a produced for Fox in cooperation with Nelvana Limited, and a series of episodic adventure games developed by Telltale Games. In addition, a variety of machinima and a webcomic have been produced for the series.

The characters are a pair of anthropomorphic, vigilante private investigators based in a dilapidated office block in New York City. Sam is a six-foot-tall dog who wears a suit and a fedora, while Max is a short and aggressive "hyperkinetic rabbity thing". Both enjoy solving problems and cases as maniacally as possible, often with complete disregard for the law. Driving a seemingly indestructible black-and-white 1960 DeSoto Adventurer, the pair travel to many contemporary and historical locations to fight crime, including the Moon, Ancient Egypt, the White House and the Philippines, as well as several fictitious locations.

The series has been very successful despite its relatively limited amount of media, and has gathered a significant fan base. However, the franchise did not gain more widespread recognition until after the 1993 release of LucasArts' "Sam & Max Hit the Road", which cultivated interest in Purcell's original comics. "Sam & Max Hit the Road" is regarded as an exceptional adventure game and an iconic classic of computer gaming in the 1990s. Subsequent video games and the television series have also fared well with both critics and fans; critics consider the episodic video games to be the first successful application of the episodic distribution model.

The idea of "Sam & Max" originated with Steve Purcell's younger brother, Dave, who invented the concept of a comic about a detective team consisting of a dog and a rabbit in his youth. Dave often left the comics around the house, so Steve, in a case of sibling rivalry, often finished the incomplete stories in parodies of their original form, deliberately making the characters mix up each other's names, over-explain things, shoot at each other and mock the way in which they had been drawn, as "kind of a parody of the way a kid talks when he's writing comics". Over time, this developed from Steve merely mocking his brother's work to him creating his own full stories with the characters. Ultimately, in the late 1970s, Dave Purcell gave Steve the rights to the characters, signing them over in a contract on Steve's birthday and allowing him to develop the characters in his own way. In 1980, Purcell began to produce "Sam & Max" comic strips for the weekly newsletter of the California College of Arts and Crafts. While the visual appearance of the characters had not yet been fully developed, the stories were similar in style to those that followed when Purcell was offered by "Fish Police" author Steven Moncuse the chance to publish his work properly in 1987.
Many aspects of the "Sam & Max" comics were influenced by Purcell's own experiences. Rats and cockroaches are common throughout the franchise, the former inspired by Purcell's pet rat. In another example, Sam and Max are occasionally shown playing a game called "fizzball", in which the object of the exercise is to hit a can of beer in mid-air with a solid axe handle. Purcell had previously invented the game with his friends, including fellow comic book writers Art Adams and Mike Mignola.

Sam is a laid-back but enthusiastic, brown-coated anthropomorphic Irish Wolfhound, described as a "canine shamus". He wears either a gray or blue suit with a matching fedora, to make people more cooperative when conversing with a six-foot talking dog. A warped sense of justice makes Sam the more passionate of the pair for their police work, only held back from taking his job seriously by Max. Nevertheless, he enjoys the mannerisms and dress that come with their line of work. Sam possesses near encyclopedic amounts of knowledge, particularly on obscure topics, and is prone to long-winded sentences filled with elaborate terminology. Although he is always keen to display this information—regardless of its accuracy—Sam can be capable of total ignorance towards more practical matters; for instance, despite his regard for his DeSoto Adventurer, he is severely negligent with the car's maintenance. Sam still retains various doglike qualities: he is excitable, enthusiastic but also susceptible to emotions of embarrassment and guilt. Nevertheless, Sam is "not above sticking his head out the car window and letting his tongue flap in the breeze". Sam rarely loses his temper, and is able to react to panic-inducing situations with extreme calm. When he does get angry, Sam tends to react in a violent, uncharacteristically savage manner, in which case it is usually Max that calms him down and prevents him from acting upon his anger. Sam usually is armed with an oversized .44 revolver. Sam is voiced by Bill Farmer in "Sam & Max Hit the Road", Harvey Atkin in the animated series and David Nowlin in Telltale's games.

Max is an anthropomorphic "hyperkinetic, three-foot rabbity thing" with white fur, but prefers being called a lagomorph. Max retains few characteristics consistent with a rabbit, with permanently rigid ears set in an excited posture and a huge jaw normally stuck in a crazed grin. Unhinged, uninhibited and near psychotic, Max enjoys violence and tends to prefer the aggressive way of solving problems, seeing the world as little more than a vessel for his "pinball-like stream of consciousness". This creates a seeming disregard for self-preservation; Max will revel in dangerous situations with little impression that he understands the risks he faces. As a result, Max is usually enthusiastic to engage in any activity, including being used by Sam as a cable cutter or an impromptu bludgeon. Despite this, Max possesses a sharp mind and an observational nature, and enjoys interpreting new experiences in as unpredictable a manner as possible. However, Max has a distaste for long stories and occasionally loses focus during lengthy scenes of plot exposition; by his own admission, Max possesses a particularly short attention span. Despite his seemingly heartless personality, he believes strongly in protecting Sam. However, Max can still act violently towards his friend, stating that when he dies he will take Sam with him. Moreover, Max is extremely possessive of Sam and their status as partners and best friends. Max traditionally carries a Luger pistol, but as he wears no clothes, other characters often make comments as to where Max keeps it on his person. Purcell considers Max to be representative of pure id, the uncoordinated instinctual trends of the human psyche. Max's voice is provided by Nick Jameson in "Sam & Max Hit the Road" and by Rob Tinkler in the animated series. Andrew Chaikin originally voiced Max in the first episode of Telltale's games before being replaced by William Kasten, while Dave Boat voices the character in "Poker Night 2".

Sam and Max debuted in the 1987 comic book series "Sam & Max: Freelance Police", published by Fishwrap Productions, also the publisher of "Fish Police". The first comic, "Monkeys Violating the Heavenly Temple", was Steve Purcell's first full story. The comic came about after Purcell agreed to create a full "Sam & Max" story for publication alongside Steve Moncuse's "Fish Police" series. "Monkeys Violating the Heavenly Temple" established many of the key features in the series; the main story of the comic saw the Freelance Police journey to the Philippines to stop a volcano god cult. "Night of the Gilded Heron-Shark" and "Night of the Cringing Wildebeest" accompanied the main story, focusing on a stand-off with a group of gangsters in Sam and Max's office and an investigation into a carnival refreshment booth respectively.

Over the subsequent years, several other comics were published, often by different publishers, including and Epic Comics. "Fair Wind to Java" was originally published in 1988 as a Munden's Bar story in the pages of First Comics' "Grimjack", featuring the Freelance Police fighting pyramid-building aliens in Ancient Egypt, and was followed in 1989 by "On the Road", a three chapter story showing what Sam and Max do on vacation. In 1990, a Christmas themed story, "The Damned Don't Dance" was released. 1992 saw the release of a further two comics; "Bad Day On The Moon" took the Freelance Police to deal with a roach infestation bothering giant rats on the Moon, and was later adapted as a story for the animated TV series, whilst "Beast From The Cereal Aisle" focused on the duo conducting an exorcism at the local supermarket. Two more comics were produced in 1997, "The Kids Take Over" and "Belly Of The Beast". The former has Sam and Max wake up from cryogenic sleep to discover that the entire world is now ruled by children while the latter sees the Freelance Police confronting a vampire abducting children at Halloween.

Purcell joined LucasArts in 1988 as an artist and game designer, where he was approached about contributing to LucasArts' new quarterly newsletter, "The Adventurer", a publication designed to inform customers about upcoming LucasArts games and company news. From its debut issue in 1990 to 1996, Purcell created twelve comic strips for the newsletter. The strips portrayed a variety of stories, from similar plots as in the comic books to parodies of LucasArts games such as "Monkey Island" and "Full Throttle" and the Lucasfilm franchises "Star Wars" and "Indiana Jones".

In 1995, all of the comics and "The Adventurer" strips published to that date were released in a compilation, "Sam & Max: Surfin' the Highway". Published by Marlowe & Company, the 154 page book was updated and republished in 1996. This original version of "Surfin' the Highway" went out of print in 1997, becoming a high priced collectors item sold through services such as eBay. In 2007, a 197-page twenty-year anniversary edition, containing all printed comics and strips as well as a variety of other artwork, was co-designed by Steve Purcell and Jake Rodkin and published by Telltale Games. This second publication received an Eisner Award nomination for "Best Graphic Album – Reprint" in 2009.

In December 2005, Purcell started a "Sam & Max" webcomic, hosted on the website of Telltale Games. Entitled "The Big Sleep", the webcomic began with Sam and Max bursting out of their graves at Kilpeck Church in England, symbolizing the Freelance Police's return after nearly a decade. In the twelve page story, Max has to save Sam after earwigs start a colony in Sam's brain. The webcomic concluded in April 2007, and was later awarded the Eisner Award for "Best Digital Comic" of 2007.

Following LucasArts' employment of Purcell in 1988, the characters of Sam and Max appeared in internal testing material for new SCUMM engine programmers; Purcell created animated versions of the characters and an office backdrop for the programmers to practice on. In 1992, LucasArts offered Purcell the chance to create a video game out of the characters, out of a wish to use new characters after the success of its two other main adventure titles, "Monkey Island" and "Maniac Mansion", and after a positive reaction from fans to the "Sam & Max" comic strips featured in LucasArts' "The Adventurer" newsletter. Consequently, development on a graphic adventure game, "Sam & Max Hit the Road", began shortly after. Based on the SCUMM engine and designed by Sean Clark, Michael Stemmle, Steve Purcell and his future wife Collette Michaud, the game was partially based on the 1989 comic "On The Road", and featured the Freelance Police travelling across America in search of an escaped bigfoot. Sam was voiced in the game by comedian Bill Farmer, while actor Nick Jameson voiced Max. "Sam & Max Hit the Road" was originally released for DOS in November 1993. Soon after "Sam & Max Hit the Road", another "Sam & Max" game using SCUMM entered planning under Purcell and Dave Grossman, but was abandoned. In a later interview Grossman described this sequel's highlight as "a giant spaceship shaped like Max's head".

In September 2001 development began on a new project, "Sam & Max Plunge Through Space". The game was to be an Xbox exclusive title, developed by Infinite Machine, a small company consisting of a number of former LucasArts employees. The story of the game was developed by Purcell and fellow designer Chuck Jordan and involved the Freelance Police travelling the galaxy to find a stolen Statue of Liberty. However, Infinite Machine went bankrupt within a year, partially due to the failure of their first game, "New Legends", and the project was abandoned.

At the 2002 Electronic Entertainment Expo convention, nearly a decade after the release of "Sam & Max Hit the Road", LucasArts announced the production of a PC sequel, entitled "Sam & Max: Freelance Police". "Freelance Police", like "Hit the Road", was to be a point-and-click graphic adventure game, using a new 3D game engine. Development of "Freelance Police" was led by Michael Stemmle. Steve Purcell contributed to the project by writing the story and producing concept art. Farmer and Jameson were also set to reprise their voice acting roles. In March 2004, however, quite far into the game's development, "Sam & Max: Freelance Police" was abruptly cancelled by LucasArts, citing "current market place realities and underlying economic considerations" in a short press release. The fan reaction to the cancellation was strong; a petition of 32,000 signatures stating the disappointment of fans was later presented to LucasArts.

After LucasArts' license with Steve Purcell expired in 2005, the "Sam & Max" franchise moved to Telltale Games, a company of former LucasArts employees who had worked on a number of LucasArts adventure games, including on the development of "Freelance Police". Under Telltale Games, a new episodic series of "Sam & Max" video games was announced. Like both "Sam & Max Hit the Road" and "Freelance Police", "Sam & Max Save the World" was in a point-and-click graphic adventure game format. The game used a new 3D game engine, different from the one used in "Freelance Police". The first season ran for six episodes, each with a self-contained storyline but with an overall story arc involving hypnotism running through the series. The first episode was released on GameTap in October 2006, with episodes following regularly until April 2007. Sam is voiced by David Nowlin, while Max is voiced by William Kasten in all episodes except the first one, where Andrew Chaikin voices the character. In addition, Telltale Games produced fifteen machinima shorts to accompany the main episodes. These shorts were released in groups of three in between the release of each episode, showing the activities of the Freelance Police in between each story.

A second season of episodic video games developed by Telltale Games was announced in July 2007. "Sam & Max Beyond Time and Space" followed the same overall format as "Save the World", with each episode having an overarching storyline involving time travel and laundering of the souls of the dead. As with "Save the World", episodes were originally published on GameTap before being made available for general release. The season consisted of five episodes and ran from November 2007 to April 2008. Nowlin and Kasten both returned to reprise their voice roles. In addition to the main games, a twenty-minute machinima video was produced, taking the form of a "Sam & Max" Christmas special.

A third game entitled "Sam & Max: The Devil's Playhouse" was confirmed in May 2008 for release in 2009; the title was later pushed back to 2010, with concept art emerging after Telltale's completion of "Tales of Monkey Island". The season again ran for five episodes, released monthly from April to August 2010. "The Devil's Playhouse" followed a structure similar to "'Tales of Monkey Island", with each episode forming a part of an ongoing narrative, involving psychic powers and forces that used them for world domination. A two-minute Flash cartoon also accompanied the game, dealing with the origin story of General Skun-ka'pe, one of the game's antagonists. Max also appears in Telltale's 2010 casual game "Poker Night at the Inventory" alongside Tycho Brahe from "Penny Arcade", the Heavy from "Team Fortress 2" and Strong Bad from "Homestar Runner". Sam and Max (now voiced by Dave Boat) also appear in the game's sequel alongside Claptrap from "Borderlands", Brock Samson from "The Venture Bros.", Ash Williams from "Evil Dead" and GLaDOS from "Portal". 

A new "Sam & Max" virtual reality game, "Sam & Max: This Time It's Virtual", was announced in August 2020 from HappyGiant. Purcell is serving as a consultant for game design, and Nowlin and Boat return to voice Sam and Max, respectively.

"Sam & Max" were adapted into a cartoon series for Fox in 1997. Produced by Canadian studio Nelvana, the series ran for 24 episodes. Each episode was approximately ten minutes, and were often aired in pairs. Broadcast on Fox Kids in the United States, YTV in Canada, and Channel 4 in the United Kingdom, the first episode was aired on October 4, 1997; the series concluded on April 25, 1998. As opposed to the more adult humor in the rest of the series, "The Adventures of Sam & Max: Freelance Police" was aimed more at children, even though some humor in it was often directed at adults. As such, the violence inherent in the franchise is toned down, including removing Sam and Max's guns, and the characters do not use the moderate profanity that they use in their other appearances. As in most "Sam & Max" stories, the series revolves around the Freelance Police accepting missions from their mysterious superior, the commissioner, and embarking on cases to a large variety of implausible locations. Sam is voiced by Harvey Atkin, while Max is voiced by Robert Tinkler. The series performed well and was considered a success, and in 1998 received the Gemini Award for "Best Animated Program or Series". Despite the series' success, a second series was never commissioned. In June 2007, it was reported that Shout! Factory were preparing a DVD release of the series. In October 2007, as part of their marketing for "Sam & Max Save the World", GameTap hosted the series on their website. The DVD release of the series was later published in March 2008.

The "Sam & Max" franchise features a variety of soundtracks that accompany its video game products. This music is mostly grounded in film noir jazz, incorporating various other styles at certain points, such as Dixieland, waltz and mariachi, usually to support the cartoon nature of the series. The first "Sam & Max" game, "Sam & Max Hit the Road", was one of the first games to feature a fully scored music soundtrack, written by LucasArts' composers Clint Bajakian, Michael Land and Peter McConnell. The music was incorporated into the game using Land and McConnell's iMUSE engine, which allowed for audio to be synchronized with the visuals. Although the full soundtrack was never released, audio renders of four of the game's MIDI tracks were included on the CD version of the game.

For "Sam & Max Save the World", "Beyond Time and Space", and "The Devil's Playhouse", Telltale Games contracted composer Jared Emerson-Johnson, a musician whose previous work included composition and sound editing for LucasArts, to write the scores. The soundtracks for the first two games were released in two disc sets after the release of the games themselves; the "Season One Soundtrack" was published in July 2007, whilst the "Season Two Soundtrack" was released in September 2008. Emerson-Johnson's scores use live performances as opposed to synthesized music often used elsewhere in the video game industry. Critics reacted positively to Emerson-Johnson's scores, IGN described Emerson-Johnson's work as a "breath of fresh air", while 1UP.com praised his work as "top-caliber" and Music4Games stated that the "whimsical nature of [the classical jazz approach] is well suited to the "Sam & Max" universe, which approaches American popular culture with a level of irreverence". Purcell later commented that Emerson-Johnson had seamlessly blended a "huge palette of genres and styles", whilst in September 2008, Brendan Q. Ferguson, one of the lead designers on "Save the World" and "Beyond Time and Space", stated that he believed that it was Emerson-Johnson's scores that created the vital atmosphere in the games, noting that prior to the implementation of the soundtracks, playing the games was an "unrelenting horror".

The "Sam & Max" franchise has been highly successful critically, and is considered an iconic and influential aspect of the video game industry in the 1990s and the adventure game genre. In 2007, Steve Purcell wrote that he was somewhat surprised at the success of his creation, noting that the series had gained a large fan gathering despite the small size of the franchise. As the series contains only a small amount of comics, video games and a short TV series, Purcell commented that there was "certainly not enough material to build that relentless traction of an endlessly renewed sitcom or a syndicated comic that has existed since the Korean Conflict". The comics were well received by critics, many praising the humor and style of the stories and characters. However, later commentators have noted that the comic book series did not gain much popularity or recognition until after the release of "Sam & Max Hit the Road" in 1993; the later episodic video games are seen to have revived interest in the comics again, resulting in the creation of the webcomic "The Big Sleep" and publication of an anniversary edition of "Surfin' The Highway".

Upon its release in 1993, "Sam & Max Hit the Road" was met with near universal acclaim. Critics praised the title for its humor, voice acting, graphics, music and gameplay. It has since come to be regarded as a classic graphic adventure game, one of the most critically successful projects by LucasArts to date. "Sam & Max Hit the Road" is regularly featured in lists of top games, and was nominated for the 1994 Annie Award for "Best Animated CD-ROM", although the award instead went to LucasArts' "". The abrupt cancellation of the sequel to "Sam & Max Hit the Road" in 2004 garnered substantial criticism of LucasArts. In addition to a petition of 32,000 signatures objecting to the termination of development on "Sam & Max: Freelance Police", both Steve Purcell and the media were critical of LucasArts' decision. Purcell stated that he failed to understand quite why the game was cancelled, as he believed the development of the game was proceeding without hindrance, while the media put forward the view that LucasArts was moving to consolidate its position with low business risk "Star Wars" video games instead of pursuing the adventure games that had brought them success in earlier years. The cancellation of "Freelance Police" is often cited as the culmination in a perceived decline in the overall adventure game genre, and LucasArts later dismissed many of the designers involved with developing their adventure games, effectively ending their adventure game era.

Although "Sam & Max Save the World" did not receive the critical acclaim that "Sam & Max Hit the Road" acquired, it still received a favorable response from critics across its release in 2006 and 2007. Critics praised the game's humor, graphics and gameplay, although concerns were voiced over the low difficulty of the puzzles and the effectiveness of the story. "Save the World" is considered by journalists in the video game industry to be the first successful application of episodic gaming, as Telltale Games had managed to release a steady stream content with only small time gaps. Previous attempts by Valve with the "Half-Life" series, Ritual Entertainment with "SiN Episodes" and Telltale Games themselves with "" were for a variety of reasons not considered successful implementations of the distribution model. "Beyond Time and Space" was considered similar to "Save the World" and reviewers equally praised and faulted the game on this, although overall "Beyond Time and Space" received a good reception from critics.

The success of the franchise has spawned a selection of merchandise, including posters and prints, items of clothing and sketchbooks of Purcell's work during various stages of the series' development. Collectable statues of the characters have also been created. However, despite references in Purcell's sketchbooks and demand from both fans and journalists alike, plush toys of Sam have not been produced. However, a plush toy of Max has been created and sold, albeit limited edition, as a collaboration between internet shop Hashtag Collectibles and Steve Purcell. In late 2018, it was announced that Boss Fight Studios would be producing a new line of action figures based on the Freelance Police for 2019, the prototypes were first revealed at New York Toy Fair 2019.



</doc>
<doc id="27767" url="https://en.wikipedia.org/wiki?curid=27767" title="Standard-definition television">
Standard-definition television

Standard-definition television (SDTV, SD, often shortened to standard definition) is a television system which uses a resolution that is not considered to be either high or enhanced definition. SDTV and high-definition television (HDTV) are the two categories of display formats for digital television (DTV) transmissions. "Standard" refers to the fact that it was the prevailing specification for broadcast (and later, cable) television in the mid- to late-20th century.

The two common SDTV signal types are 576i, with 576 interlaced lines of resolution, derived from the European-developed PAL and SECAM systems, and 480i based on the American NTSC system. Common SDTV refresh rates are 25, 29.97 and 30 frames per second. Both systems use 
a 4:3 aspect ratio. 

Standards that support digital SDTV broadcast include DVB, ATSC, and ISDB. The last two were originally developed for HDTV, but are also used for their ability to deliver multiple SD video and audio streams via multiplexing. In North America, digital SDTV is broadcast in the same 4:3 aspect ratio as NTSC signals, with widescreen content often being center cut. However, the aspect ratio of widescreen content may be preserved in a 4:3 frame through letterboxing. In other parts of the world that used the PAL or SECAM color systems, digital standard-definition television is now usually shown with a , with the transition occurring between the mid-1990s and mid-2000s depending on region. Older programs with a 4:3 aspect ratio are broadcast with a flag that switches the display to 4:3. 

Digital SDTV eliminates the ghosting and noisy images associated with analog systems. However, if the reception has interference or is poor, where the error correction cannot compensate one will encounter various other artifacts such as image freezing, stuttering or dropouts from missing intra-frames or blockiness from missing macroblocks.

The table below summarizes pixel aspect ratios for the scaling of various kinds of SDTV video lines. 

The pixel aspect ratio is the same for 720- and 704-pixel resolutions because the visible image (be it 4:3 or 16:9) is contained in the center 704 horizontal pixels of the digital frame. In the case of a digital video line having 720 horizontal pixels (including horizontal blanking), only the center 704 pixels contain the actual 4:3 or 16:9 image, and the 8-pixel-wide stripes on either side are called nominal analogue blanking or horizontal blanking and should be discarded when displaying the image. Nominal analogue blanking should not be confused with overscan, as overscan areas are part of the actual 4:3 or 16:9 image.

For SMPTE 259M-C compliance, a SDTV broadcast image is scaled to 720 pixels wide for every 480 NTSC (or 576 PAL) lines of the image with the amount of non-proportional line scaling dependent on either the display or pixel aspect ratio. The display ratio for broadcast widescreen is commonly 16:9, the display ratio for a traditional or letterboxed broadcast is 4:3.

An SDTV image outside the constraints of the SMPTE standards requires no non-proportional scaling with 640 pixels for every line of the image. The display and pixel aspect ratio is generally not required with the line height defining the aspect. For widescreen 16:9, 360 lines define a widescreen image and for traditional 4:3, 480 lines define an image.




</doc>
<doc id="27772" url="https://en.wikipedia.org/wiki?curid=27772" title="Sandstone">
Sandstone

Sandstone is a clastic sedimentary rock composed mainly of sand-sized (0.0625 to 2 mm) mineral particles or rock fragments (clasts) or organic material. They make up about 20 to 25 percent of all sedimentary rocks.

Most sandstone is composed of quartz or feldspar (both silicates) because they are the most resistant minerals to weathering processes at the Earth's surface, as seen in the Goldich dissolution series. Like uncemented sand, sandstone may be any color due to impurities within the minerals, but the most common colors are tan, brown, yellow, red, grey, pink, white, and black. Since sandstone beds often form highly visible cliffs and other topographic features, certain colors of sandstone have been strongly identified with certain regions.

Rock formations that are primarily composed of sandstone usually allow the percolation of water and other fluids and are porous enough to store large quantities, making them valuable aquifers and petroleum reservoirs. Fine-grained aquifers, such as sandstones, are better able to filter out pollutants from the surface than are rocks with cracks and crevices, such as limestone or other rocks fractured by seismic activity.

Quartz-bearing sandstone can be changed into quartzite through metamorphism, usually related to tectonic compression within orogenic belts.

Sandstones are "clastic" in origin (as opposed to either "organic", like chalk and coal, or "chemical", like gypsum and jasper).
They are formed from cemented grains that may either be fragments of a pre-existing rock or be mono-minerallic crystals. The cements binding these grains together are typically calcite, clays, and silica. Grain sizes in sands are defined (in geology) within the range of 0.0625 mm to 2 mm (0.0025–0.08 inches). Clays and sediments with smaller grain sizes not visible with the naked eye, including siltstones and shales, are typically called "argillaceous" sediments; rocks with larger grain sizes, including breccias and conglomerates, are termed "rudaceous" sediments.

The formation of sandstone involves two principal stages. First, a layer or layers of sand accumulates as the result of sedimentation, either from water (as in a stream, lake, or sea) or from air (as in a desert). Typically, sedimentation occurs by the sand settling out from suspension; i.e., ceasing to be rolled or bounced along the bottom of a body of water or ground surface (e.g., in a desert or erg). Finally, once it has accumulated, the sand becomes sandstone when it is compacted by the pressure of overlying deposits and cemented by the precipitation of minerals within the pore spaces between sand grains.

The most common cementing materials are silica and calcium carbonate, which are often derived either from dissolution or from alteration of the sand after it was buried. Colors will usually be tan or yellow (from a blend of the clear quartz with the dark amber feldspar content of the sand). A predominant additional colourant in the southwestern United States is iron oxide, which imparts reddish tints ranging from pink to dark red (terracotta), with additional manganese imparting a purplish hue. Red sandstones, both Old Red Sandstone and New Red Sandstone, are also seen in the Southwest and West of Britain, as well as central Europe and Mongolia. The regularity of the latter favours use as a source for masonry, either as a primary building material or as a facing stone, over other forms of construction.

Framework grains are sand-sized ( diameter) detrital fragments that make up the bulk of a sandstone. These grains can be classified into several different categories based on their mineral composition:


Matrix is very fine material, which is present within interstitial pore space between the framework grains. The nature of the matrix within the interstitial pore space results in a twofold classification:


Cement is what binds the siliciclastic framework grains together. Cement is a secondary mineral that forms after deposition and during burial of the sandstone. These cementing materials may be either silicate minerals or non-silicate minerals, such as calcite.


Pore space includes the open spaces within a rock or a soil. The pore space in a rock has a direct relationship to the porosity and permeability of the rock. The porosity and permeability are directly influenced by the way the sand grains are packed together.


All sandstones are composed of the same general minerals. These minerals make up the framework components of the sandstones. Such components are quartz, feldspars, and lithic fragments. Matrix may also be present in the interstitial spaces between the framework grains. Below is a list of several major groups of sandstones. These groups are divided based on mineralogy and texture. Even though sandstones have very simple compositions which are based on framework grains, geologists have not been able to agree on a specific, right way, to classify sandstones. Sandstone classifications are typically done by point-counting a thin section using a method like the Gazzi-Dickinson Method. The composition of a sandstone can have important information regarding the genesis of the sediment when used with a triangular "Q"uartz, "F"eldspar, "L"ithic fragment (QFL diagrams). Many geologists, however, do not agree on how to separate the triangle parts into the single components so that the framework grains can be plotted. Therefore, there have been many published ways to classify sandstones, all of which are similar in their general format.

Visual aids are diagrams that allow geologists to interpret different characteristics about a sandstone. The following QFL chart and the sandstone provenance model correspond with each other therefore, when the QFL chart is plotted those points can then be plotted on the sandstone provenance model. The stage of textural maturity chart illustrates the different stages that a sandstone goes through.

Dott's (1964) sandstone classification scheme is one of many such schemes used by geologists for classifying sandstones. Dott's scheme is a modification of Gilbert's classification of silicate sandstones, and it incorporates R.L. Folk's dual textural and compositional maturity concepts into one classification system. The philosophy behind combining Gilbert's and R. L. Folk's schemes is that it is better able to "portray the continuous nature of textural variation from mudstone to arenite and from stable to unstable grain composition". Dott's classification scheme is based on the mineralogy of framework grains, and on the type of matrix present in between the framework grains.

In this specific classification scheme, Dott has set the boundary between arenite and wackes at 15% matrix. In addition, Dott also breaks up the different types of framework grains that can be present in a sandstone into three major categories: quartz, feldspar, and lithic grains.


Sandstone has been used since prehistoric times for construction, decorative art works and housewares, and continues to be used. It has been widely employed around the world in constructing temples, homes, and other buildings.

Although its resistance to weathering varies, sandstone is easy to work. That makes it a common building and paving material, including in asphalt concrete. However, some types that have been used in the past, such as the Collyhurst sandstone used in North West England, have had poor long-term weather resistance, necessitating repair and replacement in older buildings. Because of the hardness of individual grains, uniformity of grain size and friability of their structure, some types of sandstone are excellent materials from which to make grindstones, for sharpening blades and other implements. Non-friable sandstone can be used to make grindstones for grinding grain, e.g., gritstone.

A type of pure quartz sandstone, orthoquartzite, with more of 90–95 percent of quartz, has been proposed for nomination to the Global Heritage Stone Resource. In some regions of Argentina, the orthoquartzite-stoned facade is one of the main features of the Mar del Plata style bungalows.




</doc>
<doc id="27773" url="https://en.wikipedia.org/wiki?curid=27773" title="Sophia of Hanover">
Sophia of Hanover

Sophia of the Palatinate (14 October 1630 – 8 June 1714) was the Electress of Hanover by marriage to Elector Ernest Augustus, and later the heir presumptive to the thrones of England (later Great Britain) and Ireland under the Act of Settlement 1701. She died less than two months before she would have become queen. Consequently, it was her son George I who succeeded her second cousin, Anne.

Born to Frederick V of the Palatinate, a member of the House of Wittelsbach, and Elizabeth Stuart, in 1630, Sophia grew up in the Dutch Republic, where her family had sought refuge after the sequestration of their Electorate during the Thirty Years' War. Sophia's brother Charles Louis was restored to the Lower Palatinate as part of the Peace of Westphalia. Sophia married Ernest Augustus of Brunswick-Lüneburg in 1658. Despite his temper and frequent absences, Sophia loved him, and bore him seven children who survived to adulthood. Initially a landless cadet, Ernest Augustus succeeded in having the House of Hanover raised to electoral dignity in 1692. Therefore, Sophia became Electress of Hanover, the title by which she is best remembered. A patron of the arts, Sophia commissioned Herrenhausen Palace its gardens and sponsored philosophers, such as Gottfried Leibniz and John Toland.

Sophia's descendants occupy the thrones of all seven European kingdoms, as well as the throne of the Grand Duchy of Luxembourg.

A daughter of Frederick V of the Palatinate by Elizabeth Stuart, also known as the "Winter King and Queen of Bohemia" for their short rule in that country, Sophia was born in The Wassenaer Hof, The Hague, Dutch Republic, where her parents had fled into exile after the Battle of White Mountain. Through her mother, she was the granddaughter of James VI and I, king of Scotland and England. At birth, Sophia was granted an annuity of 40 thalers by the Estates of Friesland. Sophia was courted by her first cousin, Charles II of England, but she rebuffed his advances as she thought he was using her in order to get money from her mother's supporter, Lord William Craven.

Before her marriage, Sophia, as the daughter of Frederick V, Elector Palatine of the Rhine, was referred to as Sophie, Princess Palatine of the Rhine, or as Sophia of the Palatinate. The Electors of the Palatinate were the Calvinist senior branch of House of Wittelsbach, whose Catholic branch ruled the Electorate of Bavaria.

On 30 September 1658, she married Ernest Augustus, Duke of Brunswick-Lüneburg, at Heidelberg, who in 1692 became the first Elector of Hanover. Ernest August was a second cousin of Sophia's mother Elizabeth Stuart, Queen of Bohemia, as they were both great-grandchildren of Christian III of Denmark.

Sophia became a friend and admirer of Gottfried Leibniz while he was librarian at the Court of Hanover. Their friendship lasted from 1676 until her death in 1714. This friendship resulted in a substantial correspondence, first published in the 19th century (Klopp 1973), that reveals Sophia to have been a woman of exceptional intellectual ability and curiosity. She was well-read in the works of René Descartes and Baruch Spinoza. Together with Ernest Augustus, she greatly improved the Herrenhausen Palace and she was the guiding spirit in the creation of the Herrenhausen Gardens surrounding the palace, where she died.

Sophia had seven children who reached adulthood. They were:


Sophia was absent for almost a year, 1664–65, during a long holiday with Ernest Augustus in Italy. She corresponded regularly with her sons' governess and took a great interest in her sons' upbringing, even more so on her return. After Sophia's tour, she bore Ernest Augustus another four sons and a daughter. In her letters, Sophia describes her eldest son as a responsible, conscientious child who set an example to his younger brothers and sisters.

Sophia was, at first, against the marriage of her son and Sophia Dorothea of Celle, looking down on Sophia Dorothea's mother (who was not of royal birth and who Sophia referred to as "mouse dirt mixed among the pepper") and concerned by Sophia Dorothea's legitimated status, but was eventually won over by the advantages inherent in the marriage.

In September 1700, Sophia met her cousin King William III of England at Het Loo Palace in Apeldoorn, the Netherlands. This happened two months after the death of his nephew Prince William, Duke of Gloucester, son of the future Queen Anne. By this time, given the ailing William III's reluctance to remarry, the inclusion of Sophia in the line of succession was becoming more likely, because she was a Protestant, as was her son. Her candidature was aided by the fact that she had grown up in the Netherlands close to William III and was able to converse fluently with him in Dutch, his native tongue.

A year after their meeting, the Parliament of England passed the Act of Settlement 1701 declaring that, in the event of no legitimate issue from Anne or William III, the crowns of England and Scotland were to settle upon "the most excellent princess Sophia, electress and duchess-dowager of Hanover" and "the heirs of her body, being Protestant". The key excerpt from the Act, naming Sophia as heir presumptive, reads:

Therefore for a further Provision of the Succession of the Crown in the Protestant Line We Your Majesties most dutifull and Loyall Subjects the Lords Spirituall and Temporall and Commons in this present Parliament assembled do beseech Your Majesty that it may be enacted and declared and be it enacted and declared by the Kings most Excellent Majesty by and with the Advice and Consent of the Lords Spirituall and Temporall and Commons in this present Parliament assembled and by the Authority of the same That the most Excellent Princess Sophia Electress and Dutchess Dowager of Hannover Daughter of the most Excellent Princess Elizabeth late Queen of Bohemia Daughter of our late Sovereign Lord King James the First of happy Memory be and is hereby declared to be the next in Succession in the Protestant Line to the Imperiall Crown and Dignity of the forsaid Realms of England France and Ireland with the Dominions and Territories thereunto belonging after His Majesty and the Princess Anne of Denmark and in Default of Issue of the said Princess Anne and of His Majesty respectively.

Sophia was made heir presumptive to cut off a claim by the Roman Catholic James Francis Edward Stuart, who would have become James III and VIII and to deny the throne to the many other Roman Catholics and spouses of Roman Catholics who held a claim. The act restricts the British throne to the "Protestant heirs" of Sophia of Hanover who had never been Roman Catholic or married a Roman Catholic. Some British politicians attempted several times to bring Sophia to England in order to enable her to assume government immediately in the event of Anne's death. It was argued that such a course was necessary to ensure Sophia's succession, for Anne's Roman Catholic half-brother was significantly closer to London than was Sophia. The Electress was eager to move to London, but the proposal was denied, as such action would mortally offend Anne who was strongly opposed to a rival court in her kingdom. Anne might have been aware that Sophia, who was active and lively despite her old age, could cut a better figure than herself. Sophia was completely uncertain of what would happen after Anne's death, saying: "What Parliament does one day, it undoes the next."

When the law was passed in mid-1701, Sophia at age 70, five of her children from ages 35 to 41, and three legitimate grandchildren from ages 14 to 18, were alive. Although Sophia was in her seventy-first year, older than Anne by thirty-five years, she was very fit and healthy, and invested time and energy in securing the succession either for herself or her son. There are more than 5,000 legitimate descendants of Sophia, although not all are in the line of succession. The Sophia Naturalization Act 1705 granted the right of British nationality to Sophia's non-Roman Catholic descendants; those who had obtained the right to British citizenship via this Act at any time before its repeal by the British Nationality Act 1948 retain this lawful right today.

Although considerably older than Queen Anne, Sophia enjoyed much better health. According to the Countess of Bückeburg in a letter to Sophia's niece, the Raugravine Luise, on 5 June 1714 Sophia felt ill after receiving an angry letter from Queen Anne. Two days later she was walking in the gardens of Herrenhausen when she ran to shelter from a sudden downpour of rain and collapsed and died, aged 83—a very advanced age for the era. Just over a month later, in August, Queen Anne died at the age of 49. Had Anne predeceased Sophia, Sophia would have been the oldest person to ascend the British throne.

Upon Sophia's death, her eldest son Elector George Louis of Brunswick-Lüneburg (1660–1727) became heir presumptive in her place, and weeks later, succeeded Anne as George I of Great Britain. Sophia's daughter Sophia Charlotte of Hanover (1668–1705) married Frederick I of Prussia, from whom the later Prussian and German monarchs descend.

Sophia was buried in the chapel of Leine Palace, as were her husband and, later, their son George I. After destruction of the palace and its chapel during World War II by British air raids, their remains were moved into the mausoleum of King Ernest Augustus I in the Berggarten of Herrenhausen Gardens in 1957.




</doc>
<doc id="27774" url="https://en.wikipedia.org/wiki?curid=27774" title="Scanning tunneling microscope">
Scanning tunneling microscope

A scanning tunneling microscope (STM) is an instrument for imaging surfaces at the atomic level. Its development in 1981 earned its inventors, Gerd Binnig and Heinrich Rohrer (at IBM Zürich), the Nobel Prize in Physics in 1986. For an STM, good resolution is considered to be 0.1 nm lateral resolution and 0.01 nm (10 pm) depth resolution. With this resolution, individual atoms within materials are routinely imaged and manipulated. The STM can be used not only in ultra-high vacuum but also in air, water, and various other liquid or gas ambients, and at temperatures ranging from near zero kelvin to over 1000 °C.

STM is based on the concept of quantum tunneling. When a conducting tip is brought very near to the surface to be examined, a bias (voltage difference) applied between the two can allow electrons to tunnel through the vacuum between them. The resulting "tunneling current" is a function of tip position, applied voltage, and the local density of states (LDOS) of the sample. Information is acquired by monitoring the current as the tip's position scans across the surface, and is usually displayed in image form. STM can be a challenging technique, as it requires extremely clean and stable surfaces, sharp tips, excellent vibration control, and sophisticated electronics, but nonetheless many hobbyists have built their own.

First, a voltage bias is applied and the tip is brought close to the sample by coarse sample-to-tip control, which is turned off when the tip and sample are sufficiently close. At close range, fine control of the tip in all three dimensions when near the sample is typically piezoelectric, maintaining tip-sample separation "w" typically in the 4-7 Å (0.4-0.7 nm) range, which is the equilibrium position between attractive (3<"w"<10Å) and repulsive ("w"<3Å) interactions. In this situation, the voltage bias will cause electrons to tunnel between the tip and sample, creating a current that can be measured. Once tunneling is established, the tip's bias and position with respect to the sample can be varied (with the details of this variation depending on the experiment) and data are obtained from the resulting changes in current.

If the tip is moved across the sample in the x-y plane, the changes in surface height and density of states causes changes in current. These changes are mapped in images. This change in current with respect to position can be measured itself, or the height, z, of the tip corresponding to a constant current can be measured. These two modes are called constant height mode and constant current mode, respectively. In constant current mode, feedback electronics adjust the height by a voltage to the piezoelectric height control mechanism. This leads to a height variation and thus the image comes from the tip topography across the sample and gives a constant charge density surface; this means contrast on the image is due to variations in charge density. In constant height mode, the voltage and height are both held constant while the current changes to keep the voltage from changing; this leads to an image made of current changes over the surface, which can be related to charge density. The benefit to using a constant height mode is that it is faster, as the piezoelectric movements require more time to register the height change in constant current mode than the current change in constant height mode. All images produced by STM are grayscale, with color optionally added in post-processing in order to visually emphasize important features.

In addition to scanning across the sample, information on the electronic structure at a given location in the sample can be obtained by sweeping voltage and measuring current at a specific location. This type of measurement is called scanning tunneling spectroscopy (STS) and typically results in a plot of the local density of states as a function of energy within the sample. The advantage of STM over other measurements of the density of states lies in its ability to make extremely local measurements: for example, the density of states at an impurity site can be compared to the density of states far from impurities.

Framerates of at least 25 Hz enable so called video-rate STM. Framerates up to 80 Hz are possible with fully working feedback that adjusts the height of the tip. Due to the line-by-line scanning motion, a proper comparison on the speed requires not only the framerate, but also the number of pixels in an image: with a framerate of 10 Hz and 100x100 pixels the tip moves with a line frequency of 1 kHz, whereas it moves with only with 500 Hz, when measuring with a faster framerate of 50 Hz but only 10x10 pixels. Video-rate STM can be used to scan surface diffusion.

The components of an STM include scanning tip, piezoelectric controlled height and x,y scanner, coarse sample-to-tip control, vibration isolation system, and computer.

The resolution of an image is limited by the radius of curvature of the scanning tip of the STM. Additionally, image artifacts can occur if the tip has two tips at the end rather than a single atom; this leads to “double-tip imaging,” a situation in which both tips contribute to the tunneling. Therefore, it has been essential to develop processes for consistently obtaining sharp, usable tips. Recently, carbon nanotubes have been used in this instance.

The tip is often made of tungsten or platinum-iridium, though gold is also used. Tungsten tips are usually made by electrochemical etching, and platinum-iridium tips by mechanical shearing.

Due to the extreme sensitivity of tunnel current to height, proper vibration insulation or an extremely rigid STM body is imperative for obtaining usable results. In the first STM by Binnig and Rohrer, magnetic levitation was used to keep the STM free from vibrations; now mechanical spring or gas spring systems are often used. Additionally, mechanisms for vibration damping using eddy currents are sometimes implemented.

Maintaining the tip position with respect to the sample, scanning the sample and acquiring the data is computer controlled. The computer may also be used for enhancing the image with the help of image processing as well as performing quantitative measurements.

Many other microscopy techniques have been developed based upon STM. These include photon scanning microscopy (PSTM), which uses an optical tip to tunnel photons; scanning tunneling potentiometry (STP), which measures electric potential across a surface; spin polarized scanning tunneling microscopy (SPSTM), which uses a ferromagnetic tip to tunnel spin-polarized electrons into a magnetic sample,
multi-tip scanning tunneling microscopy which enables electrical measurements to be performed at the nanoscale, and atomic force microscopy (AFM), in which the force caused by interaction between the tip and sample is measured.

Other STM methods involve manipulating the tip in order to change the topography of the sample. This is attractive for several reasons. Firstly the STM has an atomically precise positioning system which allows very accurate atomic scale manipulation. Furthermore, after the surface is modified by the tip, it is a simple matter to then image with the same tip, without changing the instrument. IBM researchers developed a way to manipulate xenon atoms adsorbed on a nickel surface. This technique has been used to create electron "corrals" with a small number of adsorbed atoms, which allows the STM to be used to observe electron Friedel oscillations on the surface of the material. Aside from modifying the actual sample surface, one can also use the STM to tunnel electrons into a layer of electron beam photoresist on a sample, in order to do lithography. This has the advantage of offering more control of the exposure than traditional electron beam lithography. Another practical application of STM is atomic deposition of metals (gold, silver, tungsten, etc.) with any desired (pre-programmed) pattern, which can be used as contacts to nanodevices or as nanodevices themselves.

Variable temperature STM was used to investigate temperature dependency of molecular rotations on single crystalline surfaces.
Rotating molecules appear blurred compared to non-rotating ones.

Recently groups have found they can use the STM tip to rotate individual bonds within single molecules. The electrical resistance of the molecule depends on the orientation of the bond, so the molecule effectively becomes a molecular switch.

Tunneling is a functioning concept that arises from quantum mechanics. Classically, an object hitting an impenetrable barrier will not pass through. In contrast, objects with a very small mass, such as the electron, have wavelike characteristics which permit such an event, referred to as tunneling.

Electrons behave as beams of energy, and in the presence of a potential "U"("z"), assuming 1-dimensional case, the energy levels "ψ"("z") of the electrons are given by solutions to Schrödinger’s equation,

where "ħ" is the reduced Planck’s constant, "z" is the position, and "m" is the mass of an electron. If an electron of energy "E" is incident upon an energy barrier of height "U"("z"), the electron wave function is a traveling wave solution,

where

if "E" > "U"("z"), which is true for a wave function inside the tip or inside the sample. Inside a barrier, "E" < "U"("z") so the wave functions which satisfy this are decaying waves,

where

quantifies the decay of the wave inside the barrier, with the barrier in the +"z" direction for formula_6.

Knowing the wave function allows one to calculate the probability density for that electron to be found at some location. In the case of tunneling, the tip and sample wave functions overlap such that when under a bias, there is some finite probability to find the electron in the barrier region and even on the other side of the barrier. Let us assume the bias is "V" and the barrier width is "w". This probability, "P", that an electron at "z"=0 (left edge of barrier) can be found at "z"="w" (right edge of barrier) is proportional to the wave function squared,

If the bias is small, we can let "U" − "E" ≈ "φ" in the expression for "κ", where "φ", the work function, gives the minimum energy needed to bring an electron from an occupied level, the highest of which is at the Fermi level (for metals at "T"=0 kelvins), to vacuum level. When a small bias "V" is applied to the system, only electronic states very near the Fermi level, within "eV" (a product of electron charge and voltage, not to be confused here with electronvolt unit), are excited. These excited electrons can tunnel across the barrier. In other words, tunneling occurs mainly with electrons of energies near the Fermi level.

However, tunneling does require that there be an empty level of the same energy as the electron for the electron to tunnel into on the other side of the barrier. It is because of this restriction that the tunneling current can be related to the density of available or filled states in the sample. The current due to an applied voltage "V" (assume tunneling occurs sample to tip) depends on two factors: 1) the number of electrons between "E" and "eV" in the sample, and 2) the number among them which have corresponding free states to tunnel into on the other side of the barrier at the tip. The higher the density of available states the greater the tunneling current. When "V" is positive, electrons in the tip tunnel into empty states in the sample; for a negative bias, electrons tunnel out of occupied states in the sample into the tip.

Mathematically, this tunneling current is given by

One can sum the probability over energies between "E" − "eV" and "E" to get the number of states available in this energy range per unit volume, thereby finding the local density of states (LDOS) near the Fermi level. The LDOS near some energy "E" in an interval "ε" is given by

and the tunnel current at a small bias V is proportional to the LDOS near the Fermi level, which gives important information about the sample. It is desirable to use LDOS to express the current because this value does not change as the volume changes, while probability density does. Thus the tunneling current is given by

where ρ(0,"E") is the LDOS near the Fermi level of the sample at the sample surface. This current can also be expressed in terms of the LDOS near the Fermi level of the sample at the tip surface,

The exponential term in the above equations means that small variations in "w" greatly influence the tunnel current. If the separation is decreased by 1 Å, the current increases by an order of magnitude, and vice versa.

This approach fails to account for the "rate" at which electrons can pass the barrier. This rate should affect the tunneling current, so it can be treated using the Fermi's golden rule with the appropriate tunneling matrix element. John Bardeen solved this problem in his study of the metal-insulator-metal junction. He found that if he solved Schrödinger’s equation for each side of the junction separately to obtain the wave functions for each electrode, he could obtain the tunneling matrix, M, from the overlap of these two wave functions. This can be applied to STM by making the electrodes the tip and sample, assigning formula_12 and formula_13 as sample and tip wave functions, respectively, and evaluating M at some surface S between the metal electrodes, where "z"=0 at the sample surface and "z"="w" at the vertex of the tip.

Now, Fermi’s Golden Rule gives the rate for electron transfer across the barrier, and is written

where formula_15 restricts tunneling to occur only between electron levels with the same energy. The tunneling matrix element, given by

is a description of the lower energy associated with the interaction of wave functions at the overlap, also called the resonance energy.

Summing over all the states gives the tunneling current as

where "f" is the Fermi-Dirac distribution, ρ and ρ are the density of states in the sample and tip, respectively. The Fermi-Dirac distribution function describes the filling of electron levels at a given temperature T.

An earlier, similar invention, the "Topografiner" of R. Young, J. Ward, and F. Scire from the NIST, relied on field emission. However, Young is credited by the Nobel Committee as the person who realized that it should be possible to achieve better resolution by using the tunnel effect. It was later discovered that an even higher resolution could be achieved by calculating the Doppler effect. 




</doc>
<doc id="27775" url="https://en.wikipedia.org/wiki?curid=27775" title="STM">
STM

STM may refer to:







</doc>
<doc id="27778" url="https://en.wikipedia.org/wiki?curid=27778" title="Svenska Akademiens ordbok">
Svenska Akademiens ordbok

Svenska Akademiens ordbok (), abbreviated SAOB, is a dictionary published by the Swedish Academy, with the official title "Ordbok över svenska språket utgiven av Svenska Akademien". This dictionary is the Swedish counterpart of the "Oxford English Dictionary" (OED) or the "Deutsches Wörterbuch" (DWB). 

Work on the dictionary started in 1787 and the first volume was published in 1898 and as of 2019, when the latest volume appeared, work has progressed to the word VÄVNING. The dictionary has approximately 450,000 main entries. The searchable web version has been available since 1997.




</doc>
<doc id="27781" url="https://en.wikipedia.org/wiki?curid=27781" title="Shirehorses">
Shirehorses

The Shirehorses are a spoof band comprising two BBC Radio DJs from Manchester, Mark Radcliffe and Marc Riley, known collectively as Mark and Lard.

As part of their BBC Radio 1 shows, the pair produced pastiches of chart songs, such as "You're Gormless", a parody of Babybird's "You're Gorgeous", "Lardy Boy", a parody of Placebo's "Nancy Boy", and "Why Is It Always Dairylea", spoofing Travis's "Why Does It Always Rain on Me?", using the band names 'Baby Bloke', 'Gazebo' and 'Dave Lee Travisty' respectively. When they rewrote The Seahorses' "Love Is the Law" as "(Now) I Know (Where I'm Going) Our Kid", they chose the stage-name Shirehorses, which they then retained for future recordings and performances. Other parodies include "I Want a Roll with It" (spoofing "Roll with It" by Oasis), "Feel Like Shite" ("Alright" by Supergrass), and "Country Spouse" ("Country House" by Blur). 

The band toured extensively, playing many small, university gigs to exploit their popularity with students. However, they also performed at larger venues, supporting Blur on a 1997 UK tour, taking in several stadia, and appearing at Glastonbury Festival in 1997.

Marc Riley was formerly a member of British Manchester band the Fall and later the Creepers before embarking on a radio presentation career alongside Mark Radcliffe. Formerly a double act on BBC Radio 1, in March 2004 they went their separate ways, Radcliffe initially to BBC Radio 2, Riley to BBC Radio 6 Music and later joined at the station by Mark Radcliffe as part of the afternoon Radcliffe and Maconie show.

The Shirehorses have released two albums to date:


</doc>
<doc id="27783" url="https://en.wikipedia.org/wiki?curid=27783" title="Stem cell">
Stem cell

In multicellular organisms, stem cells are undifferentiated or partially differentiated cells that can differentiate into various types of cells and proliferate indefinitely to produce more of the same stem cell. They are the earliest type of cell in a cell lineage. They are found in both embryonic and adult organisms, but they have slightly different properties in each. They are usually distinguished from progenitor cells, which cannot divide indefinitely, and precursor or blast cells, which are usually committed to differentiating into one cell type.

In mammals, roughly 50–150 cells make up the inner cell mass during the blastocyst stage of embryonic development, around days 5–14. These have stem-cell capability. "In vivo", they eventually differentiate into all of the body's cell types (making them pluripotent). This process starts with the differentiation into the three germ layers – the ectoderm, mesoderm and endoderm – at the gastrulation stage. However, when they are isolated and cultured "in vitro", they can be kept in the stem-cell stage and are known as embryonic stem cells (ESCs).

Adult stem cells are found in a few select locations in the body, known as niches, such as those in the bone marrow or gonads. They exist to replenish rapidly lost cell types and are multipotent or unipotent, meaning they only differentiate into a few cell types or one cell type. In mammals, they include, among others, hematopoietic stem cells, which replenish blood and immune cells, basal cells, which maintain the skin epithelium, and mesenchymal stem cells, which maintain bone, cartilage, muscle and fat cells. Adult stem cells are a small minority of cells; they are vastly outnumbered by the progenitor cells and terminally differentiated cells that they differentiate into.

Research into stem cells grew out of findings by Canadian biologists Ernest A. McCulloch and James E. Till at the University of Toronto in the 1960s. , the only established medical therapy using stem cells is hematopoietic stem cell transplantation, first performed in 1958 by French oncologist Georges Mathé. Since 1998 however, it has been possible to culture and differentiate human embryonic stem cells (in stem-cell lines). The process of isolating these cells has been controversial, because it typically results in the destruction of the embryo. Sources for isolating ESCs have been restricted in some European countries and Canada, but others such as the UK and China have promoted the research. Somatic cell nuclear transfer is a cloning method that can be used to create a cloned embryo for the use of its embryonic stem cells in stem cell therapy. In 2006, a Japanese team led by Shinya Yamanaka discovered a method to convert mature body cells back into stem cells. These were termed induced pluripotent stem cells (iPSCs).

The key properties of a stem cell were first defined by Ernest McCulloch and James Till at the University of Toronto in the early 1960s. They discovered the blood-forming stem cell, the hematopoietic stem cell (HSC), through their pioneering work in mice. McCulloch and Till began a series of experiments in which bone marrow cells were injected into irradiated mice. They observed lumps in the spleens of the mice that were linearly proportional to the number of bone marrow cells injected. They hypothesized that each lump (colony) was a clone arising from a single marrow cell (stem cell). In subsequent work, McCulloch and Till, joined by graduate student Andy Becker and senior scientist Lou Siminovitch, confirmed that each lump did in fact arise from a single cell. Their results were published in "Nature" in 1963. In that same year, Siminovitch was a lead investigator for studies that found colony-forming cells were capable of self-renewal, which is a key defining property of stem cells that Till and McCulloch had theorized.

The first therapy using stem cells was a bone marrow transplant performed by French oncologist Georges Mathé in 1958 on five workers at the Vinča Nuclear Institute in Yugoslavia who had been affected by a criticality accident. The workers all survived.

In 1981, embryonic stem (ES) cells were first isolated and successfully cultured using mouse blastocysts by British biologists Martin Evans and Matthew Kaufman. This allowed the formation of murine genetic models, a system in which the genes of mice are deleted or altered in order to study their function in pathology. By 1998, embryonic stem cells were first isolated by American biologist James Thomson, which made it possible to have new transplantation methods or various cell types for testing new treatments. In 2006, Shinya Yamanaka’s team in Kyoto, Japan converted fibroblasts into pluripotent stem cells by modifying the expression of only four genes. The feat represents the origin of induced pluripotent stem cells, known as iPS cells.

The classical definition of a stem cell requires that it possesses two properties:

Two mechanisms ensure that a stem cell population is maintained (doesn't shrink in size):

1. Asymmetric cell division: a stem cell divides into one mother cell, which is identical to the original stem cell, and another daughter cell, which is differentiated.

When a stem cell self-renews, it divides and does not disrupt the undifferentiated state. This self-renewal demands control of cell cycle as well as upkeep of multipotency or pluripotency, which all depends on the stem cell.

2. Stochastic differentiation: when one stem cell grows and divides into two differentiated daughter cells, another stem cell undergoes mitosis and produces two stem cells identical to the original.

Stem cells use telomerase, a protein that restores telomeres, to protect their DNA and extend their cell division limit (the Hayflick limit).

Potency specifies the differentiation potential (the potential to differentiate into different cell types) of the stem cell.

In practice, stem cells are identified by whether they can regenerate tissue. For example, the defining test for bone marrow or hematopoietic stem cells (HSCs) is the ability to transplant the cells and save an individual without HSCs. This demonstrates that the cells can produce new blood cells over a long term. It should also be possible to isolate stem cells from the transplanted individual, which can themselves be transplanted into another individual without HSCs, demonstrating that the stem cell was able to self-renew.

Properties of stem cells can be illustrated "in vitro", using methods such as clonogenic assays, in which single cells are assessed for their ability to differentiate and self-renew. Stem cells can also be isolated by their possession of a distinctive set of cell surface markers. However, "in vitro" culture conditions can alter the behavior of cells, making it unclear whether the cells shall behave in a similar manner "in vivo". There is considerable debate as to whether some proposed adult cell populations are truly stem cells.

Embryonic stem cells (ESCs) are the cells of the inner cell mass of a blastocyst, formed prior to implantation in the uterus. In human embryonic development the blastocyst stage is reached 4–5 days after fertilization, at which time it consists of 50–150 cells. ESCs are pluripotent and give rise during development to all derivatives of the three germ layers: ectoderm, endoderm and mesoderm. In other words, they can develop into each of the more than 200 cell types of the adult body when given sufficient and necessary stimulation for a specific cell type. They do not contribute to the extraembryonic membranes or to the placenta.

During embryonic development the cells of the inner cell mass continuously divide and become more specialized. For example, a portion of the ectoderm in the dorsal part of the embryo specializes as 'neurectoderm', which will become the future central nervous system. Later in development, neurulation causes the neurectoderm to form the neural tube. At the neural tube stage, the anterior portion undergoes encephalization to generate or 'pattern' the basic form of the brain. At this stage of development, the principal cell type of the CNS is considered a neural stem cell.

The neural stem cells self-renew and at some point transition into radial glial progenitor cells (RGPs). Early-formed RGPs self-renew by symmetrical division to form a reservoir group of progenitor cells. These cells transition to a neurogenic state and start to divide asymmetrically to produce a large diversity of many different neuron types, each with unique gene expression, morphological, and functional characteristics. The process of generating neurons from radial glial cells is called neurogenesis. The radial glial cell, has a distinctive bipolar morphology with highly elongated processes spanning the thickness of the neural tube wall. It shares some glial characteristics, most notably the expression of glial fibrillary acidic protein (GFAP). The radial glial cell is the primary neural stem cell of the developing vertebrate CNS, and its cell body resides in the ventricular zone, adjacent to the developing ventricular system. Neural stem cells are committed to the neuronal lineages (neurons, astrocytes, and oligodendrocytes), and thus their potency is restricted.

Nearly all research to date has made use of mouse embryonic stem cells (mES) or human embryonic stem cells (hES) derived from the early inner cell mass. Both have the essential stem cell characteristics, yet they require very different environments in order to maintain an undifferentiated state. Mouse ES cells are grown on a layer of gelatin as an extracellular matrix (for support) and require the presence of leukemia inhibitory factor (LIF) in serum media. A drug cocktail containing inhibitors to GSK3B and the MAPK/ERK pathway, called 2i, has also been shown to maintain pluripotency in stem cell culture. Human ESCs are grown on a feeder layer of mouse embryonic fibroblasts and require the presence of basic fibroblast growth factor (bFGF or FGF-2). Without optimal culture conditions or genetic manipulation, embryonic stem cells will rapidly differentiate.

A human embryonic stem cell is also defined by the expression of several transcription factors and cell surface proteins. The transcription factors Oct-4, Nanog, and Sox2 form the core regulatory network that ensures the suppression of genes that lead to differentiation and the maintenance of pluripotency. The cell surface antigens most commonly used to identify hES cells are the glycolipids stage specific embryonic antigen 3 and 4, and the keratan sulfate antigens Tra-1-60 and Tra-1-81. The molecular definition of a stem cell includes many more proteins and continues to be a topic of research.

By using human embryonic stem cells to produce specialized cells like nerve cells or heart cells in the lab, scientists can gain access to adult human cells without taking tissue from patients. They can then study these specialized adult cells in detail to try to discern complications of diseases, or to study cell reactions to proposed new drugs.

Because of their combined abilities of unlimited expansion and pluripotency, embryonic stem cells remain a theoretically potential source for regenerative medicine and tissue replacement after injury or disease., however, there are currently no approved treatments using ES cells. The first human trial was approved by the US Food and Drug Administration in January 2009. However, the human trial was not initiated until October 13, 2010 in Atlanta for spinal cord injury research. On November 14, 2011 the company conducting the trial (Geron Corporation) announced that it will discontinue further development of its stem cell programs. Differentiating ES cells into usable cells while avoiding transplant rejection are just a few of the hurdles that embryonic stem cell researchers still face. Embryonic stem cells, being pluripotent, require specific signals for correct differentiation – if injected directly into another body, ES cells will differentiate into many different types of cells, causing a teratoma. Ethical considerations regarding the use of unborn human tissue are another reason for the lack of approved treatments using embryonic stem cells. Many nations currently have moratoria or limitations on either human ES cell research or the production of new human ES cell lines. 
Mesenchymal stem cells (MSC) are known to be multipotent, which can be found in adult tissues, for example, in the muscle, liver, bone marrow. Mesenchymal stem cells usually function as structural support in various organs as mentioned above, and control the movement of substances. MSC can differentiate into numerous cell categories as an illustration of adipocytes, osteocytes, and chondrocytes, derived by the mesodermal layer. Where the mesoderm layer provides an increase to the body’s skeletal elements, such as relating to the cartilage or bone. The term “meso” means middle, infusion originated from the Greek, signifying that mesenchymal cells are able to range and travel in early embryonic growth among the ectodermal and endodermal layers. This mechanism helps with space-filling thus, key for repairing wounds in adult organisms that have to do with mesenchymal cells in the dermis (skin), bone, or muscle.

Mesenchymal stem cells are known to be essential for regenerative medicine. They are broadly studied in clinical trials. Since they are easily isolated and obtain high yield, high plasticity, which makes able to facilitate inflammation and encourage cell growth, cell differentiation, and restoring tissue derived from immunomodulation and immunosuppression. MSC comes from the bone marrow, which requires an aggressive procedure when it comes to isolating the quantity and quality of the isolated cell, and it varies by how old the donor. When comparing the rates of MSC in the bone marrow aspirates and bone marrow stroma, the aspirates tend to have lower rates of MSC than the stroma. MSC are known to be heterogeneous, and they express a high level of pluripotent markers when compared to other types of stem cells, such as embryonic stem cells.

Embryonic stem cells (ESCs) have the ability to divide indefinitely while keeping their pluripotency, which is made possible through specialized mechanisms of cell cycle control. Compared to proliferating somatic cells, ESCs have unique cell cycle characteristics—such as rapid cell division caused by shortened G1 phase, absent G0 phase, and modifications in cell cycle checkpoints—which leaves the cells mostly in S phase at any given time. ESCs’ rapid division is demonstrated by their short doubling time, which ranges from 8 to 10 hours, whereas somatic cells have doubling time of approximately 20 hours or longer. As cells differentiate, these properties change: G1 and G2 phases lengthen, leading to longer cell division cycles. This suggests that a specific cell cycle structure may contribute to the establishment of pluripotency.

Particularly because G1 phase is the phase in which cells have increased sensitivity to differentiation, shortened G1 is one of the key characteristics of ESCs and plays an important role in maintaining undifferentiated phenotype. Although the exact molecular mechanism remains only partially understood, several studies have shown insight on how ESCs progress through G1—and  potentially other phases—so rapidly.

The cell cycle is regulated by complex network of cyclins, cyclin-dependent kinases (Cdk), cyclin-dependent kinase inhibitors (Cdkn), pocket proteins of the retinoblastoma (Rb) family, and other accessory factors. Foundational insight into the distinctive regulation of ESC cell cycle was gained by studies on mouse ESCs (mESCs). mESCs showed a cell cycle with highly abbreviated G1 phase, which enabled cells to rapidly alternate between M phase and S phase. In a somatic cell cycle, oscillatory activity of Cyclin-Cdk complexes is observed in sequential action, which controls crucial regulators of the cell cycle to induce unidirectional transitions between phases: Cyclin D and Cdk4/6 are active in the G1 phase, while Cyclin E and Cdk2 are active during the late G1 phase and S phase; and Cyclin A and Cdk2 are active in the S phase and G2, while Cyclin B and Cdk1 are active in G2 and M phase. However, in mESCs, this typically ordered and oscillatory activity of Cyclin-Cdk complexes is absent. Rather, the Cyclin E/Cdk2 complex is constitutively active throughout the cycle, keeping retinoblastoma protein (pRb) hyperphosphorylated and thus inactive. This allows for direct transition from M phase to the late G1 phase, leading to absence of D-type cyclins and therefore a shortened G1 phase. Cdk2 activity is crucial for both cell cycle regulation and cell-fate decisions in mESCs; downregulation of Cdk2 activity prolongs G1 phase progression, establishes a somatic cell-like cell cycle, and induces expression of differentiation markers.

In human ESCs (hESCs), the duration of G1 is dramatically shortened. This has been attributed to high mRNA levels of G1-related Cyclin D2 and Cdk4 genes and low levels of cell cycle regulatory proteins that inhibit cell cycle progression at G1, such as p21, p27, and p57. Furthermore, regulators of Cdk4 and Cdk6 activity, such as members of the Ink family of inhibitors (p15, p16, p18, and p19), are expressed at low levels or not at all. Thus, similar to mESCs, hESCs show high Cdk activity, with Cdk2 exhibiting the highest kinase activity. Also similar to mESCs, hESCs demonstrate the importance of Cdk2 in G1 phase regulation by showing that G1 to S transition is delayed when Cdk2 activity is inhibited and G1 is arrest when Cdk2 is knocked down. However unlike mESCs, hESCs have a functional G1 phase. hESCs show that the activities of Cyclin E/Cdk2 and Cyclin A/Cdk2 complexes are cell cycle-dependent and the Rb checkpoint in G1 is functional.

ESCs are also characterized by G1 checkpoint non-functionality, even though the G1 checkpoint is crucial for maintaining genomic stability. In response to DNA damage, ESCs do not stop in G1 to repair DNA damages but instead, depend on S and G2/M checkpoints or undergo apoptosis. The absence of G1 checkpoint in ESCs allows for the removal of cells with damaged DNA, hence avoiding potential mutations from inaccurate DNA repair. Consistent with this idea, ESCs are hypersensitive to DNA damage to minimize mutations passed onto the next generation.

The primitive stem cells located in the organs of fetuses are referred to as fetal stem cells.

There are two types of fetal stem cells:


Adult stem cells, also called somatic (from Greek σωματικóς, "of the body") stem cells, are stem cells which maintain and repair the tissue in which they are found. They can be found in children, as well as adults.

There are three known accessible sources of autologous adult stem cells in humans:
Stem cells can also be taken from umbilical cord blood just after birth. Of all stem cell types, autologous harvesting involves the least risk. By definition, autologous cells are obtained from one's own body, just as one may bank his or her own blood for elective surgical procedures.

Pluripotent adult stem cells are rare and generally small in number, but they can be found in umbilical cord blood and other tissues. Bone marrow is a rich source of adult stem cells, which have been used in treating several conditions including liver cirrhosis, chronic limb ischemia and endstage heart failure. The quantity of bone marrow stem cells declines with age and is greater in males than females during reproductive years. Much adult stem cell research to date has aimed to characterize their potency and self-renewal capabilities. DNA damage accumulates with age in both stem cells and the cells that comprise the stem cell environment. This accumulation is considered to be responsible, at least in part, for increasing stem cell dysfunction with aging (see DNA damage theory of aging).

Most adult stem cells are lineage-restricted (multipotent) and are generally referred to by their tissue origin (mesenchymal stem cell, adipose-derived stem cell, endothelial stem cell, dental pulp stem cell, etc.). Muse cells (multi-lineage differentiating stress enduring cells) are a recently discovered pluripotent stem cell type found in multiple adult tissues, including adipose, dermal fibroblasts, and bone marrow. While rare, muse cells are identifiable by their expression of SSEA-3, a marker for undifferentiated stem cells, and general mesenchymal stem cells markers such as CD105. When subjected to single cell suspension culture, the cells will generate clusters that are similar to embryoid bodies in morphology as well as gene expression, including canonical pluripotency markers Oct4, Sox2, and Nanog.

Adult stem cell treatments have been successfully used for many years to treat leukemia and related bone/blood cancers through bone marrow transplants. Adult stem cells are also used in veterinary medicine to treat tendon and ligament injuries in horses.

The use of adult stem cells in research and therapy is not as controversial as the use of embryonic stem cells, because the production of adult stem cells does not require the destruction of an embryo. Additionally, in instances where adult stem cells are obtained from the intended recipient (an autograft), the risk of rejection is essentially non-existent. Consequently, more US government funding is being provided for adult stem cell research.

With the increasing demand of human adult stem cells for both research and clinical purposes (typically 1–5 million cells per kg of body weight are required per treatment) it becomes of utmost importance to bridge the gap between the need to expand the cells in vitro and the capability of harnessing the factors underlying replicative senescence. Adult stem cells are known to have a limited lifespan in vitro and to enter replicative senescence almost undetectably upon starting in vitro culturing.

Multipotent stem cells are also found in amniotic fluid. These stem cells are very active, expand extensively without feeders and are not tumorigenic. Amniotic stem cells are multipotent and can differentiate in cells of adipogenic, osteogenic, myogenic, endothelial, hepatic and also neuronal lines.
Amniotic stem cells are a topic of active research.

Use of stem cells from amniotic fluid overcomes the ethical objections to using human embryos as a source of cells. Roman Catholic teaching forbids the use of embryonic stem cells in experimentation; accordingly, the Vatican newspaper "Osservatore Romano" called amniotic stem cells "the future of medicine".

It is possible to collect amniotic stem cells for donors or for autologous use: the first US amniotic stem cells bank was opened in 2009 in Medford, MA, by Biocell Center Corporation and collaborates with various hospitals and universities all over the world.

Adult stem cells have limitations with their potency; unlike embryonic stem cells (ESCs), they are not able to differentiate into cells from all three germ layers. As such, they are deemed multipotent.

However, reprogramming allows for the creation of pluripotent cells, induced pluripotent stem cells (iPSCs), from adult cells. These are not adult stem cells, but somatic cells (e.g. epithelial cells) reprogrammed to give rise to cells with pluripotent capabilities. Using genetic reprogramming with protein transcription factors, pluripotent stem cells with ESC-like capabilities have been derived. The first demonstration of induced pluripotent stem cells was conducted by Shinya Yamanaka and his colleagues at Kyoto University. They used the transcription factors Oct3/4, Sox2, c-Myc, and Klf4 to reprogram mouse fibroblast cells into pluripotent cells. Subsequent work used these factors to induce pluripotency in human fibroblast cells. Junying Yu, James Thomson, and their colleagues at the University of Wisconsin–Madison used a different set of factors, Oct4, Sox2, Nanog and Lin28, and carried out their experiments using cells from human foreskin. However, they were able to replicate Yamanaka's finding that inducing pluripotency in human cells was possible.

Induced pluripotent stem cells differ from embryonic stem cells. They share many similar properties, such as pluripotency and differentiation potential, the expression of pluripotency genes, epigenetic patterns, embryoid body and teratoma formation, and viable chimera formation, but there are many differences within these properties. The chromatin of iPSCs appears to be more "closed" or methylated than that of ESCs. Similarly, the gene expression pattern between ESCs and iPSCs, or even iPSCs sourced from different origins. There are thus questions about the "completeness" of reprogramming and the somatic memory of induced pluripotent stem cells. Despite this, inducing somatic cells to be pluripotent appears to be viable.

As a result of the success of these experiments, Ian Wilmut, who helped create the first cloned animal Dolly the Sheep, has announced that he will abandon somatic cell nuclear transfer as an avenue of research.

IPSCs has helped the field of medicine significantly by finding numerous ways to cure diseases. Since human IPSCc has given the advantage to make vitro models to study toxins and pathogenesis.

Furthermore, induced pluripotent stem cells provide several therapeutic advantages. Like ESCs, they are pluripotent. They thus have great differentiation potential; theoretically, they could produce any cell within the human body (if reprogramming to pluripotency was "complete"). Moreover, unlike ESCs, they potentially could allow doctors to create a pluripotent stem cell line for each individual patient. Frozen blood samples can be used as a valuable source of induced pluripotent stem cells. Patient specific stem cells allow for the screening for side effects before drug treatment, as well as the reduced risk of transplantation rejection. Despite their current limited use therapeutically, iPSCs hold create potential for future use in medical treatment and research.

The key factors controlling the cell cycle also regulate pluripotency. Thus, manipulation of relevant genes can maintain pluripotency and reprogram somatic cells to an induced pluripotent state. However, reprogramming of somatic cells is often low in efficiency and considered stochastic.

With the idea that a more rapid cell cycle is a key component of pluripotency, reprogramming efficiency can be improved. Methods for improving pluripotency through manipulation of cell cycle regulators include: overexpression of Cyclin D/Cdk4, phosphorylation of Sox2 at S39 and S253, overexpression of Cyclin A and Cyclin E, knockdown of Rb, and knockdown of members of the Cip/Kip family or the Ink family. Furthermore, reprogramming efficiency is correlated with the number of cell divisions happened during the stochastic phase, which is suggested by the growing inefficiency of reprogramming of older or slow diving cells.

Lineage is an important procedure to analyze developing embryos. Since cell lineages shows the relationship between cells at each division. This helps in analyzing stem cell lineages along the way which helps recognize stem cell effectiveness, lifespan, and other factors. With the technique of cell lineage mutant genes can be analyzed in stem cell clones that can help in genetic pathways. These pathways can regulate how the stem cell perform

To ensure self-renewal, stem cells undergo two types of cell division (see "Stem cell division and differentiation" diagram). Symmetric division gives rise to two identical daughter cells both endowed with stem cell properties. Asymmetric division, on the other hand, produces only one stem cell and a progenitor cell with limited self-renewal potential. Progenitors can go through several rounds of cell division before terminally differentiating into a mature cell. It is possible that the molecular distinction between symmetric and asymmetric divisions lies in differential segregation of cell membrane proteins (such as receptors) between the daughter cells.

An alternative theory is that stem cells remain undifferentiated due to environmental cues in their particular niche. Stem cells differentiate when they leave that niche or no longer receive those signals. Studies in "Drosophila" germarium have identified the signals decapentaplegic and adherens junctions that prevent germarium stem cells from differentiating.

Stem cell therapy is the use of stem cells to treat or prevent a disease or condition. Bone marrow transplant is a form of stem cell therapy that has been used for many years because it has proven to be effective in clinical trials.

Stem cell implantation may help in strengthening the left-ventricle of the heart, as well as retaining the heart tissue to patients who have suffered from heart attacks in the past.

Stem cell treatments may lower symptoms of the disease or condition that is being treated. The lowering of symptoms may allow patients to reduce the drug intake of the disease or condition. Stem cell treatment may also provide knowledge for society to further stem cell understanding and future treatments.

Stem cell treatments may require immunosuppression because of a requirement for radiation before the transplant to remove the person's previous cells, or because the patient's immune system may target the stem cells. One approach to avoid the second possibility is to use stem cells from the same patient who is being treated.

Pluripotency in certain stem cells could also make it difficult to obtain a specific cell type. It is also difficult to obtain the exact cell type needed, because not all cells in a population differentiate uniformly. Undifferentiated cells can create tissues other than desired types.

Some stem cells form tumors after transplantation; pluripotency is linked to tumor formation especially in embryonic stem cells, fetal proper stem cells, induced pluripotent stem cells. Fetal proper stem cells form tumors despite multipotency.

Stem cell tourism is the internet based-industry in which stem cell procedures are advertised to the public as a proven cure. in the majority of cases resulting in patients and families traveling overseas to obtain procedures that are not proven, or part of an FDA approved clinical trial. These procedures have not gone through the vetting process of clinical research and they lack rigorous scientific support. Although for the general public, this advertising may sound authoritative, for translational doctors and scientists this leads to the exploitation of vulnerable patients. These procedures lack the reproducibility, the rigor that is required. Although the term may imply traveling long distances, in recent years, there has been an explosion of "stem cell clinics' in the US which has been well documented. these activities are highly profitable for the clinic but no benefit for the patients, sometimes experiencing complications like spinal tumors, death, or financial ruin, all of which are documented in the scientific literature. There is a great deal of interest in educating the public and patients, families and doctors who deal with patients requesting stem cells clinics.

Despite the great interest generated in the public for the use of stem cells, among all stem cell scientists, including the International Society for Stem Cell Research, the largest academic organization of scientist and advocates for stem cell research in the world. Stem cell therapy is still under development and although there is a great deal of research around the world. Rigorous stem cell trials are still ongoing and patients should be educated to be aware of the unethical clinics in the US or abroad, that offer stem cells procedures as a cure when it is still under investigation.

Some of the fundamental patents covering human embryonic stem cells are owned by the Wisconsin Alumni Research Foundation (WARF) – they are patents 5,843,780, 6,200,806, and 7,029,913 invented by James A. Thomson. WARF does not enforce these patents against academic scientists, but does enforce them against companies.

In 2006, a request for the US Patent and Trademark Office (USPTO) to re-examine the three patents was filed by the Public Patent Foundation on behalf of its client, the non-profit patent-watchdog group Consumer Watchdog (formerly the Foundation for Taxpayer and Consumer Rights). In the re-examination process, which involves several rounds of discussion between the USPTO and the parties, the USPTO initially agreed with Consumer Watchdog and rejected all the claims in all three patents, however in response, WARF amended the claims of all three patents to make them more narrow, and in 2008 the USPTO found the amended claims in all three patents to be patentable. The decision on one of the patents (7,029,913) was appealable, while the decisions on the other two were not. Consumer Watchdog appealed the granting of the '913 patent to the USPTO's Board of Patent Appeals and Interferences (BPAI) which granted the appeal, and in 2010 the BPAI decided that the amended claims of the '913 patent were not patentable. However, WARF was able to re-open prosecution of the case and did so, amending the claims of the '913 patent again to make them more narrow, and in January 2013 the amended claims were allowed.

In July 2013, Consumer Watchdog announced that it would appeal the decision to allow the claims of the '913 patent to the US Court of Appeals for the Federal Circuit (CAFC), the federal appeals court that hears patent cases. At a hearing in December 2013, the CAFC raised the question of whether Consumer Watchdog had legal standing to appeal; the case could not proceed until that issue was resolved.

Diseases and conditions where stem cell treatment is being investigated include:

Research is underway to develop various sources for stem cells, and to apply stem cell treatments for neurodegenerative diseases and conditions, diabetes, heart disease, and other conditions. Research is also underway in generating organoids using stem cells, which would allow for further understanding of human development, organogenesis, and modeling of human diseases.

In more recent years, with the ability of scientists to isolate and culture embryonic stem cells, and with scientists' growing ability to create stem cells using somatic cell nuclear transfer and techniques to create induced pluripotent stem cells, controversy has crept in, both related to abortion politics and to human cloning.

Hepatotoxicity and drug-induced liver injury account for a substantial number of failures of new drugs in development and market withdrawal, highlighting the need for screening assays such as stem cell-derived hepatocyte-like cells, that are capable of detecting toxicity early in the drug development process.




</doc>
<doc id="27784" url="https://en.wikipedia.org/wiki?curid=27784" title="Sappho">
Sappho

Sappho (; "Sapphō" ; Aeolic Greek "Psápphō"; c. 630 – c. 570 BC) was an Archaic Greek poet from the island of Lesbos. Sappho is known for her lyric poetry, written to be sung while accompanied by a lyre. In ancient times, Sappho was widely regarded as one of the greatest lyric poets and was given names such as the "Tenth Muse" and "The Poetess". Most of Sappho's poetry is now lost, and what is extant has mostly survived in fragmentary form; two notable exceptions are the "Ode to Aphrodite" and the Tithonus poem. As well as lyric poetry, ancient commentators claimed that Sappho wrote elegiac and iambic poetry. Three epigrams attributed to Sappho are extant, but these are actually Hellenistic imitations of Sappho's style.

Little is known of Sappho's life. She was from a wealthy family from Lesbos, though her parents' names are uncertain. Ancient sources say that she had three brothers; the names of two of them, Charaxos and Larichos, are mentioned in the Brothers Poem discovered in 2014. She was exiled to Sicily around 600 BC, and may have continued to work until around 570. Later legends surrounding Sappho's love for the ferryman Phaon and her death are unreliable.

Sappho was a prolific poet, probably composing around 10,000 lines. Her poetry was well-known and greatly admired through much of antiquity, and she was among the canon of nine lyric poets most highly esteemed by scholars of Hellenistic Alexandria. Sappho's poetry is still considered extraordinary and her works continue to influence other writers. Beyond her poetry, she is well known as a symbol of love and desire between women, with the English words "sapphic" and "lesbian" being derived from her own name and the name of her home island respectively. Whilst her importance as a poet is confirmed from the earliest times, all interpretations of her work have been coloured and influenced by discussions of her sexuality.

There are three sources of information about Sappho's life: her "testimonia", the history of her times, and what can be gleaned from her own poetry — although scholars are cautious when reading poetry as a biographical source.

"Testimonia" is a term of art in ancient studies that refers to collections of classical biographical and literary references to classical authors. The "testimonia" regarding Sappho do not contain references contemporary to Sappho. The representations of Sappho's life that occur in the "testimonia" always need to be assessed for accuracy, because many of them are certainly not correct. The "testimonia" are also a source of knowledge regarding how Sappho's poetry was received in antiquity. Some details mentioned in the "testimonia" are derived from Sappho's own poetry, which is of great interest, especially considering the "testimonia" originate from a time when more of Sappho's poetry was extant than is the case for modern readers.

Little is known about Sappho's life for certain. She was from Mytilene on the island of Lesbos and was probably born around 630 BC.
Tradition names her mother as Cleïs, though ancient scholars may simply have guessed this name, assuming that Sappho's daughter Cleïs was named after her. Sappho's father's name is less certain. Ten names are known for Sappho's father from the ancient "testimonia"; this proliferation of possible names suggests that he was not explicitly named in any of Sappho's poetry. The earliest and most commonly attested name for Sappho's father is Scamandronymus. In Ovid's "Heroides", Sappho's father died when she was seven. Sappho's father is not mentioned in any of her surviving works, but Campbell suggests that this detail may have been based on a now-lost poem. Sappho's own name is found in numerous variant spellings, even in her own Aeolian dialect; the form that appears in her own extant poetry is Psappho.

No reliable portrait of Sappho's physical appearance has survived; all extant representations, ancient and modern, are artists' conceptions. In the Tithonus poem she describes her hair as now white but formerly "melaina", i.e. black. A literary papyrus of the second century A.D. describes her as "pantelos mikra", quite tiny. Alcaeus possibly describes Sappho as "violet-haired", which was a common Greek poetic way of describing dark hair. Some scholars dismiss this tradition as unreliable.

Sappho was said to have three brothers: Erigyius, Larichus, and Charaxus. According to Athenaeus, Sappho often praised Larichus for pouring wine in the town hall of Mytilene, an office held by boys of the best families. This indication that Sappho was born into an aristocratic family is consistent with the sometimes rarefied environments that her verses record. One ancient tradition tells of a relation between Charaxus and the Egyptian courtesan Rhodopis. Herodotus, the oldest source of the story, reports that Charaxus ransomed Rhodopis for a large sum and that Sappho wrote a poem rebuking him for this.

Sappho may have had a daughter named Cleïs, who is referred to in two fragments. Not all scholars accept that Cleïs was Sappho's daughter. Fragment 132 describes Cleïs as "παῖς" ("pais"), which, as well as meaning "child", can also refer to the "youthful beloved in a male homosexual liaison". It has been suggested that Cleïs was one of Sappho's younger lovers, rather than her daughter, though Judith Hallett argues that the language used in fragment 132 suggests that Sappho was referring to Cleïs as her daughter.

According to the Suda, Sappho was married to Kerkylas of Andros. However, the name appears to have been invented by a comic poet: the name "Kerkylas" comes from the word "κέρκος" ("kerkos"), a possible meaning of which is "penis", and is not otherwise attested as a name, while "Andros", as well as being the name of a Greek island, is a form of the Greek word "ἀνήρ" ("aner"), which means man. Thus, the name may be a joke.

Sappho and her family were exiled from Lesbos to Syracuse, Sicily, around 600 BC. The Parian Chronicle records Sappho going into exile some time between 604 and 591. This may have been as a result of her family's involvement with the conflicts between political elites on Lesbos in this period, the same reason for Sappho's contemporary Alcaeus' exile from Mytilene around the same time. Later the exiles were allowed to return.

A tradition going back at least to Menander (Fr. 258 K) suggested that Sappho killed herself by jumping off the Leucadian cliffs for love of Phaon, a ferryman. This is regarded as ahistorical by modern scholars, perhaps invented by the comic poets or originating from a misreading of a first-person reference in a non-biographical poem. The legend may have resulted in part from a desire to assert Sappho as heterosexual.

Sappho probably wrote around 10,000 lines of poetry; today, only about 650 survive. She is best known for her lyric poetry, written to be accompanied by music. The Suda also attributes to Sappho epigrams, elegiacs, and iambics; three of these epigrams are extant, but are in fact later Hellenistic poems inspired by Sappho, as are the iambic and elegiac poems attributed to her in the Suda. Ancient authors claim that Sappho primarily wrote love poetry, and the indirect transmission of Sappho's work supports this notion. However, the papyrus tradition suggests that this may not have been the case: a series of papyri published in 2014 contains fragments of ten consecutive poems from Book I of the Alexandrian edition of Sappho, of which only two are certainly love poems, while at least three and possibly four are primarily concerned with family.

Sappho's poetry was probably first written down on Lesbos, either in her lifetime or shortly afterwards, initially probably in the form of a score for performers of Sappho's work. In the fifth century B.C., Athenian book publishers probably began to produce copies of Lesbian lyric poetry, some including explanatory material and glosses as well as the poems themselves. Some time in the second or third century, Alexandrian scholars produced a critical edition of Sappho's poetry. There may have been more than one Alexandrian edition – John J. Winkler argues for two, one edited by Aristophanes of Byzantium and another by his pupil Aristarchus of Samothrace. This is not certain – ancient sources tell us that Aristarchus' edition of Alcaeus replaced the edition by Aristophanes, but are silent on whether Sappho's work, too, went through multiple editions.

The Alexandrian edition of Sappho's poetry was based on the existing Athenian collections, and was divided into at least eight books, though the exact number is uncertain. Many modern scholars have followed Denys Page, who conjectured a ninth book in the standard edition; Yatromanolakis doubts this, noting that though "testimonia" refer to an eighth book of Sappho's poetry, none mention a ninth. Whatever its make-up, the Alexandrian edition of Sappho probably grouped her poems by their metre: ancient sources tell us that each of the first three books contained poems in a single specific metre. Ancient editions of Sappho, possibly starting with the Alexandrian edition, seem to have ordered the poems in at least the first book of Sappho's poetry – which contained works composed in Sapphic stanzas – alphabetically.

Even after the publication of the standard Alexandrian edition, Sappho's poetry continued to circulate in other poetry collections. For instance, the Cologne Papyrus on which the Tithonus poem is preserved was part of a Hellenistic anthology of poetry, which contained poetry arranged by theme, rather than by metre and incipit, as it was in the Alexandrian edition.

The earliest surviving manuscripts of Sappho, including the potsherd on which fragment 2 is preserved, date to the third century BC, and thus predate the Alexandrian edition. The latest surviving copies of Sappho’s poems transmitted directly from ancient times are written on parchment codex pages from the sixth and seventh centuries AD, and were surely reproduced from ancient papyri now lost. Manuscript copies of Sappho's works may have survived a few centuries longer, but around the 9th century her poetry appears to have disappeared, and by the twelfth century, John Tzetzes could write that "the passage of time has destroyed Sappho and her works".

According to legend, Sappho's poetry was lost because the church disapproved of her morals. These legends appear to have originated in the renaissance – around 1550, Jerome Cardan wrote that Gregory Nazianzen had Sappho's work publicly destroyed, and at the end of the sixteenth century Joseph Justus Scaliger claimed that Sappho's works were burned in Rome and Constantinople in 1073 on the orders of Pope Gregory VII.

In reality, Sappho's work was probably lost as the demand for it was insufficiently great for it to be copied onto parchment when codices superseded papyrus scrolls as the predominant form of book. Another contributing factor to the loss of Sappho's poems may have been the perceived obscurity of her Aeolic dialect, which contains many archaisms and innovations absent from other ancient Greek dialects. During the Roman period, by which time the Attic dialect had become the standard for literary compositions, many readers found Sappho's dialect difficult to understand and, in the second century AD, the Roman author Apuleius specifically remarks on its "strangeness".

Only approximately 650 lines of Sappho's poetry still survive, of which just one poem – the "Ode to Aphrodite" – is complete, and more than half of the original lines survive in around ten more fragments. Many of the surviving fragments of Sappho contain only a single word – for example, fragment 169A is simply a word meaning "wedding gifts", and survives as part of a dictionary of rare words. The two major sources of surviving fragments of Sappho are quotations in other ancient works, from a whole poem to as little as a single word, and fragments of papyrus, many of which were discovered at Oxyrhynchus in Egypt. Other fragments survive on other materials, including parchment and potsherds. The oldest surviving fragment of Sappho currently known is the Cologne papyrus which contains the Tithonus poem, dating to the third century BC.

Until the last quarter of the nineteenth century, only the ancient quotations of Sappho survived. In 1879, the first new discovery of a fragment of Sappho was made at Fayum. By the end of the nineteenth century, Grenfell and Hunt had begun to excavate an ancient rubbish dump at Oxyrhynchus, leading to the discoveries of many previously unknown fragments of Sappho. Fragments of Sappho continue to be rediscovered. Most recently, major discoveries in 2004 (the "Tithonus poem" and a new, previously unknown fragment) and 2014 (fragments of nine poems: five already known but with new readings, four, including the "Brothers Poem", not previously known) have been reported in the media around the world.

Sappho clearly worked within a well-developed tradition of Lesbian poetry, which had evolved its own poetic diction, meters, and conventions. Among her famous poetic forebears were Arion and Terpander. Nonetheless, her work is innovative; it is some of the earliest Greek poetry to adopt the "lyric 'I'" – to write poetry adopting the viewpoint of a specific person, in contrast to the earlier epic poets Homer and Hesiod, who present themselves more as "conduits of divine inspiration". Her poetry explores individual identity and personal emotions – desire, jealousy, and love; it also adopts and reinterprets the existing imagery epic poetry in exploring these themes.

Sappho's poetry is known for its clear language and simple thoughts, sharply-drawn images, and use of direct quotation which brings a sense of immediacy. Unexpected word-play is a characteristic feature of her style. An example is from fragment 96: "now she stands out among Lydian women as after sunset the rose-fingered moon exceeds all stars", a variation of the Homeric epithet "rosy-fingered Dawn". Sappho's poetry often uses hyperbole, according to ancient critics "because of its charm". An example is found in fragment 111, where Sappho writes that "The groom approaches like Ares [...] Much bigger than a big man".

Leslie Kurke groups Sappho with those archaic Greek poets from what has been called the "élite" ideological tradition, which valued luxury ("habrosyne") and high birth. These elite poets tended to identify themselves with the worlds of Greek myths, gods, and heroes, as well as the wealthy East, especially Lydia. Thus in fragment 2 Sappho has Aphrodite "pour into golden cups nectar lavishly mingled with joys", while in the Tithonus poem she explicitly states that "I love the finer things ["habrosyne"]". According to Page DuBois, the language, as well as the content, of Sappho's poetry evokes an aristocratic sphere. She contrasts Sappho's "flowery,[...] adorned" style with the "austere, decorous, restrained" style embodied in the works of later classical authors such as Sophocles, Demosthenes, and Pindar.

Traditional modern literary critics of Sappho's poetry have tended to see her poetry as a vivid and skilled but spontaneous and naive expression of emotion: typical of this view are the remarks of H. J. Rose that "Sappho wrote as she spoke, owing practically nothing to any literary influence," and that her verse displays "the charm of absolute naturalness." Against this essentially romantic view, one school of more recent critics argues that, on the contrary, Sappho's poetry displays and depends for its effect on a sophisticated deployment of the strategies of traditional Greek rhetorical genres - so that it seems spontaneous, whilst actually being very crafted.

Today Sappho, for many, is a symbol of female homosexuality; the common term "lesbian" is an allusion to Sappho, originating from the name of the island of Lesbos, where she was born. However, she has not always been so considered. In classical Athenian comedy (from the Old Comedy of the fifth century to Menander in the late fourth and early third centuries BC), Sappho was caricatured as a promiscuous heterosexual woman, and it is not until the Hellenistic period that the first "testimonia" which explicitly discuss Sappho's homoeroticism are preserved. The earliest of these is a fragmentary biography written on papyrus in the late third or early second century BC, which states that Sappho was "accused by some of being irregular in her ways and a woman-lover". Denys Page comments that the phrase "by some" implies that even the full corpus of Sappho's poetry did not provide conclusive evidence of whether she described herself as having sex with women. These ancient authors do not appear to have believed that Sappho did, in fact, have sexual relationships with other women, and as late as the tenth century the Suda records that Sappho was "slanderously accused" of having sexual relationships with her "female pupils".

Among modern scholars, Sappho's sexuality is still debated – André Lardinois has described it as the "Great Sappho Question". Early translators of Sappho sometimes heterosexualised her poetry. Ambrose Philips' 1711 translation of the Ode to Aphrodite portrayed the object of Sappho's desire as male, a reading that was followed by virtually every other translator of the poem until the twentieth century, while in 1781 Alessandro Verri interpreted fragment 31 as being about Sappho's love for Phaon. Friedrich Gottlieb Welcker argued that Sappho's feelings for other women were "entirely idealistic and non-sensual", while Karl Otfried Müller wrote that fragment 31 described "nothing but a friendly affection": Glenn Most comments that "one wonders what language Sappho would have used to describe her feelings if they had been ones of sexual excitement", if this theory were correct. By 1970, it would be argued that the same poem contained "proof positive of [Sappho's] lesbianism".

All critical comment is, of course, embedded in the values of its time, and the world view of the person writing it. Today, it is generally accepted that Sappho's poetry portrays homoerotic feelings: as Sandra Boehringer puts it, her works "clearly celebrate eros between women". Toward the end of the twentieth century, though, some scholars began to reject the question of whether or not Sappho was a lesbian – Glenn Most wrote that Sappho herself "would have had no idea what people mean when they call her nowadays a homosexual", André Lardinois stated that it is "nonsensical" to ask whether Sappho was a lesbian, and Page duBois calls the question a "particularly obfuscating debate".

One of the major focuses of scholars studying Sappho has been to attempt to determine the cultural context in which Sappho's poems were composed and performed. Various cultural contexts and social roles played by Sappho have been suggested, including teacher, cult-leader, and poet performing for a circle of female friends. However, the performance contexts of many of Sappho's fragments are not easy to determine, and for many more than one possible context is conceivable.

One longstanding suggestion of a social role for Sappho is that of "Sappho as schoolmistress". At the beginning of the twentieth century, the German classicist Ulrich von Wilamowitz-Moellendorff posited that Sappho was a sort of schoolteacher, in order to "explain away Sappho's passion for her 'girls and defend her from accusations of homosexuality. The view continues to be influential, both among scholars and the general public, though more recently the idea has been criticised by historians as anachronistic and has been rejected by several prominent classicists as unjustified by the evidence. In 1959, Denys Page, for example, stated that Sappho's extant fragments portray "the loves and jealousies, the pleasures and pains, of Sappho and her companions"; and he adds, "We have found, and shall find, no trace of any formal or official or professional relationship between them, ... no trace of Sappho the principal of an academy." David A. Campbell in 1967 judged that Sappho may have "presided over a literary coterie", but that "evidence for a formal appointment as priestess or teacher is hard to find". None of Sappho's own poetry mentions her teaching, and the earliest "testimonium" to support the idea of Sappho as a teacher comes from Ovid, six centuries after Sappho's lifetime. Despite these problems, many newer interpretations of Sappho's social role are still based on this idea. In these interpretations, Sappho was involved in the ritual education of girls, for instance as a trainer of choruses of girls.

Even if Sappho did compose songs for training choruses of young girls, not all of her poems can be interpreted in this light, and despite scholars' best attempts to find one, Yatromanolakis argues that there is no single performance context to which all of Sappho's poems can be attributed. Parker argues that Sappho should be considered as part of a group of female friends for whom she would have performed, just as her contemporary Alcaeus is. Some of her poetry appears to have been composed for identifiable formal occasions, but many of her songs are about – and possibly were to be performed at – banquets.

In antiquity Sappho's poetry was highly admired, and several ancient sources refer to her as the "tenth Muse". The earliest surviving poem to do so is a third-century BC epigram by Dioscorides, but poems are preserved in the "Greek Anthology" by Antipater of Sidon and attributed to Plato on the same theme. She was sometimes referred to as "The Poetess", just as Homer was "The Poet". The scholars of Alexandria included Sappho in the canon of nine lyric poets. According to Aelian, the Athenian lawmaker and poet Solon asked to be taught a song by Sappho "so that I may learn it and then die". This story may well be apocryphal, especially as Ammianus Marcellinus tells a similar story about Socrates and a song of Stesichorus, but it is indicative of how highly Sappho's poetry was considered in the ancient world.

Sappho's poetry also influenced other ancient authors. In Greek, the Hellenistic poet Nossis was described by Marilyn B. Skinner as an imitator of Sappho, and Kathryn Gutzwiller argues that Nossis explicitly positioned herself as an inheritor of Sappho's position as a woman poet. Beyond poetry, Plato cites Sappho in his "Phaedrus", and Socrates' second speech on love in that dialogue appears to echo Sappho's descriptions of the physical effects of desire in fragment 31. In the first century BC, Catullus established the themes and metres of Sappho's poetry as a part of Latin literature, adopting the Sapphic stanza, believed in antiquity to have been invented by Sappho, giving his lover in his poetry the name "Lesbia" in reference to Sappho, and adapting and translating Sappho's 31st fragment in his poem 51.

Other ancient poets wrote about Sappho's life. She was a popular character in ancient Athenian comedy, and at least six separate comedies called "Sappho" are known. The earliest known ancient comedy to take Sappho as its main subject was the early-fifth or late-fourth century BC "Sappho" by Ameipsias, though nothing is known of it apart from its name. Sappho was also a favourite subject in the visual arts, the most commonly depicted poet on sixth and fifth-century Attic red-figure vase paintings, and the subject of a sculpture by Silanion.

From the fourth century BC, ancient works portray Sappho as a tragic heroine, driven to suicide by her unrequited love for Phaon. For instance, a fragment of a play by Menander says that Sappho threw herself off of the cliff at Leucas out of her love for Phaon. Ovid's "Heroides" 15 is written as a letter from Sappho to her supposed love Phaon, and when it was first rediscovered in the 15th century was thought to be a translation of an authentic letter of Sappho's. Sappho's suicide was also depicted in classical art, for instance on a first-century BC basilica in Rome near the Porta Maggiore.

While Sappho's poetry was admired in the ancient world, her character was not always so well considered. In the Roman period, critics found her lustful and perhaps even homosexual. Horace called her "mascula Sappho" in his "Epistles", which the later Porphyrio commented was "either because she is famous for her poetry, in which men more often excel, or because she is maligned for having been a tribad". By the third century AD, the difference between Sappho's literary reputation as a poet and her moral reputation as a woman had become so significant that the suggestion that there were in fact two Sapphos began to develop. In his "Historical Miscellanies", Aelian wrote that there was "another Sappho, a courtesan, not a poetess".

By the medieval period, Sappho's works had been lost, though she was still quoted in later authors. Her work became more accessible in the sixteenth century through printed editions of those authors who had quoted her. In 1508 Aldus Manutius printed an edition of Dionysius of Halicarnassus, which contained Sappho 1, the "Ode to Aphrodite", and the first printed edition of Longinus' "On the Sublime", complete with his quotation of Sappho 31, appeared in 1554. In 1566, the French printer Robert Estienne produced an edition of the Greek lyric poets which contained around 40 fragments attributed to Sappho.

In 1652, the first English translation of a poem by Sappho was published, in John Hall's translation of "On the Sublime". In 1681 Anne Le Fèvre's French edition of Sappho made her work even more widely known. Theodor Bergk's 1854 edition became the standard edition of Sappho in the second half of the 19th century; in the first part of the 20th, the papyrus discoveries of new poems by Sappho led to editions and translations by Edwin Marion Cox and John Maxwell Edmonds, and culminated in the 1955 publication of Edgar Lobel's and Denys Page's "Poetarum Lesbiorum Fragmenta".

Like the ancients, modern critics have tended to consider Sappho's poetry "extraordinary". As early as the 9th century, Sappho was referred to as a talented woman poet, and in works such as Boccaccio's "De Claris Mulieribus" and Christine de Pisan's "Book of the City of Ladies" she gained a reputation as a learned lady. Even after Sappho's works had been lost, the Sapphic stanza continued to be used in medieval lyric poetry, and with the rediscovery of her work in the Renaissance, she began to increasingly influence European poetry. In the 16th century, members of La Pléiade, a circle of French poets, were influenced by her to experiment with Sapphic stanzas and with writing love-poetry with a first-person female voice.

From the Romantic era, Sappho's work – especially her "Ode to Aphrodite" – has been a key influence of conceptions of what lyric poetry should be. Such influential poets as Alfred Lord Tennyson in the nineteenth century, and A. E. Housman in the twentieth, have been influenced by her poetry. Tennyson based poems including "Eleanore" and "Fatima" on Sappho's fragment 31, while three of Housman's works are adaptations of the Midnight poem, long thought to be by Sappho though the authorship is now disputed. At the beginning of the twentieth century, the Imagists – especially Ezra Pound, H. D., and Richard Aldington – were influenced by Sappho's fragments; a number of Pound's poems in his early collection "Lustra" were adaptations of Sapphic poems, while H. D.'s poetry was frequently Sapphic in "style, theme or content", and in some cases, such as "Fragment 40" more specifically invoke Sappho's writing.

It was not long after the rediscovery of Sappho that her sexuality once again became the focus of critical attention. In the early seventeenth century, John Donne wrote "Sapho to Philaenis", returning to the idea of Sappho as a hypersexual lover of women. The modern debate on Sappho's sexuality began in the 19th century, with Welcker publishing, in 1816, an article defending Sappho from charges of prostitution and lesbianism, arguing that she was chaste – a position which would later be taken up by Wilamowitz at the end of the 19th and Henry Thornton Wharton at the beginning of the 20th centuries. Despite attempts to defend her good name, in the nineteenth century Sappho was co-opted by the Decadent Movement as a lesbian "daughter of de Sade", by Charles Baudelaire in France and later Algernon Charles Swinburne in England. By the late 19th century, lesbian writers such as Michael Field and Amy Levy became interested in Sappho for her sexuality, and by the turn of the twentieth century she was a sort of "patron saint of lesbians".

From the beginning of the 19th century, women poets such as Felicia Hemans ("The Last Song of Sappho") and Letitia Elizabeth Landon ("Sketch the First. Sappho", and in "Ideal Likenesses") took Sappho as one of their progenitors. Sappho also began to be regarded as a role model for campaigners for women's rights, beginning with works such as Caroline Norton's "The Picture of Sappho". Later in that century, she would become a model for the so-called New Woman – independent and educated women who desired social and sexual autonomy – and by the 1960s, the feminist Sappho was – along with the hypersexual, often but not exclusively lesbian Sappho – one of the two most important cultural perceptions of Sappho.

The discoveries of new poems by Sappho in 2004 and 2014 excited both scholarly and media attention. The announcement of the Tithonus poem was the subject of international news coverage, and was described by Marylin Skinner as "the "trouvaille" of a lifetime".




</doc>
<doc id="27786" url="https://en.wikipedia.org/wiki?curid=27786" title="Simon bar Kokhba">
Simon bar Kokhba

Simon ben Kosevah, or Cosibah, known to posterity as Bar Kokhba (; died 135 CE), was a Jewish military leader who led the Bar Kokhba revolt against the Roman Empire in 132 CE. The revolt established a three-year-long independent Jewish state in which Bar Kokhba ruled as "Nasi" ("Prince"). Some of the rabbinic scholars in his time imagined him to be the long-expected Messiah. Bar Kokhba fell in the fortified town of Betar, after a prolonged siege of three and half years.

Documents discovered in the 20th century in the Cave of Letters give his original name, with variations: Simeon bar Kosevah (), Bar Koseva () or Ben Koseva (). It is probable that his original name was Bar Koseva. The name may indicate that his father or his place of origin was named Koseva(h), but might as well be a general family name.

During the revolt, the Jewish sage Rabbi Akiva regarded Simon as the Jewish messiah, and gave him the surname "Bar Kokhba" meaning "Son of the Star" in Aramaic, from the Star Prophecy verse from Numbers : "There shall come a star out of Jacob". The name Bar Kokhba does not appear in the Talmud but in ecclesiastical sources. The Jerusalem Talmud ("Taanit" 4:5) mentions him by the name of Bar Koziva. Rabbinical writers subsequent to Rabbi Akiva did not share Rabbi Akiva's estimation of ben Kosiva. Akiva's disciple, Jose ben Halaphta, in the Seder 'Olam (chapter 30) called him "bar Koziba" (), meaning, "son of the lie". The judgment of Bar Koseba that is implied by this change of name was carried on by later rabbinic scholarship at least to the time of the codification of the Talmud, where the name is always rendered "Simon bar Koziba" () or Bar Kozevah.

Despite the devastation wrought by the Romans during the First Jewish–Roman War (66–73 CE), which left the population and countryside in ruins, a series of laws passed by Roman Emperors provided the incentive for the second rebellion. Based on the delineation of years in Eusebius' "Chronicon" (whose Latin translation is known as the Chronicle of Jerome) the Jewish revolt began under the Roman governor Tineius (Tynius) Rufus in the 16th year of Hadrian's reign, or what was equivalent to the 4th year of the 227th Olympiad. Hadrian sent an army to crush the resistance, but it faced a strong opponent, since Bar Kokhba, as the recognised leader of Israel, punished any Jew who refused to join his ranks. Two and a half years later, after the war had ended, the Roman emperor Hadrian barred Jews from entering Ælia Capitolina, the pagan city he had built on the ruins of Jewish Jerusalem. The name Aelia was derived from one of the emperor's names, Aelius. According to Philostorgius, this was done so that its former Jewish inhabitants "might not find in the name of the city a pretext for claiming it as their country."

The second Jewish rebellion took place 60 years after the first and established an independent state lasting three years. For many Jews of the time, this turn of events was heralded as the long hoped for Messianic Age. The Romans fared very poorly during the initial revolt facing a unified Jewish force, in contrast to the First Jewish-Roman War, where Flavius Josephus records three separate Jewish armies fighting each other for control of the Temple Mount during the three weeks after the Romans had breached Jerusalem's walls and were fighting their way to the center. Being outnumbered and taking heavy casualties, the Romans adopted a scorched earth policy which reduced and demoralised the Judean populace, slowly grinding away at the will of the Judeans to sustain the war.

During the final phase of the war, Bar Kokhba took up refuge in the fortress of Betar. The Romans eventually captured it after laying siege to the city for three and a half years, and they killed all the defenders except for one Jewish youth, Simeon ben Gamliel, whose life was spared. Rabbi Yohanan has related the following account of the massacre: "The brains of three-hundred children were found upon one stone, along with three-hundred baskets of what remained of phylacteries ("tefillin") were found in Betar, each and every one of which had the capacity to hold three measures (three "seahs", or what is equivalent to about 28 liters). If you should come to take [all of them] into account, you would find that they amounted to three-hundred measures." Rabban [Shimon] Gamliel said: "Five-hundred schools were in Betar, while the smallest of them wasn't less than three-hundred children. They used to say, ‘If the enemy should ever come upon us, with these metal pointers [used in pointing at the letters of sacred writ] we'll go forth and stab them.’ But since iniquities had caused [their fall], the enemy came in and wrapped up each and every child in his own book and burnt them together, and no one remained except me." According to Cassius Dio, 580,000 Jews were killed in overall war operations across the country, and some 50 fortified towns and 985 villages razed to the ground, while the number of those who perished by famine, disease and fire was past finding out.

So costly was the Roman victory, that the Emperor Hadrian, when reporting to the Roman Senate, did not see fit to begin with the customary greeting “If you and your children are healthy, it is well; I and the legions are healthy.”

In the aftermath of the war, Hadrian consolidated the older political units of Judaea, Galilee and Samaria into the new province of Syria Palaestina, which is commonly interpreted as an attempt to complete the disassociation with Judaea.

Over the past few decades, new information about the revolt has come to light, from the discovery of several collections of letters, some possibly by Bar Kokhba himself, in the Cave of Letters overlooking the Dead Sea. These letters can now be seen at the Israel Museum.

According to Israeli archaeologist Yigael Yadin, Bar Kokhba tried to revive Hebrew and make Hebrew the official language of the Jews as part of his messianic ideology. In "A Roadmap to the Heavens: An Anthropological Study of Hegemony among Priests, Sages, and Laymen (Judaism and Jewish Life)" by Sigalit Ben-Zion (page 155), Yadin remarked: "it seems that this change came as a result of the order that was given by Bar Kokhba, who wanted to revive the Hebrew language and make it the official language of the state."

Simon bar Kokhba is portrayed in rabbinic literature as being somewhat irrational and irascible in conduct. The Talmud says that he presided over an army of Jewish insurgents numbering some 200,000, but had compelled its young recruits to prove their valor by each man chopping off one of his own fingers. The Sages of Israel complained to him why he marred the people of Israel with such blemishes. Whenever he would go forth into battle, he was reported as saying: "O Master of the universe, there is no need for you to assist us [against our enemies], but do not embarrass us either!" It is also said of him that he killed his maternal uncle, Rabbi Elazar Hamudaʻi, after suspecting him of collaborating with the enemy, thereby forfeiting Divine protection, which led to the destruction of Betar in which Bar Kokhba himself also perished.

Hadrian is thought to have personally supervised over the closing military operations in the siege against Betar. When the Roman army eventually took the city, soldiers carried Bar Kokhba's severed head to Hadrian, and when Hadrian asked who it was that killed him, a Samaritan replied that he had killed him. When Hadrian requested that they bring the severed head (Latin:"protome") of the slain victim close to him that he might see it, Hadrian observed that a serpent was wrapped around the head. Hadrian then relied: "Had it not been for God who killed him, who would have been able to kill him!?"

Bar Kokhba was a ruthless leader, punishing any Jew who refused to join his ranks. According to Eusebius' "Chronicon", he severely punished the sect of Christians with death by different means of torture for their refusal to fight against the Romans.

Since the end of the nineteenth century, Bar-Kochba has been the subject of numerous works of art (dramas, operas, novels, etc.), including:

Another operetta on the subject of Bar Kokhba was written by the Russian-Jewish emigre composer Yaacov Bilansky Levanon in Palestine in the 1920s.

John Zorn's Masada Chamber Ensemble recorded an album called "Bar Kokhba", showing a photograph of the Letter of Bar Kokhba to Yeshua, son of Galgola on the cover.

According to a legend, during his reign, Bar Kokhba was once presented a mutilated man, who had his tongue ripped out and hands cut off. Unable to talk or write, the victim was incapable of telling who his attackers were. Thus, Bar Kokhba decided to ask simple questions to which the dying man was able to nod or shake his head with his last movements; the murderers were consequently apprehended.

In Hungary, this legend spawned the "Bar Kokhba game", in which one of two players comes up with a word or object, while the other must figure it out by asking questions only to be answered with "yes" or "no". The questioner usually asks first if it is a living being, if not, if it is an object, if not, it is surely an abstraction. The verb "kibarkochbázni" ("to Bar Kochba out") became a common language verb meaning "retrieving information in an extremely tedious way".




</doc>
<doc id="27790" url="https://en.wikipedia.org/wiki?curid=27790" title="Schizophrenia">
Schizophrenia

Schizophrenia is a psychiatric disorder characterized by continuous or relapsing episodes of psychosis. Major symptoms include hallucinations (often hearing voices), delusions, and disorganized thinking. Other symptoms include social withdrawal, decreased emotional expression, and apathy. Symptoms typically come on gradually, begin in young adulthood, and in many cases never resolve. There is no objective diagnostic test; diagnosis is based on observed behavior, a history that includes the person's reported experiences, and reports of others familiar with the person. To be diagnosed with schizophrenia, symptoms and functional impairment need to be present for six months, (DSM-5), or one month, (ICD-11). Many people with schizophrenia have other mental disorders that often includes an anxiety disorder such as panic disorder, an obsessive–compulsive disorder, or a substance use disorder.
About 0.3% to 0.7% of people are affected by schizophrenia during their lifetime. In 2017, there were an estimated 1.1 million new cases and in 2019 a total of 20 million cases globally. Males are more often affected and on average have an earlier onset. The causes of schizophrenia include genetic and environmental factors. Genetic factors include a variety of common and rare genetic variants. Possible environmental factors include being raised in a city, cannabis use during adolescence, infections, the ages of a person's mother or father, and poor nutrition during pregnancy.
About half of those diagnosed with schizophrenia will have a significant improvement over the long term with no further relapses, and a small proportion of these will recover completely. The other half will have a lifelong impairment, and severe cases may be repeatedly admitted to hospital. Social problems such as long-term unemployment, poverty, homelessness, exploitation, and victimization are common consequences of schizophrenia. Compared to the general population, people with schizophrenia have a higher suicide rate (about 5% overall) and more physical health problems, leading to an average decreased life expectancy of 20 years. In 2015, an estimated 17,000 deaths were caused by schizophrenia.
The mainstay of treatment is antipsychotic medication, along with counselling, job training, and social rehabilitation. Up to a third of people do not respond to initial antipsychotics, in which case the antipsychotic clozapine may be used. In situations where there is a risk of harm to self or others, a short involuntary hospitalization may be necessary. Long-term hospitalization may be needed for a small number of people with severe schizophrenia. In countries where supportive services are limited or unavailable, long-term hospital stays are more typical.

Schizophrenia is a mental disorder characterized by significant alterations in perception, thoughts, mood, and behavior. Symptoms are described in terms of positive, negative, and cognitive symptoms. The positive symptoms of schizophrenia are the same for any psychosis and are sometimes referred to as psychotic symptoms. These may be present in any of the different psychoses, and are often transient making early diagnosis of schizophrenia problematic. Psychosis noted for the first time in a person who is later diagnosed with schizophrenia is referred to as a first-episode psychosis (FEP).

Positive symptoms are those symptoms that are not normally experienced, but are present in people during a psychotic episode in schizophrenia. They include delusions, hallucinations, and disorganized thoughts and speech, typically regarded as manifestations of psychosis. Hallucinations most commonly involve the sense of hearing as hearing voices but can sometimes involve any of the other senses of taste, sight, smell, and touch. They are also typically related to the content of the delusional theme. Delusions are bizarre or persecutory in nature. Distortions of self-experience such as feeling as if one's thoughts or feelings are not really one's own, to believing that thoughts are being inserted into one's mind, sometimes termed passivity phenomena, are also common. Thought disorders can include thought blocking, and disorganized speech – speech that is not understandable is known as word salad. Positive symptoms generally respond well to medication, and become reduced over the course of the illness, perhaps related to the age-related decline in dopamine activity.

Negative symptoms are deficits of normal emotional responses, or of other thought processes. The five recognised domains of negative symptoms are: blunted affect – showing flat expressions or little emotion; alogia – a poverty of speech; anhedonia – an inability to feel pleasure; asociality – the lack of desire to form relationships, and avolition – a lack of motivation and apathy. Avolition and anhedonia are seen as motivational deficits resulting from impaired reward processing. It has been suggested that negative symptoms are multidimensional and they have been categorised into two subdomains of apathy or lack of motivation, and diminished expression. Apathy includes avolition, anhedonia, and social withdrawal; diminished expression includes blunt effect, and alogia. Sometimes diminished expression is treated as both verbal and non-verbal. Apathy accounts for around 50 per cent of the most often found negative symptoms and affects functional outcome and subsequent quality of life. Apathy is related to disrupted cognitive processing affecting memory and planning including goal-directed behaviour. The two subdomains has suggested a need for separate treatment approaches. A lack of distress – relating to a reduced experience of depression and anxiety is another noted negative symptom. A distinction is often made between those negative symptoms that are inherent to schizophrenia, termed primary; and those that result from positive symptoms, from the side effects of antipsychotics, substance abuse, and social deprivation - termed secondary negative symptoms. Negative symptoms are less responsive to medication and the most difficult to treat. However if properly assessed, secondary negative symptoms are amenable to treatment.

Scales for specifically assessing the presence of negative symptoms, and for measuring their severity, and their changes have been introduced since the earlier scales such as the PANNS that deals with all types of symptoms. These scales are the "Clinical Assessment Interview for Negative Symptoms" (CAINS), and the "Brief Negative Symptom Scale" (BNSS) also known as second-generation scales. In 2020, ten years after its introduction a cross-cultural study of the use of BNSS found valid and reliable psychometric evidence for the five-domain structure cross-culturally. The BNSS is designed to assess both the presence and severity and change of negative symptoms of the five recognised domains, and the additional item of reduced normal distress. BNSS can register changes in negative symptoms in relation to psychosocial and pharmacological intervention trials. BNSS has also been used to study a proposed non-D2 treatment called SEP-363856. Findings supported the favouring of five domains over the two-dimensional proposition.

Cognitive deficits are the earliest and most constantly found symptoms in schizophrenia. They are often evident long before the onset of illness in the prodromal stage, and may be present in early adolescence, or childhood. They are a core feature but not considered to be core symptoms, as are positive and negative symptoms. However, their presence and degree of dysfunction is taken as a better indicator of functionality than the presentation of core symptoms. Cognitive deficits become worse at first episode psychosis but then return to baseline, and remain fairly stable over the course of the illness.

The deficits in cognition are seen to drive the negative psychosocial outcome in schizophrenia, and are claimed to equate to a possible reduction in IQ from the norm of 100 to 70–85. Cognitive deficits may be of neurocognition (nonsocial) or of social cognition. Neurocognition is the ability to receive and remember information, and includes verbal fluency, memory, reasoning, problem solving, speed of processing, and auditory and visual perception. Verbal memory and attention are seen to be the most affected. Verbal memory impairment is associated with a decreased level of semantic processing (relating meaning to words). Another memory impairment is that of episodic memory. An impairment in visual perception that is consistently found in schizophrenia is that of visual backward masking. Visual processing impairments include an inability to perceive complex visual illusions. Social cognition is concerned with the mental operations needed to interpret, and understand the self and others in the social world. This is also an associated impairment, and facial emotion perception is often found to be difficult. Facial perception is critical for ordinary social interaction. Cognitive impairments do not usually respond to antipsychotics, and there are a number of 
interventions that are used to try to improve them.

Onset typically occurs between the late teens and early 30s, with the peak incidence occurring in males in the early to mid twenties, and in females in the late twenties. Onset before the age of 17 is known as early-onset, and before the age of 13, as can sometimes occur is known as childhood schizophrenia or very early-onset. A later stage of onset can occur between the ages of 40 and 60, known as late-onset schizophrenia. A later onset over the age of 60 which may be difficult to differentiate as schizophrenia, is known as very-late-onset schizophrenia-like psychosis. Late onset has shown that a higher rate of females are affected; they have less severe symptoms, and need lower doses of antipsychotics. The earlier favouring of onset in males is later seen to be balanced by a post-menopausal increase in the development in females. Estrogen produced pre-menopause, has a dampening effect on dopamine receptors but its protection can be overridden by a genetic overload. There has been a dramatic increase in the numbers of older adults with schizophrenia. An estimated 70% of those with schizophrenia have cognitive deficits, and these are most pronounced in early onset, and late-onset illness.

Onset may happen suddenly, or may occur after the slow and gradual development of a number of signs and symptoms in a period known as the prodromal stage. Up to 75% of those with schizophrenia go through a prodromal stage. The negative and cognitive symptoms in the prodrome can precede FEP by many months, and up to five years. The period from FEP and treatment is known as the duration of untreated psychosis (DUP) which is seen to be a factor in functional outcome. The prodromal stage is the high-risk stage for the development of psychosis. Since the progression to first episode psychosis, is not inevitable an alternative term is often preferred of at risk mental state Recognition and early intervention at the prodromal stage would minimize the disruption to educational and social development associated with schizophrenia, and has been the focus of many studies. It is suggested that the use of anti-inflammatory compounds such as D-serine may prevent the transition to schizophrenia. Cognitive symptoms are not secondary to positive symptoms, or to the side effects of antipsychotics.

Cognitive impairments in the prodromal stage become worse after first episode psychosis (after which they return to baseline and then remain fairly stable), making early intervention to prevent such transition of prime importance. Early treatment with cognitive behavioral therapies are the gold standard. Neurological soft signs of clumsiness and loss of fine motor movement are often found in schizophrenia, and these resolve with effective treatment of FEP.

Genetic, environmental, and vulnerability factors are involved in the development of schizophrenia. The interactions of these risk factors are complex, as numerous and diverse insults from conception to adulthood can be involved. A genetic predisposition on its own, without interacting environmental factors, will not give rise to the development of schizophrenia. Schizophrenia is described as a neurodevelopmental disorder that lacks a precise boundary in its definition.

Estimates of the heritability of schizophrenia are between 70% and 80%, which implies that 70% to 80% of the individual differences in risk to schizophrenia is associated with genetics. These estimates vary because of the difficulty in separating genetic and environmental influences, and their accuracy has been queried. The greatest risk factor for developing schizophrenia is having a first-degree relative with the disease (risk is 6.5%); more than 40% of identical twins of those with schizophrenia are also affected. If one parent is affected the risk is about 13% and if both are affected the risk is nearly 50%. However, DSM-5 points out that most people with schizophrenia have no family history of psychosis. Results of candidate gene studies of schizophrenia have generally failed to find consistent associations, and the genetic loci identified by genome-wide association studies as associated with schizophrenia explain only a small fraction of the variation in the disease.

Many genes are known to be involved in schizophrenia, each with small effect and unknown transmission and expression. The summation of these effect sizes into a polygenic risk score can explain at least 7% of the variability in liability for schizophrenia. Around 5% of cases of schizophrenia are understood to be at least partially attributable to rare copy-number variations (CNVs); these structural variations are associated with known genomic disorders involving deletions at 22q11.2 (DiGeorge syndrome), duplications at 16p11.2 "16p11.2 duplication" (most frequently found) and deletions at 15q11.2 (Burnside-Butler syndrome). Some of these CNVs increase the risk of developing schizophrenia by as much as 20-fold, and are frequently comorbid with autism and intellectual disabilities.

The genes CRHR1 and CRHBP have been shown to be associated with a severity of suicidal behavior. These genes code for stress response proteins needed in the control of the HPA axis, and their interaction can affect this axis. Response to stress can cause lasting changes in the function of the HPA axis possibly disrupting the negative feedback mechanism, homeostasis, and the regulation of emotion leading to altered behaviors.

The question of how schizophrenia could be primarily genetically influenced, given that people with schizophrenia have lower fertility rates, is a paradox. It is expected that genetic variants that increase the risk of schizophrenia would be selected against due to their negative effects on reproductive fitness. A number of potential explanations have been proposed, including that alleles associated with schizophrenia risk confers a fitness advantage in unaffected individuals. While some evidence has not supported this idea, others propose that a large number of alleles each contributing a small amount can persist.

Environmental factors, each associated with a slight risk of developing schizophrenia in later life include oxygen deprivation, infection, prenatal maternal stress, and malnutrition in the mother during fetal development. A risk is also associated with maternal obesity, in increasing oxidative stress, and dysregulating the dopamine and serotonin pathways. Both maternal stress and infection have been demonstrated to alter fetal neurodevelopment through an increase of pro-inflammatory cytokines. There is a slighter risk associated with being born in the winter or spring possibly due to vitamin D deficiency or a prenatal viral infection. Other infections during pregnancy or around the time of birth that have been linked to an increased risk include infections by "Toxoplasma gondii" and "Chlamydia". The increased risk is about five to eight percent. Viral infections of the brain during childhood are also linked to a risk of schizophrenia during adulthood.

Adverse childhood experiences (ACEs), severe forms of which are classed as childhood trauma, range from being bullied or abused, to the death of a parent. Many adverse childhood experiences can cause toxic stress and increase the risk of psychosis. Schizophrenia was the last diagnosis to benefit from the link made between ACEs and adult mental health outcomes.

Living in an urban environment during childhood or as an adult has consistently been found to increase the risk of schizophrenia by a factor of two, even after taking into account drug use, ethnic group, and size of social group. A possible link between the urban environment and pollution has been suggested to be the cause of the elevated risk of schizophrenia.

Other risk factors of importance include social isolation, immigration related to social adversity and racial discrimination, family dysfunction, unemployment, and poor housing conditions. Having a father older than 40 years, or parents younger than 20 years are also associated with schizophrenia.

About half of those with schizophrenia use recreational drugs, including cannabis, nicotine, and alcohol excessively. Use of stimulants such as amphetamine and cocaine can lead to a temporary stimulant psychosis, which presents very similarly to schizophrenia. Rarely, alcohol use can also result in a similar alcohol-related psychosis. Drugs may also be used as coping mechanisms by people who have schizophrenia, to deal with depression, anxiety, boredom, and loneliness. The use of cannabis and tobacco are not associated with the development of cognitive deficits, and sometimes a reverse relationship is found where their use improves these symptoms. However, substance abuse is associated with an increased risk of suicide, and a poor response to treatment.

Cannabis-use may be a contributory factor in the development of schizophrenia, potentially increasing the risk of the disease in those who are already at risk. The increased risk may require the presence of certain genes within an individual. Its use is associated with doubling the rate. The use of more potent strains of cannabis having a high level of its active ingredient tetrahydrocannabinol (THC), increases the risk further. One of these strains is well known as skunk.

The mechanisms of schizophrenia are unknown, and a number of models have been put forward to explain the link between altered brain function and schizophrenia. One of the most common is the dopamine model, which attributes psychosis to the mind's faulty interpretation of the misfiring of dopaminergic neurons. This has been directly related to the symptoms of delusions and hallucinations. Abnormal dopamine signaling has been implicated in schizophrenia based on the usefulness of medications that affect the dopamine receptor and the observation that dopamine levels are increased during acute psychosis. A decrease in D receptors in the dorsolateral prefrontal cortex may also be responsible for deficits in working memory.

Another hypothesis is the glutamate model that links alterations between glutamatergic neurotransmission and neural oscillations that affect connections between the thalamus and the cortex. Studies have shown that a reduced expression of a glutamate receptor – NMDA receptor, and glutamate blocking drugs such as phencyclidine and ketamine can mimic the symptoms and cognitive problems associated with schizophrenia. Post-mortem studies consistently find that a subset of these neurons fail to express GAD67 (GAD1), in addition to abnormalities in brain morphometry. The subsets of interneurons that are abnormal in schizophrenia are responsible for the synchronizing of neural ensembles needed during working memory tasks. These give the neural oscillations produced as gamma waves that have a frequency of between 30 and 80 hertz. Both working memory tasks and gamma waves are impaired in schizophrenia, which may reflect abnormal interneuron functionality.

There are often impairments in cognition, social skills, and motor skills before the onset of schizophrenia, which suggests a neurodevelopmental model. Such frameworks have hypothesized links between these biological abnormalities and symptoms. Furthermore, problems before birth such as maternal infection, maternal malnutrition and complications during pregnancy all increase risk for schizophrenia. Schizophrenia usually emerges 18-25, an age period that overlaps with certain stages of neurodevelopment that are implicated in schizophrenia.

Deficits in executive functions, such as planning, inhibition, and working memory, are pervasive in schizophrenia. Although these functions are dissociable, their dysfunction in schizophrenia may reflect an underlying deficit in the ability to represent goal related information in working memory, and to utilize this to direct cognition and behavior. These impairments have been linked to a number of neuroimaging and neuropathological abnormalities. For example, functional neuroimaging studies report evidence of reduced neural processing efficiency, whereby the dorsolateral prefrontal cortex is activated to a greater degree to achieve a certain level of performance relative to controls on working memory tasks. These abnormalities may be linked to the consistent post-mortem finding of reduced neuropil, evidenced by increased pyramidal cell density and reduced dendritic spine density. These cellular and functional abnormalities may also be reflected in structural neuroimaging studies that find reduced grey matter volume in association with deficits in working memory tasks.

Positive symptoms have been linked to reduced cortical thickness in the superior temporal gyrus. Severity of negative symptoms has been linked to reduced thickness in the left medial orbitofrontal cortex. Anhedonia, traditionally defined as a reduced capacity to experience pleasure, is frequently reported in schizophrenia. However, a large body of evidence suggests that hedonic responses are intact in schizophrenia, and that what is reported to be anhedonia is a reflection of dysfunction in other processes related to reward. Overall, a failure of reward prediction is thought to lead to impairment in the generation of cognition and behavior required to obtain rewards, despite normal hedonic responses.

It has been hypothesized that in some people, development of schizophrenia is related to intestinal tract dysfunction such as seen with non-celiac gluten sensitivity or abnormalities in the gut microbiota. A subgroup of persons with schizophrenia present an immune response to gluten differently from that found in people with celiac, with elevated levels of certain serum biomarkers of gluten sensitivity such as anti-gliadin IgG or anti-gliadin IgA antibodies.

Another theory links abnormal brain lateralization to the development of being left-handed which is significantly more common in those with schizophrenia. This abnormal development of hemispheric asymmetry is noted in schizophrenia. Studies have concluded that the link is a true and verifiable effect that may reflect a genetic link between lateralization and schizophrenia.

Bayesian models of brain functioning have been utilized to link abnormalities in cellular functioning to symptoms. Both hallucinations and delusions have been suggested to reflect improper encoding of prior expectations, thereby causing expectation to excessively influence sensory perception and the formation of beliefs. In approved models of circuits that mediate predictive coding, reduced NMDA receptor activation, could in theory result in the positive symptoms of delusions and hallucinations.

There is no objective test or biomarker to confirm diagnosis. Psychoses can occur in several conditions and are often transient making early diagnosis of schizophrenia difficult. Psychosis noted for the first time in a person that is later diagnosed with schizophrenia is referred to as a first-episode psychosis (FEP).

Schizophrenia is diagnosed based on criteria in either the "Diagnostic and Statistical Manual of Mental Disorders" (DSM) published by the American Psychiatric Association or the International Statistical Classification of Diseases and Related Health Problems (ICD) published by the World Health Organization. These criteria use the self-reported experiences of the person and reported abnormalities in behavior, followed by a psychiatric assessment. The mental status examination is an important part of the assessment. An established tool for assessing the severity of positive and negative symptoms is the Positive and Negative Syndrome Scale (PANSS). This has been seen to have shortcomings relating to negative symptoms, and other scales – the "Clinical Assessment Interview for Negative Symptoms" (CAINS), and the "Brief Negative Symptoms Scale" (BNSS) have been introduced. The DSM-5, published in 2013, gives a "Scale to Assess the Severity of Symptom Dimensions" outlining eight dimensions of symptoms.

DSM-5 states that to be diagnosed with schizophrenia, two diagnostic criteria have to be met over the period of one month, with a significant impact on social or occupational functioning for at least six months. One of the symptoms needs to be either delusions, hallucinations, or disorganized speech. A second symptom could be one of the negative symptoms, or severely disorganized or catatonic behaviour. A different diagnosis of schizophreniform disorder can be made before the six months needed for the diagnosis of schizophrenia.

In Australia the guideline for diagnosis is for six months or more with symptoms severe enough to affect ordinary functioning. In the UK diagnosis is based on having the symptoms for most of the time for one month, with symptoms that significantly affect the ability to work, study, or to carry on ordinary daily living, and with other similar conditions ruled out.

The ICD criteria are typically used in European countries; the DSM criteria are used predominantly in the United States and Canada, and are prevailing in research studies. In practice, agreement between the two systems is high. The current proposal for the ICD-11 criteria for schizophrenia recommends adding self-disorder as a symptom.

A major unresolved difference between the two diagnostic systems is that of the requirement in DSM of an impaired functional outcome. WHO for ICD argues that not all people with schizophrenia have functional deficits and so these are not specific for the diagnosis.

Both manuals have adopted the chapter heading of "Schizophrenia spectrum and other psychotic disorders"; ICD modifying this as "Schizophrenia spectrum and other primary psychotic disorders". The definition of schizophrenia remains essentially the same as that specified by the 2000 text revised DSM-IV (DSM-IV-TR). However, with the publication of DSM-5, the APA removed all sub-classifications of schizophrenia. ICD-11 has also removed subtypes. The removed subtype from both, of catatonic has been relisted in ICD-11 as a "psychomotor disturbance" that may be present in schizophrenia.

Another major change was to remove the importance previously given to Schneider's first-rank symptoms. DSM-5 still uses the listing of schizophreniform disorder but ICD-11 no longer includes it. DSM-5 also recommends that a better distinction be made between a current condition of schizophrenia and its historical progress, to achieve a clearer overall characterization.

A dimensional assessment has been included in DSM-5 covering eight dimensions of symptoms to be rated (using the "Scale to Assess the Severity of Symptom Dimensions") – these include the five diagnostic criteria plus cognitive impairments, mania, and depression. This can add relevant information for the individual in regard to treatment, prognosis, and functional outcome; it also enables the response to treatment to be more accurately described.

Two of the negative symptoms – avolition and diminished emotional expression, have been given more prominence in both manuals.

Many people with schizophrenia have one or more other disorders that often includes an anxiety disorder such as panic disorder, an obsessive-compulsive disorder, or a substance use disorder. These are separate disorders that need separate treatments. Sleep disorders are commonly found with schizophrenia, and are early signs of illness and also of relapse. Sleep disorders are linked with positive symptoms that include disorganized thinking and can adversely affect neocortical plasticity and cognition. The consolidation of memories is disrupted in sleep disorders. They are associated with severity of illness, a poor prognosis, and poor quality of life. Sleep onset and maintenance insomnia is a common symptom, regardless of whether treatment has been received or not. There is also a clozapine-induced somnolence. A related condition is antipsychotic-induced restless legs syndrome. Genetic variations have been found associated with these conditions involving the circadian rhythm, dopamine and histamine metabolism, and signal transduction.

Psychotic symptoms lasting less than a month may be diagnosed as brief psychotic disorder, and various conditions may be classed as psychotic disorder not otherwise specified; schizoaffective disorder is diagnosed if symptoms of mood disorder are substantially present alongside psychotic symptoms. If the psychotic symptoms are the direct physiological result of a general medical condition or a substance, then the diagnosis is one of a psychosis secondary to that condition. Schizophrenia is not diagnosed if symptoms of pervasive developmental disorder are present unless prominent delusions or hallucinations are also present.

Psychotic symptoms may be present in several other conditions, and mental disorders, including bipolar disorder, borderline personality disorder, substance intoxication, substance-induced psychosis, and a number of drug withdrawal syndromes. Non-bizarre delusions are also present in delusional disorder, and social withdrawal in social anxiety disorder, avoidant personality disorder and schizotypal personality disorder. Schizotypal personality disorder has symptoms that are similar but less severe than those of schizophrenia. Schizophrenia occurs along with obsessive-compulsive disorder (OCD) considerably more often than could be explained by chance, although it can be difficult to distinguish obsessions that occur in OCD from the delusions of schizophrenia.

A more general medical and neurological examination may be needed to rule out medical illnesses which may rarely produce psychotic schizophrenia-like symptoms, such as metabolic disturbance, systemic infection, syphilis, HIV-associated neurocognitive disorder, epilepsy, limbic encephalitis, and brain lesions. Stroke, multiple sclerosis, hyperthyroidism, hypothyroidism, and dementias such as Alzheimer's disease, Huntington's disease, frontotemporal dementia, and the Lewy body dementias may also be associated with schizophrenia-like psychotic symptoms. It may be necessary to rule out a delirium, which can be distinguished by visual hallucinations, acute onset and fluctuating level of consciousness, and indicates an underlying medical illness. Investigations are not generally repeated for relapse unless there is a specific "medical" indication or possible adverse effects from antipsychotic medication. In children hallucinations must be separated from typical childhood fantasies.

Prevention of schizophrenia is difficult as there are no reliable markers for the later development of the disorder. There is tentative though inconclusive evidence for the effectiveness of early intervention to prevent schizophrenia in the prodrome phase. There is some evidence that early intervention in those with first-episode psychosis may improve short-term outcomes, but there is little benefit from these measures after five years. Cognitive behavioral therapy may reduce the risk of psychosis in those at high risk after a year and is recommended in this group, by the National Institute for Health and Care Excellence (NICE). Another preventive measure is to avoid drugs that have been associated with development of the disorder, including cannabis, cocaine, and amphetamines.

Antipsychotics are prescribed following a first-episode psychosis, and following remission a preventive maintenance use is continued to avoid relapse. However, it is recognised that some people do recover following a single episode and that long-term use of antipsychotics will not be needed but there is no way of identifying this group.

The primary treatment of schizophrenia is the use of antipsychotic medications, often in combination with psychosocial interventions and social supports. Community support services including drop-in centers, visits by members of a community mental health team, supported employment, and support groups are common. The time between the onset of psychotic symptoms to being given treatment – the duration of untreated psychosis (DUP) is associated with a poorer outcome in both the short term and the long term.

Voluntary or involuntary admittance to hospital may be needed to treat a severe episode, however, hospital stays are as short as possible. In the UK large mental hospitals termed asylums began to be closed down in the 1950s with the advent of antipsychotics, and with an awareness of the negative impact of long-term hospital stays on recovery. This process was known as deinstitutionalization, and community and supportive services were developed in order to support this change. Many other countries followed suit with the US starting in the 60s. There will still remain a few people who do not improve enough to be discharged. In those countries that lack the necessary supportive and social services long-term hospital stays are more usual.      

The first-line treatment for schizophrenia is an antipsychotic. The first-generation antipsychotics, now called typical antipsychotics, are dopamine antagonists that block D2 receptors, and affect the neurotransmission of dopamine. Those brought out later, the second-generation antipsychotics known as atypical antipsychotics, can also have effect on another neurotransmitter serotonin. Antipsychotics can reduce the symptoms of anxiety within hours of their use but for other symptoms they may take several days or weeks to reach their full effect. They have little effect on negative and cognitive symptoms, which may be helped by additional psychotherapies and medications. There is no single antipsychotic suitable for first-line treatment for everyone, as responses and tolerances vary between people. Stopping medication may be considered after a single psychotic episode where there has been a full recovery with no symptoms for twelve months. Repeated relapses worsen the long-term outlook and the risk of relapse following a second episode is high, and long-term treatment is usually recommended.

Tobacco smoking increases the metabolism of some antipsychotics, by strongly activitating CYP1A2 the enzyme that breaks them down, and a significant difference is found in these levels between smokers and non-smokers. It is recommended that the dosage for those smokers on clozapine be increased by 50%, and for those on olanzapine by 30%. The result of stopping smoking can lead to an increased concentration of the antipsychotic that may result in toxicity, so that monitoring of effects would need to take place with a view to decreasing the dosage; many symptoms may be noticeably worsened, and extreme fatigue, and seizures are also possible with a risk of relapse. Likewise those who resume smoking may need their dosages adjusted accordingly. The altering effects are due to compounds in tobacco smoke and not to nicotine; the use of nicotine replacement therapy therefore has the equivalent effect of stopping smoking and monitoring would still be needed.

About 30 to 50 percent of people with schizophrenia fail to accept that they have an illness or comply with their recommended treatment. For those who are unwilling or unable to take medication regularly, long-acting injections of antipsychotics may be used, which reduce the risk of relapse to a greater degree than oral medications. When used in combination with psychosocial interventions, they may improve long-term adherence to treatment.

Research findings suggested that other neurotransmission systems including serotonin, glutamate, GABA, and acetycholine were implicated in the development of schizophrenia, and that a more inclusive medication was needed. A new first-in-class antipsychotic that targets multiple neurotransmitter systems called lumateperone (ITI-007), was trialed and approved by the FDA in December 2019 for the treatment of schizophrenia in adults. Lumateperone is a small molecule agent that shows improved safety, and tolerance. It interacts with dopamine, serotonin, and glutamate in a complex, uniquely selective manner, and is seen to improve negative symptoms, and social functioning. Lumateperone was also found to reduce potential metabolic dysfunction, have lower rates of movement disorders, and have lower cardiovascular side effects such as a fast heart rate.

Typical antipsychotics are associated with a higher rate of movement disorders including akathisia. Some atypicals are associated with considerable weight gain, diabetes and the risk of metabolic syndrome. Risperidone (atypical) has a similar rate of extrapyramidal symptoms to haloperidol (typical). A rare but potentially lethal condition of neuroleptic malignant syndrome (NMS) has been associated with the use of antipsychotics. Through its early recognition, and timely intervention rates have declined. However, an awareness of the syndrome is advised to enable intervention. Another less rare condition of tardive dyskinesia can occur due to long-term use of antipsychotics, developing after many months or years of use. It is more often reported with use of typical antipsychotics.

Clozapine is associated with side effects that include weight gain, tiredness, and hypersalivation. More serious adverse effects include seizures, NMS, neutropenia, and agranulocytosis (lowered white blood cell count) and its use needs careful monitoring. Studies have found that antipsychotic treatment following NMS and neutropenia may sometimes be successfully rechallenged (restarted) with clozapine.

Clozapine is also associated with thromboembolism (including pulmonary embolism), myocarditis, and cardiomyopathy. A systematic review of clozapine-associated pulmonary embolism indicates that this adverse effect can often be fatal, and that it has an early onset, and is dose-dependent. The findings advised the consideration of using a prevention therapy for venous thromboembolism after starting treatment with clozapine, and continuing this for six months. Constipation is three times more likely to occur with the use of clozapine, and severe cases can lead to ileus and bowel ischemia resulting in many fatalities.

However, the risk of serious adverse effects from clozapine is low, and there are the beneficial effects to be gained of a reduced risk of suicide, and aggression. Typical antipsychotics and atypical risperidone can have a side effect of sexual dysfunction. Clozapine, olanzapine, and quetiapine are associated with beneficial effects on sexual functioning helped by various psychotherapies. Unwanted side effects cause people to stop treatment, resulting in relapses.

About half of those with schizophrenia will respond favourably to antipsychotics, and have a good return of functioning. However, positive symptoms persist in up to a third of people. Following two trials of different antipsychotics over six weeks, that also prove ineffective, they will be classed as having treatment resistant schizophrenia (TRS), and clozapine will be offered. Clozapine is of benefit to around half of this group although it has the potentially serious side effect of agranulocytosis (lowered white blood cell count) in less than 4% of people. Between 12 and 20 per cent will not respond to clozapine and this group is said to have ultra treatment resistant schizophrenia. ECT may be offered to treat TRS as an add-on therapy, and is shown to sometimes be of benefit. A review concluded that this use only has an effect on medium-term TRS and that there is not enough evidence to support its use other than for this group.

TRS is often accompanied by a low quality of life, and greater social dysfunction. TRS may be the result of inadequate rather than inefficient treatment; it also may be a false label due to medication not being taken regularly, or at all. About 16 per cent of people who had initially been responsive to treatment later develop resistance. This could relate to the length of time on APs, with treatment becoming less responsive. This finding also supports the involvement of dopamine in the development of schizophrenia. Studies suggest that TRS may be a more heritable form.

TRS may be evident from first episode psychosis, or from a relapse. It can vary in its intensity and response to other therapies. This variation is seen to possibly indicate an underlying neurobiology such as dopamine supersensitivity (DSS), glutamate or serotonin dysfunction, inflammation and oxidative stress. Studies have found that dopamine supersensitivity is found in up to 70% of those with TRS. The variation has led to the suggestion that treatment responsive and treatment resistant schizophrenia be considered as two different subtypes. It is further suggested that if the subtypes could be distinguished at an early stage significant implications could follow for treatment considerations, and for research. Neuroimaging studies have found a significant decrease in the volume of grey matter in those with TRS with no such change seen in those who are treatment responsive. In those with ultra treatment resistance the decrease in grey matter volume was larger.

A link has been made between the gut microbiota and the development of TRS. The most prevalent cause put forward for TRS is that of mutation in the genes responsible for drug effectiveness. These include liver enzyme genes that control the availability of a drug to brain targets, and genes responsible for the structure and function of these targets. In the colon the bacteria encode a hundred times more genes than exist in the human genome. Only a fraction of ingested drugs reach the colon, having been already exposed to small intestinal bacteria, and absorbed in the portal circulation. This small fraction is then subject to the metabolic action of many communities of bacteria. Activation of the drug depends on the composition and enzymes of the bacteria and of the specifics of the drug, and therefore a great deal of individual variation can affect both the usefulness of the drug and its tolerability. It is suggested that parenteral administration of antipsychotics would bypass the gut and be more successful in overcoming TRS. The composition of gut microbiota is variable between individuals, but they are seen to remain stable. However, phyla can change in response to many factors including ageing, diet, substance-use, and medications – especially antibiotics, laxatives, and antipsychotics. In FEP, schizophrenia has been linked to significant changes in the gut microbiota that can predict response to treatment.

A number of psychosocial interventions that include several types of psychotherapy may be useful in the treatment of schizophrenia such as: family therapy, group therapy, cognitive remediation therapy, cognitive behavioral therapy, and metacognitive training. Skills training, and help with substance use, and weight management– often needed as a side effect of an antipsychotic, are also offered. In the US, interventions for first episode psychosis have been brought together in an overall approach known as coordinated speciality care (CSC) and also includes support for education. In the UK "care across all phases" is a similar approach that covers many of the treatment guidelines recommended. The aim is to reduce the number of relapses and stays in hospital.

Other support services for education, employment, and housing are usually offered. For people suffering from severe schizophrenia, and discharged from a stay in hospital, these services are often brought together in an integrated approach to offer support in the community away from the hospital setting. In addition to medicine management, housing, and finances, assistance is given for more routine matters such as help with shopping and using public transport. This approach is known as assertive community treatment (ACT) and has been shown to achieve positive results in symptoms, social functioning and quality of life. Another more intense approach is known as "intensive care management" (ICM). ICM is a stage further than ACT and emphasises support of high intensity in smaller caseloads, (less than twenty). This approach is to provide long-term care in the community. Studies show that ICM improves many of the relevant outcomes including social functioning.

Some studies have shown little evidence for the effectiveness of cognitive behavioral therapy (CBT) in either reducing symptoms or preventing relapse. However, other studies have found that CBT does improve overall psychotic symptoms (when in use with medication) and has been recommended in Canada, but it has been seen here to have no effect on social function, relapse, or quality of life. In the UK it is recommended as an add-on therapy in the treatment of schizophrenia, but is not supported for use in treatment resistant schizophrenia. Arts therapies are seen to improve negative symptoms in some people, and are recommended by NICE in the UK. This approach however, is criticised as having not been well-researched, and arts therapies are not recommended in Australian guidelines for example. Peer support, in which people with personal experience of schizophrenia, provide help to each other, is of unclear benefit.

Exercise therapy has been shown to improve positive and negative symptoms, cognition, and improve quality of life. Aerobic exercise has been shown to improve cognitive deficits of working memory and attention. Exercise has also been shown to increase the volume of the hippocampus in those with schizophrenia. A decrease in hippocampal volume is one of the factors linked to the development of the disease. However, there still remains the problem of increasing motivation for, and maintaining participation in physical activity. Supervised sessions are recommended. In the UK healthy eating advice is offered alongside exercise programs.

An inadequate diet is often found in schizophrenia, and associated vitamin deficiencies including those of folate, and vitamin D are linked to the risk factors for the development of schizophrenia and for early death including heart disease. Those with schizophrenia possibly have the worst diet of all the mental disorders. Lower levels of folate and vitamin D have been noted as significantly lower in first episode psychosis. The use of supplemental folate is recommended. A zinc deficiency has also been noted. Vitamin B12 is also often deficient and this is linked to worse symptoms. Supplementation with B vitamins has been shown to significantly improve symptoms, and to put in reverse some of the cognitive deficits. It is also suggested that the noted dysfunction in gut microbiota might benefit from the use of probiotics.

Schizophrenia has great human and economic costs. It results in a decreased life expectancy of 20 years. This is primarily because of its association with obesity, poor diet, a sedentary lifestyle, and smoking, with an increased rate of suicide playing a lesser role. Side effects of antipsychotics may also increase the risk. These differences in life expectancy increased between the 1970s and 1990s. An Australian study puts the rate of early death at 25 years, and views the main cause to be related to heart disease. Primary polydipsia, or excessive fluid intake, is relatively common in people with chronic schizophrenia. This may lead to hyponatremia which can be life-threatening. Antipsychotics can lead to a dry mouth, but there are several other factors that may contribute to the disorder. It is suggested to lead to a reduction in life expectancy by 13 per cent. A study has suggested that real barriers to improving the mortality rate in schizophrenia are poverty, overlooking the symptoms of other illnesses, stress, stigma, and medication side effects, and that these need to be changed.
Schizophrenia is a major cause of disability. In 2016 it was classed as the 12th most disabling condition. Approximately 75% of people with schizophrenia have ongoing disability with relapses and 16.7 million people globally are deemed to have moderate or severe disability from the condition. Some people do recover completely and others function well in society. Most people with schizophrenia live independently with community support. About 85% are unemployed. In people with a first episode of psychosis in scizophrenia a good long-term outcome occurs in 31%, an intermediate outcome in 42% and a poor outcome in 31%. Males are affected more often than females, and have a worse outcome. Outcomes for schizophrenia appear better in the developing than the developed world. These conclusions have been questioned. Social problems, such as long-term unemployment, poverty, homelessness, exploitation, stigmatization and victimization are common consequences, and lead to social exclusion.
There is a higher than average suicide rate associated with schizophrenia estimated at around 5% to 6%, most often occurring in the period following onset or first hospital admission. Several times more (20 to 40%) attempt suicide at least once. There are a variety of risk factors, including male gender, depression, a high IQ, heavy smoking, and substance abuse. Repeated relapse is linked to an increased risk of suicidal behavior. The use of clozapine can reduce the risk of suicide and aggression.
Schizophrenia and smoking have shown a strong association in studies worldwide. Use of cigarettes is especially high in those diagnosed with schizophrenia, with estimates ranging from 80 to 90% being regular smokers, as compared to 20% of the general population. Those who smoke tend to smoke heavily, and additionally smoke cigarettes with high nicotine content. Some propose that this is in an effort to improve symptoms. Among people with schizophrenia use of cannabis is also common.

In 2017, the Global Burden of Disease Study estimated there were 1.1 million new cases, and in 2019 WHO reported a total of 20 million cases globally. Schizophrenia affects around 0.3–0.7% of people at some point in their life. It occurs 1.4 times more frequently in males than females and typically appears earlier in men – the peak ages of onset are 25 years for males and 27 years for females. Onset in childhood, before the age of 13 can sometimes occur. A later onset can occur between the ages of 40 and 60, known as late onset, and also after 60 known as very late onset.

Worldwide, schizophrenia is the most common psychotic disorder. The frequency of schizophrenia varies across the world, within countries, and at the local and neighborhood level. This variation has been estimated to be fivefold. It causes approximately one percent of worldwide disability adjusted life years and resulted in 17,000 deaths in 2015.

In 2000, the World Health Organization found the percentage of people affected and the number of new cases that develop each year is roughly similar around the world, with age-standardized prevalence per 100,000 ranging from 343 in Africa to 544 in Japan and Oceania for men, and from 378 in Africa to 527 in Southeastern Europe for women. About 1.1% of adults have schizophrenia in the United States. However, in areas of conflict this figure can rise to between 4.0 and 6.5%.

The history of schizophrenia is complex and does not lend itself easily to a linear narrative. Accounts of a schizophrenia-like syndrome are rare in records before the 19th century. The earliest cases detailed were reported in 1797, and 1809. "Dementia praecox", meaning premature dementia was used by German psychiatrist Heinrich Schüle in 1886, and then in 1891 by Arnold Pick in a case report of hebephrenia. In 1893 Emil Kraepelin used the term in making a distinction, known as the Kraepelinian dichotomy, between the two psychoses – dementia praecox, and manic depression (now called bipolar disorder). Kraepelin believed that "dementia praecox" was probably caused by a systemic disease that affected many organs and nerves, affecting the brain after puberty in a final decisive cascade. It was thought to be an early form of dementia, a degenerative disease. When it became evident that the disorder was not degenerative it was renamed schizophrenia by Eugen Bleuler in 1908.

The word "schizophrenia" translates roughly as "splitting of the mind" and is Modern Latin from the Greek roots "schizein" (σχίζειν, "to split") and "phrēn", (φρεν, "mind") Its use was intended to describe the separation of function between personality, thinking, memory, and perception.

The term schizophrenia used to be associated with "split personality" by the general population but that usage went into decline when "split personality" became known as a separate disorder, first as "multiple identity disorder ", and later as dissociative identity disorder. In 2002 in Japan the name was changed to "integration disorder", and in 2012 in South Korea, the name was changed to "attunement disorder" to reduce the stigma, both with good results.
In the early 20th century, the psychiatrist Kurt Schneider listed the psychotic symptoms of schizophrenia into two groups of hallucinations, and delusions. The hallucinations were listed as specific to auditory, and the delusional included thought disorders. These were seen as the symptoms of first-rank importance and were termed first-rank symptoms. Whilst these were also sometimes seen to be relevant to the psychosis in manic-depression, they were highly suggestive of schizophrenia and typically referred to as first-rank symptoms of schizophrenia. The most common first-rank symptom was found to belong to thought disorders. In 2013 the first-rank symptoms were excluded from the DSM-5 criteria. First-rank symptoms are seen to be of limited use in detecting schizophrenia but may be of help in differential diagnosis.

The earliest attempts to treat schizophrenia were psychosurgical, involving either the removal of brain tissue from different regions or the severing of pathways. These were notably frontal lobotomies and cingulotomies which were carried out from the 1930s. In the 1930s a number of shock therapies were introduced which induced seizures (convulsions) or comas. Insulin shock therapy involved the injecting of large doses of insulin in order to induce comas, which in turn produced hypoglycemia and convulsions. The use of electricity to induce seizures was developed, and in use as electroconvulsive therapy (ECT) by 1938. Stereotactic surgeries were developed in the 1940s. Treatment was revolutionized in the mid-1950s with the development and introduction of the first typical antipsychotic, chlorpromazine. In the 1970s the first atypical antipsychotic clozapine, was introduced followed by the introduction of others.

In the early 1970s in the US, the diagnostic model used for schizophrenia was broad and clinically-based using DSM II. It had been noted that schizophrenia was diagnosed far more in the US than in Europe which had been using the ICD-9 criteria. The US model was criticised for failing to demarcate clearly those people with a mental illness, and those without. In 1980 DSM III was published and showed a shift in focus from the clinically-based biopsychosocial model to a reason-based medical model. DSM IV showed an increased focus to an evidence-based medical model. 

Subtypes of schizophrenia are no longer recognized as separate conditions from schizophrenia by DSM-5 or ICD-11. Before 2013, the subtypes of schizophrenia were classified as paranoid, disorganized, catatonic, undifferentiated, and residual type. The subtypes of schizophrenia were eliminated because of a lack of clear distinction among the subtypes and low validity of classification.

In 2002, the term for schizophrenia in Japan was changed from to to reduce stigma. The new name also interpreted as "integration disorder" was inspired by the biopsychosocial model; it increased the percentage of people who were informed of the diagnosis from 37 to 70% over three years. A similar change was made in South Korea in 2012 to "attunement disorder". A professor of psychiatry, Jim van Os, has proposed changing the English term to "psychosis spectrum syndrome". In 2013 with the reviewed DSM-5, the DSM-5 committee was in favor of giving a new name to schizophrenia but they referred this to WHO.

In the United States, the cost of schizophrenia – including direct costs (outpatient, inpatient, drugs, and long-term care) and non-health care costs (law enforcement, reduced workplace productivity, and unemployment) – was estimated to be $62.7 billion in 2002. In the UK the cost in 2016 was put at £11.8 billion per year with a third of that figure directly attributable to the cost of hospital and social care, and treatment.

The book "A Beautiful Mind" chronicled the life of John Forbes Nash who had been diagnosed with schizophrenia but who went on to win the Nobel Prize for Economics. This was later made into the film with the same name. An earlier documentary was made with the title "A Brilliant Madness".

In 1964 a lengthy case study of three males diagnosed with paranoid schizophrenia who each had the delusional belief that they were Jesus Christ was published as a book. This has the title of "The Three Christs of Ypsilanti", and a film with the title "Three Christs" was released in 2020. Such religious delusions are a fairly common feature in psychoses including schizophrenia.

People with severe mental illness, including schizophrenia, are at a significantly greater risk of being victims of both violent and non-violent crime. Schizophrenia has been associated with a higher rate of violent acts, but most appear to be related to associated substance abuse. Rates of homicide linked to psychosis are similar to those linked to substance misuse, and parallel the overall rate in a region. What role schizophrenia has on violence independent of drug misuse is controversial, but certain aspects of individual histories or mental states may be factors. About 11% of people in prison for homicide have schizophrenia and 21% have mood disorders. Another study found about 8-10% of people with schizophrenia had committed a violent act in the past year compared to 2% of the general population.

Media coverage relating to violent acts by people with schizophrenia reinforces public perception of an association between schizophrenia and violence. In a large, representative sample from a 1999 study, 12.8% of Americans believed that those with schizophrenia were "very likely" to do something violent against others, and 48.1% said that they were "somewhat likely" to. Over 74% said that people with schizophrenia were either "not very able" or "not able at all" to make decisions concerning their treatment, and 70.2% said the same of money-management decisions. The perception of people with psychosis as violent more than doubled between the 1950s and 2000, according to one meta-analysis.

Schizophrenia is not believed to occur in other animals but it may be possible to develop a pharmacologically induced non-human primate model of schizophrenia.

Effects of early intervention is an active area of research. One important aspect of this research is early detection of at-risk individuals. This includes development of risk calculators and methods for large-scale population screening.

Various agents have been explored for possible effectiveness in treating negative symptoms, for which antipsychotics have been of little benefit. There have been trials on medications with anti-inflammatory activity, based on the premise that inflammation might play a role in the pathology of schizophrenia.

Research has found a tentative benefit in using minocycline, a broad-spectrum antibiotic, as an add-on treatment for schizophrenia. Reviews have found that minocycline as an add-on therapy appears to be effective in improving all dimensions of symptoms, and has been found to be safe and well tolerated, but larger studies are called for.

A review of the effects of nidotherapy – efforts to change the environment to improve functional ability was inconclusive, and it was suggested that it be treated as an experimental approach.

Various brain stimulation techniques are being studied to treat the positive symptoms of schizophrenia, in particular auditory verbal hallucinations (AVHs). A 2015 Cochrane review found unclear evidence of benefit. Most studies focus on transcranial direct-current stimulation (tDCM), and repetitive transcranial magnetic stimulation (rTMS). Techniques based on focused ultrasound for deep brain stimulation could provide insight for the treatment of AVHs.
Another active area of research is the study of a variety of potential biomarkers that would be of invaluable help not only in the diagnosis but also in the treatment and prognosis of schizophrenia. Possible biomarkers include markers of inflammation, neuroimaging, BDNF, genetics, and speech analysis. Some inflammatory markers such as C-reactive protein are useful in detecting levels of inflammation implicated in some psychiatric disorders but they are not disorder-specific. However, other inflammatory cytokines are found to be elevated in first episode psychosis and acute relapse that are normalized after treatment with antipsychotics, and these may be considered as state markers. Deficits in sleep spindles in schizophrenia may serve as a marker of an impaired thalamocortical circuit, and a mechanism for memory impairment.


</doc>
<doc id="27791" url="https://en.wikipedia.org/wiki?curid=27791" title="Sophie Germain">
Sophie Germain

Marie-Sophie Germain (; 1 April 1776 – 27 June 1831) was a French mathematician, physicist, and philosopher. Despite initial opposition from her parents and difficulties presented by society, she gained education from books in her father's library, including ones by Leonhard Euler, and from correspondence with famous mathematicians such as Lagrange, Legendre, and Gauss (under the pseudonym of «Monsieur LeBlanc»). One of the pioneers of elasticity theory, she won the grand prize from the Paris Academy of Sciences for her essay on the subject. Her work on Fermat's Last Theorem provided a foundation for mathematicians exploring the subject for hundreds of years after. Because of prejudice against her sex, she was unable to make a career out of mathematics, but she worked independently throughout her life. Before her death, Gauss had recommended that she be awarded an honorary degree, but that never occurred. On 27 June 1831, she died from breast cancer. At the centenary of her life, a street and a girls’ school were named after her. The Academy of Sciences established the Sophie Germain Prize in her honor.

Marie-Sophie Germain was born on 1 April 1776, in Paris, France, in a house on Rue Saint-Denis. According to most sources, her father, Ambroise-François, was a wealthy silk merchant, though some believe he was a goldsmith. In 1789, he was elected as a representative of the bourgeoisie to the États-Généraux, which he saw change into the Constitutional Assembly. It is therefore assumed that Sophie witnessed many discussions between her father and his friends on politics and philosophy. Gray proposes that after his political career, Ambroise-François became the director of a bank; in any case, the family remained well-off enough to support Germain throughout her adult life.

Marie-Sophie had one younger sister, named Angélique-Ambroise, and one older sister, named Marie-Madeline. Her mother was also named Marie-Madeline, and this plethora of "Maries" may have been the reason she went by Sophie. Germain's nephew Armand-Jacques Lherbette, Marie-Madeline's son, published some of Germain's work after she died (see Work in Philosophy).

When Germain was 13, the Bastille fell, and the revolutionary atmosphere of the city forced her to stay inside. For entertainment she turned to her father's library. Here she found J. E. Montucla's "L'Histoire des Mathématiques", and his story of the death of Archimedes intrigued her.

Sophie Germain thought that if the geometry method, which at that time referred to all of pure mathematics, could hold such fascination for Archimedes, it was a subject worthy of study. So she pored over every book on mathematics in her father's library, even teaching herself Latin and Greek, so she could read works like those of Sir Isaac Newton and Leonhard Euler. She also enjoyed by Étienne Bézout and by . Later, Cousin visited Germain at home, encouraging her in her studies.

Germain's parents did not at all approve of her sudden fascination with mathematics, which was then thought inappropriate for a woman. When night came, they would deny her warm clothes and a fire for her bedroom to try to keep her from studying, but after they left, she would take out candles, wrap herself in quilts and do mathematics. After some time, her mother even secretly supported her.

In 1794, when Germain was 18, the École Polytechnique opened. As a woman, Germain was barred from attending, but the new system of education made the "lecture notes available to all who asked". The new method also required the students to "submit written observations". Germain obtained the lecture notes and began sending her work to Joseph Louis Lagrange, a faculty member. She used the name of a former student Monsieur Antoine-Auguste Le Blanc, "fearing", as she later explained to Gauss, "the ridicule attached to a female scientist". When Lagrange saw the intelligence of M. Le Blanc, he requested a meeting, and thus Sophie was forced to disclose her true identity. Fortunately, Lagrange did not mind that Germain was a woman, and he became her mentor.

Germain first became interested in number theory in 1798 when Adrien-Marie Legendre published . After studying the work, she opened correspondence with him on number theory, and later, elasticity. Legendre showed some of Germain's work in the to his second edition of the , where he calls it ("very ingenious"). See also Her work on Fermat's Last Theorem below.

Germain's interest in number theory was renewed when she read Carl Friedrich Gauss' monumental work . After three years of working through the exercises and trying her own proofs for some of the theorems, she wrote, again under the pseudonym of M. Le Blanc, to the author himself, who was one year younger than she. The first letter, dated 21 November 1804, discussed Gauss' and presented some of Germain's work on Fermat's Last Theorem. In the letter, Germain claimed to have proved the theorem for "n" = "p" − 1, where "p" is a prime number of the form "p" = 8"k" + 7. However, her proof contained a weak assumption, and Gauss' reply did not comment on Germain's proof.

Around 1807 (sources differ), during the Napoleonic wars, the French were occupying the German town of Braunschweig, where Gauss lived. Germain, concerned that he might suffer the fate of Archimedes, wrote to General Pernety, a family friend, requesting that he ensure Gauss' safety. General Pernety sent a chief of a battalion to meet with Gauss personally to see that he was safe. As it turned out, Gauss was fine, but he was confused by the mention of Sophie's name.

Three months after the incident, Germain disclosed her true identity to Gauss. He replied:

How can I describe my astonishment and admiration on seeing my esteemed correspondent M. Le Blanc metamorphosed into this celebrated person ... when a woman, because of her sex, our customs and prejudices, encounters infinitely more obstacles than men in familiarising herself with [number theory's] knotty problems, yet overcomes these fetters and penetrates that which is most hidden, she doubtless has the most noble courage, extraordinary talent, and superior genius.
Gauss' letters to Olbers show that his praise for Germain was sincere. In the same 1807 letter, Germain claimed that if formula_1 is of the form formula_2, then formula_3 is also of that form. Gauss replied with a counterexample: formula_4 can be written as formula_5, but formula_6 cannot.

Although Gauss thought well of Germain, his replies to her letters were often delayed, and he generally did not review her work. Eventually his interests turned away from number theory, and in 1809 the letters ceased. Despite the friendship of Germain and Gauss, they never met.

When Germain's correspondence with Gauss ceased, she took interest in a contest sponsored by the Paris Academy of Sciences concerning Ernst Chladni's experiments with vibrating metal plates. The object of the competition, as stated by the Academy, was "to give the mathematical theory of the vibration of an elastic surface and to compare the theory to experimental evidence". Lagrange's comment that a solution to the problem would require the invention of a new branch of analysis deterred all but two contestants, Denis Poisson and Germain. Then Poisson was elected to the Academy, thus becoming a judge instead of a contestant, and leaving Germain as the only entrant to the competition.

In 1809 Germain began work. Legendre assisted by giving her equations, references, and current research. She submitted her paper early in the fall of 1811 and did not win the prize. The judging commission felt that "the true equations of the movement were not established", even though "the experiments presented ingenious results". Lagrange was able to use Germain's work to derive an equation that was "correct under special assumptions".

The contest was extended by two years, and Germain decided to try again for the prize. At first Legendre continued to offer support, but then he refused all help. Germain's anonymous 1813 submission was still littered with mathematical errors, especially involving double integrals, and it received only an honorable mention because "the fundamental base of the theory [of elastic surfaces] was not established". The contest was extended once more, and Germain began work on her third attempt. This time she consulted with Poisson. In 1814 he published his own work on elasticity and did not acknowledge Germain's help (although he had worked with her on the subject and, as a judge on the Academy commission, had had access to her work).

Germain submitted her third paper, "", under her own name, and on 8 January 1816 she became the first woman to win a prize from the Paris Academy of Sciences. She did not appear at the ceremony to receive her award. Although Germain had at last been awarded the , the Academy was still not fully satisfied. Germain had derived the correct differential equation (a special case of the Kirchhoff–Love equation), but her method did not predict experimental results with great accuracy, as she had relied on an incorrect equation from Euler, which led to incorrect boundary conditions. Here is Germain's final equation for the vibration of a plane lamina:

where "N" is a constant.

After winning the Academy contest, she was still not able to attend its sessions because of the Academy's tradition of excluding women other than the wives of members. Seven years later this situation was transformed, when she made friends with Joseph Fourier, a secretary of the Academy, who obtained tickets to the sessions for her.

Germain published her prize-winning essay at her own expense in 1821, mostly because she wanted to present her work in opposition to that of Poisson. In the essay she pointed out some of the errors in her method.

In 1826 she submitted a revised version of her 1821 essay to the Academy. According to Andrea Del Centina, the revision included attempts to clarify her work by "introducing certain simplifying hypotheses". This put the Academy in an awkward position, as they felt the paper to be "inadequate and trivial", but they did not want to "treat her as a professional colleague, as they would any man, by simply rejecting the work". So Augustin-Louis Cauchy, who had been appointed to review her work, recommended her to publish it, and she followed his advice.

One further work of Germain's on elasticity was published posthumously in 1831, her "". She used the mean curvature in her research (see Honors in number theory).

Germain's best work was in number theory, and her most significant contribution to number theory dealt with Fermat's Last Theorem. In 1815, after the elasticity contest, the Academy offered a prize for a proof of Fermat's Last Theorem. It reawakened Germain's interest in number theory, and she wrote to Gauss again after ten years of no correspondence.

In the letter, Germain said that number theory was her preferred field and that it was in her mind all the time she was studying elasticity. She outlined a strategy for a general proof of Fermat's Last Theorem, including a proof for a special case. Germain's letter to Gauss contained her substantial progress toward a proof. She asked Gauss whether her approach to the theorem was worth pursuing. Gauss never answered.

Fermat's Last Theorem can be divided into two cases. Case 1 involves all powers "p" that do not divide any of "x", "y", or "z". Case 2 includes all "p" that divide at least one of "x", "y", or "z". Germain proposed the following, commonly called "Sophie Germain's theorem":

Let "p" be an odd prime. If there exists an auxiliary prime "P" = 2"Np" + 1 ("N" is any positive integer not divisible by 3) such that:
Then the first case of Fermat's Last Theorem holds true for "p".
Germain used this result to prove the first case of Fermat's Last Theorem for all odd primes "p" < 100, but according to Andrea Del Centina, "she had actually shown that it holds for every exponent "p" < 197". L. E. Dickson later used Germain's theorem to prove Fermat's Last Theorem for odd primes less than 1700.

In an unpublished manuscript titled , Germain showed that any counterexamples to Fermat's theorem for "p" > 5 must be numbers "whose size frightens the imagination", around 40 digits long. Germain did not publish this work. Her brilliant theorem is known only because of the footnote in Legendre's treatise on number theory, where he used it to prove Fermat's Last Theorem for "p" = 5 (see Correspondence with Legendre). Germain also proved or nearly proved several results that were attributed to Lagrange or were rediscovered years later. Del Centina states that "after almost two hundred years her ideas were still central", but ultimately her method did not work.

In addition to mathematics, Germain studied philosophy and psychology. She wanted to classify facts and generalize them into laws that could form a system of psychology and sociology, which were then just coming into existence. Her philosophy was highly praised by Auguste Comte.

Two of her philosophical works, and , were published, both posthumously. This was due in part to the efforts of Lherbette, her nephew, who collected her philosophical writings and published them. is a history of science and mathematics with Germain's commentary. In , the work admired by Comte, Germain argues that there are no differences between the sciences and the humanities.

In 1829 Germain learned that she had breast cancer. Despite the pain, she continued to work. In 1831 "Crelle's Journal" published her paper on the curvature of elastic surfaces and "a note about finding and in formula_8". Mary Gray records: "She also published in an examination of principles which led to the discovery of the laws of equilibrium and movement of elastic solids." On 27 June 1831, she died in the house at 13 rue de Savoie.

Despite Germain's intellectual achievements, her death certificate lists her as a "" (property holder), not a "". But her work was not unappreciated by everyone. When the matter of honorary degrees came up at the University of Göttingen in 1837—six years after Germain's death—Gauss lamented: "she [Germain] proved to the world that even a woman can accomplish something worthwhile in the most rigorous and abstract of the sciences and for that reason would well have deserved an honorary degree".

Germain's resting place in the Père Lachaise Cemetery in Paris is marked by a gravestone. At the centennial celebration of her life, a street and a girls' school were named after her, and a plaque was placed at the house where she died. The school houses a bust commissioned by the Paris City Council.

In January 2020, Satellogic, a high-resolution Earth observation imaging and analytics company, launched a ÑuSat type micro-satellite named in honor of Sophie Germain.

E. Dubouis defined a "sophien" of a prime to be a prime where , for such that yield such that has no solutions when and are prime to .

A Sophie Germain prime is a prime such that is also prime.

The Germain curvature (also called mean curvature) is formula_9, where and are the maximum and minimum values of the normal curvature.

Sophie Germain's identity states that for any },

Vesna Petrovich found that the educated world's response to the publication in 1821 of Germain's prize-winning essay "ranged from polite to indifferent". Yet, some critics had high praise for it. Of her essay in 1821, Cauchy said: "[it] was a work for which the name of its author and the importance of the subject both deserved the attention of mathematicians". Germain was also included in H. J. Mozans' book "Woman in Science", although Marilyn Bailey Ogilvie claims that the biography "is inaccurate and the notes and bibliography are unreliable". Nevertheless, it quotes the mathematician Claude-Louis Navier as saying that "it is a work which few men are able to read and which only one woman was able to write".

Germain's contemporaries also had good things to say relating to her work in mathematics. Gauss certainly thought highly of her and recognized that European culture presented special difficulties to a woman in mathematics (see Correspondence with Gauss).

The modern view generally acknowledges that although Germain had great talent as a mathematician, her haphazard education had left her without the strong base she needed to truly excel. As explained by Gray, "Germain's work in elasticity suffered generally from an absence of rigor, which might be attributed to her lack of formal training in the rudiments of analysis." Petrovich adds: "This proved to be a major handicap when she could no longer be regarded as a young prodigy to be admired but was judged by her peer mathematicians."

Notwithstanding the problems with Germain's theory of vibrations, Gray states that "Germain's work was fundamental in the development of a general theory of elasticity." Mozans writes, however, that when the Eiffel tower was built and the architects inscribed the names of 72 great French scientists, Germain's name was not among them, despite the salience of her work to the tower's construction. Mozans asked: "Was she excluded from this list ... because she was a woman? It would seem so."

Concerning her early work in number theory, J. H. Sampson states: "She was clever with formal algebraic manipulations; but there is little evidence that she really understood the , and her work of that period that has come down to us seems to touch only on rather superficial matters." Gray adds that "The inclination of sympathetic mathematicians to praise her work rather than to provide substantive criticism from which she might learn was crippling to her mathematical development." Yet Marilyn Bailey Ogilvie recognizes that "Sophie Germain's creativity manifested itself in pure and applied mathematics ... [she] provided imaginative and provocative solutions to several important problems", and, as Petrovich proposes, it may have been her very lack of training that gave her unique insights and approaches. Louis Bucciarelli and Nancy Dworsky, Germain's biographers, summarize as follows: "All the evidence argues that Sophie Germain had a mathematical brilliance that never reached fruition due to a lack of rigorous training available only to men."

Germain was referenced and quoted in David Auburn's 2001 play "Proof." The protagonist is a young struggling female mathematician, Catherine, who found great inspiration in the work of Germain. Germain was also mentioned in John Madden's film adaptation of the same name in a conversation between Catherine (Gwyneth Paltrow) and Hal (Jake Gyllenhaal).

In the fictional work "The Last Theorem" by Arthur C. Clarke and Frederik Pohl, Sophie Germain was credited with inspiring the central character, Ranjit Subramanian, to solve Fermat's Last Theorem.

A new musical about Sophie Germain's life, entitled The Limit, premiered at VAULT Festival in London, 2019.

The Sophie Germain Prize (), awarded annually by the Foundation Sophie Germain, is conferred by the Academy of Sciences in Paris. Its purpose is to honour a French mathematician for research in the foundations of mathematics. This award, in the amount of €8,000, was established in 2003, under the auspices of the Institut de France.





</doc>
<doc id="27793" url="https://en.wikipedia.org/wiki?curid=27793" title="Shoa">
Shoa

Shoa may refer to:




</doc>
<doc id="27796" url="https://en.wikipedia.org/wiki?curid=27796" title="Succubus">
Succubus

A succubus is a demon or supernatural entity in folklore, in female form, that appears in dreams to seduce men, usually through sexual activity. According to religious traditions, repeated sexual activity with a succubus can cause poor physical or mental health, even death.

In modern representations, a succubus is often depicted as a beautiful seductress or enchantress, rather than as demonic or frightening.

The male counterpart to the succubus is the incubus.

The word is derived from Late Latin "succuba" "paramour"; from "succubare" "to lie beneath" ("sub-" "under" and "cubare" "to lie"), used to describe this female supernatural being's implied sexual position relative to the male sleeper's position. The word "succubus" originates from the late 14th century.

As depicted in the Jewish mystical work Zohar and the medieval rabbinical text Alphabet of Ben Sira, Lilith was Adam's first wife, who later became a succubus. She left Adam and refused to return to the Garden of Eden after she mated with the archangel Samael. In Zoharistic Kabbalah, there were four succubi who mated with the archangel Samael. There were four original queens of the demons: Lilith, Eisheth, Agrat bat Mahlat, and Naamah. A succubus may take a form of a beautiful young girl but closer inspection may reveal deformities of her body, such as bird-like claws or serpentine tails. Folklore also describes the act of sexually penetrating a succubus as akin to entering a cavern of ice, and there are reports of succubi forcing men to perform cunnilingus on their vulvas, which drip with urine and other fluids. In later folklore, a succubus took the form of a siren.

Throughout history, priests and rabbis, including Hanina Ben Dosa and Abaye, tried to curb the power of succubi over humans. However, not all succubi were malevolent. According to Walter Map in the satire "De Nugis Curialium" ("Trifles of Courtiers"), Pope Sylvester II (999–1003) was allegedly involved with a succubus named Meridiana, who helped him achieve his high rank in the Catholic Church. Before his death, he confessed of his sins and died repentant.

According to the Kabbalah and the school of Rashba, the original three queens of the demons, Agrat Bat Mahlat, Naamah, Eisheth Zenunim, and all their cohorts give birth to children, except Lilith. According to other legends, the children of Lilith are called Lilin.

According to the "Malleus Maleficarum", or "Witches' Hammer", written by Heinrich Kramer (Institoris) in 1486, succubi collect semen from men they seduce. Incubi, or male demons, then use the semen to impregnate human females, thus explaining how demons could apparently sire children despite the traditional belief that they were incapable of reproduction. Children so begotten—cambions—were supposed to be those that were born deformed, or more susceptible to supernatural influences. While the book does not address why a human female impregnated with the semen of a human male would not produce regular human offspring, an explanation could be that the semen is altered before being transferred to the female host. However in some lore, the child is born deformed because the conception was unnatural.

King James in his dissertation titled Dæmonologie refutes the possibility for angelic entities to reproduce and instead offered a suggestion that a devil would carry out two methods of impregnating women: the first, to steal the sperm out of a dead man and deliver it into a woman. If a demon could extract the semen quickly, the substance could not be instantly transported to a female host, causing it to go cold. This explains his view that succubi and incubi were the same demonic entity only to be described differently based on the tormented sexes being conversed with. The second method was the idea that a dead body could be possessed by a devil, causing it to rise and have sexual relations with others. However, there is no mention of a female corpse being possessed to elicit sex from men.

In Arabian mythology, the "qarînah" () is a spirit similar to the succubus, with origins possibly in ancient Egyptian religion or in the animistic beliefs of pre-Islamic Arabia. A qarînah "sleeps with the person and has relations during sleep as is known by the dreams". They are said to be invisible, but a person with "second sight" can see them, often in the form of a cat, dog, or other household pet. "In Omdurman it is a spirit which possesses. ... Only certain people are possessed and such people cannot marry or the qarina will harm them." To date, many African myths claim that men who have similar experience with such principality (succubus) in dreams (usually in form of a beautiful woman) find themselves exhausted as soon as they awaken; often claiming spiritual attack upon them. Local rituals/divination are often invoked in order to appeal the god for divine protection and intervention.

In the field of medicine, there is some belief that the stories relating to encounters with succubi bear resemblance to the contemporary phenomenon of people reporting alien abductions, which has been ascribed to the condition known as sleep paralysis. It is therefore suggested that historical accounts of people experiencing encounters with succubi may rather have been symptoms of sleep paralysis, with the hallucination of the said creatures coming from their contemporary culture. Furthermore, the experience of nocturnal emissions or "wet dreams" may explain the sexual aspect of the phenomenon.

Throughout history, succubi have been popular characters in music, literature, film, television, and more.



</doc>
<doc id="27797" url="https://en.wikipedia.org/wiki?curid=27797" title="Suzanne Vega">
Suzanne Vega

Suzanne Nadine Vega (born July 11, 1959) is an American singer-songwriter, musician and record producer, best known for her folk-inspired music.

Vega's music career spans more than 30 years. She came to prominence in the mid 1980s, releasing four singles that entered the Top 40 charts in the UK during the 1980s and 1990s, including "Marlene on the Wall", "Left of Center", "Luka" and "No Cheap Thrill". "Tom's Diner," which was originally released as an a cappella recording on Vega's second album, "Solitude Standing" (1987), was remixed in 1990 as a dance track by English electronic duo DNA with Vega as featured artist, and it became a Top 10 hit in over five countries. The song was used as a test during the creation of the MP3 format. The critical role of her song in the development of the MP3 compression prompted Vega to be given the title of "The Mother of the MP3".

Vega has released nine studio albums to date, the latest of which is "", released in 2016.

Suzanne Nadine Vega was born on July 11, 1959, in Santa Monica, California. Her mother, Pat Vega (née Schumacher), is a computer systems analyst of German-Swedish heritage. Her father, Richard Peck, is of Scottish-English-Irish origin. They divorced soon after her birth. Her stepfather, Edgardo Vega Yunqué, also known as Ed Vega, was a writer and teacher from Puerto Rico. When Vega was two and a half, her family moved to New York City. She grew up in Spanish Harlem and the Upper West Side.

She was not aware of having a different biological father, Richard Peck, until she was nine years old. They met for the first time in her late 20s, and they remain in contact.

She attended the High School of Performing Arts, now renamed Fiorello H. LaGuardia High School, where she studied modern dance and graduated in 1977.

While majoring in English literature at Barnard College, she performed in small venues in Greenwich Village, where she was a regular contributor to Jack Hardy's Monday night songwriters' group at the Cornelia Street Cafe and had some of her first songs published on "Fast Folk" anthology albums. In 1984, she received a major label recording contract, making her one of the first "Fast Folk" artists to break out on a major label.

Vega's self-titled debut album was released in 1985 and was well received by critics in the U.S.; it reached platinum status in the United Kingdom. Produced by Lenny Kaye and Steve Addabbo, the songs feature Vega's acoustic guitar in straightforward arrangements. A video was released for the album's song "Marlene on the Wall", which went into MTV and VH1's rotations. During this period Vega also wrote lyrics for two songs ("Lightning" and "Freezing") on "Songs from Liquid Days" by composer Philip Glass.

Vega's song "Left of Center" co-written with Steve Addabbo for the 1986 John Hughes film "Pretty in Pink" reached No. 32 on the UK Singles Chart in 1986.

Her next effort, "Solitude Standing" (1987), garnered critical and commercial success, selling over one million copies in the U.S. It includes the international hit single "Luka", which is written about, and from the point of view of, an abused child—at the time an uncommon subject for a pop hit. While continuing a focus on Vega's acoustic guitar, the music is more strongly pop-oriented and features fuller arrangements. Following the success of the album, in 1989 Vega became the first female artist to headline the Glastonbury Festival. In addition, the performance was notable for Vega performing her set whilst wearing a bulletproof vest, her band having received death threats from an obsessed fan ahead of the festival.

The acappella "Tom's Diner" from "Solitude Standing" became a hit in 1990, having been remixed by two British dance producers under the name DNA. The track was originally a bootleg, until Vega allowed DNA to release it through her record company, and it became her biggest hit.

Vega's third album, "Days of Open Hand" (1990), continued in the style of her first two albums.

In 1992, she released the album "99.9F°". It consists of a mixture of folk music, dance beats and industrial music. This record was awarded Gold status by the RIAA in recognition of selling over 500,000 copies in the U.S. The single "Blood Makes Noise" from this album peaked at number-one on Billboard's Modern Rock Tracks. Vega later married the album's producer Mitchell Froom.

Her fifth album, "Nine Objects of Desire", was released in 1996. The music varies between a frugal, simple style and the industrial production of "99.9F°". This album contains "Caramel", featured in the movie "The Truth About Cats & Dogs", and later the trailer for the movie "Closer". A song not included on that album, "Woman on the Tier," was featured on the soundtrack of the movie "Dead Man Walking".

In 1997 she took a singing part on the concept album "Heaven and Hell", a musical interpretation of the seven deadly sins by her colleague Joe Jackson, with whom she had already collaborated in 1986 on "Left of Center" from the "Pretty in Pink" soundtrack (with Vega singing and Jackson playing piano).

In 1999, Avon Books published Vega's book "The Passionate Eye: The Collected Writings of Suzanne Vega", a volume of poems, lyrics, essays and journalistic pieces.

In September 2001, Vega released a new album entitled "Songs in Red and Gray". Three songs deal with Vega's divorce from her first husband, Mitchell Froom.

At the memorial concert for her brother Tim Vega in December 2002, Vega began her role as the subject of the direct-cinema documentary, "Some Journey", directed by Christopher Seufert of Mooncusser Films. The documentary has not been completed.

Underground hip hop duo Felt named a track on their album "", released in 2002, "Suzanne Vega".

In 2003, the 21-song greatest hits compilation "Retrospective: The Best of Suzanne Vega" was released. (The UK version of "Retrospective" included an eight-song bonus CD as well as a DVD containing 12 songs). In the same year she was invited by Grammy Award-winning jazz guitarist Bill Frisell to play at the "Century of Song" concerts at the famed "Ruhrtriennale" in Germany.

In 2003, she hosted the American Public Media radio series "American Mavericks", about 20th century American composers, which received the Peabody Award for Excellence in Broadcasting.

On August 3, 2006, Vega became the first major recording artist to perform live in the Internet-based virtual world, "Second Life". The event was hosted by John Hockenberry of public radio's "The Infinite Mind".

On September 17, 2006, she performed in Central Park, as part of a benefit concert for the Save Darfur Coalition. During the concert she highlighted her support for Amnesty International, of which she has been a member since 1988.
In early October 2006, Vega participated in the Academia Film Olomouc (AFO) in Olomouc, the Czech Republic, the oldest festival of documentary films in Europe, in which she appeared as a main guest. She was invited there as the subject of the documentary film by director Christopher Seufert, that had a test screening at the festival. At the end of the festival she performed her classic songs and added one brand new piece called "New York Is a Woman".

Vega is also interviewed in the book "Everything Is Just a Bet" which was published in Czech in October 2006. The book contains 12 interview transcriptions from the talk show called "Stage Talks" that regularly runs in the Švandovo divadlo (Švandovo Theatre) in Prague. Vega introduced the book to the audience of the Švandovo divadlo (Švandovo Theatre), and together with some other Czech celebrities gave a signing session.

She signed a new recording contract with Blue Note Records in the spring of 2006, and released "Beauty & Crime" on July 17, 2007. The album, produced by Jimmy Hogarth, won a Grammy Award for Best Engineered Album, Non-Classical. Her contract was not renewed and she was released in June 2008.

In 2007, Vega followed the lead of numerous other mainstream artists and released her track "Pornographer's Dream" as podsafe. The song spent two weeks at number-one during 2007 and finished as the No. 11 hit of the year on the PMC Top10's annual countdown. In 2015, Vega joined The 14th Annual Independent Music Awards judging panel to assist independent musicians' careers.

A partial cover version of her song "Tom's Diner" was used to introduce the 2010 British movie "4.3.2.1", with its lyrics largely rewritten to echo the plot. This musical hybrid was released as "Keep Moving". Vega participated in the Danger Mouse/Sparklehorse/David Lynch collaboration "Dark Night of the Soul". She wrote both melody and lyrics for her song, which is titled "The Man Who Played God", inspired by a biography of Pablo Picasso. Vega sang lead vocals on the song "Now I Am an Arsonist" with singer-songwriter Jonathan Coulton on his 2011 album, "Artificial Heart".

Vega has re-recorded her back-catalogue, both for artistic and commercial (and control) reasons, in the "Close-up" series. Vol. 1 ("Love Songs") and Vol. 2 ("People & Places") appeared in 2010 while Vol. 3 ("States of Being") was released in July 2011 followed by Vol. 4 ("Songs of Family") in September 2012. Volumes 2, 3 and 4 of the "Close-Up" albums included previously unrecorded material; Volumes 2 and 3 each included one new collaboratively written song, while Volume 4 included three songs that Vega had written years earlier, but had not previously gotten around to recording. In all, Vega's "Close-Up" series features 60 re-recorded songs and five new compositions, representing about three-quarters of her lifetime songwriting output.

While performing live, Vega and long-term collaborator Gerry Leonard began to introduce a number of new songs into the setlist, including the live favorite "I Never Wear White". Over the course of a year, the songs were completed and recorded in a live-studio setting with the help of a number of guests. Produced by Leonard, "Tales from the Realm of the Queen of Pentacles" was released in February 2014. It was her first album of new material in seven years and became Vega's first studio album to reach the UK Top 40 since 1992, peaking at No. 37.

New album "" was released on October 14, 2016.

On June 25, 2019, "The New York Times Magazine" listed Suzanne Vega among hundreds of artists whose material was reportedly destroyed in the 2008 Universal fire.

At the age of nine she began to write poetry. She was encouraged to do so by her stepfather. It took her three years to write her first song, "Brother Mine", which was finished at the age of 14. It was first published on "Close-Up Vol. 4, Songs of Family", along with her other early song, "The Silver Lady".

Vega has not learned to read musical notes; she sees the melody as a shape and chords as colors. She focuses on lyrics and melodic ideas; for advanced features – like intros or bridges – she relies on other artists she works with. Most of her albums, except the first one, were made in such cooperation.

Vega finishes 80% of the songs she starts writing.

The most important artistic influences on her work come from Lou Reed, Bob Dylan and Leonard Cohen. Some other important artists for her are Paul Simon and Laura Nyro.

Vega and Duncan Sheik wrote a play "Carson McCullers Talks About Love", about the life of the writer Carson McCullers. In the play directed by Kay Matschullat, which premiered in 2011, Vega alternates between monologue and songs. Vega and Sheik were nominated for Outstanding Music in a Play for the 57th annual Drama Desk awards.

The album "", based on this play, was released in 2016. Vega considers it to be a third version, because it's rewritten, and she made the first version in college.

In early 2020, Vega played the role of "Band Leader" in an off-Broadway musical based on the 1969 movie "Bob & Carol & Ted & Alice", directed by Scott Elliott and produced at The New Group in New York City. She replaced Sheik, who wrote the show's music and co-wrote the lyrics with Amanda Green. In his review for "The New York Times", critic Ben Brantley called the "brandy-voiced" Vega "a delightful, smoothly sardonic presence.

Vega has established her own recording label after the 2008 economic crisis. From that point, she stopped working for Blue Note Records and started thinking about re-recording her back catalog with new arrangements and gaining control over her works (which she eventually did with the "Close-Up Series").

The name "Amanuensis Productions" was meant as a private joke about "servant" (amanuensis) owning the "masters" (recording masters), also a pun at A&M still legally owning her previous master tapes.

Running the label proved to be harder than she expected. In 2015 it just "broke even", but new licenses were coming for "Tom's Diner".

On March 17, 1995, Vega married Mitchell Froom, a musician and a record producer (who played on and produced "99.9F°" and "Nine Objects of Desire"). They have a daughter, Ruby Froom (born July 8, 1994). The band Soul Coughing's "Ruby Vroom" album was named for her, with Vega's approval. Vega and Froom separated and divorced in 1998.

On February 11, 2006, Vega married Paul Mills, a lawyer and poet, "22 years after he first proposed to her."

Beginning in 2010, Ruby has occasionally performed with her mother on tour.

Vega practices Nichiren Buddhism and is a member of the American branch of the worldwide Buddhist association Soka Gakkai International.

! Year !! Awards !! Work !! Category !! Result

Studio albums

Books



</doc>
<doc id="27799" url="https://en.wikipedia.org/wiki?curid=27799" title="Semigroup">
Semigroup

In mathematics, a semigroup is an algebraic structure consisting of a set together with an associative binary operation.

The binary operation of a semigroup is most often denoted multiplicatively: "x"·"y", or simply "xy", denotes the result of applying the semigroup operation to the ordered pair . Associativity is formally expressed as that for all "x", "y" and "z" in the semigroup.

Semigroups may be considered a special case of magmas, where the operation is associative, or as a generalization of groups, without requiring the existence of an identity element or inverses. As in the case of groups or magmas, the semigroup operation need not be commutative, so "x"·"y" is not necessarily equal to "y"·"x"; a well-known example of an operation that is associative but non-commutative is matrix multiplication. If the semigroup operation is commutative, then the semigroup is called a "commutative semigroup" or (less often than in the analogous case of groups) it may be called an "abelian semigroup".

A monoid is an algebraic structure intermediate between groups and semigroups, and is a semigroup having an identity element, thus obeying all but one of the axioms of a group; existence of inverses is not required of a monoid. A natural example is strings with concatenation as the binary operation, and the empty string as the identity element. Restricting to non-empty strings gives an example of a semigroup that is not a monoid. Positive integers with addition form a commutative semigroup that is not a monoid, whereas the non-negative integers do form a monoid. A semigroup without an identity element can be easily turned into a monoid by just adding an identity element. Consequently, monoids are studied in the theory of semigroups rather than in group theory. Semigroups should not be confused with quasigroups, which are a generalization of groups in a different direction; the operation in a quasigroup need not be associative but quasigroups preserve from groups a notion of division. Division in semigroups (or in monoids) is not possible in general.

The formal study of semigroups began in the early 20th century. Early results include a Cayley theorem for semigroups realizing any semigroup as transformation semigroup, in which arbitrary functions replace the role of bijections from group theory. A deep result in the classification of finite semigroups is Krohn–Rhodes theory, analogous to the Jordan–Hölder decomposition for finite groups. Some other techniques for studying semigroups, like Green's relations, do not resemble anything in group theory.

The theory of finite semigroups has been of particular importance in theoretical computer science since the 1950s because of the natural link between finite semigroups and finite automata via the syntactic monoid. In probability theory, semigroups are associated with Markov processes. In other areas of applied mathematics, semigroups are fundamental models for linear time-invariant systems. In partial differential equations, a semigroup is associated to any equation whose spatial evolution is independent of time.

There are numerous special classes of semigroups, semigroups with additional properties, which appear in particular applications. Some of these classes are even closer to groups by exhibiting some additional but not all properties of a group. Of these we mention: regular semigroups, orthodox semigroups, semigroups with involution, inverse semigroups and cancellative semigroups. There are also interesting classes of semigroups that do not contain any groups except the trivial group; examples of the latter kind are bands and their commutative subclass—semilattices, which are also s.

A semigroup is a set formula_1 together with a binary operation "formula_2" (that is, a function formula_3) that satisfies the associative property:

More succinctly, a semigroup is an associative magma.


A left identity of a semigroup formula_1 (or more generally, magma) is an element formula_7 such that for all formula_8 in formula_1, formula_10. Similarly, a right identity is an element formula_11 such that for all formula_8 in formula_1, formula_14. Left and right identities are both called one-sided identities. A semigroup may have one or more left identities but no right identity, and vice versa.

A two-sided identity (or just identity) is an element that is both a left and right identity. Semigroups with a two-sided identity are called monoids. A semigroup may have at most one two-sided identity. If a semigroup has a two-sided identity, then the two-sided identity is the only one-sided identity in the semigroup. If a semigroup has both a left identity and a right identity, then it has a two-sided identity (which is therefore the unique one-sided identity).

A semigroup formula_1 without identity may be embedded in a monoid formed by adjoining an element formula_16 to formula_1 and defining formula_18 for all formula_19. The notation formula_20 denotes a monoid obtained from formula_1 by adjoining an identity "if necessary" (formula_22 for a monoid).

Similarly, every magma has at most one absorbing element, which in semigroup theory is called a zero. Analogous to the above construction, for every semigroup formula_1, one can define formula_24, a semigroup with 0 that embeds formula_1.

The semigroup operation induces an operation on the collection of its subsets: given subsets "A" and "B" of a semigroup "S", their product , written commonly as "AB", is the set (This notion is defined identically as it is for groups.) In terms of this operation, a subset "A" is called

If "A" is both a left ideal and a right ideal then it is called an ideal (or a two-sided ideal).

If "S" is a semigroup, then the intersection of any collection of subsemigroups of "S" is also a subsemigroup of "S".
So the subsemigroups of "S" form a complete lattice.

An example of a semigroup with no minimal ideal is the set of positive integers under addition. The minimal ideal of a commutative semigroup, when it exists, is a group.

Green's relations, a set of five equivalence relations that characterise the elements in terms of the principal ideals they generate, are important tools for analysing the ideals of a semigroup and related notions of structure.

The subset with the property that every element commutes with any other element of the semigroup is called the center of the semigroup. The center of a semigroup is actually a subsemigroup.

A semigroup homomorphism is a function that preserves semigroup structure. A function between two semigroups is a homomorphism if the equation
holds for all elements "a", "b" in "S", i.e. the result is the same when performing the semigroup operation after or before applying the map "f".

A semigroup homomorphism between monoids preserves identity if it is a monoid homomorphism. But there are semigroup homomorphisms which are not monoid homomorphisms, e.g. the canonical embedding of a semigroup formula_1 without identity into formula_20. Conditions characterizing monoid homomorphisms are discussed further. Let formula_28 be a semigroup homomorphism. The image of formula_11 is also a semigroup. If formula_30 is a monoid with an identity element formula_31, then formula_32 is the identity element in the image of formula_11. If formula_34 is also a monoid with an identity element formula_35 and formula_35 belongs to the image of formula_11, then formula_38, i.e. formula_11 is a monoid homomorphism. Particularly, if formula_11 is surjective, then it is a monoid homomorphism.

Two semigroups "S" and "T" are said to be isomorphic if there is a bijection with the property that, for any elements "a", "b" in "S", . Isomorphic semigroups have the same structure.

A semigroup congruence formula_41 is an equivalence relation that is compatible with the semigroup operation. That is, a subset formula_42 that is an equivalence relation and formula_43 and formula_44 implies formula_45 for every formula_46 in "S". Like any equivalence relation, a semigroup congruence formula_41 induces congruence classes

and the semigroup operation induces a binary operation formula_49 on the congruence classes:

Because formula_41 is a congruence, the set of all congruence classes of formula_41 forms a semigroup with formula_49, called the quotient semigroup or factor semigroup, and denoted formula_54. The mapping formula_55 is a semigroup homomorphism, called the quotient map, canonical surjection or projection; if S is a monoid then quotient semigroup is a monoid with identity formula_56. Conversely, the kernel of any semigroup homomorphism is a semigroup congruence. These results are nothing more than a particularization of the first isomorphism theorem in universal algebra. Congruence classes and factor monoids are the objects of study in string rewriting systems.

A nuclear congruence on "S" is one which is the kernel of an endomorphism of "S".

A semigroup "S" satisfies the maximal condition on congruences if any family of congruences on "S", ordered by inclusion, has a maximal element. By Zorn's lemma, this is equivalent to saying that the ascending chain condition holds: there is no infinite strictly ascending chain of congruences on "S".

Every ideal "I" of a semigroup induces a subsemigroup, the Rees factor semigroup via the congruence   ⇔   either or both "x" and "y" are in "I".

The following notions introduce the idea that a semigroup is contained in another one.

A semigroup T is a quotient of a semigroup S if there is a surjective semigroup morphism from S to T. For example, formula_57 is a quotient of formula_58, using the morphism consisting of taking the remainder modulo 2 of an integer.

A semigroup T divides a semigroup S, noted formula_59 if T is a quotient of a subsemigroup S. In particular, subsemigroups of S divides T, while it is not necessarily the case that there are a quotient of S.

Both of those relation are transitive.

For any subset "A" of "S" there is a smallest subsemigroup "T" of "S" which contains "A", and we say that "A" generates "T". A single element "x" of "S" generates the subsemigroup { "x" | "n" ∈ Z }. If this is finite, then "x" is said to be of finite order, otherwise it is of infinite order.
A semigroup is said to be periodic if all of its elements are of finite order.
A semigroup generated by a single element is said to be monogenic (or cyclic). If a monogenic semigroup is infinite then it is isomorphic to the semigroup of positive integers with the operation of addition.
If it is finite and nonempty, then it must contain at least one idempotent.
It follows that every nonempty periodic semigroup has at least one idempotent.

A subsemigroup which is also a group is called a subgroup. There is a close relationship between the subgroups of a semigroup and its idempotents. Each subgroup contains exactly one idempotent, namely the identity element of the subgroup. For each idempotent "e" of the semigroup there is a unique maximal subgroup containing "e". Each maximal subgroup arises in this way, so there is a one-to-one correspondence between idempotents and maximal subgroups. Here the term "maximal subgroup" differs from its standard use in group theory.

More can often be said when the order is finite. For example, every nonempty finite semigroup is periodic, and has a minimal ideal and at least one idempotent. The number of finite semigroups of a given size (greater than 1) is (obviously) larger than the number of groups of the same size. For example, of the sixteen possible "multiplication tables" for a set of two elements eight form semigroups whereas only four of these are monoids and only two form groups. For more on the structure of finite semigroups, see Krohn–Rhodes theory.


There is a structure theorem for commutative semigroups in terms of semilattices. A semilattice (or more precisely a meet-semilattice) formula_60 is a partially ordered set where every pair of elements formula_61 has a greatest lower bound, denoted formula_62. The operation formula_63 makes formula_64 into a semigroup satisfying the additional idempotence law formula_65.

Given a homomorphism formula_66 from an arbitrary semigroup to a semilattice, each inverse image formula_67 is a (possibly empty) semigroup. Moreover, formula_68 becomes graded by formula_64, in the sense that

formula_70

If formula_71 is onto, the semilattice formula_64 is isomorphic to the quotient of formula_1 by the equivalence relation formula_74 such that formula_75 iff formula_76. This equivalence relation is a semigroup congruence, as defined above.

Whenever we take the quotient of a commutative semigroup by a congruence, we get another commutative semigroup. The structure theorem says that for any commutative semigroup formula_1, there is a finest congruence formula_74 such that the quotient of formula_68 by this equivalence relation is a semilattice. Denoting this semilattice by formula_80, we get a homomorphism formula_71 from formula_1 onto formula_80. As mentioned, formula_68 becomes graded by this semilattice.

Furthermore, the components formula_85 are all Archimedean semigroups. An Archimedean semigroup is one where given any pair of elements formula_86, there exists an element formula_87 and formula_88 such that formula_89.

The Archimedean property follows immediately from the ordering in the semilattice formula_64, since with this ordering we have formula_91 if and only if formula_89 for some formula_87 and formula_88.

The group of fractions or group completion of a semigroup "S" is the group generated by the elements of "S" as generators and all equations which hold true in "S" as relations. There is an obvious semigroup homomorphism which sends each element of "S" to the corresponding generator. This has a universal property for morphisms from "S" to a group: given any group "H" and any semigroup homomorphism , there exists a unique group homomorphism with "k"="fj". We may think of "G" as the "most general" group that contains a homomorphic image of "S".

An important question is to characterize those semigroups for which this map is an embedding. This need not always be the case: for example, take "S" to be the semigroup of subsets of some set "X" with set-theoretic intersection as the binary operation (this is an example of a semilattice). Since holds for all elements of "S", this must be true for all generators of "G"("S") as well: which is therefore the trivial group. It is clearly necessary for embeddability that "S" have the cancellation property. When "S" is commutative this condition is also sufficient and the Grothendieck group of the semigroup provides a construction of the group of fractions. The problem for non-commutative semigroups can be traced to the first substantial paper on semigroups. Anatoly Maltsev gave necessary and sufficient conditions for embeddability in 1937.

Semigroup theory can be used to study some problems in the field of partial differential equations. Roughly speaking, the semigroup approach is to regard a time-dependent partial differential equation as an ordinary differential equation on a function space. For example, consider the following initial/boundary value problem for the heat equation on the spatial interval and times :

Let be the "L" space of square-integrable real-valued functions with domain the interval and let "A" be the second-derivative operator with domain

where "H" is a Sobolev space. Then the above initial/boundary value problem can be interpreted as an initial value problem for an ordinary differential equation on the space "X":

On an heuristic level, the solution to this problem "ought" to be . However, for a rigorous treatment, a meaning must be given to the exponential of "tA". As a function of "t", exp("tA") is a semigroup of operators from "X" to itself, taking the initial state "u" at time to the state at time "t". The operator "A" is said to be the infinitesimal generator of the semigroup.

The study of semigroups trailed behind that of other algebraic structures with more complex axioms such as groups or rings. A number of sources attribute the first use of the term (in French) to J.-A. de Séguier in "Élements de la Théorie des Groupes Abstraits" (Elements of the Theory of Abstract Groups) in 1904. The term is used in English in 1908 in Harold Hinton's "Theory of Groups of Finite Order".

Anton Sushkevich obtained the first non-trivial results about semigroups. His 1928 paper "Über die endlichen Gruppen ohne das Gesetz der eindeutigen Umkehrbarkeit" ("On finite groups without the rule of unique invertibility") determined the structure of finite simple semigroups and showed that the minimal ideal (or Green's relations J-class) of a finite semigroup is simple. From that point on, the foundations of semigroup theory were further laid by David Rees, James Alexander Green, Evgenii Sergeevich Lyapin, Alfred H. Clifford and Gordon Preston. The latter two published a two-volume monograph on semigroup theory in 1961 and 1967 respectively. In 1970, a new periodical called "Semigroup Forum" (currently edited by Springer Verlag) became one of the few mathematical journals devoted entirely to semigroup theory.

The representation theory of semigroups was developed in 1963 by Boris Schein using binary relations on a set "A" and composition of relations for the semigroup product. At an algebraic conference in 1972 Schein surveyed the literature on B, the semigroup of relations on "A". In 1997 Schein and Ralph McKenzie proved that every semigroup is isomorphic to a transitive semigroup of binary relations.

In recent years researchers in the field have become more specialized with dedicated monographs appearing on important classes of semigroups, like inverse semigroups, as well as monographs focusing on applications in algebraic automata theory, particularly for finite automata, and also in functional analysis.

If the associativity axiom of a semigroup is dropped, the result is a magma, which is nothing more than a set "M" equipped with a binary operation that is closed .

Generalizing in a different direction, an n"-ary semigroup (also n"-semigroup, polyadic semigroup or multiary semigroup) is a generalization of a semigroup to a set "G" with a "n"-ary operation instead of a binary operation. The associative law is generalized as follows: ternary associativity is , i.e. the string "abcde" with any three adjacent elements bracketed. "N"-ary associativity is a string of length with any "n" adjacent elements bracketed. A 2-ary semigroup is just a semigroup. Further axioms lead to an "n"-ary group.

A third generalization is the semigroupoid, in which the requirement that the binary relation be total is lifted. As categories generalize monoids in the same way, a semigroupoid behaves much like a category but lacks identities.

Infinitary generalizations of commutative semigroups have sometimes been considered by various authors.





</doc>
<doc id="27801" url="https://en.wikipedia.org/wiki?curid=27801" title="Super Mario Kart">
Super Mario Kart

Super Mario Kart is a 1992 kart racing video game developed and published by Nintendo for the Super Nintendo Entertainment System video game console. The first game of the "Mario Kart" series, it was released in Japan and North America in 1992, and in Europe the following year. Selling 8.76 million copies worldwide, the game went on to become the fourth best selling SNES game of all time. "Super Mario Kart" was re-released on the Wii's Virtual Console in 2009, and on the Wii U's Virtual Console in 2013. Nintendo re-released "Super Mario Kart" in the United States in September 2017 as part of the company's Super NES Classic Edition.

In "Super Mario Kart", the player takes control of one of eight "Mario" series characters, each with differing capabilities. In single player mode players can race against computer-controlled characters in multi-race cups over three difficulty levels. During the races, offensive and speed boosting power-ups can be used to gain an advantage. Alternatively players can race against the clock in a Time Trial mode. In multiplayer mode two players can simultaneously take part in the cups or can race against each other one-on-one in Match Race mode. In a third multiplayer mode – Battle Mode – the aim is to defeat the other players by attacking them with power-ups, destroying balloons which surround each kart.

"Super Mario Kart" received positive reviews and was praised for its presentation, innovation and use of Mode 7 graphics. It has been ranked among the greatest video games of all time by several organizations including "Edge", IGN, "The Age" and GameSpot, while "Guinness World Records" has named it as the top console game ever. It is often credited with creating the kart-racing subgenre of video games, leading other developers to try to duplicate its success. The game is also seen as having been key to expanding the "Mario" series into non-platforming games. This diversity has led to it becoming the best-selling game franchise of all time. Several sequels to "Super Mario Kart" have been released, for consoles, handhelds and in arcades, each enjoying critical and commercial success. While some elements have developed throughout the series, the core experience from "Super Mario Kart" has remained intact.

"Super Mario Kart" is a kart racing game featuring several single and multiplayer modes. During the game, players take control of one of eight "Mario" franchise characters and drive karts around tracks with a "Mario" franchise theme. In order for them to begin driving, Lakitu will appear with a traffic light hanging on his fishing pole at the starting line, which starts the countdown. When the light turns green, the race or battle officially begins. During a race, the player's viewpoint is from behind his or her kart. The goal of the game is to either finish a race ahead of other racers, who are controlled by the computer and other players, or complete a circuit in the fastest time. There is also a battle mode in which the aim is to attack the karts of the other human players.

Tiles marked with question marks are arrayed on the race tracks; they give special abilities (power-ups) to a player's kart if the vehicle passes over them. Power-ups, such as the ability to throw shells and bananas, allow racers to hit others with the objects, causing them to spin and lose control. A kart that obtains the star power-up is temporarily invulnerable to attack. Computer players have specific special powers associated with each character, that they are able to use throughout the race. Lines of coins are found on the tracks in competitive race modes. By running over these coins, a kart collects them and increases its top speed. Having coins also helps players when their kart is hit by another: instead of spinning and losing control, they lose a coin. Coins are also lost when karts are struck by power-ups or fall off the tracks.

The game features advanced maneuvers such as power sliding and hopping. Power sliding allows a kart to maintain its speed while turning, although executing the maneuver for too long causes the kart to spin. Hopping helps a kart execute tighter turns: the kart makes a short hop and turns in the air, speeding off in the new direction when it lands. Reviewers praised "Super Mario Kart"s gameplay, describing the battle mode as "addictive" and the single player gameplay as "incredible". IGN stated that the gameplay mechanics defined the genre.

"Super Mario Kart" has two single-player modes: Mario Kart GP (which stands for Grand Prix) and Time Trial. In Mario Kart GP, one player is required to race against seven computer-controlled characters in a series of five races which are called cups. Initially, there are three cups available – the Mushroom Cup, Flower Cup, and Star Cup – at two difficulty levels, 50cc and 100cc. By winning all three of the cups at the 100cc level, a fourth cup – the Special Cup – is unlocked. Winning all four cups at 100cc unlocks a new difficulty level, 150cc. Each cup consists of five five-lap races, each taking place on a distinct track. In order to continue through a cup, a position of fourth or higher must be achieved in each race. If a player finishes in the fifth to eighth position, they are "ranked out" and the race must be replayed – at the cost of one of a limited number of lives – until a placing of fourth or above is achieved. If the player has no lives when they rank out, the game is over. Points are accrued by finishing in the top four positions in a race; first to fourth place receive nine, six, three and one points. If a player finished in the same position three times in a row, then an extra life is awarded. The finishing order for that race will then become the starting grid for the next race; for example, if a player finished in first place, then that player will start the next race in the same position. The racer with the highest number of points after all five races have been completed wins the cup. In time trial mode, players race against the clock through the same tracks that are present in Mario Kart GP mode, attempting to set the fastest time possible.

"Super Mario Kart" also has three multiplayer modes; Mario Kart GP, Match Race, and Battle Mode. The multiplayer modes support two players and the second player uses the bottom half of the screen which is used as a map in the single-player modes. Mario Kart GP is the same as in single-player, the only difference being that there are now two human-controlled and six computer-controlled drivers. Match Race involves the two players going head to head on a track of their choice without any opponents. In Battle Mode, the two players again go head to head, but this time in one of four dedicated Battle Mode courses. Each player starts with three balloons around their kart which can be popped by power-ups fired by the other player. The first player to have all three of their balloons popped loses.

"Super Mario Kart" features eight playable characters from the "Mario" series – Mario, Luigi, Princess Peach, Yoshi, Bowser, Donkey Kong Jr., Koopa Troopa and Toad. Each character's kart has different capabilities with differing levels of top speed, acceleration and handling. Mario, Luigi, Peach, Yoshi, Bowser and Toad returned in all of the subsequent "Mario Kart" games starting with "Mario Kart 64". During races, computer-controlled characters have special items, or superpowers, which they are able to use. These powers are specific to each character; for example, Yoshi drops eggs which cause players who hit them to lose coins and spin, while Donkey Kong Jr. throws bananas.

The characters are rendered as sprites portrayed from sixteen different angles. More recently, Nintendojo called the sprites "not-so-pretty" when they are rendered at a distance, and IGN has commented on the dated look of the game. "Super Mario Kart" was the first game to feature playable characters from the "Mario" series other than Mario or Luigi in a non-platforming game and the selection and different attributes of the characters is regarded as one of the game's strengths, IGN describing a well-balanced "all-star cast". All of the characters present in "Super Mario Kart" have gone on to appear in later games in the series, except for Koopa Troopa, who has only appeared intermittently after being replaced by Wario in "Mario Kart 64". Donkey Kong Jr. was replaced by Donkey Kong, who has appeared in every "Mario Kart" game since. This was Donkey Kong Jr.'s last appearance as a playable character, except for the "Mario Tennis" sub-series, including installments on the Nintendo 64 and Virtual Boy.

The tracks in "Super Mario Kart" are based on locations in "Super Mario World" such as Donut Plains. Each of the four cups contains five different tracks for a total of twenty unique tracks, additionally there are four unique Battle Mode courses. The course outlines are marked out by impassable barriers and feature a variety of bends ranging from sharp hairpins to wide curves which players can power slide around. Numerous obstacles themed from the "Mario" series appear, such as Thwomps in the Bowser's Castle tracks, the Cheep-Cheeps from "Super Mario World" in Koopa Beach and pipe barriers which are found in the Mario Circuit tracks. Other features include off-road sections which slow down the karts such as the mud bogs in the Choco Island tracks. Each single-player track is littered with coins and power-up tiles, as well as turbo tiles which give the karts a boost of speed and jumps which launch the karts into the air.

The tracks have received positive commentary with GameSpy describing them as wonderfully designed and IGN calling them perfect. When naming its top five "Mario Kart" tracks of all time in 2008, 1UP.com named Battle Mode Course 4 at number three and Rainbow Road – along with its subsequent versions in the series – at number one. The track themes in "Super Mario Kart" influenced later games in the series; recurring themes that first appeared in "Super Mario Kart" include haunted tracks, Bowser's castle and Rainbow Road. Some of the tracks from "Super Mario Kart" have been duplicated in later games. All twenty of the original tracks are unlockable as an extra feature in the Game Boy Advance sequel "". Remakes of Mario Circuit 1, Donut Plains 1, Koopa Beach 2 and Choco Island 2 appear as part of the Retro Grand Prix series in "Mario Kart DS", remakes of Ghost Valley 2, Mario Circuit 3, and Battle Course 4 appear as part of the Retro Grand Prix and battles in "Mario Kart Wii", remakes of Mario Circuit 2 and Rainbow Road appear as part of the Retro Grand Prix in "Mario Kart 7", a remake of Donut Plains 3 appears as part of the Retro Grand Prix and battles in "Mario Kart 8", a second remake of Rainbow Road appears in Mario Kart 8's first downloadable content pack, and a remake of Battle Course 1 appears as a Retro Battle Course in "Mario Kart 8 Deluxe".

"Super Mario Kart" was produced by "Mario" creator Shigeru Miyamoto and directed by Tadashi Sugiyama and Hideki Konno. The development team set out to produce a racing game capable of displaying two players on the same game screen simultaneously, in contrast to the single-player gameplay "F-Zero". This led to simpler tracks than those of "F-Zero". "Computer and Video Games" suggest that this initial emphasis on creating a two player experience is the reason for the game's horizontal split-screen during single-player. Battle Mode was developed from the desire to create a one-on-one mode where victory was not determined simply by competing for rank. 

The game did not start out as a "Mario" series game and the first prototype featured a generic kart racer character; the team decided that characters three heads tall would best suit the design of the karts. They did not decide to incorporate "Mario" characters until a few months into development. The choice was made after the development team when observing how one kart looked to another driving past it, decided to see what it would look like with Mario in the kart. Thinking that having Mario in the kart looked better than previous designs, the idea of a Mario themed racing game was born.

Notable in the development of "Super Mario Kart" was its use of Mode 7 graphics. First seen in "F-Zero", Mode 7 is a form of texture mapping available on the SNES which allows a plane to be rotated and scaled freely, achieving a pseudo-three-dimensional appearance. 1UP.com have credited the use of Mode 7 with giving the game graphics which at the time of release were considered to be "breathtaking". Retrospective reflection on the Mode 7 visuals was mixed, with IGN stating that the once revolutionary technology now looks "crude and flickery". "Super Mario Kart" featured a DSP (Digital Signal Processor) chip; DSPs were used in SNES games as they provided a better handling of floating point calculations to assist with three-dimensional maths. The DSP-1 chip that was used in "Super Mario Kart" went on to be the most popular DSP chip to be used in SNES games. The music for the title was created by composer Soyo Oka.

"Super Mario Kart" received critical acclaim and proved to be a commercial success; it received a Player's Choice release after selling one million copies and went on to sell 8.76 million copies, becoming the fourth best selling game ever for the SNES. Aggregate scoring sites GameRankings and MobyGames both give an average of more than 90 percent. Critics praised the game's Mode 7 graphics. Another aspect of the game to have been praised is its gameplay, which Thunderbolt has described as the "deepest [and] most addictive... to be found on the SNES console". Retrospective reviews of the game have been positive with perfect scores given by review sites including Thunderbolt and HonestGamers. The use of the style and characters from the "Mario" franchise was also praised as well as the individual characteristics of each racer. Mean Machines described the game as having "struck gold" in a way that no other – not even its sequels – has matched and GameSpot named the game as one of the greatest games of all time for its innovation, gameplay and visual style. "Entertainment Weekly" wrote that although the game might appear to be a "cynical attempt by Nintendo to cash in on its Super Mario franchise" the review concluded that "plunking the familiar characters down in souped-up go-carts actually makes for a delightful racing game." "GamePro" said the game "does an excellent job of capturing the thrill of Go-card racing, and wraps it up in the familiar, fun, Mario-land atmosphere." The reviewer also praised the use of Mode 7 and challenging CPU-controlled opponents.

"Super Mario Kart" has been listed among the best games ever made several times. In 1996, "Next Generation" listed it as number 37 on their "Top 100 Games of All Time", commenting that the controls are elegantly designed to offer "supreme fun." In 1999, "Next Generation" listed "Super Mario Kart" as number 7 on their "Top 50 Games of All Time", commenting that, "Imitated a thousand times, but never, ever, equalled, "Mario Kart" changed the rules for the driving game and gave the world one of the most engrossing and addictive two-player experiences ever." "Electronic Gaming Monthly" ranked it as the 15th best console video game of all time, attributing its higher ranking than "Mario Kart 64" (which came in 49th) to its superior track design and powerups. IGN ranked it as the 15th best game ever in 2005, describing it as "the original karting masterpiece" and as the 23rd best game ever in 2007, discussing its originality at time of release. "The Age" placed it at number 19 on their list of the 50 best games in 2005 and in 2007 "Edge" ranked "Super Mario Kart" at number 14 on a list of their 100 best games, noting its continued influence on video game design. The game is also included in Yahoo! Games UK's list of the hundred greatest games of all time which praises the appealing characters and power ups and 1UP.com's "Essential 50", a list of the fifty most important games ever made. The game placed 13th in "Official Nintendo Magazine"'s 100 greatest Nintendo games of all time. "Guinness World Records" ranked it at number 1 on a list of the top 50 console games of all time based on initial impact and lasting legacy.

"Super Mario Kart" has been credited with inventing the "kart racing" subgenre of video gaming and soon after its release several other developers attempted to duplicate its success. In 1994, less than two years after the release of "Super Mario Kart", Sega released "Sonic Drift"; a kart racing game featuring characters from the "Sonic the Hedgehog" series. Also in 1994 Ubisoft released "Street Racer", a kart racing game for the SNES and Mega Drive/Genesis which included a four player mode not present in "Super Mario Kart". Apogee Software released "Wacky Wheels" for PC and Atari Corporation released "Atari Karts" for the Atari Jaguar in 1995. Future games that followed in the mould of "Super Mario Kart" include "South Park Rally", "Konami Krazy Racers", "Diddy Kong Racing", "Sonic & Sega All-Stars Racing" and several racing games in the "Crash Bandicoot" series. Response to the karting games released since "Super Mario Kart" has been mixed, with GameSpot describing them as tending to be bad while 1UP.com notes that countless developers have tried to improve upon the Mario Kart formula without success.

"Super Mario Kart" is also credited as being the first non-platforming game to feature multiple playable characters from the "Mario" franchise. As well as several sequels Nintendo has released numerous other sporting and non-sporting Mario spin-offs since "Super Mario Kart"; a trend in part accredited to the commercial and critical success of the game. The "Mario" characters have appeared in many sports games including those relating to basketball, baseball, golf, tennis, and soccer. Non-sporting franchises using the "Mario" characters have also been created, including the "Super Smash Bros." series of fighting games and the "Mario Party" series of board game based, party games. "Mario" series characters have also made cameos in games from other series such as "SSX on Tour" and "NBA Street V3", both published by EA Sports. The genre-spanning nature of the Mario series that was sparked off by the success of "Super Mario Kart" has been described as key to the success and longevity of the franchise; keeping fans interested despite the infrequency of traditional Mario platforming games. Following this model the "Mario" series has gone on to become the best selling video game franchise of all time with 193 million units sold as of January 2007, almost 40 million units ahead of second-ranked franchise ("Pokémon", also by Nintendo).

"Super Mario Kart" was re-released on the Japanese Virtual Console on June 9, 2009, and later in North America on November 23, 2009. Previously, when naming it as one of the most wanted games for the platform in November 2008, Eurogamer stated that problems emulating the Mode 7 graphics were responsible for its absence.

The game was also released for the Wii U Virtual Console in Japan during June 2013, and in Europe on March 27, 2014. In addition, North America users was able to get the game starting from August 6, 2014 to celebrate the 22nd anniversary of the game, which also includes the new game update of "Mario Kart 8" on August 27, 2014.

"Super Mario 3D World" has a stage with a look based on the Mario Circuit racetracks from "Super Mario Kart". A remixed version of the music can also be heard. "Super Mario Odyssey" also has a remix, when racing an RC car around a track in New Donk City in the Metro Kingdom.

Several sequels to "Super Mario Kart" have been brought out for successive generations of Nintendo consoles, each receiving commercial success and critical acclaim. The first of these, "Mario Kart 64" was released in 1996 for the Nintendo 64 and was the first "Mario Kart" game to feature fully 3D graphics. Although reviewers including IGN and GameSpot felt that the single player gameplay was lacking compared to its predecessor, the simultaneous four-person multiplayer modes – a first for the Nintendo 64 – were praised. The second sequel, "", was released for the Game Boy Advance in 2001. It was described by GameSpot as more of a remake of "Super Mario Kart" than a sequel to "Mario Kart 64" and featured a return to the graphical style of the original. As well as featuring all new tracks, players are able to unlock the original SNES tracks if certain achievements are completed. "" was released for the GameCube in 2003. Unlike any other "Mario Kart" game before or since, it features two riders in each kart, allowing for a new form of cooperative multiplayer where one player controls the kart's movement and the other fires weapons. "Mario Kart DS", released for the Nintendo DS in 2005, was the first "Mario Kart" game to include online play via the Nintendo Wi-Fi Connection. It went on to become the best selling hand-held racing game of all time, selling 7.83 million units. The game also marks the debut of tracks appearing in previous games. "Mario Kart Wii" was released for the Wii in 2008 and incorporates motion controls and 12-player racing. Like "Mario Kart DS", it includes on-line play; it also allows racers to play as user-created Miis (after unlocking the Mii character) as well as "Mario" series characters and comes packaged with the Wii Wheel peripheral, which can act as the game's primary control mechanism when coupled with a Wii Remote. "Mario Kart Wii" went on to be the worldwide best-selling game of 2008 ahead of another Nintendo game – "Wii Fit" – and the critically acclaimed "Grand Theft Auto IV". "Mario Kart 7" for the Nintendo 3DS was released in 2011, which features racing on land, sea, and air. Also in "Mario Kart 7" is the ability to customize your kart and to race in first-person mode. Three "Mario Kart" arcade games have also been released, "Mario Kart Arcade GP" in 2005, "Mario Kart Arcade GP 2" in 2007, and "Mario Kart Arcade GP DX" in 2013. All of them were developed jointly by Nintendo and Namco and feature classic Namco characters including Pac-Man and Blinky. The most recent entry in the series is "Mario Kart 8" for the Wii U, which was released at the end of May 2014, which brings back gliders and propellers from "Mario Kart 7" as well as 12-player racing in "Mario Kart Wii". "Mario Kart 8" also includes a new feature called Mario Kart TV, where players can watch highlights of previous races and uploading them to YouTube. Another new feature is anti-gravity racing, where players can race on walls and ceilings. An enhanced port titled “Mario Kart 8 Deluxe” was released on the Nintendo Switch on April 28, 2017. The game keeps most elements from the Wii U version, while adding more characters, kart parts, battle modes, and battle stages. The port received universal critical acclaim, and has sold over 20 million copies as of August 2020, becoming the best selling game for the console. List of bestselling games for Nintendo Switch

As the series has progressed, many aspects included in "Super Mario Kart" have been developed and altered. The power-up boxes which are flat against the track in "Super Mario Kart" due to the technical limitations of the SNES became floating boxes in later games. The roster of racers has expanded in recent games to include a greater selection of Nintendo characters including some which had not been created at the time of "Super Mario Kart's" release – such as Petey Piranha from "Super Mario Sunshine" who appeared in "Mario Kart: Double Dash!!". Multiplayer has remained a key feature of the series and has expanded from the two-player modes available in "Super Mario Kart"; first to allow up to four simultaneous players in "Mario Kart 64" and eventually up to twelve simultaneous online players in "Mario Kart Wii". Many of the track themes have been retained throughout the series, including Rainbow Road – the final track of the Special Cup – which has appeared in every "Mario Kart" console game. Other features present in "Super Mario Kart" have disappeared from the series. These include the "super-powers" of the computer characters, the feather power-up which allows players to jump high into the air and having a restricted number of lives. The only other "Mario Kart" games to feature the coin collecting of the original are "Mario Kart: Super Circuit", "Mario Kart 7", and "Mario Kart 8". The aspects of style and gameplay from "Super Mario Kart" that have been retained throughout the series have led Nintendo to face criticism for a lack of originality but the franchise is still considered to be a beloved household name by many, known for its familiar core gameplay.



</doc>
