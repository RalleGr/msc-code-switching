<doc id="29438" url="https://en.wikipedia.org/wiki?curid=29438" title="Sonar">
Sonar

Sonar (sound navigation ranging) is a technique that uses sound propagation (usually underwater, as in submarine navigation) to navigate, communicate with or detect objects on or under the surface of the water, such as other vessels. Two types of technology share the name "sonar": "passive" sonar is essentially listening for the sound made by vessels; "active" sonar is emitting pulses of sounds and listening for echoes. Sonar may be used as a means of acoustic location and of measurement of the echo characteristics of "targets" in the water. Acoustic location in air was used before the introduction of radar. Sonar may also be used for robot navigation, and SODAR (an upward-looking in-air sonar) is used for atmospheric investigations. The term "sonar" is also used for the equipment used to generate and receive the sound. The acoustic frequencies used in sonar systems vary from very low (infrasonic) to extremely high (ultrasonic). The study of underwater sound is known as underwater acoustics or hydroacoustics.

The first recorded use of the technique was by Leonardo da Vinci in 1490 who used a tube inserted into the water to detect vessels by ear. It was developed during World War I to counter the growing threat of submarine warfare, with an operational passive sonar system in use by 1918. Modern active sonar systems use an acoustic transducer to generate a sound wave which is reflected from target objects.

Although some animals (dolphins, bats, some shrews, and others) have used sound for communication and object detection for millions of years, use by humans in the water is initially recorded by Leonardo da Vinci in 1490: a tube inserted into the water was said to be used to detect vessels by placing an ear to the tube.

In the late 19th century an underwater bell was used as an ancillary to lighthouses or lightships to provide warning of hazards.

The use of sound to "echo-locate" underwater in the same way as bats use sound for aerial navigation seems to have been prompted by the disaster of 1912. The world's first patent for an underwater echo-ranging device was filed at the British Patent Office by English meteorologist Lewis Fry Richardson a month after the sinking of "Titanic", and a German physicist Alexander Behm obtained a patent for an echo sounder in 1913.

The Canadian engineer Reginald Fessenden, while working for the Submarine Signal Company in Boston, Massachusetts, built an experimental system beginning in 1912, a system later tested in Boston Harbor, and finally in 1914 from the U.S. Revenue Cutter "Miami" on the Grand Banks off Newfoundland. In that test, Fessenden demonstrated depth sounding, underwater communications (Morse code) and echo ranging (detecting an iceberg at a range). The "Fessenden oscillator", operated at about 500 Hz frequency, was unable to determine the bearing of the iceberg due to the 3-metre wavelength and the small dimension of the transducer's radiating face (less than wavelength in diameter). The ten Montreal-built British H-class submarines launched in 1915 were equipped with Fessenden oscillators.

During World War I the need to detect submarines prompted more research into the use of sound. The British made early use of underwater listening devices called hydrophones, while the French physicist Paul Langevin, working with a Russian immigrant electrical engineer Constantin Chilowsky, worked on the development of active sound devices for detecting submarines in 1915. Although piezoelectric and magnetostrictive transducers later superseded the electrostatic transducers they used, this work influenced future designs. Lightweight sound-sensitive plastic film and fibre optics have been used for hydrophones, while Terfenol-D and PMN (lead magnesium niobate) have been developed for projectors.

In 1916, under the British Board of Invention and Research, Canadian physicist Robert William Boyle took on the active sound detection project with A. B. Wood, producing a prototype for testing in mid-1917. This work, for the Anti-Submarine Division of the British Naval Staff, was undertaken in utmost secrecy, and used quartz piezoelectric crystals to produce the world's first practical underwater active sound detection apparatus. To maintain secrecy, no mention of sound experimentation or quartz was made – the word used to describe the early work ("supersonics") was changed to "ASD"ics, and the quartz material to "ASD"ivite: "ASD" for "Anti-Submarine Division", hence the British acronym "ASDIC". In 1939, in response to a question from the Oxford English Dictionary, the Admiralty made up the story that it stood for "Allied Submarine Detection Investigation Committee", and this is still widely believed, though no committee bearing this name has been found in the Admiralty archives.

By 1918, Britain and France had built prototype active systems. The British tested their ASDIC on in 1920 and started production in 1922. The 6th Destroyer Flotilla had ASDIC-equipped vessels in 1923. An anti-submarine school HMS "Osprey" and a training flotilla of four vessels were established on Portland in 1924. 

By the outbreak of World War II, the Royal Navy had five sets for different surface ship classes, and others for submarines, incorporated into a complete anti-submarine system. The effectiveness of early ASDIC was hampered by the use of the depth charge as an anti-submarine weapon. This required an attacking vessel to pass over a submerged contact before dropping charges over the stern, resulting in a loss of ASDIC contact in the moments leading up to attack. The hunter was effectively firing blind, during which time a submarine commander could take evasive action. This situation was remedied with new tactics and new weapons.

The tactical improvements developed by Frederic John Walker included the creeping attack. 2 anti-submarine ships were needed for this (usually sloops or corvettes). The "directing ship" tracked the target submarine on ASDIC from a position about 1500 to 2000 yards behind the submarine. The second ship, with her ASDIC turned off and running at 5 knots, started an attack from a position between the directing ship and the target. This attack was controlled by radio telephone from the directing ship, based on their ASDIC and the range (by rangefinder) and bearing of the attacking ship. As soon as the depth charges had been released, the attacking ship left the immediate area at full speed. The directing ship then entered the target area and also released a pattern of depth charges. The low speed of the approach meant the submarine could not predict when depth charges were going to be released. Any evasive action was detected by the directing ship and steering orders to the attacking ship given accordingly. The low speed of the attack had the advantage that the German acoustic torpedo was not effective against a warship travelling so slowly. A variation of the creeping attack was the "plaster" attack, in which 3 attacking ships working in a close line abreast were directed over the target by the directing ship.

The new weapons to deal with the ASDIC blind spot were "ahead-throwing weapons", such as Hedgehogs and later Squids, which projected warheads at a target ahead of the attacker and still in ASDIC contact. These allowed a single escort to make better aimed attacks on submarines. Developments during the war resulted in British ASDIC sets that used several different shapes of beam, continuously covering blind spots. Later, acoustic torpedoes were used.

Early in World War II (September 1940), British ASDIC technology was transferred for free to the United States. Research on ASDIC and underwater sound was expanded in the UK and in the US. Many new types of military sound detection were developed. These included sonobuoys, first developed by the British in 1944 under the codename "High Tea", dipping/dunking sonar and mine-detection sonar. This work formed the basis for post-war developments related to countering the nuclear submarine.

During the 1930s American engineers developed their own underwater sound-detection technology, and important discoveries were made, such as the existence of thermoclines and their effects on sound waves. Americans began to use the term "SONAR" for their systems, coined by Frederick Hunt to be the equivalent of RADAR.

In 1917, the US Navy acquired J. Warren Horton's services for the first time. On leave from Bell Labs, he served the government as a technical expert, first at the experimental station at Nahant, Massachusetts, and later at US Naval Headquarters, in London, England. At Nahant he applied the newly developed vacuum tube, then associated with the formative stages of the field of applied science now known as electronics, to the detection of underwater signals. As a result, the carbon button microphone, which had been used in earlier detection equipment, was replaced by the precursor of the modern hydrophone. Also during this period, he experimented with methods for towing detection. This was due to the increased sensitivity of his device. The principles are still used in modern towed sonar systems.

To meet the defense needs of Great Britain, he was sent to England to install in the Irish Sea bottom-mounted hydrophones connected to a shore listening post by submarine cable. While this equipment was being loaded on the cable-laying vessel, World War I ended and Horton returned home.

During World War II, he continued to develop sonar systems that could detect submarines, mines, and torpedoes. He published "Fundamentals of Sonar" in 1957 as chief research consultant at the US Navy Underwater Sound Laboratory. He held this position until 1959 when he became technical director, a position he held until mandatory retirement in 1963.

There was little progress in US sonar from 1915 to 1940. In 1940, US sonars typically consisted of a magnetostrictive transducer and an array of nickel tubes connected to a 1-foot-diameter steel plate attached back-to-back to a Rochelle salt crystal in a spherical housing. This assembly penetrated the ship hull and was manually rotated to the desired angle. The piezoelectric Rochelle salt crystal had better parameters, but the magnetostrictive unit was much more reliable. High losses to US merchant supply shipping early in World War II led to large scale high priority US research in the field, pursuing both improvements in magnetostrictive transducer parameters and Rochelle salt reliability. Ammonium dihydrogen phosphate (ADP), a superior alternative, was found as a replacement for Rochelle salt; the first application was a replacement of the 24 kHz Rochelle-salt transducers. Within nine months, Rochelle salt was obsolete. The ADP manufacturing facility grew from few dozen personnel in early 1940 to several thousands in 1942.

One of the earliest application of ADP crystals were hydrophones for acoustic mines; the crystals were specified for low-frequency cutoff at 5 Hz, withstanding mechanical shock for deployment from aircraft from , and ability to survive neighbouring mine explosions. One of key features of ADP reliability is its zero aging characteristics; the crystal keeps its parameters even over prolonged storage.

Another application was for acoustic homing torpedoes. Two pairs of directional hydrophones were mounted on the torpedo nose, in the horizontal and vertical plane; the difference signals from the pairs were used to steer the torpedo left-right and up-down. A countermeasure was developed: the targeted submarine discharged an effervescent chemical, and the torpedo went after the noisier fizzy decoy. The counter-countermeasure was a torpedo with active sonar – a transducer was added to the torpedo nose, and the microphones were listening for its reflected periodic tone bursts. The transducers comprised identical rectangular crystal plates arranged to diamond-shaped areas in staggered rows.

Passive sonar arrays for submarines were developed from ADP crystals. Several crystal assemblies were arranged in a steel tube, vacuum-filled with castor oil, and sealed. The tubes then were mounted in parallel arrays.

The standard US Navy scanning sonar at the end of World War II operated at 18 kHz, using an array of ADP crystals. Desired longer range, however, required use of lower frequencies. The required dimensions were too big for ADP crystals, so in the early 1950s magnetostrictive and barium titanate piezoelectric systems were developed, but these had problems achieving uniform impedance characteristics, and the beam pattern suffered. Barium titanate was then replaced with more stable lead zirconate titanate (PZT), and the frequency was lowered to 5 kHz. The US fleet used this material in the AN/SQS-23 sonar for several decades. The SQS-23 sonar first used magnetostrictive nickel transducers, but these weighed several tons, and nickel was expensive and considered a critical material; piezoelectric transducers were therefore substituted. The sonar was a large array of 432 individual transducers. At first, the transducers were unreliable, showing mechanical and electrical failures and deteriorating soon after installation; they were also produced by several vendors, had different designs, and their characteristics were different enough to impair the array's performance. The policy to allow repair of individual transducers was then sacrificed, and "expendable modular design", sealed non-repairable modules, was chosen instead, eliminating the problem with seals and other extraneous mechanical parts.

The Imperial Japanese Navy at the onset of World War II used projectors based on quartz. These were big and heavy, especially if designed for lower frequencies; the one for Type 91 set, operating at 9 kHz, had a diameter of and was driven by an oscillator with 5 kW power and 7 kV of output amplitude. The Type 93 projectors consisted of solid sandwiches of quartz, assembled into spherical cast iron bodies. The Type 93 sonars were later replaced with Type 3, which followed German design and used magnetostrictive projectors; the projectors consisted of two rectangular identical independent units in a cast iron rectangular body about . The exposed area was half the wavelength wide and three wavelengths high. The magnetostrictive cores were made from 4 mm stampings of nickel, and later of an iron-aluminium alloy with aluminium content between 12.7% and 12.9%. The power was provided from a 2 kW at 3.8 kV, with polarization from a 20 V, 8 A DC source.

The passive hydrophones of the Imperial Japanese Navy were based on moving-coil design, Rochelle salt piezo transducers, and carbon microphones.

Magnetostrictive transducers were pursued after World War II as an alternative to piezoelectric ones. Nickel scroll-wound ring transducers were used for high-power low-frequency operations, with size up to in diameter, probably the largest individual sonar transducers ever. The advantage of metals is their high tensile strength and low input electrical impedance, but they have electrical losses and lower coupling coefficient than PZT, whose tensile strength can be increased by prestressing. Other materials were also tried; nonmetallic ferrites were promising for their low electrical conductivity resulting in low eddy current losses, Metglas offered high coupling coefficient, but they were inferior to PZT overall. In the 1970s, compounds of rare earths and iron were discovered with superior magnetomechanic properties, namely the Terfenol-D alloy. This made possible new designs, e.g. a hybrid magnetostrictive-piezoelectric transducer. The most recent of these improved magnetostrictive materials is Galfenol.

Other types of transducers include variable-reluctance (or moving-armature, or electromagnetic) transducers, where magnetic force acts on the surfaces of gaps, and moving coil (or electrodynamic) transducers, similar to conventional speakers; the latter are used in underwater sound calibration, due to their very low resonance frequencies and flat broadband characteristics above them.

Active sonar uses a sound transmitter (or projector) and a receiver. When the two are in the same place it is monostatic operation. When the transmitter and receiver are separated it is bistatic operation. When more transmitters (or more receivers) are used, again spatially separated, it is multistatic operation. Most sonars are used monostatically with the same array often being used for transmission and reception. Active sonobuoy fields may be operated multistatically.

Active sonar creates a pulse of sound, often called a "ping", and then listens for reflections (echo) of the pulse. This pulse of sound is generally created electronically using a sonar projector consisting of a signal generator, power amplifier and electro-acoustic transducer/array. A transducer is a device that can transmit and receive acoustic signals ("pings"). A beamformer is usually employed to concentrate the acoustic power into a beam, which may be swept to cover the required search angles. Generally, the electro-acoustic transducers are of the Tonpilz type and their design may be optimised to achieve maximum efficiency over the widest bandwidth, in order to optimise performance of the overall system. Occasionally, the acoustic pulse may be created by other means, e.g. chemically using explosives, airguns or plasma sound sources.

To measure the distance to an object, the time from transmission of a pulse to reception is measured and converted into a range using the known speed of sound. To measure the bearing, several hydrophones are used, and the set measures the relative arrival time to each, or with an array of hydrophones, by measuring the relative amplitude in beams formed through a process called beamforming. Use of an array reduces the spatial response so that to provide wide cover multibeam systems are used. The target signal (if present) together with noise is then passed through various forms of signal processing, which for simple sonars may be just energy measurement. It is then presented to some form of decision device that calls the output either the required signal or noise. This decision device may be an operator with headphones or a display, or in more sophisticated sonars this function may be carried out by software. Further processes may be carried out to classify the target and localise it, as well as measuring its velocity.

The pulse may be at constant frequency or a chirp of changing frequency (to allow pulse compression on reception). Simple sonars generally use the former with a filter wide enough to cover possible Doppler changes due to target movement, while more complex ones generally include the latter technique. Since digital processing became available pulse compression has usually been implemented using digital correlation techniques. Military sonars often have multiple beams to provide all-round cover while simple ones only cover a narrow arc, although the beam may be rotated, relatively slowly, by mechanical scanning.

Particularly when single frequency transmissions are used, the Doppler effect can be used to measure the radial speed of a target. The difference in frequency between the transmitted and received signal is measured and converted into a velocity. Since Doppler shifts can be introduced by either receiver or target motion, allowance has to be made for the radial speed of the searching platform.

One useful small sonar is similar in appearance to a waterproof flashlight. The head is pointed into the water, a button is pressed, and the device displays the distance to the target. Another variant is a "fishfinder" that shows a small display with shoals of fish. Some civilian sonars (which are not designed for stealth) approach active military sonars in capability, with three-dimensional displays of the area near the boat.

When active sonar is used to measure the distance from the transducer to the bottom, it is known as echo sounding. Similar methods may be used looking upward for wave measurement.

Active sonar is also used to measure distance through water between two sonar transducers or a combination of a hydrophone (underwater acoustic microphone) and projector (underwater acoustic speaker). When a hydrophone/transducer receives a specific interrogation signal it responds by transmitting a specific reply signal. To measure distance, one transducer/projector transmits an interrogation signal and measures the time between this transmission and the receipt of the other transducer/hydrophone reply. The time difference, scaled by the speed of sound through water and divided by two, is the distance between the two platforms. This technique, when used with multiple transducers/hydrophones/projectors, can calculate the relative positions of static and moving objects in water.

In combat situations, an active pulse can be detected by an enemy and will reveal a submarine's position at twice the maximum distance that the submarine can itself detect a contact and give clues as to the submarines identity based on the characteristics of the outgoing ping. For these reasons, active sonar is not frequently used by military submarines.

A very directional, but low-efficiency, type of sonar (used by fisheries, military, and for port security) makes use of a complex nonlinear feature of water known as non-linear sonar, the virtual transducer being known as a "parametric array".

Project Artemis was an experimental research and development project in the late 1950s to mid 1960s to examine acoustic propagation and signal processing for a low-frequency active sonar system that might be used for ocean surveillance. A secondary objective was examination of engineering problems of fixed active bottom systems. The receiving array was located on the slope of Plantagnet Bank off Bermuda. The active source array was deployed from the converted World War II tanker . Elements of Artemis were used experimentally after the main experiment was terminated.

This is an active sonar device that receives a specific stimulus and immediately (or with a delay) retransmits the received signal or a predetermined one. Transponders can be used to remotely activate or recover subsea equipment.

A sonar target is small relative to the sphere, centred around the emitter, on which it is located. Therefore, the power of the reflected signal is very low, several orders of magnitude less than the original signal. Even if the reflected signal was of the same power, the following example (using hypothetical values) shows the problem: Suppose a sonar system is capable of emitting a 10,000 W/m signal at 1 m, and detecting a 0.001 W/m signal. At 100 m the signal will be 1 W/m (due to the inverse-square law). If the entire signal is reflected from a 10 m target, it will be at 0.001 W/m when it reaches the emitter, i.e. just detectable. However, the original signal will remain above 0.001 W/m until 3000 m. Any 10 m target between 100 and 3000 m using a similar or better system would be able to detect the pulse, but would not be detected by the emitter. The detectors must be very sensitive to pick up the echoes. Since the original signal is much more powerful, it can be detected many times further than twice the range of the sonar (as in the example).

Active sonar have two performance limitations: due to noise and reverberation. In general, one or other of these will dominate, so that the two effects can be initially considered separately.

In noise-limited conditions at initial detection:
where SL is the source level, PL is the propagation loss (sometimes referred to as transmission loss), TS is the target strength, NL is the noise level, AG is the array gain of the receiving array (sometimes approximated by its directivity index) and DT is the detection threshold.

In reverberation-limited conditions at initial detection (neglecting array gain):
where RL is the reverberation level, and the other factors are as before.


Passive sonar listens without transmitting. It is often employed in military settings, although it is also used in science applications, "e.g.", detecting fish for presence/absence studies in various aquatic environments – see also passive acoustics and passive radar. In the very broadest usage, this term can encompass virtually any analytical technique involving remotely generated sound, though it is usually restricted to techniques applied in an aquatic environment.

Passive sonar has a wide variety of techniques for identifying the source of a detected sound. For example, U.S. vessels usually operate 60 Hz alternating current power systems. If transformers or generators are mounted without proper vibration insulation from the hull or become flooded, the 60 Hz sound from the windings can be emitted from the submarine or ship. This can help to identify its nationality, as all European submarines and nearly every other nation's submarine have 50 Hz power systems. Intermittent sound sources (such as a wrench being dropped), called "transients," may also be detectable to passive sonar. Until fairly recently, an experienced, trained operator identified signals, but now computers may do this.

Passive sonar systems may have large sonic databases, but the sonar operator usually finally classifies the signals manually. A computer system frequently uses these databases to identify classes of ships, actions (i.e. the speed of a ship, or the type of weapon released), and even particular ships.

Passive sonar on vehicles is usually severely limited because of noise generated by the vehicle. For this reason, many submarines operate nuclear reactors that can be cooled without pumps, using silent convection, or fuel cells or batteries, which can also run silently. Vehicles' propellers are also designed and precisely machined to emit minimal noise. High-speed propellers often create tiny bubbles in the water, and this cavitation has a distinct sound.

The sonar hydrophones may be towed behind the ship or submarine in order to reduce the effect of noise generated by the watercraft itself. Towed units also combat the thermocline, as the unit may be towed above or below the thermocline.

The display of most passive sonars used to be a two-dimensional waterfall display. The horizontal direction of the display is bearing. The vertical is frequency, or sometimes time. Another display technique is to color-code frequency-time information for bearing. More recent displays are generated by the computers, and mimic radar-type plan position indicator displays.

Unlike active sonar, only one-way propagation is involved. Because of the different signal processing used, the minimal detectable signal-to-noise ratio will be different. The equation for determining the performance of a passive sonar is
where SL is the source level, PL is the propagation loss, NL is the noise level, AG is the array gain and DT is the detection threshold. The figure of merit of a passive sonar is

The detection, classification and localisation performance of a sonar depends on the environment and the receiving equipment, as well as the transmitting equipment in an active sonar or the target radiated noise in a passive sonar.

Sonar operation is affected by variations in sound speed, particularly in the vertical plane. Sound travels more slowly in fresh water than in sea water, though the difference is small. The speed is determined by the water's bulk modulus and mass density. The bulk modulus is affected by temperature, dissolved impurities (usually salinity), and pressure. The density effect is small. The speed of sound (in feet per second) is approximately:

This empirically derived approximation equation is reasonably accurate for normal temperatures, concentrations of salinity and the range of most ocean depths. Ocean temperature varies with depth, but at between 30 and 100 meters there is often a marked change, called the thermocline, dividing the warmer surface water from the cold, still waters that make up the rest of the ocean. This can frustrate sonar, because a sound originating on one side of the thermocline tends to be bent, or refracted, through the thermocline. The thermocline may be present in shallower coastal waters. However, wave action will often mix the water column and eliminate the thermocline. Water pressure also affects sound propagation: higher pressure increases the sound speed, which causes the sound waves to refract away from the area of higher sound speed. The mathematical model of refraction is called Snell's law.

If the sound source is deep and the conditions are right, propagation may occur in the 'deep sound channel'. This provides extremely low propagation loss to a receiver in the channel. This is because of sound trapping in the channel with no losses at the boundaries. Similar propagation can occur in the 'surface duct' under suitable conditions. However, in this case there are reflection losses at the surface.

In shallow water propagation is generally by repeated reflection at the surface and bottom, where considerable losses can occur.

Sound propagation is affected by absorption in the water itself as well as at the surface and bottom. This absorption depends upon frequency, with several different mechanisms in sea water. Long-range sonar uses low frequencies to minimise absorption effects.

The sea contains many sources of noise that interfere with the desired target echo or signature. The main noise sources are waves and shipping. The motion of the receiver through the water can also cause speed-dependent low frequency noise.

When active sonar is used, scattering occurs from small objects in the sea as well as from the bottom and surface. This can be a major source of interference. This acoustic scattering is analogous to the scattering of the light from a car's headlights in fog: a high-intensity pencil beam will penetrate the fog to some extent, but broader-beam headlights emit much light in unwanted directions, much of which is scattered back to the observer, overwhelming that reflected from the target ("white-out"). For analogous reasons active sonar needs to transmit in a narrow beam to minimize scattering.
The scattering of sonar from objects (mines, pipelines, zooplankton, geological features, fish etc.) is how active sonar detects them, but this ability can be masked by strong scattering from false targets, or 'clutter'. Where they occur (under breaking waves; in ship wakes; in gas emitted from seabed seeps and leaks etc.), gas bubbles are powerful sources of clutter, and can readily hide targets. TWIPS (Twin Inverted Pulse Sonar) is currently the only sonar that can overcome this clutter problem. This is important as many recent conflicts have occurred in coastal waters, and the inability to detect whether mines are present or not present hazards and delays to military vessels, and also to aid convoys and merchant shipping trying to support the region long after the conflict has ceased.

The sound "reflection" characteristics of the target of an active sonar, such as a submarine, are known as its target strength. A complication is that echoes are also obtained from other objects in the sea such as whales, wakes, schools of fish and rocks.

Passive sonar detects the target's "radiated" noise characteristics. The radiated spectrum comprises a continuous spectrum of noise with peaks at certain frequencies which can be used for classification.

"Active" (powered) countermeasures may be launched by a submarine under attack to raise the noise level, provide a large false target, and obscure the signature of the submarine itself.

"Passive" (i.e., non-powered) countermeasures include:

Modern naval warfare makes extensive use of both passive and active sonar from water-borne vessels, aircraft and fixed installations. Although active sonar was used by surface craft in World War II, submarines avoided the use of active sonar due to the potential for revealing their presence and position to enemy forces. However, the advent of modern signal-processing enabled the use of passive sonar as a primary means for search and detection operations. In 1987 a division of Japanese company Toshiba reportedly sold machinery to the Soviet Union that allowed their submarine propeller blades to be milled so that they became radically quieter, making the newer generation of submarines more difficult to detect.

The use of active sonar by a submarine to determine bearing is extremely rare and will not necessarily give high quality bearing or range information to the submarines fire control team. However, use of active sonar on surface ships is very common and is used by submarines when the tactical situation dictates it is more important to determine the position of a hostile submarine than conceal their own position. With surface ships, it might be assumed that the threat is already tracking the ship with satellite data as any vessel around the emitting sonar will detect the emission. Having heard the signal, it is easy to identify the sonar equipment used (usually with its frequency) and its position (with the sound wave's energy). Active sonar is similar to radar in that, while it allows detection of targets at a certain range, it also enables the emitter to be detected at a far greater range, which is undesirable.

Since active sonar reveals the presence and position of the operator, and does not allow exact classification of targets, it is used by fast (planes, helicopters) and by noisy platforms (most surface ships) but rarely by submarines. When active sonar is used by surface ships or submarines, it is typically activated very briefly at intermittent periods to minimize the risk of detection. Consequently, active sonar is normally considered a backup to passive sonar. In aircraft, active sonar is used in the form of disposable sonobuoys that are dropped in the aircraft's patrol area or in the vicinity of possible enemy sonar contacts.

Passive sonar has several advantages, most importantly that it is silent. If the target radiated noise level is high enough, it can have a greater range than active sonar, and allows the target to be identified. Since any motorized object makes some noise, it may in principle be detected, depending on the level of noise emitted and the ambient noise level in the area, as well as the technology used. To simplify, passive sonar "sees" around the ship using it. On a submarine, nose-mounted passive sonar detects in directions of about 270°, centered on the ship's alignment, the hull-mounted array of about 160° on each side, and the towed array of a full 360°. The invisible areas are due to the ship's own interference. Once a signal is detected in a certain direction (which means that something makes sound in that direction, this is called broadband detection) it is possible to zoom in and analyze the signal received (narrowband analysis). This is generally done using a Fourier transform to show the different frequencies making up the sound. Since every engine makes a specific sound, it is straightforward to identify the object. Databases of unique engine sounds are part of what is known as "acoustic intelligence" or ACINT.

Another use of passive sonar is to determine the target's trajectory. This process is called target motion analysis (TMA), and the resultant "solution" is the target's range, course, and speed. TMA is done by marking from which direction the sound comes at different times, and comparing the motion with that of the operator's own ship. Changes in relative motion are analyzed using standard geometrical techniques along with some assumptions about limiting cases.

Passive sonar is stealthy and very useful. However, it requires high-tech electronic components and is costly. It is generally deployed on expensive ships in the form of arrays to enhance detection. Surface ships use it to good effect; it is even better used by submarines, and it is also used by airplanes and helicopters, mostly to a "surprise effect", since submarines can hide under thermal layers. If a submarine's commander believes he is alone, he may bring his boat closer to the surface and be easier to detect, or go deeper and faster, and thus make more sound.

Examples of sonar applications in military use are given below. Many of the civil uses given in the following section may also be applicable to naval use.

Until recently, ship sonars were usually with hull mounted arrays, either amidships or at the bow. It was soon found after their initial use that a means of reducing flow noise was required. The first were made of canvas on a framework, then steel ones were used. Now domes are usually made of reinforced plastic or pressurized rubber. Such sonars are primarily active in operation. An example of a conventional hull mounted sonar is the SQS-56.

Because of the problems of ship noise, towed sonars are also used. These also have the advantage of being able to be placed deeper in the water. However, there are limitations on their use in shallow water. These are called towed arrays (linear) or variable depth sonars (VDS) with 2/3D arrays. A problem is that the winches required to deploy/recover these are large and expensive. VDS sets are primarily active in operation while towed arrays are passive.

An example of a modern active-passive ship towed sonar is Sonar 2087 made by Thales Underwater Systems.

Modern torpedoes are generally fitted with an active/passive sonar. This may be used to home directly on the target, but wake homing torpedoes are also used. An early example of an acoustic homer was the Mark 37 torpedo.

Torpedo countermeasures can be towed or free. An early example was the German "Sieglinde" device while the "Bold" was a chemical device. A widely used US device was the towed AN/SLQ-25 Nixie while the mobile submarine simulator (MOSS) was a free device. A modern alternative to the Nixie system is the UK Royal Navy S2170 Surface Ship Torpedo Defence system.

Mines may be fitted with a sonar to detect, localize and recognize the required target. An example is the CAPTOR mine.

Mine countermeasure (MCM) sonar, sometimes called "mine and obstacle avoidance sonar (MOAS)", is a specialized type of sonar used for detecting small objects. Most MCM sonars are hull mounted but a few types are VDS design. An example of a hull mounted MCM sonar is the Type 2193 while the SQQ-32 mine-hunting sonar and Type 2093 systems are VDS designs.

Submarines rely on sonar to a greater extent than surface ships as they cannot use radar at depth. The sonar arrays may be hull mounted or towed. Information fitted on typical fits is given in and .

Helicopters can be used for antisubmarine warfare by deploying fields of active-passive sonobuoys or can operate dipping sonar, such as the AQS-13. Fixed wing aircraft can also deploy sonobuoys and have greater endurance and capacity to deploy them. Processing from the sonobuoys or dipping sonar can be on the aircraft or on ship. Dipping sonar has the advantage of being deployable to depths appropriate to daily conditions. Helicopters have also been used for mine countermeasure missions using towed sonars such as the AQS-20A.

Dedicated sonars can be fitted to ships and submarines for underwater communication.

The United States began a system of passive, fixed ocean surveillance systems in 1950 with the classified name Sound Surveillance System (SOSUS) with American Telephone and Telegraph Company (AT&T), with its Bell Laboratories research and Western Electric manufacturing entities being contracted for development and installation. The systems exploited the deep sound (SOFAR) channel and were based on an AT&T sound spectrograph, which converted sound into a visual spectrogram representing a time–frequency analysis of sound that was developed for speech analysis and modified to analyze low-frequency underwater sounds. That process was Low Frequency Analysis and Recording and the equipment was termed the Low Frequency Analyzer and Recorder, both with the acronym LOFAR. LOFAR research was termed "Jezebel" and led to usage in air and surface systems, particularly sonobuys using the process and sometimes using "Jezebel" in their name. The proposed system offered such promise of long-range submarine detection that the Navy ordered immediate moves for implementation.
Between installation of a test array followed by a full scale, forty element, prototype operational array in 1951 and 1958 systems were installed in the Atlantic and then the Pacific under the unclassified name "Project Caesar". The original systems were terminated at classified shore stations designated Naval Facility (NAVFAC) explained as engaging in "ocean research" to cover their classified mission. The system was upgraded multiple times with more advanced cable allowing the arrays to be installed in ocean basins and upgraded processing. The shore stations were eliminated in a process of consolidation and rerouting the arrays to central processing centers into the 1990s. In 1985, with new mobile arrays and other systems becoming operational the collective system name was changed to Integrated Undersea Surveillance System (IUSS). In 1991 the mission of the system was declassified. The year before IUSS insignia were authorized for wear. Access was granted to some systems for scientific research.

A similar system is believed to have been operated by the Soviet Union. 

Sonar can be used to detect frogmen and other scuba divers. This can be applicable around ships or at entrances to ports. Active sonar can also be used as a deterrent and/or disablement mechanism. One such device is the Cerberus system.

Limpet mine imaging sonar (LIMIS) is a hand-held or ROV-mounted imaging sonar designed for patrol divers (combat frogmen or clearance divers) to look for limpet mines in low visibility water.

The LUIS is another imaging sonar for use by a diver.

Integrated navigation sonar system (INSS) is a small flashlight-shaped handheld sonar for divers that displays range.

This is a sonar designed to detect and locate the transmissions from hostile active sonars. An example of this is the Type 2082 fitted on the British s.

Fishing is an important industry that is seeing growing demand, but world catch tonnage is falling as a result of serious resource problems. The industry faces a future of continuing worldwide consolidation until a point of sustainability can be reached. However, the consolidation of the fishing fleets are driving increased demands for sophisticated fish finding electronics such as sensors, sounders and sonars. Historically, fishermen have used many different techniques to find and harvest fish. However, acoustic technology has been one of the most important driving forces behind the development of the modern commercial fisheries.

Sound waves travel differently through fish than through water because a fish's air-filled swim bladder has a different density than seawater. This density difference allows the detection of schools of fish by using reflected sound. Acoustic technology is especially well suited for underwater applications since sound travels farther and faster underwater than in air. Today, commercial fishing vessels rely almost completely on acoustic sonar and sounders to detect fish. Fishermen also use active sonar and echo sounder technology to determine water depth, bottom contour, and bottom composition.
Companies such as eSonar, Raymarine, Marport Canada, Wesmar, Furuno, Krupp, and Simrad make a variety of sonar and acoustic instruments for the deep sea commercial fishing industry. For example, net sensors take various underwater measurements and transmit the information back to a receiver on board a vessel. Each sensor is equipped with one or more acoustic transducers depending on its specific function. Data is transmitted from the sensors using wireless acoustic telemetry and is received by a hull mounted hydrophone. The analog signals are decoded and converted by a digital acoustic receiver into data which is transmitted to a bridge computer for graphical display on a high resolution monitor.

Echo sounding is a process used to determine the depth of water beneath ships and boats. A type of active sonar, echo sounding is the transmission of an acoustic pulse directly downwards to the seabed, measuring the time between transmission and echo return, after having hit the bottom and bouncing back to its ship of origin. The acoustic pulse is emitted by a transducer which receives the return echo as well. The depth measurement is calculated by multiplying the speed of sound in water(averaging 1,500 meters per second) by the time between emission and echo return.

The value of underwater acoustics to the fishing industry has led to the development of other acoustic instruments that operate in a similar fashion to echo-sounders but, because their function is slightly different from the initial model of the echo-sounder, have been given different terms.

The net sounder is an echo sounder with a transducer mounted on the headline of the net rather than on the bottom of the vessel. Nevertheless, to accommodate the distance from the transducer to the display unit, which is much greater than in a normal echo-sounder, several refinements have to be made. Two main types are available. The first is the cable type in which the signals are sent along a cable. In this case there has to be the provision of a cable drum on which to haul, shoot and stow the cable during the different phases of the operation. The second type is the cable-less net-sounder – such as Marport's Trawl Explorer – in which the signals are sent acoustically between the net and hull mounted receiver-hydrophone on the vessel. In this case no cable drum is required but sophisticated electronics are needed at the transducer and receiver.

The display on a net sounder shows the distance of the net from the bottom (or the surface), rather than the depth of water as with the echo-sounder's hull-mounted transducer. Fixed to the headline of the net, the footrope can usually be seen which gives an indication of the net performance. Any fish passing into the net can also be seen, allowing fine adjustments to be made to catch the most fish possible. In other fisheries, where the amount of fish in the net is important, catch sensor transducers are mounted at various positions on the cod-end of the net. As the cod-end fills up these catch sensor transducers are triggered one by one and this information is transmitted acoustically to display monitors on the bridge of the vessel. The skipper can then decide when to haul the net.

Modern versions of the net sounder, using multiple element transducers, function more like a sonar than an echo sounder and show slices of the area in front of the net and not merely the vertical view that the initial net sounders used.

The sonar is an echo-sounder with a directional capability that can show fish or other objects around the vessel.

Small sonars have been fitted to remotely operated vehicles (ROVs) and unmanned underwater vehicles (UUVs) to allow their operation in murky conditions. These sonars are used for looking ahead of the vehicle. The Long-Term Mine Reconnaissance System is a UUV for MCM purposes.

Sonars which act as beacons are fitted to aircraft to allow their location in the event of a crash in the sea. Short and long baseline sonars may be used for caring out the location, such as LBL.

In 2013 an inventor in the United States unveiled a "spider-sense" bodysuit, equipped with ultrasonic sensors and haptic feedback systems, which alerts the wearer of incoming threats; allowing them to respond to attackers even when blindfolded.

Detection of fish, and other marine and aquatic life, and estimation their individual sizes or total biomass using active sonar techniques. As the sound pulse travels through water it encounters objects that are of different density or acoustic characteristics than the surrounding medium, such as fish, that reflect sound back toward the sound source. These echoes provide information on fish size, location, abundance and behavior. Data is usually processed and analysed using a variety of software such as "Echoview".

An upward looking echo sounder mounted on the bottom or on a platform may be used to make measurements of wave height and period. From this statistics of the surface conditions at a location can be derived.

Special short range sonars have been developed to allow measurements of water velocity.

Sonars have been developed that can be used to characterise the sea bottom into, for example, mud, sand, and gravel. Relatively simple sonars such as echo sounders can be promoted to seafloor classification systems via add-on modules, converting echo parameters into sediment type. Different algorithms exist, but they are all based on changes in the energy or shape of the reflected sounder pings. Advanced substrate classification analysis can be achieved using calibrated (scientific) echosounders and parametric or fuzzy-logic analysis of the acoustic data.

Side-scan sonars can be used to derive maps of seafloor topography (bathymetry) by moving the sonar across it just above the bottom. Low frequency sonars such as GLORIA have been used for continental shelf wide surveys while high frequency sonars are used for more detailed surveys of smaller areas.

Powerful low frequency echo-sounders have been developed for providing profiles of the upper layers of the ocean bottom.

Gas bubbles can leak from the seabed, or close to it, from multiple sources. These can be detected by both passive and active sonar (shown in schematic figure by yellow and red systems respectively). Natural seeps of methane and carbon dioxide occur. Gas pipelines can leak, and it is important to be able to detect whether leakage occurs from Carbon Capture and Storage Facilities (CCSFs; e.g. depleted oil wells into which extracted atmospheric carbon is stored). Quantification of the amount of gas leaking is difficult, and although estimates can be made use active and passive sonar, it is important to question their accuracy because of the assumptions inherent in making such estimations from sonar data.

Various synthetic aperture sonars have been built in the laboratory and some have entered use in mine-hunting and search systems. An explanation of their operation is given in synthetic aperture sonar.

Parametric sources use the non-linearity of water to generate the difference frequency between two high frequencies. A virtual end-fire array is formed. Such a projector has advantages of broad bandwidth, narrow beamwidth, and when fully developed and carefully measured it has no obvious sidelobes: see Parametric array. Its major disadvantage is very low efficiency of only a few percent. P.J. Westervelt summarizes the trends involved.

Use of both passive and active sonar has been proposed for various extraterrestrial uses. An example of the use of active sonar is in determining the depth of hydrocarbon seas on Titan, An example of the use of passive sonar is in the detection of methanefalls on Titan,

It has been noted that those proposals which suggest use of sonar without taking proper account of the difference between the Earthly (atmosphere, ocean, mineral) environments and the extraterrestrial ones, can lead to erroneous values

Research has shown that use of active sonar can lead to mass strandings of marine mammals. Beaked whales, the most common casualty of the strandings, have been shown to be highly sensitive to mid-frequency active sonar. Other marine mammals such as the blue whale also flee away from the source of the sonar, while naval activity was suggested to be the most probable cause of a mass stranding of dolphins. The US Navy, which part-funded some of the studies, said that the findings only showed behavioural responses to sonar, not actual harm, but they "will evaluate the effectiveness of [their] marine mammal protective measures in light of new research findings". A 2008 US Supreme Court ruling on the use of sonar by the US Navy noted that there had been no cases where sonar had been conclusively shown to have harmed or killed a marine mammal.
Some marine animals, such as whales and dolphins, use echolocation systems, sometimes called "biosonar" to locate predators and prey. Research on the effects of sonar on blue whales in the Southern California Bight shows that mid-frequency sonar use disrupts the whales' feeding behavior. This indicates that sonar-induced disruption of feeding and displacement from high-quality prey patches could have significant and previously undocumented impacts on baleen whale foraging ecology, individual fitness and population health.

A review of evidence on the mass strandings of beaked whale linked to naval exercises where sonar was used was published in 2019. It concluded that the effects of mid-frequency active sonar are strongest on Cuvier's beaked whales but vary among individuals or populations. The review suggested the strength of response of individual animals may depend on whether they had prior exposure to sonar, and that symptoms of decompression sickness have been found in stranded whales that may be a result of such response to sonar. It noted that in the Canary Islands where multiple strandings had been previously reported, no more mass strandings had occurred once naval exercises during which sonar was used were banned in the area, and recommended that the ban be extended to other areas where mass strandings continue to occur.

High-intensity sonar sounds can create a small temporary shift in the hearing threshold of some fish.

The frequencies of sonars range from infrasonic to above a megahertz. Generally, the lower frequencies have longer range, while the higher frequencies offer better resolution, and smaller size for a given directionality.

To achieve reasonable directionality, frequencies below 1 kHz generally require large size, usually achieved as towed arrays.

Low frequency sonars are loosely defined as 1–5 kHz, albeit some navies regard 5–7 kHz also as low frequency. Medium frequency is defined as 5–15 kHz. Another style of division considers low frequency to be under 1 kHz, and medium frequency at between 1–10 kHz.

American World War II era sonars operated at a relatively high frequency of 20–30 kHz, to achieve directionality with reasonably small transducers, with typical maximum operational range of 2500 yd. Postwar sonars used lower frequencies to achieve longer range; e.g. SQS-4 operated at 10 kHz with range up to 5000 yd. SQS-26 and SQS-53 operated at 3 kHz with range up to 20,000 yd; their domes had size of approx. a 60-ft personnel boat, an upper size limit for conventional hull sonars. Achieving larger sizes by conformal sonar array spread over the hull has not been effective so far, for lower frequencies linear or towed arrays are therefore used.

Japanese WW2 sonars operated at a range of frequencies. The Type 91, with 30 inch quartz projector, worked at 9 kHz. The Type 93, with smaller quartz projectors, operated at 17.5 kHz (model 5 at 16 or 19 kHz magnetostrictive) at powers between 1.7 and 2.5 kilowatts, with range of up to 6 km. The later Type 3, with German-design magnetostrictive transducers, operated at 13, 14.5, 16, or 20 kHz (by model), using twin transducers (except model 1 which had three single ones), at 0.2 to 2.5 kilowatts. The simple type used 14.5 kHz magnetostrictive transducers at 0.25 kW, driven by capacitive discharge instead of oscillators, with range up to 2.5 km.

The sonar's resolution is angular; objects further apart are imaged with lower resolutions than nearby ones.

Another source lists ranges and resolutions vs frequencies for sidescan sonars. 30 kHz provides low resolution with range of 1000–6000 m, 100 kHz gives medium resolution at 500–1000 m, 300 kHz gives high resolution at 150–500 m, and 600 kHz gives high resolution at 75–150 m. Longer range sonars are more adversely affected by nonhomogenities of water. Some environments, typically shallow waters near the coasts, have complicated terrain with many features; higher frequencies become necessary there.






</doc>
<doc id="29440" url="https://en.wikipedia.org/wiki?curid=29440" title="Slavs">
Slavs

Slavs or Slavic people are peoples who speak the various Slavic languages of the larger Balto-Slavic linguistic group of the Indo-European language family. They are native to Eurasia, stretching from Central, Eastern and Southeastern Europe all the way north and eastwards to Northeast Europe, Northern Asia (Siberia) and Central Asia (especially Kazakhstan and Turkmenistan), as well as historically in Western Europe (particularly in Eastern Germany) and Western Asia (including Anatolia). From the early 6th century they spread to inhabit most of Central, Eastern and Southeastern Europe. Today, there is a large Slavic diaspora throughout North America, particularly in the United States and Canada as a result of immigration.

Slavs are the largest ethno-linguistic group in Europe, followed by Germanic peoples and Romance peoples. Present-day Slavic people are classified into East Slavs (chiefly Belarusians, Russians, Rusyns, and Ukrainians), West Slavs (chiefly Czechs, Kashubs, Poles, Slovaks and Sorbs) and South Slavs (chiefly Bosniaks, Bulgarians, Croats, Macedonians, Montenegrins, Serbs and Slovenes).

Most Slavs are traditionally Christians. Orthodox Christianity is practiced by the majority of Slavs. The Orthodox Slavs include the Belarusians, Bulgarians, Macedonians, Montenegrins, Russians, Rusyns, Serbs, and Ukrainians and are defined by Orthodox customs and Cyrillic script, as well as their cultural connection to the Byzantine Empire (Montenegrins and Serbs also use Latin script on equal terms). 

The second most common type of Christianity among the Slavs is Catholicism. The Catholic Slavs include Croats, Czechs, Kashubs, Poles, Silesians, Slovaks, Slovenes and Sorbs and are defined by their Latinate influence and heritage and connection to Western Europe. There are also substantial Protestant and Lutheran minorities, especially among the West Slavs, such as the historical Bohemian (Czech) Hussites.

The second-largest religion among the Slavs after Christianity is Islam. Muslim Slavs include the Bosniaks, Pomaks (Bulgarian Muslims), Gorani, Torbeši (Macedonian Muslims) and other Muslims of the former Yugoslavia.

Modern Slavic nations and ethnic groups are considerably diverse both genetically and culturally, and relations between them – even within the individual groups – range from "ethnic solidarity to mutual feelings of hostility".

The oldest mention of the Slavic ethnonym is the 6th century AD Procopius, writing in Byzantine Greek, using various forms such as "Sklaboi" (), "Sklabēnoi" (), "Sklauenoi" (), "Sthlabenoi" (), or "Sklabinoi" (), while his contemporary Jordanes refers to the in Latin. The oldest documents written in Old Church Slavonic, dating from the 9th century, attest the autonym as "Slověne" (). These forms point back to a Slavic autonym which can be reconstructed in Proto-Slavic as , plural "Slověne".

The reconstructed autonym is usually considered a derivation from "slovo" ("word"), originally denoting "people who speak (the same language)", i. e. people who understand each other, in contrast to the Slavic word denoting German people, namely , meaning "silent, mute people" (from Slavic "mute, mumbling"). The word "slovo" ("word") and the related "slava" ("glory, fame") and "slukh" ("hearing") originate from the Proto-Indo-European root ("be spoken of, glory"), cognate with Ancient Greek ( "fame"), as in the name Pericles, Latin ("be called"), and English .

Ancient Roman sources refer to the Early Slavic peoples as Veneti, who dwelt in a region of central Europe east of the Germanic tribe of Suebi, and west of the Iranian Sarmatians in the 1st and 2nd centuries AD. The Slavs under name of the "Antes" and the "Sclaveni" first appear in Byzantine records in the early 6th century. Byzantine historiographers under emperor Justinian I (527–565), such as Procopius of Caesarea, Jordanes and Theophylact Simocatta describe tribes of these names emerging from the area of the Carpathian Mountains, the lower Danube and the Black Sea, invading the Danubian provinces of the Eastern Empire.

Jordanes, in his work "Getica" (written in 551 AD), describes the Veneti as a "populous nation" whose dwellings begin at the sources of the Vistula and occupy "a great expanse of land". He also describes the Veneti as the ancestors of Antes and Slaveni, two early Slavic tribes, who appeared on the Byzantine frontier in the early 6th century. Procopius wrote in 545 that "the Sclaveni and the Antae actually had a single name in the remote past; for they were both called "Sporoi" in olden times". The name "Sporoi" derives from Greek σπείρω ("I scatter grain"). He described them as barbarians, who lived under democracy, believed in one god, "the maker of lightning" (Perun), to whom they made sacrifice. They lived in scattered housing, and constantly changed settlement. In war, they were mainly foot soldiers with small shields and spears, lightly clothed, some entering battle naked with only genitals covered. Their language is "barbarous" (that is, not Greek), and the two tribes are alike in appearance, being tall and robust, "while their bodies and hair are neither very fair or blond, nor indeed do they incline entirely to the dark type, but they are all slightly ruddy in color. And they live a hard life, giving no heed to bodily comforts..." Jordanes described the Sclaveni having swamps and forests for their cities. Another 6th-century source refers to them living among nearly impenetrable forests, rivers, lakes, and marshes.

Menander Protector mentions a Daurentius (circa 577–579) who slew an Avar envoy of Khagan Bayan I for asking the Slavs to accept the suzerainty of the Avars; Daurentius declined and is reported as saying: "Others do not conquer our land, we conquer theirs – so it shall always be for us".

According to eastern homeland theory, prior to becoming known to the Roman world, Slavic-speaking tribes were part of the many multi-ethnic confederacies of Eurasia – such as the Sarmatian, Hun and Gothic empires. The Slavs emerged from obscurity when the westward movement of Germanic tribes in the 5th and 6th centuries CE (thought to be in conjunction with the movement of peoples from Siberia and Eastern Europe: Huns, and later Avars and Bulgars) started the great migration of the Slavs, who settled the lands abandoned by Germanic tribes fleeing the Huns and their allies: westward into the country between the Oder and the Elbe-Saale line; southward into Bohemia, Moravia, much of present-day Austria, the Pannonian plain and the Balkans; and northward along the upper Dnieper river. It has also been suggested that some Slavs migrated with the Vandals to the Iberian Peninsula and even North Africa.

Around the 6th century, Slavs appeared on Byzantine borders in great numbers. Byzantine records note that Slav numbers were so great, that grass would not regrow where the Slavs had marched through. After a military movement even the Peloponnese and Asia Minor were reported to have Slavic settlements. This southern movement has traditionally been seen as an invasive expansion. By the end of the 6th century, Slavs had settled the Eastern Alps regions.

When Slav migrations ended, their first state organizations appeared, each headed by a prince with a treasury and a defense force. In the 7th century, the Frankish merchant Samo supported the Slavs against their Avar rulers, and became the ruler of the first known Slav state in Central Europe, Samo's Empire. This early Slavic polity probably did not outlive its founder and ruler, but it was the foundation for later West Slavic states on its territory. The oldest of them was Carantania; others are the Principality of Nitra, the Moravian principality (see under Great Moravia) and the Balaton Principality. The First Bulgarian Empire was founded in 681 as an alliance between the ruling Bulgars and the numerous slavs in the area, and their South Slavic language, the Old Church Slavonic, became the main and official language of the empire in 864. Bulgaria was instrumental in the spread of Slavic literacy and Christianity to the rest of the Slavic world. The expansion of the Magyars into the Carpathian Basin and the Germanization of Austria gradually separated the South Slavs from the West and East Slavs. Later Slavic states, which formed in the following centuries, included the Kievan Rus', the Second Bulgarian Empire, the Kingdom of Poland, Duchy of Bohemia, the Kingdom of Croatia, Banate of Bosnia and the Serbian Empire.

In late 19th century, there were only four Slavic states in the world: the Russian Empire, the Principality of Serbia, the Principality of Montenegro and the Principality of Bulgaria. In the Austro-Hungarian Empire, out of approximately 50 million people, about 23 million were Slavs. The Slavic peoples who were, for the most part, denied a voice in the affairs of Austria-Hungary, called for national self-determination. Because of the vastness and diversity of the territory occupied by Slavic people, there were several centers of Slavic consolidation. At the beginning of the 20th century, following the end of World War I and the collapse of the Central Powers, several Slavic nations re-emerged and became independent, such as the Second Polish Republic, First Czechoslovak Republic, and the Kingdom of Yugoslavia (officially named Kingdom of Serbs, Croats and Slovenes until 1929). After the end of the Cold War and subsequent collapse of the Soviet Union, Czechoslovakia, and Yugoslavia, additional new Slavic states emerged, such as the Czech Republic, Slovakia, Slovenia, Croatia, Bosnia and Herzegovina, Serbia, Montenegro, North Macedonia, Belarus and Ukraine.

Pan-Slavism, a movement which came into prominence in the mid-19th century, emphasized the common heritage and unity of all the Slavic peoples. The main focus was in the Balkans where the South Slavs had been ruled for centuries by other empires: the Byzantine Empire, Austria-Hungary, the Ottoman Empire, and Venice.

Proto-Slavic, the supposed ancestor language of all Slavic languages, is a descendant of common Proto-Indo-European, via a Balto-Slavic stage in which it developed numerous lexical and morphophonological isoglosses with the Baltic languages. In the framework of the Kurgan hypothesis, "the Indo-Europeans who remained after the migrations [from the steppe] became speakers of Balto-Slavic". Proto-Slavic is defined as the last stage of the language preceding the geographical split of the historical Slavic languages. That language was uniform, and on the basis of borrowings from foreign languages and Slavic borrowings into other languages, cannot be said to have any recognizable dialects – this suggests that there was, at one time, a relatively small Proto-Slavic homeland.

Slavic linguistic unity was to some extent visible as late as Old Church Slavonic (or Old Bulgarian) manuscripts which, though based on local Slavic speech of Thessaloniki, could still serve the purpose of the first common Slavic literary language. Slavic studies began as an almost exclusively linguistic and philological enterprise. As early as 1833, Slavic languages were recognized as Indo-European.

Standardised Slavic languages that have official status in at least one country are: Belarusian, Bosnian, Bulgarian, Croatian, Czech, Macedonian, Montenegrin, Polish, Russian, Serbian, Slovak, Slovene, and Ukrainian.

The alphabets used for Slavic languages are frequently connected to the dominant religion among the respective ethnic groups. Orthodox Christians use the Cyrillic alphabet while Catholics use the Latin alphabet; the Bosniaks, who are Muslim, also use the Latin alphabet. Additionally, some Eastern Catholics and Western Catholics use the Cyrillic alphabet. Serbian and Montenegrin use both the Cyrillic and Latin alphabets. There is also a Latin script to write in Belarusian, called Łacinka.

Slavs are customarily divided along geographical lines into three major subgroups: West Slavs, East Slavs, and South Slavs, each with a different and a diverse background based on unique history, religion and culture of particular Slavic groups within them. Apart from prehistorical archaeological cultures, the subgroups have had notable cultural contact with non-Slavic Bronze- and Iron Age civilisations. Modern Slavic nations and ethnic groups are considerably diverse both genetically and culturally, and relations between them – even within the individual ethnic groups themselves – are varied, ranging from a sense of connection to mutual feelings of hostility.

West Slavs originate from early Slavic tribes which settled in Central Europe after the East Germanic tribes had left this area during the migration period. They are noted as having mixed with Germanics, Hungarians, Celts (particularly the Boii), Old Prussians, and the Pannonian Avars. The West Slavs came under the influence of the Western Roman Empire (Latin) and of the Catholic Church.

East Slavs have origins in early Slavic tribes who mixed and contacted with Finno-Ugric peoples and Balts. Their early Slavic component, Antes, mixed or absorbed Iranians, and later received influence from the Khazars and Vikings. The East Slavs trace their national origins to the tribal unions of Kievan Rus' and Rus' Khaganate, beginning in the 10th century. They came particularly under the influence of the Byzantine Empire and of the Eastern Orthodox Church.

South Slavs from most of the region have origins in early Slavic tribes who mixed with the local Proto-Balkanic tribes (Illyrian, Dacian, Thracian, Paeonian, Hellenic tribes), and Celtic tribes (particularly the Scordisci), as well as with Romans (and the Romanized remnants of the former groups), and also with remnants of temporarily settled invading East Germanic, Asiatic or Caucasian tribes such as Gepids, Huns, Avars, Goths and Bulgars. The original inhabitants of present-day Slovenia and continental Croatia have origins in early Slavic tribes who mixed with Romans and romanized Celtic and Illyrian people as well as with Avars and Germanic peoples (Lombards and East Goths). The South Slavs (except the Slovenes and Croats) came under the cultural sphere of the Eastern Roman Empire (Byzantine Empire), of the Ottoman Empire and of the Eastern Orthodox Church and Islam, while the Slovenes and the Croats were influenced by the Western Roman Empire (Latin) and thus by the Catholic Church in a similar fashion to that of the West Slavs.

The pagan Slavic populations were Christianized between the 7th and 12th centuries. Orthodox Christianity is predominant among East and South Slavs, while Catholicism is predominant among West Slavs and some western South Slavs. The religious borders are largely comparable to the East–West Schism which began in the 11th century.

The majority of contemporary Slavic populations who profess a religion are Orthodox, followed by Catholic, while a small minority are Protestant. There are minor Slavic Muslim groups. Religious delineations by nationality can be very sharp; usually in the Slavic ethnic groups the vast majority of religious people share the same religion. In the Czech Republic 75% had no stated religion according to the 2011 census.

Mainly Eastern Orthodoxy:

Mainly Catholicism:

Mainly Islam:

Throughout their history, Slavs came into contact with non-Slavic groups. In the postulated homeland region (present-day Ukraine), they had contacts with the Iranian Sarmatians and the Germanic Goths. After their subsequent spread, the Slavs began assimilating non-Slavic peoples. For example, in the Balkans, there were Paleo-Balkan peoples, such as Romanized and Hellenized (Jireček Line) Illyrians, Thracians and Dacians, as well as Greeks and Celtic Scordisci and Serdi. Because Slavs were so numerous, most indigenous populations of the Balkans were Slavicized. Thracians and Illyrians mixed as ethnic groups in this period. A notable exception is Greece, where Slavs were Hellenized because Greeks were more numerous, especially with more Greeks returning to Greece in the 9th century and the influence of the church and administration, however, Slavicized regions within Macedonia, Thrace and Moesia Inferior also had a larger portion of locals compared to migrating Slavs. Other notable exceptions are the territory of present-day Romania and Hungary, where Slavs settled en route to present-day Greece, North Macedonia, Bulgaria and East Thrace but assimilated, and the modern Albanian nation which claims descent from Illyrians and other Balkan tribes.

Ruling status of Bulgars and their control of land cast the nominal legacy of the Bulgarian country and people onto future generations, but Bulgars were gradually also Slavicized into the present day South Slavic ethnic group known as Bulgarians. The Romance speakers within the fortified Dalmatian cities retained their culture and language for a long time. Dalmatian Romance was spoken until the high Middle Ages, but, they too were eventually assimilated into the body of Slavs.

In the Western Balkans, South Slavs and Germanic Gepids intermarried with invaders, eventually producing a Slavicized population. In Central Europe, the West Slavs intermixed with Germanic, Hungarian, and Celtic peoples, while in Eastern Europe the East Slavs had encountered Finnic and Scandinavian peoples. Scandinavians (Varangians) and Finnic peoples were involved in the early formation of the Rus' state but were completely Slavicized after a century. Some Finno-Ugric tribes in the north were also absorbed into the expanding Rus population. In the 11th and 12th centuries, constant incursions by nomadic Turkic tribes, such as the Kipchak and the Pecheneg, caused a massive migration of East Slavic populations to the safer, heavily forested regions of the north. In the Middle Ages, groups of Saxon ore miners settled in medieval Bosnia, Serbia and Bulgaria, where they were Slavicized.

"Saqaliba" refers to the Slavic mercenaries and slaves in the medieval Arab world in North Africa, Sicily and Al-Andalus. Saqaliba served as caliph's guards. In the 12th century, Slavic piracy in the Baltics increased. The Wendish Crusade was started against the Polabian Slavs in 1147, as a part of the Northern Crusades. The pagan chief of the Slavic Obodrite tribes, Niklot, began his open resistance when Lothar III, Holy Roman Emperor, invaded Slavic lands. In August 1160 Niklot was killed, and German colonization ("Ostsiedlung") of the Elbe-Oder region began. In Hanoverian Wendland, Mecklenburg-Vorpommern and Lusatia, invaders started germanization. Early forms of germanization were described by German monks: Helmold in the manuscript "Chronicon Slavorum" and Adam of Bremen in "Gesta Hammaburgensis ecclesiae pontificum." The Polabian language survived until the beginning of the 19th century in what is now the German state of Lower Saxony. In Eastern Germany, around 20% of Germans have historic Slavic paternal ancestry, as revealed in Y-DNA testing. Similarly, in Germany, around 20% of the foreign surnames are of Slavic origin.

Cossacks, although Slavic-speaking and practicing Orthodox Christianity, came from a mix of ethnic backgrounds, including Tatars and other Turks. Many early members of the Terek Cossacks were Ossetians. The Gorals of southern Poland and northern Slovakia are partially descended from Romance-speaking Vlachs, who migrated into the region from the 14th to 17th centuries and were absorbed into the local population. The population of Moravian Wallachia also descended from the Vlachs. Conversely, some Slavs were assimilated into other populations. Although the majority continued towards Southeast Europe, attracted by the riches of the area that became the state of Bulgaria, a few remained in the Carpathian Basin in Central Europe, and were assimilated into the Magyar people. Numerous river and other place names in Romania have Slavic origin.

There are an estimated 300—360 million Slavs worldwide.





</doc>
<doc id="29441" url="https://en.wikipedia.org/wiki?curid=29441" title="Skylab">
Skylab

Skylab was the first United States space station, launched by NASA, occupied for about 24 weeks between May 1973 and February 1974. It was operated by three separate three-astronaut crews: SL-2, SL-3 and SL-4. Major operations included an orbital workshop, a solar observatory, Earth observation, and hundreds of experiments.

Unable to be re-boosted by the Space Shuttle, which was not ready until the early 1980s, Skylab's orbit decayed and it disintegrated in the atmosphere on July 11, 1979, scattering debris across the Indian Ocean and Western Australia.

 Skylab was the only space station operated exclusively by the United States. A permanent US station was planned starting in 1969, but funding for this was canceled and replaced with US participation in an International Space Station in 1993.

Skylab had a weight of with a Apollo command and service module (CSM) attached and included a workshop, a solar observatory, and several hundred life science and physical science experiments. It was launched uncrewed into low Earth orbit by a Saturn V rocket modified into the Saturn INT-21, with the S-IVB third stage not available for propulsion because the orbital workshop was built out of it. This was the final flight for the rocket more commonly known for carrying the crewed Apollo Moon landing missions. Three subsequent missions delivered three-astronaut crews in the Apollo CSM launched by the smaller Saturn IB rocket. For the final two crewed missions to Skylab, NASA assembled a backup Apollo CSM/Saturn IB in case an in-orbit rescue mission was needed, but this vehicle was never flown. The station was damaged during launch when the micrometeoroid shield tore away from the workshop, taking one of the main solar panel arrays with it and jamming the other main array. This deprived Skylab of most of its electrical power and also removed protection from intense solar heating, threatening to make it unusable. The first crew deployed a replacement heat shade and freed the jammed solar panels to save Skylab. This was the first time that a repair of this magnitude was performed in space.

Skylab included the Apollo Telescope Mount (a multi-spectral solar observatory), a multiple docking adapter with two docking ports, an airlock module with extravehicular activity (EVA) hatches, and the orbital workshop, the main habitable space inside Skylab. Electrical power came from solar arrays and fuel cells in the docked Apollo CSM. The rear of the station included a large waste tank, propellant tanks for maneuvering jets, and a heat radiator. Astronauts conducted numerous experiments aboard Skylab during its operational life. The telescope significantly advanced solar science, and observation of the Sun was unprecedented. Astronauts took thousands of photographs of Earth, and the Earth Resources Experiment Package (EREP) viewed Earth with sensors that recorded data in the visible, infrared, and microwave spectral regions. The record for human time spent in orbit was extended beyond the 23 days set by the Soyuz 11 crew aboard Salyut 1 to 84 days by the Skylab 4 crew.

Later plans to reuse Skylab were stymied by delays in development of the Space Shuttle, and Skylab's decaying orbit could not be stopped. Skylab's atmospheric reentry began on July 11, 1979, amid worldwide media attention. Before re-entry, NASA ground controllers tried to adjust Skylab's orbit to minimize the risk of debris landing in populated areas, targeting the south Indian Ocean, which was partially successful. Debris showered Western Australia, and recovered pieces indicated that the station had disintegrated lower than expected. As the Skylab program drew to a close, NASA's focus had shifted to the development of the Space Shuttle. NASA space station and laboratory projects included Spacelab, Shuttle-"Mir", and Space Station "Freedom", which was merged into the International Space Station.

Rocket engineer Wernher von Braun, science fiction writer Arthur C. Clarke, and other early advocates of crewed space travel, expected until the 1960s that a space station would be an important early step in space exploration. Von Braun participated in the publishing of a series of influential articles in "Collier's" magazine from 1952 to 1954, titled "Man Will Conquer Space Soon!". He envisioned a large, circular station 250 feet (75m) in diameter that would rotate to generate artificial gravity and require a fleet of 7,000-ton (6,500-metric ton) space shuttles for construction in orbit. The 80 men aboard the station would include astronomers operating a telescope, meteorologists to forecast the weather, and soldiers to conduct surveillance. Von Braun expected that future expeditions to the Moon and Mars would leave from the station.

The development of the transistor, the solar cell, and telemetry, led in the 1950s and early 1960s to uncrewed satellites that could take photographs of weather patterns or enemy nuclear weapons and send them to Earth. A large station was no longer necessary for such purposes, and the United States Apollo program to send men to the Moon chose a mission mode that would not need in-orbit assembly. A smaller station that a single rocket could launch retained value, however, for scientific purposes.

In 1959, von Braun, head of the Development Operations Division at the Army Ballistic Missile Agency, submitted his final Project Horizon plans to the U.S. Army. The overall goal of Horizon was to place men on the Moon, a mission that would soon be taken over by the rapidly forming NASA. Although concentrating on the Moon missions, von Braun also detailed an orbiting laboratory built out of a Horizon upper stage, an idea used for Skylab. A number of NASA centers studied various space station designs in the early 1960s. Studies generally looked at platforms launched by the Saturn V, followed up by crews launched on Saturn IB using an Apollo command and service module, or a Gemini capsule on a Titan II-C, the latter being much less expensive in the case where cargo was not needed. Proposals ranged from an Apollo-based station with two to three men, or a small "canister" for four men with Gemini capsules resupplying it, to a large, rotating station with 24 men and an operating lifetime of about five years. A proposal to study the use of a Saturn S-IVB as a crewed space laboratory was documented in 1962 by the Douglas Aircraft Company.

The Department of Defense (DoD) and NASA cooperated closely in many areas of space. In September 1963, NASA and the DoD agreed to cooperate in building a space station. The DoD wanted its own crewed facility, however, and in December it announced Manned Orbital Laboratory (MOL), a small space station primarily intended for photo reconnaissance using large telescopes directed by a two-person crew. The station was the same diameter as a Titan II upper stage, and would be launched with the crew riding atop in a modified Gemini capsule with a hatch cut into the heat shield on the bottom of the capsule. MOL competed for funding with a NASA station for the next five years and politicians and other officials often suggested that NASA participate in MOL or use the DoD design. The military project led to changes to the NASA plans so that they would resemble MOL less.

NASA management was concerned about losing the 400,000 workers involved in Apollo after landing on the Moon in 1969. A reason von Braun, head of NASA's Marshall Space Flight Center during the 1960s, advocated for a smaller station after his large one was not built was that he wished to provide his employees with work beyond developing the Saturn rockets, which would be completed relatively early during Project Apollo. NASA set up the "Apollo Logistic Support System Office", originally intended to study various ways to modify the Apollo hardware for scientific missions. The office initially proposed a number of projects for direct scientific study, including an extended-stay lunar mission which required two Saturn V launchers, a "lunar truck" based on the Lunar Module (LEM), a large crewed solar telescope using a LEM as its crew quarters, and small space stations using a variety of LEM or CSM-based hardware. Although it did not look at the space station specifically, over the next two years the office would become increasingly dedicated to this role. In August 1965, the office was renamed, becoming the "Apollo Applications Program" (AAP).

As part of their general work, in August 1964 the Manned Spacecraft Center (MSC) presented studies on an expendable lab known as "Apollo "X"", short for "Apollo Extension System". "Apollo X" would have replaced the LEM carried on the top of the S-IVB stage with a small space station slightly larger than the CSM's service area, containing supplies and experiments for missions between 15 and 45 days' duration. Using this study as a baseline, a number of different mission profiles were looked at over the next six months.

In November 1964, von Braun proposed a more ambitious plan to build a much larger station built from the S-II second stage of a Saturn V. His design replaced the S-IVB third stage with an aeroshell, primarily as an adapter for the CSM on top. Inside the shell was a cylindrical equipment section. On reaching orbit, the S-II second stage would be vented to remove any remaining hydrogen fuel, then the equipment section would be slid into it via a large inspection hatch. This became known as a "wet workshop" concept, because of the conversion of an active fuel tank. The station filled the entire interior of the S-II stage's hydrogen tank, with the equipment section forming a "spine" and living quarters located between it and the walls of the booster. This would have resulted in a very large living area. Power was to be provided by solar cells lining the outside of the S-II stage.

One problem with this proposal was that it required a dedicated Saturn V launch to fly the station. At the time the design was being proposed, it was not known how many of the then-contracted Saturn Vs would be required to achieve a successful Moon landing. However, several planned Earth-orbit test missions for the LEM and CSM had been canceled, leaving a number of Saturn IBs free for use. Further work led to the idea of building a smaller "wet workshop" based on the S-IVB, launched as the second stage of a Saturn IB.

A number of S-IVB-based stations were studied at MSC from mid-1965, which had much in common with the Skylab design that eventually flew. An airlock would be attached to the hydrogen tank, in the area designed to hold the LEM, and a minimum amount of equipment would be installed in the tank itself in order to avoid taking up too much fuel volume. Floors of the station would be made from an open metal framework that allowed the fuel to flow through it. After launch, a follow-up mission launched by a Saturn IB would launch additional equipment, including solar panels, an equipment section and docking adapter, and various experiments. Douglas Aircraft, builder of the S-IVB stage, was asked to prepare proposals along these lines. The company had for several years been proposing stations based on the S-IV stage, before it was replaced by the S-IVB.
On April 1, 1966, MSC sent out contracts to Douglas, Grumman, and McDonnell for the conversion of a S-IVB spent stage, under the name "Saturn S-IVB spent-stage experiment support module" (SSESM). In May, astronauts voiced concerns over the purging of the stage's hydrogen tank in space. Nevertheless, in late July it was announced that the Orbital Workshop would be launched as a part of Apollo mission AS-209, originally one of the Earth-orbit CSM test launches, followed by two Saturn I/CSM crew launches, AAP-1 and AAP-2.

MOL remained AAP's chief competitor for funds, although the two programs cooperated on technology. NASA considered flying experiments on MOL, or using its Titan IIIC booster instead of the much more expensive Saturn IB. The agency decided that the Air Force station was not large enough, and that converting Apollo hardware for use with Titan would be too slow and too expensive. The DoD later canceled MOL in June 1969.

Design work continued over the next two years, in an era of shrinking budgets. (NASA sought $450 million for Apollo Applications in fiscal year 1967, for example, but received $42 million.) In August 1967, the agency announced that the lunar mapping and base construction missions examined by the AAP were being canceled. Only the Earth-orbiting missions remained, namely the Orbital Workshop and Apollo Telescope Mount solar observatory.

The success of Apollo 8 in December 1968, launched on the third flight of a Saturn V, made it likely that one would be available to launch a dry workshop. Later, several Moon missions were canceled as well, originally to be Apollo missions 18 through 20. The cancellation of these missions freed up three Saturn V boosters for the AAP program. Although this would have allowed them to develop von Braun's original S-II based mission, by this time so much work had been done on the S-IV based design that work continued on this baseline. With the extra power available, the wet workshop was no longer needed; the S-IC and S-II lower stages could launch a "dry workshop", with its interior already prepared, directly into orbit.

A dry workshop simplified plans for the interior of the station. Industrial design firm Raymond Loewy/William Snaith recommended emphasizing habitability and comfort for the astronauts by providing a wardroom for meals and relaxation and a window to view Earth and space, although astronauts were dubious about the designers' focus on details such as color schemes. Habitability had not previously been an area of concern when building spacecraft due to their small size and brief mission durations, but the Skylab missions would last for months. NASA sent a scientist on Jacques Piccard's "Ben Franklin" submarine in the Gulf Stream in July and August 1969 to learn how six people would live in an enclosed space for four weeks.

Astronauts were uninterested in watching movies on a proposed entertainment center or in playing games, but they did want books and individual music choices. Food was also important; early Apollo crews complained about its quality, and a NASA volunteer found it intolerable to live on the Apollo food for four days on Earth. Its taste and composition were unpleasant, in the form of cubes and squeeze tubes. Skylab food significantly improved on its predecessors by prioritizing edibility over scientific needs.

Each astronaut had a private sleeping area the size of a small walk-in closet, with a curtain, sleeping bag, and locker. Designers also added a shower and a toilet for comfort and to obtain precise urine and feces samples for examination on Earth. The waste samples were so important that they would have been priorities in any rescue mission.

Skylab did not have recycling systems such as conversion of urine to drinking water; it also did not dispose of waste by dumping it into space. The S-IVB's liquid oxygen tank below the OWS was used to store trash and waste water, passed through an airlock.

On August 8, 1969, the McDonnell Douglas Corporation received a contract for the conversion of two existing S-IVB stages to the Orbital Workshop configuration. One of the S-IV test stages was shipped to McDonnell Douglas for the construction of a mock-up in January 1970. The Orbital Workshop was renamed "Skylab" in February 1970 as a result of a NASA contest. The actual stage that flew was the upper stage of the AS-212 rocket (the S-IVB stage, S-IVB 212). The mission computer used aboard Skylab was the IBM System/4Pi TC-1, a relative of the AP-101 Space Shuttle computers. The Saturn V with serial number SA-513, originally produced for the Apollo program—before the cancellation of Apollo 18, 19, and 20—was repurposed and redesigned to launch Skylab. The Saturn V's third stage was removed and replaced with Skylab, but with the controlling Instrument Unit remaining in its standard position.

Skylab was launched on May 14, 1973 by the modified Saturn V. The launch is sometimes referred to as Skylab 1, or SL-1. Severe damage was sustained during launch and deployment, including the loss of the station's micrometeoroid shield/sun shade and one of its main solar panels. Debris from the lost micrometeoroid shield further complicated matters by becoming tangled in the remaining solar panel, preventing its full deployment and thus leaving the station with a huge power deficit.

Immediately following Skylab's launch, Pad A at Kennedy Space Center Launch Complex 39 was deactivated, and construction proceeded to modify it for the Space Shuttle program, originally targeting a maiden launch in March 1979. The crewed missions to Skylab would occur using a Saturn IB rocket from Launch Pad 39B.

SL-1 was the last uncrewed launch from LC-39A until February 19, 2017, when SpaceX CRS-10 was launched from there.

Three crewed missions, designated SL-2, SL-3 and SL-4, were made to Skylab in the Apollo command and service modules. The first crewed mission, SL-2, launched on May 25, 1973 atop a Saturn IB and involved extensive repairs to the station. The crew deployed a parasol-like sunshade through a small instrument port from the inside of the station, bringing station temperatures down to acceptable levels and preventing overheating that would have melted the plastic insulation inside the station and released poisonous gases. This solution was designed by NASA's "Mr. Fix It" Jack Kinzler, who won the NASA Distinguished Service Medal for his efforts. The crew conducted further repairs via two spacewalks (extra-vehicular activity, or EVA). The crew stayed in orbit with Skylab for 28 days. Two additional missions followed, with the launch dates of July 28, 1973 (SL-3) and November 16, 1973 (SL-4), and mission durations of 59 and 84 days, respectively. The last Skylab crew returned to Earth on February 8, 1974.

In addition to the three crewed missions, there was a rescue mission on standby that had a crew of two, but could take five back down.


Also of note was the three-man crew of Skylab Medical Experiment Altitude Test, who spent 56 days in 1972 at low-pressure on Earth to evaluate medical experiment equipment. This was a spaceflight analog test in full gravity, but Skylab hardware was tested and medical knowledge was gained.

Skylab orbited Earth 2,476 times during the 171 days and 13 hours of its occupation during the three crewed Skylab expeditions. Each of these extended the human record of 23 days for amount of time spent in space set by the Soviet Soyuz 11 crew aboard the space station Salyut 1 on June 30, 1971. Skylab 2 lasted 28 days, Skylab 3 56 days, and Skylab 4 84 days. Astronauts performed ten spacewalks, totaling 42 hours and 16 minutes. Skylab logged about 2,000 hours of scientific and medical experiments, 127,000 frames of film of the Sun and 46,000 of Earth. Solar experiments included photographs of eight solar flares, and produced valuable results that scientists stated would have been impossible to obtain with uncrewed spacecraft. The existence of the Sun's coronal holes were confirmed because of these efforts. Many of the experiments conducted investigated the astronauts' adaptation to extended periods of microgravity.

A typical day began at 6 a.m. Central Time Zone. Although the toilet was small and noisy, both veteran astronauts—who had endured earlier missions' rudimentary waste-collection systems—and rookies complimented it. The first crew enjoyed taking a shower once a week, but found drying themselves in weightlessness and vacuuming excess water difficult; later crews usually cleaned themselves daily with wet washcloths instead of using the shower. Astronauts also found that bending over in weightlessness to put on socks or tie shoelaces strained their stomach muscles.

Breakfast began at 7 am. Astronauts usually stood to eat, as sitting in microgravity also strained their stomach muscles. They reported that their food—although greatly improved from Apollo—was bland and repetitive, and weightlessness caused utensils, food containers, and bits of food to float away; also, gas in their drinking water contributed to flatulence. After breakfast and preparation for lunch, experiments, tests and repairs of spacecraft systems and, if possible, 90 minutes of physical exercise followed; the station had a bicycle and other equipment, and astronauts could jog around the water tank. After dinner, which was scheduled for 6 pm, crews performed household chores and prepared for the next day's experiments. Following lengthy daily instructions (some of which were up to 15 meters long) sent via teleprinter, the crews were often busy enough to postpone sleep.

The station offered what a later study called "a highly satisfactory living and working environment for crews", with enough room for personal privacy. Although it had a dart set, playing cards, and other recreational equipment in addition to books and music players, the window with its view of Earth became the most popular way to relax in orbit.

Prior to departure about 80 experiments were named, although they are also described as "almost 300 separate investigations". Experiments were divided into six broad categories:

Because the solar scientific airlock—one of two research airlocks—was unexpectedly occupied by the "Parasol" that replaced the missing meteorite shield, a few experiments were instead installed outside with the telescopes during space walks, or shifted to the Earth-facing scientific airlock.

Skylab 2 spent less time than planned on most experiments due to station repairs. On the other hand, Skylab 3 and Skylab 4 far exceeded the initial experiment plans, once the crews adjusted to the environment and established comfortable working relationships with ground control.

The figure (below) lists an overview of most major experiments. Skylab 4 carried out several more experiments, such as to observe Comet Kohoutek.

Riccardo Giacconi shared the 2002 Nobel Prize in Physics for his study of X-ray astronomy, including the study of emissions from the sun onboard Skylab, contributing to the birth of X-ray astronomy.

Skylab had certain features to protect vulnerable technology from radiation. The window was vulnerable to darkening, and this darkening could affect experiment S190. As a result, a light shield that could be open or shut was designed and installed on Skylab. To protect a wide variety of films, used for a variety of experiments and for astronaut photography, there were five film vaults. There were four smaller film vaults in the Multiple Docking Adapter, mainly because the structure could not carry enough weight for a single larger film vault. The orbital workshop could handle a single larger safe, which is also more efficient for shielding. The large vault in the orbital workshop had an empty mass of 2398 lb (1088 kg, 171.3 stones). The four smaller vaults had combined mass of 1545 lb. The primary construction material of all five safes was aluminum. When Skylab re-entered there was one 180 lb chunk of aluminum found that was thought to be a door to one of the film vaults. The big film vault was one of the heaviest single pieces of Skylab to re-enter Earth's atmosphere.

A later example of a radiation vault is the Juno Radiation Vault for the Juno Jupiter orbiter, launched in 2011, which was designed to protect much of the uncrewed spacecraft's electronics, using 1 cm thick walls of titanium.

The Skylab film vault was used for storing film from various sources including the Apollo Telescope Mount solar instruments. Six ATM experiments used film to record data, and over the course of the missions over 150,000 successful exposures were recorded. The film canister had to be manually retrieved on crewed spacewalks to the instruments during the missions. The film canisters were returned to Earth aboard the Apollo capsules when each mission ended, and were among the heaviest items that had to be returned at the end of each mission. The heaviest canisters weighed 40 kg and could hold up to 16,000 frames of film.

There were two types of gyroscopes on Skylab. Control-moment gyroscopes (CMG) could physically move the station, and rate gyroscopes measured the rate of rotation to find its orientation. The CMG helped provide the fine pointing needed by the Apollo Telescope Mount, and to resist various forces that can change the station's orientation.

Some of the forces acting on Skylab that the pointing system needed to resist:

Skylab was the first large spacecraft to use big gyroscopes, capable of controlling its attitude. The control could also be used to help point the instruments. The gyroscopes took about ten hours to get spun up if they were turned off. There was also a thruster system to control Skylab's attitude. There were 9 rate-gyroscope sensors, 3 for each axis. These were sensors that fed their output to the Skylab digital computer. Two of three were active and their input was averaged, while the third was a backup. From NASA SP-400 "Skylab, Our First Space Station", "each Skylab control-moment gyroscope consisted of a motor-driven rotor, electronics assembly, and power inverter assembly. The 21-inch diameter rotor weighed and rotated at approximately 8950 revolutions per minute".

There were three control movement gyroscopes on Skylab, but only two were required to maintain pointing. The control and sensor gyroscopes were part of a system that help detect and control the orientation of the station in space. Other sensors that helped with this were a Sun tracker and a star tracker. The sensors fed data to the main computer, which could then use the control gyroscopes and or the thruster system to keep Skylab pointed as desired.

Skylab had a zero-gravity shower system in the work and experiment section of the Orbital Workshop designed and built at the Manned Spaceflight Center. It had a cylindrical curtain that went from floor to ceiling and a vacuum system to suck away water. The floor of the shower had foot restraints.

To bathe, the user coupled a pressurized bottle of warmed water to the shower's plumbing, then stepped inside and secured the curtain. A push-button shower nozzle was connected by a stiff hose to the top of the shower. The system was designed for about 6 pints (2.8 liters) of water per shower, the water being drawn from the personal hygiene water tank. The use of both the liquid soap and water was carefully planned out, with enough soap and warm water for one shower per week per person.

The first astronaut to use the space shower was Paul J. Weitz on Skylab 2, the first crewed mission. He said, "It took a fair amount longer to use than you might expect, but you come out smelling good". A Skylab shower took about two and a half hours, including the time to set up the shower and dissipate used water. The procedure for operating the shower was as follows:

One of the big concerns with bathing in space was control of droplets of water so that they did not cause an electrical short by floating into the wrong area. The vacuum water system was thus integral to the shower. The vacuum fed to a centrifugal separator, filter, and collection bag to allow the system to vacuum up the fluids. Waste water was injected into a disposal bag which was in turn put in the waste tank. The material for the shower enclosure was fire-proof beta cloth wrapped around hoops of diameter; the top hoop was connected to the ceiling. The shower could be collapsed to the floor when not in use. Skylab also supplied astronauts with rayon terrycloth towels which had a color-coded stitching for each crew-member. There were 420 towels on board Skylab initially.

A simulated Skylab shower was also used during the 56-day SMEAT simulation; the crew used the shower after exercise and found it a positive experience.

There was a variety of hand-held and fixed experiments that used various types of film. In addition to the instruments in the ATM solar observatory, 35 and 70 mm film cameras were carried on board. A TV camera was carried that recorded video electronically. These electronic signals could be recorded to magnetic tape or be transmitted to Earth by radio signal. The TV camera was not a digital camera of the type that became common in the later decades, although Skylab did have a digital computer using microchips on board.

It was determined that film would fog up to due to radiation over the course of the mission. To prevent this film was stored in vaults.

Personal (hand-held) camera equipment:

Film for the DAC was contained in DAC Film Magazines, which contained up to 140 feet (42.7 m) of film. At 24 frames per second this was enough for 4 minutes of filming, with progressively longer film times with lower frame rates such as 16 minutes at 6 frames per second. The film had to be loaded or unloaded from the DAC in a photographic dark room.


Experiment S190B was the Actron Earth Terrain Camera

The S190A was the "Multispectral Photographic Camera "

There was also a Polaroid SX-70 instant camera, and a pair of Leitz Trinovid 10 x 40 binoculars modified for use in space to aid in Earth observations.

The SX-70 was used to take pictures of the Extreme Ultraviolet monitor by Dr. Garriot, as the monitor provided a live video feed of the solar corona in ultraviolet light as observed by Skylab solar observatory instruments located in the Apollo Telescope Mount.

Skylab was controlled in part by a digital computer system, and one of its main jobs was to control the pointing of the station; pointing was especially important for its solar power collection and observatory functions. The computer consisted of two actual computers, a primary and a secondary. The system ran several thousand words of code, which was also backed up on the Memory Load Unit (MLU). The two computers were linked to each other and various input and output items by the workshop computer interface. Operations could be switched from the primary to the backup, which were the same design, either automatically if errors were detected, by the Skylab crew, or from the ground.

The Skylab computer was a space-hardened and customized version of the TC-1 computer, a version of the IBM System/4 Pi, itself based on the System 360 computer. The TC-1 had a 16,000-word memory based on ferrite memory cores, while the MLU was a read-only tape drive that contained a backup of the main computer programs. The tape drive would take 11 seconds to upload the backup of the software program to a main computer. The TC-1 used 16-bit words and the central processor came from the 4Pi computer. There was a 16k and an 8k version of the software program.

The computer had a mass of 100 pounds (45.4 kg), and consumed about ten percent of the station's electrical power.

After launch the computer is what the controllers on the ground communicated with to control the station's orientation. When the sun-shield was torn off the ground staff had to balance solar heating with electrical production. On March 6, 1978 the computer system was re-activated by NASA to control the re-entry.

The system had a user interface which consisted of a display, ten buttons, and a three position switch. Because the numbers were in octal (base-8), it only had numbers zero to seven (8 keys), and the other two keys were enter and clear. The display could show minutes and seconds which would count-down to orbital benchmarks, or it could display keystrokes when using the interface. The interface could be used to change the software program. The user interface was called the Digital Address System (DAS) and could send commands to the computer's command system. The command system could also get commands from the ground.

For personal computing needs Skylab crews were equipped with models of the then new hand-held electronic scientific calculator, which was used in place of slide-rules used on prior space missions as the primary personal computer. The model used was the Hewlett Packard HP 35. Some slide rules continued in use aboard Skylab, and a circular slide rule was at the workstation.

The three crewed Skylab missions used only about 16.8 of the 24-man-months of oxygen, food, water, and other supplies stored aboard Skylab . A fourth crewed mission was under consideration, which would have used the launch vehicle kept on standby for the Skylab Rescue mission. This would have been a 20-day mission to boost Skylab to a higher altitude and do more scientific experiments. Another plan was to use a Teleoperator Retrieval System (TRS) launched aboard the Space Shuttle (then under development), to robotically re-boost the orbit. When Skylab 5 was cancelled, it was expected Skylab would stay in orbit until the 1980s, which was enough time to overlap with the beginning of Shuttle launches. Other options for launching TRS included the Titan III and Atlas Agena. No option received the level of effort and funding needed for execution before Skylab's sooner-than-expected re-entry.

The SL-4 crew left a bag filled with supplies to welcome visitors, and left the hatch unlocked. Skylab's internal systems were evaluated and tested from the ground, and effort was put into plans for re-using it, as late as 1978. NASA discouraged any discussion of additional visits due to the station's age, but in 1977 and 1978, when the agency still believed the Space Shuttle would be ready by 1979, it completed two studies on reusing the station. By September 1978, the agency believed Skylab was safe for crews, with all major systems intact and operational. It still had 180 man-days of water and 420-man-days of oxygen, and astronauts could refill both; the station could hold up to about 600 to 700-man-days of drinkable water and 420-man-days of food. Before SL-4 left they did one more boost, running the Skylab thrusters for 3 minutes which added 11 km in height to its orbit. Skylab was left in a 433 by 455 km orbit on departure. At this time, the NASA-accepted estimate for its re-entry was nine years.

The studies cited several benefits from reusing Skylab, which one called a resource worth "hundreds of millions of dollars" with "unique habitability provisions for long duration space flight". Because no more operational Saturn V rockets were available after the Apollo program, four to five shuttle flights and extensive space architecture would have been needed to build another station as large as Skylab's volume. Its ample size—much greater than that of the shuttle alone, or even the shuttle plus Spacelab—was enough, with some modifications, for up to seven astronauts of both sexes, and experiments needing a long duration in space; even a movie projector for recreation was possible.

Proponents of Skylab's reuse also said repairing and upgrading Skylab would provide information on the results of long-duration exposure to space for future stations. The most serious issue for reactivation was stationkeeping, as one of the station's gyroscopes had failed and the attitude control system needed refueling; these issues would need EVA to fix or replace. The station had not been designed for extensive resupply. However, although it was originally planned that Skylab crews would only perform limited maintenance they successfully made major repairs during EVA, such as the SL-2 crew's deployment of the solar panel and the SL-4 crew's repair of the primary coolant loop. The SL-2 crew fixed one item during EVA by, reportedly, "hit[ting] it with [a] hammer".

Some studies also said, beyond the opportunity for space construction and maintenance experience, reactivating the station would free up shuttle flights for other uses, and reduce the need to modify the shuttle for long-duration missions. Even if the station were not crewed again, went one argument, it might serve as an experimental platform.

The reactivation would likely have occurred in four phases:

The first three phases would have required about $60 million in 1980s dollars, not including launch costs.

Other options for launching TRS were Titan III or Atlas Agena.

After a boost of by SL-4's Apollo CSM before its departure in 1974, Skylab was left in a parking orbit of by that was expected to last until at least the early 1980s, based on estimates of the 11-year sunspot cycle that began in 1976. NASA first considered as early as 1962 the potential risks of a space station reentry, but decided not to incorporate a retrorocket system in Skylab due to cost and acceptable risk.

The spent 49-ton Saturn V S-II stage which had launched Skylab in 1973 remained in orbit for almost two years, and made an uncontrolled reentry on January 11, 1975. 

British mathematician Desmond King-Hele of the Royal Aircraft Establishment predicted in 1973 that Skylab would de-orbit and crash to Earth in 1979, sooner than NASA's forecast, because of increased solar activity. Greater-than-expected solar activity heated the outer layers of Earth's atmosphere and increased drag on Skylab. By late 1977, NORAD also forecast a reentry in mid-1979; a National Oceanic and Atmospheric Administration (NOAA) scientist criticized NASA for using an inaccurate model for the second most-intense sunspot cycle in a century, and for ignoring NOAA predictions published in 1976.

The reentry of the USSR's nuclear powered Cosmos 954 in January 1978, and the resulting radioactive debris fall in northern Canada, drew more attention to Skylab's orbit. Although Skylab did not contain radioactive materials, the State Department warned NASA about the potential diplomatic repercussions of station debris. Battelle Memorial Institute forecast that up to 25 tons of metal debris could land in 500 pieces over an area 4,000 miles long and 1,000 miles wide. The lead-lined film vault, for example, might land intact at 400 feet per second.

Ground controllers re-established contact with Skylab in March 1978 and recharged its batteries. Although NASA worked on plans to reboost Skylab with the Space Shuttle through 1978 and the TRS was almost complete, the agency gave up in December when it became clear that the shuttle would not be ready in time; its first flight, STS-1, did not occur until April 1981. Also rejected were proposals to launch the TRS using one or two uncrewed rockets or to attempt to destroy the station with missiles.

Skylab's demise in 1979 was an international media event, with T-shirts and hats with bullseyes and "Skylab Repellent" with a money-back guarantee, wagering on the time and place of re-entry, and nightly news reports. The "San Francisco Examiner" offered a $10,000 prize for the first piece of Skylab delivered to its offices; the competing "San Francisco Chronicle" offered $200,000 if a subscriber suffered personal or property damage. A Nebraska neighborhood painted a target so that the station would have "something to aim for", a resident said.

A report commissioned by NASA calculated that the odds were 1 in 152 of debris hitting any human, and odds of 1 in 7 of debris hitting a city of 100,000 people or more. Special teams were readied to head to any country hit by debris. The event caused so much panic in the Philippines that President Ferdinand Marcos appeared on national television to reassure the public.

A week before re-entry, NASA forecast that it would occur between July 10 and 14, with the 12th the most likely date, and the Royal Aircraft Establishment predicted the 14th. In the hours before the event, ground controllers adjusted Skylab's orientation to minimize the risk of re-entry on a populated area. They aimed the station at a spot south-southeast of Cape Town, South Africa, and re-entry began at approximately 16:37 UTC, July 11, 1979. The Air Force provided data from a secret tracking system. The station did not burn up as fast as NASA expected. Debris landed about east of Perth, Western Australia due to a four-percent calculation error, and was found between Esperance, Western Australia and Rawlinna, from 31° to 34°S and 122° to 126°E, about 130–150 km (81–93 miles) radius around Balladonia, Western Australia. Residents and an airline pilot saw dozens of colorful flares as large pieces broke up in the atmosphere; the debris landed in an almost unpopulated area, but the sightings still caused NASA to fear human injury or property damage. The Shire of Esperance light-heartedly fined NASA A$400 for littering, and Scott Barley of Highway Radio raised the funds from his morning show listeners in April 2009 and paid the fine on behalf of NASA.

Stan Thornton found 24 pieces of Skylab at his home in Esperance, and a Philadelphia businessman flew him, his parents, and his girlfriend to San Francisco where he collected the "Examiner" prize and another $1,000 from the businessman. The Miss Universe 1979 pageant was scheduled for July 20, 1979 in Perth, and a large piece of Skylab debris was displayed on the stage. Analysis of the debris showed that the station had disintegrated above the Earth, much lower than expected.

After the demise of Skylab, NASA focused on the reusable Spacelab module, an orbital workshop that could be deployed with the Space Shuttle and returned to Earth. The next American major space station project was Space Station Freedom, which was merged into the International Space Station in 1993 and launched starting in 1998. Shuttle-Mir was another project and led to the US funding Spektr, Priroda, and the Mir Docking Module in the 1990s.

There was a Skylab Rescue mission assembled for the second crewed mission to Skylab, but it was not needed. Another rescue mission was assembled for the last Skylab and was also on standby for ASTP. That launch stack might have been used for Skylab 5 (which would have been the fourth crewed Skylab mission), but this was cancelled and the SA-209 Saturn IB rocket was put on display at NASA Kennedy Space Center.

Launch vehicles:

Skylab 5 would have been a short 20-day mission to conduct more scientific experiments and use the Apollo's Service Propulsion System engine to boost Skylab into a higher orbit. Vance Brand (commander), William B. Lenoir (science pilot), and Don Lind (pilot) would have been the crew for this mission, with Brand and Lind being the prime crew for the Skylab Rescue flights. Brand and Lind also trained for a mission that would have aimed Skylab for a controlled deorbit.

The mission would have launched in April 1974 and supported later use by the Space Shuttle by boosting the station to higher orbit.

In addition to the flown Skylab space station, a second flight-quality backup Skylab space station had been built during the program. NASA considered using it for a second station in May 1973 or later, to be called Skylab B (S-IVB 515), but decided against it. Launching another Skylab with another Saturn V rocket would have been very costly, and it was decided to spend this money on the development of the Space Shuttle instead. The backup is on display at the National Air and Space Museum in Washington, D.C.

A full-size training mock-up once used for astronaut training is located at the Lyndon B. Johnson Space Center visitor's center in Houston, Texas. Another full-size training mock-up is at the U.S. Space & Rocket Center in Huntsville, Alabama. Originally displayed indoors, it was subsequently stored outdoors for several years to make room for other exhibits. To mark the 40th anniversary of the Skylab program, the Orbital Workshop portion of the trainer was restored and moved into the Davidson Center in 2013. NASA transferred the backup Skylab to the National Air and Space Museum in 1975. On display in the Museum's Space Hall since 1976, the orbital workshop has been slightly modified to permit viewers to walk through the living quarters.

The numerical identification of the crewed Skylab missions was the cause of some confusion. Originally, the uncrewed launch of Skylab and the three crewed missions to the station were numbered "SL-1" through "SL-4". During the preparations for the crewed missions, some documentation was created with a different scheme—"SLM-1" through "SLM-3"—for those missions only. William Pogue credits Pete Conrad with asking the Skylab program director which scheme should be used for the mission patches, and the astronauts were told to use 1-2-3, not 2-3-4. By the time NASA administrators tried to reverse this decision, it was too late, as all the in-flight clothing had already been manufactured and shipped with the 1-2-3 mission patches.

NASA Astronaut Group 4 and Group 6 were scientists recruited as astronauts. They and the scientific community hoped to have two on each Skylab mission, but Deke Slayton, director of flight crew operations, insisted that two trained pilots fly on each.

The "Skylab Medical Experiment Altitude Test" or SMEAT was a 56-day (8-week) Earth analog Skylab test. The test had a low-pressure high oxygen-percentage atmosphere but it operated under full gravity, as SMEAT was not in orbit. The test had a three-astronaut crew with Commander (Crippen), Science Pilot (Bobko), and Pilot (Thornton); there was a focus on medical studies and Thornton was an M.D. The crew lived and worked in the pressure chamber, converted to be like Skylab, from July 26 to September 20, 1972.

From 1966 to 1974, the Skylab program cost a total of $2.2 billion, equivalent to $10 billion in 2010 dollars. As its three three-person crews spent 510 total man-days in space, each man-day cost approximately $20 million, compared to $7.5 million for the International Space Station.

The 1969 film "Marooned" depicts three astronauts stranded in orbit after visiting the unnamed Apollo Applications Program space lab.

David Wain's 2001 comedy film "Wet Hot American Summer" depicts a fictionalized version of Skylab's re-entry, in which debris from the station is expected to land on a summer camp in Maine.

The documentary "Searching for Skylab" was released online in March 2019. It was written and directed by Dwight Steven-Boniecki and was partly crowdfunded.








</doc>
<doc id="29442" url="https://en.wikipedia.org/wiki?curid=29442" title="Sacramento (disambiguation)">
Sacramento (disambiguation)

Sacramento is the capital of the U.S. state of California.

Sacramento may also refer to:









</doc>
<doc id="29445" url="https://en.wikipedia.org/wiki?curid=29445" title="StrongARM">
StrongARM

The StrongARM is a family of computer microprocessors developed by Digital Equipment Corporation and manufactured in the late 1990s which implemented the ARM v4 instruction set architecture. It was later sold to Intel in 1997, who continued to manufacture it before replacing it with the XScale in the early 2000s.

According to Allen Baum, the StrongARM traces its history to attempts to make a low-power version of the DEC Alpha, which DEC's engineers quickly concluded was not possible. They then became interested in designs dedicated to low-power applications which led them to the ARM family. One of the only major users of the ARM for performance-related products at that time was Apple, whose Newton device was based on the ARM platform. DEC approached Apple wondering if they might be interested in a high-performance ARM, to which the Apple engineers replied "Phhht, yeah. You can’t do it, but, yeah, if you could we'd use it."

The StrongARM was a collaborative project between DEC and Advanced RISC Machines to create a faster ARM microprocessor. The StrongARM was designed to address the upper-end of the low-power embedded market, where users needed more performance than the ARM could deliver while being able to accept more external support. Targets were devices such as newer personal digital assistants and set-top boxes.

Traditionally, the semiconductor division of DEC was located in Massachusetts. In order to gain access to the design talent in Silicon Valley, DEC opened a design center in Palo Alto, California. This design center was led by Dan Dobberpuhl and was the main design site for the StrongARM project. Another design site that worked on the project was in Austin, Texas that was created by some ex-DEC designers returning from Apple Computer and Motorola. The project was set up in 1995, and quickly delivered their first design, the SA-110.

DEC agreed to sell StrongARM to Intel as part of a lawsuit settlement in 1997. Intel used the StrongARM to replace their ailing line of RISC processors, the i860 and i960.

When the semiconductor division of DEC was sold to Intel, many engineers from the Palo Alto design group moved to SiByte, a start-up company designing MIPS system-on-a-chip (SoC) products for the networking market. The Austin design group spun off to become Alchemy Semiconductor, another start-up company designing MIPS SoCs for the hand-held market. A new StrongARM core was developed by Intel and introduced in 2000 as the XScale.

The SA-110 was the first microprocessor in the StrongARM family. The first versions, operating at 100, 160, and 200 MHz, were announced on 5 February 1996. When announced, samples of these versions were available, with volume production slated for mid-1996. Faster 166 and 233 MHz versions were announced on 12 September 1996. Samples of these versions were available at announcement, with volume production slated for December 1996. Throughout 1996, the SA-110 was the highest performing microprocessor for portable devices. Towards the end of 1996 it was a leading CPU for internet/intranet appliances and thin client systems. The SA-110's first design win was the Apple MessagePad 2000. It was also used in a number of products including the Acorn Computers Risc PC and Eidos Optima video editing system. The SA-110's lead designers were Daniel W. Dobberpuhl, Gregory W. Hoeppner, Liam Madden, and Richard T. Witek.

The SA-110 had a simple microarchitecture. It was a scalar design that executed instructions in-order with a five-stage classic RISC pipeline. The microprocessor was partitioned into several blocks, the IBOX, EBOX, IMMU, DMMU, BIU, WB and PLL. The IBOX contained hardware that operated in the first two stages of the pipeline such as the program counter. It fetched, decoded and issued instructions. Instruction fetch occurs during the first stage, decode and issue during the second. The IBOX decodes the more complex instructions in the ARM instruction set by translating them into sequences of simpler instructions. The IBOX also handled branch instructions. The SA-110 did not have branch prediction hardware, but had mechanisms for their speedy processing.

Execution starts at stage three. The hardware that operates during this stage is contained in the EBOX, which comprises the register file, arithmetic logic unit (ALU), barrel shifter, multiplier and condition code logic. The register file had three read ports and two write ports. The ALU and barrel shifter executed instructions in a single cycle. The multiplier is not pipelined and has a latency of multiple cycles.

The IMMU and DMMU are memory management units for instructions and data, respectively. Each MMU contained a 32-entry fully associative translation lookaside buffer (TLB) that can map 4 KB, 64 KB or 1 MB pages. The write buffer (WB) has eight 16-byte entries. It enables the pipelining of stores. The bus interface unit (BIU) provided the SA-110 with an external interface.

The PLL generates the internal clock signal from an external 3.68 MHz clock signal. It was not designed by DEC, but was contracted to the Centre Suisse d'Electronique et de Microtechnique (CSEM) located in Neuchâtel, Switzerland.

The instruction cache and data cache each have a capacity of 16 KB and are 32-way set-associative and virtually addressed. The SA-110 was designed to be used with slow (and therefore low-cost) memory and therefore the high set associativity allows a higher hit rate than competing designs, and the use of virtual addresses allows memory to be simultaneously cached and uncached. The caches are responsible for most of the transistor count and they take up half the die area.

The SA-110 contained 2.5 million transistors and is 7.8 mm by 6.4 mm large (49.92 mm). It was fabricated by DEC in its proprietary CMOS-6 process at its Fab 6 fab in Hudson, Massachusetts. CMOS-6 was DEC's sixth-generation complementary metal–oxide–semiconductor (CMOS) process. CMOS-6 has a 0.35 µm feature size, a 0.25 µm effective channel length but for use with the SA-110, only three levels of aluminium interconnect. It used a power supply with a variable voltage of 1.2 to 2.2 volts (V) to enable designs to find a balance between power consumption and performance (higher voltages enable higher clock rates). The SA-110 was packaged in a 144-pin thin quad flat pack (TQFP).

The SA-1100 was a derivative of the SA-110 developed by DEC. Announced in 1997, the SA-1100 was targeted for portable applications such as PDAs and differs from the SA-110 by providing a number of features that are desirable for such applications. To accommodate these features, the data cache was reduced in size to 8 KB.

The extra features are integrated memory, PCMCIA, and color LCD controllers connected to an on-die system bus, and five serial I/O channels that are connected to a peripheral bus attached to the system bus. The memory controller supported FPM and EDO DRAM, SRAM, flash, and ROM. The PCMCIA controller supports two slots. The memory address and data bus is shared with the PCMCIA interface. Glue logic is required. The serial I/O channels implement a slave USB interface, a SDLC, two UARTs, an IrDA interface, a MCP, and a synchronous serial port.

The SA-1100 had a companion chip, the SA-1101. It was introduced by Intel on 7 October 1998. The SA-1101 provided additional peripherals to complement those integrated on the SA-1100 such as a video output port, two PS/2 ports, a USB controller and a PCMCIA controller that replaces that on the SA-1100. Design of the device started by DEC, but was only partially complete when acquired by Intel, who had to finish the design. It was fabricated at DEC's former Hudson, Massachusetts fabrication plant, which was also sold to Intel.

The SA-1100 contained 2.5 million transistors and measured 8.24 mm by 9.12 mm (75.15 mm). It was fabricated in a 0.35 μm CMOS process with three levels of aluminium interconnect and was packaged in a 208-pin TQFP.

One of the early recipients of this processor was-ill-fated Psion netBook and its more consumer oriented sibling Psion Series 7.

The SA-1110 was a derivative of the SA-110 developed by Intel. It was announced on 31 March 1999, positioned as an alternative to the SA-1100. At announcement, samples were set for June 1999 and volume later that year. Intel discontinued the SA-1110 in early 2003. The SA-1110 was available in 133 or 206 MHz versions. It differed from the SA-1100 by featuring support for 66 MHz (133 MHz version only) or 103 MHz (206 MHz version only) SDRAM. Its companion chip, which provided additional support for peripherals, was the SA-1111. The SA-1110 was packaged in a 256-pin micro ball grid array. It was used in mobile phones, personal data assistants (PDAs) such as the Compaq (later HP) iPAQ and HP Jornada, the Sharp SL-5x00 Linux Based Platforms and the Simputer. It was also used to run the Intel Web Tablet, a tablet device that is considered potentially the first to introduce large screen, portable web browsing. Intel dropped the product just prior to launch in 2001.

The SA-1500 was a derivative of the SA-110 developed by DEC initially targeted for set-top boxes. It was designed and manufactured in low volumes by DEC but was never put into production by Intel. The SA-1500 was available at 200 to 300 MHz. The SA-1500 featured an enhanced SA-110 core, an on-chip coprocessor called the "Attached Media Processor" (AMP), and an on-chip SDRAM and I/O bus controller. The SDRAM controller supported 100 MHz SDRAM, and the I/O controller implemented a 32-bit I/O bus that may run at frequencies up to 50 MHz for connecting to peripherals and the SA-1501 companion chip.

The AMP implemented a long instruction word instruction set containing instructions designed for multimedia, such as integer and floating-point multiply–accumulate and SIMD arithmetic. Each long instruction word is 64 bits wide and specifies an arithmetic operation and a branch or a load/store. Instructions operate on operands from a 64-entry 36-bit register file, and on a set of control registers. The AMP communicates with the SA-110 core via an on-chip bus and it shares the data cache with the SA-110. The AMP contained an ALU with a shifter, a branch unit, a load/store unit, a multiply–accumulate unit, and a single-precision floating-point unit. The AMP supported user-defined instructions via a 512-entry writable control store.

The SA-1501 companion chip provided additional video and audio processing capabilities and various I/O functions such as PS/2 ports, a parallel port, and interfaces for various peripherals.

The SA-1500 contains 3.3 million transistors and measures 60 mm. It was fabricated in a 0.28 µm CMOS process. It used a 1.5 to 2.0 V internal power supply and 3.3 V I/O, consuming less than 0.5 W at 100 MHz and 2.5 W at 300 MHz. It was packaged in a 240-pin metal quad flat package or a 256-ball plastic ball grid array.

The StrongARM latch is an electronic latch circuit topology first proposed by Toshiba engineers Tsuguo Kobayashi "et al." and got significant attention after being used in StrongARM microprocessors. It is widely used as a sense amplifier, a comparator, or just a robust latch with high sensitivity.



</doc>
<doc id="29450" url="https://en.wikipedia.org/wiki?curid=29450" title="Shaul Mofaz">
Shaul Mofaz

Lieutenant General Shaul Mofaz (; 4 November 1948) is an Israeli former soldier and politician. He joined the Israel Defense Forces in 1966 and served in the Paratroopers Brigade. He fought in the Six-Day War, Yom Kippur War, 1982 Lebanon War, and Operation Entebbe with the paratroopers and Sayeret Matkal, an elite special forces unit. In 1998 he became the sixteenth IDF's Chief of the General Staff, serving until 2002. He is of Iranian Jewish ancestry.

After leaving the army, he entered politics. He was appointed Minister of Defense in 2002, holding the position until 2006 when he was elected to the Knesset on the Kadima list. He then served as Deputy Prime Minister and Minister of Transportation and Road Safety until 2009. After becoming Kadima leader in March 2012 he became Leader of the Opposition, before returning to the cabinet during a 70-day spell in which he served as Acting Prime Minister, Vice Prime Minister and Minister without Portfolio. Kadima was reduced to just two seats in the 2013 elections, and Mofaz retired from politics shortly before the 2015 elections.

Shaul Mofaz was born Shahrām Mofazzazkār () on 4 November 1948 in Tehran, to Persian Jewish parents from Isfahan. Mofaz immigrated to Israel with his parents in 1957. Upon graduating from high school in 1966, he joined the Israel Defense Forces and served in the Paratroopers Brigade. He served in the Six-Day War, Yom Kippur War, 1982 Lebanon War, and Operation Entebbe with the paratroopers and Sayeret Matkal, an elite special forces unit.
Mofaz was then appointed an infantry brigade commander for the 1982 Lebanon War. Afterwards he attended the US Marine Corps Command and Staff College in Quantico, Virginia, United States. On his return he was briefly appointed commander of the Officers School, before returning to active service as commander of the 35th Paratroopers Brigade in 1986, and led its forces during Operation Law and Order.

Mofaz served in a series of senior military posts, having been promoted to the rank of Brigadier General (1988). In 1993 he was made commander of the IDF forces in the West Bank. In 1994, he was promoted to Major General, commanding the Southern Corps. His rapid rise continued; in 1997 Mofaz was appointed Deputy Chief of the General Staff and in 1998 he was appointed Chief of the General Staff.

His term of Chief of Staff was noted for financial and structural reforms of the Israeli Army. But the most significant event in his tenure was the eruption of the Second Intifada in September, 2000. The tough tactics undertaken by Mofaz drew widespread concern from the international community but were broadly supported by the Israeli public. Controversy erupted over the offensive in Jenin, intermittent raids in the Gaza Strip, and the continued isolation of Yasser Arafat.

Mofaz foresaw the wave of violence coming early as 1999 and prepared the IDF for intense guerrilla warfare in the territories. He fortified posts at the Gaza Strip and kept Israel Defense Forces casualties low. While he was known for claiming, "Israel has the most moral army in the world," he drew criticism from both Israeli and international human rights monitoring groups because of the methods he had undertaken, including using armored bulldozers to demolish 2,500 Palestinian civilian homes, displacing thousands, in order to create a security "buffer zone" along the Rafah border.

Following a government crisis in 2002, Shaul Mofaz was appointed Defense Minister by Ariel Sharon. Although he supported an agreement with the Palestinians, he was willing to make no compromise in the war against militant groups such as Hamas, Islamic Jihad, Tanzim, and Al-Aqsa Martyrs Brigades.

The fact that he had only recently left his position as IDF Chief of Staff prevented him from participating in the 2003 election (by which time Mofaz had joined Sharon's Likud). Nevertheless, Sharon reappointed him as Defense Minister in the new government.

On 21 November 2005, Mofaz rejected Sharon's invitation to join his new party, Kadima, and instead announced his candidacy for the leadership of Likud. But, on 11 December 2005, one day after he promised he would never leave the Likud, he withdrew from both the leadership race and the Likud to join Kadima.

Following the elections in late March 2006, Mofaz was moved from the position of Defense Minister and received the Transport ministry in the new Cabinet installed on 4 May 2006.

In 2008, with Israel's then prime minister, Ehud Olmert, being pressured to resign due to corruption charges, Mofaz announced that he would run for the leadership of the Kadima party.

On 5 August 2008, Mofaz officially entered the race to be leader of Kadima. That same day he received a blessing by Shas spiritual leader Rabbi Ovadia Yosef. On 17 September 2008, he lost the Kadima party election, losing to Tzipi Livni for the spot of the Prime Minister and leader of Kadima. Livni's narrow margin of 431 votes was 43.1% to Shaul Mofaz's 42.0%, a huge difference from the 10 to 12-point exit polls margins. She said the "national responsibility (bestowed) by the public brings me to approach this job with great reverence". Mofaz accepted the Kadima primary's result, despite his lawyer, Yehuda Weinstein's appeal advice, and telephoned Livni congratulating her. Livni got 16,936 votes, with 16,505 votes, for Mofaz. Public Security Minister Avi Dichter and Interior Minister Meir Sheetrit had 6.5% and 8.5% respectively.

Placed second on the Kadima list, Mofaz retained his seat in the 2009 elections, but lost his cabinet position after Likud formed the government.

On 27 March 2012, Shaul Mofaz won the Kadima party leadership primaries by a landslide, defeating party chairwoman Tzipi Livni. Mofaz became Vice Prime Minister as part of a deal reached for a government of national unity with Binyamin Netanyahu. Mofaz said during the Kadima primaries that he would not join a government led by Netanyahu.

Mofaz left over Netanyahu's indecision over a draft reform law and warned that the prime minister was trying to patch together a majority for a vote to plunge the region into war.

In 2013 Kadima, just 4 years prior the ruling party, received 2% of the votes, barely passing to the Knesset.

In the buildup to the 2015 elections Kadima was not expect to pass the threshold, as it was raised to 3.25%. Mofaz negotiated with the Zionist Union alliance to bring Kadima onto their slate, but ended negotiations when it became clear he would not be their candidate for Defense Minister. Immediately after Mofaz announced he was not joining the Zionist Union slate, it was announced the former Military Intelligence Directorate (Israel) head Amos Yadlin was appointed to the Zionist Union slate and would be their candidate for Defense Minister. Within a week of his announcement that he was not running with the Zionist Union, Mofaz announced his retirement from politics.

A fictionalized version of Mofaz appeared in the 2008 drama film "Lemon Tree".



</doc>
<doc id="29452" url="https://en.wikipedia.org/wiki?curid=29452" title="Stasi">
Stasi

The Ministry for State Security (, MfS) or State Security Service (, SSD), commonly known as the (), was the official state security service of the German Democratic Republic (East Germany). It has been described as one of the most effective and repressive intelligence and secret police agencies ever to have existed. The Stasi was headquartered in East Berlin, with an extensive complex in Berlin-Lichtenberg and several smaller facilities throughout the city. The Stasi motto was (Shield and Sword of the Party), referring to the ruling Socialist Unity Party of Germany (, SED) and also echoing a theme of the KGB, the Soviet counterpart and close partner, with respect to its own ruling party, the Communist Party of the Soviet Union (CPSU). Erich Mielke was the Stasi's longest-serving chief, in power for 32 years of the GDR's 40 years of existence.

One of its main tasks was spying on the population, primarily through a vast network of citizens turned informants, and fighting any opposition by overt and covert measures, including hidden psychological destruction of dissidents (, literally meaning "decomposition"). It arrested 250,000 people as political prisoners during its existence. Its Main Directorate for Reconnaissance () was responsible for both espionage and for conducting covert operations in foreign countries. Under its long-time head Markus Wolf, this directorate gained a reputation as one of the most effective intelligence agencies of the Cold War. The Stasi also maintained contacts, and occasionally cooperated, with Western terrorists.

Numerous Stasi officials were prosecuted for their crimes after 1990. After German reunification, the surveillance files that the Stasi had maintained on millions of East Germans were opened, so that any citizen could inspect their personal file on request. These files are now maintained by the Stasi Records Agency.

The Stasi was founded on 8 February 1950. Wilhelm Zaisser was the first Minister of State Security of the GDR, and Erich Mielke was his deputy. Zaisser tried to depose SED General Secretary Walter Ulbricht after the June 1953 uprising, but was instead removed by Ulbricht and replaced with Ernst Wollweber thereafter. Following the June 1953 uprising, the Politbüro decided to downgrade the apparatus to a State Secretariat and incorporate it under the Ministry of Interior under the leadership of Willi Stoph. The Minister of State Security simultaneously became a State Secretary of State Security. The Stasi held this status until November 1955, when it was restored to a ministry. Wollweber resigned in 1957 after clashes with Ulbricht and Erich Honecker, and was succeeded by his deputy, Erich Mielke.

In 1957, Markus Wolf became head of the Hauptverwaltung Aufklärung (HVA) (Main Reconnaissance Administration), the foreign intelligence section of the Stasi. As intelligence chief, Wolf achieved great success in penetrating the government, political and business circles of West Germany with spies. The most influential case was that of Günter Guillaume, which led to the downfall of West German Chancellor Willy Brandt in May 1974. In 1986, Wolf retired and was succeeded by Werner Grossmann.

Although Mielke's Stasi was superficially granted independence in 1957, until 1990 the KGB continued to maintain liaison officers in all eight main Stasi directorates, each with his own office inside the Stasi's Berlin compound, and in each of the fifteen Stasi district headquarters around East Germany. Collaboration was so close that the KGB invited the Stasi to establish operational bases in Moscow and Leningrad to monitor visiting East German tourists and Mielke referred to the Stasi officers as "Chekists of the Soviet Union". In 1978, Mielke formally granted KGB officers in East Germany the same rights and powers that they enjoyed in the Soviet Union.

The Ministry for State Security was organized according to the "Line principle". A high-ranking official was in charge of a particular mission of the Ministry and headed a division in the Central Apparatus ("Zentrale"). A corresponding division was organized in each of the 15 District Departments for State Security ("Bezirksverwaltungen für Staatssicherheit" in the Berlin Capital Region and 14 regional districts ("Bezirke")). At the local level the Stasi had Area Precincts for State Security ("Bezirksverwaltungen für Staatssicherheit" - one each for the 227 cities and municipal districts and the 11 city boroughs ("Stadtbezirken") of East Berlin). A single case officer held responsibility for the particular mission in each area precinct. The line principle meant that the case officers were subordinated to the specialized divisions at the district departments. The specialized divisions at the district departments were subordinated to the specialized division in the central apparatus and the whole line was under the direct command and control of the high-ranking Stasi officer in charge of the mission. The Stasi also fielded Location Detachments ("Objektdienststellen") at state-owned enterprises of high importance (such as the joint USSR-East German Wismar uranium mining company). Shortly before the transformation of the Stasi into the Office of National Security the Ministry had the following structure:

Minister for State Security

Policy Board (Kollegium des MfS, included the Minister and his deputies)

Central Apparatus ("Zentrale")

Divisions directly subordinated to the Minister Army general Erich Mielke ("Dem Minister für Staatssicherheit direkt unterstellte Diensteinheiten")


Divisions directly subordinated to the Deputy Minister Colonel General Werner Großmann ("Dem Stellvertreter GO Großmann unterstellte Diensteinheiten") (his predecessor was the legendary Colonel general Markus Wolf)


Divisions directly subordinated to the Deputy Minister Colonel general Rudi Mittig ("Dem Stellvertreter GO Mittig unterstellte Diensteinheiten")


Divisions directly subordinated to the Deputy Minister Lieutenant general Gerhard Neiber ("Dem Stellvertreter GL Neiber unterstellte Diensteinheiten")


Divisions directly subordinated to the Deputy Minister Lieutenant general Wolfgang Schwanitz ("Dem Stellvertreter GL Schwanitz unterstellte Diensteinheiten") (Schwanitz was appointed as the chief of the Stasi successor agency - the Office for National Security)



Selected Stasi departments:

Between 1950 and 1989, the Stasi employed a total of 274,000 people in an effort to root out the class enemy. In 1989, the Stasi employed 91,015 people full-time, including 2,000 fully employed unofficial collaborators, 13,073 soldiers and 2,232 officers of GDR army, along with 173,081 unofficial informants inside GDR and 1,553 informants in West Germany.

Regular commissioned Stasi officers were recruited from conscripts who had been honourably discharged from their 18 months' compulsory military service, had been members of the SED, had had a high level of participation in the Party's youth wing's activities and had been Stasi informers during their service in the Military. The candidates would then have to be recommended by their military unit political officers and Stasi agents, the local chiefs of the District (Bezirk) Stasi and Volkspolizei office, of the district in which they were permanently resident, and the District Secretary of the SED. These candidates were then made to sit through several tests and exams, which identified their intellectual capacity to be an officer, and their political reliability. University graduates who had completed their military service did not need to take these tests and exams. They then attended a two-year officer training programme at the Stasi college ("Hochschule") in Potsdam. Less mentally and academically endowed candidates were made ordinary technicians and attended a one-year technology-intensive course for non-commissioned officers.

By 1995, some 174,000 "inoffizielle Mitarbeiter" (IMs) Stasi informants had been identified, almost 2.5% of East Germany's population between the ages of 18 and 60. 10,000 IMs were under 18 years of age. From the volume of material destroyed in the final days of the regime, the office of the Federal Commissioner for the Stasi Records (BStU) believes that there could have been as many as 500,000 informers. A former Stasi colonel who served in the counterintelligence directorate estimated that the figure could be as high as 2 million if occasional informants were included. There is significant debate about how many IMs were actually employed.

Full-time officers were posted to all major industrial plants (the extensiveness of any surveillance largely depended on how valuable a product was to the economy) and one tenant in every apartment building was designated as a watchdog reporting to an area representative of the Volkspolizei (Vopo). Spies reported every relative or friend who stayed the night at another's apartment. Tiny holes were drilled in apartment and hotel room walls through which Stasi agents filmed citizens with special video cameras. Schools, universities, and hospitals were extensively infiltrated, as were organizations, such as computer clubs where teenagers exchanged Western video games.

The Stasi had formal categorizations of each type of informant, and had official guidelines on how to extract information from, and control, those with whom they came into contact. The roles of informants ranged from those already in some way involved in state security (such as the police and the armed services) to those in the dissident movements (such as in the arts and the Protestant Church). Information gathered about the latter groups was frequently used to divide or discredit members. Informants were made to feel important, given material or social incentives, and were imbued with a sense of adventure, and only around 7.7%, according to official figures, were coerced into cooperating. A significant proportion of those informing were members of the SED. Use of some form of blackmail was not uncommon. A large number of Stasi informants were tram conductors, janitors, doctors, nurses and teachers. Mielke believed that the best informants were those whose jobs entailed frequent contact with the public.

The Stasi's ranks swelled considerably after Eastern Bloc countries signed the 1975 Helsinki accords, which GDR leader Erich Honecker viewed as a grave threat to his regime because they contained language binding signatories to respect "human and basic rights, including freedom of thought, conscience, religion, and conviction". The number of IMs peaked at around 180,000 in that year, having slowly risen from 20,000–30,000 in the early 1950s, and reaching 100,000 for the first time in 1968, in response to "Ostpolitik" and protests worldwide. The Stasi also acted as a proxy for KGB to conduct activities in other Eastern Bloc countries, such as Poland, where the Soviets were despised.

The Stasi infiltrated almost every aspect of GDR life. In the mid-1980s, a network of IMs began growing in both German states. By the time that East Germany collapsed in 1989, the Stasi employed 91,015 employees and 173,081 informants. About one out of every 63 East Germans collaborated with the Stasi. By at least one estimate, the Stasi maintained greater surveillance over its own people than any secret police force in history. The Stasi employed one secret policeman for every 166 East Germans. By comparison, the Gestapo deployed one secret policeman per 2,000 people. As ubiquitous as this was, the ratios swelled when informers were factored in: counting part-time informers, the Stasi had one agent per 6.5 people. This comparison led Nazi hunter Simon Wiesenthal to call the Stasi even more oppressive than the Gestapo. Stasi agents infiltrated and undermined West Germany's government and spy agencies.

In some cases, spouses even spied on each other. A high-profile example of this was peace activist Vera Lengsfeld, whose husband, Knud Wollenberger, was a Stasi informant.

The Stasi perfected the technique of psychological harassment of perceived enemies known as "Zersetzung" () – a term borrowed from chemistry which literally means "decomposition".
By the 1970s, the Stasi had decided that the methods of overt persecution that had been employed up to that time, such as arrest and torture, were too crude and obvious. It was realised that psychological harassment was far less likely to be recognised for what it was, so its victims, and their supporters, were less likely to be provoked into active resistance, given that they would often not be aware of the source of their problems, or even its exact nature. "Zersetzung" was designed to side-track and "switch off" perceived enemies so that they would lose the will to continue any "inappropriate" activities.

Tactics employed under "Zersetzung" generally involved the disruption of the victim's private or family life. This often included psychological attacks, such as breaking into homes and subtly manipulating the contents, in a form of gaslighting – moving furniture, altering the timing of an alarm, removing pictures from walls or replacing one variety of tea with another. Other practices included property damage, sabotage of cars, purposely incorrect medical treatment, smear campaigns including sending falsified compromising photos or documents to the victim's family, denunciation, provocation, psychological warfare, psychological subversion, wiretapping, bugging, mysterious phone calls or unnecessary deliveries, even including sending a vibrator to a target's wife. Usually, victims had no idea that the Stasi were responsible. Many thought that they were losing their minds, and mental breakdowns and suicide could result.

One great advantage of the harassment perpetrated under "Zersetzung" was that its subtle nature meant that it was able to be plausibly denied. This was important given that the GDR was trying to improve its international standing during the 1970s and 80s, especially in conjunction with the "Ostpolitik" of West German Chancellor Willy Brandt massively improving relations between the two German states.

After German reunification, revelations of the Stasi's international activities were publicized, such as its military training of the West German Red Army Faction.


Recruitment of informants became increasingly difficult towards the end of the GDR's existence, and, after 1986, there was a negative turnover rate of IMs. This had a significant impact on the Stasi's ability to survey the populace, in a period of growing unrest, and knowledge of the Stasi's activities became more widespread. Stasi had been tasked during this period with preventing the country's economic difficulties becoming a political problem, through suppression of the very worst problems the state faced, but it failed to do so.

Stasi officers reportedly had discussed re-branding East Germany as a democratic capitalist country to the West, which in actuality would have been taken over by Stasi officers. The plan specified 2,587 OibE officers ("Offiziere im besonderen Einsatz", "officers on special assignment") who would have assumed power as detailed in the Top Secret Document 0008-6/86 of 17 March 1986. According to Ion Mihai Pacepa, the chief intelligence officer in communist Romania, other communist intelligence services had similar plans. On 12 March 1990, "Der Spiegel" reported that the Stasi was indeed attempting to implement 0008-6/86. Pacepa has noted that what happened in Russia and how KGB Colonel Vladimir Putin took over Russia resembles these plans. See Putinism.

On 7 November 1989, in response to the rapidly changing political and social situation in the GDR in late 1989, Erich Mielke resigned. On 17 November 1989, the Council of Ministers "(Ministerrat der DDR)" renamed the Stasi the "Office for National Security" "(Amt für Nationale Sicherheit" – AfNS), which was headed by "Generalleutnant" Wolfgang Schwanitz. On 8 December 1989, GDR Prime Minister Hans Modrow directed the dissolution of the AfNS, which was confirmed by a decision of the "Ministerrat" on 14 December 1989.

As part of this decision, the "Ministerrat" originally called for the evolution of the AfNS into two separate organizations: a new foreign intelligence service "(Nachrichtendienst der DDR)" and an "Office for the Protection of the Constitution of the GDR" "(Verfassungsschutz der DDR)", along the lines of the West German "Bundesamt für Verfassungsschutz", however, the public reaction was extremely negative, and under pressure from the "Round Table" "(Runder Tisch)", the government dropped the creation of the "Verfassungsschutz der DDR" and directed the immediate dissolution of the AfNS on 13 January 1990. Certain functions of the AfNS reasonably related to law enforcement were handed over to the GDR Ministry of Internal Affairs. The same ministry also took guardianship of remaining AfNS facilities.

When the parliament of Germany investigated public funds that disappeared after the Fall of the Berlin Wall, it found out that East Germany had transferred large amounts of money to Martin Schlaff through accounts in Vaduz, the capital of Liechtenstein, in return for goods "under Western embargo".

Moreover, high-ranking Stasi officers continued their post-GDR careers in management positions in Schlaff's group of companies. For example, in 1990, Herbert Kohler, Stasi commander in Dresden, transferred 170 million marks to Schlaff for "harddisks" and months later went to work for him.
The investigations concluded that "Schlaff's empire of companies played a crucial role" in the Stasi attempts to secure the financial future of Stasi agents and keep the intelligence network alive.
The "Stern" magazine noted that KGB officer (and future Russian President) Vladimir Putin worked with his Stasi colleagues in Dresden in 1989.

During the Peaceful Revolution of 1989, Stasi offices and prisons throughout the country were occupied by citizens, but not before the Stasi destroyed a number of documents (approximately 5%) consisting of, by one calculation, 1 billion sheets of paper.

With the fall of the GDR the Stasi was dissolved. Stasi employees began to destroy the extensive files and documents they held, by hand, fire and with the use of shredders. When these activities became known, a protest began in front of the Stasi headquarters, The evening of 15 January 1990 saw a large crowd form outside the gates calling for a stop to the destruction of sensitive files. The building contained vast records of personal files, many of which would form important evidence in convicting those who had committed crimes for the Stasi. The protesters continued to grow in number until they were able to overcome the police and gain entry into the complex. Once inside, specific targets of the protesters' anger were portraits of Erich Honecker and Erich Mielke which were trampled on or burnt. Among the protesters were former Stasi collaborators seeking to destroy incriminating documents.

With the German reunification on 3 October 1990, a new government agency was founded called the "Federal Commissioner for the Records of the State Security Service of the former German Democratic Republic" (), officially abbreviated "BStU". There was a debate about what should happen to the files, whether they should be opened to the people or kept closed.

Those who opposed opening the files cited privacy as a reason. They felt that the information in the files would lead to negative feelings about former Stasi members, and, in turn, cause violence. Pastor Rainer Eppelmann, who became Minister of Defense and Disarmament after March 1990, felt that new political freedoms for former Stasi members would be jeopardized by acts of revenge. Prime Minister Lothar de Maizière even went so far as to predict murder. They also argued against the use of the files to capture former Stasi members and prosecute them, arguing that not all former members were criminals and should not be punished solely for being a member. There were also some who believed that everyone was guilty of something. Peter-Michael Diestel, the Minister of Interior, opined that these files could not be used to determine innocence and guilt, claiming that "there were only two types of individuals who were truly innocent in this system, the newborn and the alcoholic". Other opinions, such as the one of West German Interior Minister Wolfgang Schäuble, believed in putting the Stasi behind them and working on German reunification.

Others argued that everyone should have the right to see their own file, and that the files should be opened to investigate former Stasi members and prosecute them, as well as not allow them to hold office. Opening the files would also help clear up some of the rumors that were currently circulating. Some also believed that politicians involved with the Stasi should be investigated.

The fate of the files was finally decided under the Unification Treaty between the GDR and West Germany. This treaty took the Volkskammer law further and allowed more access and use of the files. Along with the decision to keep the files in a central location in the East, they also decided who could see and use the files, allowing people to see their own files.

In 1992, following a declassification ruling by the German government, the Stasi files were opened, leading people to look for their files. Timothy Garton Ash, an English historian, after reading his file, wrote "The File: A Personal History".

Between 1991 and 2011, around 2.75 million individuals, mostly GDR citizens, requested to see their own files. The ruling also gave people the ability to make duplicates of their documents. Another big issue was how the media could use and benefit from the documents. It was decided that the media could obtain files as long as they were depersonalized and not regarding an individual under the age of 18 or a former Stasi member. This ruling not only gave the media access to the files, but also gave schools access.

Even though groups of this sort were active in the community, those who were tracking down ex-members were, as well. Many of these hunters succeeded in catching ex-Stasi; however, charges could not be made for merely being a member. The person in question would have to have participated in an illegal act, not just be a registered Stasi member. Among the high-profile individuals who were arrested and tried were Erich Mielke, Third Minister of State Security of the GDR, and Erich Honecker, head of state for the GDR. Mielke was sentenced to six years prison for the murder of two policemen in 1931. Honecker was charged with authorizing the killing of would-be escapees on the east–west frontier and the Berlin Wall. During his trial, he went through cancer treatment. Because he was nearing death, Honecker was allowed to spend his final time in freedom. He died in Chile in May 1994.

Reassembling the destroyed files has been relatively easy due to the number of archives and the failure of shredding machines (in some cases "shredding" meant tearing paper in two by hand and documents could be recovered easily). In 1995, the BStU began reassembling the shredded documents; 13 years later, the three dozen archivists commissioned to the projects had only reassembled 327 bags; they are now using computer-assisted data recovery to reassemble the remaining 16,000 bagsestimated at 45 million pages. It is estimated that this task may be completed at a cost of 30 million dollars.

The CIA acquired some Stasi records during the looting of the Stasi's archives. West Germany asked for their return and received some in April 2000. See also Rosenholz files.

There a number of memorial sites and museums relating to the Stasi in former Stasi prisons and administration buildings. In addition, offices of the Stasi Records Agency in Berlin, Dresden, Erfurt, Frankfurt-an-der-Oder and Halle (Saale) all have permanent and changing exhibitions relating to the activities of the Stasi in their region.


Memorial and Education Centre Andreasstrasse - a museum in Erfurt which is housed in a former Stasi remand prison. From 1952 until 1989, over 5000 political prisoners were held on remand and interrogated in the Andreasstrasse prison, which was one of 17 Stasi remand prisons in the GDR. On 4 December 1989, local citizens occupied the prison and the neighbouring Stasi district headquarters to stop the mass destruction of Stasi files. It was the first time East Germans had undertaken such resistance against the Stasi and it instigated the take over of Stasi buildings throughout the country.

 (The Bautzner Strasse Memorial in Dresden) - A Stasi remand prison and the Stasi's regional head office in Dresden. It was used as a prison by the Soviet occupying forces from 1945 to 1953, and from 1953 to 1989 by the Stasi. The Stasi held and interrogated between 12,000 and 15,000 people during the time they used the prison. The building was originally a 19th-century paper mill. It was converted into a block of flats in 1933 before being confiscated by the Soviet army in 1945. The Stasi prison and offices were occupied by local citizens on 5 December 1989, during a wave of such takeovers across the country. The museum and memorial site was opened to the public in 1994.

 - A memorial and museum at Collegienstraße 10 in Frankfurt-an-der-Oder, in a building that was used as a detention centre by the Gestapo, the Soviet occupying forces and the Stasi. The building was the Stasi district offices and a remand prison from 1950 until 1969, after which the Volkspolizei used the prison. From 1950 to 1952 it was an execution site where 12 people sentenced to death were executed. The prison closed in 1990. It has been a cultural centre and a memorial to the victims of political tyranny since June 1994, managed by the Museum Viadrina.

, a memorial and 'centre of encounter' in Gera in a former remand prison, originally opened in 1874, that was used by the Gestapo from 1933-1945, the Soviet occupying forces from 1945 to 1949, and from 1952 to 1989 by the Stasi. The building was also the district offices of the Stasi administration. Between 1952 and 1989 over 2,800 people were held in the prison on political grounds. The memorial site opened with the official name ""Die Gedenk- und Begegnungsstätte im Torhaus der politischen Haftanstalt von 1933 bis 1945 und 1945 bis 1989"" in November 2005.

The (Red Ox) is a museum and memorial site at the prison at Am Kirchtor 20, Halle (Saale). Part of the prison, built 1842, was used by the Stasi from 1950 until 1989, during with time over 9,000 political prisoners were held in the prison. From 1954 it was mainly used for women prisoners. The name "Roter Ochse" is the informal name of the prison, possibly originating in the 19th century from the colour of the external walls. It still operates as a prison for young people. Since 1996, the building which was used as an interrogation centre by the Stasi and an execution site by the Nazis has been a museum and memorial centre for victims of political persecution.


 - The memorial site at Moritzplatz in Magdeburg is a museum on the site of a former prison, built from 1873-1876, that was used by the Soviet administration from 1945-1949 and the Stasi from 1958 until 1989 to hold political prisoners. Between 1950 and 1958 the Stasi shared another prison with the civil police. The prison at Moritzplatz was used by the Volkspolizei from 1952 until 1958. Between 1945 and 1989, more than 10,000 political prisoners were held in the prison. The memorial site and museum was founded in December 1990.


The Soviet administration took over the prison in 1945, also using it as a prison for holding political prisoners on remand. The Stasi then used it as a remand prison, mainly for political prisoners from 1952 until 1989. Over 6,000 people were held in the prison by the Stasi during that time. On 27 October 1989, the prison freed all political prisoners due to a nationwide amnesty. On 5 December 1989, the Stasi Headquarters in Potsdam and the Lindenstrasse Prison were occupied by protesters. From January 1990 the building was used as offices for various citizens initiatives and new political groups, such as the Neue Forum. The building was opened to the public from 20 January 1990 and people were taken on tours of the site. It officially became a Memorial site in 1995.


The prison closed in the early 1990s. The state of Mecklenburg-Vorpommern took ownership of it in 1998, and the memorial site and museum were established in 1999. An extensive restoration of the site began in December 2018.

Former Stasi agent Matthias Warnig (codename "Arthur") is currently the head of Nord Stream.
Investigations have revealed that some of the key Gazprom Germania managers are former Stasi agents.

Former Stasi officers continue to be politically active via the "Gesellschaft zur Rechtlichen und Humanitären Unterstützung" (GRH, Society for Legal and Humanitarian Support). Former high-ranking officers and employees of the Stasi, including the last Stasi director, Wolfgang Schwanitz, make up the majority of the organization's members, and it receives support from the German Communist Party, among others.

Impetus for the establishment of the GRH was provided by the criminal charges filed against the Stasi in the early 1990s. The GRH, decrying the charges as "victor's justice", called for them to be dropped. Today the group provides an alternative if somewhat utopian voice in the public debate on the GDR legacy. It calls for the closure of the Berlin-Hohenschönhausen Memorial and can be a vocal presence at memorial services and public events. In March 2006 in Berlin, GRH members disrupted a museum event; a political scandal ensued when the Berlin Senator (Minister) of Culture refused to confront them.

Behind the scenes, the GRH also lobbies people and institutions promoting opposing viewpoints. For example, in March 2006, the Berlin Senator for Education received a letter from a GRH member and former Stasi officer attacking the Museum for promoting "falsehoods, anticommunist agitation and psychological terror against minors". Similar letters have also been received by schools organizing field trips to the museum.






</doc>
<doc id="29455" url="https://en.wikipedia.org/wiki?curid=29455" title="Sandra Bullock">
Sandra Bullock

Sandra Annette Bullock (; born July 26, 1964) is an American-German actress, producer, and philanthropist. She was the highest paid actress in the world in 2010 and 2014. In 2015, Bullock was chosen as "People's" Most Beautiful Woman and was included in "Time" 100 most influential people in the world in 2010. Bullock is the recipient of several accolades, including an Academy Award and a Golden Globe Award.

After making her acting debut with a minor role in the thriller "Hangmen" (1987), Bullock received early attention for her supporting work in the action film "Demolition Man" (1993). Her breakthrough came in the action thriller "Speed" (1994). She established herself in the 1990s with leading roles in the romantic comedies "While You Were Sleeping" (1995) and "Hope Floats" (1998), and the thrillers "The Net" (1995) and "A Time to Kill" (1996). Bullock achieved further success in the following decades with the comedies "Miss Congeniality" (2000), "Two Weeks Notice" (2002), "The Proposal" (2009), "The Heat" (2013), and "Ocean's 8" (2018), the drama "Crash" (2004), and the thrillers "Premonition" (2007) and "Bird Box" (2018). Bullock was awarded the Academy Award for Best Actress and the Golden Globe Award for Best Actress in a Drama for portraying Leigh Anne Tuohy in the biographical drama "The Blind Side" (2009). She was nominated in the same categories for playing an astronaut stranded in space in the science fiction thriller "Gravity" (2013), which was her highest-grossing live-action release.

In addition to her acting career, Bullock is the founder of the production company Fortis Films. She has produced some of the films in which she has starred, including "" (2005) and "All About Steve" (2009). She was an executive producer of the ABC sitcom "George Lopez" (2002–2007) and made several appearances during its run.

Bullock was born in Arlington, Virginia, on July 26, 1964, the daughter of Helga Mathilde Meyer (1942–2000), an opera singer and voice teacher from Germany, and John W. Bullock (1925–2018), an Army employee and part-time voice coach from Birmingham, Alabama. Her father, who was in charge of the Army's Military Postal Service in Europe, was stationed in Nuremberg when he met her mother. Her parents married in Germany. Bullock's maternal grandfather was a German rocket scientist from Nuremberg. The family returned to Arlington, where her father worked with the Army Materiel Command before becoming a contractor for The Pentagon. Bullock has a younger sister, Gesine Bullock-Prado, who served as president of Bullock's production company Fortis Films.

For 12 years Bullock was raised in Nuremberg, Germany and Vienna and Salzburg, Austria, and grew up speaking German. She had a Waldorf education in Nuremberg. As a child, while her mother went on European opera tours, Bullock usually stayed with her aunt Christl and cousin Susanne, the latter of whom later married politician Peter Ramsauer. Bullock studied ballet and vocal arts as a child and frequently accompanied her mother, taking small parts in her opera productions. In Nuremberg, she sang in the opera's children's choir. Bullock has a scar above her left eye which was caused by a fall into a creek when she was a child. While she maintains her American citizenship, Bullock applied for German citizenship in 2009.

Bullock attended Washington-Lee High School, where she was a cheerleader and performed in school theater productions. After graduating in 1982, she attended East Carolina University (ECU) in Greenville, North Carolina, where she received a BFA in Drama in 1987. While at ECU, she performed in multiple theater productions including "Peter Pan" and "Three Sisters". She then moved to Manhattan, New York, where she supported herself as a bartender, cocktail waitress, and coat checker while auditioning for roles.

While in New York, Bullock took acting classes with Sanford Meisner. She appeared in several student films, and later landed a role in an Off-Broadway play "No Time Flat". Director Alan J. Levi was impressed by Bullock's performance and offered her a part in the made-for-television film "" (1989). This led to her being cast in a series of small roles in several independent films as well as in the lead role of the short-lived NBC television version of the film "Working Girl" (1990). She went on to appear in several films, such as "Love Potion No. 9" (1992), "The Thing Called Love" (1993) and "Fire on the Amazon" (1993), before rising to early prominence with her supporting role in the sci-fi action film "Demolition Man" (1993).

Bullock's big breakthrough came in 1994, when she played Annie Porter, a passenger eventually driving the bus in the smash-hit blockbuster "Speed" alongside fellow actor Keanu Reeves. She was required to read for "Speed" to ensure that there was the right chemistry between her and Reeves. She recalls that they had to do "all these really physical scenes together, rolling around on the floor and stuff." "Speed" garnered acclaim from Rotten Tomatoes which deemed it a "terrific popcorn thriller [with] outstanding performances from Keanu Reeves, Dennis Hopper, and Sandra Bullock". It took in US$350 million worldwide.

After the success of "Speed", Bullock established herself as a Hollywood leading actress. In the romantic comedy "While You Were Sleeping" (1995), she portrayed a lonely Chicago Transit Authority token collector who saves the life of a man. While the film made US$182 million globally, it received positive reviews, with Rotten Tomatoes' critical consensus reading: ""While You Were Sleeping" is built wholly from familiar ingredients, but assembled with such skill—and with such a charming performance from Sandra Bullock—that it gives formula a good name." She received her first Golden Globe Award nomination for Best Actress – Motion Picture Musical or Comedy. In 1995, Bullock also starred in the thriller "The Net" (1995) as a computer programmer who stumbles upon a conspiracy that puts her life and the lives of those around her in great danger. Owen Gleiberman, writing for "Entertainment Weekly", complimented her performance saying "Bullock pulls you into the movie. Her overripe smile and clear, imploring eyes are sometimes evocative of Julia Roberts". "The Net" made US$110.6 million.

In the crime drama "A Time to Kill" (1996), Bullock portrayed a member of the defense team, in the trial for the murder of two men who raped a young girl, opposite Samuel L. Jackson, Matthew McConaughey and Kevin Spacey. She received a MTV Movie Award nomination for Best Breakthrough Performance. The film grossed US$152 million around the world. Bullock subsequently received US$11 million for the critically panned "" (1997), which she agreed to star in for financial backing for her next project, "Hope Floats" (1998). She has stated that she regrets making the sequel. In "Hope Floats" she starred as an unassuming housewife whose life is disrupted when her husband (played by Michael Paré) reveals his infidelity to her on a talk show. While the film made US$81.4 million, critic James Berardinelli remarked that her "undisputed strength lies in a blend of light drama and comedy".

Bullock starred in comedy "Practical Magic" (1998) with Nicole Kidman as two witch sisters who face a curse which threatens to prevent them ever finding lasting love. While the film opened atop the chart on its North American opening weekend, it flopped at the box office. The same year she provided her voice as Miriam the animated adventure film "The Prince of Egypt" and wrote, produced, and directed the short film "Making Sandwiches". Alongside Ben Affleck, she played a free-spirited drifter who begins to talk to a writer in the 1999 romantic comedy "Forces of Nature". The film was a commercial hit, grossing US$93 million worldwide, and "Boxoffice Magazine" remarked: "The combination of Affleck's deadpan by-the-book persona with the spontaneity of Bullock's character sparks with convincing chemistry, their diverse personalities causing both to grow and bring to the surface what each is running away from or can't admit."

Bullock took on the role of an FBI agent who must go undercover as a beauty pageant contestant in the comedy "Miss Congeniality" (2000). It was a financial success, grossing US$212 million worldwide and earned Bullock another Golden Globe Award nomination for Best Actress – Motion Picture Musical or Comedy. Also in 2000 she played a newspaper columnist obliged to enter a rehabilitation program for alcoholism in the dramedy "28 Days"; it was a moderate commercial success with a global gross of US$62.1 million. Bullock produced the romantic comedy "Kate & Leopold", released in 2001, then starred in the psychological thriller "Murder by Numbers" (2002) as a seasoned homicide detective. Roger Ebert awarded the film three stars out of a possible four, stating: "Bullock does a good job here of working against her natural likability, creating a character you'd like to like, and could like, if she weren't so sad, strange and turned in upon herself."

Bullock teamed up with Hugh Grant for the romantic comedy "Two Weeks Notice" (2002) in which she starred as a lawyer who walks out on her boss. Liz Braun, of "Jam! Movies", found Bullock and Grant to be "perfectly paired", stating: "The script allows the two actors to be at their comedic best, even though the film as a whole is amateurish in many ways". "Two Weeks Notice" made US$199 million globally. She was presented with the Raul Julia Award for Excellence in 2002 for helping expand career openings for Hispanic talent in the media and entertainment industry as the executive producer of the sitcom "George Lopez" (2002–2007). She also made several appearances on the show as Accident Amy, an accident-prone employee at the factory Lopez's character manages.

As part of a large ensemble cast, Bullock played the wife of a district attorney in the drama "Crash" (2004), which won the Academy Award for Best Picture. She received positive reviews for her performance; some critics suggested that it was the best performance of her career. In 2005, she received a US$17.5 million salary for "" and was a co-recipient of the Women in Film Crystal Award. In the romantic drama "The Lake House" (2006), Bullock reunited with Keanu Reeves although their characters were separated throughout the film; they were only on set together for two weeks during filming. The film had a negative critical response but made US$114.8 million. In 2006 Bullock played Harper Lee in "Infamous", a drama based on George Plimpton's 1997 book, "Truman Capote: In Which Various Friends, Enemies, Acquaintances, and Detractors Recall His Turbulent Career".

Bullock headlined the supernatural thriller "Premonition" (2007) as a housewife who experiences the days surrounding her husband's death in non-chronological order. Despite negative reviews, several critics, including Rex Reed, commended Bullock for her performance and the film grossed US$84.1 million around the globe. In 2008 Bullock was announced as the face of the cosmetic brand Artistry.

Bullock had two record highs in 2009. The romantic comedy "The Proposal", with Ryan Reynolds, grossed US$317 million at the box office worldwide which made it her fourth-most successful picture to date. She received her third Golden Globe Award nomination for Best Performance by an Actress in a Motion Picture – Musical or Comedy. The drama "The Blind Side" opened at number two behind "" with US$34.2 million, making it her second-highest opening weekend ever. "The Blind Side" grossed over US$309 million, making it her highest-grossing domestic film, her fourth-highest-grossing film worldwide, and the first one in history to pass the US$200 million mark with only one top-billed female star. Bullock had initially turned down the role of Leigh Anne Tuohy three times due to discomfort in portraying a devout Christian. She was awarded the Academy Award, Golden Globe Award, Screen Actors Guild Award and Critics' Choice Movie Award for Best Actress. "The Blind Side" also received an Academy Award for Best Picture nomination.

Winning the Oscar also gave Bullock another unique distinction—since she won two Razzies the day before for her performance in "All About Steve" (2009)—she is the only performer ever to have been named both best and worst for the same year. Following a two-year hiatus from the screen, Bullock starred alongside Tom Hanks as a widow of the September 11 attacks in the drama "Extremely Loud & Incredibly Close", a film adaptation based on the novel of the same name. Despite mixed reviews, the film was nominated for numerous awards including an Academy Award for Best Picture. Bullock was nominated for Best Actress Drama by Teen Choice Awards.

In 2013, Bullock starred alongside Melissa McCarthy in the comedy "The Heat" as an FBI Special Agent who, along with a city detective, must take down a mobster in Boston. It received positive reviews from critics, and took in US$230 million at the box office worldwide. Bullock played an astronaut stranded in space in the sci-fi thriller "Gravity", opposite George Clooney, which premiered at the 70th Venice Film Festival and was released on October 4, 2013 to coincide with the beginning of World Space Week. "Gravity" received universal acclaim among critics and a standing ovation in Venice. The film was called "the most realistic and beautifully choreographed film ever set in space" and some critics called Bullock's performance the best work of her career. "Variety" wrote:

"Gravity" took in US$716 million at the box office worldwide and made it Bullock's second-most successful picture. For her role as Dr. Ryan Stone, Bullock was nominated for the Academy Award, Golden Globe Award, BAFTA Award, Screen Actors Guild Award, and Critics' Choice Movie Award for Best Actress.

In 2015 Bullock provided the voice of the villain in the animated film "Minions", which became her highest-grossing film to date with over US$1.1 billion worldwide and she executive produced and starred, as a political consultant hired to help win a Bolivian presidential election, in the drama "Our Brand Is Crisis" based on the 2005 documentary film of the same name by Rachel Boynton. Upon the film's release, which was a critical and commercial flop, she took another sabbatical from film. Bullock returned in an all-female spin-off of the "Ocean's Eleven" franchise, "Ocean's 8" (2018) directed by Gary Ross. Bullock plays Debbie Ocean, the sister of Danny Ocean, who helps plan a sophisticated heist of the annual Met Gala in New York City. The film was a commercial success grossing US$296.9 million globally.

Bullock played Malorie, a woman who must find a way to guide herself and her children to safety despite the potential threat from an unseen adversary, in the Netflix post-apocalyptic horror film "Bird Box" (2018), based on the novel of the same name. Bullock received universal acclaim for her work. "Variety" found her to be "wonderfully self-reliant" while "TheWrap" described her performance as "fascinating and terrifying to watch." Bullock's films have grossed over US$5.3 billion worldwide and her total domestic gross stands at over US$2.6 billion.

Bullock will next star in an untitled drama film directed by Nora Fingscheidt for Netflix.

Since her acting debut, Bullock has been dubbed "America's sweetheart" in the media due to her "friendly and direct and so unpretentious" nature.

She was selected as one of "People" magazine's 50 Most Beautiful People in the world in 1996 and 1999 and was also ranked number 58 on "Empire" magazine's Top 100 Movie Stars of All Time list. On March 24, 2005, Bullock received a motion picture star on the Hollywood Walk of Fame at 6801 Hollywood Boulevard in Hollywood.

In 2010, "Time" magazine included Bullock in its annual "Time" 100 as one of the most influential people in the world. Bullock was selected by "People" magazine as its 2010 Woman of the Year and ranked number 12 on "People"s Most Beautiful 2011 list.

In September 2013, Bullock joined other Hollywood legends at the TCL Chinese Theatre on Hollywood Boulevard in making imprints of her hands and feet in cement of the theater's forecourt. In November 2013 "The Hollywood Reporter" named Bullock among the most powerful women in entertainment and she was also named "Entertainment Weekly"s Entertainer of the Year due to her success with "The Heat" and "Gravity".

Bullock ranked number two on the 2014 "Forbes" list of most powerful actresses and was honored with the Decade of Hotness Award by Spike Guys' Choice Awards. She was named the Most Beautiful Woman by "People" in 2015.

Bullock owns the production company Fortis Films. She was an executive producer of the "George Lopez" sitcom (co-produced with Robert Borden and Bruce Helford), which garnered a syndication deal of US$10 million. Bullock tried to produce a film based on F.X. Toole's short story "Million Dollar Baby" but could not interest the studios in a female boxing drama. The story was eventually adapted and directed by Clint Eastwood as the Oscar-winning film "Million Dollar Baby" (2004). Fortis Films also produced "All About Steve" which was released in September 2009. Her father, John Bullock, was the company's CEO and her sister, Gesine Bullock-Prado, is the former president.

In November 2006, Bullock founded an Austin, Texas, restaurant named Bess Bistro which was located on West 6th Street. She later opened another business, Walton's Fancy and Staple, across the street in a building she extensively renovated. Walton's is a bakery, upscale restaurant, and floral shop that also offers services including event planning. After almost nine years in business, Bess Bistro closed on September 20, 2015.

Bullock has been a public supporter of the American Red Cross and has donated US$1 million to the organization at least five times. Her first public donation of that amount was to the Red Cross's Liberty Disaster Relief Fund. Three years later, she sent money in response to the 2004 Indian Ocean earthquake and tsunamis. In 2010, she donated US$1 million to relief efforts in Haiti following the Haiti earthquake and again donated the same amount following the 2011 Tōhoku earthquake and tsunami. She donated US$1 million in 2017 to support Red Cross relief efforts for Hurricane Harvey in Texas.

Along with other stars, Bullock did a public service announcement urging people to sign a petition for clean-up efforts of the Deepwater Horizon oil spill in the Gulf of Mexico. Bullock backs the Texas non-profit organization The Kindred Life Foundation, Inc. (KLF) and in late 2008 joined other top celebrities in supporting the work of KLF's founder and CEO, Amos Ramirez. At a fundraising gala for the organization, Bullock said, "Amos has led many efforts across our nation that have helped families that are in need. Our country needs more organizations that are committed to the service that Kindred Life is."

In 2012, Bullock was inducted into the Warren Easton Hall of Fame for her donations to charities. She was honored in 2013 with the Favorite Humanitarian Award at the 39th People's Choice Awards for her contributions to New Orleans' Warren Easton High School, which was severely damaged by Hurricane Katrina.

Bullock was once engaged to actor Tate Donovan, whom she met while filming "Love Potion No. 9". Their relationship lasted three years. She previously dated football player Troy Aikman and actors Matthew McConaughey and Ryan Gosling.

Bullock married motorcycle builder and "Monster Garage" host Jesse James on July 16, 2005. They first met when Bullock arranged for her ten-year-old godson to meet James as a Christmas present. In November 2009, Bullock and James entered into a custody battle with James' second ex-wife, former adult film actress Janine Lindemulder, with whom James had a child. Bullock and James subsequently won full legal custody of James' five-year-old daughter. A scandal arose in March 2010 when several women claimed to have had affairs with James during his marriage to Bullock. Bullock canceled European promotional appearances for "The Blind Side" citing "unforeseen personal reasons".

On March 18, 2010, James responded to the rumors of infidelity by issuing a public apology to Bullock. He stated, "The vast majority of the allegations reported are untrue and unfounded ... beyond that, I will not dignify these private matters with any further public comment." James declared, "There is only one person to blame for this whole situation, and that is me." He asked that Bullock and their children one day "find it in their hearts to forgive me" for their "pain and embarrassment". James' publicist subsequently announced on March 30, 2010, that James had checked into a rehabilitation facility to "deal with personal issues" and save his relationship to Bullock. However, on April 28, 2010, it was reported that Bullock had filed for divorce on April 23 in Austin, Texas. Their divorce was finalized on June 28, 2010, with "conflict of personalities" cited as the reason.

Bullock announced on April 28, 2010, that she had proceeded with plans to adopt a son born in January 2010 in New Orleans, Louisiana. Bullock and James had begun an initial adoption process four months earlier. Bullock's son began living with them in January 2010, but they chose to keep the news private until after the Oscars in March 2010. However, given the couple's separation and then divorce, Bullock continued the adoption of her son as a single parent. Bullock announced in December 2015 that she had adopted a second child and appeared on the cover of "People" magazine with her then three-year-old new daughter.

On December 20, 2000, Bullock was in a private jet crash on a runway from which she and the two crew escaped uninjured. The pilots were unable to activate the runway lights during a night landing at Jackson Hole Airport due to the use of out-of-date approach plates but continued the landing anyway. The aircraft landed in the airport's graded safety area between the runway and parallel taxiway and hit a snowbank. The accident caused a separation of the nose cone and landing gear, partial separation of the right wing, and a bend in the left wing.

While Bullock was in Massachusetts on April 18, 2008, shooting the film "The Proposal", she and her then-husband Jesse James were in a vehicle that was hit head-on by a drunk driver. They were uninjured.

Beginning in 2002 Bullock was stalked across several states by a man named Thomas James Weldon. Bullock obtained a restraining order against him in 2003, which was renewed in 2006. After the restraining order expired and Weldon was released from a mental institution, he again traveled across several states to find Bullock; she then obtained another restraining order.

Bullock won a multimillion-dollar judgment against Benny Daneshjou, the builder of her Lake Austin, Texas, home in October 2004. The jury ruled that the house was uninhabitable. It has since been torn down and rebuilt. Daneshjou and his insurer later settled with Bullock for roughly half the awarded verdict.

On April 22, 2007, a woman named Marcia Diana Valentine was found lying outside James and Bullock's home in Orange County, California. When James confronted the woman, she ran to her car, got behind the wheel, and tried to run over him. She was said to be an obsessed fan of Bullock. Valentine was charged with one felony count each of aggravated assault and stalking. Bullock obtained a restraining order to bar Valentine from "contacting or coming near her home, family or work for three years". Valentine pleaded not guilty to charges of aggravated assault and stalking. She was subsequently convicted of stalking and sentenced to three years' probation.

Joshua James Corbett broke into Bullock's Los Angeles home in June 2014. Bullock locked herself in a room and dialed 911. Corbett pleaded no contest in 2017 and was sentenced to five years' probation for stalking Bullock and breaking into her residence. He was then subject to a ten-year protective order that required him to stay away from Bullock. After Corbett missed a court date the previous month, police officers went to his parents' residence on May 2, 2018, where he lived in a guest house, to arrest him. He refused to leave and threatened to shoot officers. A SWAT team was called and, after a five-hour standoff, they deployed gas canisters and entered the house where they found Corbett had committed suicide. Corbett's death was the result of "multiple incised wounds" according to the Los Angeles County coroner.




</doc>
<doc id="29458" url="https://en.wikipedia.org/wiki?curid=29458" title="Smallfilms">
Smallfilms

Smallfilms is a British television production company that made animated TV programmes for children from 1959 until the 1980s. In 2014 the company began operating again, producing a new series of its most famous show, "The Clangers". It was originally a partnership between Oliver Postgate (writer, animator and narrator) and Peter Firmin (modelmaker and illustrator). Several very popular series of short films were made using stop-motion animation, including "Clangers", "Noggin the Nog" and "Ivor the Engine". Another Smallfilms production, "Bagpuss", came top of a BBC poll to find the favourite British children's programme of the 20th century.

In 1957, Postgate was appointed a stage manager with Associated-Rediffusion, the company that then held the commercial weekday television franchise for London. Attached to the children's programming section, he thought he could do better with the relatively low budgets of the then black and white television productions.

He wrote "Alexander the Mouse", a story about a mouse born to be king. Using an Irish-produced magnetic system—on which animated characters were magnetically attached to a painted background, then filmed using a 45 degree mirror—he persuaded Peter Firmin, who was then teaching at the Central School of Art, to create the painted backgrounds. Postgate later recalled that they broadcast around 26 of these programmes live-to-air, a task made harder by the production problems encountered by the use and restrictions of using magnets.

After the relative success of "Alexander the Mouse", Postgate agreed a deal to make his next series on film, for a budget of £175 per programme (a minuscule amount even at that time). Making a stop motion animation table in his bedroom, he wrote the Chinese serial "The Journey of Master Ho": a formal Chinese epic, about a small boy and a water-buffalo. This was intended for deaf children, a distinct advantage in that the production required no soundtrack, which reduced production costs. He engaged a painter to produce the backgrounds, but as the painter was classically Chinese-trained he produced them in three-quarter view, rather than in the conventional Egyptian full-view manner needed for flat animation under a camera. This resulted in the Firmin-produced characters looking as if they were short in one leg, but the success of the production provided the foundation for Postgate and Firmin to start up their own company, solely producing animated children's television programmes, initially for ITV, but soon afterward with the BBC.

Postgate's initial BBC career was not solely concerned with Smallfilms. To gain experience, he accepted a contract as a television director in the BBC Children's Department in 1960, on a show entitled "Little Laura", another animated series made on film, written and drawn by V. H. Drummond. The series continued in production until 1962, with Postgate credited also as animator on the 1962 series. He also wrote serials for long-running BBC children's programmes "Blue Peter" and stories for "Vision On".

Setting up the business in a disused cowshed at Firmin's home in Blean near Canterbury, Kent, Postgate and Firmin made children's animation programmes, based on concepts that mostly originated from Postgate. Firmin did the artwork and built the models, whilst Postgate wrote the scripts, did the stop motion filming, and voiced many of the characters. "Smallfilms" was able to produce two minutes of film per day, ten times as much as a conventional animation studio, with Postgate moving the (originally cardboard) characters himself, and working his 16mm camera frame-by-frame with a home-made clicker. As Postgate voiced so many of the productions, including the WereBear story tapes, his distinctive voice became familiar to generations of children.

They began in 1959 with "Ivor the Engine", a series for ITV about a Welsh steam locomotive who wanted to sing in a choir, based on Postgate's wartime encounter with Welshman Denzyl Ellis, who was once a fireman on the Royal Scot. It was remade in colour for the BBC in the 1970s. This was followed, also in 1959, by "Noggin the Nog", their first production for the BBC, which established Smallfilms as a safe and reliable pair of hands to produce children's entertainment, in the days when the number of UK television channels was restricted to two.

In 2000, Postgate and his friend Loaf set up a small publishing company called The Dragons Friendly Society, to look after "Noggin the Nog", "Pogles' Wood" and "Pingwings".

After Postgate's death in December 2008, Smallfilms was inherited by his son Daniel. Universal took the distribution rights to the works of Smallfilms. Any such agreement does not include the materials published through The Dragons Friendly Society.

In 2014, Postgate's son, Daniel Postgate, collaborated with Peter Firmin on the production of a new series of "Clangers", with Daniel writing many of the episodes.

Postgate and Firmin recognised that their product was not sold to children, but to commissioning television executives. Postgate described in a later interview the then "gentlemanly and rather innocent" business of programme commissioning thus: "We would go to the BBC once a year, show them the films we'd made, and they would say: 'Yes, lovely, now what are you going to do next?' We would tell them, and they would say: 'That sounds fine, we'll mark it in for eighteen months from now,' and we would be given praise and encouragement and some money in advance, and we'd just go away and do it." The only occasion that this informal arrangement caused any real difficulty emerged in the 1965 series "The Pogles", which BBC management felt was too frightening for the intended audience, and led to their asking for a change of direction: resulting in a revised show, and a change of name to "Pogles' Wood".

Postgate had strict views regarding storylines, which perhaps limited the possibilities for series development. When asked if the "Clangers" adventures were quite surreal sometimes, Postgate replied: "They're surreal but logical. I have a strong prejudice against fantasy for its own sake. Once one gets to a point beyond where cause-and-effect mean anything at all, then science fiction becomes science nonsense. Everything that happened was strictly logical, according to the laws of physics which happened to apply in that part of the world."

In June 2015, the BBC's Mark Savage reported: "Firmin said the "Clangers" surrealism had led to accusations that Postgate was taking hallucinogenic drugs". Firmin told Savage: "People used to say, 'Ooh, what's Oliver on, with all of these weird ideas?' And we used to say, 'He's on cups of tea and biscuits.'"

The Smallfilms system was reliant on the company's two key employees, Postgate and Firmin, and was devoid of modern considerations and essentials, as Postgate pointed out: "[We were] excused the interference of educationalists, sociologists and other pseudo-scientists, which produces eventually a confection of formulae which have no integrity. No, the mainspring of what we did was because it was fun."

Recognising their commissioning audience, Smallfilms purposefully developed storylines that would engage both adults and children. While the storylines and production were remembered by children, the adult jokes, like those about the Welsh in "Ivor the Engine", or the fact that the Clangers swore occasionally, gave the films an instant parental engagement, and a later nostalgic revival amongst former children re-watching their favourite programmes.

From October 2008 until 2013, production company Coolabi held the merchandising and distribution rights to a number of the Smallfilms productions. Coolabi hoped to introduce "Bagpuss" to a new generation, saying that there was "significant potential to build on the affection in which this classic brand is held".

However, in the event it was Smallfilms itself that returned the classic shows to production, agreeing a deal with the BBC in 2014 to produce a further 52 episodes of "Clangers", as a third series of that show for broadcasting in 2015, which the company also pre-sold in the United States.




</doc>
<doc id="29460" url="https://en.wikipedia.org/wiki?curid=29460" title="List of mayors of Sacramento, California">
List of mayors of Sacramento, California

This is a list of Mayors of Sacramento, California. The Sacramento City Council met for the first time on August 1, 1849 and the citizens approved the city charter on October 13, 1849. The City Charter was recognized by the State of California on February 27, 1850 and Sacramento was incorporated on March 18, 1850.

"See also :" Lists of incumbents



</doc>
<doc id="29462" url="https://en.wikipedia.org/wiki?curid=29462" title="Sabotage">
Sabotage

Sabotage is a deliberate action aimed at weakening a polity, effort, or organization through subversion, obstruction, disruption, or destruction. One who engages in sabotage is a "saboteur". Saboteurs typically try to conceal their identities because of the consequences of their actions and to avoid Invoking legal and organizational requirements for addressing sabotage.

The English word derives from the French word "Saboter", meaning to “bungle, botch, wreck or sabotage”, and was originally used to refer to labour disputes, in which workers wearing wooden shoes called interrupted production through different means. A popular but the incorrect account of the origin of the term's present meaning is the story that poor workers in France would throw a wooden "sabot" into the machines to disrupt production.

One of the first appearances of "Saboter" and "Saboteur" in French literature is in the "Dictionnaire du Bas-Langage ou manières de parler usitées parmi le peuple" of D'Hautel, edited in 1808. In it the literal definition is to “ make noise with sabots” as well as “bungle, jostle, hustle, haste.” The word "Sabotage" only appears later.

The word "Sabotage" is found in 1873–1874 in the "Dictionnaire de la langue française" of Émile Littré. Here it is defined mainly as “ making sabots, sabot maker”.It is at the end of the 19th century that it really began to be used with the meaning of "deliberately and maliciously destroying property" or "working slower". In 1897, Émile Pouget, a famous syndicalist and anarchist wrote "action de saboter un travail" (action of sabotaging or bungling a work) in "Le Père Peinard" and in 1911 he also wrote a book entitled "Le Sabotage".

At the inception of the Industrial Revolution, skilled workers such as the Luddites (1811–1812) used sabotage as a means of negotiation in labor disputes.

Labor unions such as the Industrial Workers of the World (IWW) have advocated sabotage as a means of self-defense and direct action against unfair working conditions.

The IWW was shaped in part by the industrial unionism philosophy of Big Bill Haywood, and in 1910 Haywood was exposed to sabotage while touring Europe:

The experience that had the most lasting impact on Haywood was witnessing a general strike on the French railroads. Tired of waiting for parliament to act on their demands, railroad workers walked off their jobs all across the country. The French government responded by drafting the strikers into the army and then ordering them back to work. Undaunted, the workers carried their strike to the job. Suddenly, they could not seem to do anything right. Perishables sat for weeks, sidetracked and forgotten. Freight bound for Paris was misdirected to Lyon or Marseille instead. This tactic — the French called it "sabotage" — won the strikers their demands and impressed Bill Haywood.
For the IWW, sabotage's meaning expanded to include the original use of the term: any withdrawal of efficiency, including the slowdown, the strike, working to rule, or creative bungling of job assignments. 

One of the most severe examples was at the construction site of the Robert-Bourassa Generating Station in 1974, in Québec, Canada, when workers used bulldozers to topple electric generators, damaged fuel tanks, and set buildings on fire. The project was delayed a year, and the direct cost of the damage estimated at $2 million CAD. The causes were not clear, but three possible factors have been cited: inter-union rivalry, poor working conditions, and the perceived arrogance of American executives of the contractor, Bechtel Corporation.

Certain groups turn to the destruction of property to stop environmental destruction or to make visible arguments against forms of modern technology they consider detrimental to the environment. The U.S. Federal Bureau of Investigation (FBI) and other law enforcement agencies use the term eco-terrorist when applied to damage of property. Proponents argue that since property cannot feel terror, damage to property is more accurately described as sabotage. Opponents, by contrast, point out that property owners and operators can indeed feel terror. The image of the monkey wrench thrown into the moving parts of a machine to stop it from working was popularized by Edward Abbey in the novel "The Monkey Wrench Gang" and has been adopted by eco-activists to describe destruction of earth damaging machinery.

From 1992 to late 2007 a radical environmental activist movement known as ELF or Earth Liberation Front engaged in a near constant campaign of decentralized sabotage of any construction projects near wild lands and extractive industries such as logging and even the burning down of a ski resort of Vail Colorado. ELF used sabotage tactics often in loose coordination with other environmental activist movements to physically delay or destroy threats to wild lands as the political will developed to protect the targeted wild areas that ELF engaged.

In war, the word is used to describe the activity of an individual or group not associated with the military of the parties at war, such as a foreign agent or an indigenous supporter, in particular when actions result in the destruction or damaging of a productive or vital facility, such as equipment, factories, dams, public services, storage plants or logistic routes. Prime examples of such sabotage are the events of Black Tom and the Kingsland Explosion. Like spies, saboteurs who conduct a military operation in civilian clothes or enemy uniforms behind enemy lines are subject to prosecution and criminal penalties instead of detention as prisoners of war. It is common for a government in power during war or supporters of the war policy to use the term loosely against opponents of the war. Similarly, German nationalists spoke of a stab in the back having cost them the loss of World War I.

A modern form of sabotage is the distribution of software intended to damage specific industrial systems. For example, the U.S. Central Intelligence Agency (CIA) is alleged to have sabotaged a Siberian pipeline during the Cold War, using information from the Farewell Dossier. A more recent case may be the Stuxnet computer worm, which was designed to subtly infect and damage specific types of industrial equipment. Based on the equipment targeted and the location of infected machines, security experts believe it was an attack on the Iranian nuclear program by the United States, Israel or, according to the latest news, even Russia.

Sabotage, done well, is inherently difficult to detect and difficult to trace to its origin. During World War II, the U.S. Federal Bureau of Investigation (FBI) investigated 19,649 cases of sabotage and concluded the enemy had not caused any of them.

Sabotage in warfare, according to the Office of Strategic Services (OSS) manual, varies from highly technical "coup de main" acts that require detailed planning and specially trained operatives, to innumerable simple acts that ordinary citizen-saboteurs can perform. Simple sabotage is carried out in such a way as to involve a minimum danger of injury, detection, and reprisal. There are two main methods of sabotage; physical destruction and the "human element". While physical destruction as a method is self-explanatory, its targets are nuanced, reflecting objects to which the saboteur has normal and inconspicuous access in everyday life. The "human element" is based on universal opportunities to make faulty decisions, to adopt a non-cooperative attitude, and to induce others to follow suit.

There are many examples of physical sabotage in wartime. However, one of the most effective uses of sabotage is against organizations. The OSS manual provides numerous techniques under the title "General Interference with Organizations and Production":


From the section entitled, "General Devices for Lowering Morale and Creating Confusion" comes the following quintessential simple sabotage advice: "Act stupid."

The United States Office of Strategic Services, later renamed the CIA, noted the specific value in committing simple sabotage against the enemy during wartime: "... slashing tires, draining fuel tanks, starting fires, starting arguments, acting stupidly, short-circuiting electric systems, abrading machine parts will waste materials, manpower, and time." To underline the importance of simple sabotage on a widespread scale, they wrote, "Widespread practice of simple sabotage will harass and demoralize enemy administrators and police." The OSS was also focused on the battle for hearts and minds during wartime; "the very practice of simple sabotage by natives in enemy or occupied territory may make these individuals identify themselves actively with the United Nations War effort, and encourage them to assist openly in periods of Allied invasion and occupation."

On 30 July 1916, the Black Tom explosion occurred when German agents set fire to a complex of warehouses and ships in Jersey City, New Jersey that held munitions, fuel, and explosives bound to aid the Allies in their fight.

On 11 January 1917, Fiodore Wozniak, using a rag saturated with phosphorus or an incendiary pencil supplied by German sabotage agents, set fire to his workbench at an ammunition assembly plant near Lyndhurst, New Jersey, causing a four-hour fire that destroyed half a million 3-inch explosive shells and destroyed the plant for an estimated at $17 million in damages. Wozniak's involvement was not discovered until 1927.

On 12 February 1917, Bedouins allied with the British destroyed a Turkish railroad near the port of Wajh, derailing a Turkish locomotive. The Bedouins traveled by camel and used explosives to demolish a portion of track.

In Ireland, the Irish Republican Army (IRA) used sabotage against the British following the Easter 1916 uprising. The IRA compromised communication lines and lines of transportation and fuel supplies. The IRA also employed passive sabotage, refusing dock and train workers to work on ships and rail cars used by the government. In 1920, agents of the IRA committed arson against at least fifteen British warehouses in Liverpool. The following year, the IRA set fire to numerous British targets again, including the Dublin Customs House, this time sabotaging most of Liverpool's firetrucks in the firehouses before lighting the matches.

Lieutenant Colonel George T. Rheam was a British soldier, who ran Brickendonbury Manor from October 1941 to June 1945 during World War II, which was Station XVII of the Special Operations Executive (SOE), which trained specialists for the SOE. Rheam innovated many sabotage techniques, and is considered by M. R. D. Foot the "founder of modern industrial sabotage."

Sabotage training for the Allies consisted of teaching would-be saboteurs key components of working machinery to destroy.
"Saboteurs learned hundreds of small tricks to cause the Germans big trouble. The cables in a telephone junction box ... could be jumbled to make the wrong connections when numbers were dialed. A few ounces of plastique, properly placed, could bring down a bridge, cave in a mine shaft, or collapse the roof of a railroad tunnel."

The Polish Home Army Armia Krajowa, who commanded the majority of resistance organizations in Poland (even the National Forces, except the Military Organization Lizard Union; The Home Army also included the Polish Socialist Party – Freedom, Equality, Independence) and coordinating and aiding the Jewish Military Union as well as more reluctantly helping the Jewish Combat Organization, was responsible for the greatest number of acts of sabotage in German—occupied Europe. The Home Army's sabotage operations Operation Garland and Operation Ribbon are just two examples. In all, the Home Army damaged 6,930 locomotives, set 443 rail transports on fire, damaged over 19,000 rail cars "wagony", and blew up 38 rail bridges, not to mention the attacks against the railroads. The Home Army was also responsible for 4,710 built-in flaws in parts for aircraft engines and 92,000 built-in flaws in artillery projectiles, among other examples of significant sabotage. In addition, over 25,000 acts of more minor sabotage were committed. It continued to fight against both the Germans and the Soviets; however, it did aid the Western Allies by collecting constant and detailed information on the German rail, wheeled, and horse transports. As for Stalin's proxies, their actions led to a great number of the Polish and Jewish hostages, mostly civilians, being murdered in reprisal by the Germans. The Gwardia Ludowa destroyed around 200 German trains during the war, and indiscriminately threw hand grenades into places frequented by Germans.

The French Resistance ran an extremely effective sabotage campaign against the Germans during World War II. Receiving their sabotage orders through messages over the BBC radio or by aircraft, the French used both passive and active forms of sabotage. Passive forms included losing German shipments and allowing poor quality material to pass factory inspections. Many active sabotage attempts were against critical rail lines of transportation. German records count 1,429 instances of sabotage from French Resistance forces between January 1942 and February 1943. From January through March 1944, sabotage accounted for three times the number of locomotives damaged by Allied air power. See also Normandy Landings for more information about sabotage on D-Day.

During World War II, the Allies committed sabotage against the Peugeot truck factory. After repeated failures in Allied bombing attempts to hit the factory, a team of French Resistance fighters and Special Operations Executive (SOE) agents distracted the German guards with a game of soccer while part of their team entered the plant and destroyed machinery.

In December 1944, the Germans ran a false flag sabotage infiltration, Operation Greif, which was commanded by Waffen-SS commando Otto Skorzeny during the Battle of the Bulge. German commandos, wearing US Army uniforms, carrying US Army weapons, and using US Army vehicles, penetrated US lines to spread panic and confusion among US troops and to blow up bridges, ammunition dumps, and fuel stores and to disrupt the lines of communication. Many of the commandos were captured by the Americans. Because they were wearing US uniforms, a number of the Germans were executed as spies, either summarily or after military commissions.

From 1948 to 1960, the Malayan Communists committed numerous effective acts of sabotage against the British Colonial authorities, first targeting railway bridges, then hitting larger targets such as military camps. Most of their efforts were centered around crippling Malaysia's colonial economy and involved sabotage against trains, rubber trees, water pipes, and electric lines. The Communists' sabotage efforts were so successful that they caused backlash among the Malaysian population, who gradually withdrew support for the Communist movement as their livelihoods became threatened.

In Mandatory Palestine from 1945 to 1948, Jewish groups opposed British control. Though that control was to end according to the United Nations Partition Plan for Palestine in 1948, the groups used sabotage as an opposition tactic. The Haganah focused their efforts on camps used by the British to hold refugees and radar installations that could be used to detect illegal immigrant ships. The Stern Gang and the Irgun used terrorism and sabotage against the British government and against lines of communications. In November 1946, the Irgun and Stern Gang attacked a railroad twenty-one times in a three-week period, eventually causing shell-shocked Arab railway workers to strike. The 6th Airborne Division was called in to provide security as a means of ending the strike.

The Viet Cong used swimmer saboteurs often and effectively during the Vietnam War. Between 1969 and 1970, swimmer saboteurs sunk, destroyed, or damaged 77 assets of the U.S. and its allies. Viet Cong swimmers were poorly equipped but well-trained and resourceful. The swimmers provided a low-cost/low-risk option with high payoff; possible loss to the country for failure compared to the possible gains from a successful mission led to the obvious conclusion the swimmer saboteurs were a good idea.

On 1 January 1984, the Cuscatlan bridge over Lempa river in El Salvador, critical to flow of commercial and military traffic, was destroyed by guerrilla forces using explosives after using mortar fire to "scatter" the bridge's guards, causing an estimated $3.7 million in required repairs, and considerably impacting on El Salvadoran business and security.

In 1982 in Honduras, a group of nine Salvadorans and Nicaraguans destroyed a main electrical power station, leaving the capital city Tegucigalpa without power for three days.

Some criminals have engaged in acts of sabotage for reasons of extortion. For example, Klaus-Peter Sabotta sabotaged German railway lines in the late 1990s in an attempt to extort DM10 million from the German railway operator Deutsche Bahn. He is now serving a sentence of life imprisonment. In 1989, ex-Scotland Yard detective Rodney Whitchelo was sentenced to 17 years in prison for spiking Heinz baby food products in supermarkets, in an extortion attempt on the food manufacturer.

The term political sabotage is sometimes used to define the acts of one political camp to disrupt, harass or damage the reputation of a political opponent, usually during an electoral campaign, such as during Watergate. Smear campaigns are a commonly used tactic. The term could also describe the actions and expenditures of private entities, corporations, and organizations against democratically approved or enacted laws, policies and programs.

After the Cold War ended, the Mitrokhin Archives were declassified, which included detailed KGB plans of active measures to subvert politics in opposing nations.

Sabotage is a crucial tool of the successful coup d'etat, which requires control of communications before, during, and after the coup is staged. Simple sabotage against physical communications platforms using semi-skilled technicians, or even those trained only for this task, could effectively silence the target government of the coup, leaving the information battle space open to the dominance of the coup's leaders. To underscore the effectiveness of sabotage, "A single cooperative technician will be able temporarily to put out of action a radio station which would otherwise require a full-scale assault."

Railroads, were strategically important to the regime the coup is against, are prime targets for sabotage—if a section of the track is damaged entire portions of the transportation network can be stopped until it is fixed.

A sabotage radio was a small two-way radio designed for use by resistance movements in World War II, and after the war often used by expeditions and similar parties.

Arquilla and Rondfeldt, in their work entitled "Networks and Netwars", differentiate their definition of "netwar" from a list of "trendy synonyms", including "cybotage", a portmanteau from the words "sabotage" and "cyber". They dub the practitioners of cybotage "cyboteurs" and note while all cybotage is not netwar, some netwar is cybotage.

Counter-sabotage, defined by "Webster's Dictionary", is "counterintelligence designed to detect and counteract sabotage". The United States Department of Defense definition, found in the "Dictionary of Military and Associated Terms", is "action designed to detect and counteract sabotage. See also counterintelligence".

During World War II, British subject Eddie Chapman, trained by the Germans in sabotage, became a double agent for the British. The German Abwehr entrusted Chapman to destroy the British de Havilland Company's main plant which manufactured the outstanding Mosquito light bomber, but required photographic proof from their agent to verify the mission's completion. A special unit of the Royal Engineers known as the Magic Gang covered the de Havilland plant with canvas panels and scattered papier-mâché furniture and chunks of masonry around three broken and burnt giant generators. Photos of the plant taken from the air reflected devastation for the factory and a successful sabotage mission, and Chapman, as a British sabotage double-agent, fooled the Germans for the duration of the war.

In Japanese, the verb saboru (サボる) means to skip school or loaf on the job.




</doc>
<doc id="29463" url="https://en.wikipedia.org/wiki?curid=29463" title="Scabbard">
Scabbard

A scabbard is a sheath for holding a sword, knife, or other large blade. As well, rifles may be stored in a scabbard by horse riders. Military cavalry and cowboys had scabbards for their saddle ring carbine rifles and lever action rifles on their horses for storage and protection. Scabbards have been made of many materials over the millennia, including leather, wood, and metals such as brass or steel.

Most commonly, sword scabbards were worn suspended from a sword belt or shoulder belt called a baldric.

Wooden scabbards were usually covered in fabric or leather; the leather versions also usually bore metal fittings for added protection and carrying ease. Japanese blades typically have their sharp cutting edge protected by a wooden scabbard called a saya. Many scabbards, such as ones the Greeks and Romans used, were small and light. They were designed for holding the sword rather than protecting it. All-metal scabbards were popular items for a display of wealth among elites in the European Iron Age, and often intricately decorated. Little is known about the scabbards of the early Iron Age, due to their wooden construction. However, during the Middle and late Iron Ages, the scabbard became important especially as a vehicle for decorative elaboration. After 200 BC fully decorated scabbards became rare. A number of ancient scabbards have been recovered from weapons sacrifices, a few of which had a lining of fur on the inside. The fur was probably kept oily, keeping the blade free from rust. The fur would also allow a smoother, quicker draw.

Entirely metal scabbards became popular in Europe early in the 19th century and eventually superseded most other types. Metal was more durable than leather and could better withstand the rigours of field use, particularly among troops mounted on horseback. In addition, metal offered the ability to present a more military appearance, as well as the opportunity to display increased ornamentation. Nevertheless, leather scabbards never entirely lost favour among military users and were widely used as late as the American Civil War (1861–65).

Some military police forces, naval shore patrols, law enforcement and other groups used leather scabbards as a kind of truncheon.

Scabbards were historically, albeit rarely, worn across the back, but only by a handful of Celtic tribes, and only with very short lengths of sword. This is because drawing a long, sharp blade over one's shoulder and past one's head from a scabbard on the back is relatively awkward, especially in a hurry, and the length of the arm sets a hard upper limit on how long a blade can be drawn at all in this way. Sheathing the sword again is even harder since it has to be done effectively blind unless the scabbard is taken off first. Common depictions of long swords being drawn from the back are a modern invention, born from safety and convenience considerations on a film set and typically enabled by creative editing, and have enjoyed such great popularity in fiction and fantasy that they are widely and incorrectly believed to have been common in Medieval times. Some more well-known examples of this include the back scabbard depicted in the film "Braveheart" and the back scabbard seen in the video game series "The Legend of Zelda". There is some limited data from woodcuts and textual fragments that Mongol light horse archers, Chinese soldiers, Japanese Samurai and European Knights wore a slung baldric over the shoulder, allowing longer blades such as greatswords/zweihanders and nodachi/ōdachi to be strapped across the back, though these would have to be removed from the back before the sword could be unsheathed.

In "The Ancient Celts" by Barry Cunliffe, Cunliffe writes, "All these pieces of equipment [shields, spears, swords, mail] mentioned in the texts, are reflected in the archaeological record and in the surviving iconography, though it is sometimes possible to detect regional variations (page 94). Among the Parisii of Yorkshire, for example, the "...sword was sometimes worn across the back and therefore had to be drawn over the shoulder from behind the head."

The metal fitting where the blade enters the leather or metal scabbard is called the throat, which is often part of a larger scabbard mount, or locket, that bears a carrying ring or stud to facilitate wearing the sword. The blade's point in leather scabbards is usually protected by a metal tip, or chape, which, on both leather and metal scabbards, is often given further protection from wear by an extension called a drag, or shoe.



</doc>
<doc id="29467" url="https://en.wikipedia.org/wiki?curid=29467" title="Spinel">
Spinel

Spinel () is the magnesium/aluminium member of the larger spinel group of minerals. It has the formula MgAlO in the cubic crystal system. Its name comes from the Latin word "spinella", which means "spine" in reference to its pointed crystals.

Spinel crystallizes in the isometric system; common crystal forms are octahedra, usually twinned. It has an imperfect octahedral cleavage and a conchoidal fracture. Its hardness is 8, its specific gravity is 3.5–4.1, and it is transparent to opaque with a vitreous to dull luster. It may be colorless, but is usually various shades of pink, rose, red, blue, green, yellow, brown, black, or (uncommon) violet. There is a unique natural white spinel, now lost, that surfaced briefly in what is now Sri Lanka. Some spinels are among the most famous gemstones; among them are the Black Prince's Ruby and the "Timur ruby" in the British Crown Jewels, and the "Côte de Bretagne", formerly from the French Crown jewels. The Samarian Spinel is the largest known spinel in the world, weighing .

The transparent red spinels were called spinel-rubies or balas rubies. In the past, before the arrival of modern science, spinels and rubies were equally known as rubies. After the 18th century the word ruby was only used for the red gem variety of the mineral corundum and the word spinel came to be used. "Balas" is derived from Balascia, the ancient name for Badakhshan, a region in central Asia situated in the upper valley of the Panj River, one of the principal tributaries of the Oxus River. Mines in the Gorno Badakhshan region of Tajikistan constituted for centuries the main source for red and pink spinels.

Spinel is found as a metamorphic mineral, and also as a primary mineral in rare mafic igneous rocks; in these igneous rocks, the magmas are relatively deficient in alkalis relative to aluminium, and aluminium oxide may form as the mineral corundum or may combine with magnesia to form spinel. This is why spinel and ruby are often found together. The spinel petrogenesis in mafic magmatic rocks is strongly debated, but certainly results from mafic magma interaction with more evolved magma or rock (e.g. gabbro, troctolite).

Spinel, (Mg,Fe)(Al,Cr)O, is common in peridotite in the uppermost Earth's mantle, between approximately 20 km to approximately 120 km, possibly to lower depths depending on the chromium content. At significantly shallower depths, above the Moho, calcic plagioclase is the more stable aluminous mineral in peridotite while garnet is the stable phase deeper in the mantle below the spinel stability region.

Spinel, (Mg,Fe)AlO, is a common mineral in the Ca-Al-rich inclusions (CAIs) in some chondritic meteorites.

Spinel has long been found in the gemstone-bearing gravel of Sri Lanka and in limestones of the Badakshan Province in modern-day Afghanistan and Tajikistan; and of Mogok in Myanmar. Over the last decades gem quality spinels are found in the marbles of Lục Yên District (Vietnam), Mahenge and Matombo (Tanzania), Tsavo (Kenya) and in the gravels of Tunduru (Tanzania) and Ilakaka (Madagascar).

Since 2000 in several locations around the world have been discovered spinels with unusual vivid pink or blue color. Such "glowing" spinels are known from Mogok (Myanmar), Mahenge plateau (Tanzania), Lục Yên District (Vietnam) and some more localities. In 2018 bright blue spinels have been reported also in the southern part of Baffin Island (Canada). The pure blue coloration of spinel is caused by small additions of cobalt.

Synthetic spinel, accidentally produced in the middle of the 18th century, has been described more recently in scientific publications in 2000 and 2004. By 2015, transparent spinel was being made in sheets and other shapes through sintering. Synthetic spinel, which looks like glass but has notably higher strength against pressure, can also have applications in military and commercial use.





</doc>
<doc id="29468" url="https://en.wikipedia.org/wiki?curid=29468" title="Speech recognition">
Speech recognition

Speech recognition is an interdisciplinary subfield of computer science and computational linguistics that develops methodologies and technologies that enable the recognition and translation of spoken language into text by computers. It is also known as automatic speech recognition (ASR), computer speech recognition or speech to text (STT). It incorporates knowledge and research in the computer science, linguistics and computer engineering fields.

Some speech recognition systems require "training" (also called "enrollment") where an individual speaker reads text or isolated vocabulary into the system. The system analyzes the person's specific voice and uses it to fine-tune the recognition of that person's speech, resulting in increased accuracy. Systems that do not use training are called "speaker independent" systems. Systems that use training are called "speaker dependent".

Speech recognition applications include voice user interfaces such as voice dialing (e.g. "call home"), call routing (e.g. "I would like to make a collect call"), domotic appliance control, search key words (e.g. find a podcast where particular words were spoken), simple data entry (e.g., entering a credit card number), preparation of structured documents (e.g. a radiology report), determining speaker characteristics, speech-to-text processing (e.g., word processors or emails), and aircraft (usually termed direct voice input).

The term "voice recognition" or "speaker identification" refers to identifying the speaker, rather than what they are saying. Recognizing the speaker can simplify the task of translating speech in systems that have been trained on a specific person's voice or it can be used to authenticate or verify the identity of a speaker as part of a security process.

From the technology perspective, speech recognition has a long history with several waves of major innovations. Most recently, the field has benefited from advances in deep learning and big data. The advances are evidenced not only by the surge of academic papers published in the field, but more importantly by the worldwide industry adoption of a variety of deep learning methods in designing and deploying speech recognition systems.

The key areas of growth were: vocabulary size, speaker independence and processing speed.


Raj Reddy was the first person to take on continuous speech recognition as a graduate student at Stanford University in the late 1960s. Previous systems required users to pause after each word. Reddy's system issued spoken commands for playing chess.

Around this time Soviet researchers invented the dynamic time warping (DTW) algorithm and used it to create a recognizer capable of operating on a 200-word vocabulary. DTW processed speech by dividing it into short frames, e.g. 10ms segments, and processing each frame as a single unit. Although DTW would be superseded by later algorithms, the technique carried on. Achieving speaker independence remained unsolved at this time period.



During the late 1960s Leonard Baum developed the mathematics of Markov chains at the Institute for Defense Analysis. A decade later, at CMU, Raj Reddy's students James Baker and Janet M. Baker began using the Hidden Markov Model (HMM) for speech recognition. James Baker had learned about HMMs from a summer job at the Institute of Defense Analysis during his undergraduate education. The use of HMMs allowed researchers to combine different sources of knowledge, such as acoustics, language, and syntax, in a unified probabilistic model.



The 1980s also saw the introduction of the n-gram language model.

Much of the progress in the field is owed to the rapidly increasing capabilities of computers. At the end of the DARPA program in 1976, the best computer available to researchers was the PDP-10 with 4 MB ram. It could take up to 100 minutes to decode just 30 seconds of speech.

Two practical products were:

By this point, the vocabulary of the typical commercial speech recognition system was larger than the average human vocabulary. Raj Reddy's former student, Xuedong Huang, developed the Sphinx-II system at CMU. The Sphinx-II system was the first to do speaker-independent, large vocabulary, continuous speech recognition and it had the best performance in DARPA's 1992 evaluation. Handling continuous speech with a large vocabulary was a major milestone in the history of speech recognition. Huang went on to found the speech recognition group at Microsoft in 1993. Raj Reddy's student Kai-Fu Lee joined Apple where, in 1992, he helped develop a speech interface prototype for the Apple computer known as Casper.

Lernout & Hauspie, a Belgium-based speech recognition company, acquired several other companies, including Kurzweil Applied Intelligence in 1997 and Dragon Systems in 2000. The L&H speech technology was used in the Windows XP operating system. L&H was an industry leader until an accounting scandal brought an end to the company in 2001. The speech technology from L&H was bought by ScanSoft which became Nuance in 2005. Apple originally licensed software from Nuance to provide speech recognition capability to its digital assistant Siri.

In the 2000s DARPA sponsored two speech recognition programs: Effective Affordable Reusable Speech-to-Text (EARS) in 2002 and Global Autonomous Language Exploitation (GALE). Four teams participated in the EARS program: IBM, a team led by BBN with LIMSI and Univ. of Pittsburgh, Cambridge University, and a team composed of ICSI, SRI and University of Washington. EARS funded the collection of the Switchboard telephone speech corpus containing 260 hours of recorded conversations from over 500 speakers. The GALE program focused on Arabic and Mandarin broadcast news speech. Google's first effort at speech recognition came in 2007 after hiring some researchers from Nuance. The first product was GOOG-411, a telephone based directory service. The recordings from GOOG-411 produced valuable data that helped Google improve their recognition systems. Google Voice Search is now supported in over 30 languages.

In the United States, the National Security Agency has made use of a type of speech recognition for keyword spotting since at least 2006. This technology allows analysts to search through large volumes of recorded conversations and isolate mentions of keywords. Recordings can be indexed and analysts can run queries over the database to find conversations of interest. Some government research programs focused on intelligence applications of speech recognition, e.g. DARPA's EARS's program and IARPA's Babel program.

In the early 2000s, speech recognition was still dominated by traditional approaches such as Hidden Markov Models combined with feedforward artificial neural networks.
Today, however, many aspects of speech recognition have been taken over by a deep learning method called Long short-term memory (LSTM), a recurrent neural network published by Sepp Hochreiter & Jürgen Schmidhuber in 1997. LSTM RNNs avoid the vanishing gradient problem and can learn "Very Deep Learning" tasks that require memories of events that happened thousands of discrete time steps ago, which is important for speech.
Around 2007, LSTM trained by Connectionist Temporal Classification (CTC) started to outperform traditional speech recognition in certain applications. In 2015, Google's speech recognition reportedly experienced a dramatic performance jump of 49% through CTC-trained LSTM, which is now available through Google Voice to all smartphone users.

The use of deep feedforward (non-recurrent) networks for acoustic modeling was introduced during later part of 2009 by Geoffrey Hinton and his students at University of Toronto and by Li Deng and colleagues at Microsoft Research, initially in the collaborative work between Microsoft and University of Toronto which was subsequently expanded to include IBM and Google (hence "The shared views of four research groups" subtitle in their 2012 review paper). A Microsoft research executive called this innovation "the most dramatic change in accuracy since 1979". In contrast to the steady incremental improvements of the past few decades, the application of deep learning decreased word error rate by 30%. This innovation was quickly adopted across the field. Researchers have begun to use deep learning techniques for language modeling as well.

In the long history of speech recognition, both shallow form and deep form (e.g. recurrent nets) of artificial neural networks had been explored for many years during 1980s, 1990s and a few years into the 2000s.
But these methods never won over the non-uniform internal-handcrafting Gaussian mixture model/Hidden Markov model (GMM-HMM) technology based on generative models of speech trained discriminatively. A number of key difficulties had been methodologically analyzed in the 1990s, including gradient diminishing and weak temporal correlation structure in the neural predictive models. All these difficulties were in addition to the lack of big training data and big computing power in these early days. Most speech recognition researchers who understood such barriers hence subsequently moved away from neural nets to pursue generative modeling approaches until the recent resurgence of deep learning starting around 2009–2010 that had overcome all these difficulties. Hinton et al. and Deng et al. reviewed part of this recent history about how their collaboration with each other and then with colleagues across four groups (University of Toronto, Microsoft, Google, and IBM) ignited a renaissance of applications of deep feedforward neural networks to speech recognition.

By early 2010s "speech" recognition, also called voice recognition was clearly differentiated from "speaker" recognition, and speaker independence was considered a major breakthrough. Until then, systems required a "training" period. A 1987 ad for a doll had carried the tagline "Finally, the doll that understands you." – despite the fact that it was described as "which children could train to respond to their voice".

In 2017, Microsoft researchers reached a historical human parity milestone of transcribing conversational telephony speech on the widely benchmarked Switchboard task. Multiple deep learning models were used to optimize speech recognition accuracy. The speech recognition word error rate was reported to be as low as 4 professional human transcribers working together on the same benchmark, which was funded by IBM Watson speech team on the same task.

Both acoustic modeling and language modeling are important parts of modern statistically-based speech recognition algorithms. Hidden Markov models (HMMs) are widely used in many systems. Language modeling is also used in many other natural language processing applications such as document classification or statistical machine translation.

Modern general-purpose speech recognition systems are based on Hidden Markov Models. These are statistical models that output a sequence of symbols or quantities. HMMs are used in speech recognition because a speech signal can be viewed as a piecewise stationary signal or a short-time stationary signal. In a short time-scale (e.g., 10 milliseconds), speech can be approximated as a stationary process. Speech can be thought of as a Markov model for many stochastic purposes.

Another reason why HMMs are popular is because they can be trained automatically and are simple and computationally feasible to use. In speech recognition, the hidden Markov model would output a sequence of "n"-dimensional real-valued vectors (with "n" being a small integer, such as 10), outputting one of these every 10 milliseconds. The vectors would consist of cepstral coefficients, which are obtained by taking a Fourier transform of a short time window of speech and decorrelating the spectrum using a cosine transform, then taking the first (most significant) coefficients. The hidden Markov model will tend to have in each state a statistical distribution that is a mixture of diagonal covariance Gaussians, which will give a likelihood for each observed vector. Each word, or (for more general speech recognition systems), each phoneme, will have a different output distribution; a hidden Markov model for a sequence of words or phonemes is made by concatenating the individual trained hidden Markov models for the separate words and phonemes.

Described above are the core elements of the most common, HMM-based approach to speech recognition. Modern speech recognition systems use various combinations of a number of standard techniques in order to improve results over the basic approach described above. A typical large-vocabulary system would need context dependency for the phonemes (so phonemes with different left and right context have different realizations as HMM states); it would use cepstral normalization to normalize for different speaker and recording conditions; for further speaker normalization it might use vocal tract length normalization (VTLN) for male-female normalization and maximum likelihood linear regression (MLLR) for more general speaker adaptation. The features would have so-called delta and delta-delta coefficients to capture speech dynamics and in addition might use heteroscedastic linear discriminant analysis (HLDA); or might skip the delta and delta-delta coefficients and use splicing and an LDA-based projection followed perhaps by heteroscedastic linear discriminant analysis or a global semi-tied co variance transform (also known as maximum likelihood linear transform, or MLLT). Many systems use so-called discriminative training techniques that dispense with a purely statistical approach to HMM parameter estimation and instead optimize some classification-related measure of the training data. Examples are maximum mutual information (MMI), minimum classification error (MCE) and minimum phone error (MPE).

Decoding of the speech (the term for what happens when the system is presented with a new utterance and must compute the most likely source sentence) would probably use the Viterbi algorithm to find the best path, and here there is a choice between dynamically creating a combination hidden Markov model, which includes both the acoustic and language model information, and combining it statically beforehand (the finite state transducer, or FST, approach).

A possible improvement to decoding is to keep a set of good candidates instead of just keeping the best candidate, and to use a better scoring function (re scoring) to rate these good candidates so that we may pick the best one according to this refined score. The set of candidates can be kept either as a list (the N-best list approach) or as a subset of the models (a lattice). Re scoring is usually done by trying to minimize the Bayes risk (or an approximation thereof): Instead of taking the source sentence with maximal probability, we try to take the sentence that minimizes the expectancy of a given loss function with regards to all possible transcriptions (i.e., we take the sentence that minimizes the average distance to other possible sentences weighted by their estimated probability). The loss function is usually the Levenshtein distance, though it can be different distances for specific tasks; the set of possible transcriptions is, of course, pruned to maintain tractability. Efficient algorithms have been devised to re score lattices represented as weighted finite state transducers with edit distances represented themselves as a finite state transducer verifying certain assumptions.

Dynamic time warping is an approach that was historically used for speech recognition but has now largely been displaced by the more successful HMM-based approach.

Dynamic time warping is an algorithm for measuring similarity between two sequences that may vary in time or speed. For instance, similarities in walking patterns would be detected, even if in one video the person was walking slowly and if in another he or she were walking more quickly, or even if there were accelerations and deceleration during the course of one observation. DTW has been applied to video, audio, and graphics – indeed, any data that can be turned into a linear representation can be analyzed with DTW.

A well-known application has been automatic speech recognition, to cope with different speaking speeds. In general, it is a method that allows a computer to find an optimal match between two given sequences (e.g., time series) with certain restrictions. That is, the sequences are "warped" non-linearly to match each other. This sequence alignment method is often used in the context of hidden Markov models.

Neural networks emerged as an attractive acoustic modeling approach in ASR in the late 1980s. Since then, neural networks have been used in many aspects of speech recognition such as phoneme classification, phoneme classification through multi-objective evolutionary algorithms, isolated word recognition, audiovisual speech recognition, audiovisual speaker recognition and speaker adaptation.

Neural networks make fewer explicit assumptions about feature statistical properties than HMMs and have several qualities making them attractive recognition models for speech recognition. When used to estimate the probabilities of a speech feature segment, neural networks allow discriminative training in a natural and efficient manner. However, in spite of their effectiveness in classifying short-time units such as individual phonemes and isolated words, early neural networks were rarely successful for continuous recognition tasks because of their limited ability to model temporal dependencies.

One approach to this limitation was to use neural networks as a pre-processing, feature transformation or dimensionality reduction, step prior to HMM based recognition. However, more recently, LSTM and related recurrent neural networks (RNNs) and Time Delay Neural Networks(TDNN's) have demonstrated improved performance in this area.

Deep Neural Networks and Denoising Autoencoders are also under investigation. A deep feedforward neural network (DNN) is an artificial neural network with multiple hidden layers of units between the input and output layers. Similar to shallow neural networks, DNNs can model complex non-linear relationships. DNN architectures generate compositional models, where extra layers enable composition of features from lower layers, giving a huge learning capacity and thus the potential of modeling complex patterns of speech data.

A success of DNNs in large vocabulary speech recognition occurred in 2010 by industrial researchers, in collaboration with academic researchers, where large output layers of the DNN based on context dependent HMM states constructed by decision trees were adopted.
recent overview articles.

One fundamental principle of deep learning is to do away with hand-crafted feature engineering and to use raw features. This principle was first explored successfully in the architecture of deep autoencoder on the "raw" spectrogram or linear filter-bank features, showing its superiority over the Mel-Cepstral features which contain a few stages of fixed transformation from spectrograms.
The true "raw" features of speech, waveforms, have more recently been shown to produce excellent larger-scale speech recognition results.

Since 2014, there has been much research interest in "end-to-end" ASR. Traditional phonetic-based (i.e., all HMM-based model) approaches required separate components and training for the pronunciation, acoustic and language model. End-to-end models jointly learn all the components of the speech recognizer. This is valuable since it simplifies the training process and deployment process. For example, a n-gram language model is required for all HMM-based systems, and a typical n-gram language model often takes several gigabytes in memory making them impractical to deploy on mobile devices. Consequently, modern commercial ASR systems from Google and Apple (as of 2017) are deployed on the cloud and require a network connection as opposed to the device locally.

The first attempt at end-to-end ASR was with Connectionist Temporal Classification (CTC)-based systems introduced by Alex Graves of Google DeepMind and Navdeep Jaitly of the University of Toronto in 2014. The model consisted of recurrent neural networks and a CTC layer. Jointly, the RNN-CTC model learns the pronunciation and acoustic model together, however it is incapable of learning the language due to conditional independence assumptions similar to a HMM. Consequently, CTC models can directly learn to map speech acoustics to English characters, but the models make many common spelling mistakes and must rely on a separate language model to clean up the transcripts. Later, Baidu expanded on the work with extremely large datasets and demonstrated some commercial success in Chinese Mandarin and English. In 2016, University of Oxford presented LipNet, the first end-to-end sentence-level lip reading model, using spatiotemporal convolutions coupled with an RNN-CTC architecture, surpassing human-level performance in a restricted grammar dataset. A large-scale CNN-RNN-CTC architecture was presented in 2018 by Google DeepMind achieving 6 times better performance than human experts.

An alternative approach to CTC-based models are attention-based models. Attention-based ASR models were introduced simultaneously by Chan et al. of Carnegie Mellon University and Google Brain and Bahdanau et al. of the University of Montreal in 2016. The model named "Listen, Attend and Spell" (LAS), literally "listens" to the acoustic signal, pays "attention" to different parts of the signal and "spells" out the transcript one character at a time. Unlike CTC-based models, attention-based models do not have conditional-independence assumptions and can learn all the components of a speech recognizer including the pronunciation, acoustic and language model directly. This means, during deployment, there is no need to carry around a language model making it very practical for applications with limited memory. By the end of 2016, the attention-based models have seen considerable success including outperforming the CTC models (with or without an external language model). Various extensions have been proposed since the original LAS model. Latent Sequence Decompositions (LSD) was proposed by Carnegie Mellon University, MIT and Google Brain to directly emit sub-word units which are more natural than English characters; University of Oxford and Google DeepMind extended LAS to "Watch, Listen, Attend and Spell" (WLAS) to handle lip reading surpassing human-level performance.

Typically a manual control input, for example by means of a finger control on the steering-wheel, enables the speech recognition system and this is signalled to the driver by an audio prompt. Following the audio prompt, the system has a "listening window" during which it may accept a speech input for recognition.
Simple voice commands may be used to initiate phone calls, select radio stations or play music from a compatible smartphone, MP3 player or music-loaded flash drive. Voice recognition capabilities vary between car make and model. Some of the most recent car models offer natural-language speech recognition in place of a fixed set of commands, allowing the driver to use full sentences and common phrases. With such systems there is, therefore, no need for the user to memorize a set of fixed command words.

In the health care sector, speech recognition can be implemented in front-end or back-end of the medical documentation process. Front-end speech recognition is where the provider dictates into a speech-recognition engine, the recognized words are displayed as they are spoken, and the dictator is responsible for editing and signing off on the document. Back-end or deferred speech recognition is where the provider dictates into a digital dictation system, the voice is routed through a speech-recognition machine and the recognized draft document is routed along with the original voice file to the editor, where the draft is edited and report finalized. Deferred speech recognition is widely used in the industry currently.

One of the major issues relating to the use of speech recognition in healthcare is that the American Recovery and Reinvestment Act of 2009 (ARRA) provides for substantial financial benefits to physicians who utilize an EMR according to "Meaningful Use" standards. These standards require that a substantial amount of data be maintained by the EMR (now more commonly referred to as an Electronic Health Record or EHR). The use of speech recognition is more naturally suited to the generation of narrative text, as part of a radiology/pathology interpretation, progress note or discharge summary: the ergonomic gains of using speech recognition to enter structured discrete data (e.g., numeric values or codes from a list or a controlled vocabulary) are relatively minimal for people who are sighted and who can operate a keyboard and mouse.

A more significant issue is that most EHRs have not been expressly tailored to take advantage of voice-recognition capabilities. A large part of the clinician's interaction with the EHR involves navigation through the user interface using menus, and tab/button clicks, and is heavily dependent on keyboard and mouse: voice-based navigation provides only modest ergonomic benefits. By contrast, many highly customized systems for radiology or pathology dictation implement voice "macros", where the use of certain phrases – e.g., "normal report", will automatically fill in a large number of default values and/or generate boilerplate, which will vary with the type of the exam – e.g., a chest X-ray vs. a gastrointestinal contrast series for a radiology system.

Prolonged use of speech recognition software in conjunction with word processors has shown benefits to short-term-memory restrengthening in brain AVM patients who have been treated with resection. Further research needs to be conducted to determine cognitive benefits for individuals whose AVMs have been treated using radiologic techniques.

Substantial efforts have been devoted in the last decade to the test and evaluation of speech recognition in fighter aircraft. Of particular note have been the US program in speech recognition for the Advanced Fighter Technology Integration (AFTI)/F-16 aircraft (F-16 VISTA), the program in France for Mirage aircraft, and other programs in the UK dealing with a variety of aircraft platforms. In these programs, speech recognizers have been operated successfully in fighter aircraft, with applications including: setting radio frequencies, commanding an autopilot system, setting steer-point coordinates and weapons release parameters, and controlling flight display.

Working with Swedish pilots flying in the JAS-39 Gripen cockpit, Englund (2004) found recognition deteriorated with increasing g-loads. The report also concluded that adaptation greatly improved the results in all cases and that the introduction of models for breathing was shown to improve recognition scores significantly. Contrary to what might have been expected, no effects of the broken English of the speakers were found. It was evident that spontaneous speech caused problems for the recognizer, as might have been expected. A restricted vocabulary, and above all, a proper syntax, could thus be expected to improve recognition accuracy substantially.

The Eurofighter Typhoon, currently in service with the UK RAF, employs a speaker-dependent system, requiring each pilot to create a template. The system is not used for any safety-critical or weapon-critical tasks, such as weapon release or lowering of the undercarriage, but is used for a wide range of other cockpit functions. Voice commands are confirmed by visual and/or aural feedback. The system is seen as a major design feature in the reduction of pilot workload, and even allows the pilot to assign targets to his aircraft with two simple voice commands or to any of his wingmen with only five commands.

Speaker-independent systems are also being developed and are under test for the F35 Lightning II (JSF) and the Alenia Aermacchi M-346 Master lead-in fighter trainer. These systems have produced word accuracy scores in excess of 98%.

The problems of achieving high recognition accuracy under stress and noise pertain strongly to the helicopter environment as well as to the jet fighter environment. The acoustic noise problem is actually more severe in the helicopter environment, not only because of the high noise levels but also because the helicopter pilot, in general, does not wear a facemask, which would reduce acoustic noise in the microphone. Substantial test and evaluation programs have been carried out in the past decade in speech recognition systems applications in helicopters, notably by the U.S. Army Avionics Research and Development Activity (AVRADA) and by the Royal Aerospace Establishment (RAE) in the UK. Work in France has included speech recognition in the Puma helicopter. There has also been much useful work in Canada. Results have been encouraging, and voice applications have included: control of communication radios, setting of navigation systems, and control of an automated target handover system.

As in fighter applications, the overriding issue for voice in helicopters is the impact on pilot effectiveness. Encouraging results are reported for the AVRADA tests, although these represent only a feasibility demonstration in a test environment. Much remains to be done both in speech recognition and in overall speech technology in order to consistently achieve performance improvements in operational settings.

Training for air traffic controllers (ATC) represents an excellent application for speech recognition systems. Many ATC training systems currently require a person to act as a "pseudo-pilot", engaging in a voice dialog with the trainee controller, which simulates the dialog that the controller would have to conduct with pilots in a real ATC situation. Speech recognition and synthesis techniques offer the potential to eliminate the need for a person to act as pseudo-pilot, thus reducing training and support personnel. In theory, Air controller tasks are also characterized by highly structured speech as the primary output of the controller, hence reducing the difficulty of the speech recognition task should be possible. In practice, this is rarely the case. The FAA document 7110.65 details the phrases that should be used by air traffic controllers. While this document gives less than 150 examples of such phrases, the number of phrases supported by one of the simulation vendors speech recognition systems is in excess of 500,000.

The USAF, USMC, US Army, US Navy, and FAA as well as a number of international ATC training organizations such as the Royal Australian Air Force and Civil Aviation Authorities in Italy, Brazil, and Canada are currently using ATC simulators with speech recognition from a number of different vendors.

ASR is now commonplace in the field of telephony and is becoming more widespread in the field of computer gaming and simulation. In telephony systems, ASR is now being predominantly used in contact centers by integrating it with IVR systems. Despite the high level of integration with word processing in general personal computing, in the field of document production, ASR has not seen the expected increases in use.

The improvement of mobile processor speeds has made speech recognition practical in smartphones. Speech is used mostly as a part of a user interface, for creating predefined or custom speech commands.

For language learning, speech recognition can be useful for learning a second language. It can teach proper pronunciation, in addition to helping a person develop fluency with their speaking skills.

Students who are blind (see Blindness and education) or have very low vision can benefit from using the technology to convey words and then hear the computer recite them, as well as use a computer by commanding with their voice, instead of having to look at the screen and keyboard.

Students who are physically disabled or suffer from Repetitive strain injury/other injuries to the upper extremities can be relieved from having to worry about handwriting, typing, or working with scribe on school assignments by using speech-to-text programs. They can also utilize speech recognition technology to freely enjoy searching the Internet or using a computer at home without having to physically operate a mouse and keyboard.

Speech recognition can allow students with learning disabilities to become better writers. By saying the words aloud, they can increase the fluidity of their writing, and be alleviated of concerns regarding spelling, punctuation, and other mechanics of writing. Also, see Learning disability.

Use of voice recognition software, in conjunction with a digital audio recorder and a personal computer running word-processing software has proven to be positive for restoring damaged short-term-memory capacity, in stroke and craniotomy individuals.

People with disabilities can benefit from speech recognition programs. For individuals that are Deaf or Hard of Hearing, speech recognition software is used to automatically generate a closed-captioning of conversations such as discussions in conference rooms, classroom lectures, and/or religious services.

Speech recognition is also very useful for people who have difficulty using their hands, ranging from mild repetitive stress injuries to involve disabilities that preclude using conventional computer input devices. In fact, people who used the keyboard a lot and developed RSI became an urgent early market for speech recognition. Speech recognition is used in deaf telephony, such as voicemail to text, relay services, and captioned telephone. Individuals with learning disabilities who have problems with thought-to-paper communication (essentially they think of an idea but it is processed incorrectly causing it to end up differently on paper) can possibly benefit from the software but the technology is not bug proof. Also the whole idea of speak to text can be hard for intellectually disabled person's due to the fact that it is rare that anyone tries to learn the technology to teach the person with the disability.

This type of technology can help those with dyslexia but other disabilities are still in question. The effectiveness of the product is the problem that is hindering it being effective. Although a kid may be able to say a word depending on how clear they say it the technology may think they are saying another word and input the wrong one. Giving them more work to fix, causing them to have to take more time with fixing the wrong word.


The performance of speech recognition systems is usually evaluated in terms of accuracy and speed. Accuracy is usually rated with word error rate (WER), whereas speed is measured with the real time factor. Other measures of accuracy include Single Word Error Rate (SWER) and Command Success Rate (CSR).

Speech recognition by machine is a very complex problem, however. Vocalizations vary in terms of accent, pronunciation, articulation, roughness, nasality, pitch, volume, and speed. Speech is distorted by a background noise and echoes, electrical characteristics. Accuracy of speech recognition may vary with the following:

As mentioned earlier in this article, accuracy of speech recognition may vary depending on the following factors:
With discontinuous speech full sentences separated by silence are used, therefore it becomes easier to recognize the speech as well as with isolated speech. 
With continuous speech naturally spoken sentences are used, therefore it becomes harder to recognize the speech, different from both isolated and discontinuous speech.


Constraints are often represented by a grammar. 
Speech recognition is a multi-leveled pattern recognition task.
e.g. Known word pronunciations or legal word sequences, which can compensate for errors or uncertainties at lower level;
For telephone speech the sampling rate is 8000 samples per second; 
computed every 10 ms, with one 10 ms section called a frame;

Analysis of four-step neural network approaches can be explained by further information. Sound is produced by air (or some other medium) vibration, which we register by ears, but machines by receivers. Basic sound creates a wave which has two descriptions: amplitude (how strong is it), and frequency (how often it vibrates per second).
Accuracy can be computed with the help of word error rate (WER). Word error rate can be calculated by aligning the recognized word and referenced word using dynamic string alignment. The problem may occur while computing the word error rate due to the difference between the sequence lengths of recognized word and referenced word. 
Let, S be the number of substitutions,
The formula to compute the word error rate(WER) is 
While computing the word recognition rate (WRR) word error rate (WER) is used and the formula is 
Here H is the number of correctly recognized words. H= N-(S+D).

Speech recognition can become a means of attack, theft, or accidental operation. For example, activation words like "Alexa" spoken in an audio or video broadcast can cause devices in homes and offices to start listening for input inappropriately, or possibly take an unwanted action. Voice-controlled devices are also accessible to visitors to the building, or even those outside the building if they can be heard inside. Attackers may be able to gain access to personal information, like calendar, address book contents, private messages, and documents. They may also be able to impersonate the user to send messages or make online purchases.

Two attacks have been demonstrated that use artificial sounds. One transmits ultrasound and attempt to send commands without nearby people noticing. The other adds small, inaudible distortions to other speech or music that are specially crafted to confuse the specific speech recognition system into recognizing music as speech, or to make what sounds like one command to a human sound like a different command to the system.

Popular speech recognition conferences held each year or two include SpeechTEK and SpeechTEK Europe, ICASSP, Interspeech/Eurospeech, and the IEEE ASRU. Conferences in the field of natural language processing, such as ACL, NAACL, EMNLP, and HLT, are beginning to include papers on speech processing. Important journals include the IEEE Transactions on Speech and Audio Processing (later renamed IEEE Transactions on Audio, Speech and Language Processing and since Sept 2014 renamed IEEE/ACM Transactions on Audio, Speech and Language Processing—after merging with an ACM publication), Computer Speech and Language, and Speech Communication.

Books like "Fundamentals of Speech Recognition" by Lawrence Rabiner can be useful to acquire basic knowledge but may not be fully up to date (1993). Another good source can be "Statistical Methods for Speech Recognition" by Frederick Jelinek and "Spoken Language Processing (2001)" by Xuedong Huang etc., "Computer Speech", by Manfred R. Schroeder, second edition published in 2004, and "Speech Processing: A Dynamic and Optimization-Oriented Approach" published in 2003 by Li Deng and Doug O'Shaughnessey. The updated textbook "Speech and Language Processing" (2008) by Jurafsky and Martin presents the basics and the state of the art for ASR. Speaker recognition also uses the same features, most of the same front-end processing, and classification techniques as is done in speech recognition. A comprehensive textbook, "Fundamentals of Speaker Recognition" is an in depth source for up to date details on the theory and practice. A good insight into the techniques used in the best modern systems can be gained by paying attention to government sponsored evaluations such as those organised by DARPA (the largest speech recognition-related project ongoing as of 2007 is the GALE project, which involves both speech recognition and translation components).

A good and accessible introduction to speech recognition technology and its history is provided by the general audience book "The Voice in the Machine. Building Computers That Understand Speech" by Roberto Pieraccini (2012).

The most recent book on speech recognition is "Automatic Speech Recognition: A Deep Learning Approach" (Publisher: Springer) written by Microsoft researchers D. Yu and L. Deng and published near the end of 2014, with highly mathematically oriented technical detail on how deep learning methods are derived and implemented in modern speech recognition systems based on DNNs and related deep learning methods. A related book, published earlier in 2014, "Deep Learning: Methods and Applications" by L. Deng and D. Yu provides a less technical but more methodology-focused overview of DNN-based speech recognition during 2009–2014, placed within the more general context of deep learning applications including not only speech recognition but also image recognition, natural language processing, information retrieval, multimodal processing, and multitask learning.

In terms of freely available resources, Carnegie Mellon University's Sphinx toolkit is one place to start to both learn about speech recognition and to start experimenting. Another resource (free but copyrighted) is the HTK book (and the accompanying HTK toolkit). For more recent and state-of-the-art techniques, Kaldi toolkit can be used. In 2017 Mozilla launched the open source project called Common Voice to gather big database of voices that would help build free speech recognition project DeepSpeech (available free at GitHub) using Google open source platform TensorFlow.

The commercial cloud based speech recognition APIs are broadly available from AWS, Azure, IBM, and GCP.

A demonstration of an on-line speech recognizer is available on Cobalt's webpage.

For more software resources, see List of speech recognition software.




</doc>
<doc id="29469" url="https://en.wikipedia.org/wiki?curid=29469" title="Sapphire">
Sapphire

Sapphire is a precious gemstone, a variety of the mineral corundum, consisting of aluminum oxide () with trace amounts of elements such as iron, titanium, chromium, vanadium, or magnesium. It is typically blue, but natural "fancy" sapphires also occur in yellow, purple, orange, and green colors; "parti sapphires" show two or more colors. Red corundum stones also occur but are called rubies not sapphires. Pink colored corundum may be either classified as ruby or sapphire depending on locale. 
Commonly, natural sapphires are cut and polished into gemstones and worn in jewelry. They also may be created synthetically in laboratories for industrial or decorative purposes in large crystal boules. Because of the remarkable hardness of sapphires – 9 on the Mohs scale (the third hardest mineral, after diamond at 10 and moissanite at 9.5) – sapphires are also used in some non-ornamental applications, such as infrared optical components, high-durability windows, wristwatch crystals and movement bearings, and very thin electronic wafers, which are used as the insulating substrates of special-purpose solid-state electronics such as integrated circuits and GaN-based blue LEDs.

Sapphire is the birthstone for September and the gem of the 45th anniversary. A sapphire jubilee occurs after 65 years.

Sapphire is one of the two gem-varieties of corundum, the other being ruby (defined as corundum in a shade of red). Although blue is the best-known sapphire color, they occur in other colors, including gray and black, and they can be colorless. A pinkish orange variety of sapphire is called padparadscha.

Significant sapphire deposits are found in Australia, Afghanistan, Cambodia, Cameroon, China (Shandong), Colombia, Ethiopia, India (Kashmir), Kenya, Laos, Madagascar, Malawi, Mozambique, Myanmar (Burma), Nigeria, Rwanda, Sri Lanka, Tanzania, Thailand, United States (Montana) and Vietnam. Sapphire and rubies are often found in the same geographical settings, but they generally have different geological formations. For example, both ruby and sapphire are found in Myanmar's Mogok Stone Tract, but the rubies form in marble, while the sapphire forms in granitic pegmatites or corundum syenites.

Every sapphire mine produces a wide range of quality, and origin is not a guarantee of quality. For sapphire, Kashmir receives the highest premium, although Burma, Sri Lanka, and Madagascar also produce large quantities of fine quality gems.

The cost of natural sapphires varies depending on their color, clarity, size, cut, and overall quality. Sapphires that are completely untreated are worth far more than those that have been treated. Geographical origin also has a major impact on price. For most gems of one carat or more, an independent report from a respected laboratory such as American Gemological Laboratories (AGL), Gem Research Swisslab (GRS), GIA, Gübelin, Lotus Gemology, or SSEF, is often required by buyers before they will make a purchase.

Gemstone color can be described in terms of hue, saturation, and tone. Hue is commonly understood as the "color" of the gemstone. Saturation refers to the vividness or brightness of the hue, and tone is the lightness to darkness of the hue. Blue sapphire exists in various mixtures of its primary (blue) and secondary hues, various tonal levels (shades) and at various levels of saturation (vividness).

Blue sapphires are evaluated based upon the purity of their blue hue. Violet, and green are the most common secondary hues found in blue sapphires. The highest prices are paid for gems that are pure blue and of vivid saturation. Gems that are of lower saturation, or are too dark or too light in tone are of less value. However, color preferences are a personal taste, like a flavor of ice cream.

The Logan sapphire in the National Museum of Natural History, in Washington, D.C., is one of the largest faceted gem-quality blue sapphires in existence. The 422.66-ct Siren of Serendip in the Houston Museum of Natural Science is another stunning example of a Sri Lankan sapphire on public display.

Sapphires in colors other than blue are called "fancy" or "parti colored" sapphires.

Fancy sapphires are often found in yellow, orange, green, brown, purple and violet hues.

Particolored sapphires are those stones which exhibit two or more colors within a single stone. Australia is the largest source of particolored sapphires; they are not commonly used in mainstream jewelry and remain relatively unknown. Particolored sapphires cannot be created synthetically and only occur naturally.

Colorless sapphires have historically been used as diamond substitutes in jewelry.

Pink sapphires occur in shades from light to dark pink, and deepen in color as the quantity of chromium increases. The deeper the pink color, the higher their monetary value. In the United States, a minimum color saturation must be met to be called a ruby, otherwise the stone is referred to as a "pink sapphire".

"Padparadscha" is a delicate, light to medium toned, pink-orange to orange-pink hued corundum, originally found in Sri Lanka, but also found in deposits in Vietnam and parts of East Africa. Padparadscha sapphires are rare; the rarest of all is the totally natural variety, with no sign of artificial treatment.

The name is derived from the Sanskrit "padma ranga" (padma = lotus; ranga = color), a color akin to the lotus flower ("Nelumbo nucifera").

Among the fancy (non-blue) sapphires, natural padparadscha fetch the highest prices. Since 2001, more sapphires of this color have appeared on the market as a result of artificial lattice diffusion of beryllium.

A "star sapphire" is a type of sapphire that exhibits a star-like phenomenon known as asterism; red stones are known as "star rubies". Star sapphires contain intersecting needle-like inclusions following the underlying crystal structure that causes the appearance of a six-rayed "star"-shaped pattern when viewed with a single overhead light source. The inclusion is often the mineral rutile, a mineral composed primarily of titanium dioxide. The stones are cut "en cabochon", typically with the center of the star near the top of the dome. Occasionally, twelve-rayed stars are found, typically because two different sets of inclusions are found within the same stone, such as a combination of fine needles of rutile with small platelets of hematite; the first results in a whitish star and the second results in a golden-colored star. During crystallization, the two types of inclusions become preferentially oriented in different directions within the crystal, thereby forming two six-rayed stars that are superimposed upon each other to form a twelve-rayed star. Misshapen stars or 12-rayed stars may also form as a result of twinning.
The inclusions can alternatively produce a "cat's eye" effect if the girdle plane of the cabochon is oriented parallel to the crystal's c-axis rather than perpendicular to it. To get a cat's eye, the planes of exsolved inclusions must be extremely uniform and tightly packed. If the dome is oriented in between these two directions, an 'off-center' star will be visible, offset away from the high point of the dome.

At 1404.49 carats, The Star of Adam is claimed to be the largest blue star sapphire, but whenever such claims are made, one should be careful not to equate size with quality or value. The gem was mined in the city of Ratnapura, southern Sri Lanka. The Black Star of Queensland, the second largest star sapphire in the world, weighs 733 carats. The Star of India mined in Sri Lanka and weighing 563.4 carats is thought to be the third-largest star sapphire, and is currently on display at the American Museum of Natural History in New York City. The 182-carat Star of Bombay, mined in Sri Lanka and located in the National Museum of Natural History in Washington, D.C., is another example of a large blue star sapphire. The value of a star sapphire depends not only on the weight of the stone, but also the body color, visibility, and intensity of the asterism. A common mistake made by novices is to value stones with strong stars the highest. In fact, the color of the stone has more impact on the value than the visibility of the star. Since more transparent stones tend to have better colors, the most expensive star stones are semi-transparent "glass body" stones with vivid colors.

Large rubies and sapphires of poor transparency are frequently used with suspect appraisals that vastly overstate their value. This was the case of the "Life and Pride of America Star Sapphire". Circa 1985, Roy Whetstine claimed to have bought the 1905-ct stone for $10 at the Tucson gem show, but a reporter discovered that L.A. Ward of Fallbrook, CA, who appraised it at the price of $1200/ct, had appraised another stone of the exact same weight several years before Whetstine claimed to have found it.

Bangkok-based Lotus Gemology maintains an updated listing of world auction records of ruby, sapphire, and spinel. As of November 2019, no sapphire has ever sold at auction for more than $17,295,796.

A rare variety of natural sapphire, known as color-change sapphire, exhibits different colors in different light. Color change sapphires are blue in outdoor light and purple under incandescent indoor light, or green to gray-green in daylight and pink to reddish-violet in incandescent light. Color change sapphires come from a variety of locations, including Madagascar, Myanmar, Sri Lanka and Tanzania. Two types exist. The first features the chromium chromophore that creates the red color of ruby, combined with the iron + titanium chromophore that produces the blue color in sapphire. A more rare type, which comes from the Mogok area of Myanmar, features a vanadium chromophore, the same as is used in Verneuil synthetic color-change sapphire.

Virtually all gemstones that show the "alexandrite effect" (color change; a.k.a. 'metamerism') show similar absorption/transmission features in the visible spectrum. This is an absorption band in the yellow (~590 nm), along with valleys of transmission in the blue-green and red. Thus the color one sees depends on the spectral composition of the light source. Daylight is relatively balanced in its spectral power distribution (SPD) and since the human eye is most sensitive to green light, the balance is tipped to the green side. However incandescent light (including candle light) is heavily tilted to the red end of the spectrum, thus tipping the balance to red.

Color-change sapphires colored by the Cr + Fe/Ti chromophores generally change from blue or violetish blue to violet or purple. Those colored by the V chromophore can show a more pronounced change, moving from blue-green to purple.

Certain synthetic color-change sapphires have a similar color change to the natural gemstone alexandrite and they are sometimes marketed as "alexandrium" or "synthetic alexandrite". However, the latter term is a misnomer: synthetic color-change sapphires are, technically, not synthetic alexandrites but rather alexandrite "simulants". This is because genuine alexandrite is a variety of chrysoberyl: not sapphire, but an entirely different mineral.

Rubies are corundum with a dominant red body color. This is generally caused by traces of chromium (Cr) substituting for the (Al) ion in the corundum structure. The color can be modified by both iron and trapped hole color centers.

Unlike localized ("intra-atomic") absorption of light which causes color for chromium and vanadium impurities, blue color in sapphires comes from intervalence charge transfer, which is the transfer of an electron from one transition-metal ion to another via the conduction or valence band. The iron can take the form Fe or Fe, while titanium generally takes the form Ti. If Fe and Ti ions are substituted for Al, localized areas of charge imbalance are created. An electron transfer from Fe and Ti can cause a change in the valence state of both. Because of the valence change there is a specific change in energy for the electron, and electromagnetic energy is absorbed. The wavelength of the energy absorbed corresponds to yellow light. When this light is subtracted from incident white light, the complementary color blue results. Sometimes when atomic spacing is different in different directions there is resulting blue-green dichroism.

Purple sapphires contain trace amounts of chromium and iron plus titanium and come in a variety of shades. Corundum that contains extremely low levels of chromophores is near colorless. Completely colorless corundum generally does not exist in nature. If trace amounts of iron are present, a very pale yellow to green color may be seen. However, if both titanium and iron impurities are present together, and in the correct valence states, the result is a blue color.

Intervalence charge transfer is a process that produces a strong colored appearance at a low percentage of impurity. While at least 1% chromium must be present in corundum before the deep red ruby color is seen, sapphire blue is apparent with the presence of only 0.01% of titanium and iron.

The most complete description of the causes of color in corundum extant can be found in Chapter 4 of Ruby & Sapphire: A Gemologist's Guide (chapter authored by John Emmett, Emily Dubinsky and Richard Hughes).

Sapphires can be treated by several methods to enhance and improve their clarity and color. It is common practice to heat natural sapphires to improve or enhance their appearance. This is done by heating the sapphires in furnaces to temperatures between for several hours, or even weeks at a time. Different atmospheres may be used. Upon heating, the stone becomes more blue in color, but loses some of the rutile inclusions (silk). When high temperatures (1400 °C+) are used, exsolved rutile silk is dissolved and it becomes clear under magnification. The titanium from the rutile enters solid solution and thus creates with iron the blue color The inclusions in natural stones are easily seen with a jeweler's loupe. Evidence of sapphire and other gemstones being subjected to heating goes back at least to Roman times. Un-heated natural stones are somewhat rare and will often be sold accompanied by a certificate from an independent gemological laboratory attesting to "no evidence of heat treatment".
Yogo sapphires do not need heat treating because their cornflower blue color is attractive out of the ground; they are generally free of inclusions, and have high uniform clarity. When Intergem Limited began marketing the Yogo in the 1980s as the world's only guaranteed untreated sapphire, heat treatment was not commonly disclosed; by the late 1980s, heat treatment became a major issue. At that time, much of all the world's sapphires were being heated to enhance their natural color. Intergem's marketing of guaranteed untreated Yogos set them against many in the gem industry. This issue appeared as a front-page story in the "Wall Street Journal" on 29 August 1984 in an article by Bill Richards, "Carats and Schticks: Sapphire Marketer Upsets The Gem Industry". However, the biggest problem the Yogo mine faced was not competition from heated sapphires, but the fact that the Yogo stones could never produce quantities of sapphire above one carat after faceting. As a result, it has remained a niche product, with a market that largely exists in the US.

Lattice ('bulk') diffusion treatments are used to add impurities to the sapphire to enhance color. This process was originally developed and patented by Linde Air division of Union Carbide and involved diffusing titanium into synthetic sapphire to even out the blue color. It was later applied to natural sapphire. Today, titanium diffusion often uses a synthetic colorless sapphire base. The color layer created by titanium diffusion is extremely thin (less than 0.5 mm). Thus repolishing can and does produce slight to significant loss of color. Chromium diffusion has been attempted, but was abandoned due to the slow diffusion rates of chromium in corundum.

In the year 2000, beryllium diffused "padparadscha" colored sapphires entered the market. Typically beryllium is diffused into a sapphire under very high heat, just below the melting point of the sapphire. Initially ("c." 2000) orange sapphires were created, although now the process has been advanced and many colors of sapphire are often treated with beryllium. Due to the small size of the beryllium ion, the color penetration is far greater than with titanium diffusion. In some cases, it may penetrate the entire stone. Beryllium-diffused orange sapphires may be difficult to detect, requiring advanced chemical analysis by gemological labs ("e.g.", Gübelin, SSEF, GIA, American Gemological Laboratories (AGL), Lotus Gemology.

According to United States Federal Trade Commission guidelines, disclosure is required of any mode of enhancement that has a significant effect on the gem's value.

There are several ways of treating sapphire. Heat-treatment in a reducing or oxidizing atmosphere (but without the use of any other added impurities) is commonly used to improve the color of sapphires, and this process is sometimes known as "heating only" in the gem trade. In contrast, however, heat treatment combined with the deliberate addition of certain specific impurities (e.g. beryllium, titanium, iron, chromium or nickel, which are absorbed into the crystal structure of the sapphire) is also commonly performed, and this process can be known as "diffusion" in the gem trade. However, despite what the terms "heating only" and "diffusion" might suggest, both of these categories of treatment actually involve diffusion processes.

The most complete description of corundum treatments extant can be found in Chapter 6 of Ruby & Sapphire: A Gemologist's Guide (chapter authored by John Emmett, Richard Hughes and Troy R. Douthit).

Sapphires are mined from alluvial deposits or from primary underground workings. Commercial mining locations for sapphire and ruby include (but are not limited to) the following countries: Afghanistan, Australia, Myanmar/Burma, Cambodia, China, Colombia, India, Kenya, Laos, Madagascar, Malawi, Nepal, Nigeria, Pakistan, Sri Lanka, Tajikistan, Tanzania, Thailand, United States, and Vietnam. Sapphires from different geographic locations may have different appearances or chemical-impurity concentrations, and tend to contain different types of microscopic inclusions. Because of this, sapphires can be divided into three broad categories: classic metamorphic, non-classic metamorphic or magmatic, and classic magmatic.

Sapphires from certain locations, or of certain categories, may be more commercially appealing than others, particularly classic metamorphic sapphires from Kashmir, Burma, or Sri Lanka that have not been subjected to heat-treatment.

The Logan sapphire, the Star of India, The Star of Adam and the Star of Bombay originate from Sri Lankan mines. Madagascar is the world leader in sapphire production (as of 2007) specifically its deposits in and around the town of Ilakaka. Prior to the opening of the Ilakaka mines, Australia was the largest producer of sapphires (such as in 1987). In 1991 a new source of sapphires was discovered in Andranondambo, southern Madagascar. That area has been exploited for its sapphires started in 1993, but it was practically abandoned just a few years later—because of the difficulties in recovering sapphires in their bedrock.

In North America, sapphires have been mined mostly from deposits in Montana: fancies along the Missouri River near Helena, Montana, Dry Cottonwood Creek near Deer Lodge, Montana, and Rock Creek near Philipsburg, Montana. Fine blue Yogo sapphires are found at Yogo Gulch west of Lewistown, Montana. A few gem-grade sapphires and rubies have also been found in the area of Franklin, North Carolina.

The sapphire deposits of Kashmir are well known in the gem industry, although their peak production took place in a relatively short period at the end of the nineteenth and early twentieth centuries. They have a superior vivid blue hue, coupled with a mysterious and almost sleepy quality, described by some gem enthusiasts as ‘blue velvet”. Kashmir-origin contributes meaningfully to the value of a sapphire, and most corundum of Kashmir origin can be readily identified by its characteristic silky appearance and exceptional hue. The unique blue appears lustrous under any kind of light, unlike non-Kashmir sapphires which may appear purplish or grayish in comparison. Sotheby's has been in the forefront overseeing record-breaking sales of Kashmir sapphires worldwide. In October 2014, Sotheby's Hong Kong achieved consecutive per-carat price records for Kashmir sapphires – first with the 12.00 carat Cartier sapphire ring at US$193,975 per carat, then with a 17.16 carat sapphire at US$236,404, and again in June 2015 when the per-carat auction record was set at US$240,205. At present, the world record price-per-carat for sapphire at auction is held by a sapphire from Kashmir in a ring, which sold in October 2015 for approximately US$242,000 per carat (HK$52,280,000 in total, including buyer's premium, or more than US$6.74 million).

In 1902, the French chemist Auguste Verneuil announced a process for producing synthetic ruby crystals. In the flame-fusion (Verneuil process), fine alumina powder is added to an oxyhydrogen flame, and this is directed downward against a ceramic pedestal. Following the successful synthesis of ruby, Verneuil focussed his efforts on sapphire. Synthesis of blue sapphire came in 1909, after chemical analyses of sapphire suggested to Verneuil that iron and titanium were the cause of the blue color. Verneuil patented the process of producing synthetic blue sapphire in 1911.

The key to the process is that the alumina powder does not melt as it falls through the flame. Instead it forms a sinter cone on the pedestal. When the tip of that cone reaches the hottest part of the flame, the tip melts. Thus the crystal growth is started from a tiny point, ensuring minimal strain.

Next, more oxygen is added to the flame, causing it to burn slightly hotter. This expands the growing crystal laterally. At the same time, the pedestal is lowered at the same rate that the crystal grows vertically. The alumina in the flame is slowly deposited, creating a teardrop shaped "boule" of sapphire material. This step is continued until the desired size is reached, the flame is shut off and the crystal cools. The now elongated crystal contains a lot of strain due to the high thermal gradient between the flame and surrounding air. To release this strain, the now finger-shaped crystal will be tapped with a chisel to split it into two halves.

Due to the vertical layered growth of the crystal and the curved upper growth surface (which starts from a drop), the crystals will display curved growth lines following the top surface of the boule. This is in contrast to natural corundum crystals, which feature angular growth lines expanding from a single point and following the planar crystal faces.

Chemical dopants can be added to create artificial versions of the ruby, and all the other natural colors of sapphire, and in addition, other colors never seen in geological samples. Artificial sapphire material is identical to natural sapphire, except it can be made without the flaws that are found in natural stones. The disadvantage of the Verneuil process is that the grown crystals have high internal strains. Many methods of manufacturing sapphire today are variations of the Czochralski process, which was invented in 1916 by Polish chemist Jan Czochralski. In this process, a tiny sapphire seed crystal is dipped into a crucible made of the precious metal iridium or molybdenum, containing molten alumina, and then slowly withdrawn upward at a rate of 1 to 100 mm per hour. The alumina crystallizes on the end, creating long carrot-shaped boules of large size up to 200 kg in mass.

Synthetic sapphire is also produced industrially from agglomerated aluminum oxide, sintered and fused (such as by hot isostatic pressing) in an inert atmosphere, yielding a transparent but slightly porous polycrystalline product.

In 2003, the world's production of synthetic sapphire was 250 tons (1.25 × 10 carats), mostly by the United States and Russia. The availability of cheap synthetic sapphire unlocked many industrial uses for this unique material.

Synthetic sapphire—sometimes referred to as "sapphire glass"—is commonly used as a window material, because it is both highly transparent to wavelengths of light between 150 nm (UV) and 5500 nm (IR) (the visible spectrum extends about 380 nm to 750 nm), and extraordinarily scratch-resistant.

The key benefits of sapphire windows are:
Some sapphire-glass windows are made from pure sapphire boules that have been grown in a specific crystal orientation, typically along the optical axis, the c-axis, for minimum birefringence for the application.

The boules are sliced up into the desired window thickness and finally polished to the desired surface finish. Sapphire optical windows can be polished to a wide range of surface finishes due to its crystal structure and its hardness. The surface finishes of optical windows are normally called out by the scratch-dig specifications in accordance with the globally adopted MIL-O-13830 specification.

The sapphire windows are used in both high pressure and vacuum chambers for spectroscopy, crystals in various watches, and windows in grocery store barcode scanners since the material's exceptional hardness and toughness makes it very resistant to scratching.

It is used for end windows on some high-powered laser tubes as its wide-band transparency and thermal conductivity allow it to handle very high power densities in the infra-red or UV spectrum without degrading due to heating.

Along with zirconia and aluminum oxynitride, synthetic sapphire is used for shatter resistant windows in armored vehicles and various military body armor suits, in association with composites.

One type of xenon arc lamp – originally called the "Cermax" and now known generically as the "ceramic body xenon lamp" – uses sapphire crystal output windows. This product tolerates higher thermal loads and thus higher output powers when compared with conventional Xe lamps with pure silica window.

Thin sapphire wafers were the first successful use of an insulating substrate upon which to deposit silicon to make the integrated circuits known as silicon on sapphire or "SOS"; now other substrates can also be used for the class of circuits known more generally as silicon on insulator. Besides its excellent electrical insulating properties, sapphire has high thermal conductivity. CMOS chips on sapphire are especially useful for high-power radio-frequency (RF) applications such as those found in cellular telephones, public-safety band radios, and satellite communication systems. "SOS" also allows for the monolithic integration of both digital and analog circuitry all on one IC chip, and the construction of extremely low power circuits.

In one process, after single crystal sapphire boules are grown, they are core-drilled into cylindrical rods, and wafers are then sliced from these cores.

Wafers of single-crystal sapphire are also used in the semiconductor industry as substrates for the growth of devices based on gallium nitride (GaN). The use of sapphire significantly reduces the cost, because it has about one-seventh the cost of germanium. Gallium nitride on sapphire is commonly used in blue light-emitting diodes (LEDs).

The first laser was made with a rod of synthetic ruby. Titanium-sapphire lasers are popular due to their relatively rare capacity to be tuned to various wavelengths in the red and near-infrared region of the electromagnetic spectrum. They can also be easily mode-locked. In these lasers a synthetically produced sapphire crystal with chromium or titanium impurities is irradiated with intense light from a special lamp, or another laser, to create stimulated emission.

Monocrystalline sapphire is fairly biocompatible and the exceptionally low wear of sapphire–metal pairs has led to the introduction (in Ukraine) of sapphire monocrystals for hip 
joint endoprostheses.


Extensive tables listing over a hundred important and famous rubies and sapphires can be found in Chapter 10 of Ruby & Sapphire: A Gemologist's Guide.



 


</doc>
<doc id="29471" url="https://en.wikipedia.org/wiki?curid=29471" title="Slack voice">
Slack voice

Slack voice (or lax voice) is the pronunciation of consonant or vowels with a glottal opening slightly wider than that occurring in modal voice. Such sounds are often referred to informally as lenis or half-voiced in the case of consonants. In some Chinese varieties, such as Wu, and in a few Austronesian languages, the 'intermediate' phonation of slack stops confuses listeners of languages without these distinctions, so that different transcription systems may use or for the same consonant. In Xhosa, slack-voiced consonants have usually been transcribed as breathy voice. Although the IPA has no dedicated diacritic for slack voice, the voiceless diacritic (the under-ring) may be used with a voiced consonant letter, though this convention is also used for partially voiced consonants in languages such as English.

Wu Chinese "muddy" consonants are slack voice word-initially, the primary effect of which is a slightly breathy quality of the following vowel.

Javanese contrasts slack and stiff voiced bilabial, dental, retroflex, and velar stops.

Parauk contrasts slack voicing in its vowels. The contrast is between "slightly stiff" and "slightly breathy" vowels; the first are between modal and stiff voice, while the latter are captured by slack voice.


</doc>
<doc id="29472" url="https://en.wikipedia.org/wiki?curid=29472" title="SADC">
SADC

SADC may stand for:



</doc>
<doc id="29473" url="https://en.wikipedia.org/wiki?curid=29473" title="Salvation">
Salvation

Salvation (from Latin: "salvatio", from "salva", 'safe, saved') is the state of being saved or protected from harm or a dire situation. In religion and theology, "salvation" generally refers to the deliverance of the soul from sin and its consequences. The academic study of salvation is called "soteriology".

In religion and theology, "salvation" is the saving of the soul from sin and its consequences. It may also be called "deliverance" or "redemption" from sin and its effects. Depending on the religion or even denomination, salvation is considered to be caused either only by the grace of God (i.e. unmerited and unearned), or by faith, good deeds (works), or a combination thereof. Religions often emphasize that man is a sinner by nature and that the penalty of sin is death (physical death, spiritual death: spiritual separation from God and eternal punishment in hell).

In contemporary Judaism, redemption (Hebrew: ), refers to God redeeming the people of Israel from their various exiles. This includes the final redemption from the present exile.

Judaism holds that adherents do not need personal salvation as Christians believe. Jews do not subscribe to the doctrine of original sin. Instead, they place a high value on individual morality as defined in the law of God—embodied in what Jews know as the Torah or The Law, given to Moses by God on biblical Mount Sinai.

In Judaism, salvation is closely related to the idea of redemption, a saving from the states or circumstances that destroy the value of human existence. God, as the universal spirit and Creator of the World, is the source of all salvation for humanity, provided an individual honours God by observing his precepts. So redemption or salvation depends on the individual. Judaism stresses that salvation cannot be obtained through anyone else or by just invoking a deity or believing in any outside power or influence.

When examining Jewish intellectual sources throughout history, there is clearly a spectrum of opinions regarding death versus the afterlife. Possibly an over-simplification, one source says salvation can be achieved in the following manner: Live a holy and righteous life dedicated to Yahweh, the God of Creation. Fast, worship, and celebrate during the appropriate holidays.
By origin and nature, Judaism is an ethnic religion. Therefore, salvation has been primarily conceived in terms of the destiny of Israel as the elect people of Yahweh (often referred to as “the Lord”), the God of Israel.

In the biblical text of Psalms, there is a description of death, when people go into the earth or the "realm of the dead" and cannot praise God. The first reference to resurrection is collective in Ezekiel's vision of the dry bones, when all the Israelites in exile will be resurrected. There is a reference to individual resurrection in the Book of Daniel (165 BCE), the last book of the Hebrew Bible. It was not until the 2nd century BCE that there arose a belief in an afterlife, in which the dead would be resurrected and undergo divine judgment. Before that time, the individual had to be content that his posterity continued within the holy nation.

The salvation of the individual Jew was connected to the salvation of the entire people. This belief stemmed directly from the teachings of the Torah. In the Torah, God taught his people sanctification of the individual. However, he also expected them to function together (spiritually) and be accountable to one another. The concept of salvation was tied to that of restoration for Israel.

Christianity's primary premise is that the incarnation and death of Jesus Christ formed the climax of a divine plan for humanity's salvation. This plan was conceived by God consequent on the Fall of Adam, the progenitor of the human race, and it would be completed at the Last Judgment, when the Second Coming of Christ would mark the catastrophic end of the world.

For Christianity, salvation is only possible through Jesus Christ. Christians believe that Jesus' death on the cross was the once-for-all sacrifice that atoned for the sin of humanity.

The Christian religion, though not the exclusive possessor of the idea of redemption, has given to it a special definiteness and a dominant position. Taken in its widest sense, as deliverance from dangers and ills in general, most religions teach some form of it. It assumes an important position, however, only when the ills in question form part of a great system against which human power is helpless.
According to Christian belief, sin as the human predicament is considered to be universal. For example, in the Apostle Paul declared everyone to be under sin—Jew and Gentile alike. Salvation is made possible by the life, death, and resurrection of Jesus, which in the context of salvation is referred to as the "atonement". Christian soteriology ranges from exclusive salvation to universal reconciliation concepts. While some of the differences are as widespread as Christianity itself, the overwhelming majority agrees that salvation is made possible by the work of Jesus Christ, the Son of God, dying on the cross.

Variant views on salvation are among the main fault lines dividing the various Christian denominations, both between Roman Catholicism and Protestantism and within Protestantism, notably in the Calvinist–Arminian debate, and the fault lines include conflicting definitions of depravity, predestination, atonement, but most pointedly justification.

Salvation, according to most denominations, is believed to be a process that begins when a person first becomes a Christian, continues through that person's life, and is completed when they stand before Christ in judgment. Therefore, according to Catholic apologist James Akin, the faithful Christian can say in faith and hope, "I "have been" saved; I "am being" saved; and I "will be" saved."

Christian salvation concepts are varied and complicated by certain theological concepts, traditional beliefs, and dogmas. Scripture is subject to individual and ecclesiastical interpretations. While some of the differences are as widespread as Christianity itself, the overwhelming majority agrees that salvation is made possible by the work of Jesus Christ, the Son of God, dying on the cross.

The purpose of salvation is debated, but in general most Christian theologians agree that God devised and implemented his plan of salvation because he loves them and regards human beings as his children. Since human existence on Earth is said to be "given to sin," salvation also has connotations that deal with the liberation of human beings from sin, and the suffering associated with the punishment of sin—i.e., "the wages of sin are death."

Christians believe that salvation depends on the grace of God. Stagg writes that a fact assumed throughout the Bible is that humanity is in, "serious trouble from which we need deliverance…. The fact of sin as the human predicament is implied in the mission of Jesus, and it is explicitly affirmed in that connection." By its nature, salvation must answer to the plight of humankind as it actually is. Each individual's plight as sinner is the result of a fatal choice involving the whole person in bondage, guilt, estrangement, and death. Therefore, salvation must be concerned with the total person. "It must offer redemption from bondage, forgiveness for guilt, reconciliation for estrangement, renewal for the marred image of God."

According to doctrine of the Latter Day Saint movement, the plan of salvation is a plan that God created to save, redeem, and exalt humankind. The elements of this plan are drawn from various sources, including the Bible, Book of Mormon, Doctrine & Covenants, Pearl of Great Price, and numerous statements made by the leadership of The Church of Jesus Christ of Latter-day Saints (LDS Church). The first appearance of the graphical representation of the plan of salvation is in the 1952 missionary manual entitled "A Systematic Program for Teaching the Gospel."

In Islam, salvation refers to the eventual entrance to Paradise. Islam teaches that people who die disbelieving in God do not receive salvation. It also teaches that non-Muslims who die believing in the God but disbelieving in his message (Islam), are left to his will. Those who die believing in the One God and his message (Islam) receive salvation.

Narrated Anas that Muhammad said:
Islam teaches that all who enter into Islam must remain so in order to receive salvation.
For those who have not been granted Islam or to whom the message has not been brought:

Belief in the “One God”, also known as the "Tawhid" () in Arabic, consists of two parts (or principles):

Islam also stresses that in order to gain salvation, one must also avoid sinning along with performing good deeds. Islam acknowledges the inclination of humanity towards sin. Therefore, Muslims are constantly commanded to seek God's forgiveness and repent. Islam teaches that no one can gain salvation simply by virtue of their belief or deeds, instead it is the Mercy of God, which merits them salvation. However, this repentance must not be used to sin any further. Islam teaches that God is Merciful.

Islam describes a true believer to have Love of God and Fear of God. Islam also teaches that every person is responsible for their own sins. The Quran states;

Al-Agharr al-Muzani, a companion of Mohammad, reported that Ibn 'Umar stated to him that Mohammad said,

Sin in Islam is not a state, but an action (a bad deed); Islam teaches that a child is born sinless, regardless of the belief of his parents, dies a Muslim; he enters heaven, and does not enter hell.

Islam is built on five principles, acts of worship that Islam teaches to be mandatory. Not performing the mandatory acts of worship may deprive Muslims of the chance of salvation. According to Ibn 'Umar, Muhammad said that Islam is based on the following five principles:


Hinduism, Buddhism, Jainism and Sikhism share certain key concepts, which are interpreted differently by different groups and individuals. In these religions one is not liberated from sin and its consequences, but from the "saṃsāra" (cycle of rebirth) perpetuated by passions and delusions and its resulting karma. They differ however on the exact nature of this liberation.

Salvation is always self-attained in Dharmic traditions, and a more appropriate term would be "moksha" ('liberation') or "mukti" ('release'). This state and the conditions considered necessary for its realization is described in early texts of Indian religion such as the Upanishads and the Pāli Canon, and later texts such the Yoga Sutras of Patanjali and the Vedanta tradition. "Moksha" can be attained by "sādhanā", literally 'means of accomplishing something'. It includes a variety of disciplines, such as yoga and meditation.

Nirvana is the profound peace of mind that is acquired with "moksha". In Buddhism and Jainism, it is the state of being free from suffering. In Hindu philosophy, it is union with the Brahman (Supreme Being). The word literally means 'blown out' (as in a candle) and refers, in the Buddhist context, to the blowing out of the fires of desire, aversion, and delusion, and the imperturbable stillness of mind acquired thereafter.

In Theravada Buddhism the emphasis is on one's own liberation from samsara. The Mahayana traditions emphasize the "bodhisattva" path, in which "each Buddha and Bodhisattva is a redeemer," assisting the Buddhist in seeking to achieve the redemptive state. The assistance rendered is a form of self-sacrifice on the part of the teachers, who would presumably be able to achieve total detachment from worldly concerns, but have instead chosen to remain engaged in the material world to the degree that this is necessary to assist others in achieving such detachment.

In Jainism, "salvation", "moksha", and "nirvana" are one and the same. When a soul ("atman") achieves "moksha", it is released from the cycle of births and deaths, and achieves its pure self. It then becomes a "siddha" ('one who has accomplished his ultimate objective'). Attaining Moksha requires annihilation of all "karmas", good and bad, because if karma is left, it must bear fruit.




</doc>
<doc id="29475" url="https://en.wikipedia.org/wiki?curid=29475" title="Lockheed S-3 Viking">
Lockheed S-3 Viking

The Lockheed S-3 Viking is a 4-crew, twin-engine turbofan-powered jet aircraft that was used by the U.S. Navy (USN) primarily for anti-submarine warfare. In the late 1990s, the S-3B's mission focus shifted to surface warfare and aerial refueling. The Viking also provided electronic warfare and surface surveillance capabilities to a carrier battle group. A carrier-based, subsonic, all-weather, long-range, multi-mission aircraft; it carried automated weapon systems and was capable of extended missions with in-flight refueling. Because of its characteristic sound, it was nicknamed the "War Hoover" after the vacuum cleaner brand.

The S-3 was phased out from front-line fleet service aboard aircraft carriers in January 2009, with its missions taken over by aircraft like the P-3C Orion, P-8 Poseidon, Sikorsky SH-60 Seahawk and Boeing F/A-18E/F Super Hornet. Several aircraft were flown by Air Test and Evaluation Squadron Thirty (VX-30) at Naval Base Ventura County / NAS Point Mugu, California, for range clearance and surveillance operations on the NAVAIR Point Mugu Range until 2016 and one S-3 is operated by the National Aeronautics and Space Administration (NASA) at the NASA Glenn Research Center.

In the mid-1960s, the USN developed the VSX (Heavier-than-air, Anti-submarine, Experimental) requirement for a replacement for the piston-engined Grumman S-2 Tracker as an anti-submarine aircraft to fly off aircraft carriers. In August 1968, a team led by Lockheed and a Convair/Grumman team were asked to further develop their proposals to meet this requirement. Lockheed recognised that it had little experience in designing carrier based aircraft, so Ling-Temco-Vought (LTV) was brought into the team, being responsible for the folding wings and tail, the engine nacelles, and the landing gear, which was derived from LTV A-7 Corsair II (nose) and Vought F-8 Crusader (main). Sperry Univac Federal Systems was assigned the task of developing the aircraft's onboard computers which integrated input from sensors and sonobuoys.

On 4 August 1969, Lockheed's design was selected as the winner of the contest and 8 prototypes, designated YS-3A were ordered. The first prototype was flown on 21 January 1972 by military test pilot John Christiansen, and the S-3 entered service in 1974. During the production run from 1974 to 1978, a total of 186 S-3As have been built. The majority of the surviving S-3As were later upgraded to the S-3B variant, with 16 aircraft converted into ES-3A Shadow electronic intelligence (ELINT) collection aircraft.

The S-3 is a conventional monoplane with a cantilever shoulder wing, very slightly swept with a leading edge angle of 15° and an almost straight trailing edge. Its 2 GE TF-34 high-bypass turbofan engines mounted in nacelles under the wings provide excellent fuel efficiency, giving the Viking the required long range and endurance, while maintaining docile engine-out characteristics.
The aircraft can seat 4 crew members (3 officers and 1 enlisted) with pilot and copilot/tactical coordinator (COTAC) in the front of the cockpit and the tactical coordinator (TACCO) and sensor operator (SENSO) in the back. Entry is via a hatch/ladder folding down out of the lower starboard side of the fuselage behind the cockpit, in between the rear and front seats on the starboard side. When the aircraft's anti-submarine warfare (ASW) role ended in the late 1990s, the enlisted SENSOs were removed from the crew. In tanker crew configuration, the S-3B typically flew with a pilot and co-pilot/COTAC. The wing is fitted with leading edge and Fowler flaps. Spoilers are fitted to both the upper and the lower surfaces of the wings. All control surfaces are actuated by dual hydraulically boosted irreversible systems. In the event of dual hydraulic failures, an Emergency Flight Control System (EFCS) permits manual control with greatly increased stick forces and reduced control authority.

Unlike many tactical jets which required ground service equipment, the S-3 was equipped with an auxiliary power unit (APU) and capable of unassisted starts. The aircraft's original APU could provide only minimal electric power and pressurized air for both aircraft cooling and for the engines' pneumatic starters. A newer, more powerful APU could provide full electrical service to the aircraft. The APU itself was started from a hydraulic accumulator by pulling a handle in the cockpit. The APU accumulator was fed from the primary hydraulic system, but could also be pumped up manually (with much effort) from the cockpit.

All crew members sit on forward-facing, upward-firing Douglas Escapac zero-zero ejection seats. In "group eject" mode, initiating ejection from either of the front seat ejects the entire crew in sequence, with the back seats ejecting 0.5 seconds before the front in order to provide safe separation (this was to prevent the pilots, who were more aware of what was happening outside the aircraft from ejecting without the rest of the crew, or being forced to delay ejection to order the crew to eject in an emergency; ejection from either rear seat would not eject the pilots, who had to initiate their own ejections, to prevent loss of the aircraft if a rear crewmember ejected prematurely. If a pilot ejected prematurely, the plane was lost anyway, and automatic ejection prevented the crew from crashing with a pilot-less aircraft before they were aware of what had happened). The rear seats are capable of self ejection and the ejection sequence includes a pyrotechnic charge that stows the rear keyboard trays out of the occupants' way immediately before ejection. Safe ejection requires the seats to be weighted in pairs and when flying with a single crewman in the back the unoccupied seat is fitted with ballast.

At the time it entered the fleet, the S-3 introduced an unprecedented level of systems integration. Previous ASW aircraft like the Lockheed P-3 Orion and S-3's predecessor, the Grumman S-2 Tracker, featured separate instrumentation and controls for each sensor system. Sensor operators often monitored paper traces, using mechanical calipers to make precise measurements and annotating data by writing on the scrolling paper. Beginning with the S-3, all sensor systems were integrated through a single General Purpose Digital Computer (GPDC). Each crew station had its own display, the co-pilot/COTAC, TACCO and SENSO displays were Multi-Purpose Displays (MPD) capable of displaying data from any of a number of systems. This new level of integration allowed the crew to consult with each other by examining the same data at multiple stations simultaneously, to manage workload by assigning responsibility for a given sensor from one station to another and to easily combine clues from each sensor to classify faint targets. Because of this, the 4-crew S-3 was considered roughly equivalent in capability to the much larger P-3 with a crew of 12.

The aircraft has two underwing hardpoints that can be used to carry fuel tanks, general purpose and cluster bombs, missiles, rockets, and storage pods. It also has four internal bomb bay stations that can be used to carry general-purpose bombs, aerial torpedoes, and special stores (B57 and B61 nuclear weapons). Fifty-nine sonobuoys are carried, as well as a dedicated Search and Rescue (SAR) chute. The S-3 is fitted with the ALE-39 countermeasure system and can carry up to 90 rounds of chaff, flares, and expendable jammers (or a combination of all) in three dispensers. A retractable magnetic anomaly detector (MAD) Boom is fitted in the tail.

In the late 1990s, the S-3B's role was changed from anti-submarine warfare (ASW) to anti-surface warfare (ASuW). At that time, the MAD Boom was removed, along with several hundred pounds of submarine detection electronics. With no remaining sonobuoy processing capability, most of the sonobuoy chutes were faired over with a blanking plate.

On 20 February 1974, the S-3A officially became operational with the Air Antisubmarine Squadron FORTY-ONE (VS-41), the "Shamrocks," at NAS North Island, California, which served as the initial S-3 Fleet Replacement Squadron (FRS) for both the Atlantic and Pacific Fleets until a separate Atlantic Fleet FRS, VS-27, was established in the 1980s. The first operational cruise of the S-3A took place in 1975 with the VS-21 "Fighting Redtails" aboard .

Starting in 1987, some S-3As were upgraded to S-3B standard with the addition of a number of new sensors, avionics, and weapons systems, including the capability to launch the AGM-84 Harpoon anti-ship missile. The S-3B could also be fitted with "buddy stores", external fuel tanks that allowed the Viking to refuel other aircraft. In July 1988, VS-30 became the first fleet squadron to receive the enhanced capability Harpoon/ISAR equipped S-3B, based at NAS Cecil Field in Jacksonville, Florida. 16 S-3As were converted to ES-3A Shadows for carrier-based electronic intelligence (ELINT) duties. Six aircraft, designated US-3A, were converted for a specialized utility and limited cargo Carrier onboard delivery (COD) requirement. Plans were also made to develop the KS-3A carrier-based tanker aircraft, but this program was ultimately cancelled after the conversion of just one early development S-3A.

With the collapse of the Soviet Union and the breakup of the Warsaw Pact, the Soviet-Russian submarine threat was perceived as much reduced, and the Vikings had the majority of their antisubmarine warfare equipment removed. The aircraft's mission subsequently changed to sea surface search, sea and ground attack, over-the-horizon targeting, and aircraft refueling. As a result, the S-3B after 1997 was typically crewed by one pilot and one copilot [NFO]; the additional seats in the S-3B could still support additional crew members for certain missions. To reflect these new missions the Viking squadrons were redesignated from "Air Antisubmarine Warfare Squadrons" to "Sea Control Squadrons."
Prior to the aircraft's retirement from front-line fleet use aboard US aircraft carriers, a number of upgrade programs were implemented. These include the Carrier Airborne Inertial Navigation System II (CAINS II) upgrade, which replaced older inertial navigation hardware with ring laser gyroscopes with a Honeywell EGI (Enhanced GPS Inertial Navigation System) and added digital electronic flight instruments (EFI). The Maverick Plus System (MPS) added the capability to employ the AGM-65E laser-guided or AGM-65F infrared-guided air-to-surface missile, and the AGM-84H/K Stand-off Land Attack Missile Expanded Response (SLAM/ER). The SLAM/ER is a GPS/inertial/infrared guided cruise missile derived from the AGM-84 Harpoon that can be controlled by the aircrew in the terminal phase of flight if an AWW-13 data link pod is carried by the aircraft.

The S-3B saw extensive service during the 1991 Gulf War, performing attack, tanker, and ELINT duties, and launching ADM-141 TALD decoys. This was the first time an S-3B was employed overland during an offensive air strike. The first mission occurred when an aircraft from VS-24, from the , attacked an Iraqi Silkworm missile site. The aircraft also participated in the Yugoslav wars in the 1990s and in Operation Enduring Freedom in 2001.

The first ES-3A was delivered in 1991, entering service after two years of testing. The Navy established two squadrons of eight ES-3A aircraft each in both the Atlantic and Pacific Fleets to provide detachments of typically two aircraft, ten officers, and 55 enlisted aircrew, maintenance and support personnel (which comprised/supported four complete aircrews) to deploying carrier air wings. The Pacific Fleet squadron, Fleet Air Reconnaissance Squadron FIVE (VQ-5), the "Sea Shadows," was originally based at the former NAS Agana, Guam but later relocated to NAS North Island in San Diego, California, with the Pacific Fleet S-3 Viking squadrons when NAS Agana closed in 1995 as a result of a 1993 Base Realignment and Closure (BRAC) decision. The Atlantic Fleet squadron, the VQ-6 "Black Ravens," were originally based with all Atlantic Fleet S-3 Vikings at the former NAS Cecil Field in Jacksonville, Florida, but later moved to NAS Jacksonville, approximately to the east, when NAS Cecil Field was closed in 1999 as a result of the same 1993 BRAC decision that closed NAS Agana.
The ES-3A operated primarily with carrier battle groups, providing organic 'Indications and Warning' support to the group and joint theater commanders. In addition to their warning and reconnaissance roles, and their extraordinarily stable handling characteristics and range, Shadows were a preferred recovery tanker (aircraft that provide refueling for returning aircraft). They averaged over 100 flight hours per month while deployed. Excessive utilization caused earlier than expected equipment replacement when Naval aviation funds were limited, making them an easy target for budget-driven decision makers. In 1999, both ES-3A squadrons and all 16 aircraft were decommissioned and the ES-3A inventory placed in Aerospace Maintenance and Regeneration Group (AMARG) storage at Davis-Monthan AFB, Arizona.

In March 2003, during Operation Iraqi Freedom, an S-3B Viking from Sea Control Squadron 38 (The "Red Griffins") piloted by Richard McGrath Jr. launched from . The crew successfully executed a time sensitive strike and fired a laser-guided Maverick missile to neutralize a significant Iraqi naval and leadership target in the port city of Basra, Iraq. This was one of the few times in its operational history that the S-3B Viking had been employed overland on an offensive combat air strike and the first time it launched a laser-guided Maverick missile in combat.
On 1 May 2003, US President George W. Bush flew in the co-pilot seat of a VS-35 Viking from NAS North Island, California, to off the California coast. There, he delivered his "Mission Accomplished" speech announcing the end of major combat in the 2003 invasion of Iraq. During the flight, the aircraft used the customary presidential callsign of "Navy One". The aircraft that President Bush flew in was retired shortly thereafter and on 15 July 2003 was accepted as an exhibit at the National Museum of Naval Aviation at NAS Pensacola, Florida.

Between July and December 2008 the VS-22 Checkmates, the last sea control squadron, operated a detachment of four S-3Bs from the Al Asad Airbase in Al Anbar Province, west of Baghdad. The planes were fitted with LANTIRN pods and they performed non-traditional intelligence, surveillance, and reconnaissance (NTISR). After more than 350 missions, the Checkmates returned to NAS Jacksonville, Florida, on 15 December 2008, prior to disestablishing on 29 January 2009.

Though a proposed airframe known as the Common Support Aircraft was once advanced as a successor to the S-3, E-2 and C-2, this plan failed to materialize. As the surviving S-3 airframes were forced into sundown retirement, a Lockheed Martin full scale fatigue test was performed and extended the service life of the aircraft by approximately 11,000 flight-hours. This supported Navy plans to retire all Vikings from front-line fleet service by 2009 so new strike fighter and multi-mission aircraft could be introduced to recapitalize the aging fleet inventory, with former Viking missions assumed by other fixed-wing and rotary-wing aircraft.

The final carrier based S-3B Squadron, VS-22 was decommissioned at NAS Jacksonville on 29 January 2009. Sea Control Wing Atlantic was decommissioned the following day on 30 January 2009, concurrent with the U.S. Navy retiring the last S-3B Viking from front-line Fleet service.

In June 2010 the first of three aircraft to patrol the Pacific Missile Test Center's range areas off of California was reactivated and delivered. The jet aircraft's higher speed, 10-hour endurance, modern radar, and a LANTIRN targeting pod allowed it to quickly confirm the test range being clear of wayward ships and aircraft before tests commence. These S-3Bs are flown by Air Test and Evaluation Squadron Thirty (VX-30) based out of NAS Point Mugu, California. Also, the NASA Glenn Research Center acquired four S-3Bs in 2005. Since 2009, one of these aircraft (USN BuNo 160607) has also carried the civil registration N601NA and is used for various tests.

By late 2015, the U.S. Navy had three Vikings remaining operational in support roles. One was moved to The Boneyard in November 2015, and the final two were retired, one stored and the other transferred to NASA, on 11 January 2016, officially retiring the S-3 from Navy service.

Naval analysts have suggested returning the stored S-3s to service with the U.S. Navy to fill gaps it left in the carrier air wing when it was retired. This is in response to the realization that the Chinese navy is producing new weapons that can threaten carriers beyond the range their aircraft can strike them. Against the DF-21D anti-ship ballistic missile, carrier-based F/A-18 Super Hornets and F-35C Lightning IIs have about half the unrefueled strike range, so bringing the S-3 back to aerial tanking duties would extend their range against it, as well as free up more Super Hornets that were forced to fill the role. Against submarines armed with anti-ship cruise missiles like the Klub and YJ-18, the S-3 would restore area coverage for ASW duties. Bringing the S-3 out of retirement could at least be a stop-gap measure to increase the survivability and capabilities of aircraft carriers until new aircraft can be developed for such purposes.

In October 2013, the Republic of Korea Navy expressed an interest in acquiring up to 18 ex-USN S-3s to augment their fleet of 16 Lockheed P-3 Orion aircraft. In August 2015, a military program review group approved a proposal to incorporate 12 mothballed S-3s to perform ASW duties; the Viking plan will be sent to the Defense Acquisition Program Administration for further assessment before final approval by the national defense system committee. Although the planes are old, being in storage kept them serviceable and using them is a cheaper way to fulfill short-range airborne ASW capabilities left after the retirement of the S-2 Tracker than buying newer aircraft. Refurbished S-3s could be returned to use by 2019. In 2017, the Republic of Korea Navy canceled plans to purchase refurbished and upgraded Lockheed S-3 Viking aircraft for maritime patrol and anti-submarine duties, leaving offers by Airbus, Boeing, Lockheed Martin, and Saab on the table.

In April 2014, Lockheed Martin announced that they would offer refurbished and remanufactured S-3s, dubbed the C-3, as a replacement for the Northrop Grumman C-2A Greyhound for carrier onboard delivery. The requirement for 35 aircraft would be met from the 91 S-3s currently in storage. In February 2015, the Navy announced that the Bell Boeing V-22 Osprey had been selected to replace the C-2 for the COD mission.





Notes
Bibliography



</doc>
<doc id="29476" url="https://en.wikipedia.org/wiki?curid=29476" title="Kaman SH-2 Seasprite">
Kaman SH-2 Seasprite

The Kaman SH-2 Seasprite is a ship-based helicopter originally developed and produced by American manufacturer Kaman Aircraft Corporation. It has been typically used as a compact and fast-moving rotorcraft for utility and anti-submarine warfare missions.

Development of the Seasprite had been initiated during the late 1950s in response to a request from the United States Navy, calling for a suitably fast and compact naval helicopter for utility missions. Kaman's submission, internally designated as the "K-20", was favourably evaluated, leading to the issuing of a contract for the construction of four prototypes and an initial batch of 12 production helicopters, designated as the "HU2K-1". Under the 1962 United States Tri-Service aircraft designation system, the HU2K was redesignated H-2, the HU2K-1 becoming the UH-2A. Beyond the U.S. Navy, the company had also made efforts to acquire other customers for export sales, in particular the Royal Canadian Navy; however, the initial interest of the Canadians was quelled as a result of Kaman's demand for price increases and the Seasprite performing below company projections during its sea trials. Due to its unsatisfactory performance, from 1968 onwards, the U.S. Navy's existing UH-2s were remanufactured from their originally-delivered single-engine arrangement to a more powerful twin-engine configuration.

In October 1970, the Seasprite was selected by the U.S. Navy as the platform for the interim Light Airborne Multi-Purpose System (LAMPS) helicopter, which resulted in greatly enhanced anti-submarine and anti-surface threat capabilities being developed and installed upon a new variant of the type, designated as the "SH-2D/F". Accordingly, during the 1970s and 1980s, the majority of the existing UH-2 helicopters were remanufactured into the improved SH-2F model. In this configuration, the Seasprite extended and increased shipboard sensor and weapon capabilities against several types of enemy threats, including submarines of all types, surface ships and patrol craft that may be armed with anti-ship missiles.

The Seasprite served for many decades with the U.S. Navy. Highlights of its service life included operations during the lengthy Vietnam War, in which the type was primarily used to rescue downed friendly aircrews within the theatre of operations, and its deployment during the Gulf War, where Seasprites conducted combat support and surface warfare operations against hostile Iraqi forces. In more routine operations, the Seasprite was operated in a number of roles, including anti-submarine warfare (ASW), search and rescue (SAR), utility and plane guard (the latter being performed when on attachment to aircraft carriers). The type was finally withdrawn in 2001 when the last examples of the final variant, known as the SH-2G Super Seasprite were retired. During the 1990s and 2000s, ex-U.S. Navy Seasprites were offered to various nations as a form of foreign aid, which typically met with mixed interest and a limited uptake.

In 1956, the U.S. Navy launched a new competition with the intent of meeting its requirements for a compact, all-weather multipurpose naval helicopter, encouraging private companies to submit their proposals. American manufacturer Kaman Aircraft Corporation decided to produce its own response for the competition, their submitted design, which was given the internal company designation of "K-20", was of a relatively conventional helicopter powered by a single General Electric T58-8F turboshaft engine which drove a 44-foot four-bladed main rotor and a four-bladed tail rotor. Following an evaluation of the designs that had been bid in response, the U.S. Navy decided to select the submission by Kaman to proceed with further development. Accordingly, in late 1957, Kaman was promptly awarded with a contract calling for the construction of four prototypes and an initial batch of 12 production helicopters, designated as the "HU2K-1".

In 1960, the Royal Canadian Navy announced that the HU2K has been identified as the frontrunner for their own requirement for an anti-submarine warfare helicopter; this choice was confirmed when the Treasury Board of the Canadian government gave its approval for the initial procurement of 12 rotorcraft from Kaman at a price of $14.5 million. However, the Canadian purchase was disrupted by multiple factors, including Kaman's decision to abruptly raise the estimated price of the initial batch to $23 million; as the same time, there were concerns amongst officials that the manufacturer's projections of both the weight and performance criteria has been overly optimistic. In response, the Canadian Naval Board decided to hold off on issuing its approval to proceed with the HU2K purchase until after the US Navy had conducted sea trials with the type. During these sea trials, it was revealed that the HU2K was indeed overweight and underpowered; in light of this inferior performance, the HU2K was deemed to be incapable of meeting the Canadian requirements. Accordingly, during late 1961, the competing Sikorsky CH-124 Sea King was selected to fulfil the intended role instead.

Having been unable to achieve any follow-on orders for the type, Kaman decided in the late 1960s to terminate production following the completion of the delivery of 184 H-2s to the U.S. Navy. However, in 1971, production was restarted by Kaman in order to manufacture an improved variant of the helicopter, designated as the "SH-2F". A significant factor in the reopening of the production line was that the Navy's Sikorsky SH-60 Sea Hawk, which was both newer and more capable in anti-submarine operations, had been determined to be too large to allow it to be safely operated from the smaller flight decks present upon the older frigates then in service.

Upon the enactment of the 1962 United States Tri-Service aircraft designation system, the HU2K-1 had been redesignated as the "UH-2A", while the "HU2K-1U" model was redesignated as the "UH-2B". During its service, the UH-2 Seasprite would be subject to several modifications and improvements, such as the addition of fixtures for the mounting of external stores. Beginning in 1968, the Navy's remaining UH-2s were extensively remanufactured; perhaps the most extensive alteration performance was the replacement of their original single-engine arrangement with a more powerful twin-engine configuration.

In October 1970, the UH-2 was selected to be the platform to function as the interim Light Airborne Multi-Purpose System (LAMPS) helicopter. During the course of the 1960s, LAMPS had evolved out of an urgent requirement to develop a manned helicopter that would be capable of supporting a non-aviation vessel and serve as its tactical Anti-Submarine Warfare arm. Widely referred to as "LAMPS Mark I", the advanced sensors, processors, and display capabilities aboard the helicopter enabled such equipped ships to extend their situational awareness beyond the line-of-sight limitations that unavoidably hampered the performance of shipboard radars, as well as the short distances involved in the acoustic detection and prosecution of underwater threats associated with hull-mounted sonars. Those H-2s that were reconfigured to perform the LAMPS mission were accordingly re-designated as "SH-2D"s.

On 16 March 1971, the first SH-2D LAMPS prototype conducted its first flight. Beginning in 1973, production deliveries of the latest variant of the rotorcraft, designated as the "SH-2F", commenced. Amongst the features of the "SH-2F" model was the full suite of LAMPS I equipment, along with various other improvements, such as upgraded engines, an extended life main rotor, and an elevated take-off weight. During 1981, the Navy placed an order for 60 production SH-2Fs. From 1987 onwards, a total of 16 SH-2Fs were upgraded with a chin-mounted forward-looking infrared (FLIR) sensor, chaff/flare launchers, dual rear-mounted infrared countermeasures, and missile/mine detecting equipment.

Eventually, all but two H-2s that were then in the U.S. Navy inventory were remanufactured into the SH-2F configuration. The final production procurement of the SH-2F was in Fiscal Year 1986. The final six orders for production SH-2Fs were converted to the more extensive and newer SH-2G Super Seasprite variant.

In 1962, the initial UH-2 model commenced its operational service with the U.S. Navy. The U.S. Navy quickly determined that the helicopter's capabilities were greatly restricted by its single engine; thus, the service ordered Kaman to retrofit all of its Seasprites into a more capable twin-engine arrangement instead; when furnished with a pair of engines, the Seasprite was capable of attaining an airspeed of 130 knots and operating at a range of up to 411 nautical miles. The U.S. Navy would operate a total fleet of nearly 200 Seasprites to perform a variety of missions, ranging from anti-submarine warfare (ASW) operations, search and rescue (SAR) and utility transport. Under typical operational conditions, several UH-2s would be deployed upon each of the U.S. Navy's aircraft carriers in order to perform plane guard and SAR missions.

The UH-2 was introduced in time to see action in the Tonkin Gulf incident in August 1964. The Seasprite's principal contribution to what would escalate into the lengthy Vietnam War between the Soviet-backed North Vietnamese and the United States-backed South Vietnamese, was the retrieval of downed aircrews, both from the sea and from inside enemy territory. The type was increasingly relied upon to perform the retrieval mission as the conflict intensified, such as during Operation Rolling Thunder in 1965. During October 1966 alone, out of 269 downed pilots, helicopter-based SAR teams were recorded as having enabled the recovery of 103 men.
During the 1970s, the conversion of UH-2s to the SH-2 anti-submarine configuration provided the U.S. Navy with its first dedicated ASW helicopter capable of operating from vessels other than its aircraft carriers. The compact size of the SH-2 allowed the type to be operated from flight decks that were too small for the majority of helicopters; this factor would later play a role in the U.S. Navy's decision to acquire the improved SH-2F during the early 1980s.

The SH-2F fleet was utilized to enforce and support Operation Earnest Will in July 1987, Operation Praying Mantis in April 1988, and Operation Desert Storm during January 1991 in the Persian Gulf region. The countermeasures and additional equipment present upon the SH-2F allowed the type to conduct combat support and surface warfare missions within these hostile environments, which had an often-minimal submarine threat. In April 1994, the SH-2F was retired from active service with the U.S. Navy; the timing corresponded with the retirement of the last of the Vietnam-era Knox Class Frigates that were unable to accommodate the new and larger SH-60 Sea Hawks, which were used to replace the aging Seasprites.

In 1991, the U.S. Navy had begun to receive deliveries of the new SH-2G Super Seasprite; a total of 18 converted SH-2Fs and 6 new-built SH-2Gs were produced. These were assigned to Naval Reserve squadrons, the SH-2G entered service with HSL-84 in 1993. The SH-2 served in some 600 deployments and flew 1.5 million flight hours before the last of the type were finally retired in mid-2001.

The Royal New Zealand Navy (RNZN) replaced its Westland Wasps with an initial batch of four interim SH-2F Seasprites (formerly operated by the U.S. Navy), operated and maintained by a mix of Navy and Air Force personnel known as No. 3 Squadron RNZAF Naval Support Flight, to operate with ANZAC class frigates until the fleet of five new SH-2G(NZ) Super Seasprites were delivered. In October 2005, the Navy air element was transferred to No. 6 Squadron RNZAF at RNZAF Base Auckland in Whenuapai. RNZN Seasprites have seen service in East Timor. 10 of the 11 SH-2G(A)s rejected by the Royal Australian Navy were purchased in 2014 to replace the five RNZN SH-2G(NZ) Seasprites that had required either a MLU (Mid Life Upgrade) or replacement due to corrosion issues, maintenance problems and obsolescence. Kaman modified the ex-Australian aircraft and renamed them SH-2G(I), with the last one being delivered to New Zealand in early 2016. Eight of the aircraft are flying with the ninth and tenth aircraft being attritional aircraft used for spares etc. The 11th aircraft is held by Kaman as a prototype and test aircraft. The five SH-2G(NZ) have been sold to Peru. A SH-2F (ex-RNZN, NZ3442) is preserved in the Royal New Zealand Air Force Museum, donated to the museum by Kaman Aircraft Corporation after an accident while in service with the RNZN.

During the late 1990s, the United States decided to offer the surplus U.S. Navy SH-2Fs as foreign aid to a number of overseas countries. Amongst those to be offered the type included Greece, which had been offered six, and Turkey, which had been offered 14, but they rejected the offer. Egypt opted to acquire four SH-2F under this aid program, they were mainly used for spares in to support of their existing fleet of ten SH-2Gs. Poland chose to acquire the later SH-2G variant.






</doc>
<doc id="29480" url="https://en.wikipedia.org/wiki?curid=29480" title="Plosive consonant">
Plosive consonant

In phonetics, a plosive, also known as an occlusive or simply a stop, is a pulmonic consonant in which the vocal tract is blocked so that all airflow ceases.

The occlusion may be made with the tongue tip or blade (, ) tongue body (, ), lips (, ), or glottis (). Plosives contrast with nasals, where the vocal tract is blocked but airflow continues through the nose, as in and , and with fricatives, where partial occlusion impedes but does not block airflow in the vocal tract.

The terms "stop, occlusive," and "plosive" are often used interchangeably. Linguists who distinguish them may not agree on the distinction being made. The terms refer to different features of the consonant. "Stop" refers to the airflow that is stopped. "Occlusive" refers to the articulation, which occludes (blocks) the vocal tract. "Plosive" refers to the release burst (plosion) of the consonant. Some object to the use of "plosive" for inaudibly released stops, which may then instead be called "applosives". The International Phonetic Association and the International Clinical Phonetics and Linguistics Association use the term "plosive".

Either "occlusive" or "stop" may be used as a general term covering the other together with nasals. That is, 'occlusive' may be defined as oral occlusive (plosives and affricates) plus nasal occlusives (nasals such as , ), or 'stop' may be defined as oral stops (plosives) plus nasal stops (nasals). Ladefoged and Maddieson (1996) prefer to restrict 'stop' to oral non-affricated occlusives. They say,

In addition, they restrict "plosive" for a pulmonic consonants; "stops" in their usage include ejective and implosive consonants.

If a term such as "plosive" is used for oral non-affricated obstruents, and nasals are not called nasal stops, then a "stop" may mean the glottal stop; "plosive" may even mean non-glottal stop. In other cases, however, it may be the word "plosive" that is restricted to the glottal stop. Note that, generally speaking, plosives do not have plosion (a release burst). In English, for example, there are plosives with no audible release, such as the in "apt". However, English plosives do have plosion in other environments.

In Ancient Greek, the term for plosive was ("áphōnon"), which means "unpronounceable", "voiceless", or "silent", because plosives could not be pronounced without a vowel. This term was calqued into Latin as , and from there borrowed into English as "mute". "Mute" was sometimes used instead for voiceless consonants, whether plosives or fricatives, a usage that was later replaced with "surd", from Latin "deaf" or "silent", a term still occasionally seen in the literature. For more information on the Ancient Greek terms, see .

A plosive is typically analysed as having up to three phases:

Only the hold phase is requisite. A plosive may lack an approach when it is preceded by a consonant that involves an occlusion at the same place of articulation, as in in "end" or "old". In many languages, such as Malay and Vietnamese, word-final plosives lack a release burst, even when followed by a vowel, or have a nasal release. See no audible release.

Nasal occlusives are somewhat similar. In the catch and hold, airflow continues through the nose; in the release, there is no burst, and final nasals are typically unreleased across most languages.

In affricates, the catch and hold are those of a plosive, but the release is that of a fricative. That is, affricates are plosive–fricative contours.

All spoken natural languages in the world have plosives, and most have at least the voiceless plosives , , and . However, there are exceptions: Colloquial Samoan lacks the coronal , and several North American languages, such as the northern Iroquoian and southern Iroquoian languages (i.e., Cherokee), lack the labial . In fact, the labial is the least stable of the voiceless plosives in the languages of the world, as the unconditioned sound change → (→ → Ø) is quite common in unrelated languages, having occurred in the history of Classical Japanese, Classical Arabic, and Proto-Celtic, for instance. Formal Samoan has only one word with velar ; colloquial Samoan conflates and to . Ni‘ihau Hawaiian has for to a greater extent than Standard Hawaiian, but neither distinguish a from a . It may be more accurate to say that Hawaiian and colloquial Samoan do not distinguish velar and coronal plosives than to say they lack one or the other.

See Common occlusives for the distribution of both plosives and nasals.

Voiced plosives are pronounced with vibration of the vocal cords, voiceless plosives without. Plosives are commonly voiceless, and many languages, such as Mandarin Chinese and Hawaiian, have only voiceless plosives. Others, such as most Australian languages, are indeterminate: plosives may vary between voiced and voiceless without distinction.

In aspirated plosives, the vocal cords (vocal folds) are abducted at the time of release. In a prevocalic aspirated plosive (a plosive followed by a vowel or sonorant), the time when the vocal cords begin to vibrate will be delayed until the vocal folds come together enough for voicing to begin, and will usually start with breathy voicing. The duration between the release of the plosive and the voice onset is called the "voice onset time" (VOT) or the "aspiration interval". Highly aspirated plosives have a long period of aspiration, so that there is a long period of voiceless airflow (a phonetic ) before the onset of the vowel. In tenuis plosives, the vocal cords come together for voicing immediately following the release, and there is little or no aspiration (a voice onset time close to zero). In English, there may be a brief segment of breathy voice that identifies the plosive as voiceless and not voiced. In voiced plosives, the vocal folds are set for voice before the release, and often vibrate during the entire hold, and in English, the voicing after release is not breathy. A plosive is called "fully voiced" if it is voiced during the entire occlusion. In English, however, initial voiced plosives like or may have no voicing during the period of occlusion, or the voicing may start shortly before the release and continue after release, and word-final plosives tend to be fully devoiced: In most dialects of English, the final /b/, /d/ and /g/ in words like "rib", "mad" and "dog" are fully devoiced. Initial voiceless plosives, like the "p" in "pie", are aspirated, with a palpable puff of air upon release, whereas a plosive after an "s", as in "spy", is tenuis (unaspirated). When spoken near a candle flame, the flame will flicker more after the words "par, tar," and "car" are articulated, compared with "spar, star," and "scar". In the common pronunciation of "papa", the initial "p" is aspirated whereas the medial "p" is not.

In a geminate or long consonant, the occlusion lasts longer than in simple consonants. In languages where plosives are only distinguished by length (e.g., Arabic, Ilwana, Icelandic), the long plosives may be held up to three times as long as the short plosives. Italian is well known for its geminate plosives, as the double "t" in the name "Vittoria" takes just as long to say as the "ct" does in English "Victoria". Japanese also prominently features geminate consonants, such as in the minimal pair 来た "kita" 'came' and 切った "kitta" 'cut'.

Note that there are many languages where the features voice, aspiration, and length reinforce each other, and in such cases it may be hard to determine which of these features predominates. In such cases, the terms fortis is sometimes used for aspiration or gemination, whereas lenis is used for single, tenuous, or voiced plosives. Be aware, however, that the terms "fortis" and "lenis" are poorly defined, and their meanings vary from source to source.

Simple nasals are differentiated from plosives only by a lowered velum that allows the air to escape through the nose during the occlusion. Nasals are acoustically sonorants, as they have a non-turbulent airflow and are nearly always voiced, but they are articulatorily obstruents, as there is complete blockage of the oral cavity. The term occlusive may be used as a cover term for both nasals and plosives.

A prenasalized stop starts out with a lowered velum that raises during the occlusion. The closest examples in English are consonant clusters such as the [nd] in "candy", but many languages have prenasalized stops that function phonologically as single consonants. Swahili is well known for having words beginning with prenasalized stops, as in "ndege" 'bird', and in many languages of the South Pacific, such as Fijian, these are even spelled with single letters: "b" [mb], "d" [nd].

A postnasalized plosive begins with a raised velum that lowers during the occlusion. This causes an audible nasal "release", as in English "sudden". This could also be compared to the /dn/ cluster found in Russian and other Slavic languages, which can be seen in the name of the Dnieper River.

Note that the terms "prenasalization" and "postnasalization" are normally used only in languages where these sounds are phonemic: that is, not analyzed into sequences of plosive plus nasal.

Stops may be made with more than one airstream mechanism. The normal mechanism is pulmonic egressive, that is, with air flowing outward from the lungs. All languages have pulmonic stops. Some languages have stops made with other mechanisms as well: ejective stops (glottalic egressive), implosive stops (glottalic ingressive), or click consonants (lingual ingressive).

A fortis plosive is produced with more muscular tension than a lenis plosive. However, this is difficult to measure, and there is usually debate over the actual mechanism of alleged fortis or lenis consonants.

There are a series of plosives in the Korean language, sometimes written with the IPA symbol for ejectives, which are produced using "stiff voice", meaning there is increased contraction of the glottis than for normal production of voiceless plosives. The indirect evidence for stiff voice is in the following vowels, which have a higher fundamental frequency than those following other plosives. The higher frequency is explained as a result of the glottis being tense. Other such phonation types include breathy voice, or murmur; slack voice; and creaky voice.

The following plosives have been given dedicated symbols in the IPA.

Many subclassifications of plosives are transcribed by adding a diacritic or modifier letter to the IPA symbols above.





</doc>
<doc id="29482" url="https://en.wikipedia.org/wiki?curid=29482" title="Stayman convention">
Stayman convention

Stayman is a bidding convention in the card game contract bridge. It is used by a partnership to find a 4-4 or 5-3 trump fit in a suit after making a one (1NT) opening bid and it has been adapted for use after a 2NT opening, a 1NT overcall, and many other natural notrump bids.

The convention is named for Sam Stayman, who wrote the first published description in 1945, but its inventors were two other players: the British expert Jack Marx in 1939, who published it only in 1946, and Stayman's regular partner George Rapée in 1944.

A bid and made in a major suit (i.e. 4 or 4 ) scores better than a game contract bid and made in a minor suit (i.e. 5 or 5 ) or in notrump (i.e. 3NT). Also, the success rate for a game contract in a major suit when a partnership has a combined holding of 26 points and eight cards in the major is about 80%, whereas a game contract in 3NT with 26 (HCP) has a success rate of only 60%, or 50% with 25 HCP; the success rate for a minor suit game contract when holding 26 points is about 30%.

Accordingly, partnership priority is to find an eight card or better major suit fit when jointly holding sufficient values for a game contract. 5-3 and 6-2 fits are easy to find in basic methods as responder can bid 3 or 3 over 1NT, and opener will not normally have a 5 card major to bid 1NT. However, finding 4-4 fits presents a problem. The 2 and 2 bids cannot be used for this as they are weak takeouts, a sign-off bid.

After an opening bid or an overcall of 1NT (2NT), or bids an artificial 2 (3) to ask opener or overcaller if he holds a four- or five-card major suit; some partnership agreements may require the major to be headed by an honor of at least a specified rank, such as the queen. The artificial club bid typically promises four cards in at least one of the major suits (promissory Stayman) and, "in standard form", enough strength to continue bidding after partner's response (8 HCP for an invitational bid opposite a standard strong 1NT opening or overcall showing 15-17 HCP, 11 HCP opposite a weak notrump of 12-14 HCP, or 5 HCP to go to game opposite a standard 2NT showing 20-21 points). It also promises distribution that is not 4333. By invoking the Stayman convention, the responder takes control of the bidding since strength and distribution of the opener's hand is already known within a limited range. The opener responds with the following rebids.
A notrump opener should have neither a suit longer than five cards nor more than one 5-card suit since an opening notrump bid shows a balanced hand. A notrump bidder who has at least four cards in each major suit normally responds in hearts, as this can still allow a spade fit to be found. Variant methods are to bid the longer or stronger major, with a preference given to spades, or to use 2NT to show both majors.

In the standard form of Stayman over 1NT, the responder has a number of options depending on his partner's answer:
Over these bids, the notrump bidder (1) with a maximum hand (17 HCP), goes to game over an invitational bid and (2) with four (or more) cards in each major suit, corrects to the previously unbid major suit.

In the standard form of Stayman over 2NT, the responder has only two normal rebids.

In either case, a responder who rebids notrump over a response in a major suit promises four cards of the other major suit. Thus, a notrump opener who holds at least four cards in each major suit should "correct" by bidding the other major suit at the lowest level.

Of course, once a fit is found, responder who has sufficient strength also may bid 4 (Gerber) or 4NT (Blackwood), or cue bid aces, depending upon partnership agreement, to explore slam in any of the above sequences. Some partnerships also admit responder's rebids of a major suit that the notrump bidder did not name.

A bid of 4 over an opening bid of 3NT may be either Stayman or Gerber, depending upon the partnership agreement.

If an adverse suit bid is inserted immediately after a 1NT opening, Stayman may be employed via a double (by partnership agreement) or a cue bid, depending on the strength of his hand. The cue bid, which is conventional, is completely artificial and means nothing other than invoking Stayman. For example, if South opens 1NT, and West overcalls 2, North, if he has adequate values, may call 3, invoking Stayman. South would then show his major or bid game in notrump. Alternatively, North, if his hand lacks the values for game in notrump, may double, which by partnership agreement employs Stayman. This keeps the Stayman bidding at second level.

Partnerships who have not yet learned Stayman but choose to adopt Stayman (without having yet learned or having chosen not to use Jacoby Transfers) will need to adjust their use of normal two-level responses after a 1NT opening, because the availability of this convention changes the nature of what had been normal 1NT responses. When the notrump bidder's partner does not invoke Stayman but instead calls 2 or 2, it is a sign of relative weakness (since if responder held 8 HCP or more, he would have invoked Stayman). These bids are commonly referred to as "drop dead bids", as the opening notrump bidder is requested to withdraw from the auction. If opener has maximum values, a fit, and strong support, he may raise to the 3-level, but under no circumstances may he take any other action. This provides the partnership with an advantage that the non-Stayman partnership doesn't enjoy. For example, a responder may have no honors at all; that is, a total of zero HCP. His partner is likely to be set if he passes. A non-Stayman responder would have to pass, because to bid would provoke a rebid. But a Stayman responder can respond to his partner's 1NT opening at level 2 if he has a 6-card non-club suit. The responder with 3 HCP and a singleton can make a similar call with a 5-card non-club suit. This gives the partnership a better than even chance of success in making the contract, whereas without a response (and without Stayman), the contract would likely be set.

Similarly, a response of 2 indicates less than 8 HCP and should usually be passed. In rare cases, when the opener has maximum values and a fit in diamonds with at least two of the top three honors, he may raise diamonds, and responder may see a chance for game in notrump.

There are many variations on this basic theme, and partnership agreement may alter the details of its use. It is one of the most widely used conventions in bridge.

Some partnerships play that 2 Stayman does not absolutely promise a four-card major (non promissory Stayman). For example, if responder has a short suit and wishes to know if opener has four-card cover in it, so as to play in no-trumps. If opener shows hearts initially, 2 can be used to find a fit in spades when the 2 does not promise a four-card major.

1NT - 2, 2 -
Alternatively 2 can be used for all hands with four spades and not four hearts, either invitational or game values, while 3NT denies four spades.

Today, most players use Stayman in conjunction with Jacoby transfers. With Stayman in effect, the responder practically denies having a five-card major, as otherwise he would transfer to the major immediately. The only exception is when responder has 5-4 in the majors; in that case, he could use Stayman, and in the case of a 2 response, bid the five-card major at the two level (weakness take-out / Garbage Stayman) or at the three level (forcing to game). However, the latter hand can also be bid by first using a transfer and then showing the second suit naturally. The Smolen convention provides an alternative method to show a five-card major and game-going values. A minor drawback of Jacoby transfers is that a 2 contract is not possible.

The Smolen convention is an adjunct to Stayman for situations in which the notrump opener has denied holding a four-card major and responder has a five-card major and a four-card major with game-going values.

If the notrump opener responds to the Stayman 2 asking bid with 2, denying a four-card major, responder initiates the Smolen Transfer with a jump shift to three of his four-card major. The jump shift shows which is the four-card major and promises five in the other major. The notrump opener then bids four of the other major with three cards in the suit or 3NT with fewer than three.

Smolen may also be used when responder has a six-card major and a four-card major with game-going values; after the 2 negative response by opener, responder double jump shifts to four in the suit just below his six-card major and the notrump opener transfers to four of his partner's six-card major.

This convention allows a partnership to find either a 5-3 fit, 6-3 and 6-2 fit while ensuring that the notrump opener, who has the stronger hand, will be declarer.

"Garbage" Stayman (or "Weak Stayman" or "Rescue Stayman") and "Crawling" Stayman are adaptations of Stayman frequently used for damage control when holding a weak hand opposite a 1NT opening bid. For example, on the following hand.

Partner opens 1NT (15-17), and right hand opponent passes. Opponents have 23-25 HCP. Thus, 1NT is virtually certain to go down by at least three or four tricks. Indeed, in No-trumps, this dummy will be completely worthless. 

In "Garbage Stayman", you bid 2 Stayman with this "garbage" hand rather than passing on the first round, and then "pass opener's response". If opener rebids a major suit you have found a 4-4 fit and ability to trump club losers. Likewise, a response of 2 guarantees no worse than a 5-2 fit in diamonds and, with a fifth trump, a potential additional ruff. Declarer can also reach dummy with ruffs and may then be able to take finesses or execute a squeeze that otherwise would not be possible. The result is a contract that will go down fewer tricks or may even make, rather than a contract that is virtually certain to go down at least three or four tricks. However the hand must be able to tolerate any rebid from opener.

"Crawling Stayman" is an optional extension of "Garbage Stayman" for situations in which the responder's diamond suit is short. In "Crawling Stayman", the responder rebids 2 over the Notrump bidder's 2 reply. This conventional bid shows a weak hand with at least four cards in each major suit, asking the Notrump bidder to choose between the major suits at the cheapest level by either passing the 2 bid or correcting to 2. The name "Crawling Stayman" comes from the fact that the bidding "crawls" at the slowest possible pace: (pass) – 1NT – (pass) – 2; (pass) – 2 – (pass) – 2; (pass) – 2; (pass) – pass – (pass).

Alternatively, responder's 2 and 2 bids after the 2 rebid can be weak sign-offs. This allows responder to effectively bid hands which are 5-4 in the majors, by looking first for a 4-4 fit and, if none is found, signing off in his 5 card suit.

"Garbage Stayman" is even more useful opposite a weak NT opening (12-14) as it occurs more frequently and can mitigate very expensive penalties if responder is weak. It is in frequent use in Acol.

"Garbage Stayman" and "Crawling Stayman" bids over a 2NT bid work the same way, but occur at the "three" level.

Disadvantage is that it tells the opponents the opener’s distribution.

Don’t apply 2NT as showing both majors. Instead use 2.

If Jacoby transfers are not played, there are two approaches to resolve the situation when responder has a 5-card major but only invitational values. In one, more common, referred to as "non-forcing Stayman", in the sequence:
responder's simple rebid of a major suit is invitational, showing 8-9 points and a 5-card spade suit. In the "forcing Stayman" variant, the bid is one-round forcing.

In the original Precision Club system, forcing and non-forcing Stayman are differentiated in the start: 2 by responder shows only invitational values (and the continuation is the same as in basic Stayman), while 2 is forcing to game (responder bids 2NT without majors).

This allows responder to find exact shape of 1NT opener. Developed for use with weak 1 NT opening. Relay bids over opener's rebids of 2, 2, 2, 2NT, 3 allow shape to be defined further if attempting to find 5-3 major fits. Advantages are responder's shape, which may be any distribution, is undisclosed, and responder is able to locate suit shortage holdings not suitable for no trumps. 

1NT – 2♣

Developed to be used in combination with following other responses to 1NT: 2, 2 Jacoby transfers to majors; 2 range finder/transfer to minors (opener's rebids: 2NT 12-13 HCP, 3 14 HCP. Responder passes or corrects to 3 or 3 sign off if weak. After opener's 3 rebid responder bids 3 to show 4 hearts or 3 to show 4 spades both game forcing. Responder's rebid of 3NT denies 4 card major); 2NT invitational hand with both 4 card majors (opener's rebids: no bid no 4 card major 12-13 HCP, 3 4 hearts 12-13 HCP, 3 4 spades 12-13 HCP, 3 4 hearts 14 HCP, 3 4 spades 14 HCP, 3NT 14 HCP no 4 card major).

Disadvantage is that it tells the opponents the opener’s distribution.

This allows responder to find exact shape of 1NT opener that may only contain a four-card major. Developed for use with weak 1 NT opening. Relay bids over opener's rebids of 2, 2, 2 allow shape to be defined further if attempting to find 5-3 major fits. Advantages are responder's shape, which may be any distribution, is undisclosed, and responder is able to locate suit shortage holdings not suitable for notrumps. May be also used as a damage control bid, and for both invitational, and game forcing hands.

1NT – 2♣

1NT – 3♣ weak sign off.

Opener's rebids of 2, 2, 2 may all be passed if responder is weak.

Developed to be used in combination with following other responses to 1NT: 2, 2 Jacoby transfers to majors; 2 five spades four hearts 10-11 HCP; 2NT invitational hand with 5,5 minors 10-11 HCP.

This allows responder to find exact shape of 1NT opener that may contain a 5 card major. Developed for use with weak 1NT opening. Relay bids over opener's rebids of 2D, 2H, 2S allow shape to be defined further if attempting to find 5-3 major fits. Advantages are responder's shape, which may be any distribution, is undisclosed, and responder is able to locate suit shortage holdings not suitable for no trumps. May be also used as a damage control bid, and for both invitational, and game forcing hands.

1NT – 2C

Opener's rebids of 2D, 2H, 2S may all be passed if responder is weak.

Developed to be used in combination with following other responses to 1NT: 2D, 2H Jacoby transfers to majors; 2S range finder/transfer C; 2NT invitational hand with 5,5 minors 10-11 HCP.

This allows responder to check for 5-3 major fits where it is possible that opener's 1NT or 2NT might include a five card major. As described by Australian Ron Klinger, it can be played with a weak or strong 1NT.

1NT - 2

1NT - 2; 2/2NT

After a transfer, accept it with any 4333, bid 3NT with only two trumps, otherwise bid 4M.

1NT - 2; 2/2NT - 3 = Stayman

1NT - 2; 2/2NT - 3, 3

An alternative, simpler version of 5 card Stayman is:

1NT - 2

This structure permits use by weak hands with 5+ diamonds and 2+ cards in each major.

After 1NT - 2; 2

If responder has a five-card major, he begins with a transfer. After completion of the transfer, bidding the other major at the three level shows four cards in it and a game forcing hand, in line with the 1NT - 2, 2 structure above (1NT - 2, 2 - 2 = invitational 5-4).

Similarly after 2NT - 3; 3

A drawback of Five Card Major Stayman (particularly the simpler version) is that the weaker hand may become declarer in a 4-4 major fit.

Puppet Stayman is similar to Five Card Stayman. It is more complex but has the major advantage that the strong hand virtually always becomes declarer.

Initially developed by Neil Silverman and refined by Kit Woolsey and Steve Robinson in 1977-78, is a variation of the Stayman convention designed to find a 5-3 fit in a major, augmenting the search for a 4-4 major fit by standard Stayman. In 1977, Woolsey wrote that Puppet Stayman has several advantages over standard Stayman:

As in standard Stayman, Puppet Stayman begins with a 2 response to a 1NT opening and is at least game invitational; this asks opener to bid a 5-card major if he has one and otherwise to bid 2. Over a 2 response, rebids by responder are intended to disclose his distributional features in the majors as well as his strength. The original 1977 and 1978 revised rebids described by Woolsey are tabulated below: 
Opener and responder continue the bidding having a clearer understanding of each other's distributional features and are better positioned to select the ultimate and level of the contract.

Many variations to the Puppet Stayman bidding structure have been devised since Woolsey's 1978 summary; partnership review and agreement on the preferred modern treatment is required.

Some no longer advocate use of Puppet Stayman over a 1NT opening preferring to use the concept exclusively over a 2NT opening and reserving other Stayman variations and conventions such Jacoby Transfers and Smolen Transfers in search of major-suit fits after a 1NT opening.

Puppet Stayman is more commonly used after a 2NT opening than after a 1NT opening. Responses to a 2NT opening or very strong 2NT rebid (20-22 or 23-24):

Responder bids 3 seeking information about opener's major suit holding. Opener replies:

By this means all 5-3 and 4-4 major suit fits can be found.

An alternative pattern frees up the 2NT-3 sequence as a slam try in the minors. To allow 3-5 spade fits to be found when responder holds 5 spades and 4 hearts, some of the responses change:

2 Checkback Stayman (or simply Checkback) is used after a 1NT rebid by opener rather than a 1NT opening. It is used to "check back" if opener has major suit support, saying nothing additional about the club suit. It can find 3-5 fits, 4-4 fits (in Standard American) and 5-3 fits (in Acol), and also shows whether opener was maximum or minimum strength for his notrump bid. In five-card major systems, bidding Checkback implies that the responder has five cards in his major, and may have four in the other.

1m – 1M; 1NT – 2

The 2 is "Checkback Stayman". Responses by opener shows the following:

Partnership agreement is required on how to handle the case of holding four of the other major and three of partner's suit. One could agree to bid up the line, or support partner's suit first. If partner cannot support your first suit, he will invite with 2NT or bid game with 3NT and you will then correct to your other suit.

In Acol, if the opening bid was a major, opener can rebid his major after a Checkback inquiry to show that it has five cards rather than four and find 5-3 fits. Moreover, 1M – 2m; 2NT – 3 can also be used as Checkback Stayman. It is useful also to include an indication of range, particularly if opener's 2NT rebid is forcing to game and shows a wide points range (15-19). This is achieved by using 3 for minimum hands and 3/3/3NT for maximum hands, or vice versa. After 3, responder can still bid 3/3 to look for a 5-3 fit.

New Minor Forcing is an alternative to Checkback Stayman where either 2 or 2 can be used as the checkback bid. It can be used by responder with invitational values or better to find three-card support for his major or to find a 4-4 heart fit if holding five spades and four hearts); it also allows a return to the minor to play.



</doc>
<doc id="29483" url="https://en.wikipedia.org/wiki?curid=29483" title="Saks Fifth Avenue">
Saks Fifth Avenue

Saks Fifth Avenue, originally A. Saks & Co., is an American chain of luxury department stores, with its origins in Andrew Saks' A. Saks & Co. store opened in Washington, D.C.'s F Street shopping district in 1867. Saks' flagship store is located on Fifth Avenue in Midtown Manhattan, New York City. The 1924 New York store on Fifth Avenue lent its street name to the chain which would be known by what was originally the moniker of its flagship store, Saks Fifth Avenue.

Since 2013, Saks Fifth Avenue has been owned by the Hudson's Bay Company (HBC), along with HBC's namesake Canadian department stores. Spinoff Saks Off 5th, originally a clearance store for Saks Fifth Avenue, is now a large off-price retailer in its own right managed independently from Saks Fifth Avenue under HBC. 

Andrew Saks was born to a German Jewish family, in Baltimore. He worked as a peddler and paper boy before moving to Washington, D.C. where at the age of only 20, and in the still-chaotic and tough economic times of 1867, only two years after the United States prevailed in the American Civil War, he established a men's clothing store with his brother Isadore A. Saks & Co. occupied a storefront in the Avenue House Hotel building at 517 (300-308) 7th Street, N.W., in what is still Washington's downtown shopping district. Saks offered his goods at one price only, no bargaining, and offered refunds on merchandise returns, neither of which were the more common practice at that place and time. Saks was also known for its "forceful and interesting, but strictly truthful" newspaper advertising, according to the Washington "Evening Star", including a two-page spread, large for that time, in that newspaper on April 4, 1898. Saks annexed the store next door, and in 1887 started building a large new store on the site of the old Avenue Hotel Building at 7th and Market Space (now United States Navy Memorial Plaza).

In 1896 Saks opened a branch store in Indianapolis in Ingall's Block, which was a success. then they expanded to Norfolk, Virginia, and by 1897 had six stores: Washington, D.C., Richmond, Virginia, Norfolk, Virginia, Indianapolis, and two stores as well as a clothing factory in New York City. Saks called itself "Washington's Wonderful Store".

Saks opened a very large store in 1902 in New York City on 34th and Broadway. Andrew Saks ran the New York store as a family affair with his brother Isadore, and his sons Horace and William. Andrew Saks died in 1912. 

In 1923 Saks & Co. merged with Gimbel Brothers, Inc., which was owned by a cousin of Horace Saks, Bernard Gimbel, operating as a separate autonomous subsidiary. On September 15, 1924, Horace Saks and Bernard Gimbel opened Saks Fifth Avenue in New York City, with a full-block avenue frontage south of St. Patrick's Cathedral, facing what would become Rockefeller Center. The architects were Starrett & van Vleck, who developed a reticent, genteel Anglophile classicizing facade similar to their Gimbels Department Store in Pittsburgh (1914).

When Bernard's brother, Adam Gimbel, became president of Saks Fifth Avenue in 1926 after Horace Saks's sudden passing, the company expanded, opening seasonal resort branches in Palm Beach, Florida, and Southampton, New York, in 1928. The first full-line year-round Saks store opened in Chicago, in 1929, followed by another resort store in Miami Beach, Florida. In 1938, Saks expanded to the West Coast, opening in Beverly Hills, California. By the end of the 1930s, Saks Fifth Avenue had a total of 10 stores, including resort locations such as Sun Valley, Idaho, Mount Stowe, and Newport, Rhode Island. More full-line stores followed with Detroit, Michigan, in 1940 and Pittsburgh, Pennsylvania, in 1949. In Downtown Pittsburgh, the company moved to its own freestanding location approximately one block from its former home on the fourth floor in the downtown Gimbel's flagship. The San Francisco location opened in 1952, competing locally with I. Magnin. BATUS Inc. acquired Gimbel Bros., Inc. and its Saks Fifth Avenue subsidiary in 1973 as part of its diversification strategy. More expansion followed from the 1960s through the 1990s including the Midwest, and the South, particularly in Texas. In 1990, BATUS sold Saks to Investcorp S.A., which took Saks public in 1996 as Saks Holdings, Inc.

In 1990, the company launched "Saks Off 5th", an outlet store offshoot of the main brand, with 107 stores worldwide by 2016.

In 1998, Proffitt's, Inc. the parent company of Proffitt's and other department stores, acquired Saks Holdings Inc. Upon completing the acquisition, Proffitt's, Inc. changed its name to Saks, Inc.

Since 2000 Saks has opened international locations in Saudi Arabia, United Arab Emirates, Bahrain, Kazakhstan, Canada, and Mexico City.

In August 2007, the United States Postal Service began an experimental program selling the plus zip code extension to businesses. The first company to do so was Saks Fifth Avenue, which received the zip code of 10022-7463 ("SHOE") for the eighth-floor shoe department in its flagship Fifth Avenue store.

During the 2007–2009 recession, Saks Fifth Avenue had to close some stores and to cut prices and profit margins, thus according to Reuters "training shoppers to expect discounts. It took three years before it could start selling at closer to full price". In the following years, the company closed stores in locations including Orange County (2010), Denver (2011), Pittsburgh (2012), Highland Park, Illinois (2012/13) and in June 2013 its last Dallas store to implement the "strategy of employing our resources in our most productive locations".
As of 2013, the New York flagship store, whose real estate value was estimated between $800 million and over $1 billion at the time, generated around 20% of Saks' annual sales at $620 million, with other stores being less profitable according to analysts.

On July 29, 2013, the Hudson's Bay Company (HBC), the oldest commercial corporation in North America and owner of the competing chain Lord & Taylor, announced it would acquire Saks Fifth Avenue's parent company for US$2.9 billion. Plans called for up to seven Saks Fifth Avenues to open in major Canadian markets. In January 2014, HBC announced the first Saks store in Canada would occupy in its flagship Queen Street building in downtown Toronto, connected to the Toronto Eaton Centre via sky bridge. The store opened in February 2016 with a second Toronto area location in the Sherway Gardens shopping center opening in spring 2016. On February 22, 2018, Saks Fifth Avenue opened its third Canadian store in Calgary, Alberta.

In 2015 Saks began a $250 million, three-year restoration of its Fifth Avenue flagship store. In October 2015, Saks announced a new location in Greenwich, Connecticut. In autumn 2015, Saks announced it would replace its existing store at the Houston Galleria with a new store.

On March 17, 2020, Saks temporarily closed their doors in response to the coronavirus pandemic.

In 2005, vendors filed against Saks alleging unlawful chargebacks. The U.S. Securities and Exchange Commission (SEC) investigated the complaint for years and, according to the "New York Times", "exposed a tangle of illicit tactics that let Saks... keep money it owed to clothing makers", inflating Saks' yearly earnings up to 43% and abusively collecting around $30 million from suppliers over seven years. Saks settled with the SEC in 2007, after firing three or more executives involved in the fraudulent activities.

In 2014, Saks fired transgender employee Leyth Jamal after she was allegedly "belittled by coworkers, forced to use the men's room and repeatedly referred to by male pronouns (he and him)". After Jamal submitted a lawsuit for unfair dismissal, the company stated in a motion to dismiss that "it is well settled that transsexuals are not protected by Title VII of the Civil Rights Act of 1964." In a court filing, the United States Department of Justice rebuked Saks' argument, stating that "discrimination against an individual based on gender identity is discrimination because of sex." The Human Rights Campaign removed the company from its list of "allies" during the controversy. The lawsuit was later settled amicably, with undisclosed terms.

In 2017, following the events of Hurricane Maria in Puerto Rico, Saks's San Juan store located in Mall of San Juan suffered major damages along with its neighboring anchor store Nordstrom. Taubman Centers, the company who owns the mall, filed a lawsuit against Saks for failing to provide an estimated reopening date and failing to restore damages after the hurricane due to a binding contract. Although Nordstrom reopened on November 9, 2018, on October 30, 2018, Saks Fifth Avenue announced that it would officially vacate The Mall Of San Juan.

Saks-34th Street was a fashion-focused middle market department store that was spun off from Saks & Company when that upscale retailer moved to a new store on New York's Fifth Avenue, a location that Saks Fifth Avenue maintains to this day. Saks-34th Street became a part of the New York division of Gimbels, and a sky bridge across 33rd Street connected the second floors of both flagship buildings. In the 1947 movie "Miracle on 34th Street" the facade of Saks-34th Street is shown in a scene that focuses on the Gimbel's flagship store. Branch locations were opened around the greater New York area. After Gimbels decided to close the division, the first floor of the building was used as a Christmas season annex for Gimbel's before being sold to the E. J. Korvettes chain. After the demise of the Korvette's chain the building was remodeled into the Herald Center. Today the primary tenant is H&M.

Saks Fifth Avenue at 9600 Wilshire Boulevard is a department store in Beverly Hills, California. It is part of the Saks Fifth Avenue company. It was designed by the architectural firm Parkinson and Parkinson, with interiors by Paul R. Williams. The store opened in 1938. The exterior of the building was designed by the Parkinsons, with the interior completed by Williams in the Hollywood Regency style. David Gebhard and Robert Winter, writing in "Los Angeles: An Architectural Guide" described the building as having "enough curved surface to suggest that the thirties Streamline Moderne could be elegant". The store was expanded and redesigned by Williams in 1940 and 1948. The store was immediately successful upon opening and it would subsequently expand to almost and employ 500 people.

Williams's designs for the store marked a departure from traditional department stores by reducing the emphasis on commerciality that foresaw the rise of boutique stores in the 1980s and 1990s. Only a few examples of merchandise were displayed in hidden recesses. The President of Saks Fifth Avenue, Adam Gimbel, said in an interview with the "Los Angeles Times" that "Each room attempts to create a mood which is in keeping with the merchandise sold there. For example, a Pompeian room done in cool green with appropriate frieze is used for beach and swimming pool costumes and a French provincial room houses informal sports and country clothes The accessories are carried in an oval room done in a Regency spirit". The individual shipping areas of the store were semi-enclosed which prevented distraction for customers. Williams created an interior reminiscences of his designs for luxurious private residences, with rooms lit by indirect lamps and footlights focused on the clothes. New departments for furs, corsets, gifts and debutante dresses were added in the 1940 expansion. The Terrace Restaurant, a rooftop restaurant run by Perino's, served customers for several years. It was expanded in the 1940s renovations to provide cover during inclement weather.

The store featured in the 2005 film "Shopgirl". The story had originally been set in Neiman Marcus but Saks Fifth Avenue lobbied the film makers to portray their store instead.



</doc>
<doc id="29484" url="https://en.wikipedia.org/wiki?curid=29484" title="Seabee">
Seabee

United States Naval Construction Battalions, better known as the Navy Seabees, form the U.S. Naval Construction Force (NCF). The Seabee nickname is a heterograph of the first letters "C B" from the words Construction Battalion. Depending upon how the word is used "Seabee" can refer to one of three things: all enlisted personnel in the USN's occupational field 7 (OF-7), all officers and enlisted assigned to the Naval Construction Force (NCF), or Construction Battalions. Seabees serve outside the NCF as well. During WWII they served in both the Naval Combat Demolition Units and the Underwater Demolition Teams (UDTs). In addition, they served as elements of Cubs, Lions, Acorns and the United States Marine Corps. 
They also provided the manpower for the top secret CWS Flame Tank Group. Today they have many special task assignments starting with Camp David and the Naval Support Unit at the Department of State. Seabees serve under both Commanders of the Naval Surface Forces Atlantic/Pacific fleets as well as on many base Public Works and USN diving commands.
Naval Construction Battalions were conceived as a replacement for civilian construction companies on contract to the Navy after the U.S. was attacked at Pearl Harbor. At that time civilian contractors had roughly 70,000 men working on U.S. bases overseas. International law made it illegal for civilian workers to resist an attack. To do so would classify them as guerrillas and could lead to summary execution. That is exactly what happened when the Japanese invaded Wake Island and would serve as the backstory to the WWII movie "The Fighting Seabees".

Adm. Moreell's concept model CB was a USMC trained battalion of construction tradesmen: A military equivalent of those civilian companies, capable of any type of construction, anywhere needed, under any conditions or circumstances. It was realized that CBs were flexible, adaptable and could be utilized in every theater of operations. The use of USMC organization allowed for smooth co-ordination, integration or interface between NCF and Marine Corps elements. Additionally, CBs could be deployed individually or in multiples as the project scope and scale dictated. What distinguishes Seabees from Combat Engineers are the skill sets. Combat Engineering is but a sub-set in the Seabee toolbox. They have a storied legacy of creative field ingenuity, stretching from Normandy and Okinawa to Iraq and Afghanistan. Adm. Ernest King wrote to the Seabees on their second anniversary, "Your ingenuity and fortitude have become a legend in the naval service." Seabees believe that anything they are tasked with, they "Can Do". They were unique at conception and remain unchanged from Adm. Moreell's model today. In the October 1944 issue of Flying, the Seabees are described as "a phenomenon of World War II". Since their creation, all Seabee advanced military training has been under USMC instruction. Even so, they always bring their toolbox. One of those tools is the ingenuity Admiral King referenced. They gained fame for their application of it during WWII. The UDTs and flamethrowing tanks are declassified top secret examples. Postwar they followed with more of the same for the CIA and State Department. Together with their USMC training and ability to appropriate anything, they provide the Navy an unconventional asset found nowhere else in the U.S military.

CB Conceptual Formation

Pre-WWII, the concept pioneered in 1917 by the Twelfth Regiment had not been forgotten by the Navy's Civil Engineers. Planning at Bureau of Yards and Docks (BuDocks) began providing for "Navy Construction Battalions" (CB) in contingency war plans. In 1934, Capt. Carl Carlson's version of the CB was approved by Chief of Naval Operations

In 1935, RADM. Norman Smith, head of BuDocks, selected Captain Walter Allen, War Plans Officer, to represent BuDocks on the War Plans Board. Capt. Allen presented the bureau's Construction Battalion concept and the Board included it in the Rainbow war plans. The Seabees named their first training center for Capt. Allen.

The proposal was criticized because the Seabees would have a dual command; military control administrated by fleet line Officers while construction operations would be administrated by Civil Engineer Corps officers. Another issue was no provision for the military organization or military training necessary to provide unit structure, discipline, and esprit de corps. In December 1937, RADM. Ben Moreell became BuDocks Chief and the lead proponent of the CB proposal.

In 1941 civilian contractors were working on numerous projects for the Navy and BuDocks decided to improve project oversight by creating "Headquarters Construction Companies". These companies would have 2 officers and 99 enlisted, but would do no actual construction. On October 31 1941, RADM. Chester Nimitz, Chief of the Bureau of Navigation, authorized the formation of the 1st Headquarters Construction Company. Recruitment began in November and boot training began December 7 at Naval Station Newport, Rhode Island. By December 16, four additional companies had been authorized, but Pearl Harbor had changed all the plans.

On December 28, 1941, RADM Moreell requested authority to commission three Naval Construction Battalions. His request was approved on January 5, 1942 by Admiral Nimitz. The 1st HQ Construction Company was used to commission the 1st Naval Construction Detachment, which was assigned to Operation Bobcat. They were sent to Bora Bora and are known in Seabee history as "Bobcats". 

Concurrently, the other four requested HQ Construction Companies had been approved. BuDocks took Companies 2 & 3 to form the 1st Naval Construction Battalion at Charleston, South Carolina. HQ Companies 4 & 5 were used for the 2nd CB. All four deployed as independent units. CBs 3, 4, & 5 were all deployed similarly. CB 6 was the first battalion to deploy full complement to the same deployment site.

Before all this could happen, BuDocks had to address the dual command issue. Naval regs stated unit command was strictly limited to line officers. BuDocks deemed it essential that CBs be commanded by CEC officers trained in construction. The Bureau of Naval Personnel (BuPers) strongly opposed this. Adm. Moreell took the issue directly to the Secretary of the Navy, Frank Knox. On March 19, 1942, Knox gave the Civil Engineer Corps complete command of all Naval Construction units. Almost 11,400 would become CEC during WWII with 7,960 doing CB service. Two weeks prior, on March 5th all construction battalion personnel were officially named "Seabees".

The first volunteers were construction tradesmen who were given advanced rank for their trade skills. This would result in them being the highest-paid group in uniform. To recruit these men, age and physical standards were waived up to age 50. Until November 1942 the average recruit age was 37, even so all received the same physical training. In December, FDR ordered the Selective Service System to provide CB recruits. Enlistees could request CB service with a written statement certifying that they were trade qualified. This lasted until October 1943 when voluntary enlistment in the Seabees ceased until December 1944. By war's end, 258,872 officers and enlisted had served in the Seabees. They never reached the Navy's authorized quota of 321,056.

In 1942 initial CB boot was Camp Allen,VA., which moved to Camp Bradford, which moved to Camp Peary and finally moved to Camp Endicott, Rhode Island. CBs 1-5 were sent directly overseas for urgent projects. CBs that followed were sent to Advance Base Depots (ABDs) for deployment. Camp Rousseau at Port Hueneme became operational first and was the ABD to the Pacific. The Davisville ABD became operational in June with NTC Camp Endicott commissioned that August. Other CB Camps were Camp Parks, Livermore, Ca., and Camp Lee-Stephenson, Quoddy Village, Eastport, Maine and Camp Holliday, Gulfport, Ms.
CBs sent to the Pacific were attached to one of the four Amphibious Corps: I, III, and V were USMC. The VII Amphibious Force was under General Douglas MacArthur, Supereme Commander.

Advance Bases

The Office of Naval Operations created a code identifying Advance Base (AB) construction as a numbered metaphor for the size/type of base. That code was also used to identify the "unit" that would be the administration for that base. These were Lion, Cub, Oak and Acorn with a Lion being a main Fleet Base (numbered 1–6). Cubs were Secondary Fleet Bases 1/4 the size of a Lion (numbered 1–12). Oak and Acorn were the names given air installations, new or captured (airfield or airstrip). Cubs quickly gained status. The speed with which the Seabees could make one operational led the Marines to consider them a tactical component. Camp Bedilion shared a common fence-line with Camp Rousseau at Port Hueneme and was home to the Acorn Assembly and Training Detachment (AATD) As the war progressed, BuDocks realized that logistics required that Advance Base Construction Depots (ABCDs) be built and CBs built seven. When the code was first created, BuDocks foresaw two CBs constructing a Lion. By 1944 an entire Regiment was being used. The invasion of Okinawa took four Construction Brigades of 55,000 men. The Seabees built the infrastructure needed to take the war to Japan. By war's end CBs had, served on six continents, constructed over 300 bases on as many islands. They built everything: airfields, airstrips, piers, wharves, breakwaters, PT & seaplane bases, bridges, roads, com-centers, fuel farms, hospitals, barracks and anything else.

Atlantic
In the Atlantic Seabees biggest job was the preparations for the Normandy landing. Months later CBMUs 627, 628, and 629 were tasked to facilitate the crossing of the Rhine. For CBMU 629 it was front-line work. 

USMC historian Gordon L. Rottman wrote "that one of the biggest contributions the Navy made to the Marine Corps during WWII was the creation of the Seabees". As part of that contribution the Corps would be influential upon the CB organization and its history. After the experience of Guadalcanal the Department of War decided that the Marines and Seabees would make all subsequent landings together. The Order of battle would show the Seabees as being attached to the Marine Corps. That arrangement lead to numerous good-natured claims by the Seabees that they had landed first and signs left on the beach saying "What took you so long?" The Seabees in the UDTs made an effort of this. 

When the first three battalions were formed the Seabees did not have a fully functional base of their own. Upon leaving navy boot camp the first recruits were sent to National Youth Administration camps in Illinois, New Jersey, New York, and Virginia to receive military training from the Marine Corps. The Marine Corps listed CBs on their Table of organization: "D-Series Division" for 1942, "E-Series Division" for 1943, and "Amphibious Corps" for 1944/45.

When the Seabees were created the Marine Corps wanted one for each of the three Marine Divisions but were told no because of war priorities. That did not keep early Seabee units from having close contact with the Marine Corps
The 1st Naval Construction Detachment (Bobcats) together with and A Co CB 3 was transferred to the Marines and redesignated 3rd Battalion 22nd Marines. The Bobcats had deployed without receiving advanced military training. The 22nd Marines took care of that. The 4th Construction Detachment was attached to the 5th Marine Defense Battalion for two years.

By autumn, actual CBs, the 18th , 19th and 25th had been transferred to the Corps as combat engineers. Each was attached to a composite engineer regiment and redesignated as 3rd Bn of that Regiment:

There were numerous USMC/Seabee pairings. The first one in combat was the 6th CB with the 1st Marine Division. The 18th CB was sent as their relief from Fleet Marine Force depot Norfolk. Many more would follow. The 6th Special CB was tasked to the 4th Marines Advance Depot in the Russells. In November, the 14th CB was tasked to the 2nd Raider Bn on Guadalcanal. Earlier in June, the 24th CB was tasked to the 9th Marine Defense Bn on Rendova. The 33rd and 73rd Seabees had a detachment tasked to the 1st Pioneers as shore party for the 5th Marines on Peleliu. Also attached was the 17th Special CB colored. At Enogi Inlet on Munda, the 47th had a detachment support the 1st and 4th Marine Raiders. On Bougainville, the 3rd Marine Div. made the Commander of the 71st CB shore party commander. The 71st was supported by detachments from the 25th, 53rd, and the 75th CBs. At Cape Torokina the 75th had 100 men volunteer to support the assault of the 3rd Marines. Also at Bougainville, the 53rd provided shore parties to the 2nd Raiders on green beach and the 3rd Raiders on Puruata Island. The 121st was formed at the CB Training Center of MTC Camp Lejuene as 3rd Bn 20th Marines. They would be shore party to the 23rd Marines on Roi-Namur, Saipan, and Tinian.

In 1944 the Marine Engineer Regiments were inactivated. Even so, Marine Divisions still had a CB tasked to them. For Iwo Jima, the 133rd and 31st CBs were attached to the 4th and 5th Marine Divisions. The 133rd was tasked to the 23rd Marines as their shore party. The 31st CB was attached to the 5th Shore Party Regiment with their demolitionsmen attached to the 5th Marine Div. The 8th Marine Field Depot was the shore party command eschelon for Iwo Jima. They requested 26 heavy equipment operators from the 8th CB. Okinawa saw the 58th, 71st, 130th, and 145th CBs attached to the 6th, 2nd, and 1st Marine Divisions respectively.

From Iwo Jima the 5th Marine Div. returned to Camp Tarawa to have the 116th CB attached. When Japan fell the 116th CB was part of the occupation force. V-J day found thousands of Japanese troops still in China and the III Marine Amphibious Corps was sent there to get them home. The 33rd NCR was assigned to III Marine Amphib. Corps for this mission.

Seabee Battalions were also tasked individually to the four Amphibious Corps. The 19th CB started out with the I MAC prior to joining the 17th Marines. The 53rd CB was attached to I MAC as Naval Construction Battalion I M.A.C. When I MAC was redesignated III Amphibious Corps the battalion became an element of the 1st Provisional Marine Brigade. For Guam, III Amphibious Corps had the 2nd Special CB, 25th, and 53rd CBs. The CO 3/19 Marines (25 CB) was shore party commander for the 3rd Marines on beaches Red 1 and Red 2. The 3rd Marines would award 25's shore party 17 bronze stars. V Amphibious Corps (VAC) had the 23rd Special and 62nd CBs on Iwo Jima. On Tinian the 6th Construction Brigade was attached to V Amphibious Corps.

When the war ended the Seabees had a unique standing with the U.S. Marine Corps. Seabee historian William Bradford Huie wrote "that the two have a camaraderie unknown else-wheres in the U.S. military". Even though they are "Navy" the Seabees adopted USMC fatigues with a Seabee insignia in place of the EGA. At least nine WWII Seabee units incorporated USMC insignia into theirs. Admiral Moreell wrote that the Marines were the best fighting men in the Pacific. Even-so, a leatherneck had to serve 90 days with the Seabees to qualify to as a Junior Seabee.

see Notes

In early May 1943, a two-phase "Naval Demolition Project" was ordered by the Chief of Naval Operations "to meet a present and urgent requirement" for the invasion of Sicily. Phase-1 began at Amphibious Training Base (ATB) Solomons, Maryland with the creation of Operational Naval Demolition Unit # 1. Six Officers lead by Lt. Fred Wise CEC and eighteen enlisted reported from Camp Peary dynamiting and demolition school. Seabees called them "Demolitioneers".

Naval Combat Demolition Units (NCDUs) consisted of one junior CEC officer, five enlisted, and were numbered 1–216. After that first group had been trained Lt. Commander Draper Kauffman was selected to command the program. It had been set up in Camp Peary's "Area E"(explosives) at the dynamiting and demolition school. Between May and mid-July, the first six NCDU classes graduated at Camp Peary. From there the program moved to Fort Pierce where the first class began mid-July. Despite the move, Camp Peary remained Kauffman's primary recruit center. "He would go back to the dynamite school, assemble the (Seabees) in the auditorium and say, "I need volunteers for hazardous, prolonged and distant duty." Fort Pierce had two Seabee units assigned, CBD 1011 and CBMU 570. They were tasked with the construction and maintenance of obstacles needed for demolition training.

Thirty four NCDUs were assigned to the Invasion of Normandy. When the first 10 units arrived in England they had no commander. So Lt. Smith(CEC) assumed the role and split them into 3 groups to train with the 146th, 277th and 299th Combat Engineers. As more units arrived they were assigned to these groups plus had 5 army engineers attached to them. Group III(Lt. Smith) did research and development and is credited with developing the Hagensen Pack. NCDUs saw a 53 percent casualty rate at Normandy. Four from Utah beach later took part in Operation Dragoon.

With Europe invaded Admiral Turner requisitioned all available NCDUs from Fort Pierce for integration into the UDTs for the Pacific. That requisition order netted Admiral Turner 20 NCDUs that had received Presidential Unit Citations and another 11 that had gotten Navy Unit Commendations at Normandy. Before Normandy 30 NCDUs had been sent to the Pacific while three had gone to the Mediterranean. NCDUs 1–10 were staged at Turner City, Florida Island in the Solomons during January 1944. NCDU 1 went briefly to the Aleutians in 1943. NCDUs 4 and 5 were the first to see combat with the 4th Marines at Green island and Emirau Island. A few were temporarily attached to UDTs. Later NCDUs 1–10 were combined to form Underwater Demolition Team Able. That team was disbanded. NCDUs 2 and 3, plus 19, 20, 21 and 24 were assigned to MacArthur's 7th Amphibious Force and were the only NCDUs remaining at the war's end. The other men from Team Able were assigned to numeric UDTs.

see Notes

Prior to Operation Galvanic and Tarawa, V Amphibious Corps had identified coral as an issue for future amphibious operations. RADM. Kelly Turner, commander V Amphibious Corps had ordered a review to get a grip on the problem. VAC found that the only people having any applicable experience with the material were men in the Naval Construction Battalions. Lt. Thomas C. Crist, of CB 10, was in Pearl Harbor from Canton Island where he had been in charge of clearing coral heads. His being in Pearl Harbor was pivotal in UDT history. While there he learned of the Adm. Turner's interest in coral blasting and met with him. The Admiral tasked Lt. Crist to develop a method for blasting coral under combat conditions and putting together a team to do it. Lt. Crist started by getting men from CB 10. By December 1, 1943 he had close to 30 officers and 150 enlisted at Waipio Amphibious Operating Base on Oahu.

In November the Navy had a hard lesson with coral and tides at Tarawa. It prompted Adm. Turner to request the creation of nine Underwater Demolition Teams to address those issues. Six teams for VAC in the Central Pacific while the other three would go to III Amphibious Corps in the South Pacific. Adm. Turner chose the term "underwater" to distinguish from the Fort Pierce program. UDTs 1 & 2 were formed from the 180 Seabees Lt Crist had staged. Seabees make up the majority of the men in teams 1-9, 13 and 15. How many Seabees were in UDT 10 is not cited in the records nor is anything stated for UDT 12. Seabees were roughly 20% of UDT 11.
UDT officers were mostly CEC. UDT 10 had 5 officers and 24 enlisted orginally trained as OSS Maritime Unit: Operational Swimmer Group II). but, the OSS was not allowed to operate in the Pacific Theater. Adm. Nimitz needed swimmers and approved their transfer from the OSS to his control. The MU men brought with the swimfins they had trained with and the Seabees made them a part of UDT attire as quickly as the Supply dept. could get them. In the Seabee dominated teams the next largest group of UDT volunteers came from the joint Army-Navy Scouts and Raiders school that was also in Fort Pierce. Additional volunteers came from the Navy's Bomb disposal School, Marine Corps and U.S. Fleet.

The first team commanders were Cmdr. E.D. Brewster (CEC) UDT 1 and Lt. Crist (CEC) UDT 2. Both Teams were "provisional" totaling the 180 men Lt Crist had put together. Seven different CBs made up UDT 2. They wore fatigues, life-vests and were expected to stay in boats like the NCDUs. However, at Kwajalein Fort Pierce protocol was changed. Adm.Turner ordered daylight recon, and Ensign Lewis F. Luehrs and Seabee Chief Bill Acheson wore swim trunks under their fatigues. They stripped down, spent 45 minutes in the water in broad daylight. Still wet and in their trunks they were taken directly to Adm. Turner to report. He concluded individual swimmers were the only way to get accurate intel on underwater obstacles, reporting as much to Adm. Nimitz. At Engebi Cmdr. Brewster was wounded and all the men with Ens. Luehrs wore trunks under their fatigues. The success of those UDT 1 Seabees not following Fort Pierce protocol rewrote the UDT mission model and training regimen. Ens. Luehrs and Chief Acheson were each awarded a Silver Star for their exploit while unintentionally creating the UDT "naked warrior" image. Diving masks were not common in 1944 and some had tried using goggles at Kwajalein. They were a rare item in Hawaii so Lt. Crist and CB Chief Howard Roeder had requested supply get them. A fortuitous observation by one of the men spotted a magazine advertisement for diving masks. A priority dispatch was made to the States that appropriated the store's entire stock.

Adm. Turner also requested the formation of a "Naval Combat Demolition Training & Experimental Base" at Kihei. It was approved, with the lessons of UDT 1 incorporated into the training, making it distinctly different from that at Fort Pierce. Lt. Crist was briefly the first training officer until when he was made Commander of UDT 3. When UDT 3 returned from Leyte in November 1944 the team became school instructors and Lt. Crist was again OIC of training. Under Lt. Crist the training course was changed with an emphasis on swimming and recon. Also covered were: night ops, weapons, bivouacking, small unit tactics, along with coral and lava blasting. The team instructed until April 1945 when it was sent to Fort Priece to instruct there. Lt. Crist was promoted to Lt. Cmdr and returned to Hawaii. Team 3 would train teams 12–22. UDT 14 is called the first "all fleet team" even though Seabees from Team Able were attached and the Commander and XO were both CEC (Ltjg. A.B. Onderdonk and Ltjg. C.E. Emery). UDT 15 was the last team formed of NCDUs. Four teams were sent to Iwo Jima. Three were sent to clear the shoreline for five days, D+2 - D+7. After July of 1944 new UDTs were only USN. In 1945 CBMU 570 was tasked to support UDT coldwater training at ATB Oceanside, CA.

On Guam team 8 requested permission to build a base. It was approved by AdComPhibsPac, but disapproved by the Island Command. Team 8 turned to the CBs on the island to appropriate everything they needed. The coral paving got placed the night before Admiral Nimitz made an inspection. The Admiral gave the base and teams 8 & 10 a glowing review.

By V-J day 34 teams had been formed. 
Teams 1–21 saw actual deployment with the Seabees providing over half of the men in those teams. The Navy did not publicize the existence of the UDTs until post-war and when they did they gave credit to Lt. Cmdr. Kauffman and the Seabees. During WWII the Navy did not have a rating for the UDTs nor did they have an insignia. Those men with the CB rating on their uniforms considered themselves Seabees that were doing underwater demolition. They did not call themselves "UDTs" or "Frogmen", but rather "Demolitioneers" reflecting where LtCdr Kauffman had recruited them from, the CB dynamiting and demolition school.

UDTs had to be of standard recruiting age, Seabees older could not volunteer. In preparation for the invasion of Japan and the cooler waters encircling it, the UDTs created a cold water training center. Mid-year 1945 men had to pass a stricter physical. Team 9 lost 70% of the team to this change.

see Notes

In February 1942 CNO Admiral Harold Rainsford Stark recommended African Americans for ratings in the construction trades. In April the Navy announced it would enlist African Americans in the Seabees. Even so, there were just two CBs that were "colored" units, the 34th and 80th. Both had white Southern officers and black enlisted. Both battalions experienced problems with that arrangement that led to the replacement of the officers. The men of the 34th went on a hunger strike which made national news. The Commander of the 80th had 19 enlisted dishonorably discharged for sedition. The NAACP and Thurgood Marshall got 14 of those reversed. In 1943 the Navy drew up a proposal to raise the number of colored CBs to 5 and require that all non-rated men in the next 24 CBs be colored. The proposal was approved, but not acted on.

The lack of stevedores in combat zones was a huge issue for the Navy. Authorization for the formation of cargo handling CBs or "Special CBs" happened mid-September 1942. By wars end 41 Special CBs had been commissioned of which 15 were "colored". They were the first fully integrated units in the U.S. Navy. V-J Day brought the decommissioning of all of them. The Special CBs were forerunners of today's Navy Cargo Handling Battalions of the Navy Expeditionary Logistics Support Group (United States). The arrival of 15 colored Special CBs in Pearl Harbor made segregation an issue for the Navy. For some time the men slept in tents, but the disparity of treatment was obvious even to the Navy. The 14th Naval District felt they deserved proper shelter with at least separate but equal barracks. Manana Barracks and Waiawa Gulch became the United State's largest colored military installation with over 4,000 Seabee stevedores housed there. It was the site of racial strife to the point that the camp was fenced in and placed under armed guard. The Seabees would be trucked back and forth to the docks in cattle trucks. Two naval supply depots were located at Waiawa Gulch.

The 17th Special(colored) CB at Peleliu 15–18 September 1944 is omitted from the USMC order of battle. On D-day at Peleliu, the 7th Marines were in a situation where they did not have enough men to man the lines and get the wounded to safety. Coming to their aid were the 2 companies of the 16th Marine Field Depot (colored) and the 17th Special CB. The Japanese mounted a counter-attack at 0200 hours on D-day night. By the time it was over, nearly the entire 17th had volunteered to carry ammunition to the front lines on the stretchers they brought the wounded back on. They volunteered to man the line where the wounded had been, man 37mm guns that had lost their crews and volunteered for anything the Marines needed. The 17th remained with the 7th Marines until the right flank had been secured on D plus 3. According to the Military History Encyclopedia on the Web, "were it not for the Black Marine shore party---the counterattack on the 7th Marines would not have been repulsed".

A Construction Battalion Detachment (CBD) was formed from "screening Camp Peary and the NCF for geologists, petroleum engineers, oil drillers, tool pushers, roustabouts and roughnecks" and later designated 1058. Many additional enlisted and officers were chosen for their arctic experience with CB 12 and CB 66. The selected men were assembled at Camp Lee Stephenson. Congress had earmarked $1,000,000 for Operation Pet 4 to determine if there was actually oil in NPR 4 (U.S. Navy Petroleum Reserve No. 4) in 1944. NPR-4 had been created and placed in the oil reserve in 1923. Today NPR-4 is the National Petroleum Reserve in Alaska. The detachment's mission was: 

In 1944 the base camp was constructed at Point Barrow. Four D-8s with twenty sleds of supplies were prepped for the 330-mile trek to Umiat once the tundra had frozen. After those supplies were delivered the Cats returned for the heavy well equipment. During the summer of 1945 a 1,816' wildcat was drilled and designated Seabee#1 before being shut down by the cold. The well site was near four known seeps at Umiat in the very south-east of NPR 4. The rock in the area was from the Upper Cretaceous and a stratum of it was named the "Seabee Formation". On the coast the Seabees drilled test holes at Cape Simpson and Point Barrow. Once the runways were completed additional supplies were flown in. In March 1946 civilians took over the project. Some had been members of CBD 1058 and had been hired immediately upon discharge for the same job they had performed for the Navy." The Navy drew upon the cold weather experience it gained from CBD 1058 and applied it in Operation Highjump and Operation Deep Freeze. – Today Seabee #1 is a USGS monitor well.

Land surveys

Twice the Seabees have been tasked with large scale land surveys. The first was done by CBD 1058 for a proposed NPR 4 pipeline route to Fairbanks. The Trans-Alaskan pipeline follows a portion of their survey from roughly the arctic circle to Fairbanks. The second would be done by a Seabee team from MCB 10. That group was sent to Vietnam in 1956 to survey and map that country's entire road network. This work would be heavily drawn upon during the Vietnam War.

see Notes

On V-J-Day CB 114 was in the Aleutians. In September 1945 the battalion sent a detachment to the USSR to build a Fleet Weather Central. It was located outside Petropavlovsk-Kamchatsky on the Kamchatka Peninsula and code named TAMA. The original agreement gave the Seabees 3 weeks to complete the base. Upon arrival the Russians told the Seabees they had 10 days and were amazed that the Seabees did it. It was one of two that Stalin agreed to. The other was near Khabarousk, Siberia in buildings provided by the Russians.

V-J-Day lead to Operation Beleaguer for the repatriation of the remnants of the Japanese Army left in China. Part of the 33rd CB Regiment was tasked: CBs 83, 96, 122 and 32nd Special. These units landed at Tsingtao and Tangku in November 1945 attached to the 6th Marine Division. CB 42 and A Co. 33rd Special landed at Shanghai attached to Naval Advance Base Unit 13. With the war over, the ongoing discharge men eligible left only enough for one CB and the two CB Specials. The men were consolidated in the 96th with the other units decommissioned. In December the 96th started airfields at Tsingtao and Chinwangtao in support of III Marine Amphibious Corps operations. On 20 May 1946 orders were issued for CB III Marine Amphibious Corps to inactivate 96 CB on 1 August. Prior, the 6th Marine Division was renamed the 3rd Marine Brigade for a short period. The 96th CB was transferred to the 4th Marines, 1st Marine Division and deactivated from them in August.

In early 1946 the 53rd NCB was deployed with Operation Crossroads for the nuclear testing at Bikini Atoll. The unit was assigned to Task Group 1.8 and designated TU 1.8.6. 53's project list included observation, instrument and communication towers, radio beacons, seismic huts, photo reference crosses, general base and recreational facilities, as well as dredging the lagoon. From March-May the battalion strength was 1006 including stevedores. The numbers were then drawn down until August 3rd when the battalion was decommissioned. The remaining men were transferred to CBD 1156 that was commissioned on Bikini. The TU 1.8.6 designation continued with them. The CBD remained at the atoll for nine days after the second nuclear test.

UDT 3 was designated TU 1.1.3 for the operation. On 27 April 1946, seven officers and 51 enlisted embarked the USS Begor (APD-127) at CBC Port Hueneme, for transit to Bikini. Their assignment was to retrieve water samples from ground zero of the Baker blast. In 1948, the displaced bikinians put in a request that the U.S. Navy blast a channel to the island Kili where they had been relocated. This was given to the Seabee detachment on Kwajelin. They requested UDT 3 assist.

In January 1947, CBs 104 and 105 were reactivated. The 121st CB was decommissioned in December and re-designated CBD 1504. The 30th NCR was home-ported on Guam composed of CBDs 1501-13 and NCB 103. In 1949, the 103rd was made a Mobile Construction Battalion (MCB) while CBs 104 and 105 were made Amphibious Construction Battalions(ACBs). From 1949 until 1968 CBs were designated MCBs. In June 1950 the Naval Construction Force numbered roughly 2,800.

The outbreak of the Korean War led to a call-up of 10,000 from the Seabee Reserve. Seabees landed at Inchon during the assault, installing causeways dealing with enormous tides and enemy fire. Their actions there and elsewheres underscored the necessity of having CBs. During that war the authorized size of a CB was 550 men. When the truce was declared there was no CB demobilization as there had been at the end of WWII.

During the Korea, the U.S. realized the need of an air station in the region. Cubi Point in the Philippines was selected. Civilian contractors were approached for bids. After seeing the Zambales Mountains and the maze of jungle, they claimed it could not be done. The Navy then turned to the Seabees. The first to arrive was CBD 1802 to do the surveying. MCB 3 arrived on 2 October 1951 to get the project going and was joined by MCB 5 in November. Over the next five years, MCBs 2, 7, 9, 11 and CBD 1803 all contributed to the effort. They leveled a mountain to make way for a nearly runway. NAS Cubi Point turned out to be one of the largest earth-moving projects in the world, equivalent to the construction of the Panama Canal. Seabees there moved of dry fill plus another 15 million that was hydraulic fill. The $100 million facility was commissioned on 25 July 1956, and comprised an air station and an adjacent pier that was capable of docking the Navy's largest carriers. Adjusted-for-inflation, today's price-tag for what the Seabees built at Cubi Point would be $906,871,323.53.

Seabee Teams 
The WWII precursor to Seabee teams was the PT Advance base Detachment of the 113th CB. Each man was cross-trained in at least three trades with some qualified as corpsmen and divers. The first Seabees to be actually be referred to as "Seabee Teams" were CBD 1802 and CBD 1803. They were followed by Detachments Able and Baker. Then someone in the U.S. State Department learned of these teams and had an idea for making "good use" of the Seabees in the Cold War. Teams could be sent as "U.S. Good Will Ambassadors" to third world nations as a means to combat the spread of Communism and promote "Good Will", a military version of the Peace Corps. These 13 man teams would construct schools, drill wells or build clinics creating a positive image or rapport for the U.S. They were utilized by the United States Agency for International Development and were in S.E. Asia by the mid 1950s. Then in the early sixties the U.S. Army Special Forces were being sent into rural areas of South Vietnam to develop a self-defense force to counter the Communist threat and making use of the Seabee teams at these same places made sense to the CIA. To start, twelve "Seabee teams, with Secret Clearances, were sent with the Army's Special Forces in the CIA funded Civilian Irregular Defense Group program (CIDG)" in the years 1963–1965. By 1965 the U.S. Army had enough engineers in theater to end Seabee involvement with Special Forces. At first teams were called Seabee Technical Assistance Teams (STAT) and were restricted to two in theater at a time. Teams after STAT 1104 were renamed Seabee Teams and by 1969 there were 17 in theater. As a military force Seabee Teams received many awards for heroism. Teams were sent to other nations as well. The Royal Thai government requested STATs in 1963 and ever since the Seabees have continued to deploy teams.

Construction Civic Action Details or CCAD
CCADs or "See-Kads" are larger civic action units of 20–25 Seabees with the same purpose as Seabee Teams. The CCAD designation is not found in the record prior to 2013.

Operation Highjump

In December 1946, 166 Seabees sailed from Port Hueneme on the USS Yancey and USS Merrick assigned to Operation Highjump. They were part of Admiral Richard E. Byrd's Antarctic expedition. The U.S. Navy was in charge with "Classified" orders "to do all it could to establish a basis for a (U.S.) land claim in Antarctica". The Navy sent the Seabees to do the job starting with the construction of Little America (exploration base) IV as well as a runway for aerial mapping flights. This Operation was vastly larger than IGY Operation Deep Freeze that followed.

Operation Deep Freeze

In 1955, Seabees were assigned to Operation Deep Freeze making Antarctica an annual deployment site. Their task was the construction and maintenance of scientific bases for the National Science Foundation. The first "wintering over" crew included 200 Seabees. They cleared an ice runway at Mcmurdo for 
the advance party of Deep Freeze II to fly to South Pole Station. MCB 1 was assigned for Deep Freeze II.

Antarctica added to the Seabee's list of accomplishments:

see Notes

Seabees deployed to Vietnam twice in the 1950s. First in June 1954 as elements of Operation Passage to Freedom and then two years later to map and survey the nation's roads. Seabee teams 501 and 502 arrived on 25 January 1963 and are regarded as the first Seabees of the Vietnam War. They were sent to Dam Pau and Tri Ton to build camps for the Special Forces. In 1964 ACB 1 was the first CB in the theatre. Beginning in 1965 Naval Construction Regiments deployed to the theater. Seabees supported the Marines at Khe Sanh and Chu Lai combat base in addition to building numerous aircraft-support facilities, roads, and bridges. They also worked with and taught construction skills to the Vietnamese. In June 1965, Construction Mechanic 3rd Class Marvin G. Shields of Seabee Team 1104 was at the Battle of Dong Xoai. He was posthumously awarded the Medal of Honor and is the only Seabee to be awarded the medal. Seabee Teams continued to be deployed throughout the Vietnam War and often engaging enemy forces alongside Marines and Army special forces. Teams typically built schools, clinics, or drilled wells. In 1966 Seabees repaired the airfield at Khe Sahn in four days, with 3,900 feet of 60-foot-wide aluminum matting. General Westmoreland "called it one of the most outstanding military engineering feats in Vietnam." MCB 4 had a det at Con Thien whose actions were a near repeat of Dong Xoai.

In 1968 the Marine Corps requested that the Navy make a name change to the CBs to reduce confusion. The Marines were using "MCB" for Marine Corps Base while the Navy was using "MCB" for Mobile Construction Battalions. The Navy added "Naval" to MCB creating the NMCBs that now exist. During that year the 30th Naval Construction Regiment had five battalions in the Da Nang area and two at Chu Lai. The 32nd NCR had three battalions tasked near Phu Bai and one at Dong Ha. In May 1968 two reserve battalions RNMCB 12 and 22 were activated, bring the total number of battalions in Vietnam to 21. Both ACBs were in theater as well as Construction Battalion Maintenance Units (CBMUs) 301 and 302. In 1968 NMCB 10 had an unusual "tasking" supporting the 101st Airborne. During 1969 the Seabees deployed topped out at 29,000, from there their draw-down began. The last battalion withdrew late 1971 with the last Seabee teams out a year later. When it was over they had sent 137 Seabee teams, built 15 CB camps, and deployed 22 battalions. CBMU 302 became the largest CB ever at over 1400 men and was homeported at Cam Rahn Bay. On 23 April 1975 it was announced that U.S. involvement in Vietnam was over.
that day saw NMCB 4 start construction of a temporary camp for Operation New Life on Guam. In seven days 2,000 squad tents were put up and 3,500 when done.

During Vietnam the Seabees had a few uniform variations. One was the stenciling of unit numbers across the back of the field jacket M-65. Another was the collar and cover devices for E4 E6 enlisted. The Navy authorized that the "crow" be replaced by the rating insignia of each trade. Nametags were another, they started out white with a multicolored seabee until 1968 when they followed USMC OD green pattern. The NAVCATs became the only Seabees to ever be authorized to wear a shoulder patch.

NAVCATs Naval Construction Action Teams

CBMU 302 had 23 NAVCATS total with 15 active at its peak. Teams were numbered 1-23. They were Vice Admiral Elmo Zumwalt's expansion of the Seabee Team concept. He submitted it in November 1968 to General Creighton Abrams commander of Military Assistance Command, Vietnam.

Agent Orange
Many Seabees were exposed to the defoliant herbicide while in Vietnam. NCBC Gulfport was the largest storage depot in the United States for agent orange. From there it was shipped to Vietnam. In 1968 the NCBC received 68,000 barrels to forward. Long term barrel storage began in 1969. That lasted until 1977. The site covered 30 acres and was still being cleaned up in 2013.
see Notes0

In 1960 a MCB 10 detachment built a Project Mercury telemetry and ground instrumentation station on Canton island. 

On 28 January 1969 a detachment of 50 men from Amphibious Construction Battalion 2 plus 17 Seabee divers began installation of the Tektite habitat in Great Lameshur Bay at Lameshur, U.S. Virgin Islands. The Tektite program was funded by NASA and was the first scientists-in-the-sea program sponsored by the U.S. government. The Seabees also constructed a 12-hut base camp at Viers that is used today as the Virgin Islands Environmental Resource Station. The project was a by product of the space race. It caused the U.S. Navy to realize the need for a permanent Underwater Construction capability that led to the formation the Seabee Underwater Construction Teams".

At present NASA is working on the Moon to Mars program. In 2015 ACB 1 was involved in moving the Orion's Boilerplate Test Article (BTA). ACB 1 was tasked in August of 2019 in a test recovery exercise of the Orion spacecraft. ACB 2 was put through the same task a year later in August 2020.



see Notes

Naval Intelligence: NAVFACs

The Navy built 22 Naval Facilities (NAVFACs) for its Sound Surveillance System (SOSUS) to track Soviet submarines. They were in service 1954–79 with Seabees staffing the Public works at each Facility. In the 1980s technology reduced the number of tracking stations to 11 with advent of the Integrated Underwater Surveillance System (IUSS). NAVFAC tracking facilities were finally undone by further advances in tech, the end of the Cold War and disclosures by John Walker to the Soviets.

The Seabees have also been tasked building Naval Communication facilities. One at Nea Makri Greece was built by MCB 6 in 1962 and later upgraded by NMCB 133. Naval Communications Station Sidi Yahya is another going back to WWII another is NavCommSta Guam. It started out on the island as the Joint Communications Agency (JCA) in 1945.

In 1964, at the height of the Cold War, Seabees were assigned to the State Department because listening devices were found in the Embassy of the United States in Moscow. Those initial Seabees were "Naval Mobile Construction Battalion FOUR, Detachment November". The U.S. had just constructed a new embassy in Warsaw. After what had been found in Moscow Seabees were dispatched and found many "bugs" there also. This led to the creation of the Naval Support Unit in 1966 as well as the decision to make it permanent two years later. That year William Darrah, a Seabee of the support unit, is credited with saving the U.S. Embassy in Prague, Czechoslovakia from a potentially disastrous fire. In 1986, "as a result of reciprocal expulsions ordered by Washington and Moscow" Seabees were sent to "Moscow and Leningrad to help keep the embassy and the consulate functioning".

The Support Unit has a limited number of special billets for select NCOs, E-5 and above. These Seabees are assigned to the Department of State and attached to Diplomatic Security. Those chosen can be assigned to the Regional Security Officer of a specific embassy or be part of a team traveling from one embassy to the next. Duties include the installation of alarm systems, CCTV cameras, electromagnetic locks, safes, vehicle barriers, and securing compounds. They can also assist with the security engineering in sweeping embassies (electronic counter-intelligence). They are tasked with new construction or renovations in security sensitive areas and supervise private contractors in non-sensitive areas. Due to Diplomatic protocol the Support Unit is required to wear civilian clothes most of the time they are on duty and receive a supplemental clothing allowance for this. The information regarding this assignment is very scant, but State Department records in 1985 indicate Department security had 800 employees, plus 1,200 Marines and 115 Seabees. That Seabee number is roughly the same today.

see Notes

As the Cold War wound down, new challenges and changes came for the Seabees starting with the increased incidence of terrorism. This was in addition to ongoing Seabee support missions for USN/USMC bases worldwide. Even though the Cold war had wound down Cold War Facilities still required support like the Polaris and Poseidon submarines at Holy Loch, Rota. In 1971, the Seabees began their huge peacetime construction project on Diego Garcia in the Indian Ocean. That project began in 1971 and was completed in 1987 at a cost of $200 million. With the extended construction timeline, it is difficult to inflation-adjust that cost into today's dollars. The complex accommodates the Navy's largest ships and cargo planes. The base served as a staging facility for Operations Desert Shield and Desert Storm. Seabee construction was responsible for the upgrade and expansion of Naval Air Station Sigonella, Sicily, making it a major base for the United States Sixth Fleet.

There were combat related assignments as well. In 1983, a truck bomb demolished the Marine's barracks in Beirut, Lebanon. From the Beirut International Airport Druze militia artillery harassed the Marines. After consultations, NMCB-1 in Rota sent in a 70-man AirDet to construct secure bunkers for the Marines. EO2 Kirt May became the first Seabee post-Vietnam to receive a Purple Heart while on the job.

CN Carmella Jones became the first female Seabee when she cross-rated to Equipment Operator during the summer of 1972.

Robert Stethem was executed by the Lebanese Shia militia Hezbollah when they hijacked TWA Flight 847 in 1985. SW2 Stethem was a Seabee diver in UCT 1. The was named for him. On 24 August 2010, on board USS "Stethem", SW2 Stethem was posthumously made an honorary Master Chief Constructionman (CUCM) by the Master Chief Petty Officer of the Navy and awarded the Prisoner of War Medal.

Camp David is officially known as Naval Support Facility Thurmont, because it is technically a military installation. The staffing is primarily provided by the CEC, Seabees, and Marines of the U.S. Navy and Marine Corps. "In the early 1950s, the first Seabee BUs, UTs and CEs took over routine maintenance and repairs of the base. Although there have been vast changes made at the Camp over the years, Seabees continue to staff base public works while keeping the grounds in an impeccable condition." Additional Naval rates were added to oversee base administrative functions. "Selectees undergo a single scope background investigation to determine if they are eligible for a Top Secret Sensitive Compartmentalized Information (TS/SCI) Yankee White (YW) clearance. All personnel assigned to duty in Presidential support activities are required to have a "Yankee White" clearance. The tour lasts 36 months." When the base has a larger construction project a regular Naval Construction Battalion will send a detachment to take care of the job. CBs 5 and 133 have drawn these assignments.

During the Persian Gulf War, more than 5,000 Seabees served in the Middle East. In August 1990 the First Marine Expeditionary Force (I MEF) was initially assigned NMCBs 4, 5, 7, and 40. The first Seabees in theater were a Det from ABC 1 that was soon joined by a Det from ACB 2. Shortly after them CBUs 411 and 415 arrived in Saudi Arabia. Mid September the Air-Dets for the four CBs arrived to build air fields for Marine Air Groups (MAG) 11, 13, 16, and 25 of the 3rd Marine Air Wing. NMCB 7 was the first battalion to arrive. Camps were constructed for both the 1st and 2nd Marine Divisions as well as Hq complexes for MEF I and II. Overall, in Saudi Arabia, Seabees built numerous camps and galleys. They laid millions of square feet of runways, aprons as well as over 200 helo zones. They built and maintained two 500-bed Fleet Hospitals near Al-Jubayl. The 3rd NCR was activated to provide a command echelon. NMCBs 24 and 74 were also deployed in support of the Marine Corps. A desert camp was constructed at Ras Al Mishab, near the Kuwaiti border named "Camp Nomad" which supported MAG 26.

Seabees were deployed in the invasion of Afghanistan in 2001 and Iraq in 2003. All active and reserve NMCBs and NCRs were sent to repair infrastructure in both countries. . NMCB 133 deployed to FOB Camp Rhino and Kandahar Airfield where a detention facility was constructed. One of the Seabees most visible tasks was the removal of statues of Saddam Hussein in Baghdad. In Afghanistan, the Seabees' main task was the construction of multiple forward operating bases.

Since 2002, Seabees have provided vital construction skills for civic action programs in the Philippines. Their efforts have had an effect in the southern Philippines, most notably near Abu Sayyaf's jungle training area. Seabees work with Army, Marines, and Air Force under Joint Special Operations Task Force-Philippines.

see Notes


At present, there are six active-duty Naval Mobile Construction Battalions (NMCBs) in the United States Navy, split between the Pacific Fleet and the Atlantic Fleet.

30th Naval Construction Regiment, Hq Guam Pacific Fleet Homeport for the Pacific Fleet Battalions is Naval Construction Battalion Center Port Hueneme Ca.

22nd Naval Construction Regiment is stationed at Naval Construction Battalion Center (Gulfport, Mississippi) the homeport to the Alantic fleet CBs.

NCF Reserve
From the 1960s through 1991, reserve battalions were designated as "Reserve Naval Mobile Construction Battalions" (RNMCBs). After 1991 "Reserve" was dropped with the integration of reserve units within the NCF making all battalions NMCBs

Detachment: A construction crew that is "detached" from the battalion's "main body" deployment site. The size is determined by the project scale and completion date.

Battalion: The battalion is the basic NCF unit with a HQ Company plus four Construction Companies: A, B, C, & D. CBs are organized to function as independent self sufficient units.

Regiment: Naval Construction Regiments (NCRs) purpose is to provide a higher eschelon command to three or four CBs operating on close proximity.

Division: 1st Naval Construction 
Division was in service from August 2002 until May 2013 when it was decommissioned.

Naval Construction Groups: In 2013, Seabee Readiness Groups (SRGs) were decommissioned, and re-organized as Naval Construction Groups 1 and 2. They are regimental-level command groups tasked with administrative and operational control of CBs, as well as conducting pre deployment for all assigned units. Naval Construction Group 2 (NCG-2) is based at CBC Gulfport, and Naval Construction Group 1 (NCG-1) is at CBC Port Hueneme.

Seabee Engineer Reconnaissance Team (SERTs)
SERTs are the Special operations capable element of the Seabees developed by the First Naval Construction Division (1st NCD) in Operation Iraqi Freedom. They are intended to provide engineering assessments in the field in support of the United States Marine Corps Reconnaissance Battalions. A team has two Civil Engineer Corps (CEC) officers and eight enlisted Seabees, augmented by additional personnel as needed. A team has three elements: liaison, security, and reconnaissance. The liaison (LNO) element has an officer and two communications specialists responsible for communicating the assessments and intelligence. Reconnaissance has the other officer, who is the Officer-in-Charge (OIC), a BU or SW cpo with bridge construction experience. The team has a corpsman or medically-trained member, the remainder are selected for being the most qualified in their trade. All must have the earned Seabee Warfare pin. Underwater Construction teams demonstrated the SERT concept should be retained in Iraq.

Amphibious Construction Battalions (PHIBCBs)
ACBs (or PHIBCB) were preceded by the pontoon assembly CBs formed during World War II. On 31 October 1950, MCBs 104 and 105 were re-designated ACB 1 and ACB 2, and assigned to Naval Beach Groups. ACBs report to surface TYCOMs. Additionally, in an ACB half the enlisted are a construction rate while the other half are fleet.

Construction Battalion Maintenance Units:

When during World War II these units had 1/4 the personnel of a CB. Their task was to assume maintenance of bases once CBs had completed construction. Today, CBMU's provide public works support at Naval Support Activities, Forward Operating Bases, and Fleet Hospital/Expeditionary Medical Facilities during wartime or contingency operations for a Marine Expeditionary Force (MEF), Marine Expeditionary Group (MEG), or NSW. They also provide disaster recovery support to Naval Regional Commanders in CONUS.
NAVFAC Engineering & Expeditionary Warfare Center Ocean Facilities Department supports the Fleet through the support it gives the Underwater Construction Teams". UCTs deploy worldwide to conduct underwater construction, inspection, repair, and demolition operations of ocean facilities, to include repair of battle damage. They maintain a capability to support a Fleet Marine Force amphibious assault, subsequent combat service support ashore, and self-defense for their camp and facilities under construction. UCT1 is home ported at Virginia Beach, Virginia, while UCT2 is at Port Hueneme, California.

Underwater Construction Team (UCT):
"NAVFAC Engineering & Expeditionary Warfare Center Ocean Facilities Department supports the Fleet through the support it gives the Underwater Construction Teams". UCTs deploy worldwide to conduct underwater construction, inspection, repair, and demolition operations of ocean facilities, to include repair of battle damage. They maintain a capability to support a Fleet Marine Force amphibious assault, subsequent combat service support ashore, and self-defense for their camp and facilities under construction. UCT1 is home ported at Virginia Beach, Virginia, while UCT2 is at Port Hueneme, California.

Public Works: U.S. Naval Bases: These units have CEC officers leading them and enlisted Seabees for the various crews. About one-third of new Seabees are assigned to Public Works Departments (PWD) at naval installations both within the United States and overseas. While stationed at a Public Works Department, a Seabee has the opportunity to get specialized training and extensive experience in one or more facets of their rating. Some bases have civilians that augment the Seabees, but the department is a military organization.

Combat Service Support Detachments (CSSD) have several hundred Seabees assigned in support of Naval Special Warfare (NSW) units based out of Coronado, CA, and Virginia Beach, VA. Seabees provide the field support for power generation/distribution, logistical movement, vehicle repair, construction/maintenance of encampments, water purification or facilities. Seabees assigned to support NSW receive extra training in first aid, small arms, driving, and specialized equipment. and are expected to qualify as Expeditionary Warfare Specialists. Seabees assigned to NSW are eligible to receive the following Naval Enlisted Classifications upon filling the requirements: 5306 – Naval Special Warfare (Combat Service Support) or 5307 – Naval Special Warfare (Combat Support). They also can apply for selection to support the NSW Development Group.

Trainees begin "A" School (trade school) upon completion of boot: 4 weeks classroom, 8 weeks hands-on. From "A" School, trainees most often report to a NMCB or ACB. There recruits go through four-weeks of Expeditionary Combat Skills (ECS) which is also required for those who report to a Navy Expeditionary Combat Command. ECS is basic training in: map reading, combat first aid, recon, and other combat-related skills. Half of each course is spent on basic marksmanship to qualify with a M16 rifle and the M9 service pistol. Those posted to Alfa Company of a NMCB may be assigned to a crew-served weapon: MK 19 40mm grenade launcher, the .50-caliber machine gun, or the M240 machine gun. Many reserve units still field the M60 machine gun. Seabees were last U.S. military to wear the U.S. Woodland camouflage uniform or the Desert Camouflage Uniform. They now have the Navy Working Uniform NWU Type III and use ALICE field gear. Some units, with the Marines, will use USMC-issue Improved Load Bearing Equipment (ILBE).

Current rates: The current ratings were adopted by the Navy in 1948.

The Seabee "constructionman" ranks of E-1 through E-3 are designated by sky-blue stripes on uniforms. The color was adopted in 1899 as a uniform trim color designating the Civil Engineer Corps, but was later given up. Its continued use is a bit of Naval Heritage in the NCF.

At paygrade E-8, the Builder, Steelworker, and Engineering Aide rates combine into a single rate: Senior Chief Constructionman (CUCS). At the E-9 paygrade they are referred to as a Master Chief Constructionman (CUCM). 

The remaining Seabee rates combine only at the E-9 paygrade: 
Diver : is a qualification that the various rates can obtain with three grades: Basic Underwater Construction Technician/ NEC 5932 (2nd Class Diver), Advanced Underwater Construction Technician/ NEC 5931 (1st Class Diver), and Master Underwater Construction Technician/ NEC 5933 (Master diver). Seabee divers are attached to five principal commands outside the NCF:

On 1 March 1942 the RADM Moreell recommended that an insignia be created to promote "esprit de corps" in the new CBs to ID their equipment as the Air corps did to ID squadrons. It was not intended for uniforms. Frank J. Iafrate, a civilian file clerk at Quonset Point Advance Naval Base, Davisville, Rhode Island, who created the original "Disney Style" Seabee. In early 1942 his design was sent to RADM Moreell who made a single request. That the Seabee being set inside a letter Q, for Quonset Point, be changed to a hawser rope and it would officially adopted.

The Seabees had a second Logo. It was of a shirtless constructionman holding a sledge hammer with a rifle strapped across his back standing upon the words "Construimus Batuimus USN". The figure was on a shield with a blue field across the top and vertical red and white stripes. A small CEC logo is left of the figure and a small anchor is to the right. This logo was incorporated into many CB Unit insignias.

During World War II, artists working for Disney Onsignia Department designed logos for about ten Seabee units including the: 60th NCB, 78th NCB 112th NCB, and the 133rd NCB. There are two Disney published Seabee logos that are not identified with any unit. 

The end of WWII brought the decommissioning of nearly all of the CBs. They had been in existence less than four years when this happened and the Navy had not created a Historical Branch or Archive for the NCF. So, there was no central archive for Seabee history. As time passed, first with Korea and then Vietnam, Construction Battalions were reactivated with the units having no idea what the WWII insignia had been so they made new ones.

The military qualification badge for the Seabees is known as the Seabee combat warfare specialist insignia (SCW). It was created in 1993 for both officers and enlisted personnel. Only members attached to a qualifying NCF unit are eligible for the SCW pin. The qualifying units include: NMCBs, ACBs, NCF Support Units (NCFSU), UCTs, and NCRs.
The Fleet Marine Force Insignia or Fleet Marine Force pin (FMF pin), are for issue to those USN officers and enlisted trained and qualified to support the U. S. Marine Corps. Those Seabees assigned with the Fleet Marine Force can earn the FMF pin. The FMF pin comes in three classes : enlisted, officer, and chaplain. For requirements, see: Fleet Marine Force Warfare Specialist (EFMFWS) Program per OPNAV Instruction 1414.4B.

The Peltier Award is given to the "best of type" active duty Naval Construction Battalions. It was instituted by Rear Admiral Eugene J. Peltier CEC and has been given annually since 1960. He is a former head of the Bureau of Yards and Docks (1959–1962).

see: Seabee (barge)

There were six "Seabee" ships built: the SS "Cape Mendocino" (T-AKR-5064), the and the . The other three of were operated by Lykes Brothers Steamship Company and were originally the SS Doctor Lykes, the SS Tillie Lykes, and the SS Almeria Lykes. The NCF primarily uses the Seabee barges . Barges with a 2.5' draft are loaded and floated to and from a mother container ship, facilitating loading and unloading of containerized cargo at sea. These ships have an elevator system for loading the barges out of the water at the stern onto the vessel. Loaded barges can then be moved toward the vessel's bow by means of a track to be stowed on one of three decks. Seabee barge carriers can store 38 barges, 12 each on the lower decks and 14 on the upper deck. The 38 barges can hold 160 containers. A barges measures 97'x35'. A barge carrier also has storage tanks of nearly 36000 m³(9,510,194 gal.) volume built in its sides and double hull, allowing it to be used also as a tanker. The ships were purchased by Military Sealift Command.

The U.S. Navy Seabee Museum is located outside the main gate of Naval Base Ventura County, Port Hueneme, Ca. In July 2011 the new facility opened with galleries, grand hall, theater, storage, and research areas.

The Seabee Heritage Center is the Atlantic Coast Annex of the Seabee Museum in Port Hueneme. It opened in 1995. Exhibits at the Gulfport Annex are provided by the Seabee Museum in Port Hueneme.

The Seabee Museum and Memorial Park in Davisville, Rhode Island was opened in the late 1990s. A Fighting Seabee Statue is located there.



Other U.S. military construction/engineering organizations:

WWII




Marine Corps, Seabees outside the NCF






NCDUs, Seabees outside the NCF






UDTs, Seabees outside the NCF






Seabee North Slope Oil Exploration 1944



Cold War: Korea – Seabee Teams


Cold War: Antarctica



Cold War: Vietnam





Cold War: CIA




Iraq Afghanistan



Seabee insignia



Naval Support Unit


SEABEE Barge Carriers





</doc>
<doc id="29485" url="https://en.wikipedia.org/wiki?curid=29485" title="Skyscraper">
Skyscraper

A skyscraper is a continuously habitable high-rise building that has over 40 floors and is taller than . Historically, the term first referred to buildings with 10 to 20 floors in the 1880s. The meaning shifted with advancing construction technology during the 20th century. Skyscrapers may host offices, hotels, residential spaces, and retail spaces.

One common feature of skyscrapers is having a steel framework that supports curtain walls. These curtain walls either bear on the framework below or are suspended from the framework above, rather than resting on load-bearing walls of conventional construction. Some early skyscrapers have a steel frame that enables the construction of load-bearing walls taller than of those made of reinforced concrete.

Modern skyscrapers' walls are not load-bearing, and most skyscrapers are characterised by large surface areas of windows made possible by steel frames and curtain walls. However, skyscrapers can have curtain walls that mimic conventional walls with a small surface area of windows. Modern skyscrapers often have a tubular structure, and are designed to act like a hollow cylinder to resist wind, seismic, and other lateral loads. To appear more slender, allow less wind exposure and transmit more daylight to the ground, many skyscrapers have a design with setbacks, which in some cases is also structurally required.

, only nine cities have more than 100 skyscrapers that are or taller: Hong Kong (355), Shenzhen (289), New York City (284), Dubai (201), Shanghai (163), Tokyo (158), Chongqing (127), Chicago (127), and Guangzhou (118).

The term "skyscraper" was first applied to buildings of steel framed construction of at least 10 storeys in the late 19th century, a result of public amazement at the tall buildings being built in major American cities like Chicago, New York City, Philadelphia, Detroit, and St. Louis. The first steel-frame skyscraper was the Home Insurance Building (originally 10 storeys with a height of ) in Chicago, Illinois in 1885. Some point to Philadelphia's 10-storey Jayne Building (1849–50) as a proto-skyscraper, or to New York's seven-floor Equitable Life Building (New York City), built in 1870, for its innovative use of a kind of skeletal frame, but such designation depends largely on what factors are chosen. Even the scholars making the argument find it to be purely academic.

The structural definition of the word "skyscraper" was refined later by architectural historians, based on engineering developments of the 1880s that had enabled construction of tall multi-storey buildings. This definition was based on the steel skeleton—as opposed to constructions of load-bearing masonry, which passed their practical limit in 1891 with Chicago's Monadnock Building.

Some structural engineers define a highrise as any vertical construction for which wind is a more significant load factor than earthquake or weight. Note that this criterion fits not only high-rises but some other tall structures, such as towers.

Different organizations from the United States and Europe define skyscrapers as buildings at least 150 metres in height or taller., with "supertall" skyscrapers for buildings higher than and "megatall" skyscrapers for those taller than .
The tallest building in ancient times was the Great Pyramid of Giza in ancient Egypt, built in the 26th century BC. It was not surpassed in height for thousands of years, the Lincoln Cathedral having exceeded it in 1311–1549, before its central spire collapsed. The latter in turn was not surpassed until the Washington Monument in 1884. However, being uninhabited, none of these structures actually comply with the modern definition of a skyscraper.

High-rise apartments flourished in classical antiquity. Ancient Roman insulae in imperial cities reached 10 and more storeys. Beginning with Augustus (r. 30 BC-14 AD), several emperors attempted to establish limits of 20–25 m for multi-storey buildings, but met with only limited success. Lower floors were typically occupied by shops or wealthy families, the upper rented to the lower classes. Surviving Oxyrhynchus Papyri indicate that seven-storey buildings existed in provincial towns such as in 3rd century AD Hermopolis in Roman Egypt.

The skylines of many important medieval cities had large numbers of high-rise urban towers, built by the wealthy for defense and status. The residential Towers of 12th century Bologna numbered between 80 and 100 at a time, the tallest of which is the high Asinelli Tower. A Florentine law of 1251 decreed that all urban buildings be immediately reduced to less than 26 m. Even medium-sized towns of the era are known to have proliferations of towers, such as the 72 up to 51 m height in San Gimignano.

The medieval Egyptian city of Fustat housed many high-rise residential buildings, which Al-Muqaddasi in the 10th century described as resembling minarets. Nasir Khusraw in the early 11th century described some of them rising up to 14 storeys, with roof gardens on the top floor complete with ox-drawn water wheels for irrigating them. Cairo in the 16th century had high-rise apartment buildings where the two lower floors were for commercial and storage purposes and the multiple storeys above them were rented out to tenants. An early example of a city consisting entirely of high-rise housing is the 16th-century city of Shibam in Yemen. Shibam was made up of over 500 tower houses, each one rising 5 to 11 storeys high, with each floor being an apartment occupied by a single family. The city was built in this way in order to protect it from Bedouin attacks. Shibam still has the tallest mudbrick buildings in the world, with many of them over high.

An early modern example of high-rise housing was in 17th-century Edinburgh, Scotland, where a defensive city wall defined the boundaries of the city. Due to the restricted land area available for development, the houses increased in height instead. Buildings of 11 storeys were common, and there are records of buildings as high as 14 storeys. Many of the stone-built structures can still be seen today in the old town of Edinburgh. The oldest iron framed building in the world, although only partially iron framed, is The Flaxmill (also locally known as the "Maltings"), in Shrewsbury, England. Built in 1797, it is seen as the "grandfather of skyscrapers", since its fireproof combination of cast iron columns and cast iron beams developed into the modern steel frame that made modern skyscrapers possible. In 2013 funding was confirmed to convert the derelict building into offices.

In 1857, Elisha Otis introduced the safety elevator, allowing convenient and safe passenger movement to upper floors, at the E.V. Haughwout Building in New York City. Otis later introduced the first commercial passenger elevators to the Equitable Life Building in 1870, considered by a portion of New Yorkers to be the first skyscraper. Another crucial development was the use of a steel frame instead of stone or brick, otherwise the walls on the lower floors on a tall building would be too thick to be practical. An early development in this area was Oriel Chambers in Liverpool, England. It was only five floors high. Further developments led to what many individuals and organizations consider the world's first skyscraper, the ten-story Home Insurance Building in Chicago, built in 1884–1885. While its original height of 42.1 m (138ft) is not considered very impressive today, it was at that time. The building of tall buildings in the 1880s gave the skyscraper its first architectural movement the Chicago School, which developed what has been called the Commercial Style.

The architect, Major William Le Baron Jenney, created a load-bearing structural frame. In this building, a steel frame supported the entire weight of the walls, instead of load-bearing walls carrying the weight of the building. This development led to the "Chicago skeleton" form of construction. In addition to the steel frame, the Home Insurance Building also utilized fireproofing, elevators, and electrical wiring, key elements in most skyscrapers today.

Burnham and Root's Rand McNally Building in Chicago, 1889, was the first all-steel framed skyscraper, while Louis Sullivan's Wainwright Building in St. Louis, Missouri, 1891, was the first steel-framed building with soaring vertical bands to emphasize the height of the building and is therefore considered to be the first early skyscraper.

In 1889, the Mole Antonelliana in Italy was 167 m (549 ft) tall.

Most early skyscrapers emerged in the land-strapped areas of Chicago and New York City toward the end of the 19th century. A land boom in Melbourne, Australia between 1888 and 1891 spurred the creation of a significant number of early skyscrapers, though none of these were steel reinforced and few remain today. Height limits and fire restrictions were later introduced. London builders soon found building heights limited due to a complaint from Queen Victoria, rules that continued to exist with few exceptions.

Concerns about aesthetics and fire safety had likewise hampered the development of skyscrapers across continental Europe for the first half of the twentieth century. Some notable exceptions are the tall 1898 Witte Huis "(White House)" in Rotterdam; the Royal Liver Building in Liverpool, completed in 1911 and high; the tall 1924 Marx House in Düsseldorf, Germany; the Kungstornen "(Kings' Towers)" in Stockholm, Sweden, which were built 1924–25, the Edificio Telefónica in Madrid, Spain, built in 1929; the Boerentoren in Antwerp, Belgium, built in 1932; the Prudential Building in Warsaw, Poland, built in 1934; and the Torre Piacentini in Genoa, Italy, built in 1940.

After an early competition between Chicago and New York City for the world's tallest building, New York took the lead by 1895 with the completion of the tall American Surety Building, leaving New York with the title of the world's tallest building for many years.

Modern skyscrapers are built with steel or reinforced concrete frameworks and curtain walls of glass or polished stone. They use mechanical equipment such as water pumps and elevators. Since the 1960s, according to the CTHUB, the skyscraper has been reoriented away from a symbol for North American corporate power to instead communicate a city or nation's place in the world.

Skyscraper construction entered a three-decades-long era of stagnation in 1930 due to the Great Depression and then World War II. Shortly after the war ended, the Soviet Union began construction on a series of skyscrapers in Moscow. Seven, dubbed the "Seven Sisters", were built between 1947 and 1953; and one, the Main building of Moscow State University, was the tallest building in Europe for nearly four decades (1953–1990). Other skyscrapers in the style of Socialist Classicism were erected in East Germany (Frankfurter Tor), Poland (PKiN), Ukraine (Hotel Ukrayina), Latvia (Academy of Sciences) and other Eastern Bloc countries. Western European countries also began to permit taller skyscrapers during the years immediately following World War II. Early examples include Edificio España (Spain) Torre Breda (Italy).

From the 1930s onward, skyscrapers began to appear in various cities in East and Southeast Asia as well as in Latin America. Finally, they also began to be constructed in cities of Africa, the Middle East, South Asia and Oceania (mainly Australia) from the late 1950s on.

Skyscraper projects after World War II typically rejected the classical designs of the early skyscrapers, instead embracing the uniform international style; many older skyscrapers were redesigned to suit contemporary tastes or even demolished—such as New York's Singer Building, once the world's tallest skyscraper.

German architect Ludwig Mies van der Rohe became one of the world's most renowned architects in the second half of the 20th century. He conceived of the glass façade skyscraper and, along with Norwegian Fred Severud, he designed the Seagram Building in 1958, a skyscraper that is often regarded as the pinnacle of the modernist high-rise architecture.

Skyscraper construction surged throughout the 1960s. The impetus behind the upswing was a series of transformative innovations which made it possible for people to live and work in "cities in the sky".

In the early 1960s structural engineer Fazlur Rahman Khan, considered the "father of tubular designs" for high-rises, discovered that the dominating rigid steel frame structure was not the only system apt for tall buildings, marking a new era of skyscraper construction in terms of multiple structural systems. His central innovation in skyscraper design and construction was the concept of the "tube" structural system, including the "framed tube", "trussed tube", and "bundled tube". His "tube concept", using all the exterior wall perimeter structure of a building to simulate a thin-walled tube, revolutionized tall building design. These systems allow greater economic efficiency, and also allow skyscrapers to take on various shapes, no longer needing to be rectangular and box-shaped. The first building to employ the tube structure was the Chestnut De-Witt apartment building, this building is considered to be a major development in modern architecture. These new designs opened an economic door for contractors, engineers, architects, and investors, providing vast amounts of real estate space on minimal plots of land. Over the next fifteen years, many towers were built by Fazlur Rahman Khan and the "Second Chicago School", including the hundred-storey John Hancock Center and the massive Willis Tower. Other pioneers of this field include Hal Iyengar, William LeMessurier, and Minoru Yamasaki, the architect of the World Trade Center.

Many buildings designed in the 70s lacked a particular style and recalled ornamentation from earlier buildings designed before the 50s. These design plans ignored the environment and loaded structures with decorative elements and extravagant finishes. This approach to design was opposed by Fazlur Khan and he considered the designs to be whimsical rather than rational. Moreover, he considered the work to be a waste of precious natural resources. Khan's work promoted structures integrated with architecture and the least use of material resulting in the least carbon emission impact on the environment. The next era of skyscrapers will focus on the environment including performance of structures, types of material, construction practices, absolute minimal use of materials/natural resources, embodied energy within the structures, and more importantly, a holistically integrated building systems approach.

Modern building practices regarding supertall structures have led to the study of "vanity height". Vanity height, according to the CTBUH, is the distance between the highest floor and its architectural top (excluding antennae, flagpole or other functional extensions). Vanity height first appeared in New York City skyscrapers as early as the 1920s and 1930s but supertall buildings have relied on such uninhabitable extensions for on average 30 % of their height, raising potential definitional and sustainability issues. The current era of skyscrapers focuses on sustainability, its built and natural environments, including the performance of structures, types of materials, construction practices, absolute minimal use of materials and natural resources, energy within the structure, and a holistically integrated building systems approach. LEED is a current green building standard.

Architecturally, with the movements of Postmodernism, New Urbanism and New Classical Architecture, that established since the 1980s, a more classical approach came back to global skyscraper design, that remains popular today. Examples are the Wells Fargo Center, NBC Tower, Parkview Square, 30 Park Place, the Messeturm, the iconic Petronas Towers and Jin Mao Tower.

Other contemporary styles and movements in skyscraper design include organic, sustainable, neo-futurist, structuralist, high-tech, deconstructivist, blob, digital, streamline, novelty, critical regionalist, vernacular, Neo Art Deco and neo-historist, also known as revivalist.

3 September is the global commemorative day for skyscrapers, called "Skyscraper Day".

New York City developers competed among themselves, with successively taller buildings claiming the title of "world's tallest" in the 1920s and early 1930s, culminating with the completion of the Chrysler Building in 1930 and the Empire State Building in 1931, the world's tallest building for forty years. The first completed tall World Trade Center tower became the world's tallest building in 1972. However, it was overtaken by the Sears Tower (now Willis Tower) in Chicago within two years. The tall Sears Tower stood as the world's tallest building for 24 years, from 1974 until 1998, until it was edged out by Petronas Twin Towers in Kuala Lumpur, which held the title for six years.

The design and construction of skyscrapers involves creating safe, habitable spaces in very tall buildings. The buildings must support their weight, resist wind and earthquakes, and protect occupants from fire. Yet they must also be conveniently accessible, even on the upper floors, and provide utilities and a comfortable climate for the occupants. The problems posed in skyscraper design are considered among the most complex encountered given the balances required between economics, engineering, and construction management.

One common feature of skyscrapers is a steel framework from which curtain walls are suspended, rather than load-bearing walls of conventional construction. Most skyscrapers have a steel frame that enables them to be built taller than typical load-bearing walls of reinforced concrete. Skyscrapers usually have a particularly small surface area of what are conventionally thought of as walls. Because the walls are not load-bearing most skyscrapers are characterized by surface areas of windows made possible by the concept of steel frame and curtain wall. However, skyscrapers can also have curtain walls that mimic conventional walls and have a small surface area of windows.

The concept of a skyscraper is a product of the industrialized age, made possible by cheap fossil fuel derived energy and industrially refined raw materials such as steel and concrete. The construction of skyscrapers was enabled by steel frame construction that surpassed brick and mortar construction starting at the end of the 19th century and finally surpassing it in the 20th century together with reinforced concrete construction as the price of steel decreased and labour costs increased.

The steel frames become inefficient and uneconomic for supertall buildings as usable floor space is reduced for progressively larger supporting columns. Since about 1960, tubular designs have been used for high rises. This reduces the usage of material (more efficient in economic terms – Willis Tower uses a third less steel than the Empire State Building) yet allows greater height. It allows fewer interior columns, and so creates more usable floor space. It further enables buildings to take on various shapes.

Elevators are characteristic to skyscrapers. In 1852 Elisha Otis introduced the safety elevator, allowing convenient and safe passenger movement to upper floors. Another crucial development was the use of a steel frame instead of stone or brick, otherwise the walls on the lower floors on a tall building would be too thick to be practical. Today major manufacturers of elevators include Otis, ThyssenKrupp, Schindler, and KONE.

Advances in construction techniques have allowed skyscrapers to narrow in width, while increasing in height. Some of these new techniques include mass dampers to reduce vibrations and swaying, and gaps to allow air to pass through, reducing wind shear.

Good structural design is important in most building design, but particularly for skyscrapers since even a small chance of catastrophic failure is unacceptable given the high price. This presents a paradox to civil engineers: the only way to assure a lack of failure is to test for all modes of failure, in both the laboratory and the real world. But the only way to know of all modes of failure is to learn from previous failures. Thus, no engineer can be absolutely sure that a given structure will resist all loadings that could cause failure, but can only have large enough margins of safety such that a failure is acceptably unlikely. When buildings do fail, engineers question whether the failure was due to some lack of foresight or due to some unknowable factor.

The load a skyscraper experiences is largely from the force of the building material itself. In most building designs, the weight of the structure is much larger than the weight of the material that it will support beyond its own weight. In technical terms, the dead load, the load of the structure, is larger than the live load, the weight of things in the structure (people, furniture, vehicles, etc.). As such, the amount of structural material required within the lower levels of a skyscraper will be much larger than the material required within higher levels. This is not always visually apparent. The Empire State Building's setbacks are actually a result of the building code at the time (1916 Zoning Resolution), and were not structurally required. On the other hand, John Hancock Center's shape is uniquely the result of how it supports loads. Vertical supports can come in several types, among which the most common for skyscrapers can be categorized as steel frames, concrete cores, tube within tube design, and shear walls.

The wind loading on a skyscraper is also considerable. In fact, the lateral wind load imposed on supertall structures is generally the governing factor in the structural design. Wind pressure increases with height, so for very tall buildings, the loads associated with wind are larger than dead or live loads.

Other vertical and horizontal loading factors come from varied, unpredictable sources, such as earthquakes.

By 1895, steel had replaced cast iron as skyscrapers' structural material. Its malleability allowed it to be formed into a variety of shapes, and it could be riveted, ensuring strong connections. The simplicity of a steel frame eliminated the inefficient part of a shear wall, the central portion, and consolidated support members in a much stronger fashion by allowing both horizontal and vertical supports throughout. Among steel's drawbacks is that as more material must be supported as height increases, the distance between supporting members must decrease, which in turn increases the amount of material that must be supported. This becomes inefficient and uneconomic for buildings above 40 storeys tall as usable floor spaces are reduced for supporting column and due to more usage of steel.

A new structural system of framed tubes was developed by Fazlur Rahman Khan in 1963. The framed tube structure is defined as "a three dimensional space structure composed of three, four, or possibly more frames, braced frames, or shear walls, joined at or near their edges to form a vertical tube-like structural system capable of resisting lateral forces in any direction by cantilevering from the foundation". Closely spaced interconnected exterior columns form the tube. Horizontal loads (primarily wind) are supported by the structure as a whole. Framed tubes allow fewer interior columns, and so create more usable floor space, and about half the exterior surface is available for windows. Where larger openings like garage doors are required, the tube frame must be interrupted, with transfer girders used to maintain structural integrity. Tube structures cut down costs, at the same time allowing buildings to reach greater heights. Concrete tube-frame construction was first used in the DeWitt-Chestnut Apartment Building, completed in Chicago in 1963, and soon after in the John Hancock Center and World Trade Center.

The tubular systems are fundamental to tall building design. Most buildings over 40-storeys constructed since the 1960s now use a tube design derived from Khan's structural engineering principles, examples including the construction of the World Trade Center, Aon Center, Petronas Towers, Jin Mao Building, and most other supertall skyscrapers since the 1960s. The strong influence of tube structure design is also evident in the construction of the current tallest skyscraper, the Burj Khalifa.

Khan pioneered several other variations of the tube structure design. One of these was the concept of X-bracing, or the trussed tube, first employed for the John Hancock Center. This concept reduced the lateral load on the building by transferring the load into the exterior columns. This allows for a reduced need for interior columns thus creating more floor space. This concept can be seen in the John Hancock Center, designed in 1965 and completed in 1969. One of the most famous buildings of the structural expressionist style, the skyscraper's distinctive X-bracing exterior is actually a hint that the structure's skin is indeed part of its 'tubular system'. This idea is one of the architectural techniques the building used to climb to record heights (the tubular system is essentially the spine that helps the building stand upright during wind and earthquake loads). This X-bracing allows for both higher performance from tall structures and the ability to open up the inside floorplan (and usable floor space) if the architect desires.

The John Hancock Center was far more efficient than earlier steel-frame structures. Where the Empire State Building (1931), required about 206 kilograms of steel per square metre and 28 Liberty Street (1961) required 275, the John Hancock Center required only 145. The trussed tube concept was applied to many later skyscrapers, including the Onterie Center, Citigroup Center and Bank of China Tower.

An important variation on the tube frame is the bundled tube, which uses several interconnected tube frames. The Willis Tower in Chicago used this design, employing nine tubes of varying height to achieve its distinct appearance. The bundled tube structure meant that "buildings no longer need be boxlike in appearance: they could become sculpture."

The invention of the elevator was a precondition for the invention of skyscrapers, given that most people would not (or could not) climb more than a few flights of stairs at a time. The elevators in a skyscraper are not simply a necessary utility, like running water and electricity, but are in fact closely related to the design of the whole structure: a taller building requires more elevators to service the additional floors, but the elevator shafts consume valuable floor space. If the service core, which contains the elevator shafts, becomes too big, it can reduce the profitability of the building. Architects must therefore balance the value gained by adding height against the value lost to the expanding service core.

Many tall buildings use elevators in a non-standard configuration to reduce their footprint. Buildings such as the former World Trade Center Towers and Chicago's John Hancock Center use sky lobbies, where express elevators take passengers to upper floors which serve as the base for local elevators. This allows architects and engineers to place elevator shafts on top of each other, saving space. Sky lobbies and express elevators take up a significant amount of space, however, and add to the amount of time spent commuting between floors.

Other buildings, such as the Petronas Towers, use double-deck elevators, allowing more people to fit in a single elevator, and reaching two floors at every stop. It is possible to use even more than two levels on an elevator, although this has never been done. The main problem with double-deck elevators is that they cause everyone in the elevator to stop when only people on one level need to get off at a given floor.

Buildings with sky lobbies include the World Trade Center, Petronas Twin Towers, Willis Tower and Taipei 101. The 44th-floor sky lobby of the John Hancock Center also featured the first high-rise indoor swimming pool, which remains the highest in America.

Skyscrapers are usually situated in city centers where the price of land is high. Constructing a skyscraper becomes justified if the price of land is so high that it makes economic sense to build upward as to minimize the cost of the land per the total floor area of a building. Thus the construction of skyscrapers is dictated by economics and results in skyscrapers in a certain part of a large city unless a building code restricts the height of buildings.

Skyscrapers are rarely seen in small cities and they are characteristic of large cities, because of the critical importance of high land prices for the construction of skyscrapers. Usually only office, commercial and hotel users can afford the rents in the city center and thus most tenants of skyscrapers are of these classes.

Today, skyscrapers are an increasingly common sight where land is expensive, as in the centers of big cities, because they provide such a high ratio of rentable floor space per unit area of land.

One problem with skyscrapers is car parking. In the largest cities most people commute via public transport, but in smaller cities many parking spaces are needed. Multi-storey car parks are impractical to build very tall, so much land area is needed.

Another disadvantage of very high scyscrapers is the loss of usable floorspace, as many elevator shafts are needed to enable performant vertical travelling. This lead to the introduction of express lifts and sky lobbys where transfer to slower distribution lifts can be done.

The amount of steel, concrete, and glass needed to construct a single skyscraper is large, and these materials represent a great deal of embodied energy. Skyscrapers are thus energy intensive buildings, but skyscrapers have a long lifespan, for example the Empire State Building in New York City, United States completed in 1931 and is still in active use.

Skyscrapers have considerable mass, which means that they must be built on a sturdier foundation than would be required for shorter, lighter buildings. Building materials must also be lifted to the top of a skyscraper during construction, requiring more energy than would be necessary at lower heights. Furthermore, a skyscraper consumes much electricity because potable and non-potable water have to be pumped to the highest occupied floors, skyscrapers are usually designed to be mechanically ventilated, elevators are generally used instead of stairs, and natural lighting cannot be utilized in rooms far from the windows and the windowless spaces such as elevators, bathrooms and stairwells.

Skyscrapers can be artificially lit and the energy requirements can be covered by renewable energy or other electricity generation with low greenhouse gas emissions. Heating and cooling of skyscrapers can be efficient, because of centralized HVAC systems, heat radiation blocking windows and small surface area of the building. There is Leadership in Energy and Environmental Design (LEED) certification for skyscrapers. For example, the Empire State Building received a gold Leadership in Energy and Environmental Design rating in September 2011 and the Empire State Building is the tallest LEED certified building in the United States, proving that skyscrapers can be environmentally friendly. Also the 30 St Mary Axe in London, the United Kingdom is an environmentally friendly skyscraper.

In the lower levels of a skyscraper a larger percentage of the building cross section must be devoted to the building structure and services than is required for lower buildings:

In low-rise structures, the support rooms (chillers, transformers, boilers, pumps and air handling units) can be put in basements or roof space—areas which have low rental value. There is, however, a limit to how far this plant can be located from the area it serves. The farther away it is the larger the risers for ducts and pipes from this plant to the floors they serve and the more floor area these risers take. In practice this means that in highrise buildings this plant is located on 'plant levels' at intervals up the building.

At the beginning of the 20th century, New York City was a center for the Beaux-Arts architectural movement, attracting the talents of such great architects as Stanford White and Carrere and Hastings. As better construction and engineering technology became available as the century progressed, New York City and Chicago became the focal point of the competition for the tallest building in the world. Each city's striking skyline has been composed of numerous and varied skyscrapers, many of which are icons of 20th-century architecture:


Momentum in setting records passed from the United States to other nations with the opening of the Petronas Twin Towers in Kuala Lumpur, Malaysia, in 1998. The record for the world's tallest building has remained in Asia since the opening of Taipei 101 in Taipei, Taiwan, in 2004. A number of architectural records, including those of the world's tallest building and tallest free-standing structure, moved to the Middle East with the opening of the Burj Khalifa in Dubai, United Arab Emirates.

This geographical transition is accompanied by a change in approach to skyscraper design. For much of the twentieth century large buildings took the form of simple geometrical shapes. This reflected the "international style" or modernist philosophy shaped by Bauhaus architects early in the century. The last of these, the Willis Tower and World Trade Center towers in New York, erected in the 1970s, reflect the philosophy. Tastes shifted in the decade which followed, and new skyscrapers began to exhibit postmodernist influences. This approach to design avails itself of historical elements, often adapted and re-interpreted, in creating technologically modern structures. The Petronas Twin Towers recall Asian pagoda architecture and Islamic geometric principles. Taipei 101 likewise reflects the pagoda tradition as it incorporates ancient motifs such as the ruyi symbol. The Burj Khalifa draws inspiration from traditional Islamic art. Architects in recent years have sought to create structures that would not appear equally at home if set in any part of the world, but that reflect the culture thriving in the spot where they stand.

The following list measures height of the roof. The more common gauge is the "highest architectural detail"; such ranking would have included Petronas Towers, built in 1996.

Proposals for such structures have been put forward, including the Burj Mubarak Al Kabir in Kuwait and Azerbaijan Tower in Baku. Kilometer-plus structures present architectural challenges that may eventually place them in a new architectural category. The first building under construction and planned to be over one kilometre tall is the Jeddah Tower.

Several wooden skyscraper designs have been designed and built. A 14-storey housing project in Bergen, Norway known as 'Treet' or 'The Tree' became the world's tallest wooden apartment block when it was completed in late 2015. The Tree's record was eclipsed by Brock Commons, an 18-storey wooden dormitory at the University of British Columbia in Canada, when it was completed in September 2016.

A 40-storey residential building 'Trätoppen' has been proposed by architect Anders Berensson to be built in Stockholm, Sweden. Trätoppen would be the tallest building in Stockholm, though there are no immediate plans to begin construction. The tallest currently-planned wooden skyscraper is the 70-storey W350 Project in Tokyo, to be built by the Japanese wood products company Sumitomo Forestry Co. to celebrate its 350th anniversary in 2041. An 80-storey wooden skyscraper, the River Beech Tower, has been proposed by a team including architects Perkins + Will and the University of Cambridge. The River Beech Tower, on the banks of the Chicago River in Chicago, Illinois, would be 348 feet shorter than the W350 Project despite having 10 more storeys.

Wooden skyscrapers are estimated to be around a quarter of the weight of an equivalent reinforced-concrete structure as well as reducing the building carbon footprint by 60–75 %. Buildings have been designed using cross-laminated timber (CLT) which gives a higher rigidity and strength to wooden structures. CLT panels are prefabricated and can therefore speed up building time.





</doc>
<doc id="29486" url="https://en.wikipedia.org/wiki?curid=29486" title="Sagas of Icelanders">
Sagas of Icelanders

The sagas of Icelanders (), also known as family sagas, are one genre of Icelandic sagas. They are prose narratives mostly based on historical events that mostly took place in Iceland in the ninth, tenth, and early eleventh centuries, during the so-called Saga Age. They are the best-known specimens of Icelandic literature.

They are focused on history, especially genealogical and family history. They reflect the struggle and conflict that arose within the societies of the early generations of Icelandic settlers.

Eventually many of these Icelandic sagas were recorded, mostly in the thirteenth and fourteenth centuries. The 'authors', or rather recorders of these sagas are largely unknown. One saga, "Egil's Saga", is believed by some scholars to have been written by Snorri Sturluson, a descendant of the saga's hero, but this remains uncertain. The standard modern edition of Icelandic sagas is known as Íslenzk fornrit.

Among the several literary reviews of the sagas is that by Sigurður Nordal's "Sagalitteraturen", which divides the sagas into five chronological groups distinguished by the state of literary development:


A small number of sagas are thought to have existed and now to be lost. One example is the supposed "Gauks saga Trandilssonar".





</doc>
<doc id="29489" url="https://en.wikipedia.org/wiki?curid=29489" title="Staind">
Staind

Staind ( ) is an American rock band formed in Springfield, Massachusetts, in 1995. The original lineup consisted of lead vocalist and rhythm guitarist Aaron Lewis, lead guitarist Mike Mushok, bassist and backing vocalist Johnny April, and drummer Jon Wysocki. The lineup has been stable outside of the 2011 departure of Wysocki, who was replaced by Sal Giancarelli. Staind has recorded seven studio albums: "Tormented" (1996), "Dysfunction" (1999), "Break the Cycle" (2001), "14 Shades of Grey" (2003), "Chapter V" (2005), "The Illusion of Progress" (2008), and "Staind" (2011). The band's activity became more sporadic after their self-titled release, with Lewis pursuing a solo country music career and Mushok subsequently joining the band Saint Asonia, but they have continued to tour on and off in the following years. In 2016, Lewis reiterated that the band had not broken up, and would possibly create another album, but that his then-current focus was on his solo career. The band reunited more permanently in 2019 for several shows, continuing with live appearances in 2020. Many of their singles have reached high positions on US rock and all-format charts as well, including "It's Been Awhile", "Fade", "Price to Play", "So Far Away", and "Right Here".

In 1993, vocalist Aaron Lewis and guitarist Mike Mushok met at a Christmas party in Springfield, Massachusetts. Mushok introduced drummer Jon Wysocki while Lewis brought in bassist Johnny April to form the band in 1995. Their first public performance was in February 1995, playing a heavy, dark, and introspective style of metal. Extensive touring in the Northeast helped Staind acquire a regional following over the next few years.

The band started covering Korn, Rage Against the Machine, Pearl Jam, Tool, and Alice in Chains, among others, and played at local clubs (most commonly playing at Club Infinity) for a year and a half. Staind self-released their debut album, "Tormented", in November 1996, citing Tool, Faith No More, and Pantera as their influences. In October 1997, Staind acquired a concert slot through Aaron's cousin Justin Cantor with Limp Bizkit. Just prior to the performance, Limp Bizkit frontman Fred Durst was appalled by Staind's grotesque album cover and unsuccessfully attempted to remove them from the bill. Durst thought that Staind were Theistic Satanists. After being persuaded to let them perform, however, Durst was so impressed that he signed them to Flip Records by February 1998.

On April 13, 1999, Staind released their major label debut "Dysfunction" on Flip Records. The album, which was co-produced by Fred Durst and Terry Date (who also produced acts like Soundgarden, Deftones, and Pantera), received comparisons to alternative metal giants Tool and Korn. In particular, Aaron Lewis was lauded for his vocals, which were likened to those of Pearl Jam's Eddie Vedder.

The album achieved slow success, reaching the No. 1 spot on Billboard's Heatseeker Charts almost six months after its debut. In the same week, the record jumped to No. 74 on Billboard's Top 200 Album Charts. The nine-track LP (with one hidden track, "Excess Baggage") produced three singles, "Just Go", "Mudshovel", and "Home". "Mudshovel" and "Home" both received radio play, cracking the Top 20 of Billboard's Modern Rock and Mainstream Rock charts. In promotion of "Dysfunction", Staind went on several tours, including the Family Values Tour with acts like Limp Bizkit and The Crystal Method, as well as opening for Sevendust's headlining tour.

Staind toured with Limp Bizkit for the Family Values Tour during the fall of 1999, where Aaron Lewis performed an early version of "Outside" with Fred Durst at the Mississippi Coast Coliseum. Staind released their third studio album, "Break the Cycle", on May 22, 2001. Propelled by the success of the first single, "It's Been Awhile", the album debuted at No. 1 on Billboard's Top 200 Album charts, selling 716,000 copies in its first week. The record's first-week sales were the second highest of any album that year, behind Creed's "Weathered".

"Break the Cycle" saw the band retaining the nu metal sound from their previous album. Despite this, the album saw the band going further into a post-grunge sound which is evident in the smash hit song "It's Been Awhile", and the song led critics to compare the band to several other post-grunge bands at the time. The record spawned the singles "It's Been Awhile" (which hit the Billboard Top 10), "Fade", "Outside", "For You", and the acoustic ballad "Epiphany". "It's Been Awhile" spent a total of 16 and 14 weeks on top of the modern and mainstream rock charts respectively, making it one of the highest joint numbers of all time. In 2001, "Break the Cycle" sold four million copies worldwide, making it one of the best selling albums that year. "Break the Cycle" would go on to sell seven million copies worldwide, making this Staind's bestselling album.

In early 2003, Staind embarked on a worldwide tour to promote the release of the follow-up to "Break the Cycle", "14 Shades of Grey", which sold two million copies and debuted at number 1 on the Billboard 200. The album saw a departure from their previous nu metal sound as it mostly contained a lighter and more melodic post-grunge sound. "14 Shades of Grey" produced two mainstream hits, "Price to Play" and "So Far Away", which spent 14 weeks on top of the rock chart. In addition, two other singles were released: "How About You" and "Zoe Jane". The band's appearance at the Reading Festival during their 2003 tour had another impromptu acoustic set, this time due to equipment failure. The singles "So Far Away" and "Price to Play" came with two unreleased tracks, "Novocaine" and "Let It Out", which were released for the special edition of the group's subsequent album "Chapter V", which came out in late 2005. In 2003, Staind unsuccessfully sued their logo designer Jon Stainbrook in New York Federal Court for attempting to re-use the logo he had sold to the band. They re-opened the case in mid-2005.

Staind's fifth album, titled "Chapter V", was released on August 9, 2005, and became their third consecutive album to top the "Billboard" 200. The album opened to sales of 185,000 and has since been certified platinum in the U.S. The first single, "Right Here", was the biggest success from the album, garnering much mainstream radio play and peaking at number 1 on the mainstream rock chart. "Falling" was released as the second single, followed by "Everything Changes" and "King of All Excuses". Staind went on the road when the album came out, doing live shows and promoting it for a full year, including participating in the Fall Brawl tour with P.O.D., Taproot, and Flyleaf; they also had a solo tour across Europe and a mini-promotional tour in Australia for the first time. Other live shows included a cover of Pantera's "This Love", a tribute to Dimebag Darrell. Staind appeared on "The Howard Stern Show" on August 10, 2005 to promote "Chapter V". They performed acoustic renditions of the single "Right Here" and Beetlejuice's song "This is Beetle". In early November 2005, Staind released the limited edition 2-CD/DVD set of "Chapter V". On September 6, 2006, they performed an acoustic show in the Hiro Ballroom, New York City, that was recorded for their singles collection. The band played sixteen songs, including three covers: Tool's "Sober", Pink Floyd's "Comfortably Numb", and Alice in Chains's "Nutshell".

The collection "" was released on November 14, 2006. It included all the band's singles, the three covers performed at the New York show, and a remastered version of "Come Again" from Staind's first independent release "Tormented".

On August 19, 2008, Staind released their sixth album, "The Illusion of Progress". Prior to the album's release, the track "This Is It" was available for download on the iTunes Store, as well as for "Rock Band". The album debuted at No. 3 on the US Billboard 200, No. 1 on the Top Modern Rock/Alternative Albums Chart, No. 1 on the Top Digital Albums Chart, and also No. 1 on the Top Internet Albums Chart, with first-week sales of 91,800 units. The first single on the album, "Believe", topped Billboard's Top 10 Modern Rock Tracks on September 5, 2008. The band also supported Nickelback on their 2008 European tour. The second single was "All I Want", and came out on November 24. The single also became Staind's 13th top 20 hit on the rock charts. In Europe, the second single was "The Way I Am", released on January 26, 2009. The final single released from the album, "This Is It", was sent to radio stations across the country on May 4, 2009. The track was also included on the successful "" released in late June 2009. The same year, Staind embarked on a fall tour with the newly reunited Creed.

In March 2010, Aaron Lewis stated the band would start working on their seventh studio album by the end of the year. Lewis had finished recording his country music solo EP and had started a nonprofit organization to reopen his daughter's elementary school in Worthington, Massachusetts. Guitarist Mike Mushok stated in a Q&A session with fans that the band was looking to make a heavy record, but still "explore some of the things we did on the last record and take them somewhere new for us". In a webisode posted on the band's website, Lewis stated that eight songs were written and that "every one of them is as heavy or heavier than the heaviest song on the last record".

In December 2010, Staind posted three webisodes from the studio, which featured the band members discussing the writing and recording process of their new album. They announced that as of April 20, they had completed the recording of their seventh and would release it later that year.

On May 20, 2011, Staind announced that original drummer Jon Wysocki had left the band. Drummer Will Hunt filled in for a few dates, while Wysocki's drum tech Sal Giancarelli filled in for the rest of the tour. Three days later, it was reported that Staind's new album would be a self-titled release. It was released on September 13, 2011. The first single, "Not Again", was released to active radio stations on July 18. The song "The Bottom" appeared on the "" soundtrack. On June 30, Staind released a song called "Eyes Wide Open" from their new record. "Eyes Wide Open" would later be released on November 29 as the album's second single.

In November 2011, the band announced through their YouTube page that Sal Giancarelli was now an official member. The band continued to perform into 2012, embarking on an April and May tour with Godsmack and Halestorm, and they played the Uproar Festival in August and September with Shinedown and a number of other artists.

It was announced in July 2012 that the band was to be taking a hiatus. In an interview with Billboard, Aaron Lewis stated that "We're not breaking up. We're not gonna stop making music. We're just going to take a little hiatus that really hasn't ever been taken in our career. We put out seven records in 14 years. We've been pretty busy." Lewis also had plans to release his first solo album "The Road". During this time, Mike Mushok auditioned, and was selected, to play guitar for former Metallica bassist Jason Newsted's new band Newsted. He featured on their debut album "Heavy Metal Music".

Staind played their first show in two years at the Welcome To Rockville Festival on April 27, 2014. They also played the Carolina Rebellion and Rock on the Range festivals in May 2014.

In late 2014, the band went on a hiatus. Aaron Lewis continued to play solo shows and work on his next solo album. He also confirmed that the hiatus would last "for a while". Mike Mushok teamed up with former Three Days Grace singer Adam Gontier, former Finger Eleven drummer Rich Beddoe, and Eye Empire bassist Corey Lowery to form Saint Asonia.

On August 4, 2017, the band performed for the first time since November 2014 for an acoustic performance at Aaron Lewis' 6th annual charity golf tournament and concert when bassist Johnny April and drummer Sal Giancarelli joined Aaron Lewis and Mike Mushok to perform "Outside", "Something to Remind You", and "It's Been Awhile". Three days later, Lewis announced that Staind would never tour extensively again, with Lewis explaining:

In April 2019, the band announced they would reform in September 2019 for some live performances. The band was scheduled to play at Epicenter Festival on May 3rd 2020 at Charlotte Motor Speedway.

The topics of Staind's lyrics cover issues of depression, relationships, death, addiction, finding one's self, betrayal, and Lewis' thoughts about becoming a father in the song "Zoe Jane" from "14 Shades of Grey", as well as reflecting on his upbringing in the song "The Corner" from "The Illusion of Progress". Also from "14 Shades of Grey", the track titled "Layne" was written about Alice in Chains frontman Layne Staley in response to his death in 2002. The song is also about Staley's legacy and the effect his music had on the members of Staind, especially Aaron Lewis. Staind has been categorized as nu metal, alternative metal, heavy metal, hard rock, and post-grunge.

In 2001, "Rolling Stone" outlined the band's relationship to the nu metal label:
Staind's influences include Pantera, The Doors, Suicidal Tendencies, Kiss, Van Halen, Slayer, Led Zeppelin, Sepultura, Whitesnake, the Beatles, Alice in Chains, Faith No More, Deftones, Black Sabbath, Pearl Jam, Tool, Rage Against the Machine, Nirvana, Stone Temple Pilots, Helmet, James Taylor, Korn, and Crosby, Stills & Nash.

Current line-up

Former members

Studio albums



</doc>
<doc id="29490" url="https://en.wikipedia.org/wiki?curid=29490" title="Saddam Hussein">
Saddam Hussein

Saddam Hussein Abd al-Majid al-Tikriti (; Arabic: ""; 28 April 1937 – 30 December 2006) was the fifth President of Iraq from 16 July 1979 until 9 April 2003. A leading member of the revolutionary Arab Socialist Ba'ath Party, and later, the Baghdad-based Ba'ath Party and its regional organization, the Iraqi Ba'ath Party—which espoused Ba'athism, a mix of Arab nationalism and socialism—Saddam played a key role in the 1968 coup (later referred to as the 17 July Revolution) that brought the party to power in Iraq.

As vice president under the ailing General Ahmed Hassan al-Bakr, and at a time when many groups were considered capable of overthrowing the government, Saddam created security forces through which he tightly controlled conflicts between the government and the armed forces. In the early 1970s, Saddam nationalized oil and foreign banks leaving the system eventually insolvent mostly due to the Iran–Iraq War, the Gulf War, and UN sanctions. Through the 1970s, Saddam cemented his authority over the apparatus of government as oil money helped Iraq's economy grow at a rapid pace. Positions of power in the country were mostly filled with Sunni Arabs, a minority that made up only a fifth of the population.

Saddam formally rose to power in 1979, although he had already been the "de facto" head of Iraq for several years. He suppressed several movements, particularly Shi'a and Kurdish movements which sought to overthrow the government or gain independence, respectively, and maintained power during the Iran–Iraq War and the Gulf War. Hussein's rule was a repressive dictatorship. The total number of Iraqis killed by the security services of Saddam's government in various purges and genocides is conservatively estimated to be 250,000. Saddam's invasions of Iran and Kuwait also resulted in hundreds of thousands of deaths.

In 2003, a coalition led by the United States invaded Iraq to depose Saddam, in which U.S. President George W. Bush and British Prime Minister Tony Blair erroneously accused him of possessing weapons of mass destruction and having ties to al-Qaeda. Saddam's Ba'ath party was disbanded and the country's first ever set of democratic elections were held. Following his capture on 13 December 2003, the trial of Saddam took place under the Iraqi Interim Government. On 5 November 2006, Saddam was convicted by an Iraqi court of crimes against humanity related to the 1982 killing of 148 Iraqi Shi'a and sentenced to death by hanging. He was executed on 30 December 2006.

Before he was born, cancer killed both Saddam's father and brother. These deaths made Saddam's mother, Sabha, so depressed that she attempted to abort her pregnancy and commit suicide. When her son was born, Sabha "would have nothing to do with him," and Saddam was taken in by an uncle.

His mother remarried, and Saddam gained three half-brothers through this marriage. His stepfather, Ibrahim al-Hassan, treated Saddam harshly after his return. At about age 10, Saddam fled the family and returned to live in Baghdad with his uncle Kharaillah Talfah, who became a father figure to Saddam. Talfah, the father of Saddam's future wife, was a devout Sunni Muslim and a veteran of the 1941 Anglo-Iraqi War between Iraqi nationalists and the United Kingdom, which remained a major colonial power in the region. Talfah later became the mayor of Baghdad during Saddam's time in power, until his notorious corruption compelled Saddam to force him out of office.

Later in his life relatives from his native Tikrit became some of his closest advisors and supporters. Under the guidance of his uncle he attended a nationalistic high school in Baghdad. After secondary school Saddam studied at an Iraqi law school for three years, dropping out in 1957 at the age of 20 to join the revolutionary pan-Arab Ba'ath Party, of which his uncle was a supporter. During this time, Saddam apparently supported himself as a secondary school teacher. Ba'athist ideology originated in Syria and the Ba'ath Party had a large following in Syria at the time, but in 1955 there were fewer than 300 Ba'ath Party members in Iraq and it is believed that Saddam's primary reason for joining the party as opposed to the more established Iraqi nationalist parties was his familial connection to Ahmed Hassan al-Bakr and other leading Ba'athists through his uncle.
Revolutionary sentiment was characteristic of the era in Iraq and throughout the Middle East. In Iraq progressives and socialists assailed traditional political elites (colonial-era bureaucrats and landowners, wealthy merchants and tribal chiefs, and monarchists). Moreover, the pan-Arab nationalism of Gamal Abdel Nasser in Egypt profoundly influenced young Ba'athists like Saddam. The rise of Nasser foreshadowed a wave of revolutions throughout the Middle East in the 1950s and 1960s, with the collapse of the monarchies of Iraq, Egypt, and Libya. Nasser inspired nationalists throughout the Middle East by fighting the British and the French during the Suez Crisis of 1956, modernizing Egypt, and uniting the Arab world politically.

In 1958, a year after Saddam had joined the Ba'ath party, army officers led by General Abd al-Karim Qasim overthrew Faisal II of Iraq in the 14 July Revolution.

Of the 16 members of Qasim's cabinet, 12 were Ba'ath Party members; however, the party turned against Qasim due to his refusal to join Gamal Abdel Nasser's United Arab Republic (UAR). To strengthen his own position within the government, Qasim created an alliance with the Iraqi Communist Party, which was opposed to any notion of pan-Arabism. Later that year, the Ba'ath Party leadership was planning to assassinate Qasim. Saddam was a leading member of the operation. At the time, the Ba'ath Party was more of an ideological experiment than a strong anti-government fighting machine. The majority of its members were either educated professionals or students, and Saddam fit the bill. The choice of Saddam was, according to journalist Con Coughlin, "hardly surprising." The idea of assassinating Qasim may have been Nasser's, and there is speculation that some of those who participated in the operation received training in Damascus, which was then part of the UAR. However, "no evidence has ever been produced to implicate Nasser directly in the plot." Saddam himself is not believed to have received any training outside of Iraq, as he was a late addition to the assassination team.

The assassins planned to ambush Qasim at Al-Rashid Street on 7 October 1959: one man was to kill those sitting at the back of the car, the rest killing those in front. During the ambush it is claimed that Saddam began shooting prematurely, which disorganised the whole operation. Qasim's chauffeur was killed, and Qasim was hit in the arm and shoulder. The assassins believed they had killed him and quickly retreated to their headquarters, but Qasim survived. At the time of the attack the Ba'ath Party had fewer than 1,000 members. Saddam's role in the failed assassination became a crucial part of his public image for decades. Kanan Makiya recounts:
The man and the myth merge in this episode. His biography—and Iraqi television, which stages the story ad nauseam—tells of his familiarity with guns from the age of ten; his fearlessness and loyalty to the party during the 1959 operation; his bravery in saving his comrades by commandeering a car at gunpoint; the bullet that was gouged out of his flesh under his direction in hiding; the iron discipline that led him to draw a gun on weaker comrades who would have dropped off a seriously wounded member of the hit team at a hospital; the calculating shrewdness that helped him save himself minutes before the police broke in leaving his wounded comrades behind; and finally the long trek of a wounded man from house to house, city to town, across the desert to refuge in Syria.

Some of the plotters (including Saddam) quickly managed to leave the country for Syria, the spiritual home of Ba'athist ideology. There Saddam was given full membership in the party by Michel Aflaq. Some members of the operation were arrested and taken into custody by the Iraqi government. At the show trial, six of the defendants were given death sentences; for unknown reasons the sentences were not carried out. Aflaq, the leader of the Ba'athist movement, organised the expulsion of leading Iraqi Ba'athist members, such as Fuad al-Rikabi, on the grounds that the party should not have initiated the attempt on Qasim's life. At the same time, Aflaq secured seats in the Iraqi Ba'ath leadership for his supporters, one of them being Saddam. Saddam moved from Syria to Egypt itself in February 1960, and he continued to live there until 1963, graduating from high school in 1961 and unsuccessfully pursuing a law degree.

Army officers with ties to the Ba'ath Party overthrew Qasim in the Ramadan Revolution coup of February 1963. Ba'athist leaders were appointed to the cabinet and Abdul Salam Arif became president. Arif dismissed and arrested the Ba'athist leaders later that year in the November 1963 Iraqi coup d'état. Being exiled in Egypt at the time, Saddam played no role in the 1963 coup or the brutal anti-communist purge that followed; although he returned to Iraq after the coup, Saddam remained "on the fringes of the newly installed Ba'thi administration and [had] to content himself with the minor position of a member of the Party's central bureau for peasants," in the words of Efraim Karsh and Inari Rautsi. Unlike during the Qasim years, Saddam remained in Iraq following Arif's anti-Ba'athist purge in November 1963, and became involved in planning to assassinate Arif. In marked contrast to Qasim, Saddam knew that he faced no death penalty from Arif's government and knowingly accepted the risk of being arrested rather than fleeing to Syria again. Saddam was arrested in October 1964 and served approximately two years in prison before escaping in 1966. In 1966, Ahmed Hassan al-Bakr appointed him Deputy Secretary of the Regional Command. Saddam, who would prove to be a skilled organiser, revitalised the party. He was elected to the Regional Command, as the story goes, with help from Michel Aflaq—the founder of Ba'athist thought. In September 1966, Saddam initiated an extraordinary challenge to Syrian domination of the Ba'ath Party in response to the Marxist takeover of the Syrian Ba'ath earlier that year, resulting in the Party's formalized split into two separate factions. Saddam then created a Ba'athist security service, which he alone controlled.

In July 1968, Saddam participated in a bloodless coup led by Ahmed Hassan al-Bakr that overthrew Abdul Rahman Arif, Salam Arif's brother and successor. While Saddam's role in the coup was not hugely significant (except in the official account), Saddam planned and carried out the subsequent purge of the non-Ba'athist faction led by Prime Minister Abd ar-Razzaq an-Naif, whose support had been essential to the coup's success. According to a semi-official biography, Saddam personally led Naif at gunpoint to the plane that escorted him out of Iraq. Arif was given refuge in London and then Istanbul. Al-Bakr was named president and Saddam was named his deputy, and deputy chairman of the Ba'athist Revolutionary Command Council. According to biographers, Saddam never forgot the tensions within the first Ba'athist government, which formed the basis for his measures to promote Ba'ath party unity as well as his resolve to maintain power and programs to ensure social stability. Although Saddam was al-Bakr's deputy, he was a strong behind-the-scenes party politician. Al-Bakr was the older and more prestigious of the two, but by 1969 Saddam clearly had become the moving force behind the party.

In the late 1960s and early 1970s, as vice chairman of the Revolutionary Command Council, formally al-Bakr's second-in-command, Saddam built a reputation as a progressive, effective politician. At this time, Saddam moved up the ranks in the new government by aiding attempts to strengthen and unify the Ba'ath party and taking a leading role in addressing the country's major domestic problems and expanding the party's following.

After the Ba'athists took power in 1968, Saddam focused on attaining stability in a nation riddled with profound tensions. Long before Saddam, Iraq had been split along social, ethnic, religious, and economic fault lines: Sunni versus Shi'ite, Arab versus Kurd, tribal chief versus urban merchant, nomad versus peasant. The desire for stable rule in a country rife with factionalism led Saddam to pursue both massive repression and the improvement of living standards.

Saddam actively fostered the modernization of the Iraqi economy along with the creation of a strong security apparatus to prevent coups within the power structure and insurrections apart from it. Ever concerned with broadening his base of support among the diverse elements of Iraqi society and mobilizing mass support, he closely followed the administration of state welfare and development programs.

At the center of this strategy was Iraq's oil. On 1 June 1972, Saddam oversaw the seizure of international oil interests, which, at the time, dominated the country's oil sector. A year later, world oil prices rose dramatically as a result of the 1973 energy crisis, and skyrocketing revenues enabled Saddam to expand his agenda.
Within just a few years, Iraq was providing social services that were unprecedented among Middle Eastern countries. Saddam established and controlled the "National Campaign for the Eradication of Illiteracy" and the campaign for "Compulsory Free Education in Iraq," and largely under his auspices, the government established universal free schooling up to the highest education levels; hundreds of thousands learned to read in the years following the initiation of the program. The government also supported families of soldiers, granted free hospitalization to everyone, and gave subsidies to farmers. Iraq created one of the most modernized public-health systems in the Middle East, earning Saddam an award from the United Nations Educational, Scientific and Cultural Organization (UNESCO).

With the help of increasing oil revenues, Saddam diversified the largely oil-based Iraqi economy. Saddam implemented a national infrastructure campaign that made great progress in building roads, promoting mining, and developing other industries. The campaign helped Iraq's energy industries. Electricity was brought to nearly every city in Iraq, and many outlying areas. Before the 1970s, most of Iraq's people lived in the countryside and roughly two-thirds were peasants. This number would decrease quickly during the 1970s as global oil prices helped revenues to rise from less than a half billion dollars to tens of billions of dollars and the country invested into industrial expansion.

The oil revenue benefited Saddam politically. According to "The Economist", "Much as Adolf Hitler won early praise for galvanising German industry, ending mass unemployment and building autobahns, Saddam earned admiration abroad for his deeds. He had a good instinct for what the "Arab street" demanded, following the decline in Egyptian leadership brought about by the trauma of Israel's six-day victory in the 1967 war, the death of the pan-Arabist hero, Gamal Abdul Nasser, in 1970, and the "traitorous" drive by his successor, Anwar Sadat, to sue for peace with the Jewish state. Saddam's self-aggrandising propaganda, with himself posing as the defender of Arabism against Jewish or Persian intruders, was heavy-handed, but consistent as a drumbeat. It helped, of course, that his mukhabarat (secret police) put dozens of Arab news editors, writers and artists on the payroll."

In 1972, Saddam signed a 15-year Treaty of Friendship and Cooperation with the Soviet Union. According to historian Charles R. H. Tripp, the treaty upset "the U.S.-sponsored security system established as part of the Cold War in the Middle East. It appeared that any enemy of the Baghdad regime was a potential ally of the United States." In response, the U.S. covertly financed Kurdish rebels led by Mustafa Barzani during the Second Iraqi–Kurdish War; the Kurds were defeated in 1975, leading to the forcible relocation of hundreds of thousands of Kurdish civilians.

Saddam focused on fostering loyalty to the Ba'athists in the rural areas. After nationalizing foreign oil interests, Saddam supervised the modernization of the countryside, mechanizing agriculture on a large scale, and distributing land to peasant farmers. The Ba'athists established farm cooperatives and the government also doubled expenditures for agricultural development in 1974–1975. Saddam's welfare programs were part of a combination of "carrot and stick" tactics to enhance support for Saddam. The state-owned banks were put under his thumb. Lending was based on cronyism. Development went forward at such a fevered pitch that two million people from other Arab countries and even Yugoslavia worked in Iraq to meet the growing demand for labor.

In 1976, Saddam rose to the position of general in the Iraqi armed forces, and rapidly became the strongman of the government. As the ailing, elderly al-Bakr became unable to execute his duties, Saddam took on an increasingly prominent role as the face of the government both internally and externally. He soon became the architect of Iraq's foreign policy and represented the nation in all diplomatic situations. He was the "de facto" leader of Iraq some years before he formally came to power in 1979. He slowly began to consolidate his power over Iraq's government and the Ba'ath party. Relationships with fellow party members were carefully cultivated, and Saddam soon accumulated a powerful circle of support within the party.

In 1979, al-Bakr started to make treaties with Syria, also under Ba'athist leadership, that would lead to unification between the two countries. Syrian President Hafez al-Assad would become deputy leader in a union, and this would drive Saddam to obscurity. Saddam acted to secure his grip on power. He forced the ailing al-Bakr to resign on 16 July 1979, and formally assumed the presidency.

Saddam convened an assembly of Ba'ath party leaders on 22 July 1979. During the assembly, which he ordered videotaped, Saddam claimed to have found a fifth column within the Ba'ath Party and directed Muhyi Abdel-Hussein to read out a confession and the names of 68 alleged co-conspirators. These members were labelled "disloyal" and were removed from the room one by one and taken into custody. After the list was read, Saddam congratulated those still seated in the room for their past and future loyalty. The 68 people arrested at the meeting were subsequently tried together and found guilty of treason. 22 were sentenced to execution. Other high-ranking members of the party formed the firing squad. By 1 August 1979, hundreds of high-ranking Ba'ath party members had been executed.

Iraqi society fissures along lines of language, religion and ethnicity. The Ba'ath Party, secular by nature, adopted Pan-Arab ideologies which in turn were problematic for significant parts of the population. Following the Iranian Revolution of 1979, Iraq faced the prospect of régime change from two Shi'ite factions (Dawa and SCIRI) which aspired to model Iraq on its neighbour Iran as a Shia theocracy. A separate threat to Iraq came from parts of the ethnic Kurdish population of northern Iraq which opposed being part of an Iraqi state and favoured independence (an ongoing ideology which had preceded Ba'ath Party rule). To alleviate the threat of revolution, Saddam afforded certain benefits to the potentially hostile population. Membership in the Ba'ath Party remained open to all Iraqi citizens regardless of background. However, repressive measures were taken against its opponents.

The major instruments for accomplishing this control were the paramilitary and police organizations. Beginning in 1974, Taha Yassin Ramadan (himself a Kurdish Ba'athist), a close associate of Saddam, commanded the People's Army, which had responsibility for internal security. As the Ba'ath Party's paramilitary, the People's Army acted as a counterweight against any coup attempts by the regular armed forces. In addition to the People's Army, the Department of General Intelligence was the most notorious arm of the state-security system, feared for its use of torture and assassination. Barzan Ibrahim al-Tikriti, Saddam's younger half-brother, commanded Mukhabarat. Foreign observers believed that from 1982 this department operated both at home and abroad in its mission to seek out and eliminate Saddam's perceived opponents.

Saddam was notable for using terror against his own people. "The Economist" described Saddam as "one of the last of the 20th century's great dictators, but not the least in terms of egotism, or cruelty, or morbid will to power." Saddam's regime brought about the deaths of at least 250,000 Iraqis and committed war crimes in Iran, Kuwait, and Saudi Arabia. Human Rights Watch and Amnesty International issued regular reports of widespread imprisonment and torture.

As a sign of his consolidation of power, Saddam's personality cult pervaded Iraqi society. He had thousands of portraits, posters, statues and murals erected in his honor all over Iraq. His face could be seen on the sides of office buildings, schools, airports, and shops, as well as on Iraqi currency. Saddam's personality cult reflected his efforts to appeal to the various elements in Iraqi society. This was seen in his variety of apparel: he appeared in the costumes of the Bedouin, the traditional clothes of the Iraqi peasant (which he essentially wore during his childhood), and even Kurdish clothing, but also appeared in Western suits fitted by his favorite tailor, projecting the image of an urbane and modern leader. Sometimes he would also be portrayed as a devout Muslim, wearing full headdress and robe, praying toward Mecca.

He also conducted two show elections, in 1995 and 2002. In the 1995 referendum, conducted on 15 October, he reportedly received 99.96% of the votes in a 99.47% turnout, getting only 3,052 negative votes among an electorate of 8.4 million. In the 15 October 2002 referendum he officially achieved 100% of approval votes and 100% turnout, as the electoral commission reported the next day that every one of the 11,445,638 eligible voters cast a "Yes" vote for the president.

He erected statues around the country, which Iraqis toppled after his fall.

Iraq's relations with the Arab world have been extremely varied. Relations between Iraq and Egypt violently ruptured in 1977, when the two nations broke relations with each other following Iraq's criticism of Egyptian President Anwar Sadat's peace initiatives with Israel. In 1978, Baghdad hosted an Arab League summit that condemned and ostracized Egypt for accepting the Camp David Accords. However, Egypt's strong material and diplomatic support for Iraq in the war with Iran led to warmer relations and numerous contacts between senior officials, despite the continued absence of ambassadorial-level representation. Since 1983, Iraq has repeatedly called for restoration of Egypt's "natural role" among Arab countries.
Saddam developed a reputation for liking expensive goods, such as his diamond-coated Rolex wristwatch, and sent copies of them to his friends around the world. To his ally Kenneth Kaunda Saddam once sent a Boeing 747 full of presents—rugs, televisions, ornaments.

Saddam enjoyed a close relationship with Russian intelligence agent Yevgeny Primakov that dated back to the 1960s; Primakov may have helped Saddam to stay in power in 1991.

Saddam visited only two Western countries. The first visit took place in December 1974, when the dictator of Spain, Francisco Franco, invited him to Madrid and he visited Granada, Córdoba and Toledo. In September 1975 he met with Prime Minister Jacques Chirac in Paris, France.

Several Iraqi leaders, Lebanese arms merchant Sarkis Soghanalian and others have claimed that Saddam financed Chirac's party. In 1991 Saddam threatened to expose those who had taken largesse from him: "From Mr. Chirac to Mr. Chevènement, politicians and economic leaders were in open competition to spend time with us and flatter us. We have now grasped the reality of the situation. If the trickery continues, we will be forced to unmask them, all of them, before the French public." France armed Saddam and it was Iraq's largest trade partner throughout Saddam's rule. Seized documents show how French officials and businessmen close to Chirac, including Charles Pasqua, his former interior minister, personally benefitted from the deals with Saddam.

Because Saddam Hussein rarely left Iraq, Tariq Aziz, one of Saddam's aides, traveled abroad extensively and represented Iraq at many diplomatic meetings. In foreign affairs, Saddam sought to have Iraq play a leading role in the Middle East. Iraq signed an aid pact with the Soviet Union in 1972, and arms were sent along with several thousand advisers. However, the 1978 crackdown on Iraqi Communists and a shift of trade toward the West strained Iraqi relations with the Soviet Union; Iraq then took on a more Western orientation until the Gulf War in 1991.

After the oil crisis of 1973, France had changed to a more pro-Arab policy and was accordingly rewarded by Saddam with closer ties. He made a state visit to France in 1975, cementing close ties with some French business and ruling political circles. In 1975 Saddam negotiated an accord with Iran that contained Iraqi concessions on border disputes. In return, Iran agreed to stop supporting opposition Kurds in Iraq. Saddam led Arab opposition to the Camp David Accords between Egypt and Israel (1979).

Saddam initiated Iraq's nuclear enrichment project in the 1980s, with French assistance. The first Iraqi nuclear reactor was named by the French "Osirak." Osirak was destroyed on 7 June 1981 by an Israeli air strike (Operation Opera).

Nearly from its founding as a modern state in 1920, Iraq has had to deal with Kurdish separatists in the northern part of the country. Saddam did negotiate an agreement in 1970 with separatist Kurdish leaders, giving them autonomy, but the agreement broke down. The result was brutal fighting between the government and Kurdish groups and even Iraqi bombing of Kurdish villages in Iran, which caused Iraqi relations with Iran to deteriorate. However, after Saddam had negotiated the 1975 treaty with Iran, the Shah withdrew support for the Kurds, who suffered a total defeat.

In early 1979, Iran's Shah Mohammad Reza Pahlavi was overthrown by the Islamic Revolution, thus giving way to an Islamic republic led by the Ayatollah Ruhollah Khomeini. The influence of revolutionary Shi'ite Islam grew apace in the region, particularly in countries with large Shi'ite populations, especially Iraq. Saddam feared that radical Islamic ideas—hostile to his secular rule—were rapidly spreading inside his country among the majority Shi'ite population.

There had also been bitter enmity between Saddam and Khomeini since the 1970s. Khomeini, having been exiled from Iran in 1964, took up residence in Iraq, at the Shi'ite holy city of An Najaf. There he involved himself with Iraqi Shi'ites and developed a strong, worldwide religious and political following against the Iranian Government, which Saddam tolerated. However, when Khomeini began to urge the Shi'ites there to overthrow Saddam and under pressure from the Shah, who had agreed to a rapprochement between Iraq and Iran in 1975, Saddam agreed to expel Khomeini in 1978 to France. However this turned out to be an imminent failure and a political catalyst, for Khomeini had access to more media connections and also collaborated with a much larger Iranian community under his support which he used to his advantage.

After Khomeini gained power, skirmishes between Iraq and revolutionary Iran occurred for ten months over the sovereignty of the disputed Shatt al-Arab waterway, which divides the two countries. During this period, Saddam Hussein publicly maintained that it was in Iraq's interest not to engage with Iran, and that it was in the interests of both nations to maintain peaceful relations. However, in a private meeting with Salah Omar al-Ali, Iraq's permanent ambassador to the United Nations, he revealed that he intended to invade and occupy a large part of Iran within months. Later (probably to appeal for support from the United States and most Western nations), he would make toppling the Islamic government one of his intentions as well.
Iraq invaded Iran, first attacking Mehrabad Airport of Tehran and then entering the oil-rich Iranian land of Khuzestan, which also has a sizable Arab minority, on 22 September 1980 and declared it a new province of Iraq. With the support of the Arab states, the United States, and Europe, and heavily financed by the Arab states of the Persian Gulf, Saddam Hussein had become "the defender of the Arab world" against a revolutionary Iran. The only exception was the Soviet Union, who initially refused to supply Iraq on the basis of neutrality in the conflict, although in his memoirs, Mikhail Gorbachev claimed that Leonid Brezhnev refused to aid Saddam over infuriation of Saddam's treatment of Iraqi communists. Consequently, many viewed Iraq as "an agent of the civilized world." The blatant disregard of international law and violations of international borders were ignored. Instead Iraq received economic and military support from its allies, who overlooked Saddam's use of chemical warfare against the Kurds and the Iranians, in addition to Iraq's efforts to develop nuclear weapons.

In the first days of the war, there was heavy ground fighting around strategic ports as Iraq launched an attack on Khuzestan. After making some initial gains, Iraq's troops began to suffer losses from human wave attacks by Iran. By 1982, Iraq was on the defensive and looking for ways to end the war.

At this point, Saddam asked his ministers for candid advice. Health Minister Dr. Riyadh Ibrahim suggested that Saddam temporarily step down to promote peace negotiations. Initially, Saddam Hussein appeared to take in this opinion as part of his cabinet democracy. A few weeks later, Dr. Ibrahim was sacked when held responsible for a fatal incident in an Iraqi hospital where a patient died from intravenous administration of the wrong concentration of potassium supplement.

Dr. Ibrahim was arrested a few days after he started his new life as a sacked minister. He was known to have publicly declared before that arrest that he was "glad that he got away alive." Pieces of Ibrahim's dismembered body were delivered to his wife the next day.

Iraq quickly found itself bogged down in one of the longest and most destructive wars of attrition of the 20th century. During the war, Iraq used chemical weapons against Iranian forces fighting on the southern front and Kurdish separatists who were attempting to open up a northern front in Iraq with the help of Iran. These chemical weapons were developed by Iraq from materials and technology supplied primarily by West German companies as well as using dual-use technology imported following the Reagan administration's lifting of export restrictions. The United States also supplied Iraq with "satellite photos showing Iranian deployments." In a US bid to open full diplomatic relations with Iraq, the country was removed from the US list of State Sponsors of Terrorism. Ostensibly, this was because of improvement in the regime's record, although former United States Assistant Secretary of Defense Noel Koch later stated, "No one had any doubts about [the Iraqis'] continued involvement in terrorism ... The real reason was to help them succeed in the war against Iran." The Soviet Union, France, and China together accounted for over 90% of the value of Iraq's arms imports between 1980 and 1988.

Saddam reached out to other Arab governments for cash and political support during the war, particularly after Iraq's oil industry severely suffered at the hands of the Iranian navy in the Persian Gulf. Iraq successfully gained some military and financial aid, as well as diplomatic and moral support, from the Soviet Union, China, France, and the United States, which together feared the prospects of the expansion of revolutionary Iran's influence in the region. The Iranians, demanding that the international community should force Iraq to pay war reparations to Iran, refused any suggestions for a cease-fire. Despite several calls for a ceasefire by the United Nations Security Council, hostilities continued until 20 August 1988.

On 16 March 1988, the Kurdish town of Halabja was attacked with a mix of mustard gas and nerve agents, killing 5,000 civilians, and maiming, disfiguring, or seriously debilitating 10,000 more. ("see Halabja poison gas attack") The attack occurred in conjunction with the 1988 al-Anfal Campaign designed to reassert central control of the mostly Kurdish population of areas of northern Iraq and defeat the Kurdish peshmerga rebel forces. The United States now maintains that Saddam ordered the attack to terrorize the Kurdish population in northern Iraq, but Saddam's regime claimed at the time that Iran was responsible for the attack which some including the U.S. supported until several years later.

The bloody eight-year war ended in a stalemate. There were hundreds of thousands of casualties with estimates of up to one million dead. Neither side had achieved what they had originally desired and the borders were left nearly unchanged. The southern, oil rich and prosperous Khuzestan and Basra area (the main focus of the war, and the primary source of their economies) were almost completely destroyed and were left at the pre-1979 border, while Iran managed to make some small gains on its borders in the Northern Kurdish area. Both economies, previously healthy and expanding, were left in ruins.

Saddam borrowed tens of billions of dollars from other Arab states and a few billions from elsewhere during the 1980s to fight Iran, mainly to prevent the expansion of Shi'a radicalism. However, this had proven to completely backfire both on Iraq and on the part of the Arab states, for Khomeini was widely perceived as a hero for managing to defend Iran and maintain the war with little foreign support against the heavily backed Iraq and only managed to boost Islamic radicalism not only within the Arab states, but within Iraq itself, creating new tensions between the Sunni Ba'ath Party and the majority Shi'a population. Faced with rebuilding Iraq's infrastructure and internal resistance, Saddam desperately re-sought cash, this time for postwar reconstruction.

The Al-Anfal Campaign was a genocidal campaign against the Kurdish people (and many others) in Kurdish regions of Iraq led by the government of Saddam Hussein and headed by Ali Hassan al-Majid. The campaign takes its name from Surat al-Anfal in the Qur'an, which was used as a code name by the former Iraqi Ba'athist administration for a series of attacks against the "peshmerga" rebels and the mostly Kurdish civilian population of rural Northern Iraq, conducted between 1986 and 1989 culminating in 1988. This campaign also targeted Shabaks and Yazidis, Assyrians, Turkoman people and Mandeans and many villages belonging to these ethnic groups were also destroyed. Human Rights Watch estimates that between 50,000 and 100,000 people were killed. Some Kurdish sources put the number higher, estimating that 182,000 Kurds were killed.

The end of the war with Iran served to deepen latent tensions between Iraq and its wealthy neighbor Kuwait. Saddam urged the Kuwaitis to waive the Iraqi debt accumulated in the war, some $30 billion, but they refused.

Saddam pushed oil-exporting countries to raise oil prices by cutting back production; Kuwait refused, however. In addition to refusing the request, Kuwait spearheaded the opposition in OPEC to the cuts that Saddam had requested. Kuwait was pumping large amounts of oil, and thus keeping prices low, when Iraq needed to sell high-priced oil from its wells to pay off a huge debt.

Saddam had always argued that Kuwait was historically an integral part of Iraq, and that Kuwait had only come into being through the maneuverings of British imperialism; this echoed a belief that Iraqi nationalists had voiced for the past 50 years. This belief was one of the few articles of faith uniting the political scene in a nation rife with sharp social, ethnic, religious, and ideological divides.

The extent of Kuwaiti oil reserves also intensified tensions in the region. The oil reserves of Kuwait (with a population of 2 million next to Iraq's 25) were roughly equal to those of Iraq. Taken together, Iraq and Kuwait sat on top of some 20 percent of the world's known oil reserves; as an article of comparison, Saudi Arabia holds 25 percent.

Saddam complained to the U.S. State Department that Kuwait had slant drilled oil out of wells that Iraq considered to be within its disputed border with Kuwait. Saddam still had an experienced and well-equipped army, which he used to influence regional affairs. He later ordered troops to the Iraq–Kuwait border.

As Iraq-Kuwait relations rapidly deteriorated, Saddam was receiving conflicting information about how the U.S. would respond to the prospects of an invasion. For one, Washington had been taking measures to cultivate a constructive relationship with Iraq for roughly a decade. The Reagan administration gave Iraq roughly $4 billion in agricultural credits to bolster it against Iran. Saddam's Iraq became "the third-largest recipient of U.S. assistance."

Reacting to Western criticism in April 1990 Saddam threatened to destroy half of Israel with chemical weapons if it moved against Iraq. In May 1990 he criticized U.S. support for Israel warning that "the United States cannot maintain such a policy while professing friendship towards the Arabs." In July 1990 he threatened force against Kuwait and the UAE saying "The policies of some Arab rulers are American ... They are inspired by America to undermine Arab interests and security." The U.S. sent aerial planes and combat ships to the Persian Gulf in response to these threats.

U.S. ambassador to Iraq April Glaspie met with Saddam in an emergency meeting on 25 July 1990, where the Iraqi leader attacked American policy with regards to Kuwait and the United Arab Emirates:

Glaspie replied:

Saddam stated that he would attempt last-ditch negotiations with the Kuwaitis but Iraq "would not accept death."

U.S. officials attempted to maintain a conciliatory line with Iraq, indicating that while George H. W. Bush and James Baker did not want force used, they would not take any position on the Iraq–Kuwait boundary dispute and did not want to become involved.

Later, Iraq and Kuwait met for a final negotiation session, which failed. Saddam then sent his troops into Kuwait. As tensions between Washington and Saddam began to escalate, the Soviet Union, under Mikhail Gorbachev, strengthened its military relationship with the Iraqi leader, providing him military advisers, arms and aid.

On 2 August 1990, Saddam invaded Kuwait, initially claiming assistance to "Kuwaiti revolutionaries," thus sparking an international crisis. On 4 August an Iraqi-backed "Provisional Government of Free Kuwait" was proclaimed, but a total lack of legitimacy and support for it led to an 8 August announcement of a "merger" of the two countries. On 28 August Kuwait formally became the 19th Governorate of Iraq. Just two years after the 1988 Iraq and Iran truce, "Saddam Hussein did what his Gulf patrons had earlier paid him to prevent." Having removed the threat of Iranian fundamentalism he "overran Kuwait and confronted his Gulf neighbors in the name of Arab nationalism and Islam."

When later asked why he invaded Kuwait, Saddam first claimed that it was because Kuwait was rightfully Iraq's 19th province and then said "When I get something into my head I act. That's just the way I am." After Saddam's seizure of Kuwait in August 1990, a UN coalition led by the United States drove Iraq's troops from Kuwait in February 1991. The ability for Saddam Hussein to pursue such military aggression was from a "military machine paid for in large part by the tens of billions of dollars Kuwait and the Gulf states had poured into Iraq and the weapons and technology provided by the Soviet Union, Germany, and France."

Shortly before he invaded Kuwait, he shipped 100 new Mercedes 200 Series cars to top editors in Egypt and Jordan. Two days before the first attacks, Saddam reportedly offered Egypt's Hosni Mubarak 50 million dollars in cash, "ostensibly for grain."

U.S. President George H. W. Bush responded cautiously for the first several days. On one hand, Kuwait, prior to this point, had been a virulent enemy of Israel and was the Persian Gulf monarchy that had the most friendly relations with the Soviets. On the other hand, Washington foreign policymakers, along with Middle East experts, military critics, and firms heavily invested in the region, were extremely concerned with stability in this region. The invasion immediately triggered fears that the world's price of oil, and therefore control of the world economy, was at stake. Britain profited heavily from billions of dollars of Kuwaiti investments and bank deposits. Bush was perhaps swayed while meeting with British prime minister Margaret Thatcher, who happened to be in the U.S. at the time.

Cooperation between the United States and the Soviet Union made possible the passage of resolutions in the United Nations Security Council giving Iraq a deadline to leave Kuwait and approving the use of force if Saddam did not comply with the timetable. U.S. officials feared Iraqi retaliation against oil-rich Saudi Arabia, since the 1940s a close ally of Washington, for the Saudis' opposition to the invasion of Kuwait. Accordingly, the U.S. and a group of allies, including countries as diverse as Egypt, Syria and Czechoslovakia, deployed a massive number of troops along the Saudi border with Kuwait and Iraq in order to encircle the Iraqi army, the largest in the Middle East.

Saddam's officers looted Kuwait, stripping even the marble from its palaces to move it to Saddam's own palace.

During the period of negotiations and threats following the invasion, Saddam focused renewed attention on the Palestinian problem by promising to withdraw his forces from Kuwait if Israel would relinquish the occupied territories in the West Bank, the Golan Heights, and the Gaza Strip. Saddam's proposal further split the Arab world, pitting U.S.- and Western-supported Arab states against the Palestinians. The allies ultimately rejected any linkage between the Kuwait crisis and Palestinian issues.

Saddam ignored the Security Council deadline. Backed by the Security Council, a U.S.-led coalition launched round-the-clock missile and aerial attacks on Iraq, beginning 16 January 1991. Israel, though subjected to attack by Iraqi missiles, refrained from retaliating in order not to provoke Arab states into leaving the coalition. A ground force consisting largely of U.S. and British armoured and infantry divisions ejected Saddam's army from Kuwait in February 1991 and occupied the southern portion of Iraq as far as the Euphrates.

On 6 March 1991, Bush announced "What is at stake is more than one small country, it is a big idea—a new world order, where diverse nations are drawn together in common cause to achieve the universal aspirations of mankind: peace and security, freedom, and the rule of law."

In the end, the out-numbered and under-equipped Iraqi army proved unable to compete on the battlefield with the highly mobile coalition land forces and their overpowering air support. Some 175,000 Iraqis were taken prisoner and casualties were estimated at over 85,000. As part of the cease-fire agreement, Iraq agreed to scrap all poison gas and germ weapons and allow UN observers to inspect the sites. UN trade sanctions would remain in effect until Iraq complied with all terms. Saddam publicly claimed victory at the end of the war.

Iraq's ethnic and religious divisions, together with the brutality of the conflict that this had engendered, laid the groundwork for postwar rebellions. In the aftermath of the fighting, social and ethnic unrest among Shi'ite Muslims, Kurds, and dissident military units threatened the stability of Saddam's government. Uprisings erupted in the Kurdish north and Shi'a southern and central parts of Iraq, but were ruthlessly repressed. Uprisings in 1991 led to the death of 100,000–180,000 people, mostly civilians.

The United States, which had urged Iraqis to rise up against Saddam, did nothing to assist the rebellions. The Iranians, despite the widespread Shi'ite rebellions, had no interest in provoking another war, while Turkey opposed any prospect of Kurdish independence, and the Saudis and other conservative Arab states feared an Iran-style Shi'ite revolution. Saddam, having survived the immediate crisis in the wake of defeat, was left firmly in control of Iraq, although the country never recovered either economically or militarily from the Gulf War.

Saddam routinely cited his survival as "proof" that Iraq had in fact won the war against the U.S. This message earned Saddam a great deal of popularity in many sectors of the Arab world. John Esposito, however, claims that "Arabs and Muslims were pulled in two directions. That they rallied not so much to Saddam Hussein as to the bipolar nature of the confrontation (the West versus the Arab Muslim world) and the issues that Saddam proclaimed: Arab unity, self-sufficiency, and social justice." As a result, Saddam Hussein appealed to many people for the same reasons that attracted more and more followers to Islamic revivalism and also for the same reasons that fueled anti-Western feelings.

As one U.S. Muslim observer noted: "People forgot about Saddam's record and concentrated on America ... Saddam Hussein might be wrong, but it is not America who should correct him." A shift was, therefore, clearly visible among many Islamic movements in the post war period "from an initial Islamic ideological rejection of Saddam Hussein, the secular persecutor of Islamic movements, and his invasion of Kuwait to a more populist Arab nationalist, anti-imperialist support for Saddam (or more precisely those issues he represented or championed) and the condemnation of foreign intervention and occupation."

Saddam, therefore, increasingly portrayed himself as a devout Muslim, in an effort to co-opt the conservative religious segments of society. Some elements of Sharia law were re-introduced, and the ritual phrase "Allahu Akbar" ("God is great"), in Saddam's handwriting, was added to the national flag. Saddam also commissioned the production of a "Blood Qur'an," written using 27 litres of his own blood, to thank God for saving him from various dangers and conspiracies.

The United Nations sanctions placed upon Iraq when it invaded Kuwait were not lifted, blocking Iraqi oil exports. During the late 1990s, the UN considered relaxing the sanctions imposed because of the hardships suffered by ordinary Iraqis. Studies dispute the number of people who died in south and central Iraq during the years of the sanctions. On 9 December 1996, Saddam's government accepted the Oil-for-Food Programme that the UN had first offered in 1992.

Relations between the United States and Iraq remained tense following the Gulf War. The U.S. launched a missile attack aimed at Iraq's intelligence headquarters in Baghdad 26 June 1993, citing evidence of repeated Iraqi violations of the "no fly zones" imposed after the Gulf War and for incursions into Kuwait. U.S. officials continued to accuse Saddam of violating the terms of the Gulf War's cease fire, by developing weapons of mass destruction and other banned weaponry, and violating the UN-imposed sanctions. Also during the 1990s, President Bill Clinton maintained sanctions and ordered air strikes in the "Iraqi no-fly zones" (Operation Desert Fox), in the hope that Saddam would be overthrown by political enemies inside Iraq. Western charges of Iraqi resistance to UN access to suspected weapons were the pretext for crises between 1997 and 1998, culminating in intensive U.S. and British missile strikes on Iraq, 16–19 December 1998. After two years of intermittent activity, U.S. and British warplanes struck harder at sites near Baghdad in February 2001. Former CIA case officer Robert Baer reports that he "tried to assassinate" Saddam in 1995, amid "a decade-long effort to encourage a military coup in Iraq."

Saddam continued involvement in politics abroad. Video tapes retrieved after show his intelligence chiefs meeting with Arab journalists, including a meeting with the former managing director of Al-Jazeera, Mohammed Jassem al-Ali, in 2000. In the video Saddam's son Uday advised al-Ali about hires in Al-Jazeera: "During your last visit here along with your colleagues we talked about a number of issues, and it does appear that you indeed were listening to what I was saying since changes took place and new faces came on board such as that lad, Mansour." He was later sacked by Al-Jazeera.

In 2002, Austrian prosecutors investigated Saddam government's transactions with Fritz Edlinger that possibly violated Austrian money laundering and embargo regulations. Fritz Edlinger, president of the "General Secretary of the Society for Austro-Arab relations" (GÖAB) and a former member of Socialist International's Middle East Committee, was an outspoken supporter of Saddam Hussein. In 2005, an Austrian journalist revealed that Fritz Edlinger's GÖAB had received $100,000 from an Iraqi front company as well as donations from Austrian companies soliciting business in Iraq.

In 2002, a resolution sponsored by the European Union was adopted by the Commission for Human Rights, which stated that there had been no improvement in the human rights crisis in Iraq. The statement condemned President Saddam Hussein's government for its "systematic, widespread and extremely grave violations of human rights and international humanitarian law." The resolution demanded that Iraq immediately put an end to its "summary and arbitrary executions ... the use of rape as a political tool and all enforced and involuntary disappearances."

Many members of the international community, especially the U.S., continued to view Saddam as a bellicose tyrant who was a threat to the stability of the region. In his January 2002 state of the union address to Congress, President George W. Bush spoke of an "axis of evil" consisting of Iran, North Korea, and Iraq. Moreover, Bush announced that he would possibly take action to topple the Iraqi government, because of the threat of its weapons of mass destruction. Bush stated that "The Iraqi regime has plotted to develop anthrax, and nerve gas, and nuclear weapons for over a decade ... Iraq continues to flaunt its hostility toward America and to support terror."

After the passing of United Nations Security Council Resolution 1441, which demanded that Iraq give "immediate, unconditional and active cooperation" with UN and IAEA inspections, Saddam allowed U.N. weapons inspectors led by Hans Blix to return to Iraq. During the renewed inspections beginning in November 2002, Blix found no stockpiles of WMD and noted the "proactive" but not always "immediate" Iraqi cooperation as called for by UN Security Council Resolution 1441.

With war still looming on 24 February 2003, Saddam Hussein took part in an interview with CBS News reporter Dan Rather. Talking for more than three hours, he denied possessing any weapons of mass destruction, or any other weapons prohibited by UN guidelines. He also expressed a wish to have a live televised debate with George W. Bush, which was declined. It was his first interview with a U.S. reporter in over a decade. CBS aired the taped interview later that week. Saddam Hussein later told an FBI interviewer that he once left open the possibility that Iraq possessed weapons of mass destruction in order to appear strong against Iran.

The Iraqi government and military collapsed within three weeks of the beginning of the U.S.-led 2003 invasion of Iraq on 20 March. By the beginning of April, U.S.-led forces occupied much of Iraq. The resistance of the much-weakened Iraqi Army either crumbled or shifted to guerrilla tactics, and it appeared that Saddam had lost control of Iraq. He was last seen in a video which purported to show him in the Baghdad suburbs surrounded by supporters. When Baghdad fell to U.S.-led forces on 9 April, marked symbolically by the toppling of his statue, Saddam was nowhere to be found.

In April 2003, Saddam's whereabouts remained in question during the weeks following the fall of Baghdad and the conclusion of the major fighting of the war. Various sightings of Saddam were reported in the weeks following the war, but none was authenticated. At various times Saddam released audio tapes promoting popular resistance to his ousting.

Saddam was placed at the top of the "U.S. list of most-wanted Iraqis." In July 2003, his sons Uday and Qusay and 14-year-old grandson Mustapha were killed in a three-hour gunfight with U.S. forces.

On 13 December 2003, in Operation Red Dawn, Saddam Hussein was captured by American forces after being found hiding in a hole in the ground near a farmhouse in ad-Dawr, near Tikrit. Following his capture, Saddam was transported to a U.S. base near Tikrit, and later taken to the American base near Baghdad. Documents obtained and released by the National Security Archive detail FBI interviews and conversations with Hussein while he was in U.S. custody. On 14 December, U.S. administrator in Iraq Paul Bremer confirmed that Saddam Hussein had indeed been captured at a farmhouse in ad-Dawr near Tikrit. Bremer presented video footage of Saddam in custody.

Saddam was shown with a full beard and hair longer than his familiar appearance. He was described by U.S. officials as being in good health. Bremer reported plans to put Saddam on trial, but claimed that the details of such a trial had not yet been determined. Iraqis and Americans who spoke with Saddam after his capture generally reported that he remained self-assured, describing himself as a "firm, but just leader."

British tabloid newspaper "The Sun" posted a picture of Saddam wearing white briefs on the front cover of a newspaper. Other photographs inside the paper show Saddam washing his trousers, shuffling, and sleeping. The United States government stated that it considered the release of the pictures a violation of the Geneva Convention, and that it would investigate the photographs. During this period Saddam was interrogated by FBI agent George Piro.

The guards at the Baghdad detention facility called their prisoner "Vic," which stands for 'Very Important Criminal', and let him plant a small garden near his cell. The nickname and the garden are among the details about the former Iraqi leader that emerged during a March 2008 tour of the Baghdad prison and cell where Saddam slept, bathed, and kept a journal and wrote poetry in the final days before his execution; he was concerned to ensure his legacy and how the history would be told. The tour was conducted by U.S. Marine Maj. Gen. Doug Stone, overseer of detention operations for the U.S. military in Iraq at the time.

On 30 June 2004, Saddam Hussein, held in custody by U.S. forces at the U.S. base "Camp Cropper," along with 11 other senior Ba'athist leaders, were handed over to the interim Iraqi government to stand trial for crimes against humanity and other offences.

A few weeks later, he was charged by the Iraqi Special Tribunal with crimes committed against residents of Dujail in 1982, following a failed assassination attempt against him. Specific charges included the murder of 148 people, torture of women and children and the illegal arrest of 399 others.
Among the many challenges of the trial were:

On 5 November 2006, Saddam Hussein was found guilty of crimes against humanity and sentenced to death by hanging. Saddam's half-brother, Barzan Ibrahim, and Awad Hamed al-Bandar, head of Iraq's Revolutionary Court in 1982, were convicted of similar charges. The verdict and sentencing were both appealed, but subsequently affirmed by Iraq's Supreme Court of Appeals.

Saddam was hanged on the first day of Eid ul-Adha, 30 December 2006, despite his wish to be executed by firing squad (which he argued was the lawful military capital punishment citing his military position as the commander-in-chief of the Iraqi military). The execution was carried out at Camp Justice, an Iraqi army base in Kadhimiya, a neighborhood of northeast Baghdad.

Saudi Arabia condemned Iraqi authorities for carrying on with the execution on a holy day. A presenter from the Al—Ikhbariya television station officially stated "There is a feeling of surprise and disapproval that the verdict has been applied during the holy months and the first days of Eid al-Adha. Leaders of Islamic countries should show respect for this blessed occasion ... not demean it."

Video of the execution was recorded on a mobile phone and his captors could be heard insulting Saddam. The video was leaked to electronic media and posted on the Internet within hours, becoming the subject of global controversy. It was later claimed by the head guard at the tomb where his remains lay that Saddam's body had been stabbed six times after the execution. Saddam's demeanor while being led to the gallows has been discussed by two witnesses, Iraqi Judge Munir Haddad and Iraqi national security adviser Mowaffak al-Rubaie. The accounts of the two witnesses are contradictory as Haddad describes Saddam as being strong in his final moments whereas al-Rubaie says Saddam was clearly afraid.

Saddam's last words during the execution, "May God’s blessings be upon Muhammad and his household. And may God hasten their appearance and curse their enemies." Then one of the crowd repeatedly said the name of the Iraqi Shiite cleric, Moqtada Al-Sadr. Saddam later said, "Do you consider this manhood?" The crowd shouted, "go to Hell." Saddam replied, "To the hell that is Iraq!?" Again, one of the crowd asked those who shouted to keep quiet for God. Saddam Hussein started recitation of final Muslim prayers, "I bear witness that there is no god but Allah and I testify that Mohammed is the Messenger of Allah." One of the crowd shouted, "The tyrant [dictator] has collapsed!" Saddam said, "May God’s blessings be upon Mohammed and his household (family)". He recited the shahada one and a half times, as while he was about to say ‘Mohammad’ on the second shahada, the trapdoor opened, cutting him off mid-sentence. The rope broke his neck, killing him immediately.

Not long before the execution, Saddam's lawyers released his last letter.

A second unofficial video, apparently showing Saddam's body on a trolley, emerged several days later. It sparked speculation that the execution was carried out incorrectly as Saddam Hussein had a gaping hole in his neck.

Saddam was buried at his birthplace of Al-Awja in Tikrit, Iraq, on 31 December 2006. He was buried 3 km (2 mi) from his sons Uday and Qusay Hussein. His tomb was reported to have been destroyed in March 2015. Before it was destroyed, a Sunni tribal group reportedly removed his body to a secret location, fearful of what might happen.



In August 1995, Raghad and her husband Hussein Kamel al-Majid and Rana and her husband, Saddam Kamel al-Majid, defected to Jordan, taking their children with them. They returned to Iraq when they received assurances that Saddam would pardon them. Within three days of their return in February 1996, both of the Kamel brothers were attacked and killed in a gunfight with other clan members who considered them traitors.

In August 2003, Saddam's daughters Raghad and Rana received sanctuary in Amman, Jordan, where they are currently staying with their nine children. That month, they spoke with CNN and the Arab satellite station Al-Arabiya in Amman. When asked about her father, Raghad told CNN, "He was a very good father, loving, has a big heart." Asked if she wanted to give a message to her father, she said: "I love you and I miss you." Her sister Rana also remarked, "He had so many feelings and he was very tender with all of us."

With the intention of discrediting Saddam Hussein with his supporters, the CIA was considering in 2003 before the Iraq War to make a video in which he (Saddam) would be seen having sex with a teenager.

In 1979, Rev. Jacob Yasso of Chaldean Sacred Heart Church congratulated Saddam Hussein on his presidency. In return, Rev. Yasso said that Saddam Hussein donated US$250,000 to his church, which is made up of at least 1,200 families of Middle Eastern descent. In 1980, Detroit Mayor Coleman Young allowed Rev. Yasso to present the key to the city of Detroit to Saddam Hussein. At the time, Saddam then asked Rev. Yasso, "I heard there was a debt on your church. How much is it?" After the inquiry, Saddam then donated another $200,000 to Chaldean Sacred Heart Church. Rev. Yasso said that Saddam made donations to Chaldean churches all over the world, and even went on record as saying "He's very kind to Christians."






</doc>
<doc id="29491" url="https://en.wikipedia.org/wiki?curid=29491" title="Sonja Henie">
Sonja Henie

Sonja Henie (8 April 1912 – 12 October 1969) was a Norwegian figure skater and film star. She was a three-time Olympic Champion (1928, 1932, 1936) in women's Singles, a ten-time World Champion (1927–1936) and a six-time European Champion (1931–1936). Henie won more Olympic and World titles than any other ladies' figure skater. At the height of her acting career, she was one of the highest-paid stars in Hollywood and starred in a series of box-office hits, including "Thin Ice" (1937), "Happy Landing (1938 film)", "My Lucky Star" (1938), "Second Fiddle" (1939) and "Sun Valley Serenade" (1941).

Henie was born in 1912 in Kristiania (now Oslo) Norway; she was the only daughter of Wilhelm Henie (1872–1937), a prosperous Norwegian furrier, and his wife, Selma Lochmann-Nielsen (1888–1961). In addition to the income from the fur business, both of Henie's parents had inherited wealth. Wilhelm Henie had been a one-time World Cycling Champion and the Henie children were encouraged to take up a variety of sports at a young age. Henie initially showed talent at skiing, then followed her older brother, Leif, to take up figure skating. As a girl Henie also was a nationally ranked tennis player, and a skilled swimmer and equestrienne. Once Henie began serious training as a figure skater, her formal schooling ended. She was educated by tutors, and her father hired the best experts in the world, including the famous Russian ballerina, Tamara Karsavina, to transform his daughter into a sporting celebrity.

Henie won her first major competition, the senior Norwegian championships, at the age of 10. She then placed eighth in a field of eight at the 1924 Winter Olympics, at the age of eleven. During the 1924 program, she skated over to the side of the rink several times to ask her coach for directions, but by the next Olympiad, she needed no such assistance.

Henie won the first of an unprecedented ten consecutive World Figure Skating Championships in 1927 at the age of fourteen. The results of 1927 World Championships, where Henie won in 3–2 decision (or 7 vs. 8 ordinal points) over the defending Olympic and World Champion Herma Szabo of Austria, was controversial, as three of the five judges that gave Henie first-place ordinals were Norwegian (1 + 1 + 1 + 2 + 2 = 7 points) while Szabo received first-place ordinals from an Austrian and a German Judge (1 + 1 + 2 + 2 + 2 = 8 points). Henie went on to win first of her three Olympic gold medals the following year, became one of the youngest figure skating Olympic champions. She defended her Olympic titles in 1932 and in 1936, and her world titles annually until 1936. She also won six consecutive European championships from 1931 to 1936. Henie's unprecedented three Olympic gold medals haven't been matched by any ladies' single skater since; neither are her achievements as ten-time consecutive World Champion. While Irina Slutskaya of Russia won her seventh European Championship in 2006 to become the most successful ladies' skater in European Championships, Henie retains record of most consecutive titles, sharing it with Katarina Witt of Eastern Germany/Germany (1983–1988).

Towards the end of her career, she began to be strongly challenged by younger skaters including Cecilia Colledge, Megan Taylor, and Hedy Stenuf. However, she held off these competitors and went on to win her third Olympic title at the 1936 Winter Olympics, albeit in very controversial circumstances with Cecilia Colledge finishing a very close second. Indeed, after the school figures section at the 1936 Olympic competition, Colledge and Henie were virtually neck and neck with Colledge trailing by just a few points. As Sandra Stevenson recounted in her article in "The Independent" of 21 April 2008, "the closeness [of the competition] infuriated Henie, who, when the result for that section was posted on a wall in the competitors' lounge, swiped the piece of paper and tore it into little pieces. The draw for the free skating [then] came under suspicion after Henie landed the plum position of skating last, while Colledge had to perform second of the 26 competitors. The early start was seen as a disadvantage, with the audience not yet whipped into a clapping frenzy and the judges known to become freer with their higher marks as the event proceeded. Years later, a fairer, staggered draw was adopted to counteract this situation".

During her competitive career, Henie traveled widely and worked with a variety of foreign coaches. At home in Oslo, she trained at Frogner Stadium, where her coaches included Hjørdis Olsen and Oscar Holte. During the latter part of her competitive career she was coached primarily by the American Howard Nicholson in London. In addition to traveling to train and compete, she was much in demand as a performer at figure skating exhibitions in both Europe and North America. Henie became so popular with the public that police had to be called out for crowd control on her appearances in various disparate cities such as Prague and New York City. It was an open secret that, in spite of the strict amateurism requirements of the time, Wilhelm Henie demanded "expense money" for his daughter's skating appearances. Both of Henie's parents had given up their own pursuits in Norway—leaving Leif to run the fur business—in order to accompany Sonja on her travels and act as her managers.

Henie is credited with being the first figure skater to adopt the short skirt costume in figure skating, wear white boots, and make use of dance choreography. Her innovative skating techniques and glamorous demeanor transformed the sport permanently and confirmed its acceptance as a legitimate sport in the Winter Olympics.

After the 1936 World Figure Skating Championships, Henie gave up her amateur status and took up a career as a professional performer in acting and live shows. While still a girl, Henie had decided that she wanted to move to California and become a movie star when her competitive days were over, without considering that her strong accent might hinder her acting ambitions.

In 1936, following a successful ice show in Los Angeles orchestrated by her father to launch her film career, Hollywood studio chief Darryl Zanuck signed her to a long term contract at Twentieth Century Fox, which made her one of the highest-paid actresses of the time. After the success of her first film, "One in a Million" (1936), Henie's position was assured and she became increasingly demanding in her business dealings with Zanuck. Henie also insisted on having total control of the skating numbers in her films such as "Second Fiddle" (1939).

Henie tried to break the musical comedy mould with the anti-Nazi film "Everything Happens at Night" (1939) and "It's a Pleasure" (1945), a skating variation of the often-told "A Star Is Born" tale about alcoholic-star-in-decline-helps-newcomer-up. It was her only film shot in Technicolor, but it was not as huge at the box office as her other films and also proved her limitations as a dramatic actress in her only dramatic film.

When Zanuck realized her limits in the pseudo-dramatic vein, he cast her in more musical comedies; "Sun Valley Serenade" (1941) with Glenn Miller, John Payne, The Nicholas Brothers, and hit songs such as "In the Mood", "Chattanooga Choo Choo", "It Happened in Sun Valley", and "I Know Why (And So Do You)", followed by "Iceland" (1942) with Jack Oakie, Payne, and the hit song "There Will Never Be Another You", and finally "Wintertime" (1943) with Cesar Romero, Carole Landis, Cornel Wilde, and Oakie. Sonja had by now developed a comedy flair and these films were all among the top box-office hits for 20th Century-Fox the respective years. Eight Sonja Henie movies crossed the magical $100 million domestic gross mark. That is a percentage of 72.72% of her movies listed. "Happy Landing" (1938) was her biggest box office hit.

In addition to her film career at Fox from 1936 to 1943, Henie formed a business arrangement with Arthur Wirtz, who produced her touring ice shows under the name of "Hollywood Ice Revue". Wirtz also acted as Henie's financial advisor. At the time, figure skating and ice shows were not yet an established form of entertainment in the United States. Henie's popularity as a film actress attracted many new fans and instituted skating shows as a popular new entertainment. Throughout the 1940s, Henie and Wirtz produced lavish musical ice skating extravaganzas at Rockefeller Center's Center Theatre attracting millions of ticket buyers.

At the height of her fame, Henie brought as much as $2 million per year from her shows and touring activities. She also had numerous lucrative endorsement contracts, and deals to market skates, clothing, jewelry, dolls, and other merchandise branded with her name. These activities made her one of the wealthiest women in the world in her time.

Henie broke off her arrangement with Wirtz in 1950 and for the next three seasons produced her own tours under the name "Sonja Henie Ice Revue". It was an ill-advised decision to set herself up in competition with Wirtz, whose shows now featured the new Olympic champion Barbara Ann Scott. Since Wirtz controlled the best arenas and dates, Henie was left playing smaller venues and markets already saturated by other touring ice shows such as Ice Capades. The collapse of a section of bleachers during a show in Baltimore, Maryland, in 1952 compounded the tour's legal and financial woes.

In 1953, Henie formed a new partnership with Morris Chalfen to appear in his European "Holiday On Ice" tour, which proved to be a great success. She produced her own show at New York's Roxy Theatre in January 1956. However, a subsequent South American tour in 1956 was a disaster. Henie was drinking heavily at that time and could no longer keep up with the demands of touring, and this marked her retirement from skating. She did try to make a film series at her own expense; a series that would serve as a travelogue to several cities. Paris and London were mentioned, but only "Hello London" (1958) was made with her own backing, co-starring Michael Wilding and special guest star Stanley Holloway. While her ice show numbers were still worth watching, the film received few distributors and poor reviews, ending her film career. 

Her autobiography "Mitt livs eventyr", was published in 1938 which was translated and released as "Wings on My Feet" in 1940, and which was republished in a revised edition in 1954. At the time of her death, Henie was planning a comeback for a television special that would have aired in January 1970. She was to have danced to "Lara's Theme" from "Doctor Zhivago".

Henie's connections with Adolf Hitler and other high-ranking Nazi officials made her the subject of controversy before, during, and after World War II. During her amateur skating career, she performed often in Germany and was a favorite of German audiences and of Hitler personally. As a wealthy celebrity, she moved in the same social circles as royalty and heads of state and made Hitler's acquaintance as a matter of course. During the shooting of "Second Fiddle" (1939), she greeted the then Crown-Prince couple of Norway Olav and Märtha during their US tour. Through the years, her shows and later art exhibitions drew the attention of such people as Princess Margaret, Countess of Snowdon and Gustaf VI Adolf of Sweden and she met with them.

Controversy appeared first when Henie greeted Hitler with a Nazi salute at the 1936 Winter Olympics in Garmisch-Partenkirchen and after the Games she accepted an invitation to lunch with Hitler at his resort home in nearby Berchtesgaden, where Hitler presented Henie with an autographed photo with a lengthy inscription. She was strongly denounced in the Norwegian press for this. In her revised 1954 biography, she states that no Norwegian judge was in the panel for the 1936 Olympics—as she was entitled to as a Norwegian. She therefore made the most of it and won her third Olympic medal. When she—as a gold medal winner—passed Hitler's tribune with silver medalist Cecilia Colledge and bronze medalist Vivi-Anne Hultén, neither she nor the others honored Hitler with the Nazi salute. The 1936 European Figure Skating Championships also took place in Berlin and neither Henie, Colledge, nor Megan Taylor paid obeisance to Hitler. 

In her film "Everything Happens at Night" (1939), Ray Milland and Robert Cummings star as rival reporters hot on the trail of Hugo Norden (Maurice Moscovich). Norden, a Nobel Prize winner, was supposedly murdered by the Gestapo, but is rumored to be in hiding and writing anonymous dispatches advocating world peace. When Geoffrey and Ken track Norden to a small village in the Swiss Alps, they soon find themselves competing over the affections of beautiful Louise (Henie), who has a deeper connection to the missing Nobel laureate than the reporters realize. When Geoffrey and Ken get so distracted by romance that they begin to neglect their assignments, it almost leads to disaster as the Gestapo sets out to silence Norden once and for all. Released on 22 December 1939, it was banned in Nazi Germany.

Through her 1940 marriage to Dan Topping she had become an American citizen. As such, she was not eligible to speak Norway's cause and with producer Alexander Korda, who was set to produce the propaganda film That Hamilton Woman, could have faced deportation. The Senate Subcommittee (Senate Foreign Relations) dealt with such matters. After the bombing of Pearl Harbor, when America was no longer neutral, Henie pulled in uniform and visited and gave money to Little Norway. All Norwegians got free tickets to her shows during the war and she paid and held parties for them.

During the occupation of Norway by Nazi Germany, German troops saw Hitler's autographed photo prominently displayed at the piano in the Henie family home in Landøya, Asker. As a result, none of Henie's properties in Norway were confiscated or damaged by the Germans. Henie became a naturalized citizen of the United States in 1940. Like many Hollywood stars, she supported the U.S. war effort through USO and similar activities. After the Japanese attack, she invited the boys from Little Norway to her ice shows, gave the mechanics a plane as well a substantial sum of money to their educational fund. But her first rejection before the US entered the war was never to be forgotten. For this, she was condemned by many Norwegians and Norwegian-Americans. After the war, Henie was mindful that many of her countrymen considered her to be a quisling. However, she made a triumphant return to Norway with the Holiday on Ice tour in 1953 and 1955. The Norwegian Royal Family attended both events and indeed attended her funeral in 1969. The Royal Family were very mindful of whom they supported after the war and Norwegians looked to them as role models in that respect.

Henie was married three times, to Dan Topping (1940–1946), Winthrop Gardiner Jr. (1949–1956), and the Norwegian shipping magnate and art patron Niels Onstad (1956–1969) (her death). After her retirement in 1956, Henie and Onstad settled in Oslo and accumulated a large collection of modern art that formed the basis for the Henie Onstad Kunstsenter at Høvikodden in Bærum near Oslo.

She studied in Oslo together with Martin Stixrud and Erna Andersen who was her competitor and skate club member.

Henie was diagnosed with chronic lymphocytic leukemia in the mid-1960s. She died of the disease at age 57 in 1969 during a flight from Paris to Oslo. She is buried with Onstad in Oslo on the hilltop overlooking the Henie Onstad Art Centre.





Sonja Henie films gross https://www.ultimatemovierankings.com/sonja-henie-movies/



</doc>
<doc id="29493" url="https://en.wikipedia.org/wiki?curid=29493" title="Science &amp; Environmental Policy Project">
Science &amp; Environmental Policy Project

The Science & Environmental Policy Project (SEPP) is an advocacy group financed by private contributions based in Arlington, Virginia in the United States. It was founded in 1990 by atmospheric physicist S. Fred Singer. SEPP disputes the prevailing scientific views of climate change and ozone depletion. SEPP also questioned the science used to establish the dangers of secondhand smoke, arguing the risks are overstated.

SEPP's former Chairman of the Board of Directors is listed as Rockefeller University president emeritus Frederick Seitz, a former president of the National Academy of Sciences, now deceased.

SEPP listed the following key issues in 2010: 

On September 2, 1997, Singer said that "The possibility that global temperatures could rise because of an increase in carbon dioxide in the atmosphere is a concern that needs to be monitored...But there has been no indication in the last century that we've seen anything other than natural climate fluctuations. Both greenhouse theory and computer models predict that global warming should be more rapid in the polar regions than anywhere else," he says, "but in July the Antarctic experienced the coldest weather on record."

SEPP was the author of the Leipzig Declaration, which was based on the conclusions drawn from a November 1995 conference in Leipzig, Germany, which SEPP organized with the European Academy for Environmental Affairs.

SEPP's critics offer the following rebuttals to its claims:


In 2008, The Science and Environmental Policy Project completed the organization of the Nongovernmental International Panel on Climate Change (NIPCC) as the culmination of a process that began in 2003. The NIPCC calls itself "an international coalition of scientists convened to provide an independent examination of the evidence available on the causes and consequences of climate change in the published, peer-reviewed literature – examined without bias and selectivity."

The 2008 NIPCC document titled "Nature, Not Human Activity Rules the Climate: Summary for Policymakers of the Report of the Nongovernmental International Panel of Climate Change", published by The Heartland Institute, was released in February–March 2008. Singer served as General Editor and also holds the copyright.

Unnamed climate scientists from NASA, Stanford University and Princeton who were contacted by ABC News dismissed the same report as "fabricated nonsense.". In response, Singer objected to the ABC News piece, calling it "an appalling display of bias, unfairness, journalistic misbehavior, and a breakdown of ethical standards" which used "prejudicial language, distorted facts, libelous insinuations, and anonymous smears."


In 2004 Singer was coauthor of two papers published in Geophysical Research Letters:

Scientific criticism of SEPP's views:



</doc>
<doc id="29494" url="https://en.wikipedia.org/wiki?curid=29494" title="Abbey of Saint Gall">
Abbey of Saint Gall

The Abbey of Saint Gall () is a dissolved abbey (747–1805) in a Catholic religious complex in the city of St. Gallen in Switzerland. The Carolingian-era monastery has existed since 719 and became an independent principality between 9th and 13th centuries, and was for many centuries one of the chief Benedictine abbeys in Europe. It was founded by Saint Othmar on the spot where Saint Gall had erected his hermitage. The library of the Abbey is one of the richest medieval libraries in the world. The city of St. Gallen originated as an adjoining settlement of the abbey. Following the secularization of the abbey around 1800 the former Abbey church became a Cathedral in 1848. Since 1983 the whole remaining abbey precinct has been a UNESCO World Heritage Site.

Around 613 Gallus, according to tradition an Irish monk and disciple and companion of Saint Columbanus, established a hermitage on the site that would become the monastery. He lived in his cell until his death in 646, and was buried there in Arbon. Afterwards, the people venerated him as a saint and prayed at his tomb for his intercession in times of danger.

Following Gallus' death, Charles Martel appointed Otmar as custodian of St Gall's relics. Several different dates are given for the foundation of the monastery, including 719, 720, 747 and the middle of the 8th century. During the reign of Pepin the Short, in the 8th century, Othmar founded the Carolingian style Abbey of St Gall, where arts, letters and sciences flourished. The abbey grew fast and many Alemannic noblemen became monks. At the end of abbot Otmar's reign, the "Professbuch" mentions 53 names. Two monks of the Abbey of St Gall, Magnus von Füssen and Theodor, founded the monasteries in Kempten and Füssen in the Allgäu. With the increase in the number of monks the abbey grew stronger also economically. Much land in Thurgau, Zürichgau and in the rest of Alemannia as far as the Neckar was transferred to the abbey due to "Stiftungen". Under abbot Waldo of Reichenau (740–814) copying of manuscripts was undertaken and a famous library was gathered. Numerous Anglo-Saxon and Irish monks came to copy manuscripts. At Charlemagne's request Pope Adrian I sent distinguished chanters from Rome, who propagated the use of the Gregorian chant. In 744, the Alemannic nobleman Beata sells several properties to the abbey in order to finance his journey to Rome.

In the subsequent century, St Gall came into conflict with the nearby Bishopric of Constance which had recently acquired jurisdiction over the Abbey of Reichenau on Lake Constance. It was not until Emperor Louis the Pious (ruled 814–840) confirmed in 813 the imperial immediacy ("Reichsunmittelbarkeit") of the abbey, that this conflict ceased. The abbey became an Imperial Abbey ("Reichsabtei"). King Louis the German confirmed in 833 the immunity of the abbey and allowed the monks the free choice of their abbot. In 854 finally, the Abbey of St Gall reached its full autonomy by King Louis the German releasing the abbey from the obligation to pay tithes to the Bishop of Constance.

From this time until the 10th century, the abbey flourished. It was home to several famous scholars, including Notker of Liège, Notker the Stammerer, Notker Labeo and Hartker (who developed the antiphonal liturgical books for the abbey). During the 9th century a new, larger church was built and the library was expanded. Manuscripts on a wide variety of topics were purchased by the abbey and copies were made. Over 400 manuscripts from this time have survived and are still in the library today.

Between 924 and 933 the Magyars threatened the abbey and the books had to be removed to Reichenau for safety. Not all the books were returned.

On 26 April 937 a fire broke out and destroyed much of the abbey and the adjoining settlement, though the library was undamaged. About 954 they started to protect the monastery and buildings by a surrounding wall. Around 971/974 abbot Notker (about whom almost nothing is known; nephew of Notker Physicus) finalized the walling and the adjoining settlements started to become the town of St Gall. In 1006, the abbey was the northernmost place where a sighting of the 1006 supernova was recorded.

The death of abbot Ulrich II on 9 December 1076 terminated the cultural silver age of the monastery.

In 1207, abbot Ulrich von Sax becomes a Prince ("Reichsfürst", or simply "Fürst") of the Holy Roman Empire by King Philip of Germany. The abbey became a Princely Abbey ("Reichsabtei"). As the abbey became more involved in local politics, it entered a period of decline. 
The city of St. Gallen proper progressively freed itself from the rule of the abbot, acquiring Imperial immediacy, and by the late 15th century was recognized as a Free imperial city.
By about 1353 the guilds, headed by the cloth-weavers guild, gained control of the civic government. In 1415 the city bought its liberty from the German king King Sigismund.
During the 14th century Humanists were allowed to carry off some of the rare texts from the abbey library.

In the late 14th and early 15th centuries, the farmers of the abbot's personal estates (known as "Appenzell", from meaning "cell (i.e. estate) of the abbot") began seeking independence. In 1401, the first of the Appenzell Wars broke out, and following the Appenzell victory at Stoss in 1405 they became allies of the Swiss Confederation in 1411. During the Appenzell Wars, the town of St. Gallen often sided with Appenzell against the abbey. So when Appenzell allied with the Swiss, the town of St. Gallen followed just a few months later. The abbot became an ally of several members of the Swiss Confederation (Zürich, Lucerne, Schwyz and Glarus) in 1451. While Appenzell and St. Gallen became full members of the Swiss Confederation in 1454. Then, in 1457 the town of St. Gallen became officially free from the abbot.

In 1468 the abbot, Ulrich Rösch, bought the County of Toggenburg from the representatives of its counts, after the family died out in 1436. In 1487 he built a monastery at Rorschach on Lake Constance, to which he planned to move. However, he encountered stiff resistance from the St. Gallen citizenry, other clerics, and the Appenzell nobility in the Rhine Valley who were concerned about their holdings. The town of St. Gallen wanted to restrict the increase of power in the abbey and simultaneously increase the power of the town. The mayor of St. Gallen, Ulrich Varnbüler, established contact with farmers and Appenzell residents (led by the fanatical Hermann Schwendiner) who were seeking an opportunity to weaken the abbot. Initially, he protested to the abbot and the representatives of the four sponsoring Confederate cantons (Zürich, Lucerne, Schwyz, and Glarus) against the construction of the new abbey in Rorschach. Then on July 28, 1489 he had armed troops from St. Gallen and Appenzell destroy the buildings already under construction. When the abbot complained to the Confederates about the damages and demanded full compensation, Varnbüler responded with a counter suit and in cooperation with Schwendiner rejected the arbitration efforts of the non-partisan Confederates. He motivated the clerics from Wil to Rorschach to discard their loyalty to the abbey and spoke against the abbey at the town meeting at Waldkirch, where the popular league was formed. He was confident that the four sponsoring cantons would not intervene with force, due to the prevailing tensions between the Confederation and the Swabian League. He was strengthened in his resolve by the fact that the people of St. Gallen elected him again to the highest magistrate in 1490.

However, in early 1490 the four cantons decided to carry out their duty to the abbey and to invade the St. Gallen canton with an armed force. The people of Appenzell and the local clerics submitted to this force without noteworthy resistance, while the city of St. Gallen braced for a fight to the finish. However, when they learned that their compatriots had given up the fight, they lost confidence; the end result was that they concluded a peace pact that greatly restricted the city's powers and burdened the city with serious penalties and reparations payments. Varnbüler and Schwendiner fled to the court of King Maximilian and lost all their property in St. Gallen and Appenzell. However, the abbot's reliance on the Swiss to support him reduced his position almost to that of a "subject district".

The town adopted the Reformation in 1524, while the abbey remained Catholic, which damaged relations between the town and abbey. Both the abbot and a representative of the town were admitted to the Swiss Tagsatzung or Diet as the closest associates of the Confederation.

In the 16th century the abbey was raided by Calvinist groups, which scattered many of the old books. In 1530, abbot Diethelm began a restoration that stopped the decline and led to an expansion of the schools and library.

Under abbot Pius (1630–74) a printing press was started. In 1712 during the Toggenburg war, also called the second war of Villmergen, the Abbey of St. Gall was pillaged by the Swiss. They took most of the books and manuscripts to Zürich and Bern. For security, the abbey was forced to request the protection of the townspeople of St. Gallen. Until 1457 the townspeople had been serfs of the abbey, but they had grown in power until they were protecting the abbey.

Following the disturbances, the abbey was still the largest religious city-state in Switzerland, with over 77,000 inhabitants. A final attempt to expand the abbey resulted in the demolition of most of the medieval monastery. The new structures, including the cathedral by architect Peter Thumb (1681-1766), were designed in the late Baroque style and constructed between 1755 and 1768. The large and ornate new abbey did not remain a monastery for very long. In 1798 the Prince-Abbot's secular power was suppressed, and the abbey was secularized. The monks were driven out and moved into other abbeys. The abbey became a separate See in 1846, with the abbey church as its cathedral and a portion of the monastic buildings for the bishop.

The Abbey library of Saint Gall is recognized as one of the richest medieval libraries in the world. It is home to one of the most comprehensive collections of early medieval books in the German-speaking part of Europe. , the library consists of over 160,000 books, of which 2100 are handwritten. Nearly half of the handwritten books are from the Middle Ages and 400 are over 1000 years old. Lately the "Stiftsbibliothek" has launched a project for the digitisation of the priceless manuscript collection, which currently (December 2009) contains 355 documents that are available on the "Codices Electronici Sangallenses" webpage.

The library interior is exquisitely realised in the Rococo style with carved polished wood, stucco and paint used to achieve its overall effect. It was designed by the architect Peter Thumb and is open to the public. In addition it holds exhibitions as well as concerts and other events.

One of the more interesting documents in the Stiftsbibliothek is a copy of Priscian's "Institutiones grammaticae" which contains the poem "Is acher in gaíth in-nocht..." written in Old Irish.

The library also preserves a unique 9th-century document, known as the Plan of St. Gall, the only surviving major architectural drawing from the roughly 700-year period between the fall of the Western Roman Empire and the 13th century. The Plan drawn was never actually built, and was so named because it was kept at the famous medieval monastery library, where it remains to this day. The plan was an ideal of what a well-designed and well-supplied monastery should have, as envisioned by one of the synods held at Aachen for the reform of monasticism in the Frankish empire during the early years of emperor Louis the Pious (between 814 and 817).

A late 9th-century drawing of St. Paul lecturing an agitated crowd of Jews and gentiles, part of a copy of a Pauline epistles produced at and still held by the monastery, was included in a medieval-drawing show at the Metropolitan Museum of Art in New York the summer of 2009. A reviewer noted that the artist had "a special talent for depicting hair, ... with the saint's beard ending in curling droplets of ink."

St. Gall is noted for its early use of the neume, the basic element of Western and Eastern systems of musical notation prior to the invention of five-line staff notation. The earliest extant manuscripts are from the 9th or 10th century.

In 1983, the Convent of St. Gall was inscribed on the UNESCO World Heritage List as "a perfect example of a great Carolingian monastery".

There were a total of 73 ruling abbots (including six anti-abbots) between 719 and 1805.
A complete collection of abbots' biographies was published 
by Henggeler (1929). A table of abbots' names complete with their coats of arms was printed by Beat Jakob Anton Hiltensperger in 1778.





</doc>
<doc id="29498" url="https://en.wikipedia.org/wiki?curid=29498" title="Secondary education">
Secondary education

Secondary education covers two phases on the International Standard Classification of Education scale. Level 2 or lower secondary education (less common junior secondary education) is considered the second and final phase of basic education, and level 3 (upper) secondary education is the stage before tertiary education. Every country aims to provide basic education, but the systems and terminology remain unique to them. Secondary education typically takes place after six years of primary education and is followed by higher education, vocational education or employment. In most countries secondary education is compulsory, at least until the age of 16. Children typically enter the lower secondary phase around age 11. Compulsory education sometimes extends to age 19.

Since 1989, education has been seen as a basic human right for a child; Article 28, of the Convention on the Rights of the Child states that primary education should be free and compulsory while different forms of secondary education, including general and vocational education, should be available and accessible to every child. The terminology has proved difficult, and there was no universal definition before ISCED divided the period between primary education and university into junior secondary education and upper secondary education.

In classical and medieval times secondary education was provided by the church for the sons of nobility and to boys preparing for universities and the priesthood. As trade required navigational and scientific skills the church reluctantly expanded the curriculum and widened the intake. With the Reformation the state wrestled the control of learning from the church, and with Comenius and John Locke education changed from being repetition of Latin text to building up knowledge in the child. Education was for the few. Up to the middle of the 19th century, secondary schools were organised to satisfy the needs of different social classes with the labouring classes getting 4 years, the merchant class 5 years and the elite getting 7 years. The rights to a secondary education were codified after 1945, and countries are still working to achieve the goal of mandatory and free secondary education for all youth under 19.

The 1997 International Standard Classification of Education (ISCED) describes seven levels that can be used to compare education internationally. Within a country these can be implemented in different ways, with different age levels and local denominations. The seven levels are:

Within this system, Levels 1 and 2 – that is, primary education and lower secondary – together form basic education. Beyond that, national governments may attach the label of secondary education to Levels 2 through 4 together, Levels 2 and 3 together, or Level 2 alone. These level definitions were put together for statistical purposes, and to allow the gathering of comparative data nationally and internationally. They were approved by the UNESCO General Conference at its 29th session in November 1997. Though they may be dated, they do provide a universal set of definitions and remain unchanged in the 2011 update.

The start of lower secondary education is characterised by the transition from the single-class-teacher, who delivers all content to a cohort of pupils, to one where content is delivered by a series of subject specialists. Its educational aim is to complete provision of basic education (thereby completing the delivery of basic skills) and to lay the foundations for lifelong learning.

Lower secondary education is likely to show these criteria:

The end of lower secondary education often coincides with the end of compulsory education in countries where that exists.

(Upper) secondary education starts on the completion of basic education, which also is defined as completion of lower secondary education. The educational focus is varied according to the student's interests and future direction. Education at this level is usually voluntary.

(Upper) secondary education is likely to show these criteria:

More subjects may be dropped, and increased specialism occurs. Completion of (upper) secondary education provides the entry requirements to Level 5 tertiary education, the entry requirements to technical or vocational education (Level 4, non tertiary course), or direct entry into the workplace.

In 2012 the ISCED published further work on education levels where it codified particular paths and redefined the tertiary levels. Lower secondary education and (upper) secondary education could last between 2 and 5 years, and the transition between two often would be when students were allowed some subject choice.

Terminology for secondary schools varies by country, and the exact meaning of any of these varies. Secondary schools may also be called "academies", "colleges", "gymnasiums", "high schools", "lyceums", "middle schools", "preparatory schools", "sixth-form colleges", "upper schools", or "vocational schools", among other names. For further information about nomenclature, see the section below by country.

A form of education for adolescents became necessary in all societies that had an alphabet and engaged in commerce. In Western Europe, formal secondary education can be traced back to the Athenian educational reforms of 320BC. Though their civilisation was eclipsed and they were enslaved, Hellenistic Athenian teachers were valued in the Roman system. The Roman and Hellenistic schools of rhetoric taught the seven liberal arts and sciences – "grammar, rhetoric, logic, arithmetic, geometry, music" and "astronomy" – which were regarded as a preparation for the study at a tertiary level of theology, law and medicine. Boys would have been prepared to enter these schools by private tutors at home. Girls would have only received tuition at home.
When the Romans retreated, all traces of civilisation were erased.

England provides a good case study. When Augustine of Canterbury brought Christianity there in 597, no schools existed. He needed trained priests to conduct church services and boys to sing in the choir. He had to create both the grammar schools that taught Latin, to enable the English to study for the priesthood, and song schools (choir schools) that trained the 'sons of gentlefolk' to sing in cathedral choirs. In the case of Canterbury (597) and Rochester (604), both still exist. Bede in his Ecclesiastical history (732) tells that the Canterbury school taught more than the 'intended reading and understanding of Latin', but 'the rules of metric, astronomy and the computus as well as the works of the saints' Even at this stage, there was tension, as the church was worried that knowledge of Latin would give the student access to non-Christian texts that it would not wish them to read.

Over the centuries leading to the renaissance and reformation the church was the main provider of secondary education. Various invasions and schisms within the controlling church challenged the focus of the schools, and the curriculum and language of instruction waxed and waned. From 1100, With the growth of the towns, grammar schools 'free' of the church were founded, and some church grammar schools were handed over to the laïty. Universities were founded that didn't just train students for the priesthood.

Whereas in mainland Europe the renaissance preceded the reformation, local conditions in England caused the reformation to come first. The reformation was about allowing the laïty to interpret the Bible in their own way without the intervention of priests, and preferably in the vernacular. This stimulated the foundation of free Grammar schools- who searched for a less constrained curriculum. Colonialisation required navigation, mensuration, languages and administrative skills. The laïty wanted these taught to their sons. After Gutenberg1455 had mastered moveable metal type printing and Tyndale had translated the Bible into English (1525), Latin became a skill reserved for the catholic church and sons conservative nobility. Schools started to be set up for the sons of merchants in Europe and the colonies too- for example Boston Latin Grammar School (1635).

Comenius (1592–1670), a Moravian protestant proposed a new model of education- where ideas were developed from the familiar to the theoretical rather than through repetition, where languages were taught in the vernacular and supported universal education. In his "Didactica Magna" (Great Didactic), he outlined a system of schools that is the exact counterpart of many western school systems: kindergarten, elementary school, secondary school, six-form college, university.
Locke's Some Thoughts Concerning Education (1693) stressed the importance of a broader intellectual training, moral development and physical hardening. .

The grammar schools of the period can be categorised in three groups: the nine leading schools, seven of them boarding institutions which maintained the traditional curriculum of the classics, and mostly served 'the aristocracy and the squirearchy' ; most of the old endowed grammar schools serving a broad social base in their immediate localities which also stuck to the old curriculum; the grammar schools situated in the larger cities, serving the families of merchants and tradesmen who embraced change.

During the 18th century their social base widened and their curriculum developed, particularly in mathematics and the natural sciences. But this was not universal education and was self-selecting by wealth The industrial revolution changed that. Industry required an educated workforce where all workers needed to have completed a basic education. In France, Louis XIV, wrestled the control of education from the Jesuits, Condorcet set up Collèges for universal lower secondary education throughout the country, then Napoleon set up a regulated system of Lycee. In England, Robert Peel's Factory Act of 1802 required an employer to provide instruction in reading, writing and arithmetic during at least the first four years of the seven years of apprenticeship. The state had accepted responsibility for the basic education of the poor.
The provision of school places remained inadequate, so an Order in Council dated 10 April 1839 created the Committee of the Privy Council on Education.

There was considerable opposition to the idea that children of all classes should receive basic education, all the initiatives such as industrial schools and Sunday schools were initially a private or church initiative. With the Great Exhibition of 1851, it became clear just how far behind the English education system had fallen. 

Three reports were commissioned to examine the education of upper, middle and labouring class children. The Clarendon Commission sought to improve the nine Great Public Schools. The Taunton Commission looked at the 782 endowed grammar schools (private and public). They found varying quality and a patchy geographical coverage, with two thirds of all towns not having any secondary school. There was no clear conception of the purpose of secondary education. There were only thirteen girls' schools and their tuition was superficial, unorganised and unscientific. They recommended a system of first-grade schools targeted at a leaving age of 18 as preparation for upper and upper-middle class boys entering university, second-grade targeted at a leaving age of 16 for boys preparing for the army or the newer professions, and third-grade targeted at a leaving age of 14 for boys of small tenant farmers, small tradesmen, and superior artisans. This resulted in the 1869 Endowed Schools Act which advocated that girls should enjoy the same education as boys.

The Newcastle Commission inquired "into the state of public education in England and to consider and report what measures, if any, are required for the extension of sound and cheap elementary instruction to all classes of the people". It produced 1861 Newcastle Report and this led to the 1870 Elementary Education Act (Forster Act).

The school boards set up by the 1870 Elementary Education Act (Forster Act) and were stopped from providing secondary education by the Cockerton Judgement of 1899. The school leaving age at this time was 10. The Judgement prompted the 1902 Education Act (Balfour Act). Compulsory education was extended to 12. The new Local Education Authorities (LEA)s that were formed from the school boards; started to open Higher Grade Elementary Schools (ISCED Level2) or county schools to supplement the endowed grammar schools. These LEAs were allowed to build second-grade secondary schools that in the main became the future secondary modern schools. 

In the ""1904 Regulations for Secondary Schools"", the Board of Education determined that secondary schools should offer a:
a four year subject-based course leading to a certificate in English language and literature, geography, history, a foreign language, mathematics, science, drawing, manual work, physical training, and, for girls, housewifery. 

The Education Act 1918 (Fisher Act) extended compulsory full-time education to 14, and recommended compulsory part-time education from 14–18.
The Hadlow report, "Education the Adolescent" (1926) proposed that there should be a break point at eleven, establishing primary schools and secondary schools.

The United Nations, founded in 1947, was committed to education for all but the definition was difficult to formulate.
The Universal Declaration of Human Rights (1948) declared that elementary and fundamental education, which it didn't define, was a right to be enjoyed by all. The Education Act 1944 (Butler Act) made sweeping changes to the funding of state education using the tripartite system, but wasn't allowed to tackle private schools. It introduced the GCE 'O'level at 16, and the 'A' at 18, but only raised the school leaving age until 15, making the exam inaccessible to the majority. But one year of ISCED Level 3 (Upper) secondary education was mandatory and free. 

In 1972 the school leaving was raised to 16. The Education and Skills Act 2008, when it came into force in the 2013 academic year, initially required participation in some form of education or training until the school year in which the child turned 17, followed by the age being raised to the young person's 18th birthday in 2015. This was referred to as raising the "participation age" to distinguish it from the school leaving age which remains at 16. Thus the UK is following the ISCED Level 3 (Upper) secondary education guideline.

The United Nations was strong in its commitment to education for all but fell into linguistic difficulty defining that right.

“Article I: Purposes and functions
1. The purpose of the Organization is to contribute to peace and security by promoting collaboration among the nations through education, science and culture in order to further universal respect for justice, for the rule of law and for the human rights and fundamental freedoms which are affirmed for the peoples of the world, without distinction of race, sex, language or religion, by the Charter of the United Nations.”

The Universal Declaration of Human Rights (1948) declared that elementary and fundamental education was a right to be enjoyed by all, but again could not define either elementary and fundamental education.
Article 26 :(1) Everyone has the right to education. Education shall be free, at least in the elementary and fundamental stages. Elementary education shall be compulsory. Technical and professional education shall be made generally available and higher education shall be equally accessible to all on the basis of merit.
It was assumed that elementary education was basic education, the entitlement for children- and fundamental education was a right for the working man, but for a lawyer the definition is neither qualitative (stating what education means) or quantitative saying when it starts and when it is completed. The term secondary is not defined or mentioned. Together this has enabled countries to terminate free, compulsory, basic education at 11 or only continue education past eleven to boys.

Article 28, of the Convention on the Rights of the Child (1989) stated that primary education should be free and compulsory while different forms of secondary education, including general and vocational education, should be available and accessible to every
child. Free education should be provided and financial assistance offered in case of need. 
In 1990, at Jomtien again tried to define the content basic education and how it should be delivered. ‘Basic education’ is defined as ‘action designed to meet ‘basic learning needs’. ‘primary schooling’ is considered as ‘the main delivery system of basic education’.
addressing the basic learning needs of all means: early childhood care and development opportunities; relevant, quality primary schooling or equivalent out-of-school education for children; and literacy, basic knowledge and life skills training for youth and adults.’

The assumption being made that basic knowledge and life skills training for youth was the function of secondary education. This was codified by the ISCED documents. The Dakar Framework for Action 2010 goal 2 states: Ensuring that by 2015 all children, particularly girls, children in difficult circumstances and those belonging to ethnic minorities, have access to and complete free and compulsory (primary in the sense basic) education of good quality. The Dakar Framework for Action 2010 goal 5 states: Eliminating gender disparities in primary and secondary education by 2005, and achieving gender equality in education by 2015, with a focus on ensuring girls’ full and equal access to and achievement in basic education of good quality. 

Malala Yousafzai, Nobel Peace Prize winner in a said in a 2017 interview that:
“My goal is to make sure every child, girl and boy, they get the opportunity to go to school." “It is their basic human right, so I will be working on that and I will never stop until I see the last child going to school.” 
UNESCO believes that in order to prepare young people for life and work in a rapidly changing world, secondary-level education systems need to be re-oriented to impart a broad repertoire of life-skills. These skills should include the key generic competencies, non occupation-specific practical capabilities, ICT, the ability to learn independently, to work in teams, entrepreneurship and civic responsibility.

They may be best instilled through a shared foundational learning period and by deferring the directing of students into academic and vocational streams for as long as possible, and then there should be flexibility to ensure the free movement of students between the streams depending on their aptitudes and inclinations. Accreditation in one stream should have equal recognition in the other as well as for access to higher education. This will equip young people with multiple skills so that they are prepared to enter and re-enter the workforce several times in their working lives, as wage employees or self-employed entrepreneurs, and to re-train themselves when their skills become obsolete.

It recognizes that there is no single model that will suit all countries, or even all communities in a given country. Secondary-level education policy should be under continuous review to keep in step with scientific and technological, economic and societal change.

Adolescence is associated with a time of significant growth where identity, belongingness, and socialization, especially among peer groups is particularly important. Secondary schools play an important role in youth's socialization, development and forming their ideas and approach to justice, democracy and human rights.

Education systems that promote education for justice, that is, respect for the rule of law (RoL) together with international human rights and fundamental freedoms strengthen the relationship between learners and public institutions with the objective of empowering young people to become champions of peace and justice. Teachers are on the front line of this work and, along with families, play a formative role in shaping the future of youth's attitudes and behaviours.

Each country has developed the form of education most appropriate for them. There is an attempt to compare the effectiveness by using the results from the PISA that, each third year, assesses the scholastic performance on mathematics, science, and reading of a representative sample of 5000 fifteen year olds from each country.




</doc>
<doc id="29500" url="https://en.wikipedia.org/wiki?curid=29500" title="Serotonin syndrome">
Serotonin syndrome

Serotonin syndrome (SS) is a group of symptoms that may occur with the use of certain serotonergic medications or drugs. The degree of symptoms can range from mild to severe. Symptoms in mild cases include high blood pressure and a fast heart rate; usually without a fever. Symptoms in moderate cases include high body temperature, agitation, increased reflexes, tremor, sweating, dilated pupils, and diarrhea. In severe cases body temperature can increase to greater than . Complications may include seizures and extensive muscle breakdown.
Serotonin syndrome is typically caused by the use of two or more serotonergic medications or drugs. This may include selective serotonin reuptake inhibitor (SSRI), serotonin norepinephrine reuptake inhibitor (SNRI), monoamine oxidase inhibitor (MAOI), tricyclic antidepressants (TCAs), amphetamines, pethidine (meperidine), tramadol, dextromethorphan, buspirone, L-tryptophan, 5-HTP, St. John's wort, triptans, ecstasy (MDMA), metoclopramide, ondansetron, or cocaine. It occurs in about 15% of SSRI overdoses. It is a predictable consequence of excess serotonin on the central nervous system (CNS). Onset of symptoms is typically within a day of the extra serotonin.
Diagnosis is based on a person's symptoms and history of medication use. Other conditions that can produce similar symptoms such as neuroleptic malignant syndrome, malignant hyperthermia, anticholinergic toxicity, heat stroke, and meningitis should be ruled out. No laboratory tests can confirm the diagnosis.
Initial treatment consists of discontinuing medications which may be contributing. In those who are agitated, benzodiazepines may be used. If this is not sufficient, a serotonin antagonist such as cyproheptadine may be used. In those with a high body temperature active cooling measures may be needed. The number of cases of serotonin syndrome that occur each year is unclear. With appropriate treatment the risk of death is less than one percent. The high-profile case of Libby Zion, who is generally accepted to have died from serotonin syndrome, resulted in changes to graduate medical education in New York State.

Symptom onset is usually rapid, often occurring within minutes of elevated serotonin levels. Serotonin syndrome encompasses a wide range of clinical findings. Mild symptoms may consist of increased heart rate, shivering, sweating, dilated pupils, myoclonus (intermittent jerking or twitching), as well as overresponsive reflexes. However, many of these symptoms may be side effects of the drug or drug interaction causing excessive levels of serotonin; not an effect of elevated serotonin itself. Tremor is a common side effect of MDMA's action on dopamine, whereas hyperreflexia is symptomatic of exposure to serotonin agonists. Moderate intoxication includes additional abnormalities such as hyperactive bowel sounds, high blood pressure and hyperthermia; a temperature as high as . The overactive reflexes and clonus in moderate cases may be greater in the lower limbs than in the upper limbs. Mental changes include hypervigilance or insomnia and agitation. Severe symptoms include severe increases in heart rate and blood pressure that may lead to shock. Temperature may rise to above in life-threatening cases. Other abnormalities include metabolic acidosis, rhabdomyolysis, seizures, kidney failure, and disseminated intravascular coagulation; these effects usually arising as a consequence of hyperthermia.

The symptoms are often described as a clinical triad of abnormalities:


A large number of medications and street drugs can cause serotonin syndrome when taken alone at high doses or in combination with other serotonergic drugs. The table below lists some of these drugs.
Many cases of serotonin toxicity occur in people who have ingested drug combinations that synergistically increase synaptic serotonin. It may also occur due to an overdose of a single serotonergic agent. The combination of MAOIs with precursors such as L-tryptophan or 5-HTP pose a particularly acute risk of life-threatening serotonin syndrome. The case of combination of MAOIs with tryptamine agonists (commonly known as ayahuasca) can present similar dangers as their combination with precursors, but this phenomenon has been described in general terms as the "cheese effect". Many MAOIs irreversibly inhibit monoamine oxidase. It can take at least four weeks for this enzyme to be replaced by the body in the instance of irreversible inhibitors. With respect to tricyclic antidepressants only clomipramine and imipramine have a risk of causing SS.

Many medications may have been incorrectly thought to cause serotonin syndrome. For example, some case reports have implicated atypical antipsychotics in serotonin syndrome, but it appears based on their pharmacology that they are unlikely to cause the syndrome. It has also been suggested that mirtazapine has no significant serotonergic effects, and is therefore not a dual action drug. Bupropion has also been suggested to cause serotonin syndrome, although as there is no evidence that it has any significant serotonergic activity, it is thought unlikely to produce the syndrome. In 2006 the United States Food and Drug Administration issued an alert suggesting that the combined use of SSRIs or SNRIs and triptan medications or sibutramine could potentially lead to severe cases of serotonin syndrome. This has been disputed by other researchers as none of the cases reported by the FDA met the Hunter criteria for serotonin syndrome. The condition has however occurred in surprising clinical situations, and because of phenotypic variations among individuals, it has been associated with unexpected drugs, including mirtazapine.

The relative risk and severity of serotonergic side effects and serotonin toxicity, with individual drugs and combinations, is complex. Serotonin syndrome has been reported in patients of all ages, including the elderly, children, and even newborn infants due to in utero exposure. The serotonergic toxicity of SSRIs increases with dose, but even in over-dose it is insufficient to cause fatalities from serotonin syndrome in healthy adults. Elevations of central nervous system serotonin will typically only reach potentially fatal levels when drugs with different mechanisms of action are mixed together. Various drugs, other than SSRIs, also have clinically significant potency as serotonin reuptake inhibitors, (e.g. tramadol, amphetamine, and MDMA) and are associated with severe cases of the syndrome.

Serotonin is a neurotransmitter involved in multiple complex biological processes including aggression, pain, sleep, appetite, anxiety, depression, migraine, and vomiting. In humans the effects of excess serotonin were first noted in 1960 in patients receiving a monoamine oxidase inhibitor (MAOI) and tryptophan. The syndrome is caused by increased serotonin in the central nervous system. It was originally suspected that agonism of 5-HT receptors in central grey nuclei and the medulla was responsible for the development of the syndrome. Further study has determined that overstimulation of primarily the 5-HT receptors appears to contribute substantially to the condition. The 5-HT receptor may still contribute through a pharmacodynamic interaction in which increased synaptic concentrations of a serotonin agonist saturate all receptor subtypes. Additionally, noradrenergic CNS hyperactivity may play a role as CNS norepinephrine concentrations are increased in serotonin syndrome and levels appear to correlate with the clinical outcome. Other neurotransmitters may also play a role; NMDA receptor antagonists and GABA have been suggested as affecting the development of the syndrome. Serotonin toxicity is more pronounced following supra-therapeutic doses and overdoses, and they merge in a continuum with the toxic effects of overdose.

A postulated "spectrum concept" of serotonin toxicity emphasises the role that progressively increasing serotonin levels play in mediating the clinical picture as side effects merge into toxicity. The dose-effect relationship is the effects of progressive elevation of serotonin, either by raising the dose of one drug, or combining it with another serotonergic drug which may produce large elevations in serotonin levels. Some experts prefer the terms serotonin toxicity or serotonin toxidrome, to more accurately reflect that it is a form of poisoning.

There is no specific test for serotonin syndrome. Diagnosis is by symptom observation and investigation of the person's history. Several criteria have been proposed. The first evaluated criteria were introduced in 1991 by Harvey Sternbach. Researchers later developed the Hunter Toxicity Criteria Decision Rules, which have better sensitivity and specificity, 84% and 97%, respectively, when compared with the gold standard of diagnosis by a medical toxicologist. As of 2007, Sternbach's criteria were still the most commonly used.

The most important symptoms for diagnosing serotonin syndrome are tremor, extreme aggressiveness, akathisia, or clonus (spontaneous, inducible and ocular). Physical examination of the patient should include assessment of deep-tendon reflexes and muscle rigidity, the dryness of the mucosa of the mouth, the size and reactivity of the pupils, the intensity of bowel sounds, skin color, and the presence or absence of sweating. The patient's history also plays an important role in diagnosis, investigations should include inquiries about the use of prescription and over-the-counter drugs, illicit substances, and dietary supplements, as all these agents have been implicated in the development of serotonin syndrome. To fulfill the Hunter Criteria, a patient must have taken a serotonergic agent and meet one of the following conditions:

Serotonin toxicity has a characteristic picture which is generally hard to confuse with other medical conditions, but in some situations it may go unrecognized because it may be mistaken for a viral illness, anxiety disorders, neurological disorder, anticholinergic
poisoning, sympathomimetic toxicity, or worsening psychiatric condition. The condition most often confused with serotonin syndrome is neuroleptic malignant syndrome (NMS). The clinical features of neuroleptic malignant syndrome and serotonin syndrome share some features which can make differentiating them difficult. In both conditions, autonomic dysfunction and altered mental status develop. However, they are actually very different conditions with different underlying dysfunction (serotonin excess vs dopamine blockade). Both the time course and the clinical features of NMS differ significantly from those of serotonin toxicity. Serotonin toxicity has a rapid onset after the administration of a serotonergic drug and responds to serotonin blockade such as drugs like chlorpromazine and cyproheptadine. Dopamine receptor blockade (NMS) has a slow onset, typically evolves over several days after administration of a neuroleptic drug, and responds to dopamine agonists such as bromocriptine.

Differential diagnosis may become difficult in patients recently exposed to both serotonergic and neuroleptic drugs. Bradykinesia and extrapyramidal "lead pipe" rigidity are classically present in NMS, whereas serotonin syndrome causes hyperkinesia and clonus; these distinct symptoms can aid in differentiation.

Management is based primarily on stopping the usage of the precipitating drugs, the administration of serotonin antagonists such as cyproheptadine, and supportive care including the control of agitation, the control of autonomic instability, and the control of hyperthermia. Additionally, those who ingest large doses of serotonergic agents may benefit from gastrointestinal decontamination with activated charcoal if it can be administered within an hour of overdose. The intensity of therapy depends on the severity of symptoms. If the symptoms are mild, treatment may only consist of discontinuation of the offending medication or medications, offering supportive measures, giving benzodiazepines for myoclonus, and waiting for the symptoms to resolve. Moderate cases should have all thermal and cardiorespiratory abnormalities corrected and can benefit from serotonin antagonists. The serotonin antagonist cyproheptadine is the recommended initial therapy, although there have been no controlled trials demonstrating its efficacy for serotonin syndrome. Despite the absence of controlled trials, there are a number of case reports detailing apparent improvement after people have been administered cyproheptadine. Animal experiments also suggest a benefit from serotonin antagonists. Cyproheptadine is only available as tablets and therefore can only be administered orally or via a nasogastric tube; it is unlikely to be effective in people administered activated charcoal and has limited use in severe cases. Cyproheptadine can be stopped when the person is no longer experiencing symptoms and the half life of serotonergic medications already passed.

Additional pharmacological treatment for severe case includes administering atypical antipsychotic drugs with serotonin antagonist activity such as olanzapine. Critically ill people should receive the above therapies as well as sedation or neuromuscular paralysis. People who have autonomic instability such as low blood pressure require treatment with direct-acting sympathomimetics such as epinephrine, norepinephrine, or phenylephrine. Conversely, hypertension or tachycardia can be treated with short-acting antihypertensive drugs such as nitroprusside or esmolol; longer acting drugs such as propranolol should be avoided as they may lead to hypotension and shock. The cause of serotonin toxicity or accumulation is an important factor in determining the course of treatment. Serotonin is catabolized by monoamine oxidase A in the presence of oxygen, so if care is taken to prevent an unsafe spike in body temperature or metabolic acidosis, oxygenation will assist in dispatching the excess serotonin. The same principle applies to alcohol intoxication. In cases of serotonin syndrome caused by monoamine oxidase inhibitors oxygenation will not help to dispatch serotonin. In such instances, hydration is the main concern until the enzyme is regenerated.

Specific treatment for some symptoms may be required. One of the most important treatments is the control of agitation due to the extreme possibility of injury to the person themselves or caregivers, benzodiazepines should be administered at first sign of this. Physical restraints are not recommended for agitation or delirium as they may contribute to mortality by enforcing isometric muscle contractions that are associated with severe lactic acidosis and hyperthermia. If physical restraints are necessary for severe agitation they must be rapidly replaced with pharmacological sedation. The agitation can cause a large amount of muscle breakdown. This breakdown can cause severe damage to the kidneys through a condition called rhabdomyolysis.

Treatment for hyperthermia includes reducing muscle overactivity via sedation with a benzodiazepine. More severe cases may require muscular paralysis with vecuronium, intubation, and artificial ventilation. Suxamethonium is not recommended for muscular paralysis as it may increase the risk of cardiac dysrhythmia from hyperkalemia associated with rhabdomyolysis. Antipyretic agents are not recommended as the increase in body temperature is due to muscular activity, not a hypothalamic temperature set point abnormality.

Upon the discontinuation of serotonergic drugs, most cases of serotonin syndrome resolve within 24 hours, although in some cases delirium may persist for a number of days. Symptoms typically persist for a longer time frame in patients taking drugs which have a long elimination half-life, active metabolites, or a protracted duration of action.

Cases have reported muscle pain and weakness persisting for months, and antidepressant discontinuation may contribute to ongoing features. Following appropriate medical management, serotonin syndrome is generally associated with a favorable prognosis.

Epidemiological studies of serotonin syndrome are difficult as many physicians are unaware of the diagnosis or they may miss the syndrome due to its variable manifestations. In 1998 a survey conducted in England found that 85% of the general practitioners that had prescribed the antidepressant nefazodone were unaware of serotonin syndrome. The incidence may be increasing as a larger number of pro-serotonergic drugs (drugs which increase serotonin levels) are now being used in clinical practice. One postmarketing surveillance study identified an incidence of 0.4 cases per 1000 patient-months for patients who were taking nefazodone. Additionally, around 14 to 16 percent of persons who overdose on SSRIs are thought to develop serotonin syndrome.

The most widely recognized example of serotonin syndrome was the death of Libby Zion in 1984. Zion was a freshman at Bennington College at her death on March 5, 1984, at age 18. She died within 8 hours of her emergency admission to the New York Hospital Cornell Medical Center. She had an ongoing history of depression, and came to the Manhattan hospital on the evening of March 4, 1984, with a fever, agitation and "strange jerking motions" of her body. She also seemed disoriented at times. The emergency room physicians were unable to diagnose her condition definitively but admitted her for hydration and observation. Her death was caused by a combination of pethidine and phenelzine. A medical intern prescribed the pethidine. The case influenced graduate medical education and residency work hours. Limits were set on working hours for medical postgraduates, commonly referred to as interns or residents, in hospital training programs, and they also now require closer senior physician supervision.




</doc>
<doc id="29501" url="https://en.wikipedia.org/wiki?curid=29501" title="Sustainable development">
Sustainable development

Sustainable Development is the organizing principle for meeting human development goals while simultaneously sustaining the ability of natural systems to provide the natural resources and ecosystem services on which the economy and society depends. The desired result is a state of society where living conditions and resources are used to continue to meet human needs without undermining the integrity and stability of the natural system. Sustainable development can be defined as development that meets the needs of the present without compromising the ability of future generations to meet their own needs.
While the modern concept of sustainable development is yet derived mostly from the 1987 Brundtland Report, it is also rooted in earlier ideas about sustainable forest management and twentieth-century environmental concerns. As the concept developed, it has shifted its focus more towards the economic development, social development and environmental protection for future generations. It has been suggested that "the term 'sustainability' should be viewed as humanity's target goal of human-ecosystem equilibrium, while 'sustainable development' refers to the holistic approach and temporal processes that lead us to the end point of sustainability". Modern economies are endeavoring to reconcile ambitious economic development and obligations of preserving natural resources and ecosystems, as the two are usually seen as of conflicting nature. Instead of holding climate change commitments and other sustainability measures as a remedy to economic development, turning and leveraging them into market opportunities will do greater good. The economic development brought by such organized principles and practices in an economy is called Managed Sustainable Development (MSD).

The concept of sustainable development has been, and still is, subject to criticism, including the question of what is to be sustained in sustainable development. It has been argued that there is no such thing as a sustainable use of a non-renewable resource, since any positive rate of exploitation will eventually lead to the exhaustion of earth's finite stock; this perspective renders the Industrial Revolution as a whole unsustainable. It has also been argued that the meaning of the concept has opportunistically been stretched from 'conservation management' to 'economic development', and that the Brundtland Report promoted nothing but a business as usual strategy for world development, with an ambiguous and insubstantial concept attached as a public relations slogan. (see below).

Sustainability can be defined as the practice of maintaining world processes of productivity indefinitely—natural or human-made—by replacing resources used with resources of equal or greater value without degrading or endangering natural biotic systems. Sustainable development ties together concern for the carrying capacity of natural systems with the social, political, and economic challenges faced by humanity. Sustainability Science is the study of the concepts of sustainable development and environmental science. There is an additional focus on the present generations' responsibility to regenerate, maintain and improve planetary resources for use by future generations.

Sustainable development has its roots in ideas about sustainable forest management which were developed in Europe during the 17th and 18th centuries. In response to a growing awareness of the depletion of timber resources in England, John Evelyn argued that "sowing and planting of trees had to be regarded as a national duty of every landowner, in order to stop the destructive over- exploitation of natural resources" in his 1662 essay "Sylva". In 1713 Hans Carl von Carlowitz, a senior mining administrator in the service of Elector Frederick Augustus I of Saxony published "Sylvicultura economics", a 400-page work on forestry. Building upon the ideas of Evelyn and French minister Jean-Baptiste Colbert, von Carlowitz developed the concept of managing forests for sustained yield. His work influenced others, including Alexander von Humboldt and Georg Ludwig Hartig, eventually leading to the development of a science of forestry. This, in turn, influenced people like Gifford Pinchot, first head of the US Forest Service, whose approach to forest management was driven by the idea of wise use of resources, and Aldo Leopold whose land ethic was influential in the development of the environmental movement in the 1960s.

Following the publication of Rachel Carson's "Silent Spring" in 1962, the developing environmental movement drew attention to the relationship between economic growth and development and environmental degradation. Kenneth E. Boulding in his influential 1966 essay "The Economics of the Coming Spaceship Earth" identified the need for the economic system to fit itself to the ecological system with its limited pools of resources. Another milestone was the 1968 article by Garrett Hardin that popularized the term "tragedy of the commons". One of the first uses of the term sustainable in the contemporary sense was by the Club of Rome in 1972 in its classic report on the "Limits to Growth", written by a group of scientists led by Dennis and Donella Meadows of the Massachusetts Institute of Technology. Describing the desirable "state of global equilibrium", the authors wrote: "We are searching for a model output that represents a world system that is sustainable without sudden and uncontrolled collapse and capable of satisfying the basic material requirements of all of its people." That year also saw the publication of the influential "A Blueprint for Survival" book.

Following the Club of Rome report, an MIT research group prepared ten days of hearings on "Growth and Its Implication for the Future" (Roundtable Press, 1973) for the US Congress, the first hearings ever held on sustainable development. William Flynn Martin, David Dodson Gray, and Elizabeth Gray prepared the hearings under the Chairmanship of Congressman John Dingell.

In 1980 the International Union for the Conservation of Nature published a world conservation strategy that included one of the first references to sustainable development as a global priority and introduced the term "sustainable development". Two years later, the United Nations World Charter for Nature raised five principles of conservation by which human conduct affecting nature is to be guided and judged. In 1987 the United Nations World Commission on Environment and Development released the report "Our Common Future", commonly called the Brundtland Report. The report included what is now one of the most widely recognised definitions of sustainable development.

Since the Brundtland Report, the concept of sustainable development has developed beyond the initial intergenerational framework to focus more on the goal of "socially inclusive and environmentally sustainable economic growth". In 1992, the UN Conference on Environment and Development published the Earth Charter, which outlines the building of a just, sustainable, and peaceful global society in the 21st century. The action plan Agenda 21 for sustainable development identified information, integration, and participation as key building blocks to help countries achieve development that recognises these interdependent pillars. It emphasises that in sustainable development everyone is a user and provider of information. It stresses the need to change from old sector-centered ways of doing business to new approaches that involve cross-sectoral co-ordination and the integration of environmental and social concerns into all development processes. Furthermore, Agenda 21 emphasises that broad public participation in decision making is a fundamental prerequisite for achieving sustainable development.

Under the principles of the United Nations Charter the Millennium Declaration identified principles and treaties on sustainable development, including economic development, social development and environmental protection. Broadly defined, sustainable development is a systems approach to growth and development and to manage natural, produced, and social capital for the welfare of their own and future generations. The term sustainable development as used by the United Nations incorporates both issues associated with land development and broader issues of human development such as education, public health, and standard of living.

A 2013 study concluded that sustainability reporting should be reframed through the lens of four interconnected domains: ecology, economics, politics and culture.

Education for Sustainable Development (ESD) is defined as education that encourages changes in knowledge, skills, values and attitudes to enable a more sustainable and equitable society. ESD aims to empower and equip current and future generations to meet the needs using a balanced and integrated approach to the economic, social and environmental dimensions of sustainable development.

The concept of ESD was born from the need for education to address the growing and changing environmental challenges facing the planet. To do this, education must change to provide the knowledge, skills, values and attitudes that empower learners to contribute to sustainable development. At the same time, education must be strengthened in all agendas, programmes, and activities that promote sustainable development. Sustainable development must be integrated into education and education must be integrated into sustainable development. ESD promotes the integration of these critical sustainability issues in local and global contexts into the curriculum to prepare learners to understand and respond to the changing world. ESD aims to produce learning outcomes that include core competencies such as critical and systematic thinking, collaborative decision-making, and taking responsibility for the present and future generations. Since traditional single-directional delivery of knowledge is not sufficient to inspire learners to take action as responsible citizens, ESD entails rethinking the learning environment, physical and virtual. The learning environment itself must adapt and apply a whole-institution approach to embed the philosophy of sustainable development. Building the capacity of educators and policy support at international, regional, national and local levels helps drive changes in learning institutions. Empowered youth and local communities interacting with education institutions become key actors in advancing sustainable development. It is a great gift of god.

The launch of the UN Decade of Education for sustainable development (2005–2014) started a global movement to reorient education to address the challenges of sustainable development. Building on the achievement of the Decade, stated in the Aichi-Nagoya Declaration on ESD, UNESCO endorsed the Global Action Programme on ESD (GAP) in the 37th session of its General Conference. Acknowledged by UN general assembly Resolution A/RES/69/211 and launched at the UNESCO World Conference on ESD in 2014, the GAP aims to scale-up actions and good practices. UNESCO has a major role, along with its partners, in bringing about key achievements to ensure the principles of ESD are promoted through formal, non-formal and informal education.

International recognition of ESD as the key enabler for sustainable development is growing steadily. The role of ESD was recognized in three major UN summits on sustainable development: the 1992 UN Conference on Environment and Development (UNCED) in Rio de Janeiro, Brazil; the 2002 World Summit on Sustainable Development (WSSD) in Johannesburg, South Africa; and the 2012 UN Conference on Sustainable Development (UNCSD) in Rio de Janeiro. Other key global agreements such as the Paris Agreement (Article 12) also recognize the importance of ESD. Today, ESD is arguably at the heart of the 2030 Agenda for Sustainable Development and its 17 Sustainable Development Goals (SDGs) (United Nations, 2015). The SDGs recognize that all countries must stimulate action in the following key areas – people, planet, prosperity, peace and partnership – to tackle the global challenges that are crucial for the survival of humanity. ESD is explicitly mentioned in Target 4.7 of SDG4, which aims to ensure that all learners acquire the knowledge and skills needed to promote sustainable development and is understood as an important means to achieve all the other 16 SDGs (UNESCO, 2017).

Sustainable development can be thought of in terms of three spheres, dimensions, domains or pillars, i.e. the environment, the economy and society. The three-sphere framework was initially proposed by the economist Rene Passet in 1979. It has also been worded as "economic, environmental and social" or "ecology, economy and equity". This has been expanded by some authors to include a fourth pillar of culture, institutions or governance, or alternatively reconfigured as four domains of the social – ecology, economics, politics and culture, thus bringing economics back inside the social, and treating ecology as the intersection of the social and the natural.

The ecological stability of human settlements is part of the relationship between humans and their natural, social and built environments. Also termed human ecology, this broadens the focus of sustainable development to include the domain of human health. Fundamental human needs such as the availability and quality of air, water, food and shelter are also the ecological foundations for sustainable development; addressing public health risk through investments in ecosystem services can be a powerful and transformative force for sustainable development which, in this sense, extends to all species.

Environmental sustainability concerns the natural environment and how it endures and remains diverse and productive. Since natural resources are derived from the environment, the state of air, water, and the climate are of particular concern. The IPCC Fifth Assessment Report outlines current knowledge about scientific, technical and socio-economic information concerning climate change, and lists options for adaptation and mitigation. Environmental sustainability requires society to design activities to meet human needs while preserving the life support systems of the planet. This, for example, entails using water sustainably, using renewable energy, and sustainable material supplies (e.g. harvesting wood from forests at a rate that maintains the biomass and biodiversity).

An unsustainable situation occurs when natural capital (the sum total of nature's resources) is used up faster than it can be replenished. Sustainability requires that human activity only uses nature's resources at a rate at which they can be replenished naturally. Inherently the concept of sustainable development is intertwined with the concept of carrying capacity. Theoretically, the long-term result of environmental degradation is the inability to sustain human life. Such degradation on a global scale should imply an increase in human death rate until population falls to what the degraded environment can support. If the degradation continues beyond a certain tipping point or critical threshold it would lead to eventual extinction for humanity.

Integral elements for a sustainable development are research and innovation activities. A telling example is the European environmental research and innovation policy, which aims at defining and implementing a transformative agenda to greening the economy and the society as a whole so to achieve a truly sustainable development. Research and innovation in Europe is financially supported by the programme Horizon 2020, which is also open to participation worldwide. A promising direction towards sustainable development is to design systems that are flexible and reversible.

Pollution of the public resources is really not a different action, it just is a reverse tragedy of the commons, in that instead of taking something out, something is put into the commons. When the costs of polluting the commons are not calculated into the cost of the items consumed, then it becomes only natural to pollute, as the cost of pollution is external to the cost of the goods produced and the cost of cleaning the waste before it is discharged exceeds the cost of releasing the waste directly into the commons. So, the only way to solve this problem is by protecting the ecology of the commons by making it, through taxes or fines, more costly to release the waste directly into the commons than would be the cost of cleaning the waste before discharge.

So, one can try to appeal to the ethics of the situation by doing the right thing as an individual, but in the absence of any direct consequences, the individual will tend to do what is best for the person and not what is best for the common good of the public. Once again, this issue needs to be addressed. Because, left unaddressed, the development of the commonly owned property will become impossible to achieve in a sustainable way. So, this topic is central to the understanding of creating a sustainable situation from the management of the public resources that are used for personal use.

Sustainable agriculture consists of environment friendly methods of farming that allow the production of crops or livestock without damage to human or natural systems. It involves preventing adverse effects to soil, water, biodiversity, surrounding or downstream resources—as well as to those working or living on the farm or in neighbouring areas. The concept of sustainable agriculture extends intergenerationally, passing on a conserved or improved natural resource, biotic, and economic base rather than one which has been depleted or polluted. Elements of sustainable agriculture include permaculture, agroforestry, mixed farming, multiple cropping, and crop rotation. It involves agricultural methods that do not undermine the environment, smart farming technologies that enhance a quality environment for humans to thrive and reclaiming and transforming deserts into farmlands(Herman Daly, 2017). 

Numerous sustainability standards and certification systems exist, including organic certification, Rainforest Alliance, Fair Trade, UTZ Certified, Bird Friendly, and the Common Code for the Coffee Community (4C).

It has been suggested that because of rural poverty and overexploitation, environmental resources should be treated as important economic assets, called natural capital. Economic development has traditionally required a growth in the gross domestic product. This model of unlimited personal and GDP growth may be over. Sustainable development may involve improvements in the quality of life for many but may necessitate a decrease in resource consumption.
According to ecological economist , ecological economics is defined by its focus on nature, justice, and time. Issues of intergenerational equity, irreversibility of environmental change, uncertainty of long-term outcomes, and sustainable development guide ecological economic analysis and valuation.

As early as the 1970s, the concept of sustainability was used to describe an economy "in equilibrium with basic ecological support systems". Scientists in many fields have highlighted "The Limits to Growth", and economists have presented alternatives, for example a 'steady-state economy', to address concerns over the impacts of expanding human development on the planet. In 1987 the economist Edward Barbier published the study "The Concept of Sustainable Economic Development", where he recognised that goals of environmental conservation and economic development are not conflicting and can be reinforcing each other.

A World Bank study from 1999 concluded that based on the theory of genuine savings, policymakers have many possible interventions to increase sustainability, in macroeconomics or purely environmental. Several studies have noted that efficient policies for renewable energy and pollution are compatible with increasing human welfare, eventually reaching a golden-rule steady state.

The study, "Interpreting Sustainability in Economic Terms", found three pillars of sustainable development, interlinkage, intergenerational equity, and dynamic efficiency.

But Gilbert Rist points out that the World Bank has twisted the notion of sustainable development to prove that economic development need not be deterred in the interest of preserving the ecosystem. He writes: "From this angle, 'sustainable development' looks like a cover-up operation. ... The thing that is meant to be sustained is really 'development', not the tolerance capacity of the ecosystem or of human societies."

The World Bank, a leading producer of environmental knowledge, continues to advocate the win-win prospects for economic growth and ecological stability even as its economists express their doubts. Herman Daly, an economist for the Bank from 1988 to 1994, writes:
When authors of "WDR" '92 [the highly influential 1992 "World Development Report" that featured the environment] were drafting the report, they called me asking for examples of "win-win" strategies in my work. What could I say? None exists in that pure form; there are trade-offs, not "win-wins." But they want to see a world of "win-wins" based on articles of faith, not fact. I wanted to contribute because "WDR"s are important in the Bank, [because] task managers read [them] to find philosophical justification for their latest round of projects. But they did not want to hear about how things really are, or what I find in my work...

A meta review in 2002 looked at environmental and economic valuations and found a lack of "sustainability policies". A study in 2004 asked if we consume too much. A study concluded in 2007 that knowledge, manufactured and human capital (health and education) has not compensated for the degradation of natural capital in many parts of the world. It has been suggested that intergenerational equity can be incorporated into a sustainable development and decision making, as has become common in economic valuations of climate economics. A meta review in 2009 identified conditions for a strong case to act on climate change, and called for more work to fully account of the relevant economics and how it affects human welfare. According to free-market environmentalist John Baden "the improvement of environment quality depends on the market economy and the existence of legitimate and protected property rights". They enable the effective practice of personal responsibility and the development of mechanisms to protect the environment. The State can in this context "create conditions which encourage the people to save the environment".

Misum, Mistra Center for Sustainable Markets, based at Stockholm School of Economics, aims to provide policy research and advice to Swedish and international actors on Sustainable Markets. Misum is a cross-disciplinary and multi-stakeholder knowledge center dedicated to sustainability and sustainable markets and contains three research platforms: Sustainability in Financial Markets (Mistra Financial Systems), Sustainability in Production and Consumption and Sustainable Socio-Economic Development.

The total environment includes not just the biosphere of earth, air, and water, but also human interactions with these things, with nature, and what humans have created as their surroundings.

As countries around the world continue to advance economically, they put a strain on the ability of the natural environment to absorb the high level of pollutants that are created as a part of this economic growth. Therefore, solutions need to be found so that the economies of the world can continue to grow, but not at the expense of the public good. In the world of economics the amount of environmental quality must be considered as limited in supply and therefore is treated as a scarce resource. This is a resource to be protected. One common way to analyze possible outcomes of policy decisions on the scarce resource is to do a cost-benefit analysis. This type of analysis contrasts different options of resource allocation and, based on an evaluation of the expected courses of action and the consequences of these actions, the optimal way to do so in the light of different policy goals can be elicited.

Benefit-cost analysis basically can look at several ways of solving a problem and then assigning the best route for a solution, based on the set of consequences that would result from the further development of the individual courses of action, and then choosing the course of action that results in the least amount of damage to the expected outcome for the environmental quality that remains after that development or process takes place. Further complicating this analysis are the interrelationships of the various parts of the environment that might be impacted by the chosen course of action. Sometimes it is almost impossible to predict the various outcomes of a course of action, due to the unexpected consequences and the amount of unknowns that are not accounted for in the benefit-cost analysis.

Sustainable energy is clean and can be used over a long period of time. Unlike fossil fuels and biofuels that provide the bulk of the worlds energy, renewable energy sources like hydroelectric, solar and wind energy produce far less pollution. Solar energy is commonly used on public parking meters, street lights and the roof of buildings. Wind power has expanded quickly, its share of worldwide electricity usage at the end of 2014 was 3.1%. Most of California's fossil fuel infrastructures are sited in or near low-income communities, and have traditionally suffered the most from California's fossil fuel energy system. These communities are historically left out during the decision-making process, and often end up with dirty power plants and other dirty energy projects that poison the air and harm the area. These toxicants are major contributors to health problems in the communities. As renewable energy becomes more common, fossil fuel infrastructures are replaced by renewables and we may begin to see a Renewable energy transition, providing better social equity to these communities.
Overall, and in the long run, sustainable development in the field of energy is also deemed to contribute to economic sustainability and national security of communities, thus being increasingly encouraged through investment policies.

One of the core concepts in sustainable development is that technology can be used to assist people to meet their developmental needs. Technology to meet these sustainable development needs is often referred to as appropriate technology, which is an ideological movement (and its manifestations) originally articulated as intermediate technology by the economist E. F. Schumacher in his influential work "Small Is Beautiful" and now covers a wide range of technologies. Both Schumacher and many modern-day proponents of appropriate technology also emphasise the technology as people-centered. Today appropriate technology is often developed using open source principles, which have led to open-source appropriate technology (OSAT) and thus many of the plans of the technology can be freely found on the Internet. OSAT has been proposed as a new model of enabling innovation for sustainable development.

Transportation is a large contributor to greenhouse gas emissions. It is said that one-third of all gases produced are due to transportation. Motorized transport also releases exhaust fumes that contain particulate matter which is hazardous to human health and a contributor to climate change.

Sustainable transport has many social and economic benefits that can accelerate local sustainable development. According to a series of reports by the Low Emission Development Strategies Global Partnership (LEDS GP), sustainable transport can help create jobs, improve commuter safety through investment in bicycle lanes and pedestrian pathways, make access to employment and social opportunities more affordable and efficient. It also offers a practical opportunity to save people's time and household income as well as government budgets, making investment in sustainable transport a 'win-win' opportunity.

Some Western countries are making transportation more sustainable in both long-term and short-term implementations. An example is the modification in available transportation in Freiburg, Germany. The city has implemented extensive methods of public transportation, cycling, and walking, along with large areas where cars are not allowed.

Since many Western countries are highly automobile-oriented, the main transit that people use is personal vehicles. About 80% of their travel involves cars. Therefore, California, is one of the highest greenhouse gases emitters in the United States. The federal government has to come up with some plans to reduce the total number of vehicle trips to lower greenhouse gases emission. Such as:


Other states and nations have built efforts to translate knowledge in behavioral economics into evidence-based sustainable transportation policies.

The most broadly accepted criterion for corporate sustainability constitutes a firm's efficient use of natural capital. This eco-efficiency is usually calculated as the economic value added by a firm in relation to its aggregated ecological impact. This idea has been popularised by the World Business Council for Sustainable Development (WBCSD) under the following definition: "Eco-efficiency is achieved by the delivery of competitively priced goods and services that satisfy human needs and bring quality of life, while progressively reducing ecological impacts and resource intensity throughout the life-cycle to a level at least in line with the earth's carrying capacity" (DeSimone and Popoff, 1997: 47).

Similar to the eco-efficiency concept but so far less explored is the second criterion for corporate sustainability. Socio-efficiency describes the relation between a firm's value added and its social impact. Whereas, it can be assumed that most corporate impacts on the environment are negative (apart from rare exceptions such as the planting of trees) this is not true for social impacts. These can be either positive (e.g. corporate giving, creation of employment) or negative (e.g. work accidents, mobbing of employees, human rights abuses). Depending on the type of impact socio-efficiency thus either tries to minimise negative social impacts (i.e. accidents per value added) or maximise positive social impacts (i.e. donations per value added) in relation to the value added.

Both eco-efficiency and socio-efficiency are concerned primarily with increasing economic sustainability. In this process they instrumentalise both natural and social capital aiming to benefit from win-win situations. However, as Dyllick and Hockerts point out the business case alone will not be sufficient to realise sustainable development. They point towards eco-effectiveness, socio-effectiveness, sufficiency, and eco-equity as four criteria that need to be met if sustainable development is to be reached.

CASI Global, New York "CSR & Sustainability together lead to sustainable development. CSR as in corporate social responsibility is not what you do with your profits, but is the way you make profits. This means CSR is a part of every department of the company value chain and not a part of HR / independent department. Sustainability as in effects towards Human resources, Environment and Ecology has to be measured within each department of the company." CASI Global

At the present time, sustainable development can reduce poverty. Sustainable development reduces poverty through financial (among other things, a balanced budget), environmental (living conditions), and social (including equality of income) means.

In sustainable architecture the recent movements of New Urbanism and New Classical architecture promote a sustainable approach towards construction, that appreciates and develops smart growth, architectural tradition and classical design. This in contrast to modernist and International Style architecture, as well as opposing to solitary housing estates and suburban sprawl, with long commuting distances and large ecological footprints. Both trends started in the 1980s. (Sustainable architecture is predominantly relevant to the economics domain while architectural landscaping pertains more to the ecological domain.)

A study concluded that social indicators and, therefore, sustainable development indicators, are scientific constructs whose principal objective is to inform public policy-making. The International Institute for Sustainable Development has similarly developed a political policy framework, linked to a sustainability index for establishing measurable entities and metrics. The framework consists of six core areas:

The United Nations Global Compact Cities Programme has defined sustainable political development in a way that broadens the usual definition beyond states and governance. The political is defined as the domain of practices and meanings associated with basic issues of social power as they pertain to the organisation, authorisation, legitimation and regulation of a social life held in common. This definition is in accord with the view that political change is important for responding to economic, ecological and cultural challenges. It also means that the politics of economic change can be addressed. They have listed seven subdomains of the domain of politics:


This accords with the Brundtland Commission emphasis on development that is guided by human rights principles (see above).

Working with a different emphasis, some researchers and institutions have pointed out that a fourth dimension should be added to the dimensions of sustainable development, since the triple-bottom-line dimensions of economic, environmental and social do not seem to be enough to reflect the complexity of contemporary society. In this context, the Agenda 21 for culture and the United Cities and Local Governments (UCLG) Executive Bureau lead the preparation of the policy statement "Culture: Fourth Pillar of Sustainable Development", passed on 17 November 2010, in the framework of the World Summit of Local and Regional Leaders – 3rd World Congress of UCLG, held in Mexico City. This document inaugurates a new perspective and points to the relation between culture and sustainable development through a dual approach: developing a solid cultural policy and advocating a cultural dimension in all public policies. The Circles of Sustainability approach distinguishes the four domains of economic, ecological, political and cultural sustainability.

Other organizations have also supported the idea of a fourth domain of sustainable development. The Network of Excellence "Sustainable Development in a Diverse World", sponsored by the European Union, integrates multidisciplinary capacities and interprets cultural diversity as a key element of a new strategy for sustainable development. The Fourth Pillar of Sustainable Development Theory has been referenced by executive director of IMI Institute at UNESCO Vito Di Bari in his manifesto of art and architectural movement Neo-Futurism, whose name was inspired by the 1987 United Nations' report Our Common Future. The Circles of Sustainability approach used by Metropolis defines the (fourth) cultural domain as practices, discourses, and material expressions, which, over time, express continuities and discontinuities of social meaning.

Recently, human-centered design and cultural collaboration have been popular frameworks for sustainable development in marginalized communities. These frameworks involve open dialogue which entails sharing, debating, and discussing, as well as holistic evaluation of the site of development. Especially when working on sustainable development in marginalized communities, cultural emphasis is a crucial factor in project decisions, since it largely affects aspects of their lives and traditions. Collaborators use Articulation Theory in co-designing. This allows for them to understand each other's thought process and their comprehension of the sustainable projects. By using the method of co-design, the beneficiaries' holistic needs are being considered. Final decisions and implementations are made with respect to sociocultural and ecological factors.

The user-oriented framework relies heavily on user participation and user feedback in the planning process. Users are able to provide new perspective and ideas, which can be considered in a new round of improvements and changes. It is said that increased user participation in the design process can garner a more comprehensive understanding of the design issues, due to more contextual and emotional transparency between researcher and participant. A key element of human centered design is applied ethnography, which was a research method adopted from cultural anthropology. This research method requires researchers to be fully immersed in the observation so that implicit details are also recorded.

Many communities express environmental concerns, so life cycle analysis is often conducted when assessing the sustainability of a product or prototype. The assessment is done in stages with meticulous cycles of planning, design, implementation, and evaluation. The decision to choose materials is heavily weighted on its longevity, renewability, and efficiency. These factors ensure that researchers are conscious of community values that align with positive environmental, social, and economic impacts.

The United Nations Conference on Sustainable Development (UNCSD; also known as Rio 2012) was the third international conference on sustainable development, which aimed at reconciling the economic and environmental goals of the global community. An outcome of this conference was the development of the Sustainable Development Goals that aim to promote sustainable progress and eliminate inequalities around the world. However, few nations met the World Wide Fund for Nature's definition of sustainable development criteria established in 2006. Although some nations are more developed than others, all nations are constantly developing because each nation struggles with perpetuating disparities, inequalities and unequal access to fundamental rights and freedoms.

In 2007 a report for the U.S. Environmental Protection Agency stated: "While much discussion and effort has gone into sustainability indicators, none of the resulting systems clearly tells us whether our society is sustainable. At best, they can tell us that we are heading in the wrong direction, or that our current activities are not sustainable. More often, they simply draw our attention to the existence of problems, doing little to tell us the origin of those problems and nothing to tell us how to solve them." Nevertheless, a majority of authors assume that a set of well defined and harmonised indicators is the only way to make sustainability tangible. Those indicators are expected to be identified and adjusted through empirical observations (trial and error).

The most common critiques are related to issues like data quality, comparability, objective function and the necessary resources. However a more general criticism is coming from the project management community: How can a sustainable development be achieved at global level if we cannot monitor it in any single project?

The Cuban-born researcher and entrepreneur Sonia Bueno suggests an alternative approach that is based upon the integral, long-term cost-benefit relationship as a measure and monitoring tool for the sustainability of every project, activity or enterprise. Furthermore, this concept aims to be a practical guideline towards sustainable development following the principle of conservation and increment of value rather than restricting the consumption of resources.

Reasonable qualifications of sustainability are seen U.S. Green Building Council's (USGBC) Leadership in Energy and Environmental Design (LEED). This design incorporates some ecological, economic, and social elements. The goals presented by LEED design goals are sustainable sites, water efficiency, energy consumption and atmospheric emission reduction, material and resources efficiency, and indoor environmental quality. Although amount of structures for sustainability development is many, these qualification has become a standard for sustainable building.

Recent research efforts created also the SDEWES Index to benchmark the performance of cities across aspects that are related to energy, water and environment systems. The SDEWES Index consists of 7 dimensions, 35 indicators, and close to 20 sub-indicators. It is currently applied to 58 cities.

The sustainable development debate is based on the assumption that societies need to manage three types of capital (economic, social, and natural), which may be non-substitutable and whose consumption might be irreversible. Leading ecological economist and steady-state theorist Herman Daly, for example, points to the fact that natural capital can not necessarily be substituted by economic capital. While it is possible that we can find ways to replace some natural resources, it is much more unlikely that they will ever be able to replace eco-system services, such as the protection provided by the ozone layer, or the climate stabilizing function of the Amazonian forest. In fact natural capital, social capital and economic capital are often complementarities. A further obstacle to substitutability lies also in the multi-functionality of many natural resources. Forests, for example, not only provide the raw material for paper (which can be substituted quite easily), but they also maintain biodiversity, regulate water flow, and absorb CO2.

Another problem of natural and social capital deterioration lies in their partial irreversibility. The loss of biodiversity, for example, is often definitive. The same can be true for cultural diversity. For example, with globalisation advancing quickly the number of indigenous languages is dropping at alarming rates. Moreover, the depletion of natural and social capital may have non-linear consequences. Consumption of natural and social capital may have no observable impact until a certain threshold is reached. A lake can, for example, absorb nutrients for a long time while actually increasing its productivity. However, once a certain level of algae is reached lack of oxygen causes the lake's ecosystem to break down suddenly.

If the degradation of natural and social capital has such important consequence the question arises why action is not taken more systematically to alleviate it. Cohen and Winn point to four types of market failure as possible explanations: First, while the benefits of natural or social capital depletion can usually be privatised, the costs are often externalised (i.e. they are borne not by the party responsible but by society in general). Second, natural capital is often undervalued by society since we are not fully aware of the real cost of the depletion of natural capital. Information asymmetry is a third reason—often the link between cause and effect is obscured, making it difficult for actors to make informed choices. Cohen and Winn close with the realization that contrary to economic theory many firms are not perfect optimisers. They postulate that firms often do not optimise resource allocation because they are caught in a "business as usual" mentality.

"Main page: Education for sustainable development"

Education must be revisited in light of a renewed vision of sustainable human and social development that is both equitable and viable. This vision of sustainability must take into consideration the social, environmental and economic dimensions of human development and the various ways in which these relate to education: 'An empowering education is one that builds the human resources we need to be productive, to continue to learn, to solve problems, to be creative, and to live together and with nature in peace and harmony. When nations ensure that such an education is accessible to all throughout their lives, a quiet revolution is set in motion: education becomes the engine of sustainable development and the key to a better world.'


It has been argued that since the 1960s, the concept of sustainable development has changed from "conservation management" to "economic development", whereby the original meaning of the concept has been stretched somewhat.

In the 1960s, the international community realised that many African countries needed national plans to safeguard wildlife habitats, and that rural areas had to confront the limits imposed by soil, climate and water availability. This was a strategy of conservation management. In the 1970s, however, the focus shifted to the broader issues of the provisioning of basic human needs, community participation as well as appropriate technology use throughout the developing countries (and not just in Africa). This was a strategy of economic development, and the strategy was carried even further by the Brundtland Commission's report on "Our Common Future" when the issues went from regional to international in scope and application. In effect, the conservationists were crowded out and superseded by the developers.

But shifting the focus of sustainable development from conservation to development has had the imperceptible effect of stretching the original forest management term of sustainable yield from the use of renewable resources only (like forestry), to now also accounting for the use of non-renewable resources (like minerals). This stretching of the term has been questioned. Thus, environmental economist Kerry Turner has argued that literally, there can be no such thing as overall "sustainable development" in an industrialised world economy that remains heavily dependent on the extraction of earth's finite stock of exhaustible mineral resources: "It makes no sense to talk about the sustainable use of a non-renewable resource (even with substantial recycling effort and reduction in use rates). Any positive rate of exploitation will eventually lead to exhaustion of the finite stock."

In effect, it has been argued that the industrial revolution as a whole is unsustainable.

One critic has argued that the Brundtland Commission promoted nothing but a business as usual strategy for world development, with the ambiguous and insubstantial concept of "sustainable development" attached as a public relations slogan: The report on "Our Common Future" was largely the result of a political bargaining process involving many special interest groups, all put together to create a common appeal of political acceptability across borders. After World War II, the notion of "development" had been established in the West to imply the projection of the American model of society onto the rest of the world. In the 1970s and 1980s, this notion was broadened somewhat to also imply human rights, basic human needs and finally, ecological issues. The emphasis of the report was on helping poor nations out of poverty and meeting the basic needs of their growing populations—as usual. This issue demanded more economic growth, also in the rich countries, who would then import more goods from the poor countries to help them out—as usual. When the discussion switched to , the obvious dilemma was left aside by calling for economic growth with improved resource efficiency, or what was termed "a change in the "quality" of growth". However, most countries in the West had experienced such improved resource efficiency since the early-20th century already and as usual; only, this improvement had been more than offset by continuing industrial expansion, to the effect that world resource consumption was now higher than ever before—and these two historical trends were completely ignored in the report. Taken together, the policy of perpetual economic growth for the entire planet remained virtually intact. Since the publication of the report, the ambiguous and insubstantial slogan of "sustainable development" has marched on worldwide.




</doc>
<doc id="29507" url="https://en.wikipedia.org/wiki?curid=29507" title="Scientific American">
Scientific American

Scientific American (informally abbreviated SciAm or sometimes SA) is an American popular science magazine. Many famous scientists, including Albert Einstein, have contributed articles to it. It is the oldest continuously published monthly magazine in the United States (though it only became monthly in 1921).

"Scientific American" was founded by inventor and publisher Rufus M. Porter in 1845 as a four-page weekly newspaper. Throughout its early years, much emphasis was placed on reports of what was going on at the U.S. Patent Office. It also reported on a broad range of inventions including perpetual motion machines, an 1860 device for buoying vessels by Abraham Lincoln, and the universal joint which now can be found in nearly every automobile manufactured. Current issues include a "this date in history" section, featuring excerpts from articles originally published 50, 100, and 150 years earlier. Topics include humorous incidents, wrong-headed theories, and noteworthy advances in the history of science and technology. It started as a weekly publication in August 1845 before turning into monthly in November 1921. 

Porter sold the publication to Alfred Ely Beach and Orson Desaix Munn a mere ten months after founding it. Until 1948, it remained owned by Munn & Company. Under Munn's grandson, Orson Desaix Munn III, it had evolved into something of a "workbench" publication, similar to the twentieth-century incarnation of "Popular Science".

In the years after World War II, the magazine fell into decline. In 1948, three partners who were planning on starting a new popular science magazine, to be called "The Sciences", purchased the assets of the old "Scientific American" instead and put its name on the designs they had created for their new magazine. Thus the partnerspublisher Gerard Piel, editor Dennis Flanagan, and general manager Donald H. Miller, Jr.essentially created a new magazine. Miller retired in 1979, Flanagan and Piel in 1984, when Gerard Piel's son Jonathan became president and editor; circulation had grown fifteen-fold since 1948. In 1986, it was sold to the Holtzbrinck group of Germany, which has owned it since.

In the fall of 2008, "Scientific American" was put under the control of Nature Publishing Group, a division of Holtzbrinck.

Donald Miller died in December 1998, Gerard Piel in September 2004 and Dennis Flanagan in January 2005. Mariette DiChristina became editor-in-chief after John Rennie stepped down in June 2009, and stepped down herself in September 2019. On April 13, 2020, Laura Helmuth assumed the role of Editor-in-chief.

"Scientific American" published its first foreign edition in 1890, the Spanish-language "La America Cientifica". Publication was suspended in 1905, and another 63 years would pass before another foreign-language edition appeared: In 1968, an Italian edition, "Le Scienze", was launched, and a Japanese edition, ', followed three years later. A new Spanish edition, "Investigación y Ciencia" was launched in Spain in 1976, followed by a French edition, ', in France in 1977, and a German edition, ', in Germany in 1978. A Russian edition "V Mire Nauki" was launched in the Soviet Union in 1983, and continues in the present-day Russian Federation. "Kexue" (科学, "Science" in Chinese), a simplified Chinese edition launched in 1979, was the first Western magazine published in the People's Republic of China. Founded in Chongqing, the simplified Chinese magazine was transferred to Beijing in 2001. Later in 2005, a newer edition, "Global Science" (环球科学), was published instead of "Kexue", which shut down due to financial problems. A traditional Chinese edition, known as ', was introduced to Taiwan in 2002. The Hungarian edition "Tudomány" existed between 1984 and 1992. In 1986, an Arabic edition, "", was published. In 2002, a Portuguese edition was launched in Brazil.

Today, "Scientific American" publishes 18 foreign-language editions around the globe: Arabic, Brazilian Portuguese, Simplified Chinese, Traditional Chinese, Czech, Dutch, French, German, Greek, Hebrew, Italian, Japanese, Korean, Lithuanian (discontinued after 15 issues), Polish, Romanian, Russian, and Spanish.

From 1902 to 1911, "Scientific American" supervised the publication of the "Encyclopedia Americana", which during some of that period was known as "The Americana".

It originally styled itself "The Advocate of Industry and Enterprise" and "Journal of Mechanical and other Improvements". On the front page of the first issue was the engraving of "Improved Rail-Road Cars". The masthead had a commentary as follows:

The commentary under the illustration gives the flavor of its style at the time:
Also in the first issue is commentary on Signor Muzio Muzzi's proposed device for aerial navigation.



The Scientific American 50 award was started in 2002 to recognize contributions to science and technology during the magazine's previous year. The magazine's 50 awards cover many categories including agriculture, communications, defence, environment, and medical diagnostics. The complete list of each year's winners appear in the December issue of the magazine, as well as on the magazine's web site.

In March 1996, "Scientific American" launched its own website that includes articles from current and past issues, online-only features, daily news, weird science, special reports, trivia, "Scidoku" and more. The website introduced a paywall in April 2019, with readers able to view a few articles for free each month.

Notable features have included:

From 1990 to 2005 "Scientific American" produced a television program on PBS called "Scientific American Frontiers" with hosts Woodie Flowers and Alan Alda.

From 1983 to 1997, "Scientific American" has produced an encyclopedia set of volumes from their publishing division, the Scientific American Library. These books were not sold in retail stores, but as a Book of the Month Club selection priced from $24.95 to $32.95. Topics covered dozens of areas of scientific knowledge and included in-depth essays on: The Animal Mind; Atmosphere, Climate, and Change; Beyond the Third Dimension; Cosmic Clouds; Cycles of Life • Civilization and the Biosphere; The Discovery Of Subatomic Particles; Diversity and the Tropical Rain Forest; Earthquakes and Geological Discovery; Exploring Planetary Worlds; Gravity's Fatal Attraction; Fire; Fossils And The History Of Life; From Quarks to the Cosmos; A Guided Tour Of The Living Cell; Human Diversity; Perception; The Solar System; Sun and Earth; The Science of Words (Linguistics); The Science Of Musical Sound; The Second Law (of Thermodynamics); Stars; Supercomputing and the Transformation of Science.

Scientific American launched a publishing imprint in 2010 in partnership with Farrar, Straus and Giroux.


In April 1950, the U.S. Atomic Energy Commission ordered "Scientific American" to cease publication of an issue containing an article by Hans Bethe that appeared to reveal classified information about the thermonuclear hydrogen bomb. Subsequent review of the material determined that the AEC had overreacted. The incident was important for the "new" "Scientific American"'s history, as the AEC's decision to burn 3000 copies of an early press-run of the magazine containing the offending material appeared to be "book burning in a free society" when publisher Gerard Piel leaked the incident to the press.

In its January 2002 issue, "Scientific American" published a series of criticisms of the Bjørn Lomborg book "The Skeptical Environmentalist". Cato Institute fellow Patrick J. Michaels said the attacks came because the book "threatens billions of taxpayer dollars that go into the global change kitty every year." Journalist Ronald Bailey called the criticism "disturbing" and "dishonest", writing, "The subhead of the review section, 'Science defends itself against "The Skeptical Environmentalist",' gives the show away: Religious and political views need to defend themselves against criticism, but science is supposed to be a process for determining the facts."

The May 2007 issue featured a column by Michael Shermer calling for a United States pullout from the Iraq War. In response, "Wall Street Journal" online columnist James Taranto jokingly called "Scientific American" "a liberal political magazine".

The publisher was criticized in 2009 when it notified collegiate libraries that yearly subscription prices for the magazine would increase by nearly 500% for print and 50% for online access to $1500 yearly.

An editorial in the September 2016 issue of Scientific American attacked U.S. presidential candidate Donald Trump for "anti-science" attitudes and rhetoric. This marked the first time that the publication forayed into commenting on U.S. presidential politics.



In 2013, Danielle N. Lee, a female scientist who blogged at "Scientific American", was called a "whore" in an email by an editor at the science website "Biology Online" after refusing to write professional content without compensation. When Lee, outraged about the email, wrote a rebuttal on her "Scientific American" blog, the editor-in-chief of "Scientific American", Mariette DiChristina, removed the post, sparking an outrage by supporters of Lee. While DiChristina cited legal reasons for removing the blog, others criticized her for censoring Lee. The editor at Biology Online was fired after the incident.

The controversy widened in the ensuing days. The magazine's blog editor, Bora Zivkovic, was the subject of allegations of sexual harassment by another blogger, Monica Byrne. Although the alleged incident had occurred about a year earlier, editor Mariette DiChristina informed readers that the incident had been investigated and resolved to Ms. Byrne's satisfaction. However, the incident involving Dr. Lee had prompted Ms. Byrne to reveal the identity of Zivkovic, following the latter's support of Dr. Lee. Zivkovic responded on Twitter and his own blog, admitting the incident with Ms. Byrne had taken place. His blog post apologized to Ms. Byrne, and referred to the incident as "singular", stating that his behavior was not "engaged in before or since."

Due to the allegations, Zivkovic resigned from the board of Science Online, the popular science blogging conference that he helped establish. Following Zivkovic's admission, several prominent female bloggers, including other bloggers for the magazine, wrote their own accounts that contradicted Zivkovic's assertions, alleging additional incidents of sexual harassment. A day after these new revelations, Zivkovic resigned his position at "Scientific American", according to a press release from the magazine.




</doc>
<doc id="29511" url="https://en.wikipedia.org/wiki?curid=29511" title="Siouxsie and the Banshees">
Siouxsie and the Banshees

Siouxsie and the Banshees were a British rock band, formed in London in 1976 by vocalist Siouxsie Sioux and bass guitarist Steven Severin. They have been widely influential, both over their contemporaries and with later acts. "Q" included John McKay's guitar playing on "Hong Kong Garden" in their list of "100 Greatest Guitar Track Ever", while "Mojo" rated guitarist John McGeoch in their list of "100 Greatest Guitarists of All Time" for his work on "Spellbound". "The Times" cited the group as "one of the most audacious and uncompromising musical adventurers of the post-punk era".

Initially associated with the punk scene, the band rapidly evolved to create "a form of post-punk discord full of daring rhythmic and sonic experimentation". Their debut album "The Scream" was released in 1978 to widespread critical acclaim. In 1980, they changed their musical direction and became "almost a different band" with "Kaleidoscope", which peaked at number 5 in the UK Albums Chart. With "Juju" (1981) which also reached the top 10, they became an influence on the emerging gothic scene. In 1988, the band made a breakthrough in North America with the multifaceted album "Peepshow", which received critical praise. With substantial support from alternative rock radio stations, they achieved a mainstream hit in the US in 1991 with the single "Kiss Them for Me".

During their career, Siouxsie and the Banshees released 11 studio albums and 30 singles. The band experienced several line-up changes, with Siouxsie and Severin being the only constant members. They disbanded in 1996, with Siouxsie and drummer Budgie continuing to record music as the Creatures, a second band they had formed in the early 1980s. In 2004, Siouxsie began a solo career.

Siouxsie Sioux and Steven Severin met at a Roxy Music concert in September 1975, at a time when glam rock had faded and there was nothing new coming through with which they could identify. From February 1976, Siouxsie, Severin and some friends began to follow an unsigned band, the Sex Pistols. Journalist Caroline Coon dubbed them the "Bromley Contingent", as most of them came from the Bromley region of South London, a label Severin came to despise. "There was no such thing, it was just a bunch of people drawn together by the way they felt and they looked". They were all inspired by the Sex Pistols and their uncompromising attitude. When they learned that one of the bands scheduled to play the 100 Club Punk Festival, organised by Sex Pistols manager Malcolm McLaren, were pulling out from the bill at the last minute, Siouxsie suggested that she and Severin play, even though they had no band name or additional members. Two days later, the pair appeared at the festival held in London on 20 September 1976. With two borrowed musicians at their side, Marco Pirroni on guitar and John Simon Ritchie (already commonly known as Sid Vicious) on drums, their set consisted of a 20-minute improvisation based on "The Lord's Prayer".

While the band intended to split up after the gig, they were asked to play again. Two months later, Siouxsie and Severin recruited drummer Kenny Morris and guitarist Pete Fenton. After playing several gigs in early 1977, they realised that Fenton did not fit in because he was "a real rock guitarist". John McKay finally took his place in July. Their first live appearance on television took place in November on Manchester's Granada, on Tony Wilson's TV show "So It Goes". They then recorded their first John Peel session for BBC radio in which they premiered a new song "Metal Postcard", introducing a "motorik austerity" in the drums patterns.. The band described their music as "cold, machine-like and passionate at the same time". Appearing on the front cover of UK weekly "Sounds" magazine, Vivien Goldman wrote: "they sound like a 21st century industrial plant".

While the band sold-out venues in London in early 1978, they still had problems getting the right recording contract that could give them "complete artistic control". Polydor finally offered this guarantee and signed them in June. Their first single, "Hong Kong Garden", featuring a xylophone motif, reached the top 10 in the UK shortly after. A "NME" review hailed it as "a bright, vivid narrative, something like snapshots from the window of a speeding Japanese train, power charged by the most original, intoxicating guitar playing I heard in a long, long time".

The band released their debut album, "The Scream", in November 1978. Nick Kent of "NME" said of the record: "The band sounds like some unique hybrid of the Velvet Underground mated with much of the ingenuity of "Tago Mago"-era Can, if any parallel can be drawn". At the end of the article, he added this remark: "Certainly, the traditional three-piece sound has never been used in a more unorthodox fashion with such stunning results".

The Banshees' second album, "Join Hands", was released in 1979. In "Melody Maker", Jon Savage described "Poppy Day" as "a short, powerful evocation of the Great War graveyards", and "Record Mirror" described the whole record as a dangerous work that "should be heard". The Banshees embarked on a major tour to promote the album. A few dates into the tour in September, Morris and McKay left an in-store signing after an argument and quit the band. In need of replacements to fulfill tour dates, the Banshees' manager called drummer Budgie, formerly with the Slits, and asked him to audition. Budgie was hired, but Siouxsie and Severin had no success auditioning guitarists. Robert Smith of the Cure offered his services in case they could not find a guitarist (his group were already the support band on the tour), so the band held him to it after seeing too many "rock virtuosos". The tour resumed in September and after the last concert, Smith returned to the Cure.

Drummer Budgie became a permanent member, and the band entered the studios to record the single "Happy House" with guitarist John McGeoch, then still a member of Magazine. Their third album, "Kaleidoscope", released in 1980, saw the Banshees exploring new musical territories with the use of other instruments like synthesizers, sitars and drum machines. The group initially had a concept of making each song sound completely different, without regard to whether or not the material could be performed in concert. "Melody Maker" described the result as "a kaleidoscope of sound and imagery, new forms, and content, flashing before our eyes". "Kaleidoscope" was a commercial success, peaking at number 5 in the UK albums chart. This lineup, featuring McGeoch on guitar, toured the United States for the first time in support of the album, playing their first shows in New York City in November 1980.
For "Juju" (1981), the band took a different approach and practised the songs in concert first before recording them. "Juju", according to Severin, became an unintentional concept album that "drew on darker elements". "Sounds" hailed it as "intriguing, intense, brooding and powerfully atmospheric". The album later peaked at number 7 in the UK albums chart and became one of their biggest sellers. McGeoch's guitar contributions on "Juju" would be later praised by Johnny Marr of the Smiths.

During the 1981 accompanying tour, Siouxsie and Budgie secretly became a couple. At the same time, they also began a drum-and-voice duo called the Creatures, releasing their first EP, "Wild Things".

The Banshees followed in 1982 with the psychedelic "A Kiss in the Dreamhouse". The record, featuring strings on several numbers, was an intentional contrast to their previous work, with Severin later describing it as a "sexy album". The British press greeted it enthusiastically. Richard Cook finished his "NME" review with this sentence: "I promise...this music will take your breath away". At that time, McGeoch was struggling with alcohol problems, and was hospitalised on his return to a promotional trip from Madrid. The band fired him shortly thereafter. Severin asked Robert Smith to take over guitarist duties again; Smith accepted and rejoined the group in November 1982.

During 1983, the band members worked on several side projects; Siouxsie and Budgie composed the first Creatures album, "Feast", while Severin and Smith recorded as the Glove. Smith then insisted on documenting his time with the Banshees, so the group released a cover version of the Beatles' "Dear Prudence" in September 1983. It became their biggest hit, reaching number 3 on the UK Singles Chart. They also released a live album, "Nocturne", and completed their sixth studio album, "Hyæna". Shortly before its release in May 1984, Smith left the group, citing health issues due to an overloaded schedule, being in two bands at once.

Ex-Clock DVA guitarist John Valentine Carruthers replaced him. The Banshees then reworked four numbers from their repertoire, augmented by a string section, for "The Thorn" EP. "NME" praised the project at its release: "The power of a classical orchestra is the perfect foil for the band's grindingly insistent sounds". The new Banshees lineup spent much of 1985 working on a new record, "Tinderbox". The group finished the song "Cities in Dust" before the album, so they rushed its release as a single prior to their longest tour of the UK. "Tinderbox" was finally released in April 1986. "Sounds" magazine noted: ""Tinderbox" is a refreshing slant on the Banshees' disturbing perspective and restores their vivid shades to pop's pale palette". Due to the length of time spent working on "Tinderbox", the group desired spontaneity and decided to record an album of cover songs, "Through the Looking Glass", in 1987. "Mojo" magazine later praised their version of "Strange Fruit". After the album's release, the band realised Carruthers was no longer fitting in and decided to work on new material as a trio.

Following a lengthy break, the band recruited multi-instrumentalist Martin McCarrick and guitarist Jon Klein. The quintet recorded "Peepshow" in 1988, with non-traditional rock instrumentation including cello and accordion. "Q" magazine praised the album in its 5-star review: ""Peepshow" takes place in some distorted fairground of the mind where weird and wonderful shapes loom". The first single, "Peek-a-Boo", was seen by critics as a "brave move" with horns and dance elements. "Sounds" wrote: "The snare gets slapped, Siouxsie's voice meanders all around your head and it all comes magically together". "Peek-a-Boo" was their first real breakthrough in the United States. After the tour, the band decided to take a break, with Siouxsie and Budgie recording as the Creatures and releasing their most critically acclaimed album to date, "Boomerang", and Severin and McCarrick working on material together.

In 1991, Siouxsie and the Banshees returned with the single "Kiss Them for Me", mixing strings over a dance rhythm laced with exotica. The group collaborated with the then unknown Asian Tabla player Talvin Singh, who also sang during the bridge. The single received glowing reviews and later peaked in the "Billboard" Hot 100 at number 23, allowing them to reach a new audience. The album "Superstition" followed shortly afterwards, and the group toured the US as second headliners of the inaugural Lollapalooza tour. The following year, the Banshees were asked to compose "Face to Face" as a single for the film "Batman Returns", at director Tim Burton's request.

In 1993, the Banshees recorded new songs based on string arrangements, but quickly stopped the sessions to play festivals abroad. On their return home, they hired former Velvet Underground member John Cale to produce the rest of the record. At its release, 1995's "The Rapture" was described by "Melody Maker" as "a fascinating, transcontinental journey through danger and exotica". A few weeks after its release, Polydor dropped the band from its roster and Klein was replaced on the band's last tour in 1995 by ex-Psychedelic Furs guitarist Knox Chandler. In April 1996, the Banshees disbanded after 20 years of working together. Siouxsie and Budgie announced that they would carry on recording as the Creatures. In 1999, they released the album "Anima Animus" to critical acclaim.

In 2002, Universal Music kicked off the band's remastered back catalogue by releasing "The Best of Siouxsie and the Banshees". In April, Siouxsie, Severin, Budgie and Chandler reunited briefly for the Seven Year Itch tour, which spawned the "Seven Year Itch" live album and DVD in 2003. The day after their last concert in Tokyo, Japan, Siouxsie and Budgie stayed in town on their own and entered into a recording studio as the Creatures. Their fourth and final studio album, "Hái!", came out a few months later.

In 2004, "Downside Up", a box set that collected all of the Banshees' B-sides and "The Thorn" EP, was released. "The Times" wrote in its review: "for here is a group that never filled B-sides with inferior, throwaway tracks. Rather they saw them as an outlet for some of their most radical and challenging work".

In 2006, the band's first four records were remastered and compiled with previously unreleased bonus tracks. Several recordings made for the John Peel radio show from 1978 to 1986 were also compiled on "". AllMusic described the first session as "a fiery statement of intent" and qualified the other performances as "excellent". The second batch of remasters, concerning the 1982–1986 era, was issued in April 2009. It included four other reissues (including their highly regarded "A Kiss in the Dreamhouse" from 1982). The "At the BBC" box set, containing a DVD with all of the band's UK live television performances and three CDs with in-concert recordings, was also released in June of the same year.

In April 2014, their debut single "Hong Kong Garden" was reissued on double 7-inch vinyl. It was announced that this would be part of a three-year plan with Universal. In late October, their last four studio albums (1987's "Through the Looking Glass", 1988's "Peepshow", 1991's "Superstition" and 1995's "The Rapture") were reissued on CD in remastered versions with bonus tracks. Siouxsie and Severin curated a compilation CD called "It's a Wonderfull Life" for the monthly magazine "Mojo", issued in September with Siouxsie on the front cover. On this CD, the pair honoured several composers of film music and classical music that had inspired them.

In 2015, after releasing another compilation called "Spellbound: The Collection", which included singles, album tracks and B-sides, the band reissued 1979's "Join Hands" on vinyl for Record Store Day, with different cover artwork. A CD box set titled "Classic Album Selection Volume One" was released in January 2016, containing their first six albums newly remastered by Kevin Metcalfe. "Classic Album Selection Volume Two", including the other last six albums, followed in April. A vinyl picture disc edition of "The Scream" was released in November.

A vinyl reissue series on Polydor of all of the band's albums, remastered from the original ¼" tapes in 2018 by Miles Showell and cut at half speed at Abbey Road Studios, began in August with "Tinderbox", "Juju", "Through the Looking Glass" and "Join Hands". The second batch of vinyl reissues, released in September, included "Superstition", "A Kiss in the Dreamhouse" and "The Scream". The third batch, arriving in December, contained "The Rapture", "Peepshow", "Kaleidoscope" and "Hyæna". A blue vinyl edition of "The Scream", limited to 1,000 copies, was also released in November for the 40th anniversary of the album, on indie only record stores' websites.

Siouxsie and the Banshees have been described as developing "a form of post-punk discord full of daring rhythmic and sonic experimentation". "The Times" wrote that "The Banshees stand proudly [... as] one of the most audacious and uncompromising musical adventurers of the post-punk era". With some of their darkest material, the band also helped spawn the gothic scene. The band is also considered a new wave act.

They were also one of the first alternative bands; music historian Peter Buckley pointed out that they were at "the very front of the alternative-rock scene". In 1988, "Peek-a-Boo" was the very first track to top the US Modern Rock chart after "Billboard" launched this chart in the first week of September to list the most played songs on alternative and college radio stations. Simon Goddard wrote that the "Banshees - Mk II would become one of the biggest alternative pop groups of the 1980s". "Spin" described them as "alternative rockers" in 1991 when referring to their presence in the top 40 chart. Noting the band's participation in the first Lollapalooza festival, journalist Jim Gerr saw them as one of the "elements of the alternative rock community". "Mojo" retrospectively presented them as one of "alternative rock's iconic groups".

Siouxsie and the Banshees have had an impact on many later genres including post-punk, new wave, synth pop, gothic rock, alternative music, shoegazing and trip-hop, influencing a wide range of musicians including Joy Division, the Cure, the Smiths, Depeche Mode, PJ Harvey, Radiohead, Tricky and LCD Soundsystem.

Joy Division's Peter Hook, who saw the group in concert in Manchester in 1977, said: "Siouxsie and the Banshees were one of our big influences [...] The Banshees first LP was one of my favourite ever records, the way the guitarist and the drummer played was a really unusual way of playing and this album showcases a landmark performance". Joy Division producer Martin Hannett saw a difference between the Banshees' first main lineup and the other bands of 1977: "Any harmonies you got were stark, to say the least, except for the odd exception, like Siouxsie. They were interesting". The Cure's leader, Robert Smith, declared in 2003: "Siouxsie and the Banshees and Wire were the two bands I really admired. They meant something." He also pinpointed what the 1979 "Join Hands" tour brought him musically. "On stage that first night with the Banshees, I was blown away by how powerful I felt playing that kind of music. It was so different to what we were doing with the Cure. Before that, I'd wanted us to be like the Buzzcocks or Elvis Costello, the punk Beatles. Being a Banshee really changed my attitude to what I was doing".

The two songwriters of the Smiths cited the band; singer Morrissey said that "Siouxsie and the Banshees were excellent", and that "they were one of the great groups of the late 1970s, early 1980s". He also said in 1994, "If you study modern groups, those who gain press coverage and chart action, none of them are as good as Siouxsie and the Banshees at full pelt. That's not dusty nostalgia, that's fact". When asked "who do you regret not going to see live", guitarist Johnny Marr replied "Siouxsie and the Banshees mk 1. But mk 2 were even better". Marr mentioned his liking for John McGeoch and his contribution to the single "Spellbound". Marr qualified it as "clever" with a "really good picky thing going on which is very un-rock'n'roll". Smiths' historian Goddard wrote that Marr "praise[d] the McGeoch-era Banshees as a significant inspiration". U2 cited Siouxsie and the Banshees as a major influence and selected "Christine" for a "Mojo" compilation. The Edge was the presenter of an award given to Siouxsie at the "Mojo" ceremony in 2005. In December 1981, Dave Gahan of Depeche Mode named the Banshees as one of his three favourite bands along with Sparks and Roxy Music. Gahan later hailed the single "Candyman" at its release, saying, "This is a great Banshees record[...], I like their sound". Jim Reid of the Jesus and Mary Chain selected "Jigsaw Feeling" from "The Scream" as being among his favourite songs. Thurston Moore of Sonic Youth cited "Hong Kong Garden" in his top 25 all-time favourite songs, and Kevin Shields of My Bloody Valentine also mentioned them as being among his early influences. Dave Navarro of Jane's Addiction once noted a parallel between his band and the Banshees: "There are so many similar threads: melody, use of sound, attitude, sex-appeal. I always saw Jane's Addiction as the masculine Siouxsie and the Banshees". Primal Scream's Bobby Gillespie liked the group's ability to produce pop songs while transmitting something subversive. He said, "They were outsiders bringing outsider subjects to the mainstream. We’re not trying to rip off the Banshees, but that's kind of where we’re coming from".

The Banshees have been hailed by other acts. Thom Yorke related that seeing Siouxsie on stage in concert in 1985 inspired him to become a performer. Radiohead cited McGeoch-era Siouxsie records when mentioning the recording of the song "There There", and rehearsed Banshees' material prior to their 2008 tour. Jeff Buckley, who took inspiration from several female vocalists, covered "Killing Time" (from the "Boomerang" album) on various occasions. Buckley also owned all the Banshees' albums in his record collection. Suede singer Brett Anderson named "Juju" as one of his favourite records. Red Hot Chili Peppers performed "Christine" in concert, and their guitarist John Frusciante cited the Banshees in interviews. Garbage singer Shirley Manson stated, "I learned how to sing listening to "The Scream" and "Kaleidoscope". Today, I can see and hear the Banshees' influence all over the place". Siouxsie has also been praised by other female singers including PJ Harvey and Courtney Love. PJ Harvey has stated, "It's hard to beat Siouxsie Sioux, in terms of live performance. She is so exciting to watch, so full of energy and human raw quality", and selected Siouxsie's album "Anima Animus" in her top 10 albums of 1999. The band had a strong effect on two important trip hop acts. Tricky covered "Tattoo" to open his second album, "Nearly God"; the original 1983 proto-trip-hop version of that song aided Tricky in the creation of his style. Massive Attack, sampled "Metal Postcard" on the song "Superpredators", recorded prior to their "Mezzanine" album. Air's Jean-Benoît Dunckel cited the group as one of his three main influences. Billy Corgan of the Smashing Pumpkins cited the Banshees as an important influence on his music. Faith No More covered "Switch" in concert and cited "The Scream" as one of their influences.

The Banshees continue to influence younger musicians. Singer James Murphy was marked by certain Banshees albums during his childhood. His band LCD Soundsystem covered "Slowdive" as a B-side to the single "Disco Infiltrator". The Beta Band sampled "Painted Bird" on their track "Liquid Bird" from the "Heroes to Zeros" album. TV on the Radio said that they have always tried to make a song that begins like "Kiss Them for Me" where all of a sudden, there's an "element of surprise" with "a giant drum coming in". Santigold based one of her songs around the music of "Red Light". "'My Superman' is an interpolation of 'Red Light'". Indie folk group DeVotchKa covered the ballad "The Last Beat of My Heart" at the suggestion of Arcade Fire singer Win Butler; it was released on the "Curse Your Little Heart" EP. Gossip named the Banshees as one of their major influences during the promotion of their single "Heavy Cross". British indie band Bloc Party took inspiration from "Peek-a-Boo" and their singer Kele Okereke stated about that Banshees' single: "it sounded like nothing else on this planet. This is [...] a pop song that they put out in the middle of their career [...] to me it sounded like the most current but most futuristic bit of guitar-pop music I've heard". A Perfect Circle's Billy Howerdel said that the Banshees was "top three favorite bands for me". The Weeknd sampled different parts of "Happy House" for his song "House of Balloons", and also used the chorus of the initial version.






</doc>
<doc id="29513" url="https://en.wikipedia.org/wiki?curid=29513" title="Simula">
Simula

Simula is the name of two simulation programming languages, Simula I and Simula 67, developed in the 1960s at the Norwegian Computing Center in Oslo, by Ole-Johan Dahl and Kristen Nygaard. Syntactically, it is a fairly faithful superset of ALGOL 60, also influenced by the design of Simscript.

Simula 67 introduced objects, classes, inheritance and subclasses, virtual procedures, coroutines, and discrete event simulation, and features garbage collection. Also other forms of subtyping (besides inheriting subclasses) were introduced in Simula derivatives.

Simula is considered the first object-oriented programming language. As its name suggests, the first Simula version by 1962 was designed for doing simulations; Simula 67 though was designed to be a general-purpose programming language and provided the framework for many of the features of object-oriented languages today.

Simula has been used in a wide range of applications such as simulating very-large-scale integration (VLSI) designs, process modeling, communication protocols, algorithms, and other applications such as typesetting, computer graphics, and education. The influence of Simula is often understated, and Simula-type objects are reimplemented in C++, Object Pascal, Java, C#, and many other languages. Computer scientists such as Bjarne Stroustrup, creator of C++, and James Gosling, creator of Java, have acknowledged Simula as a major influence.

The following account is based on Jan Rune Holmevik's historical essay.

Kristen Nygaard started writing computer simulation programs in 1957. Nygaard saw a need for a better way to describe the heterogeneity and the operation of a system. To go further with his ideas on a formal computer language for describing a system, Nygaard realized that he needed someone with more computer programming skills than he had. Ole-Johan Dahl joined him on his work January 1962. The decision of linking the language up to ALGOL 60 was made shortly after. By May 1962, the main concepts for a simulation language were set. "SIMULA I" was born, a special purpose programming language for simulating discrete event systems.

Kristen Nygaard was invited to visit the Eckert–Mauchly Computer Corporation late May 1962 in connection with the marketing of their new UNIVAC 1107 computer. At that visit, Nygaard presented the ideas of Simula to Robert Bemer, the director of systems programming at Univac. Bemer was a great ALGOL fan and found the Simula project compelling. Bemer was also chairperson of a session at the second international conference on information processing hosted by International Federation for Information Processing (IFIP). He invited Nygaard, who presented the paper "SIMULA – An Extension of ALGOL to the Description of Discrete-Event Networks".

The Norwegian Computing Center got a UNIVAC 1107 in August 1963 at a considerable discount, on which Dahl implemented the SIMULA I under contract with UNIVAC. The implementation was based on the UNIVAC ALGOL 60 compiler. SIMULA I was fully operational on the UNIVAC 1107 by January 1965. In the following few years, Dahl and Nygaard spent a lot of time teaching Simula. Simula spread to several countries around the world and SIMULA I was later implemented on other computers including the Burroughs B5500 and the Russian Ural-16.

In 1966 C. A. R. Hoare introduced the concept of record class construct, which Dahl and Nygaard extended with the concept of prefixing and other features to meet their requirements for a generalized process concept. Dahl and Nygaard presented their paper on Class and Subclass declarations at the IFIP Working Conference on simulation languages in Oslo, May 1967. This paper became the first formal definition of Simula 67. In June 1967, a conference was held to standardize the language and initiate a number of implementations. Dahl proposed to unify the type and the class concept. This led to serious discussions, and the proposal was rejected by the board. Simula 67 was formally standardized on the first meeting of the Simula Standards Group (SSG) in February 1968.

Simula was influential in the development of Smalltalk and later object-oriented programming languages. It also helped inspire the actor model of concurrent computation although Simula only supports coroutines and not true concurrency.

In the late sixties and the early seventies, there were four main implementations of Simula:


These implementations were ported to a wide range of platforms. The TOPS-10 implemented the concept of public, protected, and private member variables and procedures, that later was integrated into Simula 87. Simula 87 is the latest standard and is ported to a wide range of platforms. There are mainly four implementations:


In November 2001, Dahl and Nygaard were awarded the IEEE John von Neumann Medal by the Institute of Electrical and Electronics Engineers "For the introduction of the concepts underlying object-oriented programming through the design and implementation of SIMULA 67". In April 2002, they received the 2001 A. M. Turing Award by the Association for Computing Machinery (ACM), with the citation: "For ideas fundamental to the emergence of object oriented programming, through their design of the programming languages Simula I and Simula 67." Unfortunately neither Dahl nor Nygaard could make it to the ACM Turing Award Lecture, scheduled to be delivered at the November 2002 OOPSLA conference in Seattle, as they died in June and August of that year, respectively.

Simula Research Laboratory is a research institute named after the Simula language, and Nygaard held a part-time position there from the opening in 2001. The new Computer Science building at the University of Oslo is named Ole Johan Dahl's House, in Dahl's honour, and the main auditorium is named Simula.

The empty computer file is the minimal program in Simula, measured by the size of the source code. It consists of one thing only; a dummy statement.

However, the minimal program is more conveniently represented as an empty block:

It begins executing and immediately terminates. The language lacks any return value from the program.

An example of a Hello world program in Simula:

Simula is case-insensitive.

A more realistic example with use of classes, subclasses and virtual procedures:

The above example has one super class (Glyph) with two subclasses (codice_1 and codice_2). There is one virtual procedure with two implementations. The execution starts by executing the main program. Simula lacks the concept of abstract classes, since classes with pure virtual procedures can be instantiated. This means that in the above example, all classes can be instantiated. Calling a pure virtual procedure will however produce a run-time error.

Simula supports call by name so the Jensen's Device can easily be implemented. However, the default transmission mode for simple parameter is call by value, contrary to ALGOL which used call by name. The source code for the Jensen's Device must therefore specify call by name for the parameters when compiled by a Simula compiler.

Another much simpler example is the summation function formula_1 which can be implemented as follows:

The above code uses call by name for the controlling variable (k) and the expression (u).
This allows the controlling variable to be used in the expression.

Note that the Simula standard allows for certain restrictions on the controlling variable
in a for loop. The above code therefore uses a while loop for maximum portability.

The following:

formula_2

can then be implemented as follows:

Simula includes a simulation package for doing discrete event simulations. This simulation package is based on Simula's object-oriented features and its coroutine concept.

Sam, Sally, and Andy are shopping for clothes. They must share one fitting room. Each one of them is browsing the store for about 12 minutes and then uses the fitting room exclusively for about three minutes, each following a normal distribution. A simulation of their fitting room experience is as follows:

The main block is prefixed with codice_3 for enabling simulation. The simulation package can be used on any block and simulations can even be nested when simulating someone doing simulations.

The fitting room object uses a queue (codice_4) for getting access to the fitting room. When someone requests the fitting room and it's in use they must wait in this queue (codice_5). When someone leaves the fitting room the first one (if any) is released from the queue (codice_6) and accordingly removed from the door queue (codice_7).

Person is a subclass of codice_8 and its activity is described using hold (time for browsing the store and time spent in the fitting room) and calls procedures in the fitting room object for requesting and leaving the fitting room.

The main program creates all the objects and activates all the person objects to put them into the event queue. The main program holds for 100 minutes of simulated time before the program terminates.




</doc>
<doc id="29515" url="https://en.wikipedia.org/wiki?curid=29515" title="SNOBOL">
SNOBOL

SNOBOL ("StriNg Oriented and symBOlic Language") is a series of programming languages developed between 1962 and 1967 at AT&T Bell Laboratories by David J. Farber, Ralph E. Griswold and Ivan P. Polonsky, culminating in SNOBOL4. It was one of a number of text-string-oriented languages developed during the 1950s and 1960s; others included COMIT and TRAC.

SNOBOL4 stands apart from most programming languages of its era by having patterns as a first-class data type ("i.e." a data type whose values can be manipulated in all ways permitted to any other data type in the programming language) and by providing operators for pattern concatenation and alternation. SNOBOL4 patterns are a type of object and admit various manipulations, much like later object-oriented languages such as JavaScript whose patterns are known as regular expressions. In addition SNOBOL4 strings generated during execution can be treated as programs and either interpreted or compiled and executed (as in the eval function of other languages).

SNOBOL4 was quite widely taught in larger US universities in the late 1960s and early 1970s and was widely used in the 1970s and 1980s as a text manipulation language in the humanities.

In the 1980s and 1990s its use faded as newer languages such as AWK and Perl made string manipulation by means of regular expressions fashionable. SNOBOL4 patterns subsume BNF grammars, which are equivalent to context-free grammars and more powerful than regular expressions. 
The "regular expressions" in current versions of AWK and Perl are in fact extensions of regular expressions in the traditional sense, but regular expressions, unlike SNOBOL4 patterns, are not recursive, which gives a distinct computational advantage to SNOBOL4 patterns. (Recursive expressions did appear in Perl 5.10, though, released in December 2007.)

One of the designers of SNOBOL, Ralph Griswold, designed successors to SNOBOL4 called SL5 and Icon, which combined the backtracking of SNOBOL4 pattern matching with more standard ALGOL-like structuring, as well as adding some features of their own.

The initial SNOBOL language was created as a tool to be used by its authors to work with the symbolic manipulation of polynomials. It was written in assembly language for the IBM 7090. It had a simple syntax, only one datatype, the string, no functions, and no declarations and very little error control. However, despite its simplicity and its "personal" nature its use began to spread to other groups. As a result, the authors decided to extend it and tidy it up. They rewrote it and added functions, both standard and user-defined, and released the result as SNOBOL3. SNOBOL2 did exist but it was a short-lived intermediate development version without user-defined functions and was never released. SNOBOL3 became quite popular and was rewritten for other computers than the IBM 7090 by other programmers. As a result, several incompatible dialects arose.

As SNOBOL3 became more popular the authors received more and more requests for extensions to the language. They also began to receive complaints about incompatibility and bugs in versions that they hadn't written. To address this and to take advantage of the new computers being introduced in the late 1960s, the decision was taken to develop SNOBOL4 with many extra datatypes and features but based on a virtual machine to allow improved portability across computers. The SNOBOL4 language translator was still written in assembly language. However the macro features of the assembler were used to define the virtual machine instructions of the SNOBOL Implementation Language, the SIL. This very much improved the portability of the language by making it relatively easy to port the virtual machine which hosted the translator by recreating its virtual instructions on any machine which included a macro assembler or indeed a high level language.

The machine-independent language SIL arose as a generalization of string manipulation macros by Douglas McIlroy, which were used extensively in the initial SNOBOL implementation. In 1969, McIlroy influenced the language again by insisting on addition of the table type to SNOBOL4.

SNOBOL4 supports a number of built-in data types, such as integers and limited precision real numbers, strings, patterns, arrays, and tables (associative arrays), and also allows the programmer to define additional data types and new functions. SNOBOL4's programmer-defined data type facility was advanced at the time—it is similar to the records of the earlier COBOL and the later Pascal programming languages.

All SNOBOL command lines are of the form

Each of the five elements is optional. In general, the "subject" is matched against the "pattern". If the "object" is present, any matched portion is replaced by the "object" via rules for replacement. The "transfer" can be an absolute branch or a conditional branch dependent upon the success or failure of the subject evaluation, the pattern evaluation, the pattern match, the object evaluation or the final assignment. It can also be a transfer to code created and compiled by the program itself during a run.

A SNOBOL pattern can be very simple or extremely complex. A simple pattern is just a text string (e.g. "ABCD"), but a complex pattern may be a large structure describing, for example, the complete grammar of a computer language. It is possible to implement a language interpreter in SNOBOL almost directly from a Backus–Naur form expression of it, with few changes. Creating a macro assembler and an interpreter for a completely theoretical piece of hardware could take as little as a few hundred lines, with a new instruction being added with a single line.

Complex SNOBOL patterns can do things that would be impractical or impossible using the more primitive regular expressions used in most other pattern-matching languages. Some of this power derives from the so-called "SPITBOL extensions" (which have since been incorporated in basically all modern implementations of the original SNOBOL 4 language too), although it is possible to achieve the same power without them. Part of this power comes from the side effects that it is possible to produce during the pattern matching operation, including saving numerous intermediate/tentative matching results and the ability to invoke user-written functions during the pattern match which can perform nearly any desired processing, and then influence the ongoing direction the interrupted pattern match takes, or even to indeed change the pattern itself during the matching operation. Patterns can be saved like any other first-class data item, and can be concatenated, used within other patterns, and used to create very complex and sophisticated pattern expressions. It is possible to write, for example, a SNOBOL4 pattern which matches "a complete name and international postal mailing address", which is well beyond anything that is practical to even attempt using regular expressions.

SNOBOL4 pattern-matching uses a backtracking algorithm similar to that used in the logic programming language Prolog, which provides pattern-like constructs via DCGs. This algorithm makes it easier to use SNOBOL as a logic programming language than is the case for most languages.

SNOBOL stores variables, strings and data structures in a single garbage-collected heap.

The "Hello, World!" program might be as follows...
END
A simple program to ask for a user's name and then use it in an output sentence...

END
To choose between three possible outputs...
MEH OUTPUT = "Hi, " Username :(END)
LOVE OUTPUT = "How nice to meet you, " Username :(END)
HATE OUTPUT = "Oh. It's you, " Username
END
To continue requesting input until no more is forthcoming...
AGAIN NameCount = NameCount + 1
GETINPUT OUTPUT = "Please give me name " NameCount + 1 
END

The classic implementation was on the PDP-10; it has been used to study compilers, formal grammars, and artificial intelligence, especially machine translation and machine comprehension of natural languages. The original implementation was on an IBM 7090 at Bell Labs, Holmdel, N.J. SNOBOL4 was specifically designed for portability; the first implementation was started on an IBM 7094 in 1966 but completed on an IBM 360 in 1967. It was rapidly ported to many other platforms.

It is normally implemented as an interpreter because of the difficulty in implementing some of its very high-level features, but there is a compiler, the SPITBOL compiler, which provides nearly all the facilities that the interpreter provides.

The Gnat Ada Compiler comes with a package (GNAT.Spitbol) which implements all of the Spitbol string manipulation semantics. This can be called from within an Ada program.

The file editor for the Michigan Terminal System (MTS) provided pattern matching based on SNOBOL4 patterns.

Several implementations are currently available. Macro SNOBOL4 in C written by Phil Budne is a free, open source implementation, capable of running on almost any platform. Catspaw, Inc provided a commercial implementation of the SNOBOL4 language for many different computer platforms, including DOS, Macintosh, Sun, RS/6000, and others, and these implementations are now available free from Catspaw. Minnesota SNOBOL4, by Viktors Berstis, the closest PC implementation to the original IBM mainframe version (even including Fortran-like FORMAT statement support) is also free.

Although SNOBOL itself has no structured programming features, a SNOBOL preprocessor called Snostorm was designed and implemented during the 1970s by Fred G. Swartz for use under the Michigan Terminal System (MTS) at the University of Michigan. Snostorm was used at the eight to fifteen sites that ran MTS. It was also available at University College London (UCL) between 1982 and 1984.

Snocone by Andrew Koenig adds block-structured constructs to the SNOBOL4 language. Snocone is a self-contained programming language, rather than a proper superset of SNOBOL4.

The SPITBOL implementation also introduced a number of features which, while not using traditional structured programming keywords, nevertheless can be used to provide many of the equivalent capabilities normally thought of as "structured programming", most notably nested if/then/else type constructs. These features have since been added to most recent SNOBOL4 implementations. After many years as a commercial product, in April 2009 SPITBOL was released as free software under the GNU General Public License.

According to Dave Farber, he, Griswold and Polonsky "finally arrived at the name Symbolic EXpression Interpreter SEXI."

Common backronyms of "SNOBOL" are 'String Oriented Symbolic Language' or (as a quasi-initialism) 'StriNg Oriented symBOlic Language'.





</doc>
<doc id="29518" url="https://en.wikipedia.org/wiki?curid=29518" title="Statistical physics">
Statistical physics

Statistical physics is a branch of physics that uses methods of probability theory and statistics, and particularly the mathematical tools for dealing with large populations and approximations, in solving physical problems. It can describe a wide variety of fields with an inherently stochastic nature. Its applications include many problems in the fields of physics, biology, chemistry, neuroscience, and even some social sciences, such as sociology and linguistics. Its main purpose is to clarify the properties of matter in aggregate, in terms of physical laws governing atomic motion.

In particular, statistical mechanics develops the phenomenological results of thermodynamics from a probabilistic examination of the underlying microscopic systems. Historically, one of the first topics in physics where statistical methods were applied was the field of mechanics, which is concerned with the motion of particles or objects when subjected to a force.

Statistical mechanics provides a framework for relating the microscopic properties of individual atoms and molecules to the macroscopic or bulk properties of materials that can be observed in everyday life, therefore explaining thermodynamics as a natural result of statistics, classical mechanics, and quantum mechanics at the microscopic level. Because of this history, statistical physics is often considered synonymous with statistical mechanics or statistical thermodynamics.

One of the most important equations in statistical mechanics (akin to formula_1 in Newtonian mechanics, or the Schrödinger equation in quantum mechanics) is the definition of the partition function formula_2, which is essentially a weighted sum of all possible states formula_3 available to a system.

where formula_5 is the Boltzmann constant, formula_6 is temperature and formula_7 is energy of state formula_3. Furthermore, the probability of a given state, formula_3, occurring is given by

Here we see that very-high-energy states have little probability of occurring, a result that is consistent with intuition.

A statistical approach can work well in classical systems when the number of degrees of freedom (and so the number of variables) is so large that the exact solution is not possible, or not really useful. Statistical mechanics can also describe work in non-linear dynamics, chaos theory, thermal physics, fluid dynamics (particularly at high Knudsen numbers), or plasma physics.

Although some problems in statistical physics can be solved analytically using approximations and expansions, most current research utilizes the large processing power of modern computers to simulate or approximate solutions. A common approach to statistical problems is to use a Monte Carlo simulation to yield insight into the properties of a complex system.

Quantum statistical mechanics is statistical mechanics applied to quantum mechanical systems. In quantum mechanics a statistical ensemble (probability distribution over possible quantum states) is described by a density operator "S", which is a non-negative, self-adjoint, trace-class operator of trace 1 on the Hilbert space "H" describing the quantum system. This can be shown under various mathematical formalisms for quantum mechanics. One such formalism is provided by quantum logic.

A significant contribution (at different times) in development of statistical physics was given by Satyendra Nath Bose, James Clerk Maxwell, Ludwig Boltzmann, J. Willard Gibbs, Marian Smoluchowski, Albert Einstein, Enrico Fermi, Richard Feynman, Lev Landau, Vladimir Fock, Werner Heisenberg, Nikolay Bogolyubov, Benjamin Widom, Lars Onsager, and others. Statistical physics is studied in the nuclear center at Los Alamos. Also, Pentagon has organized a large department for the study of turbulence at Princeton University. Work in this area is also being conducted by Saclay (Paris), Max Planck Institute, Netherlands Institute for Atomic and Molecular Physics and other research centers.

Statistical physics allowed us to explain and quantitatively describe superconductivity, superfluidity, turbulence, collective phenomena in solids and plasma, and the structural features of liquid. It underlies the modern astrophysics. It is statistical physics that helped us to create such intensively developing study of liquid crystals and to construct a theory of phase transition and critical phenomena. Many experimental studies of matter are entirely based on the statistical description of a system. These include the scattering of cold neutrons, X-ray, visible light, and more.
Statistical physics plays a major role in Physics of Solid State Physics, Materials Science, Nuclear Physics, Astrophysics, Chemistry, Biology and Medicine (e.g. study of the spread of infectious diseases), Information Theory and Technique but also in those areas of technology owing to their development in the evolution of Modern Physics. It still has important applications in theoretical sciences such as Sociology and Linguistics and is useful for researchers in higher education, corporate governance, and industry.

Thermal and Statistical Physics (lecture notes, Web draft 2001) by Mallett M., Blumler P.
by Harald J W Müller-Kirsten (University of Kaiserslautern, Germany)


</doc>
<doc id="29519" url="https://en.wikipedia.org/wiki?curid=29519" title="Side effect (computer science)">
Side effect (computer science)

In computer science, an operation, function or expression is said to have a side effect if it modifies some state variable value(s) outside its local environment, that is to say has an observable effect besides returning a value (the main effect) to the invoker of the operation. State data updated "outside" of the operation may be maintained "inside" a stateful object or a wider stateful system within which the operation is performed. 
Example side effects include modifying a non-local variable, modifying a static local variable, modifying a mutable argument passed by reference, performing I/O or calling other side-effect functions. In the presence of side effects, a program's behaviour may depend on history; that is, the order of evaluation matters. Understanding and debugging a function with side effects requires knowledge about the context and its possible histories.

The degree to which side effects are used depends on the programming paradigm. Imperative programming is commonly used to produce side effects, to update a system's state. By contrast, Declarative programming is commonly used to report on the state of system, without side effects. 

In functional programming, side effects are rarely used. The lack of side effects makes it easier to do formal verifications of a program. Functional languages such as Standard ML, Scheme and Scala do not restrict side effects, but it is customary for programmers to avoid them. The functional language Haskell expresses side effects such as I/O and other stateful computations using monadic actions.

Assembly language programmers must be aware of "hidden" side effects—instructions that modify parts of the processor state which are not mentioned in the instruction's mnemonic. A classic example of a hidden side effect is an arithmetic instruction that implicitly modifies condition codes (a hidden side effect) while it explicitly modifies a register (the overt effect). One potential drawback of an instruction set with hidden side effects is that, if many instructions have side effects on a single piece of state, like condition codes, then the logic required to update that state sequentially may become a performance bottleneck. The problem is particularly acute on some processors designed with pipelining (since 1990) or with out-of-order execution. Such a processor may require additional control circuitry to detect hidden side effects and stall the pipeline if the next instruction depends on the results of those effects.

Absence of side effects is a necessary, but not sufficient, condition for referential transparency. Referential transparency means that an expression (such as a function call) can be replaced with its value. This requires that the expression is pure, that is to say the expression must be deterministic (always give the same value for the same input) and side-effect free.

Side effects caused by the time taken for an operation to execute are usually ignored when discussing side effects and referential transparency. There are some cases, such as with hardware timing or testing, where operations are inserted specifically for their temporal side effects e.g. codice_1 or codice_2. These instructions do not change state other than taking an amount of time to complete.

A function codice_3 with side effects is said to be idempotent under sequential composition codice_4 if, when called twice with the same list of arguments, the second call has no side effects (assuming no other procedures were called between the end of the first call and the start of the second call).

For instance, consider the following Python code:
x = 0

def setx(n):

setx(5)
setx(5)
Here, codice_5 is idempotent because the second call to codice_5 (with the same argument) does not change the visible program state: codice_7 was already set to 5 in the first call, and is again set to 5 in the second call, thus keeping the same value. Note that this is distinct from idempotence under function composition codice_8. For example, the absolute value is idempotent under function composition:
def abs(n):

abs(-5) == abs(abs(-5)) == abs(5) == 5
One common demonstration of side effect behavior is that of the assignment operator in C++. For example, assignment returns the right operand and has the side effect of assigning that value to a variable. This allows for syntactically clean multiple assignment:
int i, j;
i = j = 3;
Because the operator right associates, this equates to
int i, j;
i = (j = 3); // j = 3 returns 3, which then gets assigned to i
Where the result of assigning 3 into codice_9 then gets assigned into codice_10. This presents a potential hangup for novice programmers who may confuse
while (b == 10) {} // tests if b evaluates to 10
with
while (b = 10) {} // = returns 10 which automatically casts to true so the test is always true


</doc>
<doc id="29523" url="https://en.wikipedia.org/wiki?curid=29523" title="List of science fiction editors">
List of science fiction editors

This is a list of science fiction editors, editors working for book and magazine publishing companies who have edited science fiction. Many have also edited works of fantasy and other related genres, all of which have been sometimes grouped under the name speculative fiction. 

Editors on this list should fulfill the conditions for in science fiction or related genres. Evidence for notability includes an existing wiki-biography, or evidence that one could be written. Borderline cases should be discussed on the article's talk page.




























</doc>
<doc id="29525" url="https://en.wikipedia.org/wiki?curid=29525" title="Square-free integer">
Square-free integer

In mathematics, a square-free integer (or squarefree integer) is an integer which is divisible by no perfect square other than 1. That is, its prime factorization has exactly one factor for each prime that appears in it. For example, is square-free, but is not, because 18 is divisible by . The smallest positive square-free numbers are

Every positive integer can be factored in a unique way as
formula_2where the formula_3 different from one are square-free integers that are pairwise coprime.

This is called the "square-free factorization" of .

Let
be the prime factorization of , where the are distinct prime numbers. Then the factors of the square-free factorization are defined as 

An integer is square-free if and only if formula_6 for all . An integer greater than one is the th power of another integer if and only if is a divisor of all such that formula_7

The use of the square-free factorization of integers is limited by the fact that its computation is as difficult as the computation of the prime factorization. More precisely every known algorithm for computing a square-free factorization computes also the prime factorization. This is a notable difference with the case of polynomials for which the same definitions can be given, but, in this case, the square-free factorization is not only easier to compute than the complete factorization, but it is the first step of all standard factorization algorithms.

The radical of an integer is its largest square-free factor, that is formula_8 with notation of the preceding section. An integer is square-free if and only if it is equal to its radical.

Every positive integer can be represented in a unique way as the product of a powerful number (that is an integer such that is divisible by the square of every prime factor) and a square-free integer, which are coprime. In this factorization, the square-free factor is formula_9 and the powerful number is formula_10

The "square-free part" of is formula_9 which is the largest square-free divisor of that is coprime with . The square-free part of an integer may be smaller than the largest square-free divisor, which is formula_12

Any arbitrary positive integer can be represented in a unique way as the product of a square and a square-free integer:
In this factorization, is the largest divisor of such that is a divisor of .

In summary, there are three square-free factors that are naturally associated to every integer: the square-free part, the above factor , and the largest square-free factor. Each is a factor of the next one. All are easily deduced from the prime factorization or the square-free factorization: if
are the prime factorization and the square-free factorization of , where formula_15 are distinct prime numbers, then the square-free part is
The square-free factor such the quotient is a square is 
and the largest square-free factor is 

For example, if formula_19 one has formula_20 The square-free part is , the square-free factor such that the quotient is a square is , and the largest square-free factor is .

No algorithm is known for computing any of these square-free factors which is faster than computing the complete prime factorization. In particular, there is no known polynomial-time algorithm for computing the square-free part of an integer, nor even for determining whether an integer is square-free. In contrast, polynomial-time algorithms are known for primality testing. This is a major difference between the arithmetic of the integers, and the arithmetic of the univariate polynomials, as polynomial-time algorithms are known for square-free factorization of polynomials (in short, the largest square-free factor of a polynomial is its quotient by the greatest common divisor of the polynomial and its formal derivative).

A positive integer "n" is square-free if and only if in the prime factorization of "n", no prime factor occurs with an exponent larger than one. Another way of stating the same is that for every prime factor "p" of "n", the prime "p" does not evenly divide "n" / "p". Also "n" is square-free if and only if in every factorization "n" = "ab", the factors "a" and "b" are coprime. An immediate result of this definition is that all prime numbers are square-free.

A positive integer "n" is square-free if and only if all abelian groups of order "n" are isomorphic, which is the case if and only if any such group is cyclic. This follows from the classification of finitely generated abelian groups.

A integer "n" is square-free if and only if the factor ring Z / "nZ (see modular arithmetic) is a product of fields. This follows from the Chinese remainder theorem and the fact that a ring of the form Z / "kZ is a field if and only if "k" is a prime.

For every positive integer "n", the set of all positive divisors of "n" becomes a partially ordered set if we use divisibility as the order relation. This partially ordered set is always a distributive lattice. It is a Boolean algebra if and only if "n" is square-free.

A positive integer "n" is square-free if and only if "μ"("n") ≠ 0, where "μ" denotes the Möbius function.

The absolute value of the Möbius function is the indicator function for the square-free integers – that is, is equal to 1 if is square-free, and 0 if it is not. The Dirichlet series of this indicator function is
where is the Riemann zeta function. This follows from the Euler product
where the products are taken over the prime numbers.

Let "Q"("x") denote the number of square-free integers between 1 and "x" ( shifting index by 1). For large "n", 3/4 of the positive integers less than "n" are not divisible by 4, 8/9 of these numbers are not divisible by 9, and so on. Because these ratios satisfy the multiplicative property (this follows from Chinese remainder theorem), we obtain the approximation:

This argument can be made rigorous for getting the estimate (using big O notation)

"Sketch of a proof:" the above characterization gives
observing that the last summand is zero for formula_26, it results that

By exploiting the largest known zero-free region of the Riemann zeta function Arnold Walfisz improved the approximation to
for some positive constant .

Under the Riemann hypothesis, the error term can be reduced to

Recently (2015) the error term has been further reduced to

The asymptotic/natural density of square-free numbers is therefore

Therefore over 3/5 of the integers are square-free.

Likewise, if "Q"("x","n") denotes the number of "n"-free integers (e.g. 3-free integers being cube-free integers) between 1 and "x", one can show

Since a multiple of 4 must have a square factor 4=2, it cannot occur that four consecutive integers are all square-free. On the other hand, there exist infinitely many integers "n" for which 4"n" +1, 4"n" +2, 4"n" +3 are all square-free. Otherwise, observing that 4"n" and at least one of 4"n" +1, 4"n" +2, 4"n" +3 among four could be non-square-free for sufficiently large "n", half of all positive integers minus finitely many must be non-square-free and therefore
contrary to the above asymptotic estimate for formula_34.

There exist sequences of consecutive non-square-free integers of arbitrary length. Indeed, if "n" satisfies a simultaneous congruence
for distinct primes formula_36, then each formula_37 is divisible by "p". On the other hand, the above-mentioned estimate formula_38 implies that, for some constant "c", there always exists a square-free integer between "x" and formula_39 for positive "x". Moreover, an elementary argument allows us to replace formula_39 by formula_41 The ABC conjecture would allow formula_42.

The table shows how formula_45 and formula_46 compare at powers of 10.

formula_47 , also denoted as formula_48.
formula_49 changes its sign infinitely often as formula_50 tends to infinity.

The absolute value of formula_49 is astonishingly small compared with formula_50.

If we represent a square-free number as the infinite product

then we may take those formula_54 and use them as bits in a binary number with the encoding

The square-free number 42 has factorization 2 × 3 × 7, or as an infinite product 2 · 3  · 5 · 7 · 11 · 13 ··· Thus the number 42 may be encoded as the binary sequence ...001011 or 11 decimal. (The binary digits are reversed from the ordering in the infinite product.)

Since the prime factorization of every number is unique, so also is every binary encoding of the square-free integers.

The converse is also true. Since every positive integer has a unique binary representation it is possible to reverse this encoding so that they may be decoded into a unique square-free integer.

Again, for example, if we begin with the number 42, this time as simply a positive integer, we have its binary representation 101010. This decodes to 2 · 3 · 5 · 7 · 11 · 13 = 3 × 7 × 13 = 273.

Thus binary encoding of squarefree numbers describes a bijection between the nonnegative integers and the set of positive squarefree integers.

The central binomial coefficient

is never squarefree for "n" > 4. This was proven in 1985 for all sufficiently large integers by András Sárközy, and for all integers > 4 in 1996 by Olivier Ramaré and Andrew Granville.

The multiplicative function formula_57 is defined
to map positive integers "n" to "t"-free numbers by reducing the
exponents in the prime power representation modulo "t":
The value set of formula_59, in particular, are the
square-free integers. Their Dirichlet generating functions are

OEIS representatives are ("t"=2), ("t"=3) and ("t"=4).



</doc>
<doc id="29530" url="https://en.wikipedia.org/wiki?curid=29530" title="Sentinel (comics)">
Sentinel (comics)

The Sentinels are a fictional variety of mutant-hunting robots appearing in American comic books published by Marvel Comics. They are typically depicted as antagonists to the X-Men.

The Sentinels played a large role in the 1990s "X-Men" animated series and have been featured in several X-Men video games. The Sentinels are featured prominently in the 2014 film "" while simulated versions made brief appearances in the 2006 film "" and the 2016 film "". In 2009, The Sentinels was ranked as IGN's 38th Greatest Comic Book Villain of All Time.

Created by Stan Lee and Jack Kirby, they first appeared in "The X-Men" #14 (November 1965).

Sentinels are programmed to locate mutants and capture or kill them. Though several types of Sentinels have been introduced, the typical Sentinel is three stories tall, is capable of flight, projects energy blasts, and can detect mutants. Pursuing genocide as the means of dealing with a threat has made the Sentinels an analogy for racial hatred in Marvel stories.

Sentinels are designed to hunt mutants. While many are capable of tactical thought, only a handful are self-aware.

Sentinels are technologically advanced, and have exhibited a wide variety of abilities. They are armed (primarily with energy weapons and restraining devices), capable of flight, and can detect mutants at long range. They possess vast physical strength, and their bodies are highly resistant to damage. Some are able to alter their physical forms or re-assemble and reactivate themselves after they have been destroyed.

Some Sentinel variants have the ability to learn from their experiences, developing their defenses during an engagement. Several groups of Sentinels have been created and/or led by a single massive Sentinel called Master Mold. Some Sentinels are also equipped with an inconspicuous logic loop in case they should go rogue to convince them that they are mutants as demonstrated in the Tri-Sentinel.

There are different types of Sentinels that appear in the comics:



The following are alternative versions of the Sentinels, which appear outside of regular Marvel canon.

In the "Age of Apocalypse" timeline, Bolivar Trask created the Sentinels with his wife Moira. These Sentinels are equipped with several body-mounted gun turrets, and their primary directive is to protect humans rather than to hunt mutants. They are capable of cooperating with mutants in order to further this mission. Later, the Sentinels are adapted by Weapon Omega to serve a reverse purpose, and now aid in the hunting of the human race.

In the "Days of Future Past" timeline, which takes place in an alternate future, the "Omega Sentinels" have advanced technologically and become the "de facto" rulers of the United States. The most powerful among them is Nimrod.

In the joke comic "Fred Hembeck Destroys the Marvel Universe", the X-Men are killed by silent, black, man-sized "Ninja Sentinels".

In the "Here Comes Tomorrow" future timeline, a Sentinel named Rover is Tom Skylark's companion and protector. After more than 150 years of being active, Rover has become self-aware and, possibly, capable of emotion.

In the "House of M" storyline, Magneto is victorious in a mutant/human war. The Sentinels are adapted by Sebastian Shaw, now the director of S.H.I.E.L.D., to serve a reverse purpose, and now aid in the hunting of sapien rebels.

In the MC2 timeline, Wild Thing encounters a Prime Sentinel that has accidentally been activated by a faulty microwave.

In the alternate reality of "X-Men: Ronin", the story is played out in Japan. A police unit called "Sentinel Force" designs, builds and pilots the robots. These are aesthetically similar to regular Sentinels, but each is subtly different from the others.

In the comic crossover "X-Men/Star Trek: Second Contact", the X-Men work with the crew of the "Enterprise"-E to battle Kang the Conqueror. An away team composed of Captain Picard, Deanna Troi, Nightcrawler and Colossus encounter an approximation of the "Days of Future Past" timeline, in which the Sentinels have merged with the Borg.

The Ultimate Marvel version of Sentinels were created by Bolivar Trask, were already in action in the "Ultimate X-Men" story arc, hunting down and killing mutants on the streets, in a program apparently openly and publicly acknowledged by the U.S. government. Later on, there were also the New Sentinels that were sixty of S.H.I.E.L.D.'s top agents in Sentinel battle armor and they were described to have enough hardware to take on a fleet of the old Sentinel models. A new breed of Sentinel robots, created by Trask under the Fenris twins' orders, was later created. After the events of the Ultimatum Wave, Nimrod Sentinels was deployed to hunt, capture or kill mutants that refused to turn themselves in. William Stryker, Jr., using Sentinel tech, later displayed an ability to summon a fleet of Sentinels after being attacked by the Shroud.




Sentinels have appeared as major antagonists in almost every video game to feature the X-Men:


Several different toys of Sentinels have been made since their introduction. One is the X-Men Classics 10" Sentinel by Toybiz. A "Build-A-Figure" version of the character (based on the "Here Comes Tomorrow" storyline) was made in wave ten of the "Marvel Legends" line. In 2010, Hasbro released a large Sentinel (available in two color schemes) as part of the Marvel Universe line. Along with a large, unposeable statue, two Minimates figures have been made of the Sentinels. The first, a classic version, came with Rachel Summers in either her Phoenix or Marvel Girl guises. The second, based on "", comes with a red-haired "First Appearance" figure of Ryu.
In 2014, The Lego group released a set in the Marvel Super Heroes line titled "X-Men vs. the Sentinel", featuring the sentinel as a buildable figure, also including the Blackbird, Magneto, Wolverine, Storm, and Cyclops.
In July 2020, Hasbro announced a twenty-six inch Sentinel as part of their HasLab crowdfunding releases.




</doc>
<doc id="29532" url="https://en.wikipedia.org/wiki?curid=29532" title="Sebastian Shaw">
Sebastian Shaw

Sebastian Shaw is the name of:



</doc>
<doc id="29533" url="https://en.wikipedia.org/wiki?curid=29533" title="Savage Land">
Savage Land

The Savage Land is a hidden prehistoric land appearing in American comic books published by Marvel Comics. It is a tropical preserve hidden in Antarctica. It has appeared in many story arcs in "Uncanny X-Men" as well as related books.

The Savage Land first appeared as The Land Where Time Stands Still in "Marvel Mystery Comics" #22 (August 1941), in the tale "Khor, the Black Sorcerer" by Joe Simon, Jack Kirby, and Syd Shores.
It gained its familiar form and moniker in "X-Men" #10 (March 1965) courtesy of Stan Lee and Jack Kirby.

The Savage Land was created by the alien Nuwali at the behest of the other-dimensional, nigh-omnipotent aliens known as the Beyonders who sought to observe the process of evolution under relatively controlled conditions and had the Nuwali set up a number of game preserves on several planets. One of these planets was Earth during the Triassic period where the Nuwali chose a valley in Antarctica surrounded by active volcanoes, where they installed a number of advanced technological devices in order to maintain a tropical climate. The aliens then stocked the area with all manner of Earth life over the following several millennia. They also brought over the Man-Apes, earlier hominid ancestors of "Homo sapiens".

The Beyonders eventually grew bored with the experiment, and the Nuwali stopped maintaining the Savage Land during the Late Pleistocene (the Ice Age era). However, the self-maintaining technology that allowed the pocket of tropical climate was left running, and many species which became extinct in other areas of the Earth continued to thrive within the Savage Land.

Later on, a group of human survivors from Atlantis sailed to Antarctica before the "Great Cataclysm" which sank Atlantis into the ocean. There, they discovered a cavern where they found an immense climate-controlling device and harnessed the technology used to keep the Savage Land's volcanoes working. They named their location "Pangea", which is Atlantean for "paradise".

They mastered genetic engineering, which had been used on the Man-Apes when the Nuwali were still maintaining the Savage Land area. They used their genetic engineering techniques to transform other Savage Land inhabitants like the Golden People, the Lizard Men, the Reptile Men, the Tubantis, and others. The Atlanteans then forced them to work for them until these animal people revolted. After a time of war, the animal people demanded civil rights and the Atlanteans used technology to expand the Savage Land's surface area for the animal people to live in. When the Great Cataclysm struck, the Atlantean empire fell and thanks to the machines, the Savage Land locations were spared from sinking into the sea.

In more recent years, the Savage Land was rediscovered by Lord Robert Plunder, who took back a sample of the metal known as "anti-metal" or "Antarctic vibranium" with him. This mysterious metal had the ability to produce vibrations which would liquefy all other metals. Fleeing from those who sought to steal this discovery, Plunder took his eldest son Kevin with him for a second trip into the Savage Land. Unfortunately, the elder Plunder was killed by a local tribe of Man-Apes.

Kevin survived, thanks to the timely intervention of the orphaned sabretooth tiger later known as Zabu. He grew to adulthood in the Savage Land, becoming the adventurer known as Ka-Zar. Ka-Zar had many team-ups with the X-Men, who first revealed the Savage Land's existence, Spider-Man, and many other superheroes who had visited the Savage Land. He later met and married Shanna the She-Devil.

The Savage Land's existence is common knowledge throughout the world. At one time, there were press junkets, sponsored by the oil company Roxxon. "Daily Bugle" photographer Peter Parker was sent and helped uncover Roxxon's unethical and dangerous manipulation of the local resources.

At one point, Spider-Man teamed up with Ka-Zar to save Gwen Stacy from Kraven the Hunter and Gog at the time when her class and J. Jonah Jameson were visiting the Savage Land.

Many villains have threatened the Savage Land, including Sauron, Garokk, Magneto, and Thanos.

The Savage Land was decimated by an evil alien named Terminus (or one of his pawns) when he destroyed the machines that maintained the tropical climate. Many of the Savage Land's native people were saved from the ensuing destruction by M'rin: The Warlord of The Skies who took them into her own native dimension to safety. Ka-Zar, Shanna, and Zabu wandered until the High Evolutionary (with help from the X-Men, M'rin, and Garokk) restored the region and its creatures, allowing them to return to the Savage Land with their newborn son. The other natives who had taken refuge in M'rin's dimension returned as well.

Sometime after that, Spider-Man had Devil Dinosaur and Moon-Boy emigrated to the Savage Land after rescuing them from Ringmaster.

Evidence in the pages of the "New Avengers" suggests that S.H.I.E.L.D. is operating in the Savage Land, mining vibranium while using the indigenous population as slave labor, but these operations have been classified, and the operation was apparently decimated by a missile strike from the Helicarrier during an attack by the New Avengers. The team only survived thanks to Iron Man's force field.

The Savage Land is featured in the limited series "Claws", serving as a place of revenge for Wolverine and Black Cat on Arcade and White Rabbit. After defeating the two villains, the heroes left them stranded.

In "", Cyclops and Emma Frost were vacationing there until Archangel contacted them about San Francisco looking like the 1960s.

Alyosha Kravinoff fled to the Savage Land after Punisher sabotaged his zoo.

During the "Secret Invasion" storyline, Ka-Zar and Shanna discover Skrulls mining for vibranium. The New Avengers and the Mighty Avengers head toward the Savage Land where a downed Skrull ship was sighted. Luke Cage opens the downed Skrull ship and a large group of Marvel superheroes with older appearances and costumes come out, speaking as if they believe themselves to be authentic. They soon break out into a fight where the Spider-Man from the ship is killed by a Tyrannosaurus and regresses to a Skrull. The Hawkeye from the ship is killed by Ronin and regresses to a Skrull. This causes the superheroes from the ship to scatter into the jungle. The New Avengers' Spider-Man is knocked away by a Tyrannosaurus and ends up confronting Ka-Zar, Shanna, Zabu, and Sauron as well as some of the other locals (ranging from the Man-Apes, the Sun People, the Swamp Men, the Tree People, and the Zebra People). At the point where Spider-Man accuses Ka-Zar and Shanna for being Skrulls, the Captain America from the ship attacks who thinks the same thing for Spider-Man. When the Captain America is hit by a dart coated in some type of poison by one of the Swamp Men, it regresses to a Skrull named Pit'o Nilli and is then killed by Shanna. The ship's Beast is trapped underground with Wonder Man. The two try to escape together, but Beast betrays Wonder Man as the two are about to return to the surface. During this, Iron Man uses an abandoned scientific facility nearby to try and recreate his original armor. When it came to the confrontation with both Avengers teams, the Savage Land natives, and the heroes from the ship, Mister Fantastic and Abigail Brand used a laser to identify the heroes from the ship as Skrulls. Ka-Zar joined the Avengers teams into fighting the Skrulls in New York while Shanna and the other Savage Land natives hunted down the remaining Skrulls hiding out in the Savage Land.

After the events of "Second Coming" during the "Heroic Age" storyline, Cyclops takes some time off to go hunting in the Savage Land during which he encounters Steve Rogers. Steve Rogers suggests to Cyclops that he brings the X-Men out of the shadows and into the light as heroes. Steve Rogers also arranges to have the president award Scott the Presidential Medal of Freedom which sways the people of San Francisco to welcome the X-Men back.

Around the same time following their defeat after the hunt for "spiders" in the "Grim Hunt" storyline, the Kravinoff Family are also currently residing in the Savage Land.

It is later revealed that Miek and the other Imperials and Natives from Sakaar that came with Hulk back in "World War Hulk" had settled in the Savage Land constructing a village called New Imperia.

During the "Avengers vs. X-Men" storyline, Captain America ends up fighting Gambit in the Savage Land.

As part of the "Marvel NOW!" event, some of The Garden's evolution seeds had fallen into the Savage Land. While working to get it under control, the Avengers find that A.I.M. is also there where they test the extracted formula from one of the pods and tests it on their intern Dr. Jema. The formula puts a strain on Dr. Jema just as the Avengers arrive.

As part of the "All-New, All-Different Marvel", Magneto led a new team of X-Men to protect mutant-kind at all costs with their base in the Savage Land.

During the "Empyre" storyline, the Cotati have invaded the Savage Land.

The Marvel Universe's version of the United Nations considers the Savage Land an international wildlife preserve and forbids any commercial exploitation of its resources.

There are some famous locations in the Savage Land:


There are many types of races in the Savage Land and Pangea. The Nuwali transported primitive man now known as the Man-Apes, which unlike the rest of the world thrived until the 21st century. The next arrivals were the Ancient Atlanteans who added the region as part of their empire. They used the Nuwali technology to mutate the Man-Apes into various Beast-Men to perform certain tasks. These slaves rebelled after the great Cataclysm and made Pangea their home. Many Atlanteans remained and their decedents became the various human tribes, with some clinging to the old ways and technology but most forget and resort to more primitive hunter-gatherer societies. Among the Savage Land races are:


In the "Age of Apocalypse" reality, the Savage Land houses Avalon, a secret haven for humans and mutants. A method to reach it exists, but it will only cost the refugee everything they own and even then, there is no guarantee of arriving alive. It is led by Destiny, a pacifist Juggernaut and Douglas Ramsey, the latter of whom provides a field that allows everybody to understand each other despite speaking different languages. Avalon was eventually found by Apocalypse forces and destroyed by the Shadow King who mind-controlled its inhabitants into killing each other. He was defeated, but casualties were high.

During the "Age of Ultron" storyline, the superhero resistance against Ultron had relocated to the Savage Land to come up with a plan to defeat Ultron.

In "Marvel Zombies Return", the Savage Land, like everywhere else on Earth, has been eaten by the superhuman zombies, with the surviving zombies musing that the Savage Land was their 'number one' meal in the aftermath, as it contained such an abundance of food that they were actually full for a full hour after eating there, as opposed to the usual ravenous hunger they feel. It is also the location of the final battle between the zombies and 'New Avengers'- three zombies who have beaten their hunger and the cyborg James Rhodes- at the storyline's conclusion, with Rhodes using one of his fingers to lure the zombies into an ambush.

In the "Earth X" universe, the Savage Land is where Magneto built his sanctuary called Sentinel City.

In the "House of M" reality created by an insane Scarlet Witch, the Savage Land was known as "Pangea." It is also known that Kevin Plunder has been granted political asylum in the United States for his human rights activism in this prehistoric land.

In the alternate future depicted in Marvel 2099, an alien attack floods much of the earth rendering the Savage Land the only habitable space. Thousands of refugees (including Miguel O'Hara and most of X-Nation and X-Men) make new homes here. It is not without its own dangers.

In the "Transformers" Marvel comics continuity, shortly after the "Ark"' spacecraft crashed on Earth 4 million years before the present day, the computer aboard the Ark detected Shockwave landing on the prehistoric Savage Land. The "Ark" used the last of its capabilities to revive the five Autobot warriors by scanning the Savage Land's dominant lifeform: dinosaurs, and rebuild them into the Dinobots. The Dinobots fought Shockwave, a battle that ended in permanent stalemate when Snarl brought down the mountain that Shockwave stood upon, knocking all of them into a tar pit. They remained deactivated until the year 1984. Since the Dinobots' alt-mode forms resemble creatures that were long-extinct by 4 million years ago, the Savage Land provided author Bob Budiansky a way to explain this within the canon timeline.

During the "Spider-Geddon" storyline, an alternate unidentified Earth has a version of Spider-Man that lives in the Savage Land and was raised by a tribe of giant spiders following an airplane crash. It was mentioned by Ka-Zar the Hunter to Wilson Fisk that his father killed the last of the Man-Apes.

In the Ultimate Marvel universe, the Savage Land is a large island somewhere in the southern hemisphere. It was originally said to have been created by Magneto, using theories and methods developed by Professor X, as the site for genetic experiments. Magneto's goal there was to create a new human race who would be less trouble to rule than the current one, that he decided to restart evolution from scratch, and control the process to his own specifications. As a result of this, at its current level of advancement, it has dinosaurs and that Magneto has shown no further interest in advancing the evolution of the Savage Land. It has remained in its dinosaur state since the departure of Professor X. This story is later revealed as false (see below).

Magneto's original base was on the Savage Land. When it was destroyed in the first arc of "Ultimate X-Men", the computer controlling the base gained self-awareness, and hijacked the genetic experiment project to create an army of nanotech-enhanced, zombie-like thralls. It planned to take over the world, but was stopped by Wolverine, Cyclops, and Kitty Pryde.

The Savage Land is now the home of Longshot, who managed to get there in a small boat which launched from Genosha. Longshot recently aided Magneto in breaking out of prison, and the two may be planning something.

In "Ultimates 3", it is revealed that the dinosaurs were conjured by Scarlet Witch as a result of her reality warping abilities and not by Magneto's creation. The aboriginal inhabitants were wiped out and only a small tribe of survivors including Ka-Zar and Shanna remain.

The inhabitants help the Ultimates remove the last of Magneto's forces as of "Ultimatum".

The Savage Land appears in a "What If" story where the Savage Land was terraforming and has taken over New York. Both Ka-Zar and Parnival sacrifice themselves to return New York to normal, with Shanna the only survivor of his "family."

Additionally, in the "What If" issues involving alternative outcomes to the Age of Ultron, a group composed of Wolverine, the Hulk, Peter Parker and a Ghost Rider venture to the Savage Land in order to prevent a Master Mold under the control of a future version of Ezekiel Stane from unleashing a wave of Stark armors on the world.







</doc>
<doc id="29536" url="https://en.wikipedia.org/wiki?curid=29536" title="Stephen Schneider">
Stephen Schneider

Stephen Henry Schneider (February 11, 1945 – July 19, 2010) was Professor of Environmental Biology and Global Change at Stanford University, a Co-Director at the Center for Environment Science and Policy of the Freeman Spogli Institute for International Studies and a Senior Fellow in the Stanford Woods Institute for the Environment. Schneider served as a consultant to federal agencies and White House staff in the Richard Nixon, Jimmy Carter, Ronald Reagan, George H. W. Bush, Bill Clinton, George W. Bush and Barack Obama administrations.

Schneider's research included modeling of the atmosphere, climate change, and the effect of global climate change on biological systems. Schneider was the founder and editor of the journal "Climatic Change" and authored or co-authored over 450 scientific papers and other publications. He was a Coordinating Lead Author in Working Group II Intergovernmental Panel on Climate Change (IPCC) Third Assessment Report and was engaged as a co-anchor of the Key Vulnerabilities Cross-Cutting Theme for the Fourth Assessment Report (AR4) at the time of his death. During the 1980s, Schneider emerged as a leading public advocate of sharp reductions of greenhouse gas emissions to combat global warming. In 2006 Professor Schneider was an Adelaide Thinker in Residence advising the South Australian Government of Premier Mike Rann on climate change and renewable energy policies. In ten years South Australia went from zero to 31% of its electricity generation coming from renewables.

An annual award for outstanding climate science communication was created in Schneider's honor after his death, by the Commonwealth Club of California. The Stephen Schneider Memorial Lecture of the American Geophysical Union honors Schneider's life and work.

Schneider grew up on Long Island, New York. He studied engineering at Columbia University, receiving his bachelor's degree in mechanical engineering in 1966. In 1971, he earned a Ph.D. in mechanical engineering and plasma physics. Schneider studied the role of greenhouse gases and suspended particulate material on climate as a postdoctoral fellow at NASA's Goddard Institute for Space Studies. Schneider was awarded the Marshall Scholarship.

In 1971, Schneider was second author on a "Science" paper with S. Ichtiaque Rasool titled "Atmospheric Carbon Dioxide and Aerosols: Effects of Large Increases on Global Climate" ("Science" 173, 138–141). This paper used a one-dimensional radiative transfer model to examine the competing effects of cooling from aerosols and warming from CO. The paper concluded that:

Carbon dioxide was predicted to have only a minor role. However, the model was very simple and the calculation of the CO effect was lower than other estimates by a factor of about three, as noted in a footnote to the paper.

The story made headlines in the "New York Times". Shortly afterwards, Schneider became aware that he had overestimated the cooling effect of aerosols, and underestimated the warming effect of CO by a factor of about three. He had mistakenly assumed that measurements of air particles he had taken near the source of pollution applied worldwide. He also found that much of the effect was due to natural aerosols which would not be affected by human activities, so the cooling effect of changes in industrial pollution would be much less than he had calculated. Having found that recalculation showed that global warming was the more likely outcome, he published a retraction of his earlier findings in 1974.

In a 1976 book "The Genesis Strategy" he discusses both long-term warming due to carbon dioxide and short-term cooling due to aerosols, and advocated for adopting policies that are resilient to future changes in climate.

Schneider was a frequent contributor to commercial and noncommercial print and broadcast media on climate and environmental issues, e.g., "Nova", "Planet Earth", "Nightline", "Today Show", "The Tonight Show", Bill Maher's shows, "Good Morning America", "Dateline", The Discovery Channel, as well as appearances on the British, Canadian and Australian Broadcasting Corporations.

Schneider commented about the frustrations and difficulties involved with assessing and communicating scientific ideas. In a January 2002 "Scientific American" article, he wrote:
In 1989, Schneider addressed the challenge scientists face trying to communicate complex, important issues without adequate time during media interviews. This citation sometimes was used by his critics to accuse him of supporting misuse of science for political goals:

For the original, together with Schneider's commentary on its misrepresentation, see also American Physical Society, "APS News" August/September 1996.


Schneider was married to the biologist Terry Root. Schneider was a survivor of an aggressive cancer, mantle cell lymphoma. He documented his struggle to conquer the condition, including applying his own knowledge of science to design his own treatment regime, in a self-published 2005 book, "The Patient from Hell". He died unexpectedly on July 19, 2010 after suffering a pulmonary embolism while returning from a scientific meeting in , Sweden.






</doc>
<doc id="29537" url="https://en.wikipedia.org/wiki?curid=29537" title="Scientific misconduct">
Scientific misconduct

Scientific misconduct is the violation of the standard codes of scholarly conduct and ethical behavior in the publication of professional scientific research. A "Lancet" review on "Handling of Scientific Misconduct in Scandinavian countries" provides the following sample definitions,reproduced in The COPE report 1999:

The consequences of scientific misconduct can be damaging for perpetrators and journal audience and for any individual who exposes it. In addition there are public health implications attached to the promotion of medical or other interventions based on false or fabricated research findings.

Three percent of the 3,475 research institutions that report to the US Department of Health and Human Services' Office of Research Integrity, indicate some form of scientific misconduct. However the ORI will only investigate allegations of impropriety where research was funded by federal grants. They routinely monitor such research publication for red flags and their investigation is subject to a statute of limitations. Other private organizations like the Committee of Medical Journal Editors (COJE) can only police their own members.

The validity of the methods and results of scientific papers are often scrutinized in journal clubs. In this venue, members can decide amongst themselves with the help of peers if a scientific paper's ethical standards are met.

According to David Goodstein of Caltech, there are motivators for scientists to commit misconduct, which are briefly summarised here.

The U.S. National Science Foundation defines three types of research misconduct: fabrication, falsification, and plagiarism.
Other types of research misconduct are also recognized:

Compared to other forms of scientific misconduct, image fraud (manipulation of images to distort their meaning) is of particular interest since it can frequently be detected by external parties. In 2006, the "Journal of Cell Biology" gained publicity for instituting tests to detect photo manipulation in papers that were being considered for publication. This was in response to the increased usage of programs such as Adobe Photoshop by scientists, which facilitate photo manipulation. Since then more publishers, including the Nature Publishing Group, have instituted similar tests and require authors to minimize and specify the extent of photo manipulation when a manuscript is submitted for publication. However, there is little evidence to indicate that such tests are applied rigorously. One Nature paper published in 2009 has subsequently been reported to contain around 20 separate instances of image fraud.

Although the type of manipulation that is allowed can depend greatly on the type of experiment that is presented and also differ from one journal to another, in general the following manipulations are not allowed:

Image manipulations are typically done on visually repetitive images such as those of western blots, histologies or data visualisations like graphs.

All authors of a scientific publication are expected to have made reasonable attempts to check findings submitted to academic journals for publication.

Simultaneous submission of scientific findings to more than one journal or duplicate publication of findings is usually regarded as misconduct, under what is known as the Ingelfinger rule, named after the editor of the New England Journal of Medicine 1967-1977, Franz Ingelfinger.

Guest authorship (where there is stated authorship in the absence of involvement, also known as gift authorship) and ghost authorship (where the real author is not listed as an author) are commonly regarded as forms of research misconduct. In some cases coauthors of faked research have been accused of inappropriate behavior or research misconduct for failing to verify reports authored by others or by a commercial sponsor. Examples include the case of Gerald Schatten who co-authored with Hwang Woo-Suk, the case of Professor Geoffrey Chamberlain named as guest author of papers fabricated by Malcolm Pearce, (Chamberlain was exonerated from collusion in Pearce's deception) – and the coauthors with Jan Hendrik Schön at Bell Laboratories. More recent cases include that of Charles Nemeroff, then the editor-in-chief of "Neuropsychopharmacology", and a well-documented case involving the drug Actonel.

Authors are expected to keep all study data for later examination even after publication. The failure to keep data may be regarded as misconduct. Some scientific journals require that authors provide information to allow readers to determine whether the authors might have commercial or non-commercial conflicts of interest. Authors are also commonly required to provide information about ethical aspects of research, particularly where research involves human or animal participants or use of biological material. Provision of incorrect information to journals may be regarded as misconduct. Financial pressures on universities have encouraged this type of misconduct. The majority of recent cases of alleged misconduct involving undisclosed conflicts of interest or failure of the authors to have seen scientific data involve collaborative research between scientists and biotechnology companies.

In general, defining whether an individual is guilty of misconduct requires a detailed investigation by the individual's employing academic institution. Such investigations require detailed and rigorous processes and can be extremely costly. Furthermore, the more senior the individual under suspicion, the more likely it is that conflicts of interest will compromise the investigation. In many countries (with the notable exception of the United States) acquisition of funds on the basis of fraudulent data is not a legal offence and there is consequently no regulator to oversee investigations into alleged research misconduct. Universities therefore have few incentives to investigate allegations in a robust manner, or act on the findings of such investigations if they vindicate the allegation.

Well publicised cases illustrate the potential role that senior academics in research institutions play in concealing scientific misconduct. A King's College (London) internal investigation showed research findings from one of their researchers to be 'at best unreliable, and in many cases spurious' but the college took no action, such as retracting relevant published research or preventing further episodes from occurring. It was only 10 years later, when an entirely separate form of misconduct by the same individual was being investigated by the General Medical Council, that the internal report came to light.

In a more recent case an internal investigation at the National Centre for Cell Science (NCCS), Pune determined that there was evidence of misconduct by Dr. Gopal Kundu, but an external committee was then organised which dismissed the allegation, and the NCCS issued a memorandum exonerating the authors of all charges of misconduct. Undeterred by the NCCS exoneration, the relevant journal ("Journal of Biological Chemistry") withdrew the paper based on its own analysis.

If there was clear evidence of misconduct, the impacted journal can conduct and conclude investigation independent of the author's corresponding research institution. In 2019, the Optical Society of America retracted a research paper by Julia Zhu and Baisong Xie based on its own investigation. The corresponding institutions are Phillips Academy Andover and Beijing Normal University. 

Some academics believe that scientific colleagues who suspect scientific misconduct should consider taking informal action themselves, or reporting their concerns. This question is of great importance since much research suggests that it is very difficult for people to act or come forward when they see unacceptable behavior, unless they have help from their organizations. A "User-friendly Guide," and the existence of a confidential organizational ombudsman may help people who are uncertain about what to do, or afraid of bad consequences for their speaking up.

Journals are responsible for safeguarding the research record and hence have a critical role in dealing with suspected misconduct. This is recognised by the Committee on Publication Ethics (COPE) which has issued clear guidelines on the form (e.g. retraction) that concerns over the research record should take.

Evidence emerged in 2012 that journals learning of cases where there is strong evidence of possible misconduct, with issues potentially affecting a large portion of the findings, frequently fail to issue an expression of concern or correspond with the host institution so that an investigation can be undertaken. In one case the Journal of Clinical Oncology issued a Correction despite strong evidence that the original paper was invalid. 
In another case, Nature allowed a Corrigendum to be published despite clear evidence of image fraud. Subsequent Retraction of the paper required the actions of an independent whistleblower.

The cases of Joachim Boldt and Yoshitaka Fujii in anaesthesiology focussed attention on the role that journals play in perpetuating scientific fraud as well as how they can deal with it. In the Boldt case, the Editors-in-Chief of 18 specialist journals (generally anaesthesia and intensive care) made a joint statement regarding 88 published clinical trials conducted without Ethics Committee approval. In the Fujii case, involving nearly 200 papers, the journal Anesthesia & Analgesia, which published 24 of Fujii's papers, has accepted that its handling of the issue was inadequate. Following publication of a Letter to the Editor from Kranke and colleagues in April 2000, along with a non-specific response from Dr. Fujii, there was no follow-up on the allegation of data manipulation and no request for an institutional review of Dr. Fujii's research. Anesthesia & Analgesia went on to publish 11 additional manuscripts by Dr. Fujii following the 2000 allegations of research fraud, with Editor Steven Shafer stating in March 2012 that subsequent submissions to the Journal by Dr. Fujii should not have been published without first vetting the allegations of fraud. In April 2012 Shafer led a group of editors to write a joint statement, in the form of an ultimatum made available to the public, to a large number of academic institutions where Fujii had been employed, offering these institutions the chance to attest to the integrity of the bulk of the allegedly fraudulent papers.

The consequences of scientific fraud vary based on the severity of the fraud, the level of notice it receives, and how long it goes undetected. For cases of fabricated evidence, the consequences can be wide-ranging, with others working to confirm (or refute) the false finding, or with research agendas being distorted to address the fraudulent evidence. The Piltdown Man fraud is a case in point: The significance of the bona-fide fossils that were being found was muted for decades because they disagreed with Piltdown Man and the preconceived notions that those faked fossils supported. In addition, the prominent paleontologist Arthur Smith Woodward spent time at Piltdown each year until he died, trying to find more Piltdown Man remains. The misdirection of resources kept others from taking the real fossils more seriously and delayed the reaching of a correct understanding of human evolution. (The Taung Child, which should have been the death knell for the view that the human brain evolved first, was instead treated very critically because of its disagreement with the Piltdown Man evidence.)

In the case of Prof Don Poldermans, the misconduct occurred in reports of trials of treatment to prevent death and myocardial infarction in patients undergoing operations. The trial reports were relied upon to issue guidelines that applied for many years across North America and Europe.

In the case of Dr Alfred Steinschneider, two decades and tens of millions of research dollars were lost trying to find the elusive link between infant sleep apnea, which Steinschneider said he had observed and recorded in his laboratory, and sudden infant death syndrome (SIDS), of which he stated it was a precursor. The cover was blown in 1994, 22 years after Steinschneider's 1972 "Pediatrics" paper claiming such an association, when Waneta Hoyt, the mother of the patients in the paper, was arrested, indicted and convicted on five counts of second-degree murder for the smothering deaths of her five children. While that in itself was bad enough, the paper, presumably written as an attempt to save infants' lives, ironically was ultimately used as a defense by parents suspected in multiple deaths of their own children in cases of Münchausen syndrome by proxy. The 1972 "Pediatrics" paper was cited in 404 papers in the interim and is still listed on Pubmed without comment.

The potentially severe consequences for individuals who are found to have engaged in misconduct also reflect on the institutions that host or employ them and also on the participants in any peer review process that has allowed the publication of questionable research. This means that a range of actors in any case may have a motivation to suppress any evidence or suggestion of misconduct. Persons who expose such cases, commonly called whistleblowers, find themselves open to retaliation by a number of different means. These negative consequences for exposers of misconduct have driven the development of whistle blowers charters – designed to protect those who raise concerns. A whistleblower is almost always alone in their fight – their career becomes completely dependent on the decision about alleged misconduct. If the accusations prove false, their career is completely destroyed, but even in case of positive decision the career of the whistleblower can be under question: their reputation of "troublemaker" will prevent many employers from hiring them. There is no international body where a whistleblower could give their concerns. If a university fails to investigate suspected fraud or provides a fake investigation to save their reputation the whistleblower has no right of appeal.

With the advancement of the internet, there are now several tools available to aid in the detection of plagiarism and multiple publication within biomedical literature. One tool developed in 2006 by researchers in Dr. Harold Garner's laboratory at the University of Texas Southwestern Medical Center at Dallas is Déjà vu, an open-access database containing several thousand instances of duplicate publication. All of the entries in the database were discovered through the use of text data mining algorithm eTBLAST, also created in Dr. Garner's laboratory. The creation of Déjà vu and the subsequent classification of several hundred articles contained therein have ignited much discussion in the scientific community concerning issues such as ethical behavior, journal standards, and intellectual copyright. Studies on this database have been published in journals such as "Nature" and "Science", among others.

Other tools which may be used to detect fraudulent data include error analysis. Measurements generally have a small amount of error, and repeated measurements of the same item will generally result in slight differences in readings. These differences can be analyzed, and follow certain known mathematical and statistical properties. Should a set of data appear to be too faithful to the hypothesis, i.e., the amount of error that would normally be in such measurements does not appear, a conclusion can be drawn that the data may have been forged. Error analysis alone is typically not sufficient to prove that data have been falsified or fabricated, but it may provide the supporting evidence necessary to confirm suspicions of misconduct.

Kirby Lee and Lisa Bero suggest, "Although reviewing raw data can be difficult, time-consuming and expensive, having such a policy would hold authors more accountable for the accuracy of their data and potentially reduce scientific fraud or misconduct."

Andrew Wakefield, who claimed links between the MMR vaccine, autism and inflammatory bowel disease, was found guilty of dishonesty in his research and banned from medicine by the UK General Medical Council following an investigation by Brian Deer of the London Sunday Times.





</doc>
<doc id="29538" url="https://en.wikipedia.org/wiki?curid=29538" title="Set (card game)">
Set (card game)

Set (stylized as SET) is a real-time card game designed by Marsha Falco in 1974 and published by Set Enterprises in 1991. The deck consists of 81 unique cards that vary in four features across three possibilities for each kind of feature: number of shapes (one, two, or three), shape (diamond, squiggle, oval), shading (solid, striped, or open), and color (red, green, or purple). Each possible combination of features (e.g. a card with [three] [striped] [green] [diamonds]) appears as a card precisely "once" in the deck.

In the game, certain combinations of three cards are said to make up a set. For each one of the four categories of features — color, number, shape, and shading — the three cards must display that feature as a) either all the same, or b) all different. Put another way: For each feature the three cards must "avoid" having two cards showing one version of the feature and the remaining card showing a different version.

For example, 3 solid red diamonds, 2 solid green squiggles, and 1 solid purple oval form a set, because the shadings of the three cards are all the same, while the numbers, the colors, and the shapes among the three cards are all different.

For any "set", the number of features that are all the same and the number of features that are all different may break down as 0 the same + 4 different; or 1 the same + 3 different; or 2 the same + 2 different; or 3 the same + 1 different. (It cannot break down as 4 features the same + 0 different as the cards would be identical, and there are no identical cards in the Set deck.)

The game evolved out of a coding system that the designer used in her job as a geneticist. "Set" won American Mensa's "Mensa Select" award in 1991 and placed 9th in the 1995 "Deutscher Spiele Preis".

Several games can be played with these cards, all involving the concept of a "set". A set consists of three cards satisfying "all" of these conditions:

The rules of "Set" are summarized by: If you can sort a group of three cards into "two of ____ and one of ____", then it is not a set.

For example, these three cards form a set:

Given any two cards from the deck, there is one and only one other card that forms a set with them.

In the standard Set game, the dealer lays out cards on the table until either twelve are laid down or someone sees a set and calls "Set!". The player who called "Set" takes the cards in the set, and the dealer continues to deal out cards until twelve are on the table. A player who sees a set among the twelve cards calls "Set" and takes the three cards, and the dealer lays three more cards on the table. (To call out "set" and not pick one up quickly enough results in a penalty.) It is possible that there is no set among the twelve cards; in this case, the dealer deals out three more cards to make fifteen dealt cards, or eighteen or more, as necessary. This process of dealing by threes and finding sets continues until the deck is exhausted and there are no more sets on the table. At this point, whoever has collected the most sets wins.

Variants were included with the Set game that involve different mechanics to find sets, as well as different player interaction. Additional variants continue to be created by avid players of the game.



</doc>
<doc id="29539" url="https://en.wikipedia.org/wiki?curid=29539" title="Silver Star">
Silver Star

The Silver Star Medal is the United States Armed Forces' third-highest personal decoration for valor in combat. The Silver Star Medal is awarded primarily to members of the United States Armed Forces for gallantry in action against an enemy of the United States.

The Silver Star Medal (SSM) is the successor award to the "Citation Star" ( silver star) which was established by an Act of Congress on July 9, 1918, during World War I. On July 19, 1932, the Secretary of War approved the conversion of the "Citation Star" to the SSM with the original "Citation Star" incorporated into the center of the medal.

Authorization for the Silver Star Medal was placed into law by an Act of Congress for the U.S. Navy on August 7, 1942, and an Act of Congress for the U.S. Army on December 15, 1942. The current statutory authorization for the medal is Title 10 of the United States Code, for the U.S. Army, for the U.S. Air Force, and for the U.S. Navy.

The U.S. Army and Air Force award the medal as the "Silver Star". The U.S. Navy, Marine Corps, and Coast Guard continue to award the medal as the "Silver Star Medal". Since 21 December 2016, the Department of Defense (DoD) refers to the decoration as the Silver Star Medal.

The Silver Star Medal is awarded for gallantry, so long as the action does not justify the award of one of the next higher valor awards: the Distinguished Service Cross, the Navy Cross, the Air Force Cross, or the Coast Guard Cross. The gallantry displayed must have taken place while in action against an enemy of the United States, while engaged in military operations involving conflict with an opposing foreign force, or while serving with friendly foreign forces engaged in an armed conflict against an opposing armed force in which the United States is not a belligerent party.

The Silver Star Medal is awarded for singular acts of valor or heroism over a brief period, such as one or two days of a battle.

Air Force pilots and combat systems officers and Navy/Marine Corps naval aviators and flight officers flying fighter aircraft, are often considered eligible to receive the Silver Star upon becoming an ace (i.e., having five or more confirmed aerial kills), which entails the pilot and, in multi-seat fighters, the weapons system officer or radar intercept officer, intentionally and successfully risking his life multiple times under combat conditions and emerging victorious. However, during the Vietnam War, the last conflict to produce U.S. fighter aces: an Air Force pilot and two navigators/weapon systems officers (who were later retrained as Air Force pilots), a naval aviator and a naval flight officer/radar intercept officer who had achieved this distinction, were eventually awarded the Air Force Cross and Navy Cross, respectively, in addition to SSMs previously awarded for earlier aerial kills.


The Silver Star Medal is a gold five-pointed star, in circumscribing diameter with a laurel wreath encircling rays from the center and a diameter silver star superimposed in the center. The pendant is suspended from a rectangular shaped metal loop with rounded corners. The reverse has the inscription "FOR GALLANTRY IN ACTION". The ribbon is wide and consists of the following stripes: Old Glory red (center stripe); proceeding outward in pairs white; ultramarine blue; white; and ultramarine blue.

Second and subsequent awards of the Silver Star Medal are denoted by bronze or silver oak leaf clusters in the Army and Air Force and by gold or silver inch stars in the Navy, Marine Corps, and Coast Guard.

The Department of Defense does not keep extensive records for the Silver Star Medal. Independent groups estimate that between 100,000 and 150,000 SSMs have been awarded since the decoration was established. Colonel David Hackworth who was awarded ten SSMs while serving in the Army during the Korean War and Vietnam War, is likely to be the person awarded the most SSMs. Donald H. Russell, a civilian Vought F4U Corsair technical support engineer attached to a Marine Corps fighter wing, received the SSM for his actions aboard after the carrier was attacked by a Japanese dive bomber in March 1945. In the fall of 1944, President Roosevelt’s close adviser Harry Hopkins, the U.S. Ambassador in Moscow W. Averell Harriman and a military attaché presented the SSM to Soviet Red Army artillery officer Alexei Voloshin, who was the first to cross the Dnieper with his battery and was one of four junior Red Army officers who received the award. Wyatt Waldron was a Marine with 3rd Battalion 4th Marines and received the SSM, the Purple Heart and two other medals of Valor during his three combat deployments in Iraq. 

Three Army nurses that served in World War I were cited in 1919 and 1920 with Citation Stars for gallantry in attending to the wounded while under artillery fire in July 1918. In 2007, it was discovered that they had never been awarded their Citation Stars. The three nurses (Army nurses served without rank until 1920) were awarded the Silver Star Medal posthumously:


An unknown number of servicewomen received the award in World War II. Four Army nurses serving in Italy during the war—First Lieutenant Mary Roberts, Second Lieutenant Elaine Roe, Second Lieutenant Rita Virginia Rourke, and Second Lieutenant Ellen Ainsworth (posthumous)—became the first women recipients of the Silver Star, all cited for their bravery in evacuating the 33rd Field Hospital at Anzio on February 10, 1944. Later that same year, Corporal Magdalena Leones, a Filipino American, received the medal for clandestine activities on Luzon; , she is the only female Asian American to receive a Silver Star.

The next known servicewomen to receive the Silver Star is Army National Guard Sergeant Leigh Ann Hester in 2005, for gallantry during an insurgent ambush on a convoy in Iraq and Army Specialist Monica Lin Brown in March 2008, for extraordinary heroism as a combat medic in the War in Afghanistan.

Notable recipients include:


















</doc>
<doc id="29540" url="https://en.wikipedia.org/wiki?curid=29540" title="Single UNIX Specification">
Single UNIX Specification

The Single UNIX Specification (SUS) is the collective name of a family of standards for computer operating systems, compliance with which is required to qualify for using the "UNIX" trademark. The core specifications of the SUS are developed and maintained by the Austin Group, which is a joint working group of IEEE, ISO JTC 1 SC22 and The Open Group. If an operating system is submitted to The Open Group for certification, and passes conformance tests, then it is deemed to be compliant with a UNIX standard such as UNIX 98 or UNIX 03.

Very few BSD and Linux-based operating systems are submitted for compliance with the Single UNIX Specification, although system developers generally aim for compliance with POSIX standards, which form the core of the Single UNIX Specification.

The SUS emerged from a mid-1980s project to standardize operating system interfaces for software designed for variants of the Unix operating system. The need for standardization arose because enterprises using computers wanted to be able to develop programs that could be used on the computer systems of different manufacturers without reimplementing the programs. Unix was selected as the basis for a standard system interface partly because it was manufacturer-neutral.

In 1988, these standards became IEEE 1003 (also registered as ISO/IEC 9945), or POSIX, which loosely stands for Portable Operating System Interface.

In the early 1990s, a separate effort known as the Common API Specification or Spec 1170 was initiated by several major vendors, who formed the COSE alliance in the wake of the Unix wars. This specification became more popular because it was available at no cost, whereas the IEEE charged a substantial fee for access to the POSIX specification. Management over these specifications was assigned to X/Open who also received the Unix trademark from Novell in 1993. Unix International (UI) merged into Open Software Foundation (OSF) in 1994 only to merge with X/Open to form The Open Group in 1996.

This was a repackaging of the X/Open Portability Guide (XPG), Issue 4, Version 2.

In 1995, the Open Group released the Single UNIX Specification Version 1, 1995 Edition.

This specification consisted of:
and was at the core of the UNIX 95 brand.

In 1997, the Open Group released the Single UNIX Specification Version 2.

This specification consisted of:
and was at the core of the UNIX 98 brand.

Beginning in 1998, a joint working group known as the Austin Group began to develop the combined standard that would be known as the Single UNIX Specification Version 3 and as POSIX:2001 (formally: IEEE Std 1003.1-2001). It was released on January 30, 2002.

This standard consisted of:
and is at the core of the UNIX 03 brand.

In 2004, a new edition of the POSIX:2001 standard was released, incorporating two technical corrigenda. It is called POSIX:2004 (formally: IEEE Std 1003.1-2004).

In December 2008, the Austin Group published a new major revision, known as POSIX:2008 (formally: IEEE Std 1003.1-2008). This is the core of the Single UNIX Specification, Version 4 (SUSv4).

This standard consists of:

The Technical Corrigendum 1 is mostly targeting internationalization and it introduces a role-based access model. It was published in 2012 for the Unix Base specification and it is registered as the 2013 Edition of POSIX 2008. A trademark "UNIX V7" (not to be confused with V7 UNIX, the version of Research Unix from 1979) has been created to mark compliance with SUS Version 4.

The Technical Corrigendum 2 has been published in September 2016, leading into "IEEE Std 1003.1-2008, 2016 Edition" and "Single UNIX Specification, Version 4, 2016 Edition".

In January 2018 an "administrative rollup" edition, susv4-2018, was released. It incorporates Single UNIX Specification version 4 TC1 and TC2, and is technically identical to the 2016 edition.

SUSv3 totals some 3700 pages, which are thematically divided into four main parts:


The standard user command line and scripting interface is the POSIX shell, an extension of the Bourne Shell based on an early version of the Korn Shell. Other user-level programs, services and utilities include awk, echo, ed, vi, and hundreds of others. Required program-level services include basic I/O (file, terminal, and network) services. A test suite accompanies the standard. It is called PCTS or the POSIX Certification Test Suite.

Additionally, SUS includes CURSES (XCURSES) specification, which specifies 372 functions and 3 header files. All in all, SUSv3 specifies 1742 interfaces.

Note that a system need not include source code derived in any way from AT&T Unix to meet the specification. For instance, IBM OS/390, now z/OS, qualifies as a "Unix" despite having no code in common.

There are three official marks for conforming systems

Older UNIX standards (superseded)

AIX 5L V5.2 with some updates, AIX 5L V5.3 and AIX 6.1, are registered as UNIX 03 compliant.

EulerOS 2.0 for the x86-64 architecture was certified as UNIX 03 compliant. The UNIX 03 conformance statement shows that the standard C compiler is from the GNU Compiler Collection (gcc), and that the system is a Linux distribution of the Red Hat family.

HP-UX 11i V3 Release B.11.31 is registered as UNIX 03 compliant. Previous releases are registered as UNIX 95.

Apple's macOS (previously known as Mac OS X or OS X) is a UNIX 03 registered product,
first becoming registered with Mac OS X 10.5 "Leopard" on October 26, 2007 (when run on Macs with Intel processors). All newer versions of macOS have been registered.

SCO UnixWare 7.1.3 and later are registered as UNIX 95 compliant. UnixWare belongs now to Xinuos. The next renewal date is on 16 May 2021.

z/OS 1.9, released on September 28, 2007, and subsequent releases "better align" with UNIX 03.

Solaris 11.4 was registered as UNIX v7 compliant; Solaris is the only system that was registered as v7 compliant . Solaris 11 and Solaris 10 was registered as UNIX 03 compliant on 32-bit and 64-bit x86 (X86-64) and SPARC systems. Solaris 8 and 9 was registered as UNIX 98 compliant on 32-bit x86 and SPARC systems; 64-bit x86 systems were not supported.

Solaris 2.5.1 was also registered as UNIX 95 compliant on the PReP PowerPC platform in 1996, but the product was withdrawn before more than a few dozen copies had been sold.

The last Reliant UNIX versions were registered as UNIX 95 compliant (XPG4 hard branding).

Inspur K-UX 2.0 and 3.0 for the x86-64 architecture were certified as UNIX 03 compliant. The UNIX 03 conformance statement for Inspur K-UX 2.0 and 3.0 shows that the standard C compiler is from the GNU Compiler Collection (gcc), and that the system is a Linux distribution of the Red Hat family.

Tru64 UNIX V5.1A and later were registered as UNIX 98 compliant.

Other operating systems previously registered as UNIX 95 or UNIX 93 compliant:

Developers and vendors of Unix-like operating systems such as Linux, FreeBSD, and MINIX, typically do not certify their distributions and do not install full POSIX utilities by default. Sometimes, SUS compliance can be improved by installing additional packages, but very few Linux systems can be configured to be completely conformant.

Darwin, the open source subset of macOS, has behavior that can be set to comply with UNIX 03.

FreeBSD previously had a "C99 and POSIX Conformance Project" which aimed for compliance with a subset of the Single UNIX Specification, and documentation where there were differences.

For Linux, the Linux Standard Base was formed in 2001 as an attempt to standardize the internal structures of Linux-based systems for increased compatibility. It is based on the POSIX specifications, the Single UNIX Specification, and other open standards, and also extends them in several areas; but there are some conflicts between the LSB and The POSIX standards. However, although these standards are commonly accepted, few Linux distributions actually go through certification as LSB compliant.




</doc>
<doc id="29544" url="https://en.wikipedia.org/wiki?curid=29544" title="Scientific Revolution">
Scientific Revolution

The Scientific Revolution was a series of events that marked the emergence of modern science during the early modern period, when developments in mathematics, physics, astronomy, biology (including human anatomy) and transformed the views of society about nature. The Scientific Revolution took place in Europe towards the end of the Renaissance period and continued through the late 18th century, influencing the intellectual social movement known as the Enlightenment. While its dates are debated, the publication in 1543 of Nicolaus Copernicus' "De revolutionibus orbium coelestium" ("On the Revolutions of the Heavenly Spheres") is often cited as marking the beginning of the Scientific Revolution.

The concept of a scientific revolution taking place over an extended period emerged in the eighteenth century in the work of Jean Sylvain Bailly, who saw a two-stage process of sweeping away the old and establishing the new. The beginning of the Scientific Revolution, the 'Scientific Renaissance', was focused on the recovery of the knowledge of the ancients; this is generally considered to have ended in 1632 with publication of Galileo's "Dialogue Concerning the Two Chief World Systems". The completion of the Scientific Revolution is attributed to the "grand synthesis" of Isaac Newton's 1687 "Principia". The work formulated the laws of motion and universal gravitation, thereby completing the synthesis of a new cosmology. By the end of the 18th century, the Age of Enlightenment that followed the Scientific Revolution had given way to the "Age of Reflection".

Great advances in science have been termed "revolutions" since the 18th century. In 1747, the French mathematician Alexis Clairaut wrote that "Newton was said in his own life to have created a revolution". The word was also used in the preface to Antoine Lavoisier's 1789 work announcing the discovery of oxygen. "Few revolutions in science have immediately excited so much general notice as the introduction of the theory of oxygen ... Lavoisier saw his theory accepted by all the most eminent men of his time, and established over a great part of Europe within a few years from its first promulgation."

In the 19th century, William Whewell described the revolution in science itself – the scientific method – that had taken place in the 15th-16th century. "Among the most conspicuous of the revolutions which opinions on this subject have undergone, is the transition from an implicit trust in the internal powers of man's mind to a professed dependence upon external observation; and from an unbounded reverence for the wisdom of the past, to a fervid expectation of change and improvement." This gave rise to the common view of the Scientific Revolution today:

The Scientific Revolution is traditionally assumed to start with the Copernican Revolution (initiated in 1543) and to be complete in the "grand synthesis" of Isaac Newton's 1687 "Principia". Much of the change of attitude came from Francis Bacon whose "confident and emphatic announcement" in the modern progress of science inspired the creation of scientific societies such as the Royal Society, and Galileo who championed Copernicus and developed the science of motion.

In the 20th century, Alexandre Koyré introduced the term "scientific revolution", centering his analysis on Galileo. The term was popularized by Butterfield in his "Origins of Modern Science". Thomas Kuhn's 1962 work "The Structure of Scientific Revolutions" emphasized that different theoretical frameworks—such as Einstein's theory of relativity and Newton's theory of gravity, which it replaced—cannot be directly compared without meaning loss.

The period saw a fundamental transformation in scientific ideas across mathematics, physics, astronomy, and biology in institutions supporting scientific investigation and in the more widely held picture of the universe. The Scientific Revolution led to the establishment of several modern sciences. In 1984, Joseph Ben-David wrote:

Many contemporary writers and modern historians claim that there was a revolutionary change in world view. In 1611 the English poet, John Donne, wrote:
Mid-20th-century historian Herbert Butterfield was less disconcerted, but nevertheless saw the change as fundamental:
The history professor Peter Harrison attributes Christianity to having contributed to the rise of the Scientific Revolution:

The Scientific Revolution was built upon the foundation of ancient Greek learning and science in the Middle Ages, as it had been elaborated and further developed by Roman/Byzantine science and medieval Islamic science. Some scholars have noted a direct tie between "particular aspects of traditional Christianity" and the rise of science. The "Aristotelian tradition" was still an important intellectual framework in the 17th century, although by that time natural philosophers had moved away from much of it. Key scientific ideas dating back to classical antiquity had changed drastically over the years, and in many cases been discredited. The ideas that remained, which were transformed fundamentally during the Scientific Revolution, include:

It is important to note that ancient precedent existed for alternative theories and developments which prefigured later discoveries in the area of physics and mechanics; but in light of the limited number of works to survive translation in a period when many books were lost to warfare, such developments remained obscure for centuries and are traditionally held to have had little effect on the re-discovery of such phenomena; whereas the invention of the printing press made the wide dissemination of such incremental advances of knowledge commonplace. Meanwhile, however, significant progress in geometry, mathematics, and astronomy was made in medieval times.

It is also true that many of the important figures of the Scientific Revolution shared in the general Renaissance respect for ancient learning and cited ancient pedigrees for their innovations. Nicolaus Copernicus (1473–1543), Galileo Galilei (1564–1642), Johannes Kepler (1571–1630) and Isaac Newton (1642–1727) all traced different ancient and medieval ancestries for the heliocentric system. In the Axioms Scholium of his "Principia," Newton said its axiomatic three laws of motion were already accepted by mathematicians such as Christiaan Huygens (1629–1695), Wallace, Wren and others. While preparing a revised edition of his "Principia", Newton attributed his law of gravity and his first law of motion to a range of historical figures.

Despite these qualifications, the standard theory of the history of the Scientific Revolution claims that the 17th century was a period of revolutionary scientific changes. Not only were there revolutionary theoretical and experimental developments, but that even more importantly, the way in which scientists worked was radically changed. For instance, although intimations of the concept of inertia are suggested sporadically in ancient discussion of motion, the salient point is that Newton's theory differed from ancient understandings in key ways, such as an external force being a requirement for violent motion in Aristotle's theory.

Under the scientific method as conceived in the 17th century, natural and artificial circumstances were set aside as a research tradition of systematic experimentation was slowly accepted by the scientific community. The philosophy of using an inductive approach to obtain knowledge—to abandon assumption and to attempt to observe with an open mind—was in contrast with the earlier, Aristotelian approach of deduction, by which analysis of known facts produced further understanding. In practice, many scientists and philosophers believed that a healthy mix of both was needed—the willingness to question assumptions, yet also to interpret observations assumed to have some degree of validity.

By the end of the Scientific Revolution the qualitative world of book-reading philosophers had been changed into a mechanical, mathematical world to be known through experimental research. Though it is certainly not true that Newtonian science was like modern science in all respects, it conceptually resembled ours in many ways. Many of the hallmarks of modern science, especially with regard to its institutionalization and professionalization, did not become standard until the mid-19th century.

The Aristotelian scientific tradition's primary mode of interacting with the world was through observation and searching for "natural" circumstances through reasoning. Coupled with this approach was the belief that rare events which seemed to contradict theoretical models were aberrations, telling nothing about nature as it "naturally" was. During the Scientific Revolution, changing perceptions about the role of the scientist in respect to nature, the value of evidence, experimental or observed, led towards a scientific methodology in which empiricism played a large, but not absolute, role.

By the start of the Scientific Revolution, empiricism had already become an important component of science and natural philosophy. Prior thinkers, including the early-14th-century nominalist philosopher William of Ockham, had begun the intellectual movement toward empiricism.

The term British empiricism came into use to describe philosophical differences perceived between two of its founders Francis Bacon, described as empiricist, and René Descartes, who was described as a rationalist. Thomas Hobbes, George Berkeley, and David Hume were the philosophy's primary exponents, who developed a sophisticated empirical tradition as the basis of human knowledge.

An influential formulation of empiricism was John Locke's "An Essay Concerning Human Understanding" (1689), in which he maintained that the only true knowledge that could be accessible to the human mind was that which was based on experience. He wrote that the human mind was created as a "tabula rasa", a "blank tablet," upon which sensory impressions were recorded and built up knowledge through a process of reflection.

The philosophical underpinnings of the Scientific Revolution were laid out by Francis Bacon, who has been called the father of empiricism. His works established and popularised inductive methodologies for scientific inquiry, often called the "Baconian method", or simply the scientific method. His demand for a planned procedure of investigating all things natural marked a new turn in the rhetorical and theoretical framework for science, much of which still surrounds conceptions of proper methodology today.

Bacon proposed a great reformation of all process of knowledge for the advancement of learning divine and human, which he called "Instauratio Magna" (The Great Instauration). For Bacon, this reformation would lead to a great advancement in science and a progeny of new inventions that would relieve mankind's miseries and needs. His "Novum Organum" was published in 1620. He argued that man is "the minister and interpreter of nature", that "knowledge and human power are synonymous", that "effects are produced by the means of instruments and helps", and that "man while operating can only apply or withdraw natural bodies; nature internally performs the rest", and later that "nature can only be commanded by obeying her". Here is an abstract of the philosophy of this work, that by the knowledge of nature and the using of instruments, man can govern or direct the natural work of nature to produce definite results. Therefore, that man, by seeking knowledge of nature, can reach power over it—and thus reestablish the "Empire of Man over creation", which had been lost by the Fall together with man's original purity. In this way, he believed, would mankind be raised above conditions of helplessness, poverty and misery, while coming into a condition of peace, prosperity and security.

For this purpose of obtaining knowledge of and power over nature, Bacon outlined in this work a new system of logic he believed to be superior to the old ways of syllogism, developing his scientific method, consisting of procedures for isolating the formal cause of a phenomenon (heat, for example) through eliminative induction. For him, the philosopher should proceed through inductive reasoning from fact to axiom to physical law. Before beginning this induction, though, the enquirer must free his or her mind from certain false notions or tendencies which distort the truth. In particular, he found that philosophy was too preoccupied with words, particularly discourse and debate, rather than actually observing the material world: "For while men believe their reason governs words, in fact, words turn back and reflect their power upon the understanding, and so render philosophy and science sophistical and inactive."

Bacon considered that it is of greatest importance to science not to keep doing intellectual discussions or seeking merely contemplative aims, but that it should work for the bettering of mankind's life by bringing forth new inventions, having even stated that "inventions are also, as it were, new creations and imitations of divine works". He explored the far-reaching and world-changing character of inventions, such as the printing press, gunpowder and the compass.

Despite his influence on scientific methodology, he himself rejected correct novel theories such as William Gilbert's magnetism, Copernicus's heliocentrism, and Kepler's laws of planetary motion.

Bacon first described the experimental method.
William Gilbert was an early advocate of this method. He passionately rejected both the prevailing Aristotelian philosophy and the Scholastic method of university teaching. His book "De Magnete" was written in 1600, and he is regarded by some as the father of electricity and magnetism. In this work, he describes many of his experiments with his model Earth called the terrella. From these experiments, he concluded that the Earth was itself magnetic and that this was the reason compasses point north.

"De Magnete" was influential not only because of the inherent interest of its subject matter, but also for the rigorous way in which Gilbert described his experiments and his rejection of ancient theories of magnetism. According to Thomas Thomson, "Gilbert['s]... book on magnetism published in 1600, is one of the finest examples of inductive philosophy that has ever been presented to the world. It is the more remarkable, because it preceded the "Novum Organum" of Bacon, in which the inductive method of philosophizing was first explained."

Galileo Galilei has been called the "father of modern observational astronomy", the "father of modern physics", the "father of science", and "the Father of Modern Science". His original contributions to the science of motion were made through an innovative combination of experiment and mathematics.

Galileo was one of the first modern thinkers to clearly state that the laws of nature are mathematical. In "The Assayer" he wrote "Philosophy is written in this grand book, the universe ... It is written in the language of mathematics, and its characters are triangles, circles, and other geometric figures;..." His mathematical analyses are a further development of a tradition employed by late scholastic natural philosophers, which Galileo learned when he studied philosophy. He ignored Aristotelianism. In broader terms, his work marked another step towards the eventual separation of science from both philosophy and religion; a major development in human thought. He was often willing to change his views in accordance with observation. In order to perform his experiments, Galileo had to set up standards of length and time, so that measurements made on different days and in different laboratories could be compared in a reproducible fashion. This provided a reliable foundation on which to confirm mathematical laws using inductive reasoning.

Galileo showed an appreciation for the relationship between mathematics, theoretical physics, and experimental physics. He understood the parabola, both in terms of conic sections and in terms of the ordinate (y) varying as the square of the abscissa (x). Galilei further asserted that the parabola was the theoretically ideal trajectory of a uniformly accelerated projectile in the absence of friction and other disturbances. He conceded that there are limits to the validity of this theory, noting on theoretical grounds that a projectile trajectory of a size comparable to that of the Earth could not possibly be a parabola, but he nevertheless maintained that for distances up to the range of the artillery of his day, the deviation of a projectile's trajectory from a parabola would be only very slight.

Scientific knowledge, according to the Aristotelians, was concerned with establishing true and necessary causes of things. To the extent that medieval natural philosophers used mathematical problems, they limited social studies to theoretical analyses of local speed and other aspects of life. The actual measurement of a physical quantity, and the comparison of that measurement to a value computed on the basis of theory, was largely limited to the mathematical disciplines of astronomy and optics in Europe.

In the 16th and 17th centuries, European scientists began increasingly applying quantitative measurements to the measurement of physical phenomena on the Earth. Galileo maintained strongly that mathematics provided a kind of necessary certainty that could be compared to God's: "...with regard to those few [mathematical propositions] which the human intellect does understand, I believe its knowledge equals the Divine in objective certainty..."

Galileo anticipates the concept of a systematic mathematical interpretation of the world in his book "Il Saggiatore":

Aristotle recognized four kinds of causes, and where applicable, the most important of them is the "final cause". The final cause was the aim, goal, or purpose of some natural process or man-made thing. Until the Scientific Revolution, it was very natural to see such aims, such as a child's growth, for example, leading to a mature adult. Intelligence was assumed only in the purpose of man-made artifacts; it was not attributed to other animals or to nature.

In "mechanical philosophy" no field or action at a distance is permitted, particles or corpuscles of matter are fundamentally inert. Motion is caused by direct physical collision. Where natural substances had previously been understood organically, the mechanical philosophers viewed them as machines. As a result, Isaac Newton's theory seemed like some kind of throwback to "spooky action at a distance". According to Thomas Kuhn, Newton and Descartes held the teleological principle that God conserved the amount of motion in the universe:

Gravity, interpreted as an innate attraction between every pair of particles of matter, was an occult quality in the same sense as the scholastics' "tendency to fall" had been... By the mid eighteenth century that interpretation had been almost universally accepted, and the result was a genuine reversion (which is not the same as a retrogression) to a scholastic standard. Innate attractions and repulsions joined size, shape, position and motion as physically irreducible primary properties of matter.

Newton had also specifically attributed the inherent power of inertia to matter, against the mechanist thesis that matter has no inherent powers. But whereas Newton vehemently denied gravity was an inherent power of matter, his collaborator Roger Cotes made gravity also an inherent power of matter, as set out in his famous preface to the "Principia's" 1713 second edition which he edited, and contradicted Newton himself. And it was Cotes's interpretation of gravity rather than Newton's that came to be accepted.

The first moves towards the institutionalization of scientific investigation and dissemination took the form of the establishment of societies, where new discoveries were aired, discussed and published. The first scientific society to be established was the Royal Society of London. This grew out of an earlier group, centred around Gresham College in the 1640s and 1650s. According to a history of the College:

The scientific network which centred on Gresham College played a crucial part in the meetings which led to the formation of the Royal Society.

These physicians and natural philosophers were influenced by the "new science", as promoted by Francis Bacon in his "New Atlantis", from approximately 1645 onwards. A group known as "The Philosophical Society of Oxford" was run under a set of rules still retained by the Bodleian Library.

On 28 November 1660, the 1660 committee of 12 announced the formation of a "College for the Promoting of Physico-Mathematical Experimental Learning", which would meet weekly to discuss science and run experiments. At the second meeting, Robert Moray announced that the King approved of the gatherings, and a Royal charter was signed on 15 July 1662 creating the "Royal Society of London", with Lord Brouncker serving as the first President. A second Royal Charter was signed on 23 April 1663, with the King noted as the Founder and with the name of "the Royal Society of London for the Improvement of Natural Knowledge"; Robert Hooke was appointed as Curator of Experiments in November. This initial royal favour has continued, and since then every monarch has been the patron of the Society.

The Society's first Secretary was Henry Oldenburg. Its early meetings included experiments performed first by Robert Hooke and then by Denis Papin, who was appointed in 1684. These experiments varied in their subject area, and were both important in some cases and trivial in others. The society began publication of "Philosophical Transactions" from 1665, the oldest and longest-running scientific journal in the world, which established the important principles of scientific priority and peer review.

The French established the Academy of Sciences in 1666. In contrast to the private origins of its British counterpart, the Academy was founded as a government body by Jean-Baptiste Colbert. Its rules were set down in 1699 by King Louis XIV, when it received the name of 'Royal Academy of Sciences' and was installed in the Louvre in Paris.

As the Scientific Revolution was not marked by any single change, the following new ideas contributed to what is called the Scientific Revolution. Many of them were revolutions in their own fields.


For almost five millennia, the geocentric model of the Earth as the center of the universe had been accepted by all but a few astronomers. In Aristotle's cosmology, Earth's central location was perhaps less significant than its identification as a realm of imperfection, inconstancy, irregularity and change, as opposed to the "heavens" (Moon, Sun, planets, stars), which were regarded as perfect, permanent, unchangeable, and in religious thought, the realm of heavenly beings. The Earth was even composed of different material, the four elements "earth", "water", "fire", and "air", while sufficiently far above its surface (roughly the Moon's orbit), the heavens were composed of different substance called "aether". The heliocentric model that replaced it involved not only the radical displacement of the earth to an orbit around the sun, but its sharing a placement with the other planets implied a universe of heavenly components made from the same changeable substances as the Earth. Heavenly motions no longer needed to be governed by a theoretical perfection, confined to circular orbits.

Copernicus' 1543 work on the heliocentric model of the solar system tried to demonstrate that the sun was the center of the universe. Few were bothered by this suggestion, and the pope and several archbishops were interested enough by it to want more detail. His model was later used to create the calendar of Pope Gregory XIII. However, the idea that the earth moved around the sun was doubted by most of Copernicus' contemporaries. It contradicted not only empirical observation, due to the absence of an observable stellar parallax, but more significantly at the time, the authority of Aristotle.

The discoveries of Johannes Kepler and Galileo gave the theory credibility. Kepler was an astronomer who, using the accurate observations of Tycho Brahe, proposed that the planets move around the sun not in circular orbits, but in elliptical ones. Together with his other laws of planetary motion, this allowed him to create a model of the solar system that was an improvement over Copernicus' original system. Galileo's main contributions to the acceptance of the heliocentric system were his mechanics, the observations he made with his telescope, as well as his detailed presentation of the case for the system. Using an early theory of inertia, Galileo could explain why rocks dropped from a tower fall straight down even if the earth rotates. His observations of the moons of Jupiter, the phases of Venus, the spots on the sun, and mountains on the moon all helped to discredit the Aristotelian philosophy and the Ptolemaic theory of the solar system. Through their combined discoveries, the heliocentric system gained support, and at the end of the 17th century it was generally accepted by astronomers.

This work culminated in the work of Isaac Newton. Newton's "Principia" formulated the laws of motion and universal gravitation, which dominated scientists' view of the physical universe for the next three centuries. By deriving Kepler's laws of planetary motion from his mathematical description of gravity, and then using the same principles to account for the trajectories of comets, the tides, the precession of the equinoxes, and other phenomena, Newton removed the last doubts about the validity of the heliocentric model of the cosmos. This work also demonstrated that the motion of objects on Earth and of celestial bodies could be described by the same principles. His prediction that the Earth should be shaped as an oblate spheroid was later vindicated by other scientists. His laws of motion were to be the solid foundation of mechanics; his law of universal gravitation combined terrestrial and celestial mechanics into one great system that seemed to be able to describe the whole world in mathematical formulae.

As well as proving the heliocentric model, Newton also developed the theory of gravitation. In 1679, Newton began to consider gravitation and its effect on the orbits of planets with reference to Kepler's laws of planetary motion. This followed stimulation by a brief exchange of letters in 1679–80 with Robert Hooke, who had been appointed to manage the Royal Society's correspondence, and who opened a correspondence intended to elicit contributions from Newton to Royal Society transactions. Newton's reawakening interest in astronomical matters received further stimulus by the appearance of a comet in the winter of 1680–1681, on which he corresponded with John Flamsteed. After the exchanges with Hooke, Newton worked out proof that the elliptical form of planetary orbits would result from a centripetal force inversely proportional to the square of the radius vector (see Newton's law of universal gravitation – History and "De motu corporum in gyrum"). Newton communicated his results to Edmond Halley and to the Royal Society in "De motu corporum in gyrum", in 1684. This tract contained the nucleus that Newton developed and expanded to form the "Principia".

The "Principia" was published on 5 July 1687 with encouragement and financial help from Edmond Halley. In this work, Newton stated the three universal laws of motion that contributed to many advances during the Industrial Revolution which soon followed and were not to be improved upon for more than 200 years. Many of these advancements continue to be the underpinnings of non-relativistic technologies in the modern world. He used the Latin word "gravitas" (weight) for the effect that would become known as gravity, and defined the law of universal gravitation.

Newton's postulate of an invisible force able to act over vast distances led to him being criticised for introducing "occult agencies" into science. Later, in the second edition of the "Principia" (1713), Newton firmly rejected such criticisms in a concluding General Scholium, writing that it was enough that the phenomena implied a gravitational attraction, as they did; but they did not so far indicate its cause, and it was both unnecessary and improper to frame hypotheses of things that were not implied by the phenomena. (Here Newton used what became his famous expression "hypotheses non fingo").

The writings of Greek physician Galen had dominated European medical thinking for over a millennium. The Flemish scholar Vesalius demonstrated mistakes in the Galen's ideas. Vesalius dissected human corpses, whereas Galen dissected animal corpses. Published in 1543, Vesalius' "De humani corporis fabrica" was a groundbreaking work of human anatomy. It emphasized the priority of dissection and what has come to be called the "anatomical" view of the body, seeing human internal functioning as an essentially corporeal structure filled with organs arranged in three-dimensional space. This was in stark contrast to many of the anatomical models used previously, which had strong Galenic/Aristotelean elements, as well as elements of astrology.

Besides the first good description of the sphenoid bone, he showed that the sternum consists of three portions and the sacrum of five or six; and described accurately the vestibule in the interior of the temporal bone. He not only verified the observation of Etienne on the valves of the hepatic veins, but he described the vena azygos, and discovered the canal which passes in the fetus between the umbilical vein and the vena cava, since named ductus venosus. He described the omentum, and its connections with the stomach, the spleen and the colon; gave the first correct views of the structure of the pylorus; observed the small size of the caecal appendix in man; gave the first good account of the mediastinum and pleura and the fullest description of the anatomy of the brain yet advanced. He did not understand the inferior recesses; and his account of the nerves is confused by regarding the optic as the first pair, the third as the fifth and the fifth as the seventh.

Before Vesalius, the anatomical notes by Alessandro Achillini demonstrate a detailed description of the human body and compares what he has found during his dissections to what others like Galen and Avicenna have found and notes their similarities and differences. Niccolò Massa was an Italian anatomist who wrote an early anatomy text "Anatomiae Libri Introductorius" in 1536, described the cerebrospinal fluid and was the author of several medical works. Jean Fernel was a French physician who introduced the term "physiology" to describe the study of the body's function and was the first person to describe the spinal canal.

Further groundbreaking work was carried out by William Harvey, who published "De Motu Cordis" in 1628. Harvey made a detailed analysis of the overall structure of the heart, going on to an analysis of the arteries, showing how their pulsation depends upon the contraction of the left ventricle, while the contraction of the right ventricle propels its charge of blood into the pulmonary artery. He noticed that the two ventricles move together almost simultaneously and not independently like had been thought previously by his predecessors.

In the eighth chapter, Harvey estimated the capacity of the heart, how much blood is expelled through each pump of the heart, and the number of times the heart beats in half an hour. From these estimations, he demonstrated that according to Gaelen's theory that blood was continually produced in the liver, the absurdly large figure of 540 pounds of blood would have to be produced every day. Having this simple mathematical proportion at hand—which would imply a seemingly impossible role for the liver—Harvey went on to demonstrate how the blood circulated in a circle by means of countless experiments initially done on serpents and fish: tying their veins and arteries in separate periods of time, Harvey noticed the modifications which occurred; indeed, as he tied the veins, the heart would become empty, while as he did the same to the arteries, the organ would swell up.

This process was later performed on the human body (in the image on the left): the physician tied a tight ligature onto the upper arm of a person. This would cut off blood flow from the arteries and the veins. When this was done, the arm below the ligature was cool and pale, while above the ligature it was warm and swollen. The ligature was loosened slightly, which allowed blood from the arteries to come into the arm, since arteries are deeper in the flesh than the veins. When this was done, the opposite effect was seen in the lower arm. It was now warm and swollen. The veins were also more visible, since now they were full of blood.

Various other advances in medical understanding and practice were made. French physician Pierre Fauchard started dentistry science as we know it today, and he has been named "the father of modern dentistry". Surgeon Ambroise Paré (c. 1510–1590) was a leader in surgical techniques and battlefield medicine, especially the treatment of wounds, and Herman Boerhaave (1668–1738) is sometimes referred to as a "father of physiology" due to his exemplary teaching in Leiden and his textbook "Institutiones medicae" (1708).

Chemistry, and its antecedent alchemy, became an increasingly important aspect of scientific thought in the course of the 16th and 17th centuries. The importance of chemistry is indicated by the range of important scholars who actively engaged in chemical research. Among them were the astronomer Tycho Brahe, the chemical physician Paracelsus, Robert Boyle, Thomas Browne and Isaac Newton. Unlike the mechanical philosophy, the chemical philosophy stressed the active powers of matter, which alchemists frequently expressed in terms of vital or active principles—of spirits operating in nature.

Practical attempts to improve the refining of ores and their extraction to smelt metals were an important source of information for early chemists in the 16th century, among them Georg Agricola (1494–1555), who published his great work "De re metallica" in 1556. His work describes the highly developed and complex processes of mining metal ores, metal extraction and metallurgy of the time. His approach removed the mysticism associated with the subject, creating the practical base upon which others could build.

English chemist Robert Boyle (1627–1691) is considered to have refined the modern scientific method for alchemy and to have separated chemistry further from alchemy. Although his research clearly has its roots in the alchemical tradition, Boyle is largely regarded today as the first modern chemist, and therefore one of the founders of modern chemistry, and one of the pioneers of modern experimental scientific method. Although Boyle was not the original discover, he is best known for Boyle's law, which he presented in 1662: the law describes the inversely proportional relationship between the absolute pressure and volume of a gas, if the temperature is kept constant within a closed system.

Boyle is also credited for his landmark publication "The Sceptical Chymist" in 1661, which is seen as a cornerstone book in the field of chemistry. In the work, Boyle presents his hypothesis that every phenomenon was the result of collisions of particles in motion. Boyle appealed to chemists to experiment and asserted that experiments denied the limiting of chemical elements to only the classic four: earth, fire, air, and water. He also pleaded that chemistry should cease to be subservient to medicine or to alchemy, and rise to the status of a science. Importantly, he advocated a rigorous approach to scientific experiment: he believed all theories must be tested experimentally before being regarded as true. The work contains some of the earliest modern ideas of atoms, molecules, and chemical reaction, and marks the beginning of the history of modern chemistry.


Important work was done in the field of optics. Johannes Kepler published "Astronomiae Pars Optica" ("The Optical Part of Astronomy") in 1604. In it, he described the inverse-square law governing the intensity of light, reflection by flat and curved mirrors, and principles of pinhole cameras, as well as the astronomical implications of optics such as parallax and the apparent sizes of heavenly bodies. "Astronomiae Pars Optica" is generally recognized as the foundation of modern optics (though the law of refraction is conspicuously absent).

Willebrord Snellius (1580–1626) found the mathematical law of refraction, now known as Snell's law, in 1621. Subsequently René Descartes (1596–1650) showed, by using geometric construction and the law of refraction (also known as Descartes' law), that the angular radius of a rainbow is 42° (i.e. the angle subtended at the eye by the edge of the rainbow and the rainbow's centre is 42°). He also independently discovered the law of reflection, and his essay on optics was the first published mention of this law.

Christiaan Huygens (1629–1695) wrote several works in the area of optics. These included the "Opera reliqua" (also known as "Christiani Hugenii Zuilichemii, dum viveret Zelhemii toparchae, opuscula posthuma") and the "Traité de la lumière".

Isaac Newton investigated the refraction of light, demonstrating that a prism could decompose white light into a spectrum of colours, and that a lens and a second prism could recompose the multicoloured spectrum into white light. He also showed that the coloured light does not change its properties by separating out a coloured beam and shining it on various objects. Newton noted that regardless of whether it was reflected or scattered or transmitted, it stayed the same colour. Thus, he observed that colour is the result of objects interacting with already-coloured light rather than objects generating the colour themselves. This is known as Newton's theory of colour. From this work he concluded that any refracting telescope would suffer from the dispersion of light into colours. The interest of the Royal Society encouraged him to publish his notes "On Colour" (later expanded into "Opticks"). Newton argued that light is composed of particles or "corpuscles" and were refracted by accelerating toward the denser medium, but he had to associate them with waves to explain the diffraction of light.

In his "Hypothesis of Light" of 1675, Newton posited the existence of the ether to transmit forces between particles. In 1704, Newton published "Opticks", in which he expounded his corpuscular theory of light. He considered light to be made up of extremely subtle corpuscles, that ordinary matter was made of grosser corpuscles and speculated that through a kind of alchemical transmutation "Are not gross Bodies and Light convertible into one another, ...and may not Bodies receive much of their Activity from the Particles of Light which enter their Composition?"

Dr. William Gilbert, in "De Magnete", invented the New Latin word "electricus" from "" ("elektron"), the Greek word for "amber". Gilbert undertook a number of careful electrical experiments, in the course of which he discovered that many substances other than amber, such as sulphur, wax, glass, etc., were capable of manifesting electrical properties. Gilbert also discovered that a heated body lost its electricity and that moisture prevented the electrification of all bodies, due to the now well-known fact that moisture impaired the insulation of such bodies. He also noticed that electrified substances attracted all other substances indiscriminately, whereas a magnet only attracted iron. The many discoveries of this nature earned for Gilbert the title of "founder of the electrical science". By investigating the forces on a light metallic needle, balanced on a point, he extended the list of electric bodies, and found also that many substances, including metals and natural magnets, showed no attractive forces when rubbed. He noticed that dry weather with north or east wind was the most favourable atmospheric condition for exhibiting electric phenomena—an observation liable to misconception until the difference between conductor and insulator was understood.

Robert Boyle also worked frequently at the new science of electricity, and added several substances to Gilbert's list of electrics. He left a detailed account of his researches under the title of "Experiments on the Origin of Electricity". Boyle, in 1675, stated that electric attraction and repulsion can act across a vacuum. One of his important discoveries was that electrified bodies in a vacuum would attract light substances, this indicating that the electrical effect did not depend upon the air as a medium. He also added resin to the then known list of electrics.

This was followed in 1660 by Otto von Guericke, who invented an early electrostatic generator. By the end of the 17th century, researchers had developed practical means of generating electricity by friction with an electrostatic generator, but the development of electrostatic machines did not begin in earnest until the 18th century, when they became fundamental instruments in the studies about the new science of electricity. The first usage of the word "electricity" is ascribed to Sir Thomas Browne in his 1646 work, "Pseudodoxia Epidemica". In 1729 Stephen Gray (1666–1736) demonstrated that electricity could be "transmitted" through metal filaments.

As an aid to scientific investigation, various tools, measuring aids and calculating devices were developed in this period.

John Napier introduced logarithms as a powerful mathematical tool. With the help of the prominent mathematician Henry Briggs their logarithmic tables embodied a computational advance that made calculations by hand much quicker. His Napier's bones used a set of numbered rods as a multiplication tool using the system of lattice multiplication. The way was opened to later scientific advances, particularly in astronomy and dynamics.

At Oxford University, Edmund Gunter built the first analog device to aid computation. The 'Gunter's scale' was a large plane scale, engraved with various scales, or lines. Natural lines, such as the line of chords, the line of sines and tangents are placed on one side of the scale and the corresponding artificial or logarithmic ones were on the other side. This calculating aid was a predecessor of the slide rule. It was William Oughtred (1575–1660) who first used two such scales sliding by one another to perform direct multiplication and division, and thus is credited as the inventor of the slide rule in 1622.

Blaise Pascal (1623–1662) invented the mechanical calculator in 1642. The introduction of his Pascaline in 1645 launched the development of mechanical calculators first in Europe and then all over the world. Gottfried Leibniz (1646–1716), building on Pascal's work, became one of the most prolific inventors in the field of mechanical calculators; he was the first to describe a pinwheel calculator, in 1685, and invented the Leibniz wheel, used in the arithmometer, the first mass-produced mechanical calculator. He also refined the binary number system, foundation of virtually all modern computer architectures.

John Hadley (1682–1744) was the inventor of the octant, the precursor to the sextant (invented by John Bird), which greatly improved the science of navigation.

Denis Papin (1647–1712) was best known for his pioneering invention of the steam digester, the forerunner of the steam engine. The first working steam engine was patented in 1698 by the English inventor Thomas Savery, as a "...new invention for raising of water and occasioning motion to all sorts of mill work by the impellent force of fire, which will be of great use and advantage for drayning mines, serveing townes with water, and for the working of all sorts of mills where they have not the benefitt of water nor constant windes." The invention was demonstrated to the Royal Society on 14 June 1699 and the machine was described by Savery in his book "The Miner's Friend; or, An Engine to Raise Water by Fire" (1702), in which he claimed that it could pump water out of mines. Thomas Newcomen (1664–1729) perfected the practical steam engine for pumping water, the Newcomen steam engine. Consequently, Thomas Newcomen can be regarded as a forefather of the Industrial Revolution.

Abraham Darby I (1678–1717) was the first, and most famous, of three generations of the Darby family who played an important role in the Industrial Revolution. He developed a method of producing high-grade iron in a blast furnace fueled by coke rather than charcoal. This was a major step forward in the production of iron as a raw material for the Industrial Revolution.

Refracting telescopes first appeared in the Netherlands in 1608, apparently the product of spectacle makers experimenting with lenses. The inventor is unknown but Hans Lippershey applied for the first patent, followed by Jacob Metius of Alkmaar. Galileo was one of the first scientists to use this new tool for his astronomical observations in 1609.

The reflecting telescope was described by James Gregory in his book "Optica Promota" (1663). He argued that a mirror shaped like the part of a conic section, would correct the spherical aberration that flawed the accuracy of refracting telescopes. His design, the "Gregorian telescope", however, remained un-built.

In 1666, Isaac Newton argued that the faults of the refracting telescope were fundamental because the lens refracted light of different colors differently. He concluded that light could not be refracted through a lens without causing chromatic aberrations. From these experiments Newton concluded that no improvement could be made in the refracting telescope. However, he was able to demonstrate that the angle of reflection remained the same for all colors, so he decided to build a reflecting telescope. It was completed in 1668 and is the earliest known functional reflecting telescope.

50 years later, John Hadley developed ways to make precision aspheric and parabolic objective mirrors for reflecting telescopes, building the first parabolic Newtonian telescope and a Gregorian telescope with accurately shaped mirrors. These were successfully demonstrated to the Royal Society.

The invention of the vacuum pump paved the way for the experiments of Robert Boyle and Robert Hooke into the nature of vacuum and atmospheric pressure. The first such device was made by Otto von Guericke in 1654. It consisted of a piston and an air gun cylinder with flaps that could suck the air from any vessel that it was connected to. In 1657, he pumped the air out of two conjoined hemispheres and demonstrated that a team of sixteen horses were incapable of pulling it apart. The air pump construction was greatly improved by Robert Hooke in 1658.

Evangelista Torricelli (1607–1647) was best known for his invention of the mercury barometer. The motivation for the invention was to improve on the suction pumps that were used to raise water out of the mines. Torricelli constructed a sealed tube filled with mercury, set vertically into a basin of the same substance. The column of mercury fell downwards, leaving a Torricellian vacuum above.

Surviving instruments from this period, tend to be made of durable metals such as brass, gold, or steel, although examples such as telescopes made of wood, pasteboard, or with leather components exist. Those instruments that exist in collections today tend to be robust examples, made by skilled craftspeople for and at the expense of wealthy patrons. These may have been commissioned as displays of wealth. In addition, the instruments preserved in collections may not have received heavy use in scientific work; instruments that had visibly received heavy use were typically destroyed, deemed unfit for display, or excluded from collections altogether. It is also postulated that the scientific instruments preserved in many collections were chosen because they were more appealing to collectors, by virtue of being more ornate, more portable, or made with higher-grade materials.

Intact air pumps are particularly rare. The pump at right included a glass sphere to permit demonstrations inside the vacuum chamber, a common use. The base was wooden, and the cylindrical pump was brass. Other vacuum chambers that survived were made of brass hemispheres.

Instrument makers of the late seventeenth and early eighteenth century were commissioned by organizations seeking help with navigation, surveying, warfare, and astronomical observation. The increase in uses for such instruments, and their widespread use in global exploration and conflict, created a need for new methods of manufacture and repair, which would be met by the Industrial Revolution.

People and key ideas that emerged from the 16th and 17th centuries:

The idea that modern science took place as a kind of revolution has been debated among historians. A weakness of the idea of scientific revolution is the lack of a systematic approach to the question of knowledge in the period comprehended between the 14th and 17th centuries, leading to misunderstandings on the value and role of modern authors. From this standpoint, the continuity thesis is the hypothesis that there was no radical discontinuity between the intellectual development of the Middle Ages and the developments in the Renaissance and early modern period and has been deeply and widely documented by the works of scholars like Pierre Duhem, John Hermann Randall, Alistair Crombie and William A. Wallace, who proved the preexistence of a wide range of ideas used by the followers of the Scientific Revolution thesis to substantiate their claims. Thus, the idea of a scientific revolution following the Renaissance is—according to the continuity thesis—a myth. Some continuity theorists point to earlier intellectual revolutions occurring in the Middle Ages, usually referring to either a European Renaissance of the 12th century or a medieval Muslim scientific revolution, as a sign of continuity.

Another contrary view has been recently proposed by Arun Bala in his dialogical history of the birth of modern science. Bala proposes that the changes involved in the Scientific Revolution—the mathematical realist turn, the mechanical philosophy, the atomism, the central role assigned to the Sun in Copernican heliocentrism—have to be seen as rooted in multicultural influences on Europe. He sees specific influences in Alhazen's physical optical theory, Chinese mechanical technologies leading to the perception of the world as a machine, the Hindu-Arabic numeral system, which carried implicitly a new mode of mathematical atomic thinking, and the heliocentrism rooted in ancient Egyptian religious ideas associated with Hermeticism.

Bala argues that by ignoring such multicultural impacts we have been led to a Eurocentric conception of the Scientific Revolution. However, he clearly states: "The makers of the revolution—Copernicus, Kepler, Galileo, Descartes, Newton, and many others—had to selectively appropriate relevant ideas, transform them, and create new auxiliary concepts in order to complete their task... In the ultimate analysis, even if the revolution was rooted upon a multicultural base it is the accomplishment of Europeans in Europe." Critics note that lacking documentary evidence of transmission of specific scientific ideas, Bala's model will remain "a working hypothesis, not a conclusion".

A third approach takes the term "Renaissance" literally as a "rebirth". A closer study of Greek philosophy and Greek mathematics demonstrates that nearly all of the so-called revolutionary results of the so-called scientific revolution were in actuality restatements of ideas that were in many cases older than those of Aristotle and in nearly all cases at least as old as Archimedes. Aristotle even explicitly argues against some of the ideas that were espoused during the Scientific Revolution, such as heliocentrism. The basic ideas of the scientific method were well known to Archimedes and his contemporaries, as demonstrated in the well-known discovery of buoyancy. Atomism was first thought of by Leucippus and Democritus. Lucio Russo claims that science as a unique approach to objective knowledge was born in the Hellenistic period (c. 300 BC), but was extinguished with the advent of the Roman Empire. This approach to the Scientific Revolution reduces it to a period of relearning classical ideas that is very much an extension of the Renaissance. This view does not deny that a change occurred but argues that it was a reassertion of previous knowledge (a renaissance) and not the creation of new knowledge. It cites statements from Newton, Copernicus and others in favour of the Pythagorean worldview as evidence.

In more recent analysis of the Scientific Revolution during this period, there has been criticism of not only the Eurocentric ideologies spread, but also of the dominance of male scientists of the time. Female scholars were not always given the opportunities that a male scholar would have had, and the incorporation of women's work in the sciences during this time tends to be obscured. Scholars have tried to look into the participation of women in the 17th century in science, and even with sciences as simple as domestic knowledge women were making advances. With the limited history provided from texts of the period we are not completely aware if women were helping these scientists develop the ideas they did. Another idea to consider is the way this period influenced even the women scientists of the periods following it. Annie Jump Cannon was an astronomer who benefitted from the laws and theories developed from this period; she made several advances in the century following the Scientific Revolution. It was an important period for the future of science, including the incorporation of women into fields using the developments made.





</doc>
<doc id="29545" url="https://en.wikipedia.org/wiki?curid=29545" title="Salian dynasty">
Salian dynasty

The Salian dynasty or Salic dynasty () was a dynasty in the High Middle Ages. The dynasty provided four kings of Germany (1024–1125), all of whom went on to be crowned Holy Roman emperors (1027–1125).

After the death of the last Ottonian emperor in 1024, the Kingdom of Germany and later the entire Holy Roman Empire passed to Conrad II, a Salian. He was followed by three more Salian rulers: Henry III, Henry IV, and Henry V. They established their monarchy as a major European power. The Salian dynasty developed a permanent administrative system based on a class of public officials answerable to the crown.

Modern historians suppose that the Salians descended from the Widonids, a prominent noble kindred emerging in the 7th century. Their estates were located at the confluence of rivers Moselle and Saar and they supported the Carolingians. The Widonids' eastward expansion towards the river Rhine started after they founded Hornbach Abbey in the Bliesgau around 750. Hornbach remained their proprietary monastery and royal grants to the abbey established their presence in the Wormsgau. As time passed, several branches split off the Widonids. The late 9th-century Holy Roman Emperor Guy (or Wido) of Spoleto descended from one of these branches, the Lambertines. The Salians' forefathers remained in Rhenish Franconia. 

Wipo of Burgundy, the biographer of the first Salian monarch, Emperor Conrad II, described Conrad's father and uncle as "distinguished noble lords from Rhenish Franconia", but without calling them Salians around 1044. Wipo added that Conrad's mother, Adelaide of Metz, "supposedly descended from the ancient royal house of Troy". The statement made a connection between Conrad and the royal Merovingians who had claimed a Troyan ancestry for themselves. Historian Stefan Weinfurter proposes that the putative relationship between the Salians and the Merovingians gave rise to the family name, because the Salian Franks had been the most renowned Frankish group. Their memory was preserved through a Frankish law code, known as the Salic law. A less likely etymology links the appellation to the old German world "sal" ("lordship"), proposing that the name can be traced to the Salian monarchs' well-documented inclination towards hierarchical structures.

The term "reges salici" (or Salian kings) was most probably coined early in the 12th century. A list of monarchs and archbishops from Mainz, which was completed around 1139–40, is the first extant document to contain it. Bishop Otto of Freising, a maternal descendant of the Salian monarchs, also used the term in his "Chronicle or History of the Two Cities" in the middle of the 12th century. In a narrow sense, only the four German monarchs who ruled from 1024 to 1125 could be called Salians, but the same appellation has already been expanded to their ancestors by modern historians.

All male members of the family who were destined to a secular career were named Conrad or Henry. Emperor Conrad II's grandfather, Otto of Worms, established this tradition in the late 10th century. He named his eldest son, Henry of Worms, after his maternal great-grandfather, King Henry the Fowler; and he gave the name of his father, Conrad the Red, to one of his younger sons, Conrad of Carinthia. Conrad the Red was most probably named for King Conrad I of Germany.

Count Werner who held estates in the Nahegau, Speyergau and Wormsgau early in the 10th century is the Salian monarchs' first certainly identified ancestor. His family links to the Widonids cannot be securely established, but his patrimonial lands and his close relationship with the Hornbach Abbey provide indirect evidence of his Widonid ancestry. He married a kinswoman, most probably a sister, of King Conrad I of Germany. This marriage alliance with the Conradines introduced Conrad as a leading name in his family.

Werner's son, Conrad the Red, inherited his father Franconian estates. His family links with the Conradines facilitated his acquisition of large portions of their domains after King Otto I of Germany crushed their revolt in 939. The Conradines lost their preeminent position in Franconia and Conrad the Red emerged as Otto I's principal supporter in the region. He was awarded with the Duchy of Lotharingia in 944 or 945 and he married the King's daughter, Luidgard, in 947. The marriage forged a link between the royal Ottonian dynasty and the Salians. He lost Lotharingia after he joined a revolt against his father-in-law in 953 or 954. He died fighting against the invading Magyars in the Battle of Lech in 955. The contemporaneous Widukind of Corvey praised him for his bravery. He was buried in the Worms Cathedral, although mainly bishops and kings had so far been buried in cathedrals.

Conrad the Red's son, Otto of Worms, founded favour with his maternal grandfather, King (from 962 Emperor) Otto I. Still a minor, he was mentioned as a count in the Nahegau in 956. He also seized Wormsgau, Speyergau, Niddagau, Elsenzgau, Kraichgau and Pfinzgau, thus uniting almost all lands between the rivers Rhine and Neckar by the time Otto I deceased in 973. The parentage of his wife, Judith, is uncertain: she may have been related either to Arnulf, Duke of Bavaria, to Count Henry of Arlon, or to Burchard, Margrave in the Eastern Marches. Otto I's son and successor, Emperor Otto II, was obviously worried about the concentration of lands in his nephew's hands in Franconia. The Emperor appointed Otto of Worms to administer the faraway Duchy of Carinthia and March of Verona in 978. The Emperor also persuaded Otto to cede his right to administer justice in Worms and also parts of his revenues in the town, to the local bishop. Otto was persuaded to renounce Carinthia and Verona, but he was lavishly compensated with a large forest in Wasgau, the royal palace at Kaiserslautern and the properietary rights over Weissenburg Abbey. He could also preserve the title of duke, thus he was the first duke to bear the title without ruling a duchy in Germany. Otto was the cousin of Otto III, Holy Roman Emperor, thus he had a strong claim to the throne after the Emperor's death, but he concluded an agreement with the Ottonian candidate, Henry of Bavaria in 1002. Henry restored Carinthia to Otto in 1002 and he ruled the duchy until his death in 1004.

Henry was Otto of Worms's eldest son. His wife, Adelaide, was born into a prominent Lotharingian family, being the daughter of Richard, Count of Metz. Their son, Conrad, would be the first Salian monarch, but Henry could not transfer his seniority rights to his son, because he predeceased his father most probably in 990 or 991.

After Henry of Worms' premature death, his seniority rights shifted to his younger brother, Conrad, enabling him to inherit the major part of the patrimonial lands from his father. Conrad married a daughter of Herman II, Duke of Swabia, Matilda most probably in 1002. Two years later, he succeeded his father as Duke of Carinthia—the duchy passed from father to son for the first time on this occasion. His rule in Carinthia is poorly documented and he died in 1011.

Bruno—the future Pope Gregory V—was a younger son of Otto of Worms. His father's cousin, Otto III, placed him on the papal throne in 996, ignoring the provisions of his own "Diploma Ottonianum" on papal elections. Bruno, who was the first German pope, assumed his papal name in memory of Pope Gregory the Great. He crowned Otto III emperor on the Feast of the Ascension in the same year. The Roman aristocrat Crescentius the Younger expelled him from Rome, but the Emperor crushed the revolt and restored the papal throne to Gregory V. The Pope died at the age of twenty-six or twenty-seven in 999.

William was Otto of Worms' youngest son. After serving in the royal court as archchaplain to Queen Gisella, William was made bishop of Strasbourg in 1028 or 1029. The see of Strasbourg was one of the wealthiest German bishoprics. His tenure was almost uneventful and he died in 1046 or 1047.

Conrad, the elder son of Duke Conrad I of Carinthia and Matilda of Swabia, was born between 1002 and 1005. He was underage when his father died in 1011. He inherited his father's patrimonial lands, but Emperor Henry II made Adalbero of Eppelstein the new duke of Carinthia. After Emperor Henry II died in 1024, both Conrad and his cousin, Conrad the Elder, laid claim to the throne and Conrad the Elder was elected the new monarch.

After the death of the last Saxon Emperor Henry II, the first Salian regent, Conrad II was elected by the majority of the Prince-electors and was crowned German king in Mainz on 8 September 1024. Early in 1026 Conrad went to Milan, where Ariberto, archbishop of Milan, crowned him king of Italy. When Rudolph III, King of Burgundy died in 1032, Conrad II also claimed this kingship on the basis of an inheritance Henry II had extorted from the former in 1006. Despite some opposition, the Burgundian and Provençal nobles paid homage to Conrad in Zürich in 1034. This Kingdom of Burgundy would become known as the Kingdom of Arles under Conrad's successors.

Already in 1028 Conrad II had his son Henry III elected and anointed king of Germany. Henry's tenure led to an overstatement of previously unknown sacral kingship. So during this reign Speyer Cathedral was expanded to be the largest church in Western Christendom. Henry's conception of a legitimate power of royal disposition in the duchies was successful against the dukes, and thus secured royal control. However, in Lorraine, this led to years of conflict, from which Henry emerged as the winner. But also in southern Germany a powerful opposition group was formed in the years 1052–1055. 1046 Henry ended the papal schism, freed the Papacy from dependence on the Roman nobility, and laid the basis for its universal applicability. His early death in 1056 was long regarded as a disaster for the Empire.

The early Salians owed much of their success to their alliance with the Church, a policy begun by Otto I, which gave them the material support they needed to subdue rebellious dukes. In time, however, the Church came to regret this close relationship. The alliance broke down in 1075 during what came to be known as the Investiture Controversy (or "Investiture Dispute"), a struggle in which the reformist Pope, Gregory VII, demanded that Emperor Henry IV renounce his rights over the Church in Germany. The pope also attacked the concept of monarchy by divine right and gained the support of significant elements of the German nobility interested in limiting imperial absolutism. More important, the pope forbade ecclesiastical officials under pain of excommunication to support Henry as they had so freely done in the past. In the end, Henry IV journeyed to Canossa in northern Italy in 1077 to do penance and to receive absolution from the pope. However, he resumed the practice of lay investiture (appointment of religious officials by civil authorities) and arranged the election of an antipope (Antipope Clement III) in 1080.

The monarch's struggle with the papacy resulted in a war that ravaged through the Holy Roman Empire from 1077 until the Concordat of Worms in 1122. The reign of the last ruler of the Salian dynasty Henry V coincided with the final phase of the great Investiture Controversy, which had pitted pope against emperor. By the settlement of the Concordat of Worms, Henry V surrendered to the demands of the second generation of Gregorian reformers. This agreement stipulated that the pope would appoint high church officials but gave the German king the right to veto the papal choices. Imperial control of Italy was lost for a time, and the imperial crown became dependent on the political support of competing aristocratic factions. Feudalism also became more widespread as freemen sought protection by swearing allegiance to a lord. These powerful local rulers, having thereby acquired extensive territories and large military retinues, took over administration within their territories and organized it around an increasing number of castles. The most powerful of these local rulers came to be called princes rather than dukes.

According to the laws of the feudal system of the Holy Roman Empire, the king had no claims on the vassals of other princes, only on those living within his family's territory. Lacking the support of the formerly independent vassals and weakened by the increasing hostility of the Church, the monarchy lost its pre-eminence. Thus the Investiture Contest strengthened local power in the Holy Roman Empire – in contrast to the trend in France and England, where centralized royal power grew. The Investiture Contest had an additional effect. The long struggle between emperor and pope hurt the Holy Roman Empire's intellectual life, in this period largely confined to monasteries, and the empire no longer led or even kept pace with developments occurring in France and Italy. For instance, no universities were founded in the Holy Roman Empire until the fourteenth century.

The first Hohenstaufen king Conrad III was a grandson of the Salian Henry IV, Holy Roman Emperor. (Agnes, Henry IV's daughter and Henry V's sister, was the heiress of Salian dynasty's lands: her first marriage produced the royal and imperial Hohenstaufen dynasty and her second marriage the ducal Babenberg potentates of Duchy of Austria which was elevated much due to such connections Privilegium Minus.)


Their regnal dates as emperor take into account elections and subsequent coronations.





</doc>
<doc id="29549" url="https://en.wikipedia.org/wiki?curid=29549" title="Self-replication">
Self-replication

Self-replication is any behavior of a dynamical system that yields construction of an identical or similar copy of itself. Biological cells, given suitable environments, reproduce by cell division. During cell division, DNA is replicated and can be transmitted to offspring during reproduction. Biological viruses can replicate, but only by commandeering the reproductive machinery of cells through a process of infection. Harmful prion proteins can replicate by converting normal proteins into rogue forms. Computer viruses reproduce using the hardware and software already present on computers. Self-replication in robotics has been an area of research and a subject of interest in science fiction. Any self-replicating mechanism which does not make a perfect copy (mutation) will experience genetic variation and will create variants of itself. These variants will be subject to natural selection, since some will be better at surviving in their current environment than others and will out-breed them.

Early research by John von Neumann established that replicators have several parts:


Exceptions to this pattern may be possible, although none have yet been achieved. For example, scientists have come close to constructing RNA that can be copied in an "environment" that is a solution of RNA monomers and transcriptase. In this case, the body is the genome, and the specialized copy mechanisms are external. The requirement for an outside copy mechanism has not yet been overcome, and such systems are more accurately characterized as "assisted replication" than "self-replication".

However, the simplest possible case is that only a genome exists. Without some specification of the self-reproducing steps, a genome-only system is probably better characterized as something like a crystal.

Recent research has begun to categorize replicators, often based on the amount of support they require.


The design space for machine replicators is very broad. A comprehensive study to date by Robert Freitas and Ralph Merkle has identified 137 design dimensions grouped into a dozen separate categories, including: (1) Replication Control, (2) Replication Information, (3) Replication Substrate, (4) Replicator Structure, (5) Passive Parts, (6) Active Subunits, (7) Replicator Energetics, (8) Replicator Kinematics, (9) Replication Process, (10) Replicator Performance, (11) Product Structure, and (12) Evolvability.

In computer science a quine is a self-reproducing computer program that, when executed, outputs its own code. For example, a quine in the Python programming language is:

A more trivial approach is to write a program that will make a copy of any stream of data that it is directed to, and then direct it at itself. In this case the program is treated as both executable code, and as data to be manipulated. This approach is common in most self-replicating systems, including biological life, and is simpler as it does not require the program to contain a complete description of itself.

In many programming languages an empty program is legal, and executes without producing errors or other output. The output is thus the same as the source code, so the program is trivially self-reproducing.

In geometry a self-replicating tiling is a tiling pattern in which several congruent tiles may be joined together to form a larger tile that is similar to the original. This is an aspect of the field of study known as tessellation. The "sphinx" hexiamond is the only known self-replicating pentagon. For example, four such concave pentagons can be joined together to make one with twice the dimensions. Solomon W. Golomb coined the term rep-tiles for self-replicating tilings.

In 2012, Lee Sallows identified rep-tiles as a special instance of a self-tiling tile set or setiset. A setiset of order "n" is a set of "n" shapes that can be assembled in "n" different ways so as to form larger replicas of themselves. Setisets in which every shape is distinct are called 'perfect'. A rep-"n" rep-tile is just a setiset composed of "n" identical pieces.

One form of natural self-replication that isn't based on DNA or RNA occurs in clay crystals. Clay consists of a large number of small crystals, and clay is an environment that promotes crystal growth. Crystals consist of a regular lattice of atoms and are able to grow if e.g. placed in a water solution containing the crystal components; automatically arranging atoms at the crystal boundary into the crystalline form. Crystals may have irregularities where the regular atomic structure is broken, and when crystals grow, these irregularities may propagate, creating a form of self-replication of crystal irregularities. Because these irregularities may affect the probability of a crystal breaking apart to form new crystals, crystals with such irregularities could even be considered to undergo evolutionary development.

It is a long-term goal of some engineering sciences to achieve a clanking replicator, a material device that can self-replicate. The usual reason is to achieve a low cost per item while retaining the utility of a manufactured good. Many authorities say that in the limit, the cost of self-replicating items should approach the cost-per-weight of wood or other biological substances, because self-replication avoids the costs of labor, capital and distribution in conventional manufactured goods.

A fully novel artificial replicator is a reasonable near-term goal.
A NASA study recently placed the complexity of a clanking replicator at approximately that of Intel's Pentium 4 CPU. That is, the technology is achievable with a relatively small engineering group in a reasonable commercial time-scale at a reasonable cost.

Given the currently keen interest in biotechnology and the high levels of funding in that field, attempts to exploit the replicative ability of existing cells are timely, and may easily lead to significant insights and advances.

A variation of self replication is of practical relevance in compiler construction, where a similar bootstrapping problem occurs as in natural self replication. A compiler (phenotype) can be applied on the compiler's own source code (genotype) producing the compiler itself. During compiler development, a modified (mutated) source is used to create the next generation of the compiler. This process differs from natural self-replication in that the process is directed by an engineer, not by the subject itself.

An activity in the field of robots is the self-replication of machines. Since all robots (at least in modern times) have a fair number of the same features, a self-replicating robot (or possibly a hive of robots) would need to do the following:


On a nano scale, assemblers might also be designed to self-replicate under their own power. This, in turn, has given rise to the "grey goo" version of Armageddon, as featured in such science fiction novels as "Bloom", "Prey", and "Recursion".

The Foresight Institute has published guidelines for researchers in mechanical self-replication. The guidelines recommend that researchers use several specific techniques for preventing mechanical replicators from getting out of control, such as using a broadcast architecture.

For a detailed article on mechanical reproduction as it relates to the industrial age see mass production.

Research has occurred in the following areas:


The goal of self-replication in space systems is to exploit large amounts of matter with a low launch mass. For example, an autotrophic self-replicating machine could cover a moon or planet with solar cells, and beam the power to the Earth using microwaves. Once in place, the same machinery that built itself could also produce raw materials or manufactured objects, including transportation systems to ship the products. Another model of self-replicating machine would copy itself through the galaxy and universe, sending information back.

In general, since these systems are autotrophic, they are the most difficult and complex known replicators. They are also thought to be the most hazardous, because they do not require any inputs from human beings in order to reproduce.

A classic theoretical study of replicators in space is the 1980 NASA study of autotrophic clanking replicators, edited by Robert Freitas.

Much of the design study was concerned with a simple, flexible chemical system for processing lunar regolith, and the differences between the ratio of elements needed by the replicator, and the ratios available in regolith. The limiting element was Chlorine, an essential element to process regolith for Aluminium. Chlorine is very rare in lunar regolith, and a substantially faster rate of reproduction could be assured by importing modest amounts.

The reference design specified small computer-controlled electric carts running on rails. Each cart could have a simple hand or a small bull-dozer shovel, forming a basic robot.

Power would be provided by a "canopy" of solar cells supported on pillars. The other machinery could run under the canopy.

A "casting robot" would use a robotic arm with a few sculpting tools to make plaster molds. Plaster molds are easy to make, and make precise parts with good surface finishes. The robot would then cast most of the parts either from non-conductive molten rock (basalt) or purified metals. An electric oven melted the materials.

A speculative, more complex "chip factory" was specified to produce the computer and electronic systems, but the designers also said that it might prove practical to ship the chips from Earth as if they were "vitamins".

Nanotechnologists in particular believe that their work will likely fail to reach a state of maturity until human beings design a self-replicating assembler of nanometer dimensions .

These systems are substantially simpler than autotrophic systems, because they are provided with purified feedstocks and energy. They do not have to reproduce them. This distinction is at the root of some of the controversy about whether molecular manufacturing is possible or not. Many authorities who find it impossible are clearly citing sources for complex autotrophic self-replicating systems. Many of the authorities who find it possible are clearly citing sources for much simpler self-assembling systems, which have been demonstrated. In the meantime, a Lego-built autonomous robot able to follow a pre-set track and assemble an exact copy of itself, starting from four externally provided components, was demonstrated experimentally in 2003 .

Merely exploiting the replicative abilities of existing cells is insufficient, because of limitations in the process of protein biosynthesis (also see the listing for RNA).
What is required is the rational design of an entirely novel replicator with a much wider range of synthesis capabilities.

In 2011, New York University scientists have developed artificial structures that can self-replicate, a process that has the potential to yield new types of materials. They have demonstrated that it is possible to replicate not just molecules like cellular DNA or RNA, but discrete structures that could in principle assume many different shapes, have many different functional features, and be associated with many different types of chemical species.

For a discussion of other chemical bases for hypothetical self-replicating systems, see alternative biochemistry.





</doc>
<doc id="29550" url="https://en.wikipedia.org/wiki?curid=29550" title="Shmuel Yosef Agnon">
Shmuel Yosef Agnon

Shmuel Yosef Agnon () (July 17, 1888 – February 17, 1970) was a Nobel Prize laureate writer and was one of the central figures of modern Hebrew fiction. In Hebrew, he is known by the acronym Shai Agnon (). In English, his works are published under the name S. Y. Agnon.

Agnon was born in Polish Galicia, then part of the Austro-Hungarian Empire, and later immigrated to Mandatory Palestine, and died in Jerusalem, Israel.

His works deal with the conflict between the traditional Jewish life and language and the modern world. They also attempt to recapture the fading traditions of the European "shtetl" (village). In a wider context, he also contributed to broadening the characteristic conception of the narrator's role in literature. Agnon had a distinctive linguistic style mixing modern and rabbinic Hebrew. Agnon shared the Nobel Prize with the poet Nelly Sachs in 1966.

Shmuel Yosef Halevi Czaczkes (later Agnon) was born in Buczacz (Polish spelling, pronounced "Buchach") or Butschatsch (German spelling), Polish Galicia (then within the Austro-Hungarian Empire), now Buchach, Ukraine. Officially, his date of birth on the Hebrew calendar was 18 Av 5648 (July 26), but he always said his birthday was on the Jewish fast day of Tisha B'Av, the Ninth of Av.

His father, Shalom Mordechai Halevy, was ordained as a rabbi, but worked in the fur trade, and had many connections among the Hasidim, His mother's side had ties to the Mitnagdim.

He did not attend school and was schooled by his parents. In addition to studying Jewish texts, Agnon studied writings of the Haskalah, and was also tutored in German. At the age of eight, he began to write in Hebrew and Yiddish, At the age of 15, he published his first poem – a Yiddish poem about the Kabbalist Joseph della Reina. He continued to write poems and stories in Hebrew and Yiddish, which were published in Galicia.

In 1908, he moved to Jaffa in Ottoman Palestine. The first story he published there was "Agunot" ("Forsaken Wives"), which appeared that same year in the journal "Ha`omer." He used the pen name "Agnon," derived from the title of the story, which he adopted as his official surname in 1924. In 1910, "Forsaken Wives" was translated into German. In 1912, at the urging of Yosef Haim Brenner, he published a novella, "Vehaya Ha'akov Lemishor" ("The Crooked Shall Be Made Straight").

In 1913, Agnon moved to Germany, where he met Esther Marx (1889-1973). They married in 1920 and had two children. In Germany he lived in Berlin and Bad Homburg vor der Höhe (1921–24). Salman Schocken, a businessman and later also publisher, became his literary patron and freed him from financial worries. From 1931 on, his work was published by Schocken Books, and his short stories appeared regularly in the newspaper "Haaretz", also owned by the Schocken family. In Germany, he continued to write short stories and collaborated with Martin Buber on an anthology of Hasidic stories. Many of his early books appeared in Buber's "Jüdischer Verlag" (Berlin). The mostly assimilated, secular German Jews, Buber and Franz Rosenzweig among them, considered Agnon to be a legitimate relic, being a religious man, familiar with Jewish scripture. Gershom Scholem called him "the Jews' Jew".

In 1924, a fire broke out in his home, destroying his manuscripts and rare book collection. This traumatic event crops up occasionally in his stories. Later that year, Agnon returned to Palestine and settled with his family in the Jerusalem neighborhood of Talpiot. In 1929, his library was destroyed again during anti-Jewish riots.

When his novel "Hachnasat Kalla" ("The Bridal Canopy") appeared in 1931 to great critical acclaim, Agnon's place in Hebrew literature was assured. In 1935, he published "Sippur Pashut" ("A Simple Story"), a novella set in Buchach at the end of the 19th century. Another novel, "Tmol Shilshom" ("Only Yesterday"), set in Eretz Yisrael (Israel) of the early 20th century, appeared in 1945.

Agnon's writing has been the subject of extensive academic research. Many leading scholars of Hebrew literature have published books and papers on his work, among them Baruch Kurzweil, Dov Sadan, Nitza Ben-Dov, Dan Miron, Dan Laor and Alan Mintz. Agnon writes about Jewish life, but with his own unique perspective and special touch. In his Nobel acceptance speech, Agnon claimed "Some see in my books the influences of authors whose names, in my ignorance, I have not even heard, while others see the influences of poets whose names I have heard but whose writings I have not read." He went on to detail that his primary influences were the stories of the Bible. Agnon acknowledged that he was also influenced by German literature and culture, and European literature in general, which he read in German translation. A collection of essays on this subject, edited in part by Hillel Weiss, with contributions from Israeli and German scholars, was published in 2010: "Agnon and Germany: The Presence of the German World in the Writings of S.Y. Agnon". The budding Hebrew literature also influenced his works, notably that of his friend, Yosef Haim Brenner. In Germany, Agnon also spent time with the Hebraists Hayim Nahman Bialik and Ahad Ha'am.

The communities he passed through in his life are reflected in his works:

Nitza Ben-Dov writes about Agnon's use of allusiveness, free-association and imaginative dream-sequences, and discusses how seemingly inconsequential events and thoughts determine the lives of his characters.

Some of Agnon's works, such as "The Bridal Canopy", "And the Crooked Shall Be Made Straight", and "The Doctor's Divorce", have been adapted for theatre. A play based on Agnon's letters to his wife, "Esterlein Yakirati", was performed at the Khan Theater in Jerusalem.

Agnon's writing often used words and phrases that differed from what would become established modern Hebrew. His distinct language is based on traditional Jewish sources, such as the Torah and the Prophets, Midrashic literature, the Mishnah, and other Rabbinic literature. Some examples include:

Bar-Ilan University has made a computerized concordance of his works in order to study his language.

Agnon was twice awarded the Bialik Prize for literature (1934 and 1950). He was also twice awarded the Israel Prize, for literature (1954 and 1958).

In 1966, he was awarded the Nobel Prize in Literature "for his profoundly characteristic narrative art with motifs from the life of the Jewish people". The prize was shared with German Jewish author Nelly Sachs. In his speech at the award ceremony, Agnon introduced himself in Hebrew: "As a result of the historic catastrophe in which Titus of Rome destroyed Jerusalem and Israel was exiled from its land, I was born in one of the cities of the Exile. But always I regarded myself as one who was born in Jerusalem".

In later years, Agnon's fame was such that when he complained to the municipality that traffic noise near his home was disturbing his work, the city closed the street to cars and posted a sign that read: "No entry to all vehicles, writer at work!"

 
Agnon died in Jerusalem on February 17, 1970. His daughter, Emuna Yaron, has continued to publish his work posthumously. Agnon's archive was transferred by the family to the National Library in Jerusalem. His home in Talpiot, built in 1931 in the Bauhaus style, was turned into a museum, "Beit Agnon." The study where he wrote many of his works was preserved intact. Agnon's image, with a list of his works and his Nobel Prize acceptance speech, appeared on the fifty-shekel bill, second series, in circulation from 1985 to 2014. The main street in Jerusalem's Givat Oranim neighborhood is called Sderot Shai Agnon, and a synagogue in Talpiot, a few blocks from his home, is named after him. Agnon is also memorialized in Buchach, now in Ukraine, where he was born. There is an extensive (relative to the size of the museum) exhibition in the Historical Museum in Buchach and, just a few yards away, a bust of Agnon is mounted on a pedestal in a plaza across the street from the house where he lived. The house itself is preserved and marked as the home where Agnon lived from birth till the age of (approximately) 19; the street that runs in front of the house is named "Agnon Street" (in Ukrainian).

Agnotherapy is a method developed in Israel to help elderly people express their feelings.

After Agnon's death, the former mayor of Jerusalem Mordechai Ish-Shalom initiated the opening of his home to the public. In the early 1980s, the kitchen and family dining room were turned into a lecture and conference hall, and literary and cultural evenings were held there. In 2005, the Agnon House Association in Jerusalem renovated the building, which reopened in January 2009. The house was designed by the German-Jewish architect Fritz Korenberg, who was also his neighbor.






In 1977 the Hebrew University published "Yiddish Works", a collection of stories and poems that Agnon wrote in Yiddish during 1903–1906.




</doc>
<doc id="29551" url="https://en.wikipedia.org/wiki?curid=29551" title="Steve Ditko">
Steve Ditko

Stephen J. Ditko (; November 2, 1927 – c. June 29, 2018) was an American comics artist and writer best known as the artist and co-creator, with Stan Lee, of the Marvel Comics superheroes Spider-Man and Doctor Strange.

Ditko studied under Batman artist Jerry Robinson at the Cartoonist and Illustrators School in New York City. He began his professional career in 1953, working in the studio of Joe Simon and Jack Kirby, beginning as an inker and coming under the influence of artist Mort Meskin. During this time, he then began his long association with Charlton Comics, where he did work in the genres of science fiction, horror, and mystery. He also co-created the superhero Captain Atom in 1960.

During the 1950s, Ditko also drew for Atlas Comics, a forerunner of Marvel Comics. He went on to contribute much significant work to Marvel. In 1966, after being the exclusive artist on "The Amazing Spider-Man" and the "Doctor Strange" feature in "Strange Tales", Ditko left Marvel for unclear reasons.

Ditko continued to work for Charlton and also DC Comics, including a revamp of the long-running character the Blue Beetle, and creating or co-creating the Question, the Creeper, Shade the Changing Man, and Hawk and Dove. Ditko also began contributing to small independent publishers, where he created Mr. A, a hero reflecting the influence of Ayn Rand's philosophy of Objectivism. Ditko largely declined to give interviews, saying he preferred to communicate through his work.

Ditko was inducted into the comics industry's Jack Kirby Hall of Fame in 1990, and into the Will Eisner Award Hall of Fame in 1994.

Ditko was born on November 2, 1927 in Johnstown, Pennsylvania, the son of first-generation American Rusyn immigrants from the former Czechoslovakia (now Slovakia), father Stephen Ditko, an artistically talented master carpenter at a steel mill, and mother Anna, a homemaker. The second-oldest child in a working-class family, he was preceded by sister Anna Marie, and followed by sister Elizabeth and brother Patrick. Inspired by his father's love of newspaper comic strips, particularly Hal Foster's "Prince Valiant", Ditko found his interest in comics accelerated by the introduction of the superhero Batman in 1939, and by Will Eisner's "The Spirit", which appeared in a tabloid-sized comic-book insert in Sunday newspapers.

Ditko in junior high school was part of a group of students who crafted wooden models of German airplanes to aid civilian World War II aircraft-spotters. Upon graduating from Greater Johnstown High School in 1945, he enlisted in the U.S. Army on October 26, 1945, and did military service in postwar Germany, where he drew comics for an Army newspaper.

Following his discharge, Ditko learned that his idol, Batman artist Jerry Robinson, was teaching at the Cartoonists and Illustrators School (later the School of Visual Arts) in New York City. Moving there in 1950, he enrolled in the art school under the G.I. Bill. Robinson found the young student "a very hard worker who really focused on his drawing" and someone who "could work well with other writers as well as write his own stories and create his own characters", and he helped Ditko acquire a scholarship for the following year. "He was in my class for two years, four or five days a week, five hours a night. It was very intense." Robinson, who invited artists and editors to speak with his class, once brought in Stan Lee, then editor of Marvel Comics' 1950s precursor Atlas Comics and, "I think that was when Stan first saw Steve's work."

Ditko began professionally illustrating comic books in early 1953, drawing writer Bruce Hamilton's science-fiction story "Stretching Things" for the Key Publications imprint Stanmor Publications, which sold the story to Ajax/Farrell, where it finally found publication in "Fantastic Fears" #5 (cover-dated Feb. 1954). Ditko's first published work was his second professional story, the six-page "Paper Romance" in "Daring Love" #1 (Oct. 1953), published by the Key imprint Gillmor Magazines.

Shortly afterward, Ditko found work at the studio of writer-artists Joe Simon and Jack Kirby, who had created Captain America and other characters. Beginning as an inker on backgrounds, Ditko was soon working with and learning from Mort Meskin, an artist whose work he had long admired. "Meskin was fabulous," Ditko once recalled. "I couldn't believe the ease with which he drew: strong compositions, loose pencils, yet complete; detail without clutter. I loved his stuff". Ditko's known assistant work includes aiding inker Meskin on the Jack Kirby pencil work of Harvey Comics' "Captain 3-D" #1 (Dec. 1953). For his own third published story, Ditko penciled and inked the six-page "A Hole in His Head" in "Black Magic" vol. 4, #3 (Dec. 1953), published by Simon & Kirby's Crestwood Publications imprint Prize Comics.

Ditko then began a long association with the Derby, Connecticut publisher Charlton Comics, a low-budget division of a company best known for song-lyric magazines. Beginning with the cover of "The Thing!" #12 (Feb. 1954) and the eight-page vampire story "Cinderella" in that issue, Ditko would continue to work intermittently for Charlton until the company's demise in 1986, producing science fiction, horror and mystery stories, as well as co-creating Captain Atom, with writer Joe Gill, in "Space Adventures" #33 (March 1960). He first went on hiatus from the company, and comics altogether, in mid-1954, when he contracted tuberculosis and returned to his parents' home in Johnstown to recuperate.

After he recovered and moved back to New York City in late 1955, Ditko began drawing for Atlas Comics, the 1950s precursor of Marvel Comics, beginning with the four-page "There'll Be Some Changes Made" in "Journey into Mystery" #33 (April 1956); this debut tale would be reprinted in Marvel's "Curse of the Weird" #4 (March 1994). Ditko would go on to contribute a large number of stories, many considered classic, to Atlas/Marvel's "Strange Tales" and the newly launched "Amazing Adventures", "Strange Worlds", "Tales of Suspense" and "Tales to Astonish", issues of which would typically open with a Kirby-drawn monster story, followed by one or two twist-ending thrillers or sci-fi tales drawn by Don Heck, Paul Reinman, or Joe Sinnott, all capped by an often-surreal, sometimes self-reflexive short by Ditko and writer-editor Stan Lee.

These Lee-Ditko short stories proved so popular that "Amazing Adventures" was reformatted to feature such stories exclusively beginning with issue #7 (Dec. 1961), when the comic was rechristened "Amazing Adult Fantasy", a name intended to reflect its more "sophisticated" nature, as likewise the new tagline "The magazine that respects your intelligence". Lee in 2009 described these "short, five-page filler strips that Steve and I did together", originally "placed in any of our comics that had a few extra pages to fill", as "odd fantasy tales that I'd dream up with O. Henry-type endings." Giving an early example of what would later be known as the "Marvel Method" of writer-artist collaboration, Lee said, "All I had to do was give Steve a one-line description of the plot and he'd be off and running. He'd take those skeleton outlines I had given him and turn them into classic little works of art that ended up being far cooler than I had any right to expect."

After Marvel Comics editor-in-chief Stan Lee obtained permission from publisher Martin Goodman to create a new "ordinary teen" superhero named "Spider-Man", Lee originally approached his leading artist, Jack Kirby. Kirby told Lee about his own 1950s character conception, variously called the Silver Spider and Spiderman, in which an orphaned boy finds a magic ring that gives him super powers. Comics historian Greg Theakston says Lee and Kirby "immediately sat down for a story conference" and Lee afterward directed Kirby to flesh out the character and draw some pages. "A day or two later", Kirby showed Lee the first six pages, and, as Lee recalled, "I hated the way he was doing it. Not that he did it badly — it just wasn't the character I wanted; it was too heroic".

Lee turned to Ditko, who developed a visual motif Lee found satisfactory, although Lee would later replace Ditko's original cover with one penciled by Kirby. Ditko said, "The Spider-Man pages Stan showed me were nothing like the (eventually) published character. In fact, the only drawings of Spider-Man were on the splash <nowiki>[</nowiki>i.e., page 1] and at the end [where] Kirby had the guy leaping at you with a web gun... Anyway, the first five pages took place in the home, and the kid finds a ring and turns into Spider-Man."

Ditko also recalled that, "One of the first things I did was to work up a costume. A vital, visual part of the character. I had to know how he looked ... before I did any breakdowns. For example: A clinging power so he wouldn't have hard shoes or boots, a hidden wrist-shooter versus a web gun and holster, etc. ... I wasn't sure Stan would like the idea of covering the character's face but I did it because it hid an obviously boyish face. It would also add mystery to the character..."

Much earlier, in a rare contemporaneous account, Ditko described his and Lee's contributions in a mail interview with Gary Martin published in "Comic Fan" #2 (Summer 1965): "Stan Lee thought the name up. I did costume, web gimmick on wrist & spider signal". He added he would continue drawing Spider-Man "[i]f nothing better comes along." That same year, he expressed to the fanzine "Voice of Comicdom", regarding a poll of "Best Liked" fan-created comics, "It seems a shame, since comics themselves have so little variety of stories and styles that you would deliberately restrict your own creative efforts to professional comics['] shallow range. What is 'Best Liked' by most readers is what they are most familiar in seeing and any policy based on readers likes has to end up with a lot of look-a-like (sic) strips. You have a great opportunity to show everyone a whole new range of ideas, unlimited types of stories and styles—why FLUB it!"

From 1958 to 1968, Ditko shared a Manhattan studio at 43rd Street and Eighth Avenue with noted fetish artist Eric Stanton, an art-school classmate. When either artist was under deadline pressure, it was not uncommon for them to pitch in and help the other with his assignment. Ditko biographer Blake Bell, without citing sources, said, "At one time in history, Ditko denied ever touching Stanton's work, even though Stanton himself said they would each dabble in each other's art; mainly spot-inking", and the introduction to one book of Stanton's work says, "Eric Stanton drew his pictures in India ink, and they were then hand-coloured by Ditko". In a 1988 interview with Theakston, Stanton recalled that although his contribution to Spider-Man was "almost nil", he and Ditko had "worked on storyboards together and I added a few ideas. But the whole thing was created by Steve on his own... I think I added the business about the webs coming out of his hands".

Spider-Man debuted in "Amazing Fantasy" #15 (Aug. 1962), the final issue of that science-fiction/fantasy anthology series. When the issue proved to be a top seller, Spider-Man was given his own series, "The Amazing Spider-Man". Lee and Ditko's collaboration on the series saw the creation of many of the character's best known antagonists including Doctor Octopus in issue #3 (July 1963); the Sandman in #4 (Sept. 1963); the Lizard in #6 (Nov. 1963); Electro in #9 (March 1964); and the Green Goblin in #14 (July 1964). Ditko eventually desired credit for the plotting he was contributing under the Marvel Method. Lee concurred, and starting with #25 (June 1965), Ditko received plot credit for the stories.

One of the most celebrated issues of the Lee-Ditko run is #33 (Feb. 1966), the third part of the story arc "If This Be My Destiny...!", and featuring the dramatic scene of Spider-Man, through force of will and thoughts of family, escaping from being pinned by heavy machinery. Comics historian Les Daniels noted, "Steve Ditko squeezes every ounce of anguish out of Spider-Man's predicament, complete with visions of the uncle he failed and the aunt he has sworn to save." Peter David observed, "After his origin, this two-page sequence from "Amazing Spider-Man" #33 is perhaps the best-loved sequence from the Stan Lee/Steve Ditko era." Steve Saffel stated the "full page Ditko image from "The Amazing Spider-Man" #33 is one of the most powerful ever to appear in the series and influenced writers and artists for many years to come." Matthew K. Manning wrote that "Ditko's illustrations for the first few pages of this Lee story included what would become one of the most iconic scenes in Spider-Man's history." The story was chosen as #15 in the 100 Greatest Marvels of All Time poll of Marvel's readers in 2001. Editor Robert Greenberger wrote in his introduction to the story, "These first five pages are a modern-day equivalent to Shakespeare as Parker's soliloquy sets the stage for his next action. And with dramatic pacing and storytelling, Ditko delivers one of the great sequences in all comics."

Ditko created the supernatural hero Doctor Strange in "Strange Tales" #110 (July 1963). Ditko in the 2000s told a visiting fan that Lee gave Dr. Strange the first name "Stephen".

Though often overshadowed by his Spider-Man work, Ditko's Doctor Strange artwork has been equally acclaimed for its surrealistic mystical landscapes and increasingly psychedelic visuals that helped make the feature a favorite of college students. "People who read 'Doctor Strange' thought people at Marvel must be heads [i.e. drug users]," recalled then-associate editor and former Doctor Strange writer Roy Thomas in 1971, "because they had had similar experiences high on mushrooms. But ... I don't use hallucinogens, nor do I think any artists do."

Eventually Lee & Ditko would take Strange into ever-more-abstract realms. In an epic 17-issue story arc in "Strange Tales" #130–146 (March 1965 – July 1966), Lee and Ditko introduced the cosmic character Eternity, who personified the universe and was depicted as a silhouette whose outlines are filled with the cosmos. As historian Bradford W. Wright describes,

The cartoonist and fine artist Seth in 2003 described Ditko's style as: 

In addition to Dr. Strange, Ditko in the 1960s also drew comics starring the Hulk and Iron Man. He penciled and inked the final issue of "The Incredible Hulk" (#6, March 1963), then continued to collaborate with writer-editor Lee on a relaunched Hulk feature in the omnibus "Tales to Astonish", beginning with issue #60 (Oct. 1964). Ditko, inked by George Roussos, penciled the feature through #67 (May 1965). Ditko designed the Hulk's primary antagonist, the Leader, in #62 (Dec. 1964).

Ditko also penciled the Iron Man feature in "Tales of Suspense" #47–49 (Nov. 1963 – Jan. 1964), with various inkers. The first of these debuted the initial version of Iron Man's modern red-and-golden armor, though whether Ditko or cover-penciler and principal character designer Jack Kirby designed the costume is uncertain.

Whichever feature he drew, Ditko's idiosyncratic, cleanly detailed, instantly recognizable art style, emphasizing mood and anxiety, found great favor with readers. The character of Spider-Man and his troubled personal life meshed well with Ditko's own interests, which Lee eventually acknowledged by giving the artist plotting credits on the latter part of their 38-issue run. But after four years on the title, Ditko left Marvel; he and Lee had not been on speaking terms for some time, with art and editorial changes handled through intermediaries. The details of the rift remain uncertain, even to Lee, who confessed in 2003, "I never really knew Steve on a personal level." Ditko later claimed it was Lee who broke off contact and disputed the long-held belief that the disagreement was over the true identity of the Green Goblin: "Stan never knew what he was getting in my Spider-Man stories and covers until after [production manager] Sol Brodsky took the material from me ... so there couldn't have been any disagreement or agreement, no exchanges ... no problems between us concerning the Green Goblin or anything else from before issue #25 to my final issues". Spider-Man successor artist John Romita, in a 2010 deposition, recalled that Lee and Ditko "ended up not being able to work together because they disagreed on almost everything, cultural, social, historically, everything, they disagreed on characters. ..." A friendly farewell was given to Ditko in the "Bullpen Bulletins" of comics cover-dated July 1966, including "Fantastic Four" #52: "Steve recently told us he was leaving for personal reasons. After all these years, we're sorry to see him go, and we wish the talented guy success with his future endeavors."

Regardless, said Lee in 2007, "Quite a few years ago I met him up at the Marvel offices when I was last in New York. And we spoke; he's a hell of a nice guy and it was very pleasant. ... I haven't heard from him since that meeting."

Back at Charlton—where the page rate was low but creators were allowed greater freedom—Ditko worked on such characters as the Blue Beetle (1967–1968), the Question (1967–1968), and Captain Atom (1965–1967), returning to the character he'd co-created in 1960. In addition, in 1966 and 1967, he drew 16 stories, most of them written by Archie Goodwin, for Warren Publishing's horror-comic magazines "Creepy" and "Eerie", generally using an ink-wash technique.

In 1967, Ditko gave his Objectivist ideas ultimate expression in the form of Mr. A, published in Wally Wood's independent title "witzend" # 3. Ditko's hard line against criminals was controversial and he continued to produce Mr. A stories and one-pagers until the end of the 1970s. Ditko returned to Mr. A in 2000 and in 2009.

Ditko moved to DC Comics in 1968, where he co-created the Creeper in "Showcase" #73 (April 1968) with Don Segall, under editor Murray Boltinoff. DC Comics writer and executive Paul Levitz observed that Ditko's art on the "Creeper" stories made "them look unlike anything else being published by DC at the time." Ditko co-created the team Hawk and Dove in "Showcase" #75 (June 1968), with writer Steve Skeates. Around this time, he penciled the lead story, written and inked by Wally Wood, in Wood's early mature-audience, independent-comics publication "Heroes, Inc. Presents Cannon" (1969).

Ditko's stay at DC was short—he would work on all six issues of the Creeper's own title, "Beware the Creeper" (June 1968 – April 1969), though leaving midway through the final one—and the reasons for his departure uncertain. But while at DC, Ditko recommended Charlton staffer Dick Giordano to the company, who would go on to become a top DC penciller, inker, editor, and ultimately, in 1981, the managing editor.

From this time up through the mid-1970s, Ditko worked exclusively for Charlton and various small press/independent publishers. Frank McLaughlin, Charlton's art director during this period, describes Ditko as living "in a local hotel in Derby for a while. He was a very happy-go-lucky guy with a great sense of humor at that time, and always supplied the [female] color separators with candy and other little gifts".

For Charlton in 1974 he did Liberty Belle backup stories in "E-Man" and conceived Killjoy. Ditko produced much work for Charlton's science-fiction and horror titles, as well as for former Marvel publisher Martin Goodman's start-up line Atlas/Seaboard Comics, where he co-created the superhero the Destructor with writer Archie Goodwin, and penciled all four issues of the namesake series (Feb.–Aug. 1975), the first two of which were inked by Wally Wood. Ditko worked on the second and third issues of "Tiger-Man" and the third issue of "Morlock 2001", with Bernie Wrightson inking.

Ditko returned to DC Comics in 1975, creating a short-lived title, "Shade, the Changing Man" (1977–1978). Shade was later revived, without Ditko's involvement, in DC's mature-audience imprint Vertigo. With writer Paul Levitz, he co-created the four-issue sword and sorcery series "Stalker" (1975–1976). Ditko and writer Gerry Conway produced the first issue of a two-issue "Man-Bat" series. He also revived the Creeper and did such various other jobs as a short Demon backup series in 1979 and stories in DC's horror and science-fiction anthologies. Editor Jack C. Harris hired Ditko as guest artist on several issues of "The Legion of Super-Heroes", a decision which garnered a mixed reaction from the title's readership. Ditko also drew the Prince Gavyn version of Starman in "Adventure Comics" #467–478 (1980). He then decamped to do work for a variety of publishers, briefly contributing to DC again in the mid-1980s, with four pinups of his characters for "Who's Who: The Definitive Directory of the DC Universe" and a pinup for "Superman" #400 (Oct. 1984) and its companion portfolio.

Ditko returned to Marvel in 1979, taking over Jack Kirby's "Machine Man", drawing "The Micronauts" and Captain Universe, and continuing to freelance for the company into the late 1990s. Starting in 1984, he penciled the last two years of the space-robot series "Rom". A Godzilla story by Ditko and Marv Wolfman was changed into a Dragon Lord story published in "Marvel Spotlight". Ditko and writer Tom DeFalco introduced the Speedball character in "The Amazing Spider-Man Annual" #22 (1988) and Ditko drew a ten-issue series based on the character.

In 1982, he also began freelancing for the early independent comics label Pacific Comics, beginning with "Captain Victory and the Galactic Rangers" #6 (Sept. 1982), in which he introduced the superhero Missing Man, with Mark Evanier scripting to Ditko's plot and art. Subsequent Missing Man stories appeared in "Pacific Presents" #1–3 (Oct. 1982 – March 1984), with Ditko scripting the former and collaborating with longtime friend Robin Snyder on the script for the latter two. Ditko also created The Mocker for Pacific, in "Silver Star" #2 (April 1983).

For Eclipse Comics, he contributed a story featuring his character Static (no relation to the later Milestone Comics character) in "Eclipse Monthly" #1–3 (Aug.–Oct. 1983), introducing supervillain the Exploder in #2. With writer Jack C. Harris, Ditko drew the backup feature "The Faceless Ones" in First Comics' "Warp" #2–4 (April–June 1983). Working with that same writer and others, Ditko drew a handful of the Fly, Fly-Girl and Jaguar stories for "The Fly" #2–8 (July 1983 – Aug. 1984), for Archie Comics' short-lived 1980s superhero line; in a rare latter-day instance of Ditko inking another artist, he inked penciler Dick Ayers on the Jaguar story in "The Fly" #9 (Oct. 1984). Western Publishing in 1982 announced a series by Ditko and Harris would appear in a new science-fiction comic, "Astral Frontiers", but that title never materialized.

In the early 1990s Ditko worked for Jim Shooter's newly founded company Valiant Comics, drawing, among others, issues of "Magnus, Robot Fighter", "Solar, Man of the Atom" and "X-O-Manowar". In 1992 Ditko worked with writer Will Murray to produce one of his last original characters for Marvel Comics, the superheroine Squirrel Girl, who debuted in "Marvel Super-Heroes" vol. 2, #8, a.k.a. "Marvel Super-Heroes Winter Special" (Jan. 1992).

In 1993, he did the Dark Horse Comics one-shot "The Safest Place in the World". For the Defiant Comics series "Dark Dominion," he drew issue #0, which was released as a set of trading cards. In 1995, he pencilled a four-issue series for Marvel based on the "Phantom 2040" animated TV series. This included a poster that was inked by John Romita Sr. "Steve Ditko's Strange Avenging Tales" was announced as a quarterly series from Fantagraphics Books, although it only ran one issue (Feb. 1997) due to publicly unspecified disagreements between Ditko and the publisher.

"The New York Times" assessed in 2008 that, "By the '70s he was regarded as a slightly old-fashioned odd-ball; by the '80s he was a commercial has-been, picking up wretched work-for-hire gigs. ...following the example of [Ayn] Rand's John Galt, Ditko hacked out moneymaking work, saving his care for the crabbed Objectivist screeds he published with tiny presses. And boy, could Ditko hack: seeing samples of his "Transformers" coloring book and his Big Boy comic is like hearing Orson Welles sell frozen peas."

Ditko retired from mainstream comics in 1998. His later work for Marvel and DC included such established superheroes as the Sub-Mariner (in "Marvel Comics Presents") and newer, licensed characters such as the "Mighty Morphin Power Rangers". The last mainstream character he created was Marvel's Longarm in "Shadows & Light" #1 (Feb. 1998), in a self-inked, 12-page Iron Man story "A Man's Reach...", scripted by Len Wein. His final mainstream work was a five-page New Gods story for DC Comics, "Infinitely Gentle Infinitely Suffering", inked by Mick Gray and believed to be intended for the 2000–2002 "Orion" series but not published until the 2008 trade paperback "Tales of the New Gods".

Thereafter, Ditko's solo work was published intermittently by Robin Snyder, who was his editor at Charlton, Archie Comics, and Renegade Press in the 1980s. The Snyder publications have included a number of original books as well as reprints such as "Static", "The Missing Man", "The Mocker" and, in 2002, "Avenging World", a collection of stories and essays spanning 30 years.

In 2008, Ditko and Snyder released "The Avenging Mind", a 32-page essay publication featuring several pages of new artwork; and "Ditko, Etc...", a 32-page comic book composed of brief vignettes and editorial cartoons. Releases have continued in that format, with stories introducing such characters as the Hero, Miss Eerie, the Cape, the Madman, the Grey Negotiator, the !? and the Outline. He said in 2012 of his self-published efforts, "I do those because that's all they'll let me do."

In addition to the new material, Ditko and Snyder reprinted earlier Ditko material. In 2010 they published a new edition of the 1973 "Mr. A" comic and a selection of Ditko covers in "The Cover Series". In 2011 they published a new edition of the 1975 comic "...Wha...!? Ditko's H. Series".

Two "lost" stories drawn by Ditko in 1978 have been published by DC in hardcover collections of the artist's work. A Creeper story scheduled for the never published "Showcase" #106 appears in "The Creeper by Steve Ditko" (2010) and an unpublished "Shade, the Changing Man" story appears in "The Steve Ditko Omnibus Vol. 1" (2011). A Hulk and the Human Torch story written by Jack C. Harris and drawn by Ditko in the 1980s was published by Marvel as "Incredible Hulk and the Human Torch: From the Marvel Vault" #1 in August 2011.

As of 2012, Ditko continued to work in Manhattan's Midtown West neighborhood. He mostly declined to give interviews or make public appearances, explaining in 1969 that, "When I do a job, it's not my personality that I'm offering the readers but my artwork. It's not what I'm like that counts; it's what I did and how well it was done. I produce a product, a comic art story. Steve Ditko is the brand name." However, he did contribute numerous essays to Robin Snyder's fanzine "The Comics". Ditko was an ardent supporter of Objectivism.

He had a nephew who became an artist, also named Steve Ditko. As far as it is known, he never married and had no surviving children at the time of his death. Will Eisner stated that Ditko had a son out of wedlock, but this may have been a confused reference to the nephew.

Ditko said in 2012 that he had made no income on the four "Spider-Man" films released to that time. However, a neighbor of Ditko's stated that he received royalty checks. Those involved with creating the "Doctor Strange" film purposely declined to contact him during production, believing they would not be welcome.

Ditko was found unresponsive in his apartment in New York City on June 29, 2018. Police said he had died within the previous two days. He was pronounced dead at age 90, with the cause of death initially deemed as a result of a myocardial infarction, brought on by arteriosclerotic and hypertensive cardiovascular disease.

The final words of Ditko's last essay, published posthumously in "Down Memory Lane" in February 2019, quoted an "old toast" and were appropriately cantankerous: "Here's to those who wish me well, and those that don't can go to hell."


In September 2007, presenter Jonathan Ross hosted a one-hour documentary for BBC Four titled "In Search of Steve Ditko". The program covers Ditko's work at Marvel, DC, and Charlton Comics and at Wally Wood's "witzend", as well as his following of Objectivism. It includes testimonials by Alan Moore, Mark Millar, Jerry Robinson and Stan Lee, among others. Ross, accompanied by writer Neil Gaiman, met Ditko briefly at his New York office, but he declined to be filmed, interviewed or photographed. He did, however, give the two a selection of some comic books. At the end of the show, Ross said he had since spoken to Ditko on the telephone and, as a joke, that he was now on first name terms with him.

As penciller (generally but not exclusively self-inked), unless otherwise noted

Farrell Publications


Harvey Comics


Key Publications


Prize Comics


Charlton Comics


Marvel Comics


St. John Publications


DC Comics

ACG


Dell Publishing


Warren Publishing

Tower Comics


Independent

Atlas/Seaboard

CPL Gang


Star*Reach Productions

M W Communications


Pacific Comics

New Media Publishing


First Comics


Eclipse Comics

Epic Comics

Archie Comics


Deluxe Comics


Renegade Press

Globe Communications


Ace Comics


3-D- Zone


Valiant Comics


Marvel UK


Dark Horse Comics

Defiant Comics


Topps Comics


Yoe! Studio

Fantagraphics Books


AC Comics


Robin Snyder



</doc>
<doc id="29555" url="https://en.wikipedia.org/wiki?curid=29555" title="List of tourist attractions in Sardinia">
List of tourist attractions in Sardinia

This is a list of the most famous tourist destinations of Sardinia. Minor islands are included from Olbia, clockwise — industrial sites are not included.





</doc>
<doc id="29556" url="https://en.wikipedia.org/wiki?curid=29556" title="List of people from Sardinia">
List of people from Sardinia

Sardinia is the second-largest island in the Mediterranean Sea, with a population of about 1.6 million people. The list includes notable natives of Sardinia, as well as those who were born elsewhere but spent a large part of their active life in Sardinia. People of Sardinian heritage and descent are in a separate section of this article. 









Emanuela Loi


























</doc>
<doc id="29558" url="https://en.wikipedia.org/wiki?curid=29558" title="Gavinus">
Gavinus

Saint Gavinus () is a Christian saint who is greatly celebrated in Sardinia, Italy, as one of the Martyrs of Torres (), along with his companions SS Protus and Januarius.

He was probably a Roman soldier martyred for the Christian faith during the persecution of Diocletian in 304 in the city of Porto Torres (), according to the legend on the orders of the governor ("preside") of Sardinia and Corsica, a certain Barbarus.

The well-known Romanesque church of Gavoi is dedicated to him, as is the town of San Gavino Monreale, and a number of communes in Corsica.

The 11th-century Basilica of San Gavino in Porto Torres, Sassari, is also dedicated to this saint. It was built by Comita or Gomida, Judge of Torres, and contains the relics, discovered in 1614, not only of Saint Gavinus, but also of his companions, Saints Protus and Januarius.

His feast day is given in the Roman Martyrology as 30 May.




</doc>
<doc id="29559" url="https://en.wikipedia.org/wiki?curid=29559" title="Sienna">
Sienna

Sienna (from , meaning "Siena earth") is an earth pigment containing iron oxide and manganese oxide. In its natural state, it is yellowish brown and is called raw sienna. When heated, it becomes a reddish brown and is called burnt sienna. It takes its name from the city-state of Siena, where it was produced during the Renaissance. Along with ochre and umber, it was one of the first pigments to be used by humans, and is found in many cave paintings. Since the Renaissance, it has been one of the brown pigments most widely used by artists.

The first recorded use of "sienna" as a colour name in English was in 1760.

Like the other earth colours, such as yellow ochre and umber, sienna is a clay containing iron oxide, called limonite, which in its natural state has a yellowish colour. In addition to iron oxide, natural or raw sienna also contains about five percent of manganese oxide, which makes it darker than ochre. When heated, the iron oxide is dehydrated and turns partially to haematite, which gives it a reddish-brown colour. 
Sienna is lighter in shade than raw umber, which is also clay with iron oxide, but which has a higher content of manganese (5 to 20 percent) which makes it greenish brown or dark brown. When heated, raw umber becomes burnt umber, a very dark brown.
The pigment sienna was known and used, in its natural form, by the ancient Romans. It was mined near Arcidosso, formerly under Sienese control, now in the province of Grosseto, on Monte Amiata in southern Tuscany. It was called "terra rossa" (red earth), "terra gialla", or terra di Siena"." During the Renaissance, it was noted by the most widely read author about painting techniques, Giorgio Vasari, under the name terra rossa. It became, along with umber and yellow ochre, one of the standard browns used by artists from the 16th to 19th centuries, including Caravaggio (1571-1610) and Rembrandt (1606-1669), who used all the earth colours, including ochre, sienna and umber, in his palette.

By the 1940s, the traditional sources in Italy were nearly exhausted. Much of today's sienna production is carried out in the Italian islands of Sardinia and Sicily, while other major deposits are found in the Appalachian Mountains, where it is often found alongside the region's iron deposits. It is also still produced in the French Ardennes, in the small town of Bonne Fontaine near Ecordal.

In the 20th century, pigments began to be produced using synthetic iron oxide rather than the natural earth. The labels on paint tubes indicate whether they contain natural or synthetic ingredients. PY-43 indicates natural raw sienna, PR-102 indicates natural burnt sienna.

There is no single agreed standard for the colour of sienna, and the name is used today for a wide variety of hues and shades. They vary by country and colour list, and there are many proprietary variations offered by paint companies. The colour box at the top of the article shows one variation from the ISCC-NBS colour list.

Raw sienna is a yellowish-brown natural earth pigment, composed primarily of iron oxide hydroxide. The box shows the colour of the pigment in its natural, or raw state. It contains a large quantity of iron oxide and a small quantity (about five percent) of manganese oxide.

This kind of pigment is known as yellow ochre, yellow earth, limonite, or terra gialla. The pigment name for natural raw sienna from the Colour Index International, shown on the labels of oil paints, is PY-43.

This box at right shows a variation of raw sienna from the Italian Ferrario 1919 colour list. 
Burnt sienna contains a large proportion of anhydrous iron oxide. It is made by heating raw sienna, which dehydrates the iron oxide, changing it partially to haematite, giving it rich reddish-brown colour.

The pigment is also known as red earth, red ochre, and terra rossa. On the Colour Index International, the pigment is known as PR-102.

This version is from the Italian Ferrario 1919 colour list.

The first recorded use of "burnt sienna" as a colour name in English was in 1853.

This variation of burnt sienna is from the Maerz and Paul "A Dictionary of Color" from 1930. It is considerably lighter than most other versions of burnt sienna. It was a mix of burnt orange and raw sienna.

This infobox shows the colour dark sienna. This variation is from the ISCC-NBS colour list. A similar dark sienna paint was frequently used on Bob Ross' TV show, "The Joy of Painting".

The web colour sienna is defined by the list of X11 colours used in web browsers and web design.


</doc>
<doc id="29564" url="https://en.wikipedia.org/wiki?curid=29564" title="Super Bowl XXXVI">
Super Bowl XXXVI

Super Bowl XXXVI was an American football game between the National Football Conference (NFC) champion St. Louis Rams and the American Football Conference (AFC) champion New England Patriots to decide the National Football League (NFL) champion for the 2001 season. The Patriots defeated the Rams by the score of 20–17. It was New England's first Super Bowl championship, and the franchise's first league championship of any kind. The game was also notable for snapping the AFC East's long streak of not being able to win a Super Bowl championship, as the division's teams had lost eight Super Bowls in total (prior to the Patriots victory in XXXVI). It would be the last time the Rams reached a Super Bowl during their time in St. Louis; the team would return to Super Bowl LIII in 2019 as the Los Angeles Rams 17 years later, where they would again face the Patriots, only to lose 13–3.

The game was played at the Louisiana Superdome in New Orleans, on February 3, 2002. Following the September 11, 2001 attacks earlier in the season, the NFL postponed a week of regular-season games and moved the league's playoff schedule back. As a result, Super Bowl XXXVI was rescheduled from the original date of January 27 to February 3, becoming the first Super Bowl played in February. The pregame ceremonies and the halftime show headlined by the Irish rock band U2 honored the victims of 9/11. Due to heightened security measures following the attacks, this was the first Super Bowl designated as a National Special Security Event (NSSE) by the Office of Homeland Security (OHS). The Department of Homeland Security (DHS), which replaced the OHS in 2003, later established the practice of naming each subsequent Super Bowl an NSSE. Additionally, it was the last Super Bowl to be played in New Orleans before Hurricane Katrina slammed the city on August 29, 2005; the first since then was Super Bowl XLVII in 2013.

This game marked the Rams' third Super Bowl appearance in franchise history and the second in three seasons. St. Louis posted an NFL-best 14–2 regular season record, led by quarterback Kurt Warner and "The Greatest Show on Turf" offense. The Patriots clinched their third Super Bowl berth after posting an 11–5 regular season record, led by second-year quarterback Tom Brady and a defense that ended the regular season ranked sixth in scoring.

Although the Rams out-gained the Patriots 427–267 in total yards, New England built a 17–3 third-quarter lead off three Rams turnovers. After a holding penalty in the fourth quarter negated a Patriots fumble return for a touchdown, Warner scored a 2-yard touchdown run and threw a 26-yard touchdown pass to tie the game, 17–17, with 1:30 remaining. Without any timeouts, Brady led his team down the field to set up kicker Adam Vinatieri's game-winning 48-yard field goal as time expired. Brady, who completed 16 of 27 passes for 145 yards and a touchdown, was named Super Bowl MVP.

After the Rams’ 1999 season that had culminated in a gripping victory over the Tennessee Titans in Super Bowl XXXIV, their offense again dominated the league in 2000, leading the NFL in passing, scoring, and total yards. However, the Rams had one of the worst defenses in the league, ranking last in points allowed (471). This, along with injury problems and a coaching change from championship-winning coach Dick Vermeil – who resigned just 48 hours after the game – to his offensive coordinator Mike Martz, caused the Rams to slip to a 10–6 record in 2000. The season ended with a disappointing loss to the New Orleans Saints in the wild card round of the playoffs.

After signing several new defensive players in the off-season, and hiring new defensive coordinator Lovie Smith, the Rams finished the 2001 season with the NFL's best regular season record at 14–2. They led the league in both total offensive yards (6,930) and scoring (503). This was the Rams' third consecutive season with over 500 points, an NFL record. On defense, they only allowed 271 points, improving their 31st ranking in 2000 to 7th in 2001.

The Rams' 1999–2001 offense, nicknamed "The Greatest Show on Turf", is widely considered one of the best in NFL history. The team possessed an incredible amount of offensive talent at nearly every position. In 2001, quarterback Kurt Warner was awarded his second NFL Most Valuable Player Award after throwing for 4,830 yards and 36 touchdowns, with 22 interceptions, and earned a league high 101.4 passer rating. Wide receivers Torry Holt and Isaac Bruce each amassed over 1,100 receiving yards, combining for 142 receptions, 2,469 yards, and 13 touchdowns. Wide receiver Ricky Proehl caught 40 passes for 563 yards and 5 touchdowns. Tight end Ernie Conwell caught 38 passes for 431 yards and 4 touchdowns. Wide receiver Az-Zahir Hakim caught 39 passes for 374 yards, and added another 333 yards returning punts.

Running back Marshall Faulk won NFL Offensive Player of the Year Award for the third year in a row in 2001. He rushed for 1,382 yards, caught 83 passes for 765 yards, scored 21 touchdowns, and became the first NFL player ever to gain more than 2,000 combined rushing and receiving yards for 4 consecutive seasons. Running back Trung Canidate was also a major contributor, rushing for 441 yards, catching 17 passes for 154 yards, returning kickoffs for 748 yards, and scoring 6 touchdowns. The Rams offensive line was led by guard Adam Timmerman and offensive tackle Orlando Pace, who was selected to the Pro Bowl for the third consecutive year.

The Rams' defense ranked third in the league in fewest yards allowed (4,733). The line was anchored by Pro Bowl defensive end Leonard Little, who led the team with 14.5 sacks and recovered a fumble, and defensive end Grant Wistrom, who recorded 9 sacks, 2 interceptions, and 1 fumble recovery. The Rams linebackers unit was led by London Fletcher, who had 4.5 sacks, 2 interceptions, and 4 forced fumbles. St. Louis also had an outstanding secondary, led by Dré Bly (6 interceptions, 150 return yards, and 2 touchdowns), Pro Bowl selection Aeneas Williams (4 interceptions, 69 return yards, 2 touchdowns), and Dexter McCleon (4 interceptions, 66 yards).

The Rams also bested the Patriots in a nationally televised ESPN Sunday night game on November 18 at Foxboro Stadium. Although the Patriots jumped out to an early lead, a critical turnover before the end of the first half that led to a Rams score proved costly. In the second half, the Rams wore New England down and won 24–17. The Rams lost four of their defensive players with injuries. The Patriots' physical play led Rams coach Mike Martz to say after the game that the Patriots were "a Super Bowl–caliber team."

The Patriots' chances for a Super Bowl appearance seemed bleak shortly after the season had begun. Before the season even started, quarterbacks coach Dick Rehbein died of a heart attack at the age of 45. The Patriots, coached by Bill Belichick, lost their first two games, and moreover, in their second loss at home to the New York Jets, starting quarterback Drew Bledsoe suffered a sheared blood vessel on a hit by Jets linebacker Mo Lewis that caused him to miss several weeks. His replacement was second-year quarterback Tom Brady, a sixth-round draft pick who had thrown only 3 passes in 2000. Also, midway through the season, wide receiver Terry Glenn, the team's leading receiver in 2000, was benched due to off-the-field problems. He had been suspended for the first four games for failing a drug test and after serving it he played in just four more before injuries and disputes with the coaching staff caused Belichick to deactivate him for good.
Upon assuming the role of starting quarterback, Brady enjoyed immediate success in the regular season, leading New England to a 44–13 win over the Indianapolis Colts in his first start and eventually to an 11–5 record. He completed 63.9 percent of his passes for 2,843 yards and 18 touchdowns with 12 interceptions and was selected to the Pro Bowl. Veteran Pro Bowl wide receiver Troy Brown was the main receiving threat, recording 101 receptions for 1,199 yards and 5 touchdowns, while also adding another 413 yards and 2 touchdowns returning punts. His 14.2 yards per punt return average led the NFL. Wide receiver David Patten also was productive, catching 51 passes for 749 yards and 4 touchdowns. Running back Antowain Smith provided the team with a stable running game, rushing for 1,157 yards, catching 19 passes for 192 yards, and scoring 13 touchdowns.

New England was outstanding on defense as well. Up front, linemen Bobby Hamilton (7 sacks, 1 fumble recovery) and rookie Richard Seymour excelled at pressuring quarterbacks and stuffing the run. Behind them, the Patriots had three outstanding linebackers: Mike Vrabel (2 interceptions, 3 sacks), Willie McGinest (5 sacks), and Tedy Bruschi (2 interceptions). The secondary also featured outstanding talent such as defensive back Otis Smith, who led the team with 5 interceptions for 181 yards and 2 touchdowns. Cornerback Ty Law intercepted 3 passes, returning them for 91 yards and 2 touchdowns. Safety Lawyer Milloy had 2 interceptions during the season, and was selected along with Law to represent the New England defense in the Pro Bowl. The defense ended the season ranked 6th in scoring, but 24th in total yards allowed. Following their loss to the Rams at home, the Patriots dropped to 5–5, but did not lose again the rest of the season to clinch a first-round bye in the AFC playoffs.

Coincidentally, this was the third straight time that the New England Patriots' Super Bowl appearance would be at the Superdome, meaning they joined the Dallas Cowboys as the only teams to play three different Super Bowls in one stadium; the Cowboys had played three at the old Miami Orange Bowl in the 1970s. In their maiden Super Bowl appearance in Super Bowl XX, the Patriots lost 46–10 – the biggest margin of victory in a Super Bowl to that point – to a Chicago Bears team coached by Mike Ditka and including Mike Singletary and Walter Payton. The Patriots returned to the Superdome 11 years later for Super Bowl XXXI but lost 35–21 to a Green Bay Packers team including Brett Favre, Reggie White and Desmond Howard and coached by Mike Holmgren. Milloy, Law, Vinatieri, Bledsoe, McGinest, Bruschi and Otis Smith were among the players who had played in that game, while Belichick had been assistant head coach to Bill Parcells. The Patriots did not appear in a Super Bowl hosted by another city until the team played in Super Bowl XXXVIII two years later in Houston, Texas.

The Rams began their postseason run with a 45–17 win over the Green Bay Packers in the NFC divisional round. Expected to be a close shootout between Warner and Packers quarterback Brett Favre, the Rams defense dominated the Packers by intercepting a playoff record 6 passes from Favre and returning 3 of them for touchdowns. The Rams offense also racked up 24 points on 2 touchdown passes by Warner, a touchdown run by Faulk, and a field goal by Jeff Wilkins, helping St. Louis put the game away by the end of the third quarter.

One week later, the Rams advanced to the Super Bowl with a 29–24 win over the Philadelphia Eagles in the NFC Championship Game. Philadelphia managed to build a 17–13 halftime lead, but St. Louis scored 16 consecutive second half points (2 touchdown runs by Faulk and a Wilkins field goal) to earn the win, limiting the Eagles to only one touchdown pass in the second half. Warner finished the game with 22 of 33 pass completions for 212 yards and a touchdown, with no interceptions, while Faulk rushed for 159 yards and 2 touchdowns.

In the AFC Divisional Round, the Patriots defeated the Oakland Raiders 16–13 during a raging New England snowstorm in the last game ever played at Foxboro Stadium. The signature moment of the game was a controversial ruling by referee Walt Coleman in the fourth quarter that caused this game to be commonly known as the "Tuck Rule Game." While the Patriots possessed the ball, trailing the Raiders 13–10 with under two minutes left in regulation and no time outs, Brady was sacked by defensive back Charles Woodson, and appeared to fumble the ball. The fumble was recovered by Raiders linebacker Greg Biekert, presumably ending the game with a Raiders victory. After reviewing the play using instant replay, Coleman reversed the call on the field pursuant to the "tuck rule", where a loose ball is ruled an incomplete pass if lost while "tucking" the ball. Most of the controversy centered on whether Brady was still trying to tuck the ball away when he lost control. Brady then led his team to the Raiders 27-yard line, where kicker Adam Vinatieri made a 45-yard field goal which barely cleared the crossbar to send the game into overtime. The Patriots won the toss in overtime and won on another Vinatieri field goal from 23 yards; per the overtime rules in place at that time. Oakland's offense never regained possession.

In the AFC Championship Game, the Patriots traveled to Heinz Field to face the Pittsburgh Steelers, who were coming off a 27–10 win over the previous season's Super Bowl champion Baltimore Ravens. New England scored first with a 55-yard punt return touchdown by Brown, but in the second quarter, Brady was knocked out of the game with a sprained ankle. He was replaced by Bledsoe in Bledsoe's first game action since being injured in September. Upon entering the game, Bledsoe quickly moved the Patriots down the field and threw an 11-yard touchdown pass to Patten to give the Patriots a 14–3 halftime lead. Early in the second half, the Steelers moved from their own 32 to the New England 16, where they lined up for a field goal by Kris Brown. However, Brandon Mitchell blocked the kick, Brown picked up the ball at the 40 and ran 11 yards before lateraling to Antwan Harris, who took it 49 yards for a touchdown that made the score 21–3. But Pittsburgh scored two third-quarter touchdowns to make the score 21–17. The Patriots ended the comeback attempt by scoring a field goal in the fourth quarter and intercepting 2 passes from Steelers quarterback Kordell Stewart in the final 3 minutes of the game.

New Orleans had been preparing for Super Bowl XXXVI ever since the city was awarded the game on October 28, 1998 during the NFL's meetings in Kansas City, Missouri, beating out San Diego as host city. However, the September 11, 2001 terrorist attacks led the league to postpone its September 16 games and play them a week after the scheduled conclusion of the regular season. This caused the playoffs and Super Bowl to be delayed by one week. Rescheduling Super Bowl XXXVI from January 27 to February 3 proved extraordinarily difficult. In addition to rescheduling the game itself, all related events and activities had to be accommodated. This marked the first time in NFL history that the Super Bowl was played in February; all subsequent Super Bowls (excluding Super Bowl XXXVII in 2003) after that have been played in February.

Historically, the NFL made allowance for an open weekend between the Conference Championship games and the Super Bowl. However, there wasn't one scheduled for 2001, due to the NFL's decision beginning in the 1999 season to move the opening week of games to the weekend after Labor Day. Because the date of the Super Bowl had been set through 2003, the bye week prior to the Super Bowl did not return until 2004.

The NFL and New Orleans officials worked diligently to put together a deal to reschedule the game. The league considered a number of options, including shortening the regular season, shortening the playoffs, condensing the three playoff rounds in two weeks, and moving the game to the Rose Bowl in Pasadena, California. It was eventually decided to make every effort to maintain a full regular season and playoff, and push the Super Bowl back to February 3. Also, due to the Super Bowl being sent back a week, the first week of New Orleans Mardi Gras parades rolled one week earlier than normal.

One of the most significant logistical challenges was accommodating the National Automobile Dealers Association (NADA) Convention, which was originally slated to occupy the Superdome on February 3. On October 3, 2001, the NFL announced its intentions to hold the game on February 3, even though no agreement had been reached with NADA. Several weeks later, the three parties came to an accord in which the NADA agreed to move its convention date to the original Super Bowl week in exchange for financial and other considerations, including promotional spots shown during selected regular season NFL games. This agreement permitted the NFL to move the game back to February 3, and allowed for a full standard playoff tournament.
The original logo for Super Bowl XXXVI had a style that reflected the host city, and was distributed on some memorabilia items during 2001. However, after the 9/11 attacks, a new logo reflecting American patriotism was designed, featuring the shape of the 48 contiguous states and the American flag colors of red, white, and blue. Rob Tornoe of "The Philadelphia Inquirer" noted that it had "become one of the most iconic logos in Super Bowl history".

Janet Jackson was originally scheduled to perform during the Halftime Show, but allowed U2 to perform to tribute the events of September 11.

This was the final Super Bowl played on the first-generation AstroTurf surface. From 2000 to 2005, NFL stadiums phased out the short-pile AstroTurf in favor of natural grass or other, newer artificial surfaces which closely simulate grass, like FieldTurf.

Prior to Super Bowl XXXVI, Superdome officials considered installing natural grass for the game. The proposed installation method was comparable to what had been used at the Silverdome during the 1994 FIFA World Cup, and at Giants Stadium from 2000 to 2002. The plan called for large trays of grass to be grown and cultivated outdoors, then brought inside the dome and placed on the field for the game. In the end, cost and quality concerns prompted stadium and league officials to abandon the project.

The Rams entered as 14-point favorites. This was partly because Rams quarterback Kurt Warner statistically had his best year of his career, with a quarterback rating of 101.4, a 68.7 percent completion rate, and threw for 4,830 yards. Many had believed that the Patriots' Cinderella story was simply a fluke, especially after beating the veteran Oakland Raiders in a controversial playoff game in which a recovered fumble by the Raiders was reversed by the tuck rule.

There had been speculation on whether longtime starter Drew Bledsoe might start the game. As stated above, Bledsoe replaced an injured Brady against the Steelers in the AFC Championship game. Eventually, though, Brady was named starter.

The game was broadcast in the United States by Fox; the telecast was presented in a 480p enhanced-definition format marketed as "Fox Widescreen". While promoted as having higher quality than standard-definition, and being the first widescreen sports telecast on U.S. television to use a singular telecast for all viewers (rather than using a separate production exclusive to the widescreen feed), it was not true high definition, but still matched the aspect ratio of HDTV sets.

The game was called by play-by-play announcer Pat Summerall and color commentator John Madden. Pam Oliver and Ron Pitts served as sideline reporters. This was Summerall's 26th and final Super Bowl broadcast on television or radio. It was also the eighth and final Super Bowl telecast (and final NFL telecast of any kind) for the Summerall and Madden announcing team. The two had become the NFL's most famous broadcast duo since they were paired together in 1981 on CBS. After this game, Summerall retired from broadcasting and Madden moved to ABC. As a result, Madden was the first person to announce Super Bowls on different networks in consecutive years when he called Super Bowl XXXVII on ABC with Al Michaels.

James Brown hosted all the events with help from his fellow "Fox NFL Sunday" cast members Terry Bradshaw, Howie Long, and Cris Collinsworth. Jillian Barberie served as the weather and entertainment reporter during the pre-game show.

Memorable television commercials that aired during the game included Sony Pictures' trailer for "Spider-Man", Budweiser's "Picking a Card", and Super Bowl Ad Meter commercial of the year winners Bud Light "Satin Sheets." The best commercial of the year from Adbowl M&M's "Chocolate on our Pillow or Hotel Check In" and EA Sports' Madden NFL 2002 which aired during the game three days after Madden NFL 2002 start selling in Japan by Electronic Arts Square.

Before the game, an ensemble of singers featured Barry Manilow, Yolanda Adams, James Ingram, Wynonna Judd, and Patti LaBelle performing Manilow's song "Let Freedom Ring."

In a video segment, past and present NFL players read excerpts from the Declaration of Independence, which has become a part of all subsequent Super Bowls carried by Fox Sports. Super Bowls XXXIX, XLII, and XLV used different active and former players (and a player's widow) reading the Declaration for each version. Former U.S. presidents Gerald Ford, Jimmy Carter, George H. W. Bush, and Bill Clinton appeared in another videotaped segment and recited some of the speeches by Abraham Lincoln. Because Ronald Reagan had Alzheimer's disease, his wife Nancy appeared on the segment in place of him.

Singers Mary J. Blige and Marc Anthony, along with the Boston Pops Orchestra, performed "America the Beautiful". Paul McCartney then sang his post-9/11 song "Freedom". Afterwards, singer Mariah Carey, accompanied by the Boston Pops Orchestra, performed the national anthem.

George H. W. Bush became the first president, past or present, to participate in a Super Bowl coin toss in person (Ronald Reagan participated in the Super Bowl XIX coin toss via satellite from the White House in 1985). Bush was joined by former Dallas Cowboys Hall of Fame quarterback Roger Staubach. Staubach played at the United States Naval Academy and was the Most Valuable Player of Super Bowl VI, which was played 30 years prior at New Orleans' Tulane Stadium.

As was customary at the time, the Rams' individual offensive starters were introduced first, as the Rams were considered the visitors. However, when it came time to introduce the Patriots' starters, Pat Summerall, making the public address announcement, revealed that the Patriots chose "to be introduced as a team." According to David Halberstam's book, "The Education of a Coach", Belichick was given a choice by the NFL to introduce either the offense or defense. Belichick chose neither, asking that the team be introduced all at once in the spirit of unity. Although this was initially rejected by the NFL, Belichick held his ground and the NFL honored his request. The full team introduction demonstrated solidarity, and struck a chord with the audience in the wake of the 9/11 attacks. Since the next Super Bowl game, both Super Bowl participants have been introduced collectively as a team, a precedent which has continued.

The halftime show featured a three-song set from Irish rock band U2, who had just completed their successful Elevation Tour. After a rendition of "Beautiful Day", the band played "MLK" and "Where the Streets Have No Name" as the names of the victims from the September 11 attacks were projected onto a sheet behind the stage. While singing "Where the Streets Have No Name", the group's lead singer Bono replaced the lyrics "take shelter from the poison rain" with "dance in the Louisiana rain", "high on a desert plain" with "where there's no sorrow or pain", and the final line "it's all I can do" with "it's all we can do". At the conclusion of the song, Bono opened his jacket to reveal an American flag printed into the lining. U2's halftime show captivated the audience as a poignant tribute to those who had been lost in the attacks. In 2009, SI.com ranked it as the best halftime show in Super Bowl history, while it was rated the second-greatest by "Askmen.com".

Janet Jackson was originally selected to perform during the Halftime Show, but due to traveling concerns following the September 11 tragedies, the NFL opted for other acts. The NFL thought U2 would "set the right tone" in respect to the tragedy, after executives attended the band's Elevation Tour in New York City. She performed for the Super Bowl halftime two years later, when her highly controversial Super Bowl Halftime Show performance incident occurred.

The Rams scored first midway through the first quarter, with quarterback Kurt Warner completing 6-of-7 passes for 43 yards on a 48-yard, 10-play drive to set up a 50-yard field goal by kicker Jeff Wilkins. At the time, the field goal was the third longest in Super Bowl history. While the rest of the quarter was scoreless, the Patriots were stifling the typically high powered Rams offense by playing physical man coverage with the Rams receivers, forcing them into long drives that would end in punts or field goal attempts.

Early in the second quarter, the Rams drove to New England's 34-yard line, but Warner threw an incompletion on third down, and Wilkins' subsequent 52-yard field goal attempt sailed wide left.

With 8:49 left in the second quarter, a blitz by linebacker Mike Vrabel led Warner to be intercepted by Patriots defensive back Ty Law on a pass that was intended for wide receiver Isaac Bruce, Law then scored on a 47-yard return to give the Patriots a 7–3 lead. With less than two minutes left in the first half, Warner completed a pass to receiver Ricky Proehl at the Rams 40-yard line, but New England defensive back Antwan Harris tackled him, and forced a fumble which was recovered by Patriots defensive back Terrell Buckley. Patriots quarterback Tom Brady started off the Patriots drive with a 16-yard completion to Troy Brown and finished it with an 8-yard touchdown pass to receiver David Patten with 31 seconds left in the half. By halftime, New England owned a surprising 14–3 lead. It was the first time in the entire 2001 season that the Rams fell behind by more than eight points in a game.

The Patriots received the opening kickoff of the second half, but could only reach the St. Louis 43-yard line before being forced to punt. Aided by a 20-yard reception by wide receiver Az-Zahir Hakim, a 22-yard reception by Bruce, and a defensive pass interference penalty on Patriots defensive back Otis Smith, the Rams advanced to the New England 41-yard line. However, on the next play, Vrabel and defensive lineman Richard Seymour sacked Warner for a 9-yard loss. Warner then threw two consecutive incomplete passes, which resulted in the Rams punting.

Later in the third quarter, Smith intercepted a pass intended for Rams wide receiver Torry Holt after Holt slipped while coming off the line of scrimmage, and returned the ball 30 yards to the Rams 33-yard line. Though St. Louis' defense did not give up a touchdown to the Patriots, kicker Adam Vinatieri made a 37-yard field goal to increase New England's lead to 17–3.

The Rams responded by driving to the Patriots' 3-yard line on their ensuing drive. On fourth-and-goal, the Rams attempted to score a touchdown. Warner went back to pass and finding no one open scrambled to his right trying to run the ball in for a touchdown. Warner fumbled the ball while being tackled by linebacker Roman Phifer, which was recovered by defensive back Tebucky Jones who returned it 97 yards for a touchdown that would have increased the Patriots lead to 23–3. However, the play was nullified by a holding penalty on linebacker Willie McGinest, who illegally hugged Rams running back Marshall Faulk and prevented him from becoming an eligible receiver. This gave the Rams a first down on the 1-yard line. On second down, Warner scored on a 2-yard touchdown run to cut the Patriots' lead to 17–10.

After Warner's touchdown, the Rams defense forced the Patriots to a three-and-out. St. Louis then drove from own 7-yard line to the New England 36-yard line, aided by a 30-yard reception by Proehl. However, McGinest sacked Warner for a 16-yard loss on second down, pushing the Rams back to their 46-yard line. St. Louis punted after Warner's third down pass was incomplete.

The Rams forced New England to another three-and-out, and got the ball back on their own 45-yard line with 1:51 left in the game. Warner threw three consecutive completions: an 18-yard pass to Hakim, an 11-yard one to wide receiver Yo Murphy, and finally a 26-yard touchdown completion to Proehl that tied the game 17–17 with 1:30 left in the fourth quarter.
The Patriots had no timeouts left for their ensuing drive, which led Fox color commentator John Madden to initially suggest that the Patriots should run out the clock and attempt to win in overtime. Instead, New England attempted to get the winning score in regulation on the final drive. Bill Belichick conferred with offensive coordinator Charlie Weis and they agreed to go for it. Belichick later stated, "With a quarterback like Brady, going for the win is not that dangerous, because he's not going to make a mistake." Brady opened the drive with three dump-off completions to running back J. R. Redmond, who got out of bounds on the last one and moved the ball to their 41-yard line with 33 seconds left. At this point, Madden admitted on the air that he now liked what the Patriots were doing. After an incomplete pass, Brady completed a 23-yard pass underneath the Rams' zone defense to wide receiver Troy Brown—who also got out of bounds—and followed it up with a 6-yard completion to tight end Jermaine Wiggins to advance to the Rams' 30-yard line. Brady then spiked the ball with seven seconds left, which set up Vinatieri's 48-yard field goal attempt. Vinatieri, who had never missed a field goal indoors, made the kick as time ran out, marking the first time in Super Bowl history that a game was won by a score on the final play.

Warner finished the game with 28 completions out of 44 passes for 365 yards, 1 touchdown, and 2 interceptions, and rushed 3 times for 6 yards and a touchdown. Warner's 365 passing yards were the second highest total in Super Bowl history behind his own record of 414 yards set in Super Bowl XXXIV. Hakim was the top receiver of the game with 5 catches for 90 yards, and also rushed once for 5 yards. Faulk led the team with 76 rushing yards, and also caught 4 passes for 54 yards.

Patriots running back Antowain Smith was the top rusher of the game with 92 yards, and caught a pass for 4 yards. Troy Brown was the Patriots leading receiver with 6 catches for 89 yards. Brown also had a 15-yard kickoff return, and a 4-yard punt return, which gave him 108 total yards. Although the Rams outgained the Patriots 427–267 in total yards, New England forced three turnovers that were converted into 17 points. The Patriots, on the other hand, committed no turnovers.


Sources: NFL.com Super Bowl XXXVI, Super Bowl XXXVI Play Finder NE, Super Bowl XXXVI Play Finder StL

Completions/attempts
Carries
Long gain
Receptions
Times targeted

The following records were set in Super Bowl XXXVI, according to the official NFL.com boxscore, the 2016 NFL Record & Fact Book and the ProFootball reference.com game summary.

Records tied

Source:

Four hours after the game ended, Tom Brady visited Bill Belichick's hotel room where, as per team rules, he had to get his coach's permission to miss the team flight and instead travel to Walt Disney World in Orlando. Belichick gave him a perplexed look, and after a few seconds of dead silence, responded, "Of course you can go. How many times do you win the Super Bowl?"

The game heralded the Patriots dynasty, being the first of nine Super Bowl appearances under the duo of head coach Belichick and quarterback Brady. The Patriots finished the 2002 NFL season 9–7, missing the playoffs. But they went on to win Super Bowl XXXVIII, Super Bowl XXXIX, thus winning three Super Bowls in four years. Then, they won their fourth, fifth, and sixth Super Bowls (Super Bowl XLIX, Super Bowl LI, and Super Bowl LIII) a decade after their third. Brady also won three more Super Bowl MVP awards in Super Bowl XXXVIII, Super Bowl XLIX, and Super Bowl LI, making him the only player to be named Super Bowl MVP four times. Super Bowl XXXVI later became part of the wider 2007 New England Patriots videotaping controversy, also known as "Spygate". In addition to other videotaping allegations, the "Boston Herald" reported, citing an unnamed source, that the Patriots had also taped the Rams' walkthrough practice prior to the game. After further investigations, the league determined that no tape of the Rams' Super Bowl walkthrough was made, and the "Herald" later issued an apology in 2008 for their article about the alleged walkthrough tape. Nevertheless, the Patriots finished the 2007 regular season with a perfect 16–0 record, but failed to record an undefeated 19–0 championship season after losing Super Bowl XLII to the New York Giants. And at the conclusion of the 2015 NFL season, the Patriots held the NFL's best record since Spygate, compiling a 96–32 record from 2008 to 2015.

The Patriots' win in this Super Bowl, beyond just serving as a springboard to five more championships, also became the starting point for a decade of success in Boston sports, with the city's teams winning seven championships in the four major North American sports leagues (the NFL, the NBA, the NHL and MLB), including at least one in each league. Over the next fifteen years, in addition to the Patriots' five additional Super Bowls:

Following the Bruins winning the 2011 Stanley Cup Finals, "Boston Globe" columnist Dan Shaughnessy ranked all seven championships from the past decade and ranked the Patriots winning Super Bowl XXXVI as the second-greatest Boston sports championship of the decade behind only the Red Sox winning the 2004 World Series.

After the Patriots won their first championship in franchise history, it started a run of a team in American sports from NCAA and the four major sports winning their first (or next) franchise championship with a wait of 17 years or more between titles. This streak is still continuing in 2019 after the University of Virginia won their first NCAA Men's Basketball National Championship, the St. Louis Blues won their first Stanley Cup championship in their 52-year franchise history (which was coincidentally against another New England team, the Boston Bruins), the Toronto Raptors won their first NBA Championship in their 24-year franchise history, the Washington Nationals won their first World Series in their 15th season in Washington, D.C. and their 51st season overall, including their time as the Montreal Expos, and the Kansas City Chiefs won their first Super Bowl championship in 50 years.

Brady, Malloy and Vinatieri also provided the team's commentary in the 2001 Patriots' episode of "", narrated by actor Martin Sheen.

Beginning with the Rams' appearance in Super Bowl XXXVI, 10 different NFC teams appeared in the Super Bowl over the next 10 years. This trend was broken when the New York Giants earned a trip to Super Bowl XLVI after participating in Super Bowl XLII four years earlier (the Giants defeated the Patriots in both games).

This game was regarded as a "Super Bowl hangover" for the Rams because they lost a Super Bowl where they were heavy favorites, with a win potentially ushering in a Rams dynasty, but instead the loss signaled the beginning of the end of The Greatest Show on Turf era. Due to injuries to Kurt Warner and Marshall Faulk, the Rams finished with a 7–9 record the following year and missed the postseason. They qualified for the playoffs only two more times (2003 and 2004), and only won one more playoff game (the 2004–05 wild card game) during the remainder of their tenure in St. Louis. Head coach Mike Martz was fired after missing most of the 2005 season due to illness. Warner suffered a concussion on opening day in 2003, and was later demoted to backup quarterback for the rest of that season. He then signed with the New York Giants in 2004 as a caretaker quarterback, eventually losing the starting job to rookie quarterback Eli Manning. Warner later joined the Arizona Cardinals in 2005 and eventually gained the starting job where led that team to their first Super Bowl appearance, XLIII, following the 2008 season.

Super Bowl XXXVI ended up being the last Super Bowl that the Rams participated while based in St. Louis; they relocated back to Los Angeles in 2016. The Rams would once again reach the Super Bowl in the 2018 season.

The Patriots and the now Los Angeles Rams rematched 17 years later in Super Bowl LIII (2019); again featuring the head coach-quarterback tandem of Bill Belichick and Tom Brady who, in addition to Dante Scarnecchia, are the only active personnel left from Super Bowl XXXVI. Coincidentally like XXXVI, LIII also featured one of the league's top offenses (Rams) against one of the top defenses (Patriots). The Patriots won their record-tying sixth Super Bowl by defeating the Rams 13–3, with Brady remarking that LIII had a "throwback feel" to XXXVI.




</doc>
<doc id="29570" url="https://en.wikipedia.org/wiki?curid=29570" title="Scansano">
Scansano

Scansano is a town and comune, of medieval origin, in the province of Grosseto, Tuscany, central Italy. The area which Scansano lies within is called Maremma.

Scansano area is home to the production of Morellino di Scansano, a type of wine.

The municipality is formed by the municipal seat of Scansano and the villages ("frazioni") of Baccinello, Montorgiali, Murci, Pancole, Poggioferro, Polveraia, Pomonte and Preselle.

<BR>


</doc>
<doc id="29574" url="https://en.wikipedia.org/wiki?curid=29574" title="List of maritime explorers">
List of maritime explorers

This is a list of maritime explorers.

The list includes explorers which had contributed, and continue to contribute to human knowledge of the planet's geography, weather, biodiversity, human cultures, the expansion of trade, or established communication between diverse populations...



</doc>
<doc id="29577" url="https://en.wikipedia.org/wiki?curid=29577" title="Wednesday Morning, 3 A.M.">
Wednesday Morning, 3 A.M.

Wednesday Morning, 3 A.M. is the debut studio album by American folk rock duo Simon & Garfunkel. Following their early gig as "Tom and Jerry", Columbia Records signed the two in late 1963. It was produced by Tom Wilson and engineered by Roy Halee. The cover and the label include the subtitle "exciting new sounds in the folk tradition". Recorded in March 1964, the album was released on October 19.

The album was initially unsuccessful, so Paul Simon moved to London, England and finished his first solo album The Paul Simon Songbook. Art Garfunkel continued his studies at Columbia University in his native New York City, before reuniting with Simon in late 1965. "Wednesday Morning, 3 A.M." was re-released in January 1966 (to capitalize on their newly found radio success because of the overdubbing of the song "The Sound of Silence" in June 1965, adding electric guitars, bass guitar and a drum kit), and reached  30 on the Billboard 200. It was belatedly released in the UK two years later (in 1968) in both mono and stereo formats.

The song "He Was My Brother" was dedicated to Andrew Goodman, who was their friend and a classmate of Simon at Queens College. Andrew Goodman volunteered in Freedom Summer during 1964 and was abducted and killed in the murders of Chaney, Goodman, and Schwerner.

The album is included in its entirety as part of the Simon & Garfunkel box sets "Collected Works" and "The Columbia Studio Recordings (1964–1970)".

The album was produced by Tom Wilson and engineered by Roy Halee between March 10-31, 1964.

“Benedictus” was arranged and adapted from Orlando di Lasso's "Missa Octavi toni", a Renaissance setting of the ordinary of the mass. The text, in Latin, is "benedictus qui venit in nomine Domini" (KJV: "Blessed be he that cometh in the name of the Lord" ). The song is arranged for two voices with cello and sparse guitar accompaniment.

The album's cover photo was shot at the Fifth Avenue / 53rd Street subway station in New York City. In several concerts, Art Garfunkel related that during the photo session, several hundred pictures were taken that were unusable due to the "old familiar suggestion" on the wall in the background, which inspired Paul Simon to write the song "A Poem on the Underground Wall" for the duo's later "Parsley, Sage, Rosemary and Thyme" album.

The album was initially unsuccessful, having been released in the shadow of the British Invasion. This resulted in Paul Simon moving to England and Art Garfunkel continuing his studies at Columbia University in New York City. Following the success of "The Sound of Silence," the album peaked at  30 on the "Billboard" album chart in 1966.




</doc>
<doc id="29578" url="https://en.wikipedia.org/wiki?curid=29578" title="Sheldon Rampton">
Sheldon Rampton

Sheldon Rampton (born August 4, 1957) is an American editor and author. He was editor of "PR Watch", and is the author of several books that criticize the public relations industry and what he sees as other forms of corporate and government propaganda.

Rampton was born in Long Beach, California. At the age of one, his family moved to Las Vegas, Nevada, where his father worked as a musician. Raised as a member of The Church of Jesus Christ of Latter-day Saints (LDS Church), he spent two years in Japan as a Latter-day Saint missionary from 1976 to 1978. Upon returning to the United States, however, he left the LDS Church, influenced in part by Mormon feminist Sonia Johnson.

Upon graduation in 1982, Rampton worked as a newspaper reporter before becoming a peace activist. During the 1980s and 1990s, he worked closely with the Wisconsin Coordinating Council on Nicaragua (WCCN), which opposed the Reagan administration's military interventions in Central America and works to promote economic development, human rights, and mutual friendship between the people of the United States and Nicaragua. At WCCN, Rampton helped establish the Nicaraguan Credit Alternatives Fund (NICA Fund) in 1992, which channels loans from US investors to support microcredit and other "alternative credit" programs in Nicaragua.

In 1995, Rampton teamed with John Stauber as co-editors of PR Watch, a publication of the Center for Media and Democracy (CMD). They were described as liberal, and their writings are regarded by some members of the public relations industry as one-sided and hostile, but their work drew wide attention. ActivistCash, a website hosted by Washington lobbyist Richard Berman, has castigated them as "self-anointed watchdogs," "scare-mongers," "reckless" and "left-leaning." Rampton and Stauber have in turn argued that the ActivistCash critique contains a number of "demonstrably false" claims. According to a review in the Denver Post, their 1995 book, "Toxic Sludge Is Good for You," offered "a sardonic, wide-ranging look at the public relations industry."

Rampton is also a contributor to the Wikipedia open content project, and was the person who coined the name "Wikimedia" which later became the name of the foundation that manages Wikipedia and its sister projects. Inspired by Wikipedia's collaborative writing model, Rampton founded Disinfopedia (now known as SourceWatch), another CMD project, to complement his PR Watch work to expose what Rampton perceives as deceptive and misleading public relations campaigns.

After leaving the Center for Media and Democracy in 2009, Rampton became a website developer, joining an open government initiative led by New York State Senate chief information officer Andrew Hoppin. In 2010, Hoppin and Rampton co-founded NuCivic, an open source software company, which they sold in December 2014 to GovDelivery, a software services company now known as Granicus. Rampton currently works as a software engineer at Granicus.




</doc>
<doc id="29579" url="https://en.wikipedia.org/wiki?curid=29579" title="Miller test">
Miller test

The Miller test, also called the three-prong obscenity test, is the United States Supreme Court's test for determining whether speech or expression can be labeled obscene, in which case it is not protected by the First Amendment to the United States Constitution and can be prohibited.

The Miller test was developed in the 1973 case "Miller v. California". It has three parts:

The work is considered obscene only if "all three" conditions are satisfied.

The first two prongs of the Miller test are held to the standards of the community, and the last prong is held to what is reasonable to a person of the United States as a whole. The national reasonable person standard of the third prong acts as a check on the community standard of the first two prongs, allowing protection for works that in a certain community might be considered obscene but on a national level might have redeeming value.

For legal scholars, several issues are important. One is that the test allows for community standards rather than a national standard. What offends the average person in Manhattan, Kansas, may differ from what offends the average person in Manhattan, New York. The relevant community, however, is not defined.

Another important issue is that the Miller test asks for an interpretation of what the "average" person finds offensive, rather than what the more sensitive persons in the community are offended by, as obscenity was defined by the previous test, the Hicklin test, stemming from the English precedent.

In practice, pornography showing genitalia and sexual acts is not "ipso facto" obscene according to the Miller test. For instance, in 2000, a jury in Provo, Utah, took only a few minutes to clear Larry Peterman, owner of a Movie Buffs video store, in Utah County, Utah. He had been charged with distributing obscene material for renting pornographic videos which were displayed in a screened-off area of the store clearly marked as adult-only. The Utah County region had often boasted of being one of the most socially conservative areas in the United States. However, researchers had shown that guests at the local Marriott Hotel were disproportionately large consumers of pay-per-view pornographic material, accessing far more material than the store was distributing.

Because it allows for community standards and demands "serious" value, Justice Douglas worried in his dissent that this test would make it easier to suppress speech and expression. "Miller" replaced a previous test asking whether the speech or expression was "utterly without redeeming social value". As used, however, the test generally makes it difficult to outlaw any form of expression. Many works decried as pornographic have been successfully argued to have some artistic or literary value, most publicly in the context of the National Endowment for the Arts in the 1990s.

The advent of the Internet has made the "community standards" part of the test even more difficult to judge; as material published on a web server in one place can be read by a person residing anywhere else, there is a question as to which jurisdiction should apply. In "United States of America v. Extreme Associates", a pornography distributor from North Hollywood, California, was judged to be held accountable to the community standards applying in western Pennsylvania, where the Third Circuit made its ruling, because the materials were available via Internet in that area. The United States Court of Appeals for the Ninth Circuit has ruled in "United States v. Kilbride" that a "national community standard" should be used for the Internet, but this has yet to be upheld at the national level.



</doc>
<doc id="29580" url="https://en.wikipedia.org/wiki?curid=29580" title="Set-top box">
Set-top box

A set-top box (STB), also colloquially known as a cable box, is an information appliance device that generally contains a TV-tuner input and displays output to a television set and an external source of signal, turning the source signal into content in a form that can then be displayed on the television screen or other display device. They are used in cable television, satellite television, and over-the-air television systems as well as other uses.

According to the "Los Angeles Times", the cost to a cable provider in the United States for a set-top box is between $150 for a basic box to $250 for a more sophisticated box. In 2016, the average pay-TV subscriber paid $231 per year to lease their set-top box from a cable service provider.

The signal source might be an Ethernet cable, a satellite dish, a coaxial cable (see cable television), a telephone line (including DSL connections), broadband over power lines (BPL), or even an ordinary VHF or UHF antenna. Content, in this context, could mean any or all of video, audio, Internet web pages, interactive video games, or other possibilities. Satellite and microwave-based services also require specific external receiver hardware, so the use of set-top boxes of various formats has never completely disappeared. Set-top boxes can also enhance source signal quality.

Before the All-Channel Receiver Act of 1962 required US television receivers to be able to tune the entire VHF and UHF range (which in North America was NTSC-M channels 2 through 83 on 54 to 890 MHz), a set-top box known as a UHF converter would be installed at the receiver to shift a portion of the UHF-TV spectrum onto low-VHF channels for viewing. As some 1960s-era 12-channel TV sets remained in use for many years, and Canada and Mexico were slower than the US to require UHF tuners to be factory-installed in new TVs, a market for these converters continued to exist for much of the 1970s.

Cable television represented a possible alternative to deployment of UHF converters as broadcasts could be frequency-shifted to VHF channels at the cable head-end instead of the final viewing location. However, most cable systems could not accommodate the full 54-890 MHz VHF/UHF frequency range and the twelve channels of VHF space were quickly exhausted on most systems. Adding any additional channels therefore needed to be done by inserting the extra signals into cable systems on nonstandard frequencies, typically either below VHF channel 7 (midband) or directly above VHF channel 13 (superband).

These frequencies corresponded to non-television services (such as two-way radio) over-the-air and were therefore not on standard TV receivers. Before cable-ready TV sets became common in the late 1980s, an electronic tuning device called a cable converter box was needed to receive the additional analog cable TV channels and transpose or convert the selected channel to analog radio frequency (RF) for viewing on a regular TV set on a single channel, usually VHF channel 3 or 4. The box allowed an analog non-cable-ready television set to receive analog encrypted cable channels and was a prototype topology for later date digital encryption devices. Newer televisions were then converted to be analog cypher cable-ready, with the standard converter built-in for selling premium television (aka pay per view). Several years later and slowly marketed, the advent of digital cable continued and increased the need for various forms of these devices. Block conversion of the entire affected frequency band onto UHF, while less common, was used by some models to provide full VCR compatibility and the ability to drive multiple TV sets, albeit with a somewhat nonstandard channel numbering scheme.

Newer television receivers greatly reduced the need for external set-top boxes, although cable converter boxes continue to be used to descramble premium cable channels according to carrier-controlled access restrictions, and to receive digital cable channels, along with using interactive services like video on demand, pay per view, and home shopping through television.

Set-top boxes were also made to enable closed captioning on older sets in North America, before this became a mandated inclusion in new TV sets. Some have also been produced to mute the audio (or replace it with noise) when profanity is detected in the captioning, where the offensive word is also blocked. Some also include a V-chip that allows only programs of some television content ratings. A function that limits children's time watching TV or playing video games may also be built in, though some of these work on main electricity rather than the video signal.

The transition to digital terrestrial television after the turn of the millennium left many existing television receivers unable to tune and display the new signal directly. In the United States, where analog shutdown was completed in 2009 for full-service broadcasters, a federal subsidy was offered for coupon-eligible converter boxes with deliberately limited capability which would restore signals lost to digital transition.

Professional set-top boxes are referred to as IRDs or integrated receiver/decoders in the professional broadcast audio/video industry. They are designed for more robust field handling and rack mounting environments. IRDs are capable of outputting uncompressed serial digital interface signals, unlike consumer STBs which usually don't, mostly because of copyright reasons.

Hybrid set-top boxes, such as those used for Smart TV programming, enable viewers to access multiple TV delivery methods (including terrestrial, cable, internet, and satellite); like IPTV boxes, they include video on demand, time-shifting TV, Internet applications, video telephony, surveillance, gaming, shopping, TV-centric electronic program guides, and e-government. By integrating varying delivery streams, hybrids (sometimes known as "TV-centric") enable pay-TV operators more flexible application deployment, which decreases the cost of launching new services, increases speed to market, and limits disruption for consumers.

As examples, Hybrid Broadcast Broadband TV (HbbTV) set-top boxes allow traditional TV broadcasts, whether from terrestrial (DTT), satellite, or cable providers, to be brought together with video delivered over the Internet and personal multimedia content. Advanced Digital Broadcast (ADB) launched its first hybrid DTT/IPTV set-top box in 2005, which provided Telefónica with the digital TV platform for its Movistar TV service by the end of that year. In 2009, ADB provided Europe's first three-way hybrid digital TV platform to Polish digital satellite operator n, which enables subscribers to view integrated content whether delivered via satellite, terrestrial, or internet.

UK based Inview Technology has over 8M STBs deployed in the UK for Teletext and an original push VOD service for Top Up TV.

In IPTV networks, the set-top box is a small computer providing two-way communications on an IP network and decoding the video streaming media. IP set-top boxes have a built-in home network interface that can be Ethernet, Wireless (802.11 g,n,ac), or one of the existing wire home networking technologies such as HomePNA or the ITU-T G.hn standard, which provides a way to create a high-speed (up to 1Gbit/s) local area network using existing home wiring (power lines, phone lines, and coaxial cables).

In the US and Europe, telephone companies use IPTV (often on ADSL or optical fiber networks) as a means to compete with traditional local cable television monopolies.

This type of service is distinct from Internet television, which involves third-party content over the public Internet not controlled by the local system operator.

Electronic program guides and interactive program guides provide users of television, radio, and other media applications with continuously updated menus displaying broadcast programming or scheduling information for current and upcoming programming. Some guides, such as ITV, also feature backward scrolling to promote their catch-up content.

This feature allows the user to choose preferred channels, making them easier and quicker to access; this is handy with the wide range of digital channels on offer. The concept of favourite channels is superficially similar to that of the "bookmark" function offered in many Web browsers.

The timer allows the user to program and enable the box to switch between channels at certain times: this is handy to record from more than one channel while the user is out. The user still needs to program the VCR or DVD recorder.

Some models have controls on the box, as well as on the remote control. This is useful should the user lose the remote or if the batteries age.

Some remote controls can also control some basic functions of various brands of TVs. This allows the user to use just one remote to turn the TV on and off, adjust volume, or switch between digital and analog TV channels or between terrestrial and internet channels.

The parental lock or content filters allow users over 18 years old to block access to channels that are not appropriate for children, using a personal identification number. Some boxes simply block all channels, while others allow the user to restrict access to chosen channels not suitable for children below certain ages.

As complexity and potential programming faults of the set-top box increase, software such as MythTV, Select-TV and Microsoft's Media Center have developed features comparable to those of set-top boxes, ranging from basic DVR-like functionality to DVD copying, home automation, and housewide music or video playback.

Almost all modern set-top boxes feature automatic firmware update processes. The firmware update is typically provided by the service provider.

With the advent of flat-panel televisions, set-top boxes are now deeper in profile than the tops of most modern TV sets. Because of this, set-top boxes are often placed beneath televisions, and the term set-top box has become something of a misnomer, possibly helping the adoption of the term "digibox". Additionally, newer set-top boxes that sit at the edge of IP-based distribution networks are often called net-top boxes or NTBs, to differentiate between IP and RF inputs. The Roku LT is around the size of a pack of cards and delivers Smart TV to conventional sets.

The distinction between external tuner or demodulator boxes (traditionally considered to be "set-top boxes") and storage devices (such as VCR, DVD, or disc-based PVR units) is also blurred by the increasing deployment of satellite and cable tuner boxes with hard disk, network or USB interfaces built-in.

Devices with the capabilities of computer terminals, such as the WebTV thin client, also fall into the grey area that could invite the term "NTB".

In Europe, a set-top box does not necessarily contain a tuner of its own. A box connected to a television (or VCR) SCART connector is fed with the baseband television signal from the set's tuner, and can have the television display the returned processed signal instead.
This SCART feature had been used for connection to analogue decoding equipment by pay TV operators in Europe, and in the past was used for connection to teletext equipment before the decoders became built-in. The outgoing signal could be of the same nature as the incoming signal, or RGB component video, or even an "insert" over the original signal, due to the "fast switching" feature of SCART.

In case of analogue pay-TV, this approach avoided the need for a second remote control. The use of digital television signals in more modern pay-TV schemes requires that decoding take place before the digital-to-analogue conversion step, rendering the video outputs of an analogue SCART connector no longer suitable for interconnection to decryption hardware. Standards such as DVB's Common Interface and ATSC's CableCARD therefore use a PCMCIA-like card inserted as part of the digital signal path as their alternative to a tuner-equipped set-top box.

In June 2011 a report from the American National Resources Defense Council brought attention to the energy efficiency of set-top boxes, and the US Department of Energy announced plans to consider the adoption of energy efficiency standards for set-top boxes. In November 2011, the National Cable & Telecommunications Association announced a new energy efficiency initiative that commits the largest American cable operators to the purchase of set-top boxes that meet Energy Star standards and the development of sleep modes that will use less energy when the set-top box is not being used to watch or record video.




</doc>
<doc id="29582" url="https://en.wikipedia.org/wiki?curid=29582" title="Scatology">
Scatology

In medicine and biology, scatology or coprology is the study of feces.

Scatological studies allow one to determine a wide range of biological information about a creature, including its diet (and thus where it has been), health and diseases such as tapeworms.

A comprehensive study of scatology was documented by John Gregory Bourke under the title " Rites of All Nations" (1891). An abbreviated version of the work (with a foreword by Sigmund Freud), was published as "The Portable Scatalog" in 1994.

The word derives from the Greek ( ) meaning "dung, feces"; "coprology" derives from the Greek of similar meaning.

In psychology, a scatology is an obsession with excretion or excrement, or the study of such obsessions.

In sexual fetishism, scatology (usually abbreviated "scat") refers to coprophilia, when a person is sexually aroused by fecal matter, whether in the use of feces in various sexual acts, watching someone defecating, or simply seeing the feces. Entire subcultures in sexuality are devoted to this fetish.

In literature, "scatological" is a term to denote the literary trope of the grotesque body. It is used to describe works that make particular reference to excretion or excrement, as well as to toilet humor. Well known for his scatological tropes is the late medieval fictional character of Till Eulenspiegel. Another common example is John Dryden's "Mac Flecknoe", a poem that employs extensive scatological imagery to ridicule Dryden's contemporary Thomas Shadwell. In German literature in particular is a wealth of scatological texts and references, which includes such books as Collofino's "Non Olet". A case which has provoked an unusual amount of comment in the academic literature is Mozart's scatological humour. Smith, in his review of English literature's representations of scatology from the Middle Ages to the 18th century, notes two attitudes towards scatology. One of these emphasises the merry and the carnivalesque. This is found in Chaucer and Shakespeare. The other attitude is one of self-disgust and misanthropy. This is found in the works of the Earl of Rochester and Jonathan Swift.




</doc>
<doc id="29586" url="https://en.wikipedia.org/wiki?curid=29586" title="Σ-algebra">
Σ-algebra

In mathematical analysis and in probability theory, a σ-algebra (also σ-field) on a set "X" is a collection Σ of subsets of "X" that
includes "X" itself, is closed under complement, and is closed under countable unions.

The definition implies that it also includes 
the empty subset and that it is closed under countable intersections.
The pair ("X", Σ) is called a measurable space or Borel space.

A σ-algebra is a type of algebra of sets. An algebra of sets needs only to be closed under the union or intersection of "finitely" many subsets, which is a weaker condition.

The main use of σ-algebras is in the definition of measures; specifically, the collection of those subsets for which a given measure is defined is necessarily a σ-algebra. This concept is important in mathematical analysis as the foundation for Lebesgue integration, and in probability theory, where it is interpreted as the collection of events which can be assigned probabilities. Also, in probability, σ-algebras are pivotal in the definition of conditional expectation.

In statistics, (sub) σ-algebras are needed for the formal mathematical definition of a sufficient statistic, particularly when the statistic is a function or a random process and the notion of conditional density is not applicable.

If one possible σ-algebra on "X" is where ∅ is the empty set. In general, a finite algebra is always a σ-algebra.

If {"A", "A", "A", …} is a countable partition of "X" then the collection of all unions of sets in the partition (including the empty set) is a σ-algebra.

A more useful example is the set of subsets of the real line formed by starting with all open intervals and adding in all countable unions, countable intersections, and relative complements and continuing this process (by transfinite iteration through all countable ordinals) until the relevant closure properties are achieved - the σ-algebra produced by this process is known as the Borel algebra on the real line, and can also be conceived as the smallest (i.e. "coarsest") σ-algebra containing all the open sets, or equivalently containing all the closed sets. It is foundational to measure theory, and therefore modern probability theory, and a related construction known as the Borel hierarchy is of relevance to descriptive set theory.

There are at least three key motivators for σ-algebras: defining measures, manipulating limits of sets, and managing partial information characterized by sets.

A measure on "X" is a function that assigns a non-negative real number to subsets of "X"; this can be thought of as making precise a notion of "size" or "volume" for sets. We want the size of the union of disjoint sets to be the sum of their individual sizes, even for an infinite sequence of disjoint sets.

One would like to assign a size to "every" subset of "X", but in many natural settings, this is not possible. For example, the axiom of choice implies that, when the size under consideration is the ordinary notion of length for subsets of the real line, then there exist sets for which no size exists, for example, the Vitali sets. For this reason, one considers instead a smaller collection of privileged subsets of "X". These subsets will be called the measurable sets. They are closed under operations that one would expect for measurable sets; that is, the complement of a measurable set is a measurable set and the countable union of measurable sets is a measurable set. Non-empty collections of sets with these properties are called σ-algebras.

Many uses of measure, such as the probability concept of almost sure convergence, involve limits of sequences of sets. For this, closure under countable unions and intersections is paramount. Set limits are defined as follows on σ-algebras.

In much of probability, especially when conditional expectation is involved, one is concerned with sets that represent only part of all the possible information that can be observed. This partial information can be characterized with a smaller σ-algebra which is a subset of the principal σ-algebra; it consists of the collection of subsets relevant only to and determined only by the partial information. A simple example suffices to illustrate this idea.

Imagine you and another person are betting on a game that involves flipping a coin repeatedly and observing whether it comes up Heads ("H") or Tails ("T"). Since you and your opponent are each infinitely wealthy, there is no limit to how long the game can last. This means the sample space Ω must consist of all possible infinite sequences of "H" or "T":

However, after "n" flips of the coin, you may want to determine or revise your betting strategy in advance of the next flip. The observed information at that point can be described in terms of the 2 possibilities for the first "n" flips. Formally, since you need to use subsets of Ω, this is codified as the σ-algebra

Observe that then

where formula_8 is the smallest σ-algebra containing all the others.

Let "X" be some set, and let formula_9 represent its power set. Then a subset formula_10 is called a "σ"-algebra if it satisfies the following three properties:


From these properties, it follows that the σ-algebra is also closed under countable intersections (by applying De Morgan's laws).

It also follows that the empty set ∅ is in Σ, since by (1) "X" is in Σ and (2) asserts that its complement, the empty set, is also in Σ. Moreover, since } satisfies condition (3) as well, it follows that } is the smallest possible σ-algebra on "X". The largest possible σ-algebra on "X" is 2:=formula_9.

Elements of the "σ"-algebra are called measurable sets. An ordered pair , where "X" is a set and Σ is a "σ"-algebra over "X", is called a measurable space. A function between two measurable spaces is called a measurable function if the preimage of every measurable set is measurable. The collection of measurable spaces forms a category, with the measurable functions as morphisms. Measures are defined as certain types of functions from a "σ"-algebra to [0, ∞].

A σ-algebra is both a π-system and a Dynkin system (λ-system). The converse is true as well, by Dynkin's theorem (below).

This theorem (or the related monotone class theorem) is an essential tool for proving many results about properties of specific σ-algebras. It capitalizes on the nature of two simpler classes of sets, namely the following.

Dynkin's π-λ theorem says, if "P" is a π-system and "D" is a Dynkin system that contains "P" then the σ-algebra σ("P") generated by "P" is contained in "D". Since certain π-systems are relatively simple classes, it may not be hard to verify that all sets in "P" enjoy the property under consideration while, on the other hand, showing that the collection "D" of all subsets with the property is a Dynkin system can also be straightforward. Dynkin's π-λ Theorem then implies that all sets in σ("P") enjoy the property, avoiding the task of checking it for an arbitrary set in σ("P").

One of the most fundamental uses of the π-λ theorem is to show equivalence of separately defined measures or integrals. For example, it is used to equate a probability for a random variable "X" with the Lebesgue-Stieltjes integral typically associated with computing the probability:

where "F"("x") is the cumulative distribution function for "X", defined on R, while formula_13 is a probability measure, defined on a σ-algebra Σ of subsets of some sample space Ω.

Suppose formula_14 is a collection of σ-algebras on a space "X".



Suppose "Y" is a subset of "X" and let ("X", Σ) be a measurable space.

A "σ"-algebra Σ is just a "σ"-ring that contains the universal set "X". A "σ"-ring need not be a "σ"-algebra, as for example measurable subsets of zero Lebesgue measure in the real line are a "σ"-ring, but not a "σ"-algebra since the real line has infinite measure and thus cannot be obtained by their countable union. If, instead of zero measure, one takes measurable subsets of finite Lebesgue measure, those are a ring but not a "σ"-ring, since the real line can be obtained by their countable union yet its measure is not finite.

"σ"-algebras are sometimes denoted using calligraphic capital letters, or the Fraktur typeface. Thus may be denoted as formula_23 or formula_24.

A separable σ-algebra (or separable σ-field) is a σ-algebra formula_25 that is a separable space when considered as a metric space with metric formula_26 for formula_27 and a given measure formula_28 (and with formula_29 being the symmetric difference operator). Note that any σ-algebra generated by a countable collection of sets is separable, but the converse need not hold. For example, the Lebesgue σ-algebra is separable (since every Lebesgue measurable set is equivalent to some Borel set) but not countably generated (since its cardinality is higher than continuum).

A separable measure space has a natural pseudometric that renders it separable as a pseudometric space. The distance between two sets is defined as the measure of the symmetric difference of the two sets. Note that the symmetric difference of two distinct sets can have measure zero; hence the pseudometric as defined above need not to be a true metric. However, if sets whose symmetric difference has measure zero are identified into a single equivalence class, the resulting quotient set can be properly metrized by the induced metric. If the measure space is separable, it can be shown that the corresponding metric space is, too.

Let "X" be any set.

A stopping time formula_30 can define a formula_31-algebra formula_32, the
so-called formula_33-Algebra of τ-past, which in a filtered probability space describes the information up to the random time formula_30 in the sense that, if the filtered probability space is interpreted as a random experiment, the maximum information that can be found out about the experiment from arbitrarily often repeating it until the time formula_30 is formula_32.

Let "F" be an arbitrary family of subsets of "X". Then there exists a unique smallest σ-algebra which contains every set in "F" (even though "F" may or may not itself be a σ-algebra). It is, in fact, the intersection of all σ-algebras containing "F". (See intersections of σ-algebras above.) This σ-algebra is denoted σ("F") and is called the σ-algebra generated by "F".

Then σ("F") consists of all the subsets of "X" that can be made from elements of "F" by a countable number of complement, union and intersection operations. If "F" is empty, then σ("F") = }, since an empty union and intersection produce the empty set and universal set, respectively.

For a simple example, consider the set "X" = {1, 2, 3}. Then the σ-algebra generated by the single subset {1} is </nowiki>}}. By an abuse of notation, when a collection of subsets contains only one element, "A", one may write σ("A") instead of σ({"A"}) if it is clear that "A" is a subset of "X"; in the prior example σ({1}) instead of σ(<nowiki></nowiki>). Indeed, using to mean is also quite common.

There are many families of subsets that generate useful σ-algebras. Some of these are presented here.

If "f" is a function from a set "X" to a set "Y" and "B" is a σ-algebra of subsets of "Y", then the σ-algebra generated by the function "f", denoted by σ("f"), is the collection of all inverse images "f"("S") of the sets "S" in "B". i.e.

A function "f" from a set "X" to a set "Y" is measurable with respect to a σ-algebra Σ of subsets of "X" if and only if σ("f") is a subset of Σ.

One common situation, and understood by default if "B" is not specified explicitly, is when "Y" is a metric or topological space and "B" is the collection of Borel sets on "Y".

If "f" is a function from "X" to R then σ("f") is generated by the family of subsets which are inverse images of intervals/rectangles in R:

A useful property is the following. Assume "f" is a measurable map from ("X", Σ) to ("S", Σ) and "g" is a measurable map from ("X", Σ) to ("T", Σ). If there exists a measurable map "h" from ("T", Σ) to ("S", Σ) such that "f"("x") = "h"("g"("x")) for all "x", then σ("f") ⊂ σ("g"). If "S" is finite or countably infinite or, more generally, ("S", Σ) is a standard Borel space (e.g., a separable complete metric space with its associated Borel sets), then the converse is also true. Examples of standard Borel spaces include R with its Borel sets and R with the cylinder σ-algebra described below.

An important example is the Borel algebra over any topological space: the σ-algebra generated by the open sets (or, equivalently, by the closed sets). Note that this σ-algebra is not, in general, the whole power set. For a non-trivial example that is not a Borel set, see the Vitali set or Non-Borel sets.

On the Euclidean space R, another σ-algebra is of importance: that of all Lebesgue measurable sets. This σ-algebra contains more sets than the Borel σ-algebra on R and is preferred in integration theory, as it gives a complete measure space.

Let formula_39 and formula_40 be two measurable spaces. The σ-algebra for the corresponding product space formula_41 is called the product σ-algebra and is defined by

Observe that formula_43 is a π-system.

The Borel σ-algebra for R is generated by half-infinite rectangles and by finite rectangles. For example,

For each of these two examples, the generating family is a π-system.

Suppose

is a set of real-valued functions on formula_46. Let formula_47 denote the Borel subsets of R. For each formula_48 and formula_49 a cylinder subset of is a finitely restricted set defined as

For each formula_48,

is a π-system that generates a σ-algebra formula_53. Then the family of subsets

is an algebra that generates the cylinder σ-algebra for . This σ-algebra is a subalgebra of the Borel σ-algebra determined by the product topology of formula_55 restricted to .

An important special case is when formula_46 is the set of natural numbers and is a set of real-valued sequences. In this case, it suffices to consider the cylinder sets

for which

is a non-decreasing sequence of σ-algebras.

Suppose formula_59 is a probability space. If formula_60 is measurable with respect to the Borel σ-algebra on R then is called a random variable ("n = 1") or random vector ("n" > 1). The σ-algebra generated by is

Suppose formula_59 is a probability space and formula_63 is the set of real-valued functions on formula_46. If formula_65 is measurable with respect to the cylinder σ-algebra formula_66 (see above) for , then is called a stochastic process or random process. The σ-algebra generated by is

the σ-algebra generated by the inverse images of cylinder sets.




</doc>
