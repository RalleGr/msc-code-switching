<doc id="28134" url="https://en.wikipedia.org/wiki?curid=28134" title="Second Vatican Council">
Second Vatican Council

The Second Ecumenical Council of the Vatican, commonly known as the , , or V2, addressed relations between the Catholic Church and the modern world. The Council, through the Holy See, was formally opened under the pontificate of Pope John XXIII on 11 October 1962 and was closed under Pope Paul VI on the Solemnity of the Immaculate Conception on 8 December 1965. 

Several changes resulted from the Council, including the renewal of consecrated life with a revised charism, ecumenical efforts towards dialogue with other religions, and the universal call to holiness, which according to Pope Paul VI was "the most characteristic and ultimate purpose of the teachings of the Council".

According to Pope Benedict XVI, the most important and essential message of the Council is "the Paschal Mystery as the center of what it is to be Christian and therefore of the Christian life, the Christian year, the Christian seasons". Other changes which followed the Council included the widespread use of vernacular languages in the Mass instead of Latin, the subtle disuse of ornate clerical regalia, the revision of Eucharistic (liturgical) prayers, the abbreviation of the liturgical calendar, the ability to celebrate the Mass "versus populum" (with the officiant facing the congregation), as well as "ad orientem" (facing the "East" and the Crucifix), and modern aesthetic changes encompassing contemporary Catholic liturgical music and artwork. Many of these changes remain divisive among the Catholic faithful.

Of those who took part in the Council's opening session, four have become popes: Cardinal Giovanni Battista Montini, who on succeeding John XXIII took the name Pope Paul VI; Bishop Albino Luciani, the future Pope John Paul I; Bishop Karol Wojtyła, who became Pope John Paul II; and Father Joseph Ratzinger, present as a theological consultant, who became Pope Benedict XVI.

In the 1950s, theological and biblical studies in the Catholic Church had begun to sway away from the Neo-Scholasticism and biblical literalism which a reaction to Catholic modernism had enforced since the First Vatican Council. This shift could be seen in theologians such as Karl Rahner and John Courtney Murray who, following Pope John XXIII's call for "aggiornamento", looked to integrate modern human experience with church principles based on Jesus Christ, as well as in others such as Yves Congar, Henri de Lubac, and Joseph Ratzinger who looked to an accurate understanding of scripture and the early Church Fathers as a source of renewal ("ressourcement").

At the same time, the world's bishops faced challenges driven by political, social, economic, and technological change. Some of these bishops sought new ways of addressing those challenges. The First Vatican Council had been held nearly a century before but had been cut short in 1870 when the Italian Army entered the city of Rome at the end of Italian unification. As a result, only deliberations on the role of the papacy and the congruent relationship of faith and reason were completed, with the role of the bishops and laity in the Church left unaddressed.

Pope John XXIII gave notice of his intention to convene the Council on 25 January 1959, less than three months after his election in October 1958. This sudden announcement, which caught the Curia by surprise, caused little initial official comment from Church insiders. Reaction to the announcement was widespread and largely positive from both religious and secular leaders outside the Catholic Church, and the Council was formally summoned by the apostolic constitution "Humanae Salutis" on 25 December 1961. In various discussions before the Council convened, John XXIII said that it was time to "open the windows [of the Church] and let in some fresh air". He invited other Christians outside the Catholic Church to send observers to the Council. Acceptances came from both the Eastern Orthodox Church and Protestant denominations as internal observers, but these observers did not cast votes in the approbation of the conciliar documents.

Pope John XXIII's announcement on 25 January 1959, in the chapter hall of the Benedictine monastery attached to the Basilica of Saint Paul Outside the Walls in Rome, of his intention to call a general council came as a surprise even to the cardinals present. The Pontiff pre-announced the council under a full moon when the faithful with their candlelights gathered in St. Peter's square and jokingly noted about the brightness of the moon.

He had tested the idea only ten days before with one of them, his Cardinal Secretary of State Domenico Tardini, who gave enthusiastic support to the idea. Although the Pope later said the idea came to him in a flash in his conversation with Tardini, two cardinals had earlier attempted to interest him in the idea. They were two of the most conservative, Ernesto Ruffini and Alfredo Ottaviani, who had already in 1948 proposed the idea to Pope Pius XII and who put it before John XXIII on 27 October 1958.

Actual preparations for the Council took more than two years, and included work from 10 specialised commissions, people for mass media and Christian Unity, and a Central Preparatory Commission with 120 members for overall coordination, composed mostly of members of the Roman Curia. At Vatican I, 737 attended, mostly from Europe. Attendance at Vatican II varied in later sessions from 2,100 to over 2,300. In addition, a varying number of "periti" ("experts") were available for theological consultation—a group that turned out to have a major influence as the Council went forward. Seventeen Orthodox Churches and Protestant denominations sent observers. More than three dozen representatives of other Christian communities were present at the opening session, and the number grew to nearly 100 by the end of the 4th Council Sessions.

Pope John XXIII opened the Council on 11 October 1962 in a public session at St. Peter's basilica in Vatican City and read the declaration "Gaudet Mater Ecclesia" before the Council Fathers.

What is needed at the present time is a new enthusiasm, a new joy and serenity of mind in the unreserved acceptance by all of the entire Christian faith, without forfeiting that accuracy and precision in its presentation which characterized the proceedings of the Council of Trent and the First Vatican Council. What is needed, and what everyone imbued with a truly Christian, Catholic and apostolic spirit craves today, is that this doctrine shall be more widely known, more deeply understood, and more penetrating in its effects on men's moral lives. What is needed is that this certain and immutable doctrine, to which the faithful owe obedience, be studied afresh and reformulated in contemporary terms. For this deposit of faith, or truths which are contained in our time-honored teaching is one thing; the manner in which these truths are set forth (with their meaning preserved intact) is something else. ()

The first working session of the Council was on 13 October 1962. That day's agenda included the election of members of the ten conciliar commissions. Each commission would have sixteen elected and eight appointed members, and they were expected to do most of the work of the Council. It had been expected that the members of the preparatory commissions, where the Curia was heavily represented, would be confirmed as the majorities on the conciliar commissions. But senior French Cardinal Achille Liénart addressed the Council, saying that the bishops could not intelligently vote for strangers. He asked that the vote be postponed to give all the bishops a chance to draw up their own lists. German Cardinal Josef Frings seconded that proposal, and the vote was postponed. The first meeting of the Council adjourned after only fifteen minutes.

The bishops met to discuss the membership of the commissions, along with other issues, both in national and regional groups, as well as in gatherings that were more informal. The "schemata" (Latin for drafts) from the preparatory sessions were rejected and new ones were created. When the Council met on 16 October 1962, a new slate of commission members was presented and approved by the Council. One important change was a significant increase in membership from Central and Northern Europe, beyond countries such as Spain or Italy. More than 100 bishops from Africa, Asia, and Latin America were Dutch or Belgian and tended to associate with the bishops from those countries. These groups were led by Cardinals Bernardus Johannes Alfrink of the Netherlands and Leo Suenens of Belgium.

Eleven commissions and three secretariats were established, with their respective presidents:


After adjournment on 8 December, work began on preparations for the sessions scheduled for 1963. These preparations, however, were halted upon the death of Pope John XXIII on 3 June 1963, since a Catholic ecumenical council is automatically interrupted and suspended upon the death of the pope who convened it, until the next pope orders the council to be continued or dissolved. Pope Paul VI was elected on 21 June 1963 and immediately announced that the Council would continue.

In the months prior to the second session, Pope Paul VI worked to correct some of the problems of organization and procedure that had been discovered during the first session. The changes included inviting additional lay Catholic and non-Catholic observers, reducing the number of proposed schemata to seventeen (which were made more general, in keeping with the pastoral nature of the Council) and later eliminating the requirement of secrecy surrounding general sessions.

Pope Paul's opening address on 29 September 1963 stressed the pastoral nature of the Council, and set out four purposes for it:

During this second session, the bishops approved the constitution on the liturgy, "Sacrosanctum Concilium", and the decree on social communication, "Inter mirifica". Work went forward with the schemata on the Church, bishops and dioceses, and on ecumenism. On 8 November 1963, Josef Frings criticized the Holy Office, and drew an articulate and impassioned defense by its Secretary, Alfredo Ottaviani, in one of the most dramatic exchanges of the Council. (Cardinal Frings' theological adviser was the young Joseph Ratzinger, who would later as a Cardinal head the same department of the Holy See, and from 2005–13 reign as Pope Benedict XVI). The second session ended on 4 December.

In the time between the second and third sessions, the proposed schemata were further revised on the basis of comments from the Council Fathers. A number of topics were reduced to statements of fundamental propositions that could gain approval during the third session, with postconciliar commissions handling implementation of these measures.

At the end of the second session, Cardinal Leo Joseph Suenens of Belgium had asked the other bishops: "Why are we even discussing the reality of the church when half of the church is not even represented here?", referring to women. In response, 15 women were appointed as auditors in September 1964. Eventually 23 women were auditors at the Second Vatican Council, including 10 women religious. The auditors had no official role in the deliberations, although they attended the meetings of subcommittees working on Council documents, particularly texts that dealt with the laity. They also met together on a weekly basis to read draft documents and to comment on them.

During the third session, which began on 14 September 1964, the Council Fathers worked through a large volume of proposals. There "were approved and promulgated by the Pope" schemata on ecumenism ("Unitatis redintegratio"); the official view on Protestant and Eastern Orthodox "separated brethren"; the Eastern Rite churches ("Orientalium Ecclesiarum"); and the Dogmatic Constitution of the Church ("Lumen gentium").

Schemata on the life and ministry of priests and the missionary activity of the Church were rejected and sent back to commissions for complete rewriting. Work continued on the remaining schemata, in particular those on the Church in the modern world and on religious freedom. There was controversy over revisions of the decree on religious freedom and the failure to vote on it during the third session, but Pope Paul promised that this schema would be the first to be reviewed in the next session.

Pope Paul closed the third session on 21 November by announcing a change in the Eucharistic fast and formally reaffirming Mary as "Mother of the Church". While some called for more dogmas about Mary, in a 2 February 1965 speech Paul VI referred to the "Christocentric and Church-centered direction which the Council intends to give to our doctrine and devotion to our Lady". 

Going into the final session, Paul VI and most of the bishops wanted this to be the final session. Cardinal Ritter observed that, "We were stalled by the delaying tactics of a very small minority" in the Curia who were more industrious in communicating with the pope then was the more progressive majority. Eleven schemata remained unfinished at the end of the third session, and commissions worked to give them their final form. Schema 13, on the Church in the modern world, was revised by a commission that worked with the assistance of laypersons.

Pope Paul VI opened the last session of the Council on 14 September 1965 and on the following day promulgated the "motu proprio" establishing the Synod of Bishops. This more permanent structure was intended to preserve close cooperation of the bishops with the Pope after the Council.

The first business of the fourth session was the consideration of the decree on religious freedom, "Dignitatis humanae", one of the more controversial of the conciliar documents that passed on 21 September by a vote of 1,997 for to 224 against. The principal work of the other part of the session was work on three documents, all of which were approved by the Council Fathers. The lengthened and revised pastoral constitution on the Church in the modern world, "Gaudium et spes", was followed by decrees on missionary activity, "Ad gentes," and on the ministry and life of priests, "Presbyterorum ordinis".

The Council also gave final approval to other documents that had been considered in earlier sessions. These included the Dogmatic Constitution on Divine Revelation ("Dei verbum") and the decrees on the pastoral office of bishops ("Christus Dominus"), on the life of persons in religious orders (expanded and modified from earlier sessions, finally titled "Perfectae caritatis"), on education for the priesthood ("Optatam totius"), on Christian education ("Gravissimum educationis"), and on the role of the laity ("Apostolicam actuositatem").

One of the more controversial documents was "Nostra aetate", which stated that the Jews of the time of Christ, taken indiscriminately, and all Jews today are no more responsible for the death of Christ than Christians.
Better Jewish-Catholic relations have been emphasized since the Council.

A major event of the final days of the Council was the act of Pope Paul and Orthodox Patriarch Athenagoras of a joint expression of regret for many of the past actions that had led up to the Great Schism between the western and eastern churches.

"The old story of the Samaritan has been the model of the spirituality of the Council" (Paul VI., address, 7 December). On 8 December, the Council was formally closed, with the bishops professing their obedience to the Council's decrees. To help carry forward the work of the Council, Pope Paul:

During the Second Vatican Council the bishops produced four major "constitutions" and twelve other documents.

The first document passed by the Council was "Sacrosanctum Concilium" ("Most Sacred Council") on the church's liturgy. Pope Benedict XVI explained that an essential idea of the Council itself is the "Paschal Mystery (Christ's passion, death and resurrection) as the center of what it is to be Christian and therefore of the Christian life, the Christian year, the Christian seasons, expressed in Eastertide and on Sunday which is always the day of the Resurrection." Thus, the liturgy, especially the Eucharist which makes the Paschal Mystery present, is "the summit toward which the activity of the Church is directed; at the same time it is the font from which all her power flows."

The matter that had the most immediate effect on the lives of individual Catholics was the revision of the liturgy. The central idea was that there ought to be lay participation in the liturgy which means they "take part fully aware of what they are doing, actively engaged in the rite, and enriched by its effects" (SC 11). Since the mid-1960s, permission has been granted to celebrate the Mass in vernacular languages. It has been emphasized that the language used should be known to the gathered people. The amount of Scripture read during Mass was greatly expanded, through different annual cycles of readings. The revised version of the Latin text of the Mass remains the authoritative text on which translations are based. The invitation for more active, conscious participation of the laity through Mass in the vernacular did not stop with the decree on the liturgy. It was taken up by the later documents of the Council that called for a more active participation of the laity in the life of the Church, a turn away from clericalism toward a new age of the laity.

The Dogmatic Constitution on the Church "Lumen gentium" "(""Light of the Nations") gave direction to several of the documents that followed it, including those on Ecumenism, on Non-Christian Religions, on Religious Freedom, and on The Church in the Modern World (see below). A most contentious conclusion that seems to follow from the Bishops' teaching in the decree is that while "in some sense other Christian communities are institutionally defective," these communities can "in some cases be more effective as vehicles of grace." Belgian Bishop Emil de Smedt, commenting on institutional defects that had crept into the Catholic church, "contrasted the hierarchical model of the church that embodied the triad of 'clericalism, legalism, and triumphalism' with one that emphasized the 'people of God', filled with the gifts of the Holy Spirit and radically equal in grace," that was extolled in "Lumen Gentium". According to Pope Paul VI, "the most characteristic and ultimate purpose of the teachings of the Council" is the universal call to holiness. John Paul II calls this "an intrinsic and essential aspect of [the Council Fathers'] teaching on the Church", where "all the faithful of Christ of whatever rank or status, are called to the fullness of the Christian life and to the perfection of charity" ("Lumen gentium", 40). Pope Francis, in his apostolic letter "Evangelii Gaudium" (17) which laid out the programmatic for his pontificate, said that "on the basis of the teaching of the Dogmatic Constitution "Lumen Gentium"" he would discuss the entire People of God which evangelizes, missionary outreach, the inclusion of the poor in society, and peace and dialogue within society. Francis has also followed the call of the Council for a more collegial style of leadership, through synods of bishops and through his personal use of a worldwide advisory council of eight cardinals.

The Council's document "Dei Verbum" ("The Word of God") states the principle active in the other Council documents that "The study of the sacred page is, as it were, the soul of sacred theology". It is said of "Dei Verbum" that "arguably it is the most seminal of all the conciliar documents," with the fruits of a return to the Bible as the foundation of Christian life and teaching, evident in the other Council documents. Joseph Ratzinger, who would become Pope Benedict XVI, said of the emphasis on the Bible in the Council that prior to Vatican II the theology manuals continued to confuse "propositions about revelation with the content of revelation. It represented not abiding truths of faith, but rather the peculiar characteristics of post-Reformation polemic." In spite of the guarded approval of biblical scholarship under Pius XII, scholars suspected of Modernism were silenced right up to Vatican II. The Council brought a definitive end to the Counter-Reformation and, in a spirit of "aggiornamento", reached back "behind St. Thomas himself and the Fathers, to the biblical theology which governs the first two chapters of the Constitution on the Church." "The documents of the Second Vatican Council are shot through with the language of the Bible. ...The church's historical journey away from its earlier focus upon these sources was reversed at Vatican II." For instance, the Council's document on the liturgy called for a broader use of liturgical texts, which would now be in the vernacular, along with more enlightened preaching on the Bible explaining "the love affair between God and humankind".

This document, named for its first words "Gaudium et Spes" ("Joy and Hope"), built on "Lumen Gentium"'s understanding of the Church as the “pilgrim people of God” and as “communion”, aware of the long history of the Church's teaching and in touch with what it calls the “signs of the times”. It reflects the understanding that Baptism confers on all the task that Jesus entrusted to the Church, to be on mission to the world in ways that the present age can understand, in cooperation with the ongoing work of the Spirit. And for those who "draw a distinction between non-negotiable teachings on human sexuality and negotiable teachings on social justice, "Gaudium et Spes" is an insuperable obstacle and the pontificate of Pope Francis is making that obvious for all with eyes to see."

Opening declaration – "Gaudet Mater Ecclesia" ("Mother Church Rejoices") was the opening declaration of the Second Vatican Council, delivered by Pope John XXIII on 11 October 1962 before the bishops and representatives of 86 governments or international groups. He criticizes the "prophets of doom who are always forecasting disaster" for the church or world. He speaks of the advantage of separation of Church and state but also the challenge to integrate faith with public life. The Church "meets today's needs by explaining the validity of her doctrine more fully rather than by condemning," by reformulating ancient doctrine for pastoral effectiveness. Also, the Church is "moved by mercy and goodness towards her separated children." John XXIII before his papacy had proven his gifts as a papal diplomat and as Apostolic Nuncio to France.

On the Means of Social Communication – The decree "Inter mirifica" ("Among the wonderful", 1963) addresses issues concerning the press, cinema, television, and other media of communication.

Ecumenism – The decree "Unitatis redintegratio" ("Reintegration of Unity", 1964) opens with the statement: "The restoration of unity among all Christians is one of the principal concerns of the Second Vatican Council."

Of the Eastern Catholic Churches – The decree "Orientalium Ecclesiarum" ("Of the Eastern Churches", 1964) recognizes the right of Eastern Catholics in communion with the Holy See to keep their distinct liturgical practices and avoid Latinisation. It encourages them to "take steps to return to their ancestral traditions."

Mission Activity – The decree "Ad gentes" ("To the Nations", 1965) treats evangelization as the fundamental mission of the Catholic Church, "to bring good news to the poor." It includes sections on training missionaries and on forming communities.

The Apostolate of the Laity – The decree "Apostolicam actuositatem" ("Apostolic Activity", 1965) declares that the apostolate of the laity is "not only to bring the message and grace of Christ to men but also to penetrate and perfect the temporal order with the spirit of the Gospel", in every field of life, together or through various groups, with respectful cooperation with the Church's hierarchy.

The Pastoral Office of Bishops – The decree "Christus Dominus" ("Christ the Lord", 1965) places renewed emphasis on collegiality and on strong conferences of bishops, while respecting the papacy.

On Religious Freedom – The declaration "Dignitatis humanae" ("Of the Dignity of the Human Person", 1965) is "on the right of the person and of communities to social and civil freedom in matters religious".

Non-Christian Religions – The declaration "Nostra aetate" ("In our time", 1965) reflects that people are being drawn closer together in our time. The Church "regards with sincere reverence those ways of conduct and of life, those precepts and teachings which, though differing in many aspects from the ones she holds and sets forth, nonetheless often reflect a ray of that Truth which enlightens all men.<nowiki>"</nowiki> And Jews today "should not be presented as rejected or accursed by God" for what happened to Jesus.

The Adaptation and Renewal of Religious Life – The decree "Perfectae Caritatis" ("Of perfect charity", 1965) calls for "adaptation and renewal of the religious life [that] includes both the constant return to the sources of all Christian life and to the original spirit of the institutes and their adaptation to the changed conditions of our time."

On the Ministry and Life of Priests – The decree "Presbyterorum ordinis" ("The order of priests", 1965) describes priests as "father and teacher" but also "brothers among brothers with all those who have been reborn at the baptismal font." Priests must "promote the dignity" of the laity, "willingly listen" to them, acknowledge and diligently foster "exalted charisms of the laity", and "entrust to the laity duties in the service of the Church, allowing them freedom and room for action." Also, the human and spiritual needs of priests are discussed in detail.

On Priestly Training – The decree "Optatam totius" ("Desired [renewal] of the whole", 1965).

On Christian Education – The declaration "Gravissimum educationis" ("Extremely important [time] of education", 1965).

Closing Statement – On 12 January, 1966, a month after the close of the Council, Pope Paul VI wrote the letter "Udienze Generale" on how the Council was to be interpreted.

The questioning of the nature of and even validity of the Second Vatican Council continues to be a contending point of rejection and conflict among various religious communities, some of which are not in communion with the Catholic Church. In particular, two schools of thought may be discerned:

The most recent edition of the 1983 Code of Canon Law states that Catholics may not disregard the teaching of an ecumenical council even if it does not propose such as definitive. Accordingly, it also maintains the view that the present living Pope alone judges the criterion of membership for being in "in communio" with the Church. The present canon law further articulates:

In addition to general spiritual guidance, the Second Vatican Council produced very specific recommendations, such as in the document "Gaudium et Spes": "Any act of war aimed indiscriminately at the destruction of entire cities of extensive areas along with their population is a crime against God and man himself. It merits unequivocal and unhesitating condemnation." "Dignitatis humanae", authored largely by United States theologian John Courtney Murray, challenged the Council fathers to find "reasons for religious freedom" in which they believed, and drew from scripture scholar John L. McKenzie the comment: "The Church can survive the disorder of development better than she can stand the living death of organized immobility." 

By "the spirit of Vatican II" is often meant promoting teachings and intentions attributed to the Second Vatican Council in ways not limited to literal readings of its documents, spoken of as the "letter" of the Council (cf. Saint Paul's phrase, "the letter kills, but the Spirit gives life").

The spirit of Vatican II is invoked for a great variety of ideas and attitudes. Bishop John Tong Hon of Hong Kong used it with regard merely to an openness to dialogue with others, saying: "We are guided by the spirit of Vatican II: only dialogue and negotiation can solve conflicts."

In contrast, Michael Novak described it as a spirit that:

To mark the fiftieth anniversary of the beginning of Vatican II, in October 2011, Pope Benedict XVI declared the period from October 2012 to the Solemnity of Christ the King at the end of November 2013 a "Year of Faith", as:

It has been suggested that the pontificate of Pope Francis will be looked upon as the "decisive moment in the history of the church in which the full force of the Second Vatican Council's reformist vision was finally realized." Francis returned to the Vatican II theme of "ressourcement", breaking with the Catholic philosophical tradition that had originated with Thomas Aquinas seven centuries before, and looked to original sources in the New Testament. In contrast to John Paul II who emphasized continuity with the past in Vatican II's teachings, Francis' words and actions were noted from the start for their discontinuities, with an emphasis on Jesus himself and on mercy: a "church that is poor and for the poor", "disposal of the baroque trappings" in liturgical celebrations, and revision of the institutional aspects of the church. From his first gesture when elected Pope, calling himself simply Bishop of Rome, Francis connected with the thrust of the Council away from "legalism, triumphalism, and clericalism". He made greater use of church synods, and instituted a more collegial manner of governance by constituting a Council of Cardinal Advisers from throughout the world to assist him which a church historian calls the "most important step in the history of the church for the past 10 centuries." His refocusing the Church on “a moral theology that rests on scripture and Jesus’ command to love” is also seen as coming from the Council, as is his lifting up the laity for mission and calling for the presence of women in theologates. He has softened the "forbidding" image of the Church by applying Vatican II's views on respect for conscience to issues like atheism, homosexuality, and the sacraments. This has led to a struggle between "anti-Vatican II diehards and clerics who prefer John XXIII’s (and Francis’s) generosity of spirit." On the issue of liturgy, he has tried to advance the renewal initiated by Vatican II that would elicit more conscious, active participation by the people. And while his predecessors had taken a dim view of liberation theology, his more positive view is seen as flowing from a discernment of "the signs of the times" called for by "Gaudium et spes". He appointed more cardinals from the southern hemisphere and constituted an advisory counsel of eight cardinals from around the world to advise him on reform, which a church historian calls the "most important step in the history of the church for the past 10 centuries."
Several of the Fathers and theologians-experts, as well as several Roman Popes and council observers, became canonized saints or are in the process of canonization. These include:







</doc>
<doc id="28135" url="https://en.wikipedia.org/wiki?curid=28135" title="Slovene language">
Slovene language

Slovene ( or ), or alternatively Slovenian (; "slovenski jezik" or "slovenščina"), is a South Slavic language spoken by the Slovenes. It is spoken by about 2.5 million speakers worldwide, the majority of whom live in Slovenia, where it is one of the three official languages. As Slovenia is part of the European Union, Slovene is also one of its 24 official and working languages.

Standard Slovene is the national standard language that was formed in the 18th and 19th century, based on Upper and Lower Carniolan dialect groups, more specifically on language of Ljubljana and its adjacent areas. The Lower Carniolan dialect group was the dialect used by Primož Trubar while he also used the Slovene language as spoken in Ljubljana, since he lived in the city for more than 20 years. It was the speech of Ljubljana that Trubar took as a foundation of what later became standard Slovene, with small addition of his native speech, that is Lower Carniolan dialect Trubar's choice was later adopted also by other Protestant writers in the 16th century, and ultimately led to a formation of more standard language. The Upper dialect was also used by most authors during the language revival of the 18th and early 19th century, and was also the language spoken by France Prešeren, the latter, as was the case with most of Slovene writers and poets, lived and worked in Ljubljana, which speech ultimately grew closer to the Upper Carniolan dialect group.
Unstandardized dialects are more preserved in regions of the Slovene Lands where compulsory schooling was in languages other than Standard Slovene, as was the case with the Carinthian Slovenes in Austria, and the Slovene minority in Italy. For example, the Resian and Torre (Ter) dialects in the Italian Province of Udine differ most from other Slovene dialects.

The distinctive characteristics of Slovene are dual grammatical number, two accentual norms (one characterized by pitch accent), and abundant inflection (a trait shared with many Slavic languages). Although Slovene is basically an SVO language, word order is very flexible, often adjusted for emphasis or stylistic reasons. Slovene has a T–V distinction: second-person plural forms are used for individuals as a sign of respect.

Slovene is an Indo-European language belonging to the Western subgroup of the South Slavic branch of the Slavic languages, together with Serbo-Croatian. It is close to the Chakavian and especially Kajkavian dialects of Serbo-Croatian, but further from the Shtokavian dialect, the basis for the Bosnian, Croatian, Montenegrin, and Serbian standard languages. Furthermore, Slovene shares certain linguistic characteristics with all South Slavic languages, including those of the Eastern subgroup, such as Bulgarian.

Mutual intelligibility with varieties of Serbo-Croatian is hindered by differences in vocabulary, grammar, and pronunciation, Kajkavian being the most mutually intelligible. The Slovene language has many commonalities with the West Slavic languages.

Like all Slavic languages, Slovene traces its roots to the same proto-Slavic group of languages that produced Old Church Slavonic. The earliest known examples of a distinct, written dialect possibly connected to Slovene are from the "Freising manuscripts," known in Slovene as "Brižinski spomeniki". The consensus estimate of their date of origin is between 972 and 1039 (most likely before 1000). These religious writings are among the oldest surviving manuscripts in any Slavic language.

The "Freising manuscripts" are a record of a proto-Slovene language that was spoken in a more scattered territory than modern Slovene, which included most of the present-day Austrian states of Carinthia and Styria, as well as East Tyrol, the Val Pusteria in South Tyrol, and some areas of Upper and Lower Austria.

By the 15th century, most of the northern areas were gradually Germanized: the northern border of the Slovene-speaking territory stabilized on the line going from north of Klagenfurt to south of Villach and east of Hermagor in Carinthia, while in Styria it was pretty much identical with the current Austrian-Slovenian border.

This linguistic border remained almost unchanged until the late 19th century, when a second process of Germanization took place, mostly in Carinthia. Between the 9th and 12th century, proto-Slovene spread into northern Istria and in the areas around Trieste.

During most of the Middle Ages, Slovene was a vernacular language of the peasantry, although it was also spoken in most of the towns on Slovenian territory, together with German or Italian. Although during this time, German emerged as the spoken language of the nobility, Slovene had some role in the courtly life of the Carinthian, Carniolan and Styrian nobility, as well. This is proved by the survival of certain ritual formulas in Slovene (such as the ritual installation of the Dukes of Carinthia). The words "Buge waz primi, gralva Venus!" ("God be With You, Queen Venus!"), with which Bernhard von Spanheim greeted the poet Ulrich von Liechtenstein, who was travelling around Europe in guise of Venus, upon his arrival in Carinthia in 1227 (or 1238), is another example of some level of Slovene knowledge among high nobility in the region.

The first printed Slovene words, "stara pravda" (meaning 'old justice' or 'old laws'), appeared in 1515 in Vienna in a poem of the German mercenaries who suppressed the Slovene peasant revolt: the term was presented as the peasants' motto and battle cry. Standard Slovene emerged in the second half of the 16th century, thanks to the works of Slovene Lutheran authors, who were active during the Protestant Reformation. The most prominent authors from this period are Primož Trubar, who wrote the first books in Slovene; Adam Bohorič, the author of the first Slovene grammar; and Jurij Dalmatin, who translated the entire Bible into Slovene.

From the high Middle Ages up to the dissolution of the Austro-Hungarian Empire in 1918, in the territory of present-day Slovenia, German was the language of the elite, and Slovene was the language of the common people. During this period, German had a strong influence on Slovene, and many Germanisms are preserved in contemporary colloquial Slovene. Many Slovene scientists before the 1920s also wrote in foreign languages, mostly German, which was the "lingua franca" of science throughout Central Europe at the time.

During the rise of Romantic nationalism in the 19th century, the cultural movements of Illyrism and Pan-Slavism brought words from Serbo-Croatian, specifically Croatian dialects, and Czech into standard Slovene, mostly to replace words previously borrowed from German. Most of these innovations have remained, although some were dropped in later development. In the second half of the 19th century, many nationalist authors made an abundant use of Serbo-Croatian words: among them were Fran Levstik and Josip Jurčič, who wrote the first novel in Slovene in 1866. This tendency was reversed in the Fin de siècle period by the first generation of modernist Slovene authors (most notably the writer Ivan Cankar), who resorted to a more "pure" and simple language without excessive Serbo-Croatian borrowings.

During the Kingdom of Yugoslavia in the 1920s and 1930s, the influence of Serbo-Croatian increased again. This was opposed by the younger generations of Slovene authors and intellectuals; among the most fierce opponents of an excessive Serbo-Croatian influence on Slovene were the intellectuals associated with the leftist journal "Sodobnost", as well as some younger Catholic activists and authors. After 1945, numerous Serbo-Croatian words that had been used in the previous decades were dropped. The result was that a Slovene text from the 1910s is frequently closer to modern Slovene than a text from the 1920s and 1930s.

Between 1920 and 1941, the official language of the Kingdom of Yugoslavia was defined as "Serbian-Croatian-Slovene". In practice, Slovene was used in Slovenia, both in education and administration. Many state institutions used only Serbo-Croatian, and a Slovene–Serbo-Croatian bilingualism was applied in many spheres of public life in Slovenia. For examples, at the post offices, railways and in administrative offices, Serbo-Croatian was used together with Slovene. However, state employees were expected to be able to speak Slovene in Slovenia.

During the same time, western Slovenia (the Slovenian Littoral and the western districts of Inner Carniola) was under Italian administration and submitted to a violent policy of Fascist Italianization; the same policy was applied to Slovene speakers in Venetian Slovenia, Gorizia and Trieste. Between 1923 and 1943, all public use of the Slovene language in these territories was strictly prohibited, and Slovene language activists were persecuted by the state.

After the Carinthian Plebiscite of 1920, a less severe policy of Germanization took place in the Slovene-speaking areas of southern Carinthia which remained under Austrian administration. After the Anschluss of 1938, the use of Slovene was strictly forbidden in Carinthia, as well. This accelerated a process of language shift in Carinthia, which continued throughout the second half of the 20th century: according to the Austro-Hungarian census of 1910, around 21% of inhabitants of Carinthia spoke Slovene in their daily communication; by 1951, this figure dropped to less than 10%, and by 2001 to a mere 2.8%.

During World War II, Slovenia was divided among the Axis Powers of Fascist Italy, Nazi Germany, and Hungary. Each of the occupying powers tried to either discourage or entirely suppress the Slovene language.

Following World War II, Slovenia became part of the Socialist Federal Republic of Yugoslavia. Slovene was one of the official languages of the federation. In the territory of Slovenia, it was commonly used in almost all areas of public life. One important exception was the Yugoslav army, where Serbo-Croatian was used exclusively, even in Slovenia.

National independence has revitalized the language: since 1991, when Slovenia gained independence, Slovene has been used as an official language in all areas of public life. In 2004 it became one of the official languages of the European Union upon Slovenia's admission.

Joža Mahnič, a literary historian and president of the publishing house "Slovenska matica", said in February 2008 that Slovene is a language rich enough to express everything, including the most sophisticated and specialised texts. In February 2010, Janez Dular, a prominent Slovenian linguist, commented that, although Slovene is not an endangered language, its scope has been shrinking, especially in science and higher education.

The language is spoken by about 2.5 million people, mainly in Slovenia, but also by Slovene national minorities in Friuli-Venezia Giulia, Italy (around 90,000 in Venetian Slovenia, Resia Valley, Canale Valley, Province of Trieste and in those municipalities of the Province of Gorizia bordering with Slovenia), in southern Carinthia and some parts of Styria in Austria (25,000). It is also spoken in Croatia, especially in Istria, Rijeka and Zagreb (11,800-13,100), in southwestern Hungary (3-5,000), in Serbia (5,000), and by the Slovene diaspora throughout Europe and the rest of the world (around 300,000), particularly in the United States (most notably Ohio, home to an estimated 3,400 speakers), Canada, Argentina, Australia and South Africa.

Slovene is sometimes characterized as the most diverse Slavic language in terms of dialects, with different degrees of mutual intelligibility. Accounts of the number of dialects range from as few as seven dialects, often considered dialect groups or dialect bases that are further subdivided into as many as 50 dialects. Other sources characterize the number of dialects as nine or eight. The Slovene proverb "Every village has its own voice" ("Vsaka vas ima svoj glas") depicts the differences in dialects. Although pronunciation differs greatly from area to area, those differences do not pose major obstacles to understanding. The standard language is mainly used in public presentations or on formal occasions.

The Prekmurje dialect used to have a written norm of its own at one point. The Resian dialects have an independent written norm that is used by their regional state institutions. Speakers of those two dialects have considerable difficulties with being understood by speakers of other varieties of Slovene, needing code-switching to Standard Slovene. Other dialects are mutually intelligible when speakers avoid the excessive usage of regionalisms.

Regionalisms are mostly limited to culinary and agricultural expressions, although there are many exceptions. Some loanwords have become so deeply rooted in the local language that people have considerable difficulties in finding a standard expression for the dialect term (for instance, "kovter" meaning blanket is "prešita odeja" in Standard Slovene, but the latter term is very rarely used in speech, being considered inappropriate for non-literary registers). Southwestern dialects incorporate a great deal of calques and loanwords from Italian, whereas eastern and northwestern dialects are replete with lexemes of German origin. Usage of such words hinders intelligibility between dialects and is greatly discouraged in formal situations.

Slovene has a phoneme set consisting of 21 consonants and 8 vowels.

Slovene has 21 distinctive consonant phonemes.

All voiced obstruents are devoiced at the end of words unless immediately followed by a word beginning with a vowel or a voiced consonant. In consonant clusters, voicing distinction is neutralized and all consonants assimilate the voicing of the rightmost segment. In this context, , and may occur as voiced allophones of , and , respectively (e.g. "vŕh drevésa" ).


The sequences , and occur only before a vowel. Before a consonant or word-finally, they are reduced to , and respectively. This is reflected in the spelling in the case of , but not for and .

Under certain (somewhat unpredictable) circumstances, at the end of a syllable may become , merging with the allophone of in that position.

Slovene has an eight-vowel (or, according to Peter Jurgec, nine-vowel) system, in comparison to the five-vowel system of Serbo-Croatian.

Slovene nouns retain six of the seven Slavic noun cases: nominative, accusative, genitive, dative, locative and instrumental. There is no distinct vocative; the nominative is used in that role. Nouns, adjectives and pronouns have three numbers: singular, dual and plural.

Nouns in Slovene are either masculine, feminine or neuter gender. In addition, there is a distinction between animate and inanimate nouns, although this is only relevant for masculine nouns and only in the singular. Animate nouns have an accusative singular form that is identical to the genitive, while for inanimate nouns the accusative singular is the same as the nominative. Animacy is based mostly on semantics and is less rigid than gender. Generally speaking a noun is animate if it refers to something that is generally thought to have free will or the ability to move of its own accord. This includes all nouns for people and animals. All other nouns are inanimate, including plants and other non-moving life forms, and also groups of people or animals. However, there are some nouns for inanimate objects that are generally animate, which mostly include inanimate objects that are named after people or animals. This includes:

Slovene, like most other European languages, has a T–V distinction, or two forms of 'you' for formal and informal situations, respectively. Although informal address using the 2nd person singular "ti" form (known as "tikanje") is officially limited to friends and family, talk among children, and addressing animals, it is increasingly used among the middle generation to signal a relaxed attitude or lifestyle instead of its polite or formal counterpart using the 2nd person plural "vi" form (known as "vikanje").

An additional nonstandard but widespread use of a singular participle combined with a plural auxiliary verb (known as "polvikanje") signals a somewhat more friendly and less formal attitude while maintaining politeness:


The use of nonstandard forms ("polvikanje") might be frowned upon by many people and would not likely be used in a formal setting.

The use of the 3rd person plural "oni" ('they') form (known as "onikanje" in both direct address and indirect reference; this is similar to using "Sie" in German) as an ultra-polite form is now archaic or dialectal. It is associated with servant-master relationships in older literature, the child-parent relationship in certain conservative rural communities, and parishioner-priest relationships.

Foreign words used in Slovene are of various types depending on the assimilation they have undergone. The types are:
The loanwords are mostly from German and Italian, while the more recently borrowed and less assimilated words are typically from English.

There are no definite or indefinite articles as in English ("a", "an", "the") or German ("der", "die", "das", "ein", "eine"). A whole verb or a noun is described without articles and the grammatical gender is found from the word's termination. It is enough to say "barka" ("a" or "the barge"), "Noetova barka" ('Noah's ark'). The gender is known in this case to be feminine. In declensions, endings are normally changed; see below. If one should like to somehow distinguish between definiteness or indefiniteness of a noun, one would say "(prav/natanko/ravno) tista barka" ('that/precise/exact barge') for "the barge" and "neka/ena barka" ('some/a barge') for "a barge".

Definiteness of a noun phrase can also be discernible through the ending of the accompanying adjective. One should say "rdeči šotor" ([exactly that] red tent) or "rdeč šotor" ([a] red tent). This difference is observable only for masculine nouns in nominative or accusative case. Because of the lack of article in Slovene and audibly insignificant difference between the masculine adjective forms, most dialects do not distinguish between definite and indefinite variants of the adjective, leading to hypercorrection when speakers try to use Standard Slovenian.

This alphabet () was derived in the mid-1840s from the system created by Croatianist Ljudevit Gaj. Intended for the Serbo-Croatian language (in all its varieties), it was patterned on the Czech alphabet of the 1830s. Before that was, for example, written as , or ; as , , or ; sometimes as as a relic from the now modern Russian yery character, usually transliterated as "y"; as ; as ; as ; as , or .

The standard Slovenian orthography, used in almost all situations, uses only the letters of the ISO basic Latin alphabet plus , , and :

The orthography thus underdifferentiates several phonemic distinctions:


In the tonemic varieties of Slovene, the ambiguity is even worse: "e" in a final syllable can stand for any of (although is rare).

The reader is expected to gather the interpretation of the word from the context, as in these examples:


To compensate for the shortcomings of the standard orthography, Slovenian also uses standardized diacritics or accent marks to denote stress, vowel length and pitch accent, much like the closely related Serbo-Croatian. However, as in Serbo-Croatian, use of such accent marks is restricted to dictionaries, language textbooks and linguistic publications. In normal writing, the diacritics are almost never used, except in a few minimal pairs where real ambiguity could arise.

Two different and mutually incompatible systems of diacritics are used. The first is the simpler non-tonemic system, which can be applied to all Slovene dialects. It is more widely used and is the standard representation in dictionaries such as SSKJ. The tonemic system also includes tone as part of the representation. However, neither system reliably distinguishes schwa from the front mid-vowels, nor vocalised l from regular l . Some sources write these as "ə" and "ł", respectively, but this is not as common.

In the non-tonemic system, the distinction between the two mid-vowels is indicated, as well as the placement of stress and length of vowels:


The tonemic system uses the diacritics somewhat differently from the non-tonemic system. The high-mid vowels and are written "ẹ ọ" with a subscript dot, while the low-mid vowels and are written as plain "e o".

Pitch accent and length is indicated by four diacritical marks:


The schwa vowel is written ambiguously as "e", but its accentuation will sometimes distinguish it: a long vowel mark can never appear on a schwa, while a grave accent can appear only on a schwa. Thus, only "ȅ" and unstressed "e" are truly ambiguous.

Standard Slovene spelling and grammar are defined by the Orthographic Committee and the Fran Ramovš Institute of the Slovenian Language, which are both part of the Slovenian Academy of Sciences and Arts ("Slovenska akademija znanosti in umetnosti", SAZU). The newest reference book of standard Slovene spelling (and to some extent also grammar) is the "Slovenski pravopis" ("SP2001"; Slovene Normative Guide). The latest printed edition was published in 2001 (reprinted in 2003 with some corrections) and contains more than 130,000 dictionary entries. In 2003, an electronic version was published.

The official dictionary of modern Slovene, which was also prepared by SAZU, is "Slovar slovenskega knjižnega jezika" ("SSKJ"; Standard Slovene Dictionary). It was published in five volumes by Državna Založba Slovenije between 1970 and 1991 and contains more than 100,000 entries and subentries with accentuation, part-of-speech labels, common collocations, and various qualifiers. In the 1990s, an electronic version of the dictionary was published and is available online.

The SAZU considers SP2001 to be the normative source on Slovenian language. When dictionary entries in SP2001 and SSKJ differ, the SP2001 entry takes precedence. SP2001 is called a Spelling Dictionary by the European Network of e-Lexicography.

Below is the preamble of the Universal Declaration of Human Rights in Slovene."Ker" priznanje prirojenega dostojanstva ter enakih in neodtujljivih pravic vseh članov človeške družine pomeni temelj svobode, pravičnosti in miru v svetu,

"ker" sta zanikanje in teptanje človekovih pravic pripeljala do barbarskih dejanj, ki so pretresla zavest človeštva, in ker je bila za najvišjo spoznana težnja človeštva, da bi nastopil svet, v katerem bodo ljudje uživali svobodo govora in prepričanja ter svobodo živeti brez strahu in pomanjkanja,

"ker" je nujno potrebno človekove pravice zavarovati z vladavino prava, da se človek v skrajni sili ne bi bil prisiljen zateči k uporu zoper tiranijo in zatiranje, 

"ker" je nujno potrebno spodbujati razvoj prijateljskih odnosov med narodi,

"ker" so ljudstva Organizacije združenih narodov v Ustanovni listini potrdila svojo vero v temeljne človekove pravice, dostojanstvo in vrednost človeškega bitja ter v enake pravice moških in žensk ter se odločila, da bodo spodbujala družbeni napredek in boljše življenjske razmere v večji svobodi,

"ker" so se države članice zavezale, da bodo, v sodelovanju z Organizacijo združenih narodov, zagotavljale splošno spoštovanje in upoštevanje človekovih pravic in temeljnih svoboščin,

"ker" je skupno razumevanje teh pravic in svoboščin največjega pomena za celovito uresničitev te zaveze,

"Generalna skupščina"

"razglaša" Splošno deklaracijo človekovih pravic kot skupen ideal vseh ljudstev in vseh narodov z namenom, da bi vsi posamezniki in vsi organi družbe, vselej ob upoštevanju te deklaracije, z vzgojo in izobraževanjem spodbujali spoštovanje teh pravic in svoboščin ter s postopnimi državnimi in mednarodnimi ukrepi zagotovili njihovo splošno in dejansko priznanje in upoštevanje, tako med ljudstvi držav članic samih kakor tudi med ljudstvi ozemelj pod njihovo upravo.







</doc>
<doc id="28136" url="https://en.wikipedia.org/wiki?curid=28136" title="Slovak language">
Slovak language

Slovak () is a West Slavic language of the Czech–Slovak group. Spoken by approximately 5 million people as a native language, it serves as the official language of Slovakia and one of the 24 official languages of the European Union.

Slovak is closely related to Czech, to the point of mutual intelligibility to a very high degree, as well as Polish. Like other Slavic languages, Slovak is a fusional language with a complex system of morphology and relatively flexible word order. Its vocabulary has been extensively influenced by Latin and German and other Slavic languages.

The Czech–Slovak group developed within West Slavic in the high medieval period, and the standardization of Czech and Slovak within the Czech–Slovak dialect continuum emerged in the early modern period. In the later mid-19th century, the modern Slovak alphabet and written standard became codified by Ľudovít Štúr and reformed by Martin Hattala. The Moravian dialects spoken in the western part of the country along the border with the Czech Republic are also sometimes classified as Slovak, although some of their western variants are closer to Czech; they nonetheless form the bridge dialects between the two languages.

Slovak speakers are also found in the Slovak diaspora in the United States, the Czech Republic, Argentina, Serbia, Ireland, Romania, Poland (where Slovak is a recognised minority language), Canada, Hungary, Germany, Croatia, Israel, the United Kingdom, Australia, Austria, Ukraine, Norway and in other countries to a lesser extent.

Slovak uses the Latin script with small modifications that include the four diacritics (ˇ, ´, ¨, ˆ) placed above certain letters ()

The primary principle of Slovak spelling is the phonemic principle. The secondary principle is the morphological principle: forms derived from the same stem are written in the same way even if they are pronounced differently. An example of this principle is the assimilation rule (see below). The tertiary principle is the etymological principle, which can be seen in the use of "i" after certain consonants and of "y" after other consonants, although both "i" and "y" are usually pronounced the same way.

Finally, the rarely applied grammatical principle is present when, for example, the basic singular form and plural form of masculine adjectives are written differently with no difference in pronunciation (e.g. = nice – singular versus = nice – plural).

In addition, the following rules are present:

Most loanwords from foreign languages are respelt using Slovak principles either immediately or later. For example, "weekend" is spelled , "software" – , "gay" – (both not exclusively), and "quality" is spelled . Personal and geographical names from other languages using Latin alphabets keep their original spelling unless a fully Slovak form of the name exists (e.g. for "London").

Slovak features some heterophonic homographs (words with identical spelling but different pronunciation and meaning), the most common examples being (beautiful) versus (beautifully).
The main features of Slovak syntax are as follows:

Some examples include the following:


Word order in Slovak is relatively free, since strong inflection enables the identification of grammatical roles (subject, object, predicate, etc.) regardless of word placement. This relatively free word order allows the use of word order to convey topic and emphasis.

Some examples are as follows:

The unmarked order is subject–verb–object. Variation in word order is generally possible, but word order is not completely free.
In the above example, the noun phrase cannot be split up, so that the following combinations are not possible:

And the following sentence is stylistically infelicitous:

The regular variants are as follows: 

Slovak does not have articles. The demonstrative pronoun (fem: , neuter: ) may be used in front of the noun in situations where definiteness must be made explicit.

Slovak nouns are inflected for case and number. There are six cases: nominative, genitive, dative, accusative, locative, and instrumental. The vocative is no longer morphologically marked. There are two numbers: singular and plural. Nouns have inherent gender. There are three genders: masculine, feminine, and neuter. Adjectives and pronouns must agree with nouns in case, number, and gender.

The numerals 0–10 have unique forms, with numerals 1–4 requiring specific gendered representations. Numerals 11–19 are formed by adding to the end of each numeral. The suffix is used to create numerals 20, 30 and 40; for numerals 50, 60, 70, 80 and 90, is used. Compound numerals (21, 1054) are combinations of these words formed in the same order as their mathematical symbol is written (e.g. 21 = , literally "twenty-one").

The numerals are as follows:
Some higher numbers: (200) , (300) , (900) , (1,000) , (1,100) , (2,000) , (100,000) , (200,000) , (1,000,000) , (1,000,000,000) .

Counted nouns have two forms. The most common form is the plural genitive (e.g. = five houses or = one hundred two women), while the plural form of the noun when counting the amounts of 2–4, etc., is usually the nominative form without counting (e.g. = two houses or = two women) but gender rules do apply in many cases.

Verbs have three major conjugations. Three persons and two numbers (singular and plural) are distinguished. Several conjugation paradigms exist as follows:













Adverbs are formed by replacing the adjectival ending with the ending - or - / -. Sometimes both - and - are possible. Examples include the following:

The comparative/superlative of adverbs is formed by replacing the adjectival ending with a comparative/superlative ending - or -. Examples include the following:

Each preposition is associated with one or more grammatical cases. The noun governed by a preposition must appear in the case required by the preposition in the given context (e.g. from friends = ). is the genitive case of . It must appear in this case because the preposition (= from) always calls for its objects to be in the genitive.

The Slovak language is a descendant of Proto-Slavic, itself a descendant of Proto-Indo-European. It is closely related to the other West Slavic languages, primarily to Czech and Polish. Czech also influenced the language in its later development. The highest number of borrowings in the old Slovak vocabulary come from Latin, German, Czech, Hungarian, Polish and Greek (in that order). Recently, it is also influenced by English.

Although most dialects of Czech and Slovak are mutually intelligible (see Comparison of Slovak and Czech), eastern Slovak dialects are less intelligible to speakers of Czech and closer to Polish, Ruthenian and Ukrainian and contact between speakers of Czech and speakers of the eastern dialects is limited.

Since the dissolution of Czechoslovakia it has been permitted to use Czech in TV broadcasting and during court proceedings (Administration Procedure Act 99/1963 Zb.). From 1999 to August 2009, the Minority Language Act 184/1999 Z.z., in its section (§) 6, contained the variously interpreted unclear provision saying that "When applying this act, it holds that the use of the Czech language fulfills the requirement of fundamental intelligibility with the state language"; the state language is Slovak and the Minority Language Act basically refers to municipalities with more than 20% ethnic minority population (no such Czech municipalities are found in Slovakia). Since 1 September 2009 (due to an amendment to the State Language Act 270/1995 Z.z.) a language "fundamentally intelligible with the state language" (i.e. the Czech language) may be used in contact with state offices and bodies by its native speakers, and documents written in it and issued by bodies in the Czech Republic are officially accepted. Regardless of its official status, Czech is used commonly both in Slovak mass media and in daily communication by Czech natives as an equal language.

Czech and Slovak have a long history of interaction and mutual influence well before the creation of Czechoslovakia in 1918, a state which existed until 1993. Literary Slovak shares significant orthographic features with Czech, as well as technical and professional terminology dating from the Czechoslovak period, but phonetic, grammatical, and vocabulary differences do exist.

Slavic language varieties are relatively closely related, and have had a large degree of mutual influence, due to the complicated ethnopolitical history of their historic ranges. This is reflected in the many features Slovak shares with neighboring language varieties. Standard Slovak shares high degrees of mutual intelligibility with many Slavic varieties. Despite this closeness to other Slavic varieties, significant variation exists among Slovak dialects. In particular, eastern varieties differ significantly from the standard language, which is based on central and western varieties.

Eastern Slovak dialects have the greatest degree of mutual intelligibility with Polish of all the Slovak dialects, followed by Rusyn, but both Eastern Slovak and Rusyn lack familiar technical terminology and upper register expressions. Polish and Sorbian also differ quite considerably from Czech and Slovak in upper registers, but non-technical and lower register speech is readily intelligible. Some mutual intelligibility occurs with spoken Rusyn, Ukrainian, and even Russian (in this order), although their orthographies are based on the Cyrillic script.


Sports:

Food:

Clothing:

Exclamations:

Nouns:

Verbs:

Greetings:

Hungarians and Slovaks have had a language interaction ever since the settlement of Hungarians in the Carpathian area. Hungarians also adopted many words from various Slavic languages related to agriculture and administration, and a number of Hungarian loanwords are found in Slovak. Some examples are as follows:

There are many Slovak dialects, which are divided into the following four basic groups:

The fourth group of dialects is often not considered a separate group, but a subgroup of Central and Western Slovak dialects (see e.g. Štolc, 1968), but it is currently undergoing changes due to contact with surrounding languages (Serbo-Croatian, Romanian, and Hungarian) and long-time geographical separation from Slovakia (see the studies in "Zborník Spolku vojvodinských slovakistov", e.g. Dudok, 1993).

For an external map of the three groups in Slovakia see here.

The dialect groups differ mostly in phonology, vocabulary, and tonal inflection. Syntactic differences are minor. Central Slovak forms the basis of the present-day standard language. Not all dialects are fully mutually intelligible. It may be difficult for an inhabitant of the western Slovakia to understand a dialect from eastern Slovakia and the other way around.
The dialects are fragmented geographically, separated by numerous mountain ranges. The first three groups already existed in the 10th century. All of them are spoken by the Slovaks outside Slovakia (USA, Canada, Croatian Slavonia, and elsewhere), and central and western dialects form the basis of the lowland dialects (see above).

The western dialects contain features common with the Moravian dialects in the Czech Republic, the southern central dialects contain a few features common with South Slavic languages, and the eastern dialects a few features common with Polish and the East Slavonic languages (cf. Štolc, 1994). Lowland dialects share some words and areal features with the languages surrounding them (Serbo-Croatian, Hungarian, and Romanian).

Standard Slovak () is defined by an Act of Parliament on the State Language of the Slovak Republic (language law). According to this law, Ministry of Culture approves and publishes the codified form of the Slovak language based on the judgment of specialised Slovakistic linguistic institutes and specialists in the area of the state language. This is traditionally Ľudovit Štúr Institute of Linguistics, which is part of the Slovak Academy of Sciences. In practice, Ministry of Culture publishes a document that specifies authoritative reference books for standard Slovak usage. There are four such publications:





</doc>
<doc id="28142" url="https://en.wikipedia.org/wiki?curid=28142" title="Supercluster">
Supercluster

A supercluster is a large group of smaller galaxy clusters or galaxy groups; it is among the largest known structures of the universe. The Milky Way is part of the Local Group galaxy group (which contains more than 54 galaxies), which in turn is part of the Virgo Supercluster, which is part of the Laniakea Supercluster. The large size and low density of superclusters means that they, unlike clusters, expand with the Hubble expansion. The number of superclusters in the observable universe is estimated to be 10 million.

The existence of superclusters indicates that the galaxies in the Universe are not uniformly distributed; most of them are drawn together in groups and clusters, with groups containing up to some dozens of galaxies and clusters up to several thousand galaxies. Those groups and clusters and additional isolated galaxies in turn form even larger structures called superclusters.

Their existence was first postulated by George Abell in his 1958 Abell catalogue of galaxy clusters. He called them "second-order clusters", or clusters of clusters.

Superclusters form massive structures of galaxies, called "filaments", "supercluster complexes", "walls" or "sheets", that may span between several hundred million light-years to 10 billion light-years, covering more than 5% of the observable universe. These are the largest structures known to date. Observations of superclusters can give information about the initial condition of the universe, when these superclusters were created. The directions of the rotational axes of galaxies within superclusters are studied by those who believe that they may give insight and information into the early formation process of galaxies in the history of the Universe.
Interspersed among superclusters are large voids of space where few galaxies exist. Superclusters are frequently subdivided into groups of clusters called galaxy groups and clusters.

Although superclusters are supposed to be the largest structures in the universe, according to the Cosmological principle, larger structures have been observed in surveys, including the Sloan Great Wall.



</doc>
<doc id="28143" url="https://en.wikipedia.org/wiki?curid=28143" title="Salicylic acid">
Salicylic acid

Salicylic acid (from Latin "salix", "willow tree") is a lipophilic monohydroxybenzoic acid, a type of phenolic acid, and a beta hydroxy acid (BHA). It has the formula CHO. This colorless crystalline organic acid is widely used in organic synthesis and functions as a plant hormone. It is derived from the metabolism of salicin. 

In addition to serving as an important active metabolite of aspirin ("acetylsalicylic acid"), which acts in part as a prodrug to salicylic acid, it is probably best known for its use as a key ingredient in topical anti-acne products. The salts and esters of salicylic acid are known as salicylates.
It is on the World Health Organization's List of Essential Medicines, the safest and most effective medicines needed in a health system.

Salicylic acid as a medication is used most commonly to help remove the outer layer of the skin. As such, it is used to treat warts, psoriasis, acne, ringworm, dandruff, and ichthyosis. 

Similar to other hydroxy acids, salicylic acid is a key ingredient in many skincare products for the treatment of seborrhoeic dermatitis, acne, psoriasis, calluses, corns, keratosis pilaris, acanthosis nigricans, ichthyosis and warts.

Salicylic acid is used in the production of other pharmaceuticals, including 4-aminosalicylic acid, sandulpiride, and landetimide (via Salethamide).

Salicylic acid was one of the original starting materials for making acetylsalicylic acid (aspirin) in 1897.

Bismuth subsalicylate, a salt of bismuth and salicylic acid, is the active ingredient in stomach relief aids such as Pepto-Bismol, is the main ingredient of Kaopectate and "displays anti-inflammatory action (due to salicylic acid) and also acts as an antacid and mild antibiotic".

Other derivatives include methyl salicylate used as a liniment to soothe joint and muscle pain and choline salicylate used topically to relieve the pain of mouth ulcers.

Salicylic acid is used as a food preservative, a bactericidal and an antiseptic.

Sodium salicylate is a useful phosphor in the vacuum ultraviolet spectral range, with nearly flat quantum efficiency for wavelengths between 10 and 100 nm. It fluoresces in the blue at 420 nm. It is easily prepared on a clean surface by spraying a saturated solution of the salt in methanol followed by evaporation.

Aspirin (acetylsalicylic acid or ASA) can be prepared by the esterification of the phenolic hydroxyl group of salicylic acid with the acetyl group from acetic anhydride or acetyl chloride. 

Salicylic acid modulates COX2 gene expression to decrease the formation of pro-inflammatory prostaglandins. Salicylate may competitively inhibit prostaglandin formation. Salicylate's antirheumatic (nonsteroidal anti-inflammatory) actions are a result of its analgesic and anti-inflammatory mechanisms. 

Salicylic acid works by causing the cells of the epidermis to slough off more readily, preventing pores from clogging up, and allowing room for new cell growth. Salicylic acid inhibits the oxidation of uridine-5-diphosphoglucose (UDPG) competitively with nicotinamide adenosine dinucleotide (NAD) and noncompetitively with UDPG. It also competitively inhibits the transferring of glucuronyl group of uridine-5-phosphoglucuronic acid (UDPGA) to the phenolic acceptor. 

The wound-healing retardation action of salicylates is probably due mainly to its inhibitory action on mucopolysaccharide synthesis.

17% to 27% salicylic acid used in the form of a paint, and 20% to 50% in plaster form, which are sold for wart and corn removal should not be applied to the face and should not be used for acne treatment. Even for wart removal, such a solution should be applied once or twice a day – more frequent use may lead to an increase in side-effects without an increase in efficacy.

If high concentrations of salicylic ointment are applied to a large percentage of body surface, high levels of salicylic acid can enter the blood, requiring hemodialysis to avoid further complications.

Salicylic acid has the formula CH(OH)COOH, where the OH group is "ortho" to the carboxyl group. It is also known as 2-hydroxybenzoic acid. It is poorly soluble in water (2 g/L at 20 °C). 

Salicylic acid is biosynthesized from the amino acid phenylalanine. In "Arabidopsis thaliana" it can be synthesized via a phenylalanine-independent pathway.

Sodium salicylate is commercially prepared by treating sodium phenolate (the sodium salt of phenol) with carbon dioxide at high pressure (100 atm) and high temperature (115°C) – a method known as the Kolbe-Schmitt reaction. Acidification of the product with sulfuric acid gives salicylic acid:

It can also be prepared by the hydrolysis of aspirin (acetylsalicylic acid) or methyl salicylate (oil of wintergreen) with a strong acid or base.

Salicylic acid degrades to phenol and carbon dioxide at 200 - 230°C: 

Hippocrates, Galen, Pliny the Elder and others knew that willow bark could ease pain and reduce fevers. It was used in Europe and China to treat these conditions. This remedy is mentioned in texts from ancient Egypt, Sumer and Assyria. The Cherokee and other Native Americans use an infusion of the bark for fever and other medicinal purposes.

In 2014, archaeologists identified traces of salicylic acid on 7th century pottery fragments found in east central Colorado. The Reverend Edward Stone, a vicar from Chipping Norton, Oxfordshire, England, noted in 1763 that the bark of the willow was effective in reducing a fever.

The active extract of the bark, called "salicin", after the Latin name for the white willow ("Salix alba"), was isolated and named by the German chemist Johann Andreas Buchner in 1828. A larger amount of the substance was isolated in 1829 by Henri Leroux, a French pharmacist. Raffaele Piria, an Italian chemist, was able to convert the substance into a sugar and a second component, which on oxidation becomes salicylic acid.

Salicylic acid was also isolated from the herb meadowsweet ("Filipendula ulmaria", formerly classified as "Spiraea ulmaria") by German researchers in 1839. While their extract was somewhat effective, it also caused digestive problems such as gastric irritation, bleeding, diarrhea and even death when consumed in high doses.

Salicylic acid occurs in plants as free salicylic acid and its carboxylated esters and phenolic glycosides. Several studies suggest that humans metabolize salicylic acid in measurable quantities from these plants. High-salicylate beverages and foods include beer, coffee, tea, numerous fruits and vegetables, sweet potato, nuts, and olive oil, among others. Meat, poultry, fish, eggs, dairy products, sugar, and breads and cereals have low salicylate content.

Some people with sensitivity to dietary salicylates may have symptoms of allergic reaction, such as bronchial asthma, rhinitis, gastrointestinal disorders, or diarrhea, and so may need to adopt a low-salicylate diet.

Salicylic acid is a phenolic phytohormone and is found in plants with roles in plant growth and development, photosynthesis, transpiration, ion uptake and transport. Salicylic acid is involved in endogenous signaling, mediating in plant defense against pathogens. It plays a role in the resistance to pathogens by inducing the production of pathogenesis-related proteins. 

It is involved in the systemic acquired resistance in which a pathogenic attack on one part of the plant induces resistance in other parts. The signal can also move to nearby plants by salicylic acid being converted to the volatile ester methyl salicylate.Methyl salicylate is taken up by the stomata of the nearby plant, and once deep in the leaf, is converted back to salicylic acid to induce the immune response.



</doc>
<doc id="28144" url="https://en.wikipedia.org/wiki?curid=28144" title="Seaborgium">
Seaborgium

Seaborgium is a synthetic chemical element with the symbol Sg and atomic number 106. It is named after the American nuclear chemist Glenn T. Seaborg. As a synthetic element, it can be created in a laboratory but is not found in nature. It is also radioactive; the most stable known isotope, Sg, has a half-life of approximately 14 minutes.

In the periodic table of the elements, it is a d-block transactinide element. It is a member of the 7th period and belongs to the group 6 elements as the fourth member of the 6d series of transition metals. Chemistry experiments have confirmed that seaborgium behaves as the heavier homologue to tungsten in group 6. The chemical properties of seaborgium are characterized only partly, but they compare well with the chemistry of the other group 6 elements.

In 1974, a few atoms of seaborgium were produced in laboratories in the Soviet Union and in the United States. The priority of the discovery and therefore the naming of the element was disputed between Soviet and American scientists, and it was not until 1997 that International Union of Pure and Applied Chemistry (IUPAC) established seaborgium as the official name for the element. It is one of only two elements named after a living person at the time of naming, the other being oganesson, element 118.

Following claims of the observation of elements 104 and 105 in 1970 by Albert Ghiorso et al. at the Lawrence Livermore National Laboratory, a search for element 106 using oxygen-18 projectiles and the previously used californium-249 target was conducted. Several 9.1 MeV alpha decays were reported and are now thought to originate from element 106, though this was not confirmed at the time. In 1972, the HILAC accelerator received equipment upgrades, preventing the team from repeating the experiment, and data analysis was not done during the shutdown. This reaction was tried again several years later, in 1974, and the Berkeley team realized that their new data agreed with their 1971 data, to the astonishment of Ghiorso. Hence, element 106 could have actually been discovered in 1971 if the original data was analyzed more carefully.

Two groups claimed discovery of the element. Unambiguous evidence of element 106 was first reported in 1974 by a Russian research team in Dubna led by Yuri Oganessian, in which targets of lead-208 and lead-207 were bombarded with accelerated ions of chromium-54. In total, fifty-one spontaneous fission events were observed with a half-life between four and ten milliseconds. After having ruled out nucleon transfer reactions as a cause for these activities, the team concluded that the most likely cause of the activities was the spontaneous fission of isotopes of element 106. The isotope in question was first suggested to be seaborgium-259, but was later corrected to seaborgium-260.

A few months later in 1974, researchers including Glenn T. Seaborg, Carol Alonso and Albert Ghiorso at the University of California, Berkeley, and E. Kenneth Hulet from the Lawrence Livermore National Laboratory, also synthesized the element by bombarding a californium-249 target with oxygen-18 ions, using equipment similar to that which had been used for the synthesis of element 104 five years earlier, observing at least seventy alpha decays, seemingly from the isotope seaborgium-263m with a half-life of seconds. The alpha daughter rutherfordium-259 and granddaughter nobelium-255 had previously been synthesised and the properties observed here matched with those previously known, as did the intensity of their production. The cross-section of the reaction observed, 0.3 nanobarns, also agreed well with theoretical predictions. These bolstered the assignment of the alpha decay events to seaborgium-263m.

A dispute thus arose from the initial competing claims of discovery, though unlike the case of the synthetic elements up to element 105, neither team of discoverers chose to announce proposed names for the new elements, thus averting an element naming controversy temporarily. The dispute on discovery, however, dragged on until 1992, when the IUPAC/IUPAP Transfermium Working Group (TWG), formed to put an end to the controversy by making conclusions regarding discovery claims for elements 101 to 112, concluded that the Soviet synthesis of seaborgium-260 was not convincing enough, "lacking as it is in yield curves and angular selection results", whereas the American synthesis of seaborgium-263 was convincing due to its being firmly anchored to known daughter nuclei. As such, the TWG recognised the Berkeley team as official discoverers in their 1993 report.

Seaborg had previously suggested to the TWG that if Berkeley was recognised as the official discoverer of elements 104 and 105, they might propose the name "kurchatovium" (symbol Kt) for element 106 to honour the Dubna team, which had proposed this name for element 104 after Igor Kurchatov, the former head of the Soviet nuclear research programme. However, due to the worsening relations between the competing teams after the publication of the TWG report (because the Berkeley team vehemently disagreed with the TWG's conclusions, especially regarding element 104), this proposal was dropped from consideration by the Berkeley team. After being recognized as official discoverers, the Berkeley team started deciding on a name in earnest:

Seaborg's son Eric remembered the naming process as follows:

The name "seaborgium" and symbol "Sg" were announced at the 207th national meeting of the American Chemical Society in March 1994 by Kenneth Hulet, one of the co-discovers. However, IUPAC resolved in August 1994 that an element could not be named after a living person, and Seaborg was still alive at the time. Thus, in September 1994, IUPAC recommended a set of names in which the names proposed by the three laboratories (the third being the GSI Helmholtz Centre for Heavy Ion Research in Darmstadt, Germany) with competing claims to the discovery for elements 104 to 109 were shifted to various other elements, in which "rutherfordium" (Rf), the Berkeley proposal for element 104, was shifted to element 106, with "seaborgium" being dropped entirely as a name.

This decision ignited a firestorm of worldwide protest for disregarding the historic discoverer's right to name new elements, and against the new retroactive rule against naming elements after living persons; the American Chemical Society stood firmly behind the name "seaborgium" for element 106, together with all the other American and German naming proposals for elements 104 to 109, approving these names for its journals in defiance of IUPAC. At first, IUPAC defended itself, with an American member of its committee writing: "Discoverers don't have a right to name an element. They have a right to suggest a name. And, of course, we didn't infringe on that at all." However, Seaborg responded:

Bowing to public pressure, IUPAC proposed a different compromise in August 1995, in which the name "seaborgium" was reinstated for element 106 in exchange for the removal of all but one of the other American proposals, which met an even worse response. Finally, IUPAC rescinded these previous compromises and made a final, new recommendation in August 1997, in which the American and German proposals for elements 104 to 109 were all adopted, including "seaborgium" for element 106, with the single exception of element 105, named "dubnium" to recognise the contributions of the Dubna team to the experimental procedures of transactinide synthesis. This list was finally accepted by the American Chemical Society, which wrote:

Seaborg commented regarding the naming:

Seaborg died a year and a half later, on 25 February 1999, at the age of 86.

Superheavy elements such as seaborgium are produced by bombarding lighter elements in particle accelerators that induces fusion reactions. Whereas most of the isotopes of seaborgium can be synthesized directly this way, some heavier ones have only been observed as decay products of elements with higher atomic numbers.

Depending on the energies involved, fusion reactions that generate superheavy elements are separated into "hot" and "cold". In hot fusion reactions, very light, high-energy projectiles are accelerated toward very heavy targets (actinides), giving rise to compound nuclei at high excitation energy (~40–50 MeV) that may either fission or evaporate several (3 to 5) neutrons. In cold fusion reactions, the produced fused nuclei have a relatively low excitation energy (~10–20 MeV), which decreases the probability that these products will undergo fission reactions. As the fused nuclei cool to the ground state, they require emission of only one or two neutrons, and thus, allows for the generation of more neutron-rich products. The latter is a distinct concept from that of where nuclear fusion claimed to be achieved at room temperature conditions (see cold fusion).

Seaborgium has no stable or naturally occurring isotopes. Several radioactive isotopes have been synthesized in the laboratory, either by fusing two atoms or by observing the decay of heavier elements. Twelve different isotopes of seaborgium have been reported with atomic masses 258–267, 269, and 271, three of which, seaborgium-261, 263, and 265, have known metastable states. All of these decay only through alpha decay and spontaneous fission, with the single exception of seaborgium-261 that can also undergo electron capture to dubnium-261.

There is a trend toward increasing half-lives for the heavier isotopes; thus the heaviest three known isotopes, Sg, Sg, and Sg, are also the longest-lived, having half-lives in minutes. Some other isotopes in this region are predicted to have comparable or even longer half-lives. Additionally, Sg, Sg, and Sg have half-lives measured in seconds. All the remaining isotopes have half-lives measured in milliseconds, with the exception of the shortest-lived isotope, Sg, with a half-life of only 92 microseconds.

The proton-rich isotopes from Sg to Sg were directly produced by cold fusion; all heavier isotopes were produced from the repeated alpha decay of the heavier elements hassium, darmstadtium, and flerovium, with the exceptions of the isotopes Sg, Sg, Sg, and Sg, which were directly produced by hot fusion through irradiation of actinide targets. The twelve isotopes of seaborgium have half-lives ranging from 92 microseconds for Sg to 14 minutes for Sg.

Very few properties of seaborgium or its compounds have been measured; this is due to its extremely limited and expensive production and the fact that seaborgium (and its parents) decays very quickly. A few singular chemistry-related properties have been measured, but properties of seaborgium metal remain unknown and only predictions are available.

Seaborgium is expected to be a solid under normal conditions and assume a body-centered cubic crystal structure, similar to its lighter congener tungsten. It should be a very heavy metal with a density of around 35.0 g/cm, which would be the fourth-highest of any of the 118 known elements, lower only than bohrium (37.1 g/cm), meitnerium (37.4 g/cm) and hassium (41 g/cm), the three following elements in the periodic table. In comparison, the densest known element that has had its density measured, osmium, has a density of only 22.61 g/cm. This results from seaborgium's high atomic weight, the lanthanide and actinide contractions, and relativistic effects, although production of enough seaborgium to measure this quantity would be impractical, and the sample would quickly decay.

Seaborgium is the fourth member of the 6d series of transition metals and the heaviest member of group 6 in the periodic table, below chromium, molybdenum, and tungsten. All the members of the group form a diversity of oxoanions. They readily portray their group oxidation state of +6, although this is highly oxidising in the case of chromium, and this state becomes more and more stable to reduction as the group is descended: indeed, tungsten is the last of the 5d transition metals where all four 5d electrons participate in metallic bonding. As such, seaborgium should have +6 as its most stable oxidation state, both in the gas phase and in aqueous solution, and this is the only oxidation state that is experimentally known for it; the +5 and +4 states should be less stable, and the +3 state, the most common for chromium, would be the least stable for seaborgium.

This stabilisation of the highest oxidation state occurs in the early 6d elements because of the similarity between the energies of the 6d and 7s orbitals, since the 7s orbitals are relativistically stabilised and the 6d orbitals are relativistically destabilised. This effect is so large in the seventh period that seaborgium is expected to lose its 6d electrons before its 7s electrons (Sg, [Rn]5f6d7s; Sg, [Rn]5f6d7s; Sg, [Rn]5f6d7s; Sg, [Rn]5f6d; Sg, [Rn]5f). Because of the great destabilisation of the 7s orbital, Sg should be even more unstable than W and should be very readily oxidised to Sg. The predicted ionic radius of the hexacoordinate Sg ion is 65 pm, while the predicted atomic radius of seaborgium is 128 pm. Nevertheless, the stability of the highest oxidation state is still expected to decrease as Lr > Rf > Db > Sg. Some predicted standard reduction potentials for seaborgium ions in aqueous acidic solution are as follows:

Seaborgium should form a very volatile hexafluoride (SgF) as well as a moderately volatile hexachloride (SgCl), pentachloride (SgCl), and oxychlorides SgOCl and SgOCl. SgOCl is expected to be the most stable of the seaborgium oxychlorides and to be the least volatile of the group 6 oxychlorides, with the sequence MoOCl > WOCl > SgOCl. The volatile seaborgium(VI) compounds SgCl and SgOCl are expected to be unstable to decomposition to seaborgium(V) compounds at high temperatures, analogous to MoCl and MoOCl; this should not happen for SgOCl due to the much higher energy gap between the highest occupied and lowest unoccupied molecular orbitals, despite the similar Sg–Cl bond strengths (similarly to molybdenum and tungsten).

Molybdenum and tungsten are very similar to each other and show important differences to the smaller chromium, and seaborgium is expected to follow the chemistry of tungsten and molybdenum quite closely, forming an even greater variety of oxoanions, the simplest among them being seaborgate, , which would form from the rapid hydrolysis of , although this would take place less readily than with molybdenum and tungsten as expected from seaborgium's greater size. Seaborgium should hydrolyse less readily than tungsten in hydrofluoric acid at low concentrations, but more readily at high concentrations, also forming complexes such as SgOF and : complex formation competes with hydrolysis in hydrofluoric acid.

Experimental chemical investigation of seaborgium has been hampered due to the need to produce it one atom at a time, its short half-life, and the resulting necessary harshness of the experimental conditions. The isotope Sg and its isomer Sg are advantageous for radiochemistry: they are produced in the Cm(Ne,5n) reaction.

In the first experimental chemical studies of seaborgium in 1995 and 1996, seaborgium atoms were produced in the reaction Cm(Ne,4n)Sg, thermalised, and reacted with an O/HCl mixture. The adsorption properties of the resulting oxychloride were measured and compared with those of molybdenum and tungsten compounds. The results indicated that seaborgium formed a volatile oxychloride akin to those of the other group 6 elements, and confirmed the decreasing trend of oxychloride volatility down group 6:

In 2001, a team continued the study of the gas phase chemistry of seaborgium by reacting the element with O in a HO environment. In a manner similar to the formation of the oxychloride, the results of the experiment indicated the formation of seaborgium oxide hydroxide, a reaction well known among the lighter group 6 homologues as well as the pseudohomologue uranium.

Predictions on the aqueous chemistry of seaborgium have largely been confirmed. In experiments conducted in 1997 and 1998, seaborgium was eluted from cation-exchange resin using a HNO/HF solution, most likely as neutral SgOF or the anionic complex ion [SgOF] rather than . In contrast, in 0.1 M nitric acid, seaborgium does not elute, unlike molybdenum and tungsten, indicating that the hydrolysis of [Sg(HO)] only proceeds as far as the cationic complex [Sg(OH)(HO)] or [Sg(OH)(HO)], while that of molybdenum and tungsten proceeds to neutral [MO(OH))].

The only other oxidation state known for seaborgium other than the group oxidation state of +6 is the zero oxidation state. Similarly to its three lighter congeners, forming chromium hexacarbonyl, molybdenum hexacarbonyl, and tungsten hexacarbonyl, seaborgium has been shown in 2014 to also form seaborgium hexacarbonyl, Sg(CO). Like its molybdenum and tungsten homologues, seaborgium hexacarbonyl is a volatile compound that reacts readily with silicon dioxide.




</doc>
<doc id="28145" url="https://en.wikipedia.org/wiki?curid=28145" title="September 15">
September 15





</doc>
<doc id="28146" url="https://en.wikipedia.org/wiki?curid=28146" title="September 18">
September 18





</doc>
<doc id="28147" url="https://en.wikipedia.org/wiki?curid=28147" title="September 19">
September 19





</doc>
<doc id="28148" url="https://en.wikipedia.org/wiki?curid=28148" title="September 20">
September 20





</doc>
<doc id="28149" url="https://en.wikipedia.org/wiki?curid=28149" title="Serpens">
Serpens

Serpens ("the Serpent", Greek ) is a constellation of the northern hemisphere. One of the 48 constellations listed by the 2nd-century astronomer Ptolemy, it remains one of the 88 modern constellations defined by the International Astronomical Union. It is unique among the modern constellations in being split into two non-contiguous parts, Serpens Caput (Serpent Head) to the west and Serpens Cauda (Serpent Tail) to the east. Between these two halves lies the constellation of Ophiuchus, the "Serpent-Bearer". In figurative representations, the body of the serpent is represented as passing behind Ophiuchus between Mu Serpentis in "Serpens Caput" and Nu Serpentis in "Serpens Cauda".

The brightest star in Serpens is the red giant star Alpha Serpentis, or Unukalhai, in Serpens Caput, with an apparent magnitude of 2.63. Also located in Serpens Caput are the naked-eye globular cluster Messier 5 and the naked-eye variables R Serpentis and Tau Serpentis. Notable extragalactic objects include Seyfert's Sextet, one of the densest galaxy clusters known; Arp 220, the prototypical ultraluminous infrared galaxy; and Hoag's Object, the most famous of the very rare class of galaxies known as ring galaxies.

Part of the Milky Way's galactic plane passes through Serpens Cauda, which is therefore rich in galactic deep-sky objects, such as the Eagle Nebula (IC 4703) and its associated star cluster Messier 16. The nebula measures 70 light-years by 50 light-years and contains the Pillars of Creation, three dust clouds that became famous for the image taken by the Hubble Space Telescope. Other striking objects include the Red Square Nebula, one of the few objects in astronomy to take on a square shape; and Westerhout 40, a massive nearby star-forming region consisting of a molecular cloud and an H II region.

In Greek mythology, Serpens represents a snake held by the healer Asclepius. Represented in the sky by the constellation Ophiuchus, Asclepius once killed a snake, but the animal was subsequently resurrected after a second snake placed a revival herb on it before its death. As snakes shed their skin every year, they were known as the symbol of rebirth in ancient Greek society, and legend says Asclepius would revive dead humans using the same technique he witnessed. Although this is likely the logic for Serpens' presence with Ophiuchus, the true reason is still not fully known. Sometimes, Serpens was depicted as coiling around Ophiuchus, but the majority of atlases showed Serpens passing either behind Ophiuchus' body or between his legs.

In some ancient atlases, the constellations Serpens and Ophiuchus were depicted as two separate constellations, although more often they were shown as a single constellation. One notable figure to depict Serpens separately was Johann Bayer; thus, Serpens' stars are cataloged with separate Bayer designations from those of Ophiuchus. When Eugène Delporte established modern constellation boundaries in the 1920s, he elected to depict the two separately. However, this posed the problem of how to disentangle the two constellations, with Deporte deciding to split Serpens into two areas—the head and the tail—separated by the continuous Ophiuchus. These two areas became known as Serpens Caput and Serpens Cauda, "caput" being the Latin word for head and "cauda" the Latin word for tail.

In Chinese astronomy, most of the stars of Serpens represented part of a wall surrounding a marketplace, known as Tianshi, which was in Ophiuchus and part of Hercules. Serpens also contains a few Chinese constellations. Two stars in the tail represented part of Shilou, the tower with the market office. Another star in the tail represented Liesi, jewel shops. One star in the head (Mu Serpentis) marked Tianru, the crown prince's wet nurse, or sometimes rain.

There were two "serpent" constellations in Babylonian astronomy, known as Mušḫuššu and Bašmu. It appears that Mušḫuššu was depicted as a hybrid of a dragon, a lion and a bird, and loosely corresponded to Hydra. Bašmu was a horned serpent (c.f. Ningishzida) and roughly corresponds to the Ὄφις constellation of Eudoxus of Cnidus on which the Ὄφις ("Serpens") of Ptolemy is based.

Serpens is the only one of the 88 modern constellations to be split into two disconnected regions in the sky: "Serpens Caput" (the head) and "Serpens Cauda" (the tail). The constellation is also unusual in that it depends on another constellation for context; specifically, it is being held by the Serpent Bearer Ophiuchus.

Serpens Caput is bordered by Libra to the south, Virgo and Boötes to the east, Corona Borealis to the north, and Ophiuchus and Hercules to the west; Serpens Cauda is bordered by Sagittarius to the south, Scutum and Aquila to the east, and Ophiuchus to the north and west. Covering 636.9 square degrees total, it ranks 23rd of the 88 constellations in size. It appears prominently in both the northern and southern skies during the Northern Hemisphere's summer. Its main asterism consists of 11 stars, and 108 stars in total are brighter than magnitude 6.5, the traditional limit for naked-eye visibility.

Serpens Caput's boundaries, as set by Eugène Delporte in 1930, are defined by a 10-sided polygon, while Serpens Cauda's are defined by a 22-sided polygon. In the equatorial coordinate system, the right ascension coordinates of Serpens Caput's borders lie between and , while the declination coordinates are between and . Serpens Cauda's boundaries lie between right ascensions of and and declinations of and . The International Astronomical Union (IAU) adopted the three-letter abbreviation "Ser" for the constellation in 1922.

Marking the heart of the serpent is the constellation's brightest star, Alpha Serpentis. Traditionally called Unukalhai, is a red giant of spectral type K2III located approximately 23 parsecs distant with a visual magnitude of 2.630 ± 0.009, meaning it can easily be seen with the naked eye even in areas with substantial light pollution. A faint companion is in orbit around the red giant star, although it is not visible to the naked eye. Situated near Alpha is Lambda Serpentis, a magnitude 4.42 ± 0.05 star rather similar to the Sun positioned only 12 parsecs away. Another solar analog in Serpens is the primary of Psi Serpentis, a binary star located slightly further away at approximately 14 parsecs.

Beta, Gamma, and Iota Serpentis form a distinctive triangular shape marking the head of the snake, with Kappa Serpentis (the proper name is Gudja) being roughly midway between Gamma and Iota. The brightest of the four with an apparent magnitude of roughly 3.67, Beta Serpentis is a white main-sequence star roughly 160 parsecs distant. It is likely that a nearby 10th-magnitude star is physically associated with Beta, although it is not certain. The Mira variable R Serpentis, situated between Beta and Gamma, is visible to the naked eye at its maximum of 5th-magnitude, but, typical of Mira variables, it can fade to below magnitude 14. Gamma Serpentis itself is an F-type subgiant located only 11 parsecs distant and thus is quite bright, being of magnitude 3.84 ± 0.05. The star is known to show solar-like oscillations.

Delta Serpentis, forming part of the body of the snake between the heart and the head, is a multiple star system positioned around 70 parsecs from Earth. Consisting of four stars, the system has a total apparent magnitude of 3.79 as viewed from Earth, although two of the stars, with a combined apparent magnitude of 3.80, provide nearly all the light. The primary, a white subgiant, is a Delta Scuti variable with an average apparent magnitude of 4.23. Positioned very near Delta, both in the night sky and likely in actual space at an estimated distance of around 70 parsecs, is the barium star 16 Serpentis. Another notable variable star visible to the naked eye is Chi Serpentis, an Alpha² Canum Venaticorum variable situated midway between Delta and Beta which varies from its median brightness of 5.33 by 0.03 magnitudes over a period of approximately 1.5 days.

The two stars in Serpens Caput that form part of the Snake's body below the heart are Epsilon and Mu Serpentis, both third-magnitude A-type main-sequence stars. Both have a peculiarity: Epsilon is an Am star, while Mu is a binary. Located slightly northwest of Mu is 36 Serpentis, another A-type main-sequence star. This star also has a peculiarity; it is a binary with the primary component being a Lambda Boötis star, meaning that it has solar-like amounts of carbon, nitrogen, and oxygen, while containing very low amounts of iron peak elements. 25 Serpentis, positioned a few degrees northeast of Mu Serpentis, is a spectroscopic binary consisting of a hot B-type giant and an A-type main-sequence star. The primary is a slowly pulsating B star, which causes the system to vary by 0.03 magnitudes.

Serpens Caput contains many RR Lyrae variables, although most are too faint to be seen without professional photography. The brightest is VY Serpentis, only of 10th magnitude. This star's period has been increasing by approximately 1.2 seconds per century. A variable star of a different kind is Tau Serpentis, a cool red giant that pulsates between magnitudes 5.89 and 7.07 in 87 days. This star has been found to display an inverse P Cygni profile, where cold infalling gas on to the star creates redshifted hydrogen absorption lines next to the normal emission lines.

Several stars in Serpens have been found to have planets. The brightest, Omega Serpentis, located between Epsilon and Mu, is an orange giant with a planet of at least 1.7 Jupiter-masses. NN Serpentis, an eclipsing post-common-envelope binary consisting of a white dwarf and a red dwarf, is very likely to have two planets causing variations in the period of the eclipses. Although it does not have a planet, the solar analog HD 137510 has been found to have a brown dwarf companion within the brown-dwarf desert.

PSR B1534+11 is a system consisting of two neutron stars orbiting each other, one of which is a pulsar with a period of 37.9 milliseconds. Situated approximately 1000 parsecs distant, the system was used to test Albert Einstein's theory of general relativity, validating the system's relativistic parameters to within 0.2% of values predicted by the theory. The X-ray emission from the system has been found to be present when the non-pulsar star intersects the equatorial pulsar wind of the pulsar, and the system's orbit has been found to vary slightly.

The brightest star in the tail, Eta Serpentis, is similar to Alpha Serpentis' primary in that it is a red giant of spectral class K. This star, however, is known to exhibit solar-like oscillations over a period of approximately 2.16 hours. The other two stars in Serpens Cauda forming its asterism are Theta and Xi Serpentis. Xi, where the asterism crosses over to Mu Serpentis in the head, is a triple star system located approximately 105 parsecs away. Two of the stars, with a combined apparent magnitude of around 3.5, form a spectroscopic binary with an angular separation of only 2.2 milliarcseconds, and thus cannot be resolved with modern equipment. The primary is a white giant with an excess of strontium. Theta, forming the tip of the tail, is also a multiple system, consisting of two A-type main-sequence stars with a combined apparent magnitude of around 4.1 separated by almost half an arcminute.

Lying near the boundary with Ophiuchus are Zeta, Nu, and Omicron Serpentis. All three are 4th-magnitude main-sequence stars, with Nu and Omicron being of spectral type A and Zeta being of spectral type F. Nu is a single star with a 9th-magnitude visual companion, while Omicron is a Delta Scuti variable with amplitude variations of 0.01 magnitudes. In 1909, the symbiotic nova RT Serpentis appeared near Omicron, although it only reached a maximum magnitude of 10.

The star system 59 Serpentis, also known as d Serpentis, is a triple star system consisting of a spectroscopic binary containing an A-type star and an orange giant and an orange giant secondary. The system shows irregular variations in brightness between magnitudes 5.17 and 5.2. In 1970, the nova FH Serpentis appeared just slightly north of 59 Serpentis, reaching a maximum brightness of 4.5. Also near 59 Serpentis in the Serpens Cloud are several Orion variables. MWC 297 is a Herbig Be star that in 1994 exhibited a large X-ray flare and increased in X-ray luminosity by five times before returning to the quiescent state. The star also appears to possess a circumstellar disk. Another Orion variable in the region is VV Serpentis, a Herbig Ae star that has been found to exhibit Delta Scuti pulsations. VV Serpentis has also, like MWC 297, been found to have a dusty disk surrounding it, and is also a UX Orionis star, meaning that it shows irregular variations in its brightness.

The star HR 6958, also known as MV Serpentis, is an Alpha Canum Venaticorum variable that is faintly visible to the naked eye. The star's metal abundance is ten times higher than the Sun for most metals at the iron peak and up to 1,000 times more for heavier elements. It has also been found to contain excess silicon. Barely visible to the naked eye is HD 172365, a likely post-blue straggler in the open cluster IC 4756 that contains a large excess of lithium. HD 172189, also located in IC 4756, is an Algol variable eclipsing binary with a 5.70 day period. The primary star in the system is also a Delta Scuti variable, undergoing multiple pulsation frequencies, which, combined with the eclipses, causes the system to vary by around a tenth of a magnitude.

As the galactic plane passes through it, Serpens Cauda contains many massive OB stars. Several of these are visible to the naked eye, such as NW Serpentis, an early Be star that has been found to be somewhat variable. The variability is interesting; according to one study, it could be one of the first discovered hybrids between Beta Cephei variables and slowly pulsating B stars. Although not visible to the naked eye, HD 167971 (MY Serpentis) is a Beta Lyrae variable triple system consisting of three very hot O-type stars. A member of the cluster NGC 6604, the two eclipsing stars are both blue giants, with one being of the very early spectral type O7.5III. The remaining star is either a blue giant or supergiant of a late O or early B spectral type. Also an eclipsing binary, the HD 166734 system consists of two O-type blue supergiants in orbit around each other. Less extreme in terms of mass and temperature is HD 161701, a spectroscopic binary consisting of a B-type primary and an Ap secondary, although it is the only known spectroscopic binary to consist of a star with excess of mercury and manganese and an Ap star.

South of the Eagle Nebula on the border with Sagittarius is the eclipsing binary W Serpentis, whose primary is a white giant that is interacting with the secondary. The system has been found to contain an accretion disk, and was one of the first discovered Serpentids, which are eclipsing binaries containing exceptionally strong far-ultraviolet spectral lines. It is suspected that such Serpentids are in an earlier evolutionary phase, and will evolve first into double periodic variables and then classical Algol variables. Also near the Eagle Nebula is the eclipsing Wolf–Rayet binary CV Serpentis, consisting of a Wolf–Rayet star and a hot O-type subgiant. The system is surrounded by a ring-shaped nebula, likely formed during the Wolf–Rayet phase of the primary. The eclipses of the system vary erratically, and although there are two theories as to why, neither of them is completely consistent with current understanding of stars.

Serpens Cauda contains a few X-ray binaries. One of these, GX 17+2, is a low-mass X-ray binary consisting of a neutron star and, as in all low-mass X-ray binaries, a low-mass star. The system has been classified as a Sco-like Z source, meaning that its accretion is near the Eddington limit. The system has also been found to approximately every 3 days brighten by around 3.5 K-band magnitudes, possibly due to the presence of a synchrotron jet. Another low-mass X-ray binary, Serpens X-1, undergoes occasional X-ray bursts. One in particular lasted nearly four hours, possibly explained by the burning of carbon in "a heavy element ocean".

As the galactic plane does not pass through this part of Serpens, a view to many galaxies beyond it is possible. However, a few structures of the Milky Way Galaxy are present in Serpens Caput, such as Messier 5, a globular cluster positioned approximately 8° southwest of α Serpentis, next to the star 5 Serpentis. Barely visible to the naked eye under good conditions, and is located approximately 25,000 ly distant. Messier 5 contains a large number of known RR Lyrae variable stars, and is receding from us at over 50 km/s. The cluster contains two millisecond pulsars, one of which is in a binary, allowing the proper motion of the cluster to be measured. The binary could help our understanding of neutron degenerate matter; the current median mass, if confirmed, would exclude any "soft" equation of state for such matter. The cluster has been used to test for magnetic dipole moments in neutrinos, which could shed light on some hypothetical particles such as the axion. Another globular cluster is Palomar 5, found just south of Messier 5. Many stars are leaving this globular cluster due to the Milky Way's gravity, forming a tidal tail over 30000 light-years long.

The L134/L183 is a dark nebula complex that, along with a third cloud, is likely formed by fragments of a single original cloud located 36 degrees away from the galactic plane, a large distance for dark nebulae. The entire complex is thought to be around 140 parsecs distant. L183, also referred to as L134N, is home to several infrared sources, indicating pre-stellar sources thought to present the first known observation of the contraction phase between cloud cores and prestellar cores. The core is split into three regions, with a combined mass of around 25 solar masses.

Outside of the Milky Way, there are no bright deep-sky objects for amateur astronomers in Serpens Caput, with nothing else above 10th magnitude. The brightest is NGC 5962, a spiral galaxy positioned around 28 megaparsecs distant with an apparent magnitude of 11.34. Slightly fainter is NGC 5921, a barred spiral galaxy with a LINER-type active galactic nucleus situated somewhat closer at a distance of 21 megaparsecs. A type II supernova was observed in this galaxy in 2001 and was designated SN 2001X. Fainter still are the spirals NGC 5964 and NGC 6118, with the latter being host to the supernova SN 2004dk.
Hoag's Object, located 600 million light-years from Earth, is a member of the very rare class of galaxies known as ring galaxies. The outer ring is largely composed of young blue stars while the core is made up of older yellow stars. The predominant theory regarding its formation is that the progenitor galaxy was a barred spiral galaxy whose arms had velocities too great to keep the galaxy's coherence and therefore detached. Arp 220 is another unusual galaxy in Serpens. The prototypical ultraluminous infrared galaxy, Arp 220 is somewhat closer than Hoag's Object at 250 million light-years from Earth. It consists of two large spiral galaxies in the process of colliding with their nuclei orbiting at a distance of 1,200 light-years, causing extensive star formation throughout both components. It possesses a large cluster of more than a billion stars, partially covered by thick dust clouds near one of the galaxies' core. Another interacting galaxy pair, albeit in an earlier stage, consists of the galaxies NGC 5953 and NGC 5954. In this case, both are active galaxies, with the former a Seyfert 2 galaxy and the latter a LINER-type galaxy. Both are undergoing a burst of star formation triggered by the interaction.

Seyfert's Sextet is a group of six galaxies, four of which are interacting gravitationally and two of which simply appear to be a part of the group despite their greater distance. The gravitationally bound cluster lies at a distance of 190 million light-years from Earth and is approximately 100,000 light-years across, making Seyfert's Sextet one of the densest galaxy group known. Astronomers predict that the four interacting galaxies will eventually merge to form a large elliptical galaxy. The radio source 3C 326 was originally though to emanate from a giant elliptical galaxy. However, in 1990, it was shown that the source is instead a brighter, smaller galaxy a few arcseconds north. This object, designated 3C 326 N, has enough gas for star formation, but is being inhibited due to the energy from the radio galaxy nucleus.

A much larger galaxy cluster is the redshift-0.0354 Abell 2063. The cluster is thought to be interacting with the nearby galaxy group MKW 3s, based on radial velocity measurements of galaxies and the positioning of the cD galaxy at the center of Abell 2063. The active galaxy at the center of MKW 3s—NGC 5920—appears to be creating a bubble of hot gas from its radio activity. Near the 5th-magnitude star Pi Serpentis lies AWM 4, a cluster containing an excess of metals in the intracluster medium. The central galaxy, NGC 6051, is a radio galaxy that is probably responsible for this enrichment. Similar to AWM 4, the cluster Abell 2052 has central cD radio galaxy, 3C 317. This radio galaxy is believed to have restarted after a period of inactivity less than 200 years ago. The galaxy has over 40,000 known globular clusters, the highest known total of any galaxy as of 2002.
Consisting of two quasars with a separation of less than 5 arcseconds, the quasar pair 4C 11.50 is one of the visually closest pairs of quasars in the sky. The two have markedly different redshifts, however, and are thus unrelated. The foreground member of the pair (4C 11.50 A) does not have enough mass to refract light from the background component (4C 11.50 B) enough to produce a lensed image, although it does have a true companion of its own. An even stranger galaxy pair is 3C 321. Unlike the previous pair, the two galaxies making up 3C 321 are interacting with each other and are in the process of merging. Both members appear to be active galaxies; the primary radio galaxy may be responsible for the activity in the secondary by means of the former's jet driving material onto the latter's supermassive black hole.

An example of gravitational lensing is found in the radio galaxy 3C 324. First thought to be a single overluminous radio galaxy with a redshift of "z" = 1.206, it was found in 1987 to actually be two galaxies, with the radio galaxy at the aforementioned redshift being lensed by another galaxy at redshift "z" = 0.845. The first example of a multiply-imaged radio galaxy discovered, the source appears to be an elliptical galaxy with a dust lane obscuring our view of the visual and ultraviolet emission from the nucleus. In even shorter wavelengths, the BL Lac object PG 1553+113 is a heavy emitter of gamma rays. This object is the most distant found to emit photons with energies in the TeV range as of 2007. The spectrum is unique, with hard emission in some ranges of the gamma-ray spectrum in stark contrast to soft emission in others. In 2012, the object flared in the gamma-ray spectrum, tripling in luminosity for two nights, allowing the redshift to be accurately measured as "z" = 0.49.

Several gamma-ray bursts (GRBs) have been observed in Serpens Caput, such as GRB 970111, one of the brightest GRBs observed. An optical transient event associated with this GRB has not been found, despite its intensity. The host galaxy initially also proved elusive, however it now appears that the host is a Seyfert I galaxy located at redshift "z" = 0.657. The X-ray afterglow of the GRB has also been much fainter than for other dimmer GRBs. More distant is GRB 060526 (redshift "z" = 3.221), from which X-ray and optical afterglows were detected. This GRB was very faint for a long-duration GRB.

Part of the galactic plane passes through the tail, and thus Serpens Cauda is rich in deep-sky objects within our own galaxy. The Eagle Nebula and its associated star cluster, Messier 16 lie 7,000 light-years from Earth in the direction of the galactic center. The nebula measures 70 light-years by 50 light-years and contains the Pillars of Creation, three dust clouds that became famous for the image taken by the Hubble Space Telescope. The stars being born in the Eagle Nebula, added to those with an approximate age of 5 million years have an average temperature of 45,000 kelvins and produce prodigious amounts of radiation that will eventually destroy the dust pillars. Despite its fame, the Eagle Nebula is fairly dim, with an integrated magnitude of approximately 6.0. The star-forming regions in the nebula are often evaporating gaseous globules; unlike Bok globules they only hold one protostar.

North of Messier 16, at a distance of approximately 2000 parsecs, is the OB association Serpens OB2, containing over 100 OB stars. Around 5 million years old, the association appears to still contain star-forming regions, and the light from its stars is illuminating the HII region S 54. Within this HII region is the open cluster NGC 6604, which is the same age as the surrounding OB association, and the cluster is now thought to simply be the densest part of it. The cluster appears to be producing a thermal chimney of ionized gas, caused by the interaction of the gas from the galactic disk with the galactic halo.

Another open cluster in Serpens Cauda is IC 4756, containing at least one naked-eye star, HD 172365 (another naked-eye star in the vicinity, HD 171586, is most likely unrelated). Positioned approximately 440 parsecs distant, the cluster is estimated to be around 800 million years old, quite old for an open cluster. Despite the presence of the Milky Way in Serpens Cauda, one globular cluster can be found: NGC 6535, although invisible to the naked eye, can be made out in small telescopes just north of Zeta Serpentis. Rather small and sparse for a globular cluster, this cluster contains no known RR Lyrae variables, which is unusual for a globular cluster.

MWC 922 is a star surrounded by a planetary nebula. Dubbed the Red Square Nebula due to its similarities to the Red Rectangle Nebula, the planetary nebula appears to be a nearly perfect square with a dark band around the equatorial regions. The nebula contains concentric rings, which are similar to those seen in the supernova SN 1987A. MWC 922 itself is an FS Canis Majoris variable, meaning that it is a Be star containing exceptionally bright hydrogen emission lines as well as select forbidden lines, likely due to the presence of a close binary. East of Xi Serpentis is another planetary nebula, Abell 41, containing the binary star MT Serpentis at its center. The nebula appears to have a bipolar structure, and the axis of symmetry of the nebula has been found to be within 5° of the line perpendicular to the orbital plane of the stars, strengthening the link between binary stars and bipolar planetary nebulae. On the other end of the stellar age spectrum is L483, a dark nebula which contains the protostar IRAS 18418-0440. Although classified as a class 0 protostar, it has some unusual features for such an object, such as a lack of high-velocity stellar winds, and it has been proposed that this object is in transition between class 0 and class I. A variable nebula exists around the protostar, although it is only visible in infrared light.
The Serpens cloud is a massive star-forming molecular cloud situated in the southern part of Serpens Cauda. Only two million years old and 420 parsecs distant, the cloud is known to contain many protostars such as Serpens FIRS 1 and Serpens SVS 20. The Serpens South protocluster was uncovered by NASA's Spitzer Space Telescope in the southern portion of the cloud, and it appears that star formation is still continuing in the region. Another site of star formation is the Westerhout 40 complex, consisting of a prominent HII region adjacent to a molecular cloud. Located around 500 parsecs distant, it is one of the nearest massive regions of star formation, but as the molecular cloud obscures the HII region, rendering it and its embedded cluster tough to see visibly, it is not as well-studied as others. The embedded cluster likely contains over 600 stars above 0.1 solar masses, with several massive stars, including at least one O-type star, being responsible for lighting the HII region and the production of a bubble.

Despite the presence of the Milky Way, several active galaxies are visible in Serpens Cauda as well, such as PDS 456, found near Xi Serpentis. The most intrinsically luminous nearby active galaxy, this AGN has been found to be extremely variable in the X-ray spectrum. This has allowed light to be shed on the nature of the supermassive black hole at the center, likely a Kerr black hole. It is possible that the quasar is undergoing a transition from an ultraluminous infrared galaxy to a classical radio-quiet quasar, but there are problems with this theory, and the object appears to be an exceptional object that does not completely lie within current classification systems. Nearby is NRAO 530, a blazar that has been known to flare in the X-rays occasionally. One of these flares was for less than 2000 seconds, making it the shortest flare ever observed in a blazar as of 2004. The blazar also appears to show periodic variability in its radio wave output over two different periods of six and ten years.

There are two daytime meteor showers that radiate from Serpens, the Omega Serpentids and the Sigma Serpentids. Both showers peak between December 18 and December 25.



</doc>
<doc id="28150" url="https://en.wikipedia.org/wiki?curid=28150" title="Sculptor Group">
Sculptor Group

The Sculptor Group is a loose group of galaxies visible near the south galactic pole. The group is one of the closest groups of galaxies to the Local Group; the distance to the center of the group from the Milky Way is approximately .

The Sculptor Galaxy (NGC 253) and a few other galaxies form a gravitationally-bound core in the center of this group. A few other galaxies at the periphery may be associated with the group but may not be gravitationally bound. Because most of the galaxies in this group are actually weakly gravitationally bound, the group may also be described as a filament. It is considered to be at an early stage of evolution in which galaxies are still falling into the group along filamentary structures.

The table below lists galaxies that have been identified as associated with the Sculptor Galaxy (and hence associated with the group) by I. D. Karachentsev and collaborators.

The object names used in the above table differ from the names used by Karachentsev and collaborators. NGC, IC, UGC, and PGC numbers have been used when possible to allow for easier referencing.

The irregular galaxy NGC 55, the spiral galaxy NGC 300, and their companion galaxies have been considered by many researchers to be part of this group. However, recent distance measurements to these and other galaxies in the same region of the sky show that NGC 55, NGC 300, and their companions may simply be foreground galaxies that are physically unassociated with the Sculptor Group. The galaxies NGC 24 and NGC 45 are located in the vicinity of the Sculptor Group, but are now considered background objects.


</doc>
<doc id="28151" url="https://en.wikipedia.org/wiki?curid=28151" title="State (polity)">
State (polity)

A state is a polity under a system of governance. There is no undisputed definition of a state. A widely used definition from the German sociologist Max Weber is that a "state" is a polity that maintains a monopoly on the legitimate use of violence, although other definitions are not uncommon.

Some states are sovereign (known as sovereign states), while others are subject to external sovereignty or hegemony, wherein supreme authority lies in another state. 

In a federal union, the term "state" is sometimes used to refer to the federated polities that make up the federation. (Other terms that are used in such federal systems may include “province”, “region” or other terms.) In international law, such entities are not considered states, which is a term that relates only to the national entity, commonly referred to as the country or nation.

Most of the human population has existed within a state system for millennia; however, for most of prehistory people lived in stateless societies. The first states arose about 5,500 years ago in conjunction with rapid growth of cities, invention of writing and codification of new forms of religion. Over time, a variety of different forms developed, employing a variety of justifications for their existence (such as divine right, the theory of the social contract, etc.). Today, the modern nation state is the predominant form of state to which people are subject.

The word "state" and its cognates in some other European languages ("stato" in Italian, "estado" in Spanish and Portuguese, "état" in French, "Staat" in German) ultimately derive from the Latin word "status", meaning "condition, circumstances".

The English noun "state" in the generic sense "condition, circumstances" predates the political sense. It is introduced to Middle English c. 1200 both from Old French and directly from Latin.

With the revival of the Roman law in 14th-century Europe, the term came to refer to the legal standing of persons (such as the various "estates of the realm" – noble, common, and clerical), and in particular the special status of the king. The highest estates, generally those with the most wealth and social rank, were those that held power. The word also had associations with Roman ideas (dating back to Cicero) about the ""status rei publicae"", the "condition of public matters". In time, the word lost its reference to particular social groups and became associated with the legal order of the entire society and the apparatus of its enforcement.

The early 16th-century works of Machiavelli (especially "The Prince") played a central role in popularizing the use of the word "state" in something similar to its modern sense. The contrasting of church and state still dates to the 16th century. The North American colonies were called "states" as early as the 1630s. The expression "L'Etat, c'est moi" ("I am the State") attributed to Louis XIV of France is probably apocryphal, recorded in the late 18th century.

There is no academic consensus on the most appropriate definition of the state. The term "state" refers to a set of different, but interrelated and often overlapping, theories about a certain range of political phenomena. The act of defining the term can be seen as part of an ideological conflict, because different definitions lead to different theories of state function, and as a result validate different political strategies. According to Jeffrey and Painter, "if we define the 'essence' of the state in one place or era, we are liable to find that in another time or space something which is also understood to be a state has different 'essential' characteristics".

Different definitions of the state often place an emphasis either on the ‘means’ or the ‘ends’ of states. Means-related definitions include those by Max Weber and Charles Tilly, both of whom define the state according to its violent means. For Weber, the state "is a human community that (successfully) claims the monopoly of the legitimate use of physical force within a given territory" (Politics as a Vocation), while Tilly characterises them as "coercion-wielding organisations" (Coercion, Capital, and European States).

Ends-related definitions emphasis instead the teleological aims and purposes of the state. Marxist thought regards the ends of the state as being the perpetuation of class domination in favour of the ruling class which, under the capitalist mode of production, is the bourgeoisie. The state exists to defend the ruling class's claims to private property and its capturing of surplus profits at the expense of the proletariat. Indeed, Marx claimed that "the executive of the modern state is nothing but a committee for managing the common affairs of the whole bourgeoisie" (Communist Manifesto).

Liberal thought provides another possible teleology of the state. According to John Locke, the goal of the state/commonwealth was "the preservation of property" (Second Treatise on Government), with 'property' in Locke's work referring not only to personal possessions but also to one's life and liberty. On this account, the state provides the basis for social cohesion and productivity, creating incentives for wealth creation by providing guarantees of protection for one's life, liberty and personal property.

The most commonly used definition is Max Weber's, which describes the state as a compulsory political organization with a centralized government that maintains a monopoly of the legitimate use of force within a certain territory. General categories of state institutions include administrative bureaucracies, legal systems, and military or religious organizations.

Another commonly accepted definition of the state is the one given at the Montevideo Convention on Rights and Duties of States in 1933. It provides that "[t]he state as a person of international law should possess the following qualifications: (a) a permanent population; (b) a defined territory; (c) government; and (d) capacity to enter into relations with the other states." And that "[t]he federal state shall constitute a sole person in the eyes of international law."

According to the "Oxford English Dictionary", a state is "a. an organized political community under one government; a commonwealth; a nation. b. such a community forming part of a federal republic, esp the United States of America".

Confounding the definition problem is that "state" and "government" are often used as synonyms in common conversation and even some academic discourse. According to this definition schema, the states are nonphysical persons of international law, governments are organizations of people. The relationship between a government and its state is one of representation and authorized agency.

States may be classified by political philosophers as sovereign if they are not dependent on, or subject to any other power or state. Other states are subject to external sovereignty or hegemony where ultimate sovereignty lies in another state. Many states are federated states which participate in a federal union. A federated state is a territorial and constitutional community forming part of a federation. (Compare confederacies or confederations such as Switzerland.) Such states differ from sovereign states in that they have transferred a portion of their sovereign powers to a federal government.

One can commonly and sometimes readily (but not necessarily usefully) classify states according to their apparent make-up or focus. The concept of the nation-state, theoretically or ideally co-terminous with a "nation", became very popular by the 20th century in Europe, but occurred rarely elsewhere or at other times. In contrast, some states have sought to make a virtue of their multi-ethnic or multi-national character (Habsburg Austria-Hungary, for example, or the Soviet Union), and have emphasised unifying characteristics such as autocracy, monarchical legitimacy, or ideology. Imperial states have sometimes promoted notions of racial superiority.
Other states may bring ideas of commonality and inclusiveness to the fore: note the "res publica" of ancient Rome and the "Rzeczpospolita" of Poland-Lithuania which finds echoes in the modern-day republic. The concept of temple states centred on religious shrines occurs in some discussions of the ancient world.
Relatively small city-states, once a relatively common and often successful form of polity,
have become rarer and comparatively less prominent in modern times. Modern-day independent city-states include Vatican City, Monaco, and Singapore. Other city-states survive as federated states, like the present day German city-states, or as otherwise autonomous entities with limited sovereignty, like Hong Kong, Gibraltar and Ceuta. To some extent, urban secession, the creation of a new city-state (sovereign or federated), continues to be discussed in the early 21st century in cities such as London.

A state can be distinguished from a government. The state is the organization while the government is the particular group of people, the administrative bureaucracy that controls the state apparatus at a given time. That is, governments are the means through which state power is employed. States are served by a continuous succession of different governments. States are immaterial and nonphysical social objects, whereas governments are groups of people with certain coercive powers.

Each successive government is composed of a specialized and privileged body of individuals, who monopolize political decision-making, and are separated by status and organization from the population as a whole.

States can also be distinguished from the concept of a "nation", where "nation" refers to a cultural-political community of people. A nation-state refers to a situation where a single ethnicity is associated with a specific state.

In the classical thought, the state was identified with both political society and civil society as a form of political community, while the modern thought distinguished the nation state as a political society from civil society as a form of economic society.
Thus in the modern thought the state is contrasted with civil society.

Antonio Gramsci believed that civil society is the primary locus of political activity because it is where all forms of "identity formation, ideological struggle, the activities of intellectuals, and the construction of hegemony take place." and that civil society was the nexus connecting the economic and political sphere. Arising out of the collective actions of civil society is what Gramsci calls "political society", which Gramsci differentiates from the notion of the state as a polity. He stated that politics was not a "one-way process of political management" but, rather, that the activities of civil organizations conditioned the activities of political parties and state institutions, and were conditioned by them in turn. Louis Althusser argued that civil organizations such as church, schools, and the family are part of an "ideological state apparatus" which complements the "repressive state apparatus" (such as police and military) in reproducing social relations.

Jürgen Habermas spoke of a public sphere that was distinct from both the economic and political sphere.

Given the role that many social groups have in the development of public policy and the extensive connections between state bureaucracies and other institutions, it has become increasingly difficult to identify the boundaries of the state. Privatization, nationalization, and the creation of new regulatory bodies also change the boundaries of the state in relation to society. Often the nature of quasi-autonomous organizations is unclear, generating debate among political scientists on whether they are part of the state or civil society. Some political scientists thus prefer to speak of policy networks and decentralized governance in modern societies rather than of state bureaucracies and direct state control over policy.

Most countries have two names, a protocol name and a geographical name or short name. 

The protocol name (full name, formal name, official name) e.g. the "Slovak Republic", the "Czech Republic", the "Swiss Confederation", the "State of Qatar", the "Principality of Monaco", the "Kingdom of Norway", the "Grand Duchy of Luxembourg", the "Federal Democratic Republic of Ethiopia", the "People's Democratic Republic of Algeria", the "Argentine Republic", the "United Kingdom of Great Britain and Northern Ireland", the "United States of America", the "United Mexican States", the "Commonwealth of Massachusetts", the "Free State of Bavaria", the "Union of Soviet Socialist Republics". The long form (official title) is used when the state is targeted as a legal entity: e.g. "This Decision is addressed to the United Kingdom of Great Britain and Northern Ireland.", "The French Republic is authorised to …", "Agreement between the Arab Republic of Egypt and the Russian Federation …". If the recurrence of the name of a state in the text leads to a preference for using the short form, it can be introduced with the phrase ‘hereinafter referred to as …’.

The geographical name (short name) e.g. "Slovakia", "Czechia", "Switzerland", "Qatar", "Monaco", "Norway", "Luxembourg", "Ethiopia", "Algeria", "Argentina", the "United Kingdom", the "United States", "Mexico", "Massachusetts", "Bavaria", the "Soviet Union". The short form (short name) is used when the state is referred to geographically or economically:  e.g. "Workers residing in France.", "Exports from Greece …".

For certain states, the long form and the short form are identical: e.g. the "Central African Republic", the "Democratic Republic of the Congo", the "Dominican Republic", the "United Arab Emirates", "Bosnia and Herzegovina", "Canada", "Georgia", "Hungary", "Iceland", "Ireland", "Jamaica", "Japan", "Malaysia", "Mongolia", "Montenegro", "New Zealand", "Romania", "Saint Lucia", "Saint Vincent and the Grenadines", the "Solomon Islands", "Turkmenistan", "Tuvalu", "Ukraine".


The earliest forms of the state emerged whenever it became possible to centralize power in a durable way. Agriculture and writing are almost everywhere associated with this process: agriculture because it allowed for the emergence of a social class of people who did not have to spend most of their time providing for their own subsistence, and writing (or an equivalent of writing, like Inca quipus) because it made possible the centralization of vital information.

The first known states were created in the Fertile Crescent, India, China, Mesoamerica, the Andes, and others, but it is only in relatively modern times that states have almost completely displaced alternative "stateless" forms of political organization of societies all over the planet. Roving bands of hunter-gatherers and even fairly sizable and complex tribal societies based on herding or agriculture have existed without any full-time specialized state organization, and these "stateless" forms of political organization have in fact prevailed for all of the prehistory and much of the history of the human species and civilization.

Initially states emerged over territories built by conquest in which one culture, one set of ideals and one set of laws have been imposed by force or threat over diverse nations by a civilian and military bureaucracy. Currently, that is not always the case and there are multinational states, federated states and autonomous areas within states.

Since the late 19th century, virtually the entirety of the world's inhabitable land has been parcelled up into areas with more or less definite borders claimed by various states. Earlier, quite large land areas had been either unclaimed or uninhabited, or inhabited by nomadic peoples who were not organised as states. However, even within present-day states there are vast areas of wilderness, like the Amazon rainforest, which are uninhabited or inhabited solely or mostly by indigenous people (and some of them remain uncontacted). Also, there are states which do not hold de facto control over all of their claimed territory or where this control is challenged. Currently the international community comprises around 200 sovereign states, the vast majority of which are represented in the United Nations.

For most of human history, people have lived in stateless societies, characterized by a lack of concentrated authority, and the absence of large inequalities in economic and political power.

The anthropologist Tim Ingold writes:

During the Neolithic period, human societies underwent major cultural and economic changes, including the development of agriculture, the formation of sedentary societies and fixed settlements, increasing population densities, and the use of pottery and more complex tools.

Sedentary agriculture led to the development of property rights, domestication of plants and animals, and larger family sizes. It also provided the basis for the centralized state form by producing a large surplus of food, which created a more complex division of labor by enabling people to specialize in tasks other than food production. Early states were characterized by highly stratified societies, with a privileged and wealthy ruling class that was subordinate to a monarch. The ruling classes began to differentiate themselves through forms of architecture and other cultural practices that were different from those of the subordinate laboring classes.

In the past, it was suggested that the centralized state was developed to administer large public works systems (such as irrigation systems) and to regulate complex economies. However, modern archaeological and anthropological evidence does not support this thesis, pointing to the existence of several non-stratified and politically decentralized complex societies.

Mesopotamia is generally considered to be the location of the earliest civilization or complex society, meaning that it contained cities, full-time division of labor, social concentration of wealth into capital, unequal distribution of wealth, ruling classes, community ties based on residency rather than kinship, long distance trade, monumental architecture, standardized forms of art and culture, writing, and mathematics and science. It was the world's first literate civilization, and formed the first sets of written laws.

Although state-forms existed before the rise of the Ancient Greek empire, the Greeks were the first people known to have explicitly formulated a political philosophy of the state, and to have rationally analyzed political institutions. Prior to this, states were described and justified in terms of religious myths.

Several important political innovations of classical antiquity came from the Greek city-states and the Roman Republic. The Greek city-states before the 4th century granted citizenship rights to their free population, and in Athens these rights were combined with a directly democratic form of government that was to have a long afterlife in political thought and history.

During Medieval times in Europe, the state was organized on the principle of feudalism, and the relationship between lord and vassal became central to social organization. Feudalism led to the development of greater social hierarchies.

The formalization of the struggles over taxation between the monarch and other elements of society (especially the nobility and the cities) gave rise to what is now called the Standestaat, or the state of Estates, characterized by parliaments in which key social groups negotiated with the king about legal and economic matters. These estates of the realm sometimes evolved in the direction of fully-fledged parliaments, but sometimes lost out in their struggles with the monarch, leading to greater centralization of lawmaking and military power in his hands. Beginning in the 15th century, this centralizing process gives rise to the absolutist state.

Cultural and national homogenization figured prominently in the rise of the modern state system. Since the absolutist period, states have largely been organized on a national basis. The concept of a national state, however, is not synonymous with nation state. Even in the most ethnically homogeneous societies there is not always a complete correspondence between state and nation, hence the active role often taken by the state to promote nationalism through emphasis on shared symbols and national identity.

Most political theories of the state can roughly be classified into two categories. The first are known as "liberal" or "conservative" theories, which treat capitalism as a given, and then concentrate on the function of states in capitalist society. These theories tend to see the state as a neutral entity separated from society and the economy. Marxist and anarchist theories on the other hand, see politics as intimately tied in with economic relations, and emphasize the relation between economic power and political power. They see the state as a partisan instrument that primarily serves the interests of the upper class.

Anarchism is a political philosophy which considers the state and hierarchies to be immoral, unnecessary and harmful and instead promotes a stateless society, or anarchy, a self-managed, self-governed society based on voluntary, cooperative institutions.

Anarchists believe that the state is inherently an instrument of domination and repression, no matter who is in control of it. Anarchists note that the state possesses the monopoly on the legal use of violence. Unlike Marxists, anarchists believe that revolutionary seizure of state power should not be a political goal. They believe instead that the state apparatus should be completely dismantled, and an alternative set of social relations created, which are not based on state power at all.

Various Christian anarchists, such as Jacques Ellul, have identified the State and political power as the Beast in the Book of Revelation.

Marx and Engels were clear in that the communist goal was a classless society in which the state would have "withered away", replaced only by "administration of things". Their views are found throughout their Collected Works, and address past or then extant state forms from an analytical and tactical viewpoint, but not future social forms, speculation about which is generally antithetical to groups considering themselves Marxist but who – not having conquered the existing state power(s) – are not in the situation of supplying the institutional form of an actual society. To the extent that it makes sense, there is no single "Marxist theory of state", but rather several different purportedly "Marxist" theories have been developed by adherents of Marxism.

Marx's early writings portrayed the bourgeois state as parasitic, built upon the superstructure of the economy, and working against the public interest. He also wrote that the state mirrors class relations in society in general, acting as a regulator and repressor of class struggle, and as a tool of political power and domination for the ruling class. The "Communist Manifesto" claimed that the state to be nothing more than "a committee for managing the common affairs of the "bourgeoisie".

For Marxist theorists, the role of the modern bourgeois state is determined by its function in the global capitalist order. Ralph Miliband argued that the ruling class uses the state as its instrument to dominate society by virtue of the interpersonal ties between state officials and economic elites. For Miliband, the state is dominated by an elite that comes from the same background as the capitalist class. State officials therefore share the same interests as owners of capital and are linked to them through a wide array of social, economic, and political ties.

Gramsci's theories of state emphasized that the state is only one of the institutions in society that helps maintain the hegemony of the ruling class, and that state power is bolstered by the ideological domination of the institutions of civil society, such as churches, schools, and mass media.

Pluralists view society as a collection of individuals and groups, who are competing for political power. They then view the state as a neutral body that simply enacts the will of whichever groups dominate the electoral process. Within the pluralist tradition, Robert Dahl developed the theory of the state as a neutral arena for contending interests or its agencies as simply another set of interest groups. With power competitively arranged in society, state policy is a product of recurrent bargaining. Although pluralism recognizes the existence of inequality, it asserts that all groups have an opportunity to pressure the state. The pluralist approach suggests that the modern democratic state's actions are the result of pressures applied by a variety of organized interests. Dahl called this kind of state a polyarchy.

Pluralism has been challenged on the ground that it is not supported by empirical evidence. Citing surveys showing that the large majority of people in high leadership positions are members of the wealthy upper class, critics of pluralism claim that the state serves the interests of the upper class rather than equitably serving the interests of all social groups.

Jürgen Habermas believed that the base-superstructure framework, used by many Marxist theorists to describe the relation between the state and the economy, was overly simplistic. He felt that the modern state plays a large role in structuring the economy, by regulating economic activity and being a large-scale economic consumer/producer, and through its redistributive welfare state activities. Because of the way these activities structure the economic framework, Habermas felt that the state cannot be looked at as passively responding to economic class interests.

Michel Foucault believed that modern political theory was too state-centric, saying "Maybe, after all, the state is no more than a composite reality and a mythologized abstraction, whose importance is a lot more limited than many of us think." He thought that political theory was focusing too much on abstract institutions, and not enough on the actual practices of government. In Foucault's opinion, the state had no essence. He believed that instead of trying to understand the activities of governments by analyzing the properties of the state (a reified abstraction), political theorists should be examining changes in the practice of government to understand changes in the nature of the state. Foucault argues that it is technology that has created and made the state so elusive and successful, and that instead of looking at the state as something to be toppled we should look at the state as technological manifestation or system with many heads; Foucault argues instead of something to be overthrown as in the sense of the Marxist and Anarchist understanding of the state. Every single scientific technological advance has come to the service of the state Foucault argues and it is with the emergence of the Mathematical sciences and essentially the formation of Mathematical statistics that one gets an understanding of the complex technology of producing how the modern state was so successfully created. Foucault insists that the Nation state was not a historical accident but a deliberate production in which the modern state had to now manage coincidentally with the emerging practice of the Police (Cameral science) 'allowing' the population to now 'come in' into "jus gentium" and "civitas" (Civil society) after deliberately being excluded for several millennia. Democracy wasn't (the newly formed voting franchise) as is always painted by both political revolutionaries and political philosophers as a cry for political freedom or wanting to be accepted by the 'ruling elite', Foucault insists, but was a part of a skilled endeavour of switching over new technology such as; Translatio imperii, Plenitudo potestatis and "extra" "Ecclesiam nulla salus" readily available from the past Medieval period, into mass persuasion for the future industrial 'political' population(deception over the population) in which the political population was now asked to insist upon itself "the president must be elected". Where these political symbol agents, represented by the pope and the president are now democratised. Foucault calls these new forms of technology Biopower and form part of our political inheritance which he calls Biopolitics.

Heavily influenced by Gramsci, Nicos Poulantzas, a Greek neo-Marxist theorist argued that capitalist states do not always act on behalf of the ruling class, and when they do, it is not necessarily the case because state officials consciously strive to do so, but because the 'structural' position of the state is configured in such a way to ensure that the long-term interests of capital are always dominant. Poulantzas' main contribution to the Marxist literature on the state was the concept of 'relative autonomy' of the state. While Poulantzas' work on 'state autonomy' has served to sharpen and specify a great deal of Marxist literature on the state, his own framework came under criticism for its 'structural functionalism'.

It can be considered as a single structural universe: the historical reality that takes shape in societies characterized by a codified or crystallized right, with a power organized hierarchically and justified by the law that gives it authority, with a well-defined social and economic stratification, with an economic and social organization that gives the society precise organic characteristics, with one (or multiple) religious organizations, in justification of the power expressed by such a society and in support of the religious beliefs of individuals and accepted by society as a whole. Such a structural universe, evolves in a cyclical manner, presenting two different historical phases (a mercantile phase, or “open society”, and a feudal phase or “closed society”), with characteristics so divergent that it can qualify as two different levels of civilization which, however, are never definitive, but that alternate cyclically, being able, each of the two different levels, to be considered progressive (in a partisan way, totally independent of the real value of well-being, degrees of freedom granted, equality realized and a concrete possibility to achieve further progress of the level of civilization), even by the most cultured fractions, educated and intellectually more equipped than the various societies, of both historical phases. 

State autonomy theorists believe that the state is an entity that is impervious to external social and economic influence, and has interests of its own.

"New institutionalist" writings on the state, such as the works of Theda Skocpol, suggest that state actors are to an important degree autonomous. In other words, state personnel have interests of their own, which they can and do pursue independently of (at times in conflict with) actors in society. Since the state controls the means of coercion, and given the dependence of many groups in civil society on the state for achieving any goals they may espouse, state personnel can to some extent impose their own preferences on civil society.

States generally rely on a claim to some form of political legitimacy in order to maintain domination over their subjects.

The rise of the modern day state system was closely related to changes in political thought, especially concerning the changing understanding of legitimate state power and control. Early modern defenders of absolutism (Absolute monarchy), such as Thomas Hobbes and Jean Bodin undermined the doctrine of the divine right of kings by arguing that the power of kings should be justified by reference to the people. Hobbes in particular went further to argue that political power should be justified with reference to the individual(Hobbes wrote in the time of the English Civil War), not just to the people understood collectively. Both Hobbes and Bodin thought they were defending the power of kings, not advocating for democracy, but their arguments about the nature of sovereignty were fiercely resisted by more traditional defenders of the power of kings, such as Sir Robert Filmer in England, who thought that such defenses ultimately opened the way to more democratic claims.

Max Weber identified three main sources of political legitimacy in his works. The first, legitimacy based on traditional grounds is derived from a belief that things should be as they have been in the past, and that those who defend these traditions have a legitimate claim to power. The second, legitimacy based on charismatic leadership, is devotion to a leader or group that is viewed as exceptionally heroic or virtuous. The third is rational-legal authority, whereby legitimacy is derived from the belief that a certain group has been placed in power in a legal manner, and that their actions are justifiable according to a specific code of written laws. Weber believed that the modern state is characterized primarily by appeals to rational-legal authority.

Some states are often labeled as "weak" or "failed". In David Samuels's words "...a failed state occurs when sovereignty over claimed territory has collapsed or was never effectively at all". Authors like Samuels and Joel S. Migdal have explored the emergence of weak states, how they are different from Western "strong" states and its consequences to the economic development of developing countries.

Early state formation

To understand the formation of weak states, Samuels compares the formation of European states in the 1600s with the conditions under which more recent states were formed in the twentieth century. In this line of argument, the state allows a population to resolve a collective action problem, in which citizens recognize the authority of the state and this exercise the power of coercion over them. This kind of social organization required a decline in legitimacy of traditional forms of ruling (like religious authorities) and replaced them with an increase in the legitimacy of depersonalized rule; an increase in the central government's sovereignty; and an increase in the organizational complexity of the central government (bureaucracy).

The transition to this modern state was possible in Europe around 1600 thanks to the confluence of factors like the technological developments in warfare, which generated strong incentives to tax and consolidate central structures of governance to respond to external threats. This was complemented by the increasing on the production of food (as a result of productivity improvements), which allowed to sustain a larger population and so increased the complexity and centralization of states. Finally, cultural changes challenged the authority of monarchies and paved the way to the emergence of modern states.

Late state formation

The conditions that enabled the emergence of modern states in Europe were different for other countries that started this process later. As a result, many of these states lack effective capabilities to tax and extract revenue from their citizens, which derives in problems like corruption, tax evasion and low economic growth. Unlike the European case, late state formation occurred in a context of limited international conflict that diminished the incentives to tax and increase military spending. Also, many of these states emerged from colonization in a state of poverty and with institutions designed to extract natural resources, which have made more difficult to form states. European colonization also defined many arbitrary borders that mixed different cultural groups under the same national identities, which has made difficult to build states with legitimacy among all the population, since some states have to compete for it with other forms of political identity.

As a complement of this argument, Migdal gives a historical account on how sudden social changes in the Third World during the Industrial Revolution contributed to the formation of weak states. The expansion of international trade that started around 1850, brought profound changes in Africa, Asia and Latin America that were introduced with the objective of assure the availability of raw materials for the European market. These changes consisted in: i) reforms to landownership laws with the objective of integrate more lands to the international economy, ii) increase in the taxation of peasants and little landowners, as well as collecting of these taxes in cash instead of in kind as was usual up to that moment and iii) the introduction of new and less costly modes of transportation, mainly railroads. As a result, the traditional forms of social control became obsolete, deteriorating the existing institutions and opening the way to the creation of new ones, that not necessarily lead these countries to build strong states. This fragmentation of the social order induced a political logic in which these states were captured to some extent by "strongmen", who were capable to take advantage of the above-mentioned changes and that challenge the sovereignty of the state. As a result, these decentralization of social control impedes to consolidate strong states.





</doc>
<doc id="28152" url="https://en.wikipedia.org/wiki?curid=28152" title="Stevia">
Stevia

Stevia () is a sweetener and sugar substitute derived from the leaves of the plant species "Stevia rebaudiana", native to Brazil and Paraguay. The active compounds are steviol glycosides (mainly stevioside and rebaudioside), which have 30 to 150 times the sweetness of sugar, are heat-stable, pH-stable, and not fermentable. The body does not metabolize the glycosides in stevia, so it contains zero calories, like some artificial sweeteners. Stevia's taste has a slower onset and longer duration than that of sugar, and some of its extracts may have a bitter or licorice-like aftertaste at high concentrations.

The legal status of stevia as a food additive or dietary supplement varies from country to country. In the United States, high-purity "stevia glycoside" extracts have been generally recognized as safe (GRAS) since 2008, and are allowed in food products, but stevia leaf and crude extracts do not have GRAS or Food and Drug Administration (FDA) approval for use in food. The European Union approved "Stevia" additives in 2011, while in Japan, stevia has been widely used as a sweetener for decades.

The plant "Stevia rebaudiana" has been used for more than 1,500 years by the Guaraní peoples of South America, who called it "ka'a he'ê" ("sweet herb"). The leaves have been used traditionally for hundreds of years in both Brazil and Paraguay to sweeten local teas and medicines, and as a "sweet treat". The genus was named for the Spanish botanist and physician Petrus Jacobus Stevus.

In 1899, Swiss botanist Moisés Santiago Bertoni, while conducting research in eastern Paraguay, first described the plant and the sweet taste in detail. Only limited research was conducted on the topic until, in 1931, two French chemists isolated the glycosides that give stevia its sweet taste.

During the 1990s, the United States Food and Drug Administration (FDA) received two petitions requesting that stevia be classified as generally recognized as safe (GRAS), but the FDA "disagreed with [the] conclusions [detailed in the petitions]". Stevia remained banned for all uses until the Dietary Supplement Health and Education Act of 1994, after which the FDA revised its stance and permitted stevia to be used as a dietary supplement, although still not as a food additive. In 1999, prompted by early studies, the European Commission banned stevia's use in food products within the European Union pending further research. In 2006, research data compiled in the safety evaluation released by the World Health Organization found no adverse effects.

In December 2008, the FDA gave a "no objection" approval for GRAS status to Truvia and PureVia, both of which use derived from the "Stevia" plant. However, the FDA said that these products are not stevia, but a highly purified "Stevia"-extract product. In 2015, the FDA still regarded stevia as "not an approved food additive", and stated that it "has not been affirmed as GRAS in the United States due to inadequate toxicological information". In June 2016, the U.S. Customs and Border Protection issued an order of detention for stevia products made in China based on information that the products were made using prison labor. As of 2017, high-purity "Stevia" glycosides are considered safe and allowable as ingredients in food products sold in the United States.

In the early 1970s, sweeteners such as cyclamate and saccharin were gradually decreased or removed from a variant formulation of Coca-Cola. Consequently, use of stevia as an alternative began in Japan, with the aqueous extract of the leaves yielding purified steviosides developed as sweeteners. The first commercial "Stevia" sweetener in Japan was produced by the Japanese firm Morita Kagaku Kogyo Co., Ltd. in 1971. The Japanese have been using stevia in food products and soft drinks, (including Coca-Cola), and for table use. In 2006, Japan consumed more stevia than any other country, with stevia accounting for 40% of the sweetener market.

In the mid-1980s, stevia became popular in U.S. natural foods and health food industries, as a noncaloric natural sweetener for teas and weight-loss blends. The makers of the synthetic sweetener NutraSweet (at the time Monsanto) asked the FDA to require testing of the herb. 
As of 2006, China was the world's largest exporter of stevioside products. In 2007, the Coca-Cola Company announced plans to obtain approval for its "Stevia"-derived sweetener, Rebiana, for use as a food additive within the United States by 2009, as well as plans to market Rebiana-sweetened products in 12 countries that allow stevia's use as a food additive.

In May 2008, Coca-Cola and Cargill announced the availability of Truvia, a consumer-brand "Stevia" sweetener containing erythritol and Rebiana, which the FDA permitted as a food additive in December 2008. Coca-Cola announced intentions to release stevia-sweetened beverages in late December 2008. From 2013 onwards, Coca-Cola Life, containing stevia as a sweetener, was launched in various countries around the world.

Shortly afterward, PepsiCo and Pure Circle announced PureVia, their brand of "Stevia"-based sweetener, but withheld release of beverages sweetened with until receipt of FDA confirmation. Since the FDA permitted Truvia and PureVia, both Coca-Cola and PepsiCo have introduced products that contain their new sweeteners.

Rebaudioside A has the least bitterness of all the steviol glycosides in the "Stevia rebaudiana" plant. To produce rebaudioside A commercially, "Stevia" plants are dried and subjected to a water extraction process. This crude extract contains about 50% rebaudioside A. The various glycosides are separated and purified via crystallization techniques, typically using ethanol or methanol as solvent.

"Stevia" extracts and derivatives are produced industrially and marketed under different trade names.

Glycosides are molecules that contain glucose residues bound to other non-sugar substances called aglycones (molecules with other sugars are polysaccharides). Preliminary experiments deduce that the tongue's taste receptors react to the glycosides and transduce the sweet taste sensation and the lingering bitter aftertaste by direct activation of sweet and bitter receptors.

According to basic research, steviol glycosides and steviol interact with a protein channel called TRPM5, potentiating the signal from the sweet or bitter receptors, amplifying the taste of other sweet, bitter and umami tastants. The synergetic effect of the glycosides on the sweet receptor and TRPM5 explains the sweetness sensation. Some steviol glycosides (rebaudioside A) are perceived sweeter than others (stevioside).

Steviol cannot be further digested in the digestive tract and is taken up into the bloodstream, metabolised by the liver to steviol glucuronide, and excreted in the urine.

A three-dimensional map of the proteins in stevia, showing the crystalline structures that produce both the sensation of sweetness and bitter aftertaste in the sweetener, was reported in 2019.

A 2011 review found that the use of "Stevia" sweeteners as replacements for sugar might benefit people with diabetes, children, and those wishing to lower their intake of calories.

Although both steviol and rebaudioside A have been found to be mutagenic in laboratory "in vitro" testing, these effects have not been demonstrated for the doses and routes of administration to which humans are exposed. Two 2010 review studies found no health concerns with "Stevia" or its sweetening extracts.

The WHO's Joint Experts Committee on Food Additives has approved, based on long-term studies, an acceptable daily intake of steviol glycoside of up to 4 mg/kg of body weight. In 2010, The European Food Safety Authority established an acceptable daily intake of 4 mg/kg/day of steviol, in the form of steviol glycosides. Meanwhile, the Memorial Sloan Kettering Cancer Center warns that "steviol at high dosages may have weak mutagenic activity," and a review "conducted for" the Center for Science in the Public Interest notes that there are no published carcinogenicity results for rebaudioside A (or stevioside).

In August 2019, the US FDA placed an import alert on "Stevia" leaves and crude extracts – which do not have GRAS status – and on foods or dietary supplements containing them due to concerns about safety and potential for toxicity.

The plant may be grown legally in most countries, although some countries restrict its use as a sweetener. The legally allowed uses and maximum dosage of the extracts and derived products vary widely from country to country.




</doc>
<doc id="28153" url="https://en.wikipedia.org/wiki?curid=28153" title="Search for extraterrestrial intelligence">
Search for extraterrestrial intelligence

The search for extraterrestrial intelligence (SETI) is a collective term for scientific searches for intelligent extraterrestrial life, for example, monitoring electromagnetic radiation for signs of transmissions from civilizations on other planets.

Scientific investigation began shortly after the advent of radio in the early 1900s, and focused international efforts have been going on since the 1980s. In 2015, Stephen Hawking and Russian billionaire Yuri Milner announced a well-funded effort called Breakthrough Listen.

There have been many earlier searches for extraterrestrial intelligence within the Solar System. In 1896, Nikola Tesla suggested that an extreme version of his wireless electrical transmission system could be used to contact beings on Mars. In 1899, while conducting experiments at his Colorado Springs experimental station, he thought he had detected a signal from that planet since an odd repetitive static signal seemed to cut off when Mars set in the night sky. Analysis of Tesla's research has led to a range of explanations including: Tesla simply misunderstood the new technology he was working with, that he may have been observing signals from Marconi's European radio experiments, and even speculation that he could have picked up naturally occurring radio noise caused by a moon of Jupiter (Io) moving through the magnetosphere of Jupiter. In the early 1900s, Guglielmo Marconi, Lord Kelvin and David Peck Todd also stated their belief that radio could be used to contact Martians, with Marconi stating that his stations had also picked up potential Martian signals.

On August 21–23, 1924, Mars entered an opposition closer to Earth than at any time in the century before or the next 80 years. In the United States, a "National Radio Silence Day" was promoted during a 36-hour period from August 21–23, with all radios quiet for five minutes on the hour, every hour. At the United States Naval Observatory, a radio receiver was lifted above the ground in a dirigible tuned to a wavelength between 8 and 9 km, using a "radio-camera" developed by Amherst College and Charles Francis Jenkins. The program was led by David Peck Todd with the military assistance of Admiral Edward W. Eberle (Chief of Naval Operations), with William F. Friedman (chief cryptographer of the United States Army), assigned to translate any potential Martian messages.

A 1959 paper by Philip Morrison and Giuseppe Cocconi first pointed out the possibility of searching the microwave spectrum, and proposed frequencies and a set of initial targets.

In 1960, Cornell University astronomer Frank Drake performed the first modern SETI experiment, named "Project Ozma", after the Queen of Oz in L. Frank Baum's fantasy books. Drake used a radio telescope in diameter at Green Bank, West Virginia, to examine the stars Tau Ceti and Epsilon Eridani near the 1.420 gigahertz marker frequency, a region of the radio spectrum dubbed the "water hole" due to its proximity to the hydrogen and hydroxyl radical spectral lines. A 400 kilohertz band around the marker frequency was scanned, using a single-channel receiver with a bandwidth of 100 hertz. He found nothing of interest.

Soviet scientists took a strong interest in SETI during the 1960s and performed a number of searches with omnidirectional antennas in the hope of picking up powerful radio signals. Soviet astronomer Iosif Shklovsky wrote the pioneering book in the field, "Universe, Life, Intelligence" (1962), which was expanded upon by American astronomer Carl Sagan as the best-selling book "Intelligent Life in the Universe" (1966).

In the March 1955 issue of "Scientific American", John D. Kraus described an idea to scan the cosmos for natural radio signals using a flat-plane radio telescope equipped with a parabolic reflector. Within two years, his concept was approved for construction by Ohio State University. With a total of US$71,000 in grants from the National Science Foundation, construction began on an plot in Delaware, Ohio. This Ohio State University Radio Observatory telescope was called "Big Ear". Later, it began the world's first continuous SETI program, called the Ohio State University SETI program.

In 1971, NASA funded a SETI study that involved Drake, Bernard M. Oliver of Hewlett-Packard Corporation, and others. The resulting report proposed the construction of an Earth-based radio telescope array with 1,500 dishes known as "Project Cyclops". The price tag for the Cyclops array was US$10 billion. Cyclops was not built, but the report formed the basis of much SETI work that followed.

The Ohio State SETI program gained fame on August 15, 1977, when Jerry Ehman, a project volunteer, witnessed a startlingly strong signal received by the telescope. He quickly circled the indication on a printout and scribbled the exclamation "Wow!" in the margin. Dubbed the "Wow! signal", it is considered by some to be the best candidate for a radio signal from an artificial, extraterrestrial source ever discovered, but it has not been detected again in several additional searches.

In 1980, Carl Sagan, Bruce Murray, and Louis Friedman founded the U.S. Planetary Society, partly as a vehicle for SETI studies.

In the early 1980s, Harvard University physicist Paul Horowitz took the next step and proposed the design of a spectrum analyzer specifically intended to search for SETI transmissions. Traditional desktop spectrum analyzers were of little use for this job, as they sampled frequencies using banks of analog filters and so were restricted in the number of channels they could acquire. However, modern integrated-circuit digital signal processing (DSP) technology could be used to build autocorrelation receivers to check far more channels. This work led in 1981 to a portable spectrum analyzer named "Suitcase SETI" that had a capacity of 131,000 narrow band channels. After field tests that lasted into 1982, Suitcase SETI was put into use in 1983 with the Harvard/Smithsonian radio telescope at Oak Ridge Observatory in Harvard, Massachusetts. This project was named "Sentinel" and continued into 1985.

Even 131,000 channels were not enough to search the sky in detail at a fast rate, so Suitcase SETI was followed in 1985 by Project "META", for "Megachannel Extra-Terrestrial Assay". The META spectrum analyzer had a capacity of 8.4 million channels and a channel resolution of 0.05 hertz. An important feature of META was its use of frequency Doppler shift to distinguish between signals of terrestrial and extraterrestrial origin. The project was led by Horowitz with the help of the Planetary Society, and was partly funded by movie maker Steven Spielberg. A second such effort, META II, was begun in Argentina in 1990, to search the southern sky. META II is still in operation, after an equipment upgrade in 1996.

The follow-on to META was named "BETA", for "Billion-channel Extraterrestrial Assay", and it commenced observation on October 30, 1995. The heart of BETA's processing capability consisted of 63 dedicated fast Fourier transform (FFT) engines, each capable of performing a 2-point complex FFTs in two seconds, and 21 general-purpose personal computers equipped with custom digital signal processing boards. This allowed BETA to receive 250 million simultaneous channels with a resolution of 0.5 hertz per channel. It scanned through the microwave spectrum from 1.400 to 1.720 gigahertz in eight hops, with two seconds of observation per hop. An important capability of the BETA search was rapid and automatic re-observation of candidate signals, achieved by observing the sky with two adjacent beams, one slightly to the east and the other slightly to the west. A successful candidate signal would first transit the east beam, and then the west beam and do so with a speed consistent with Earth's sidereal rotation rate. A third receiver observed the horizon to veto signals of obvious terrestrial origin. On March 23, 1999, the 26-meter radio telescope on which Sentinel, META and BETA were based was blown over by strong winds and seriously damaged. This forced the BETA project to cease operation.

In 1978, the NASA SETI program had been heavily criticized by Senator William Proxmire, and funding for SETI research was removed from the NASA budget by Congress in 1981; however, funding was restored in 1982, after Carl Sagan talked with Proxmire and convinced him of the program's value. In 1992, the U.S. government funded an operational SETI program, in the form of the NASA Microwave Observing Program (MOP). MOP was planned as a long-term effort to conduct a general survey of the sky and also carry out targeted searches of 800 specific nearby stars. MOP was to be performed by radio antennas associated with the NASA Deep Space Network, as well as the radio telescope of the National Radio Astronomy Observatory at Green Bank, West Virginia and the radio telescope at the Arecibo Observatory in Puerto Rico. The signals were to be analyzed by spectrum analyzers, each with a capacity of 15 million channels. These spectrum analyzers could be grouped together to obtain greater capacity. Those used in the targeted search had a bandwidth of 1 hertz per channel, while those used in the sky survey had a bandwidth of 30 hertz per channel.

MOP drew the attention of the United States Congress, where the program was ridiculed and canceled one year after its start. SETI advocates continued without government funding, and in 1995 the nonprofit SETI Institute of Mountain View, California resurrected the MOP program under the name of Project "Phoenix", backed by private sources of funding. Project Phoenix, under the direction of Jill Tarter, is a continuation of the targeted search program from MOP and studies roughly 1,000 nearby Sun-like stars. From 1995 through March 2004, Phoenix conducted observations at the Parkes radio telescope in Australia, the radio telescope of the National Radio Astronomy Observatory in Green Bank, West Virginia, and the radio telescope at the Arecibo Observatory in Puerto Rico. The project observed the equivalent of 800 stars over the available channels in the frequency range from 1200 to 3000 MHz. The search was sensitive enough to pick up transmitters with 1 GW EIRP to a distance of about 200 light-years. According to Prof. Tarter, in 2012 it costs around "$2 million per year to keep SETI research going at the SETI Institute" and approximately 10 times that to support "all kinds of SETI activity around the world".

Many radio frequencies penetrate Earth's atmosphere quite well, and this led to radio telescopes that investigate the cosmos using large radio antennas. Furthermore, human endeavors emit considerable electromagnetic radiation as a byproduct of communications such as television and radio. These signals would be easy to recognize as artificial due to their repetitive nature and narrow bandwidths. If this is typical, one way of discovering an extraterrestrial civilization might be to detect artificial radio emissions from a location outside the Solar System.

Many international radio telescopes are currently being used for radio SETI searches, including the Low Frequency Array (LOFAR) in Europe, the Murchison Widefield Array (MWA) in Australia, and the Lovell Telescope in the United Kingdom.

The SETI Institute collaborated with the Radio Astronomy Laboratory at the Berkeley SETI Research Center to develop a specialized radio telescope array for SETI studies, something like a mini-cyclops array. Formerly known as the One Hectare Telescope (1HT), the concept was renamed the "Allen Telescope Array" (ATA) after the project's benefactor Paul Allen. Its sensitivity would be equivalent to a single large dish more than 100 meters in diameter if completed. Presently, the array under construction has 42 dishes at the Hat Creek Radio Observatory in rural northern California.

The full array (ATA-350) is planned to consist of 350 or more offset-Gregorian radio dishes, each in diameter. These dishes are the largest producible with commercially available satellite television dish technology. The ATA was planned for a 2007 completion date, at a cost of US$25 million. The SETI Institute provided money for building the ATA while University of California, Berkeley designed the telescope and provided operational funding. The first portion of the array (ATA-42) became operational in October 2007 with 42 antennas. The DSP system planned for ATA-350 is extremely ambitious. Completion of the full 350 element array will depend on funding and the technical results from ATA-42.

ATA-42 (ATA) is designed to allow multiple observers simultaneous access to the interferometer output at the same time. Typically, the ATA snapshot imager (used for astronomical surveys and SETI) is run in parallel with the beam forming system (used primarily for SETI). ATA also supports observations in multiple synthesized pencil beams at once, through a technique known as "multibeaming". Multibeaming provides an effective filter for identifying false positives in SETI, since a very distant transmitter must appear at only one point on the sky.

SETI Institute's Center for SETI Research (CSR) uses ATA in the search for extraterrestrial intelligence, observing 12 hours a day, 7 days a week. From 2007-2015, ATA has identified hundreds of millions of technological signals. So far, all these signals have been assigned the status of noise or radio frequency interference because a) they appear to be generated by satellites or Earth-based transmitters, or b) they disappeared before the threshold time limit of ~1 hour. Researchers in CSR are presently working on ways to reduce the threshold time limit, and to expand ATA's capabilities for detection of signals that may have embedded messages.

Berkeley astronomers used the ATA to pursue several science topics, some of which might have turned up transient SETI signals, until 2011, when the collaboration between the University of California, Berkeley and the SETI Institute was terminated.

CNET published an article and pictures about the Allen Telescope Array (ATA) on December 12, 2008.

In April 2011, the ATA was forced to enter an 8-month "hibernation" due to funding shortfalls. Regular operation of the ATA was resumed on December 5, 2011.

In 2012, new life was breathed into the ATA thanks to a $3.6M philanthropic donation by Franklin Antonio, Co-Founder and Chief Scientist of QUALCOMM Incorporated. This gift supports upgrades of all the receivers on the ATA dishes to have dramatically (2x - 10x from 1–8 GHz) greater sensitivity than before and supporting sensitive observations over a wider frequency range from 1–18 GHz, though initially the radio frequency electronics go to only 12 GHz. As of July, 2013 the first of these receivers was installed and proven. Full installation on all 42 antennas is expected in June, 2014. ATA is especially well suited to the search for extraterrestrial intelligence SETI and to discovery of astronomical radio sources, such as heretofore unexplained non-repeating, possibly extragalactic, pulses known as fast radio bursts or FRBs.

SERENDIP (Search for Extraterrestrial Radio Emissions from Nearby Developed Intelligent Populations) is a SETI program launched in 1979 by the Berkeley SETI Research Center. SERENDIP takes advantage of ongoing "mainstream" radio telescope observations as a "piggy-back" or "commensal" program, using large radio telescopes including the NRAO 90m telescope at Green Bank and the Arecibo 305m telescope. Rather than having its own observation program, SERENDIP analyzes deep space radio telescope data that it obtains while other astronomers are using the telescopes.

The most recently deployed SERENDIP spectrometer, SERENDIP V.v, was installed at the Arecibo Observatory in June 2009 and is currently operational. The digital back-end instrument is an FPGA-based 128 million-channel digital spectrometer covering 200 MHz of bandwidth. It takes data commensally with the seven-beam Arecibo L-band Feed Array (ALFA). The program has found around 400 suspicious signals, but there is not enough data to prove that they belong to extraterrestrial intelligence.

"Breakthrough Listen" is a ten-year initiative with $100 million funding begun in July 2015 to actively search for intelligent extraterrestrial communications in the universe, in a substantially expanded way, using resources that had not previously been extensively used for the purpose. It has been described as the most comprehensive search for alien communications to date. The science program for Breakthrough Listen is based at Berkeley SETI Research Center, located in the Astronomy Department at the University of California, Berkeley.

Announced in July 2015, the project is observing for thousands of hours every year on two major radio telescopes, the Green Bank Observatory in West Virginia, and the Parkes Observatory in Australia. Previously, only about 24 to 36 hours of telescope per year were used in the search for alien life. Furthermore, the Automated Planet Finder at Lick Observatory is searching for optical signals coming from laser transmissions. The massive data rates from the radio telescopes (24 GB/s at Green Bank) necessitated the construction of dedicated hardware at the telescopes to perform the bulk of the analysis. Some of the data are also analyzed by volunteers in the SETI@home distributed computing network. Founder of modern SETI Frank Drake is one of the scientists on the project's advisory committee.

In October 2019, Breakthrough Listen started a collaboration with scientists from the TESS team (Transiting Exoplanet Survey Satellite) to look for signs of advanced extraterrestrial life. Thousands of new planets found by TESS will be scanned for technosignatures by Breakthrough Listen partner facilities across the globe. Data from TESS monitoring of stars will also be searched for anomalies.

China's 500 meter Aperture Spherical Telescope (FAST) lists "detecting interstellar communication signals" as part of its science mission. It is funded by the National Development and Reform Commission (NDRC) and managed by the National Astronomical observatories (NAOC) of the Chinese Academy of Sciences (CAS). FAST is the first radio observatory built with SETI as a core scientific goal. FAST consists of a fixed diameter spherical dish constructed in a natural depression sinkhole caused by karst processes in the region. It is the world's largest filled-aperture radio telescope.
According to its website, FAST could search out to 28 light-years, and would be able to reach 1400 stars. If the transmitter's radiated power is increased to 1000,000 MW, FAST would be able to reach one million stars. This is compared to the Arecibo 305 meter telescope detection distance of 18 light-years.

Since 2016, UCLA undergraduate and graduate students have been participating in radio searches for technosignatures with the Green Bank Telescope. Targets include the Kepler field, TRAPPIST-1, and solar-type stars. The search is sensitive to Arecibo-class transmitters located within 420 ly of Earth and to transmitters that are 1000 times more powerful than Arecibo located within 13,000 ly of Earth.

The SETI@home project uses distributed computing to analyze signals acquired by the SERENDIP project.

SETI@home was conceived by David Gedye along with Craig Kasnoff and is a popular volunteer distributed computing project that was launched by the Berkeley SETI Research Center at the University of California, Berkeley, in May 1999. It was originally funded by The Planetary Society and Paramount Pictures, and later by the state of California. The project is run by director David P. Anderson and chief scientist Dan Werthimer. Any individual can become involved with SETI research by downloading the Berkeley Open Infrastructure for Network Computing (BOINC) software program, attaching to the SETI@home project, and allowing the program to run as a background process that uses idle computer power. The SETI@home program itself runs signal analysis on a "work unit" of data recorded from the central 2.5 MHz wide band of the SERENDIP IV instrument. After computation on the work unit is complete, the results are then automatically reported back to SETI@home servers at University of California, Berkeley. By June 28, 2009, the SETI@home project had over 180,000 active participants volunteering a total of over 290,000 computers. These computers give SETI@home an average computational power of 617 teraFLOPS. In 2004 radio source SHGb02+14a set off speculation in the media that a signal had been detected but researchers noted the frequency drifted rapidly and the detection on three SETI@home computers fell within random chance.

As of 2010, after 10 years of data collection, SETI@home has listened to that one frequency at every point of over 67 percent of the sky observable from Arecibo with at least three scans (out of the goal of nine scans), which covers about 20 percent of the full celestial sphere.

SETI Network is the only operational private search system.

The SETI Net station consists of off-the-shelf, consumer-grade electronics to minimize cost and to allow this design to be replicated as simply as possible. It has a 3-meter parabolic antenna that can be directed in azimuth and elevation, an LNA that covers the 1420 MHz spectrum, a receiver to reproduce the wideband audio, and a standard personal computer as the control device and for deploying the detection algorithms.

The antenna can be pointed and locked to one sky location, enabling the system to integrate on it for long periods. Currently the Wow! signal area is being monitored when it is above the horizon. All search data are collected and made available on the Internet archive.

SETI Net started operation in the early 1980s as a way to learn about the science of the search, and has developed several software packages for the amateur SETI community. It has provided an astronomical clock, a file manager to keep track of SETI data files, a spectrum analyzer optimized for amateur SETI, remote control of the station from the Internet, and other packages.

It can be reached at https://www.seti.net

Founded in 1994 in response to the United States Congress cancellation of the NASA SETI program, The SETI League, Inc. is a membership-supported nonprofit organization with 1,500 members in 62 countries. This grass-roots alliance of amateur and professional radio astronomers is headed by executive director emeritus H. Paul Shuch, the engineer credited with developing the world's first commercial home satellite TV receiver. Many SETI League members are licensed radio amateurs and microwave experimenters. Others are digital signal processing experts and computer enthusiasts.

The SETI League pioneered the conversion of backyard satellite TV dishes in diameter into research-grade radio telescopes of modest sensitivity. The organization concentrates on coordinating a global network of small, amateur-built radio telescopes under Project Argus, an all-sky survey seeking to achieve real-time coverage of the entire sky. Project Argus was conceived as a continuation of the all-sky survey component of the late NASA SETI program (the targeted search having been continued by the SETI Institute's Project Phoenix). There are currently 143 Project Argus radio telescopes operating in 27 countries. Project Argus instruments typically exhibit sensitivity on the order of 10 Watts/square metre, or roughly equivalent to that achieved by the Ohio State University Big Ear radio telescope in 1977, when it detected the landmark "Wow!" candidate signal.

The name "Argus" derives from the mythical Greek guard-beast who had 100 eyes, and could see in all directions at once. In the SETI context, the name has been used for radio telescopes in fiction (Arthur C. Clarke, ""Imperial Earth""; Carl Sagan, ""Contact""), was the name initially used for the NASA study ultimately known as "Cyclops," and is the name given to an omnidirectional radio telescope design being developed at the Ohio State University.

While most SETI sky searches have studied the radio spectrum, some SETI researchers have considered the possibility that alien civilizations might be using powerful lasers for interstellar communications at optical wavelengths. The idea was first suggested by R. N. Schwartz and Charles Hard Townes in a 1961 paper published in the journal "Nature" titled "Interstellar and Interplanetary Communication by Optical Masers". However, the 1971 Cyclops study discounted the possibility of optical SETI, reasoning that construction of a laser system that could outshine the bright central star of a remote star system would be too difficult. In 1983, Townes published a detailed study of the idea in the United States journal "Proceedings of the National Academy of Sciences", which was met with widespread agreement by the SETI community.

There are two problems with optical SETI. The first problem is that lasers are highly "monochromatic", that is, they emit light only on one frequency, making it troublesome to figure out what frequency to look for. However, emitting light in narrow pulses results in a broad spectrum of emission; the spread in frequency becomes higher as the pulse width becomes narrower, making it easier to detect an emission.

The other problem is that while radio transmissions can be broadcast in all directions, lasers are highly directional. Interstellar gas and dust is almost transparent to near infrared, so these signals can be seen from greater distances, but the extraterrestrial laser signals would need to be transmitted in the direction of Earth in order to be detected.

Optical SETI supporters have conducted paper studies of the effectiveness of using contemporary high-energy lasers and a ten-meter diameter mirror as an interstellar beacon. The analysis shows that an infrared pulse from a laser, focused into a narrow beam by such a mirror, would appear thousands of times brighter than the Sun to a distant civilization in the beam's line of fire. The Cyclops study proved incorrect in suggesting a laser beam would be inherently hard to see.

Such a system could be made to automatically steer itself through a target list, sending a pulse to each target at a constant rate. This would allow targeting of all Sun-like stars within a distance of 100 light-years. The studies have also described an automatic laser pulse detector system with a low-cost, two-meter mirror made of carbon composite materials, focusing on an array of light detectors. This automatic detector system could perform sky surveys to detect laser flashes from civilizations attempting contact.

Several optical SETI experiments are now in progress. A Harvard-Smithsonian group that includes Paul Horowitz designed a laser detector and mounted it on Harvard's optical telescope. This telescope is currently being used for a more conventional star survey, and the optical SETI survey is "piggybacking" on that effort. Between October 1998 and November 1999, the survey inspected about 2,500 stars. Nothing that resembled an intentional laser signal was detected, but efforts continue. The Harvard-Smithsonian group is now working with Princeton University to mount a similar detector system on Princeton's 91-centimeter (36-inch) telescope. The Harvard and Princeton telescopes will be "ganged" to track the same targets at the same time, with the intent being to detect the same signal in both locations as a means of reducing errors from detector noise.

The Harvard-Smithsonian SETI group led by Professor Paul Horowitz built a dedicated all-sky optical survey system along the lines of that described above, featuring a 1.8-meter (72-inch) telescope. The new optical SETI survey telescope is being set up at the Oak Ridge Observatory in Harvard, Massachusetts.

The University of California, Berkeley, home of SERENDIP and SETI@home, is also conducting optical SETI searches and collaborates with the NIROSETI program. The optical SETI program at Breakthrough Listen is being directed by Geoffrey Marcy, an extrasolar planet hunter, and it involves examination of records of spectra taken during extrasolar planet hunts for a continuous, rather than pulsed, laser signal. This survey uses the Automated Planet Finder 2.4-m telescope at the Lick Observatory, situated on the summit of Mount Hamilton, east of San Jose, California, USA. The other Berkeley optical SETI effort is being pursued by the Harvard-Smithsonian group and is being directed by Dan Werthimer of Berkeley, who built the laser detector for the Harvard-Smithsonian group. This survey uses a 76-centimeter (30-inch) automated telescope at Leuschner Observatory and an older laser detector built by Werthimer.

In May 2017, astronomers reported studies related to laser light emissions from stars, as a way of detecting technology-related signals from an alien civilization. The reported studies included KIC 8462852, an oddly dimming star in which its unusual starlight fluctuations may be the result of interference by an artificial megastructure, such as a Dyson swarm, made by such a civilization. No evidence was found for technology-related signals from KIC 8462852 in the studies.

The possibility of using interstellar messenger probes in the search for extraterrestrial intelligence was first suggested by Ronald N. Bracewell in 1960 (see Bracewell probe), and the technical feasibility of this approach was demonstrated by the British Interplanetary Society's starship study Project Daedalus in 1978. Starting in 1979, Robert Freitas advanced arguments for the proposition that physical space-probes are a superior mode of interstellar communication to radio signals. See Voyager Golden Record.

In recognition that any sufficiently advanced interstellar probe in the vicinity of Earth could easily monitor the terrestrial Internet, Invitation to ETI was established by Prof. Allen Tough in 1996, as a Web-based SETI experiment inviting such spacefaring probes to establish contact with humanity. The project's 100 Signatories includes prominent physical, biological, and social scientists, as well as artists, educators, entertainers, philosophers and futurists. Prof. H. Paul Shuch, executive director emeritus of The SETI League, serves as the project's Principal Investigator.

Inscribing a message in matter and transporting it to an interstellar destination can be enormously more energy efficient than communication using electromagnetic waves if delays larger than light transit time can be tolerated. That said, for simple messages such as "hello," radio SETI could be far more efficient. If energy requirement is used as a proxy for technical difficulty, then a solarcentric Search for Extraterrestrial Artifacts (SETA) may be a useful supplement to traditional radio or optical searches.

Much like the "preferred frequency" concept in SETI radio beacon theory, the Earth-Moon or Sun-Earth libration orbits might therefore constitute the most universally convenient parking places for automated extraterrestrial spacecraft exploring arbitrary stellar systems. A viable long-term SETI program may be founded upon a search for these objects.

In 1979, Freitas and Valdes conducted a photographic search of the vicinity of the Earth-Moon triangular libration points and , and of the solar-synchronized positions in the associated halo orbits, seeking possible orbiting extraterrestrial interstellar probes, but found nothing to a detection limit of about 14th magnitude. The authors conducted a second, more comprehensive photographic search for probes in 1982 that examined the five Earth-Moon Lagrangian positions and included the solar-synchronized positions in the stable L4/L5 libration orbits, the potentially stable nonplanar orbits near L1/L2, Earth-Moon , and also in the Sun-Earth system. Again no extraterrestrial probes were found to limiting magnitudes of 17–19th magnitude near L3/L4/L5, 10–18th magnitude for /, and 14–16th magnitude for Sun-Earth .

In June 1983, Valdes and Freitas used the 26 m radiotelescope at Hat Creek Radio Observatory to search for the tritium hyperfine line at 1516 MHz from 108 assorted astronomical objects, with emphasis on 53 nearby stars including all visible stars within a 20 light-year radius. The tritium frequency was deemed highly attractive for SETI work because (1) the isotope is cosmically rare, (2) the tritium hyperfine line is centered in the SETI waterhole region of the terrestrial microwave window, and (3) in addition to beacon signals, tritium hyperfine emission may occur as a byproduct of extensive nuclear fusion energy production by extraterrestrial civilizations. The wideband- and narrowband-channel observations achieved sensitivities of 5–14 x 10 W/m²/channel and 0.7-2 x 10 W/m²/channel, respectively, but no detections were made.

Technosignatures, including all signs of technology, are a recent avenue in the search for extraterrestrial intelligence. Technosignatures may originate from various sources, from megastructures such as Dyson spheres and space mirrors or space shaders to the atmospheric contamination created by an industrial civilization, or city lights on extrasolar planets, and may be detectable in the future with large hypertelescopes.

Technosignatures can be divided into three broad categories: astroengineering projects, signals of planetary origin, and spacecraft within and outside the Solar System.

An astroengineering installation such as a Dyson sphere, designed to convert all of the incident radiation of its host star into energy, could be detected through the observation of an infrared excess from a solar analog star, or by the star's apparent disappearance in the visible spectrum over several years. After examining some 100,000 nearby large galaxies, a team of researchers has concluded that none of them display any obvious signs of highly advanced technological civilizations.

Another hypothetical form of astroengineering, the Shkadov thruster, moves its host star by reflecting some of the star's light back on itself, and would be detected by observing if its transits across the star abruptly end with the thruster in front. Asteroid mining within the Solar System is also a detectable technosignature of the first kind.

Individual extrasolar planets can be analyzed for signs of technology. Avi Loeb of the Harvard-Smithsonian Center for Astrophysics has proposed that persistent light signals on the night side of an exoplanet can be an indication of the presence of cities and an advanced civilization. In addition, the excess infrared radiation and chemicals produced by various industrial processes or terraforming efforts may point to intelligence.

Light and heat detected from planets need to be distinguished from natural sources to conclusively prove the existence of civilization on a planet. However, as argued by the Colossus team,
a civilization heat signature should be within a "comfortable" temperature range, like terrestrial urban heat islands, i.e. only a few degrees warmer than the planet itself. In contrast, such natural sources as wild fires, volcanoes, etc. are significantly hotter, so they will be well distinguished by their maximum flux at a different wavelength.

Extraterrestrial craft are another target in the search for technosignatures. Magnetic sail interstellar spacecraft should be detectable over thousands of light-years of distance through the synchrotron radiation they would produce through interaction with the interstellar medium; other interstellar spacecraft designs may be detectable at more modest distances. In addition, robotic probes within the Solar System are also being sought out with optical and radio searches.

For a sufficiently advanced civilization, hyper energetic neutrinos from Planck scale accelerators should be detectable at a distance of many Mpc.

Italian physicist Enrico Fermi suggested in the 1950s that if technologically advanced civilizations are common in the universe, then they should be detectable in one way or another. (According to those who were there, Fermi either asked "Where are they?" or "Where is everybody?")

The Fermi paradox is commonly understood as asking why extraterrestrials have not visited Earth, but the same reasoning applies to the question of why signals from extraterrestrials have not been heard. The SETI version of the question is sometimes referred to as "the Great Silence".

The Fermi paradox can be stated more completely as follows:
There are multiple explanations proposed for the Fermi paradox, ranging from analyses suggesting that intelligent life is rare (the "Rare Earth hypothesis"), to analyses suggesting that although extraterrestrial civilizations may be common, they would not communicate with us, could not travel across interstellar distances, or destroy themselves before they master the technology of either interstellar travel or communication.

The German astrophysicist and radio astronomer Sebastian von Hoerner suggested that the average duration of civilization was 6,500 years. After this time, according to him, it disappears for external reasons (the destruction of life on the planet, the destruction of only rational beings) or internal causes (mental or physical degeneration). According to his calculations, on a habitable planet (one in 3 million stars) there is a sequence of technological species over a time distance of hundreds of millions of years, and each of them "produces" an average of 4 technological species. With these assumptions, the average distance between civilizations in the Milky Way is 1,000 light years.

Science writer Timothy Ferris has posited that since galactic societies are most likely only transitory, an obvious solution is an interstellar communications network, or a type of library consisting mostly of automated systems. They would store the cumulative knowledge of vanished civilizations and communicate that knowledge through the galaxy. Ferris calls this the "Interstellar Internet", with the various automated systems acting as network "servers". If such an Interstellar Internet exists, the hypothesis states, communications between servers are mostly through narrow-band, highly directional radio or laser links. Intercepting such signals is, as discussed earlier, very difficult. However, the network could maintain some broadcast nodes in hopes of making contact with new civilizations.

Although somewhat dated in terms of "information culture" arguments, not to mention the obvious technological problems of a system that could work effectively for billions of years and requires multiple lifeforms agreeing on certain basics of communications technologies, this hypothesis is actually testable (see below).

A significant problem is the vastness of space. Despite piggybacking on the world's most sensitive radio telescope, Charles Stuart Bowyer said, the instrument could not detect random radio noise emanating from a civilization like ours, which has been leaking radio and TV signals for less than 100 years. For SERENDIP and most other SETI projects to detect a signal from an extraterrestrial civilization, the civilization would have to be beaming a powerful signal directly at us. It also means that Earth civilization will only be detectable within a distance of 100 light-years.

The International Academy of Astronautics (IAA) has a long-standing SETI Permanent Study Group (SPSG, formerly called the IAA SETI Committee), which addresses matters of SETI science, technology, and international policy. The SPSG meets in conjunction with the International Astronautical Congress (IAC) held annually at different locations around the world, and sponsors two SETI Symposia at each IAC. In 2005, the IAA established the SETI: Post-Detection Science and Technology Taskgroup (Chairman, Professor Paul Davies) "to act as a Standing Committee to be available to be called on at any time to advise and consult on questions stemming from the discovery of a putative signal of extraterrestrial intelligent (ETI) origin."

However, the protocols mentioned apply only to radio SETI rather than for METI (Active SETI). The intention for METI is covered under the SETI charter "Declaration of Principles Concerning Sending Communications with Extraterrestrial Intelligence".

On October 2000 astronomers Iván Almár and Jill Tarter presented a paper to The SETI Permanent Study Group in Rio de Janeiro, Brazil which proposed a scale (modelled after the Torino scale) which is an ordinal scale between zero and ten that quantifies the impact of any public announcement regarding evidence of extraterrestrial intelligence; the Rio scale has since inspired the 2005 San Marino Scale (in regard to the risks of transmissions from Earth) and the 2010 London Scale (in regard to the detection of extraterrestrial life) The Rio Scale itself was revised in 2018.

The SETI Institute does not officially recognize the Wow! signal as of extraterrestrial origin (as it was unable to be verified). The SETI Institute has also publicly denied that the candidate signal Radio source SHGb02+14a is of extraterrestrial origin. Although other volunteering projects such as Zooniverse credit users for discoveries, there is currently no crediting or early notification by SETI@Home following the discovery of a signal.

Some people, including Steven M. Greer, have expressed cynicism that the general public might not be informed in the event of a genuine discovery of extraterrestrial intelligence due to significant vested interests. Some, such as Bruce Jakosky have also argued that the official disclosure of extraterrestrial life may have far reaching and as yet undetermined implications for society, particularly for the world's religions.

Active SETI, also known as messaging to extraterrestrial intelligence (METI), consists of sending signals into space in the hope that they will be picked up by an alien intelligence.

In November 1974, a largely symbolic attempt was made at the Arecibo Observatory to send a message to other worlds. Known as the Arecibo Message, it was sent towards the globular cluster M13, which is 25,000 light-years from Earth. Further IRMs Cosmic Call, Teen Age Message, Cosmic Call 2, and A Message From Earth were transmitted in 1999, 2001, 2003 and 2008 from the Evpatoria Planetary Radar.

Physicist Stephen Hawking, in his book "A Brief History of Time", suggests that "alerting" extraterrestrial intelligences to our existence is foolhardy, citing humankind's history of treating its own kind harshly in meetings of civilizations with a significant technology gap, e.g., the extermination of Tasmanian aborigines. He suggests, in view of this history, that we "lay low". In one response to Hawking, in September 2016, astronomer Seth Shostak, allays such concerns. Astronomer Jill Tarter also disagrees with Hawking, arguing that aliens developed and long-lived enough to communicate and travel across interstellar distances would have evolved a cooperative and less violent intelligence. She does think it is too soon for humans to attempt active SETI and that humans should be more advanced technologically first but keep listening in the meantime.

The concern over METI was raised by the science journal "Nature" in an editorial in October 2006, which commented on a recent meeting of the International Academy of Astronautics SETI study group. The editor said, "It is not obvious that all extraterrestrial civilizations will be benign, or that contact with even a benign one would not have serious repercussions" (Nature Vol 443 12 October 06 p 606). Astronomer and science fiction author David Brin has expressed similar concerns.

Richard Carrigan, a particle physicist at the Fermi National Accelerator Laboratory near Chicago, Illinois, suggested that passive SETI could also be dangerous and that a signal released onto the Internet could act as a computer virus. Computer security expert Bruce Schneier dismissed this possibility as a "bizarre movie-plot threat".

To lend a quantitative basis to discussions of the risks of transmitting deliberate messages from Earth, the SETI Permanent Study Group of the International Academy of Astronautics adopted in 2007 a new analytical tool, the San Marino Scale. Developed by Prof. Ivan Almar and Prof. H. Paul Shuch, the scale evaluates the significance of transmissions from Earth as a function of signal intensity and information content. Its adoption suggests that not all such transmissions are equal, and each must be evaluated separately before establishing blanket international policy regarding active SETI.

However, some scientists consider these fears about the dangers of METI as panic and irrational superstition; see, for example, Alexander L. Zaitsev's papers. Biologist João Pedro de Magalhães also proposed in 2015 transmitting an invitation message to any extraterrestrial intelligences watching us already in the context of the Zoo Hypothesis and inviting them to respond, arguing this would not put us in any more danger than we are already if the Zoo Hypothesis is correct.

On 13 February 2015, scientists (including Geoffrey Marcy, Seth Shostak, Frank Drake, Elon Musk and David Brin) at a convention of the American Association for the Advancement of Science, discussed Active SETI and whether transmitting a message to possible intelligent extraterrestrials in the Cosmos was a good idea; one result was a statement, signed by many, that a "worldwide scientific, political and humanitarian discussion must occur before any message is sent". On 28 March 2015, a related essay was written by Seth Shostak and published in "The New York Times".

The Breakthrough Message program is an open competition announced in July 2015 to design a digital message that could be transmitted from Earth to an extraterrestrial civilization, with a US$1,000,000 prize pool. The message should be "representative of humanity and planet Earth". The program pledges "not to transmit any message until there has been a wide-ranging debate at high levels of science and politics on the risks and rewards of contacting advanced civilizations".

As various SETI projects have progressed, some have criticized early claims by researchers as being too "euphoric". For example, Peter Schenkel, while remaining a supporter of SETI projects, wrote in 2006 that

SETI has also occasionally been the target of criticism by those who suggest that it is a form of pseudoscience. In particular, critics allege that no observed phenomena suggest the existence of extraterrestrial intelligence, and furthermore that the assertion of the existence of extraterrestrial intelligence has no good Popperian criteria for falsifiability, as explained in a 2009 editorial in "Nature", which said:

"Nature" added that SETI was "marked by a hope, bordering on faith" that aliens were aiming signals at us, that a hypothetical alien SETI project looking at Earth with "similar faith" would be "sorely disappointed" (despite our many untargeted radar and TV signals, and our few targeted Active SETI radio signals denounced by those fearing aliens), and that it had difficulties attracting even sympathetic working scientists and Government funding because it was "an effort so likely to turn up nothing".

However "Nature" also added that "Nonetheless, a small SETI effort is well worth supporting, especially given the enormous implications if it did succeed" and that "happily, a handful of wealthy technologists and other private donors have proved willing to provide that support".

Supporters of the Rare Earth Hypothesis argue that advanced lifeforms are likely to be very rare, and that, if that is so, then SETI efforts will be futile. However the Rare Earth Hypothesis itself faces many criticisms.

In 1993 Roy Mash claimed that "Arguments favoring the existence of extraterrestrial intelligence nearly always contain an overt appeal to big numbers, often combined with a covert reliance on generalization from a single instance" and concluded that "the dispute between believers and skeptics is seen to boil down to a conflict of intuitions which can barely be engaged, let alone resolved, given our present state of knowledge". In 2012 Milan M. Ćirković (who was then research professor at the Astronomical Observatory of Belgrade and a research associate of the Future of Humanity Institute at the University of Oxford) claimed that Mash was unrealistically over-reliant on excessive abstraction that ignored the empirical information available to modern SETI researchers.

George Basalla, Emeritus Professor of History at the University of Delaware, is a critic of SETI who argued in 2006 that "extraterrestrials discussed by scientists are as imaginary as the spirits and gods of religion or myth", and has in turn been criticized by Milan M. Ćirković for, among other things, being unable to distinguish between "SETI believers" and "scientists engaged in SETI", who are often sceptical (especially about quick detection), such as Freeman Dyson (and, at least in their later years, Iosif Shklovsky and Sebastian von Hoerner), and for ignoring the difference between the knowledge underlying the arguments of modern scientists and those of ancient Greek thinkers.

Massimo Pigliucci, Professor of Philosophy at CUNY-City College, asked in 2010 whether SETI is "uncomfortably close to the status of pseudoscience" due to the lack of any clear point at which negative results cause the hypothesis of Extraterrestrial Intelligence to be abandoned, before eventually concluding that SETI is "almost-science", which is described by Milan M. Ćirković as Pigliucci putting SETI in "the illustrious company of string theory, interpretations of quantum mechanics, evolutionary psychology and history (of the 'synthetic' kind done recently by Jared Diamond)", while adding that his justification for doing so with SETI "is weak, outdated, and reflecting particular philosophical prejudices similar to the ones described above in Mash and Basalla".




</doc>
<doc id="28154" url="https://en.wikipedia.org/wiki?curid=28154" title="Sextans">
Sextans

Sextans is a minor equatorial constellation which was introduced in 1687 by Johannes Hevelius. Its name is Latin for the astronomical sextant, an instrument that Hevelius made frequent use of in his observations.

Sextans as a constellation covers a rather dim, sparse region of the sky. It has only one star above the fifth magnitude, namely α Sextantis at 4.49. The constellation contains a few double stars, including γ, 35, and 40 Sextantis. There are a few notable variable stars, including β, 25, 23 Sextantis, and LHS 292. NGC 3115, an edge-on lenticular galaxy, is the only noteworthy deep-sky object. It also lies near the ecliptic, which causes the Moon, and some of the planets to occasionally pass through it for brief periods of time.

The constellation is the location of the field studied by the COSMOS project, undertaken by the Hubble Space Telescope.

Sextans B is a fairly bright dwarf irregular galaxy at magnitude 6.6, 4.3 million light-years from Earth. It is part of the Local Group of galaxies.

CL J1001+0220 is as of 2016 the most distant-known galaxy cluster at redshift z=2.506, 11.1 billion light-years from Earth.

In June 2015, astronomers reported evidence for population III stars in the Cosmos Redshift 7 galaxy (at "z" = 6.60) found in Sextans. Such stars are likely to have existed in the very early universe (i.e., at high redshift), and may have started the production of chemical elements heavier than hydrogen that are needed for the later formation of planets and life as we know it.



</doc>
<doc id="28156" url="https://en.wikipedia.org/wiki?curid=28156" title="Salem al-Hazmi">
Salem al-Hazmi

Salem al-Hazmi (, , also transliterated as Alhazmi) (February 2, 1981 – September 11, 2001) was one of five hijackers of American Airlines Flight 77 as part of the September 11 attacks.

Hazmi had a relatively long history with al-Qaeda before being selected for the attacks. He obtained a tourist visa through the Visa Express program and arrived in the United States in June 2001 where he would settle in New Jersey with other American 77 hijackers up until the attacks.

On September 11, 2001, Hazmi boarded American Airlines Flight 77 and helped subdue the passengers and crew for Hani Hanjour, the pilot among the hijackers, to crash the plane into west facade of the Pentagon. His older brother, Nawaf al-Hazmi, was another hijacker aboard the same flight. Along with Ahmed al-Haznawi, who was one of the hijackers on United Airlines Flight 93 and Hamza al-Ghamdi, who was also co-hijacker with his older brother on United Airlines 175, he was among the youngest hijackers in the attacks.

Hazmi was born on February 2, 1981 to Muhammad Salim al-Hazmi, a grocer, in Mecca, Saudi Arabia. His father described Salem as a quarrelsome teenager who had problems with alcohol and petty theft. However, he stopped drinking and began to attend the mosque about three months before he left his family.

There are reports that he fought in Afghanistan with his brother, Nawaf al-Hazmi, and other reports say the two fought together in Chechnya. Salem al-Hazmi was an al-Qaeda veteran by the time he was selected for participation in the 9/11 attacks. U.S. intelligence learned of Hazmi's involvement with al-Qaeda as early as 1999, but he was not placed on any watchlists.

Known as "Bilal" during the preparations, both he and Ahmed al-Ghamdi flew to Beirut in November 2000, though on separate flights.

Along with Nawaf al-Hazmi and several other future hijackers, Salem al-Hazmi may have attended the 2000 Al Qaeda Summit in Kuala Lumpur, Malaysia. It was there that the details of the 9/11 attacks were decided upon.
According to the FBI and the 9/11 Commission report, Hazmi first entered the United States on June 29, 2001, although there are numerous unconfirmed reports that he was living in San Antonio, Texas with fellow hijacker Satam al-Suqami much earlier. Hazmi used the controversial Visa Express program to gain entry into the country.

Hazmi moved to Paterson, New Jersey where he lived with Hani Hanjour. Both were among the five hijackers who applied for Virginia identity cards at the Arlington office of the Virginia Department of Motor Vehicles on August 2, 2001, although Salem already held an NJ identity card.

On August 27, brothers Nawaf and Salem purchased flight tickets through Travelocity.com using Nawaf's visa card.

With the four other Flight 77 hijackers, he worked out at a Gold's Gym in Greenbelt, Maryland from September 2 to September 6 of the same year.

On September 11, 2001, Hazmi boarded American Airlines Flight 77. Airport surveillance video from Washington's Dulles Airport shows two of the five hijackers, including Salem al-Hazmi, being pulled aside to undergo additional scrutiny after setting off metal detectors.

The flight was scheduled to depart at 08:10, but ended up departing 10 minutes late from Gate D26 at Dulles. The last normal radio communications from the aircraft to air traffic control occurred at 08:50:51. At 08:54, Flight 77 began to deviate from its normal, assigned flight path and turned south, and then hijackers set the flight's autopilot heading for Washington, D.C. Passenger Barbara Olson called her husband, United States Solicitor General Theodore Olson, and reported that the plane had been hijacked and that the assailants had box cutters and knives. At 09:37, American Airlines Flight 77 crashed into the west facade of the Pentagon, killing all 64 aboard (including the hijackers), along with 125 on the ground in the Pentagon. In the recovery process at the Pentagon, remains of all five Flight 77 hijackers were identified through a process of elimination, as not matching any DNA samples for the victims, and put into custody of the FBI. Forensics teams confirmed that it seemed two of the hijackers were brothers, based on their DNA similarities.

Shortly after the attacks, several sources reported that Salem al-Hazmi, 26, was alive and working at a petrochemical plant in Yanbu, Saudi Arabia. He claimed that his passport had been stolen by a pickpocket in Cairo three years before, and that the pictures and details such as date of birth released to the public by the FBI were his own. He also stated that he had never visited the United States, but volunteered to fly to the U.S. to prove his innocence. On September 19, "Al-Sharq Al-Awsat" published his photograph alongside Badr Alhazmi's, who they claimed was the actual hijacker who had stolen his identity.

After some confusion and doubt Saudi Arabia admitted that in fact the names of the hijackers were correct. "The names that we got confirmed that," Interior Minister Prince Nayef said in an interview with The Associated Press. "Their families have been notified." Nayef said the Saudi leadership was shocked to learn 15 of the hijackers were from Saudi Arabia and said it was natural that the kingdom had not noticed their involvement beforehand.



</doc>
<doc id="28157" url="https://en.wikipedia.org/wiki?curid=28157" title="Satsuma Province">
Satsuma Province

Satsuma's provincial capital was Satsumasendai. During the Sengoku period, Satsuma was a fief of the Shimazu "daimyō", who ruled much of southern Kyūshū from their castle at Kagoshima city. They were the initial patrons of Satsuma ware, which was later widely exported to the West.

In 1871, with the abolition of feudal domains and the establishment of prefectures after the Meiji Restoration, the provinces of Satsuma and Ōsumi were combined to eventually establish Kagoshima Prefecture.

Satsuma was one of the main provinces that rose in opposition to the Tokugawa shogunate in the mid 19th century. Because of this, the oligarchy that came into power after the Meiji Restoration of 1868 had a strong representation from the Satsuma province, with leaders such as Ōkubo Toshimichi and Saigō Takamori taking up key government positions.

Satsuma is well known for its production of sweet potatoes, known in Japan as 薩摩芋 (satsuma-imo or "Satsuma potato"). Satsuma mandarins (known as "mikan" in Japan) do not specifically originate from Satsuma but were imported into the West through this province in the Meiji era.






</doc>
<doc id="28159" url="https://en.wikipedia.org/wiki?curid=28159" title="Scottish">
Scottish

Scottish usually refers to something of, from, or related to Scotland, including:




</doc>
<doc id="28161" url="https://en.wikipedia.org/wiki?curid=28161" title="List of brightest stars">
List of brightest stars

This is a list of stars down to magnitude +2.50, as determined by their "maximum", "total", or "combined" visual magnitudes as viewed from Earth. Although several of the brightest stars are known binary or multiple star systems and are relatively close to Earth, they appear to the naked eye as single stars. The list below combines/adds the magnitudes of bright individual components. Most of the proper names in this list are those approved by the Working Group on Star Names. Popular star names here that have not been approved by the IAU appear with a short note.

The Sun is the brightest star as viewed from Earth. The apparent visual magnitudes of the brightest stars can also be compared to non-stellar objects in our Solar System. Here the maximum visible magnitudes above the second brightest star, Sirius (−1.46), are as follows. Excluding the Sun, the brightest objects are the Moon (−12.7), Venus (−4.89), Jupiter (−2.94), Mars (−2.91), Mercury (−2.45), and Saturn (−0.49).

Any exact order of the visual brightness of stars is not perfectly defined for four reasons:


The source of magnitudes cited in this list is the linked Wikipedia articles—this basic list is a catalog of what Wikipedia itself documents. References can be found in the individual articles.
The 92 stars listed above (thus omitting the sun) are in 38 modern constellations (of the 88 possible constellations), in turn covering 61.1% of our surrounds (the celestial sphere). 

As drawn, diminutive Crux, which has "three" of these stars is the most densely populated as to these stars (this 3.26% of the list being 19.2 times more than the expected 0.17% that would result on a homogenous distribution of all bright stars and a randomised drawing of constellations, being its area). Virgo and Hydra have one such star yet more than 3% each of the night sky associated with them by professional astronomers, as these constellation's limits have been drawn by the IAU. Among the 50 constellations with none of these, Hercules is the largest, covering 2.97% of the galactic and extra-galactic surrounds.




</doc>
<doc id="28162" url="https://en.wikipedia.org/wiki?curid=28162" title="List of nearest stars and brown dwarfs">
List of nearest stars and brown dwarfs

Some 52 stellar systems beyond our own, the Solar System, currently lie within of the Sun. These systems contain a total of 63 stars, of which 50 are red dwarfs, by far the most common type of star in the Milky Way. Much more massive stars, such as our own, make up the remaining 13. In addition to these "true" stars, scientists have identified 11 brown dwarfs (objects not quite massive enough to fuse hydrogen), and four white dwarfs (extremely dense collapsed cores that remain after stars such as our Sun have exhausted all fusable hydrogen in their core and have shed slowly their outer layers). Despite the relative proximity of these 78 objects to Earth, only nine are bright enough in visible light to reach or exceed the dimmest brightness to be visible to the naked eye from Earth, 6.5 apparent magnitude. All of these objects are currently moving in the Local Bubble, a region within the Orion–Cygnus Arm of the Milky Way.
Based on results from the Gaia telescope's second data release from April 2018, an estimated 694 stars will possibly approach the Solar System to less than 5 parsecs in the next 15 million years. Of these, 26 have a good probability to come within and another 7 within . This number is likely much higher, due to the sheer number of stars needed to be surveyed; a star approaching the Solar System 10 million years ago, moving at a typical Sun-relative 20–200 kilometers per second, would be 600–6,000 light-years from the Sun at present day, with millions of stars closer to the Sun. The closest encounter to the Sun so far predicted is the low-mass orange dwarf star Gliese 710 / HIP 89825 with roughly 60% the mass of the Sun. It is currently predicted to pass from the Sun in million years from the present, close enough to significantly disturb our Solar System's Oort cloud.
The easiest way to determine stellar distance to the Sun for objects at these distances is parallax, which measures how much stars appear to move against background objects over the course of Earth's orbit around the Sun. As a parsec (parallax-second) is defined by the distance of an object that would appear to move exactly one second of arc against background objects, stars less than 5 parsecs away will have measured parallaxes of over 0.2 arcseconds, or 200 milliarcseconds. Determining past and future positions relies on accurate astrometric measurements of their parallax and total proper motions (how far they move across the sky due to their actual velocity relative to the Sun), along with spectroscopically determined radial velocities (their speed directly towards or away from us, which combined with proper motion defines their true movement through the sky relative to the Sun). Both of these measurements are subject to increasing and significant errors over very long time spans, especially over the several thousand-year time spans it takes for stars to noticeably move relative to each other.

The classes of the stars and brown dwarfs are shown in the color of their spectral types (these colors are derived from conventional names for the spectral types and do not represent the star's observed color). Many brown dwarfs are not listed by visual magnitude but are listed by near-infrared J band apparent magnitude due to how dim (and often invisible) they are in visible color bands (U, B or V). Absolute magnitude (with electromagnetic wave, 'light' band denoted in subscript) is a measurement at a 10-parsec distance across imaginary empty space devoid of all its sparse dust and gas. Some of the parallaxes and resultant distances are rough measurements.
Over long periods of time, the slow independent motion of stars change in both relative position and in their distance from the observer. This can cause other currently distant stars to fall within a stated range, which may be readily calculated and predicted using accurate astrometric measurements of parallax and total proper motions, along with spectroscopically determined radial velocities. Although predictions can be extrapolated back into the past or forward into the future, they are subject to increasing significant cumulative errors over very long periods. Inaccuracies of these measured parameters make determining the true minimum distances of any encountering stars or brown dwarfs fairly difficult.

One of the first stars known to approach the Sun particularly close is Gliese 710. The star, whose mass is roughly half that of the Sun, is currently 62 light-years from the Solar System. It was first noticed in 1999 using data from the Hipparcos satellite, and was estimated to pass less than from the Sun in 1.4 million years. With the release of "Gaia"'s observations of the star, it has since been refined to a much closer , close enough to significantly disturb objects in the Oort cloud, which extends out to from the Sun.

The second-closest object known to approach the Sun was only discovered in 2018 after "Gaia" second data release, known as 2MASS J0610-4246. Its approach has not been fully described due to it being a distant binary star with a red dwarf, but almost certainly passed less than 1 light-year from the Solar System roughly 1.16 million years ago.




</doc>
<doc id="28163" url="https://en.wikipedia.org/wiki?curid=28163" title="Sagitta">
Sagitta

Sagitta is a dim but distinctive constellation in the northern sky. Its name is Latin for "arrow", and it should not be confused with the significantly larger constellation Sagittarius, the archer. Although Sagitta is an ancient constellation, it has no star brighter than 3rd magnitude and has the third-smallest area of all constellations. It was included among the 48 constellations listed by the 2nd century astronomer Ptolemy, and it remains one of the 88 modern constellations defined by the International Astronomical Union. Located to the north of the equator, Sagitta can be seen from every location on Earth except within the Antarctic circle.

The red giant Gamma Sagittae is the constellation's brightest star, with an apparent magnitude of 3.47. Delta, Epsilon, Zeta and Theta Sagittae are multiple stars whose components can be seen in small telescopes. V Sagittae is a cataclysmic variable binary star system that is expected to go nova and briefly become the most luminous star in the Milky Way and one of the brightest stars in our sky around the year 2083. Two star systems in Sagitta have Jupiter-like planets, while a third—15 Sagittae—has a brown dwarf companion.

The Ancient Greeks called this constellation "Oistos" "the arrow". It was regarded as the weapon that Hercules used to kill the eagle (Aquila) of Jove that perpetually gnawed Prometheus' liver. The Arrow is located beyond the north border of Aquila, the Eagle. Richard Hinckley Allen proposed that the Arrow could be the one shot by Hercules towards the adjacent Stymphalian birds (6th labor) who had claws, beaks and wings of iron, and who lived on human flesh in the marshes of Arcadia—Aquila the Eagle, Cygnus the Swan, and Lyra (the Vulture)—and still lying between them, whence the title Herculea. Eratosthenes claimed it as the arrow with which Apollo exterminated the Cyclopes. The Romans named it Sagitta. In Arabic, it became "al-sahm" "arrow", though this name became "Sham" and was transferred to Alpha Sagittae only. The Greek name has also been mistranslated as "ὁ istos" "the loom" and thus in Arabic "al-nawl". It was also called "al-'anaza" "pike/javelin".

Four stars make up an arrow-shaped asterism located due north of the bright star Altair. Covering 79.9 square degrees and hence 0.194% of the sky, Sagitta ranks 86th of the 88 modern constellations by area. Only Equuleus and Crux are smaller. Sagitta is most readily observed from the late spring to early autumn to northern hemisphere observers, with midnight culmination occurring on 17 July. Its position in the Northern Celestial Hemisphere means that the whole constellation is visible to observers north of 69°S. Sagitta is bordered by Vulpecula to the north, Hercules to the west, Aquila to the south, and Delphinus to the east. The three-letter abbreviation for the constellation, as adopted by the International Astronomical Union in 1922, is "Sge"; American astronomer Henry Norris Russell, who devised the code, had to resort to using the genitive form of the name to come up with a letter to include ('e') that was not in the name of the constellation Sagittarius. The official constellation boundaries, as set by Eugène Delporte in 1930, are defined by a polygon of twelve segments ("illustrated in infobox"). In the equatorial coordinate system, the right ascension coordinates of these borders lie between and , while the declination coordinates are between 16.08° and 21.64°.

Johann Bayer gave Bayer designations to eight stars, labelling them Alpha to Theta. What was viewed by Bayer, Friedrich Wilhelm Argelander, and Heis as a single star Theta was in fact three stars, and it is now equated to HR 7705. John Flamsteed added the letters x (mistaken as Chi), y and z to 13, 14 and 15 Sagittae in his "Catalogus Britannicus". All three were dropped by later astronomers John Bevis and Francis Baily.

In his "Uranometria", Bayer depicted Alpha, Beta and Epsilon Sagittae as the fins of the arrow. Also known as Sham, Alpha is a yellow bright giant star of spectral class G1 II with an apparent magnitude of 4.38, which lies at a distance of from Earth. Originally 4 times as massive as the Sun, it has swollen and brightened to 20 times the Sun's diameter and 340 times its luminosity. Also of magnitude 4.38, Beta is a G-type giant located distant from Earth. Estimated to be around 129 million years old, it is 4.33 times as massive as the Sun, and has expanded to roughly 27 times its diameter. Epsilon Sagittae is a double star whose component stars can be seen in a small telescope. With an apparent magnitude of 5.77, the main star is a 331-million year-old yellow giant of spectral type G8 III around 3.09 times as massive as the Sun, that has swollen to its diameter. It is distant. Its companion of magnitude 8.35 is 87.4 arc seconds distant, but is actually an unrelated blue supergiant around 7,000 light-years distant from Earth. Delta and Zeta depicted the spike according to Bayer. The Delta Sagittae system is composed of a red supergiant of spectral type M2 II that has 3.9 times the Sun's mass and 152 times its diameter and a blue-white main sequence star that is 2.9 time as massive as the Sun. The two orbit each other every ten years. Zeta Sagittae is a triple system, approximately 326 light-years from Earth, the primary an A-type star.
Ptolemy saw the constellation's brightest star Gamma Sagittae as marking the arrow's head, while Bayer saw Gamma, Eta and Theta as depicting the arrow's shaft. Gamma Sagittae is a red giant of spectral type M0III, and magnitude 3.47. It lies at a distance of from Earth. It has around 90% of the Sun's mass yet has a radius 54 times that of the Sun and is 575 times as bright. It is most likely on the red-giant branch of its evolutionary lifespan, having exhausted its core hydrogen and now burning it in a shell.

Eta Sagittae is an orange giant of spectral class K2 III with a magnitude of 5.1. Located from Earth, it has a 61.1% chance of being a member of the Hyades-Pleiades stream of stars that share a common motion through space. Theta Sagittae is a binary double star system, with two components 12 arcseconds apart visible in a small telescope. At magnitude 6.5, the brighter is a yellow-white main sequence star of spectral type F3V, located from Earth. The 8.8-magnitude fainter companion is a main sequence star of spectral type G5V. A 7.4-magnitude orange giant of spectral type K2III is also visible from the binary pair, located away.

R Sagittae is a member of the rare RV Tauri variable class of star. It ranges in magnitude from 8.2 to 10.4. It is around 8,100 light-years distant. It has a diameter times that of the Sun, and is as luminous, yet most likely is less massive than the Sun. An ageing star, it has moved on from the asymptotic giant branch of stellar evolution and is on its way to becoming a planetary nebula. FG Sagittae is a "born again" star, a highly luminous star around 4000 light years distant from Earth. It reignited fusion of a helium shell shortly before becoming a white dwarf, and has expanded first to a blue supergiant and then to a K-class supergiant in less than 100 years. It is surrounded by a faint (visual magnitude 23) planetary nebula, Henize 1-5, that formed when FG Sagittae first left the asymptotic giant branch.

S Sagittae is a classical Cepheid that varies from magnitude 5.24 to 6.04 every 8.38 days. It is a yellow-white supergiant that pulsates between spectral types F6Ib and G5Ib. Around 6 or 7 times as massive and 3500 times as luminous as the Sun, it is located around 5,100 light-years away from Earth. HD 183143 is a remote highly luminous star around 7900 light-years away, that has been classified as a blue hypergiant. Infrared bands of ionised buckminsterfullerene have also been found in its spectrum. WR 124 is a Wolf-Rayet star moving at great speed surrounded by a nebula of ejected gas.

U Sagittae is an eclipsing binary that varies between magnitudes 6.6 and 9.2 over 3.4 days, making it a suitable target for enthusiasts with small telescopes. There are two component stars—a blue-white star of spectral type B8V and an ageing star that has cooled and expanded into a yellow subgiant of spectral type G4III-IV. They orbit each other close enough so that the cooler subgiant has filled its Roche lobe and is passing material to the hotter star, and hence it is a semidetached binary system. The change in brightness is due to the smaller brighter star being eclipsed by the larger fainter star. The system is distant. Near U Sagittae is X Sagittae, a semiregular variable ranging between magnitudes 7.9 and 8.4 over 196 days. A carbon star, X Sagittae has a surface temperature of .

Located near 18 Sagittae is V Sagittae, the prototype of the V Sagittae variables, cataclysmic variables that are also super soft X-ray source. It is expected to become a luminous red nova when the two stars merge around the year 2083, and briefly become the most luminous star in the Milky Way and one of the brightest stars in our sky. WZ Sagittae is another cataclysmic variable, composed of a white dwarf that has about 85% the mass of the Sun, and low mass star companion that has been calculated to be a brown dwarf of spectral class L2 that is only 8% as massive as the Sun. Normally a faint object dimmer than magnitude 15, it flared up in 1913, 1946 and 1978 to be visible in binoculars. The black widow pulsar (B1957+20) is the second millisecond pulsar ever discovered. It is a massive neutron star that is ablating its brown dwarf-sized companion which causes the pulsar's radio signals to attenuate as they pass through the outflowing material.

HD 231701 is a yellow-white main sequence star hotter and larger than our Sun, with a Jupiter-like planet that was discovered in 2007 by the radial velocity technique . The planet orbits at a distance of from the star with a period of 141.6 days. HAT-P-34 is a star times as massive as the Sun with times its radius and times its luminosity. With an apparent magnitude of 10.4, it is distant. A planet times as massive as Jupiter was discovered transiting it in 2012. With a period of 5.45 days and a distance of 0.06 astronomical units from its star, it has an estimated surface temperature of . 15 Sagittae is a solar analog—a star similar to the Sun, with times its mass, times its radius and times its luminosity. It has a brown dwarf substellar companion that is around the same size as Jupiter but 69 times as massive with a surface temperature of between 1,510 and , taking around 73.3 years to complete an orbit around the star. The system is estimated to be billion years old.

The band of the Milky Way and the Great Rift within it pass though Sagitta, with Alpha, Beta and Epsilon Sagittae marking the Rift's border. Located between Beta and Gamma Sagittae is Messier 71, a very loose globular cluster mistaken for quite some time for a dense open cluster. At a distance of about 13,000 light-years from Earth,

There are two notable planetary nebulae in Sagitta: NGC 6886—composed of a hot central post-AGB star that has 55% of the Sun's mass yet 2700 ± 850 times its luminosity, with a surface teperature of 142,000 K, and surrounding nebula estimated to have been expanding for between 1280 and 1600 years, and the Necklace Nebula—originally a close binary that one component of which swallowed the other as it expanded to become a giant star. The smaller star remained in orbit inside the larger, whose rotation speed increased greatly, resulting in it flinging its outer layers off into space, forming a ring with knots of bright gas formed from clumps of stellar material. Both nebulae are around 15,000 light-years from Earth.




</doc>
<doc id="28164" url="https://en.wikipedia.org/wiki?curid=28164" title="Simon Ockley">
Simon Ockley

Simon Ockley (16789 August 1720) was a British Orientalist.

Ockley was born at Exeter. He was educated at Queens' College, Cambridge, and graduated B.A. in 1697, MA. in 1701, and B.D. in 1710. He became fellow of Jesus College and vicar of Swavesey, and in 1711 was chosen Adams Professor of Arabic in the university. He had a large family, and his latter days were embittered by pecuniary embarrassments, which form the subject of a chapter in Isaac D'Israeli's "Calamities of Authors". The preface to the second volume of his "History of the Saracens" is dated from Cambridge Castle, where he lay a prisoner for debt.

Ockley maintained that a knowledge of Oriental literature was essential to the proper study of theology, and in the preface to his first book, the "Introductio ad linguas orientales" (1706), he urges the importance of the study.

He died at Swavesey.



</doc>
<doc id="28165" url="https://en.wikipedia.org/wiki?curid=28165" title="Sharable Content Object Reference Model">
Sharable Content Object Reference Model

Shareable Content Object Reference Model (SCORM) is a collection of standards and specifications for web-based electronic educational technology (also called e-learning). It defines communications between client side content and a host system (called "the run-time environment"), which is commonly supported by a learning management system. SCORM also defines how content may be packaged into a transferable ZIP file called "Package Interchange Format."

SCORM is a specification of the Advanced Distributed Learning (ADL) Initiative from the Office of the United States Secretary of Defense.

SCORM 2004 introduced a complex idea called sequencing, which is a set of rules that specifies the order in which a learner may experience content objects. In simple terms, they constrain a learner to a fixed set of paths through the training material, permit the learner to "bookmark" their progress when taking breaks, and assure the acceptability of test scores achieved by the learner. The standard uses XML, and it is based on the results of work done by AICC, IEEE LTSC, and Ariadne.

SCORM was designed to be web-based and utilizes JavaScript to facilitate communication between the client side content and the run-time environment. Each SCORM version specifies the methods that the run-time environment should support and how those methods should behave. Content launched by the run time environment can then call those methods utilizing JavaScript.

This was the first version that was widely used. It is still widely used and is supported by most Learning Management Systems.

This is the current version. It is based on new standards for API and content object-to-runtime environment communication, with many ambiguities of previous versions resolved. Includes ability to specify adaptive sequencing of activities that use the content objects. Includes ability to share and use information about the success status for multiple learning objectives or competencies across content objects and across courses for the same learner within the same learning management system. A more robust test suite helps ensure good interoperability.



The Experience API (also known as xAPI or Tin Can API) was finalized to version 1.0 in April 2013. The Experience API solves many of the problems inherent with older versions of SCORM. Just like SCORM, ADL is the steward of the Experience API. AICC with their cmi5 planned to use xAPI as their transport standard, but AICC membership decided to dissolve the organization and transferred cmi5 to ADL.

The Experience API (Tin Can API) is a web service that allows software clients to read and write experiential data in the form of “statement” objects. In their simplest form, statements are in the form of “I did this”, or more generally “actor verb object”. More complex statement forms can be used. There is also a built-in query API to help filter recorded statements, and a state API that allows for a sort of “scratch space” for consuming applications. Experience API statements are stored in a data store called a Learning Record Store, which can exist on its own or within a Learning Management System.


Server software
Content editing software



</doc>
<doc id="28167" url="https://en.wikipedia.org/wiki?curid=28167" title="Sejm">
Sejm

The Sejm ( – ), officially known as the Sejm of the Republic of Poland, is the lower house of the bicameral parliament of Poland.

The Sejm has been the highest governing body of the Third Polish Republic since the transition of government in 1989. Along with the upper house of parliament, the Senate, it forms the national legislature in Poland. The Diet is composed of 460 deputies (singular "deputowany" or "poseł" – "envoy") elected every four years by a universal ballot. The Sejm is presided over by a speaker called the "Marshal of the Sejm" ("Marszałek Sejmu").

The first true lower house of parliament was founded in 1493 as an assembly of Polish nobles and their representatives. In the Kingdom of Poland, the term ""Sejm"" referred to an entire two-chamber parliament, comprising the Chamber of Deputies (), the Senate and the King. It was thus a three-estate parliament, and the absolute authority of the monarch was diminished by parliament's ruling under the "nihil novi" act from 1505. The Henrician Articles from 1573 further strengthened the assembly's jurisdiction, making Poland a constitutional elective monarchy. Since the Second Polish Republic (1918–1939), ""Sejm"" has referred only to the larger house of the parliament.

The origin of the contemporary "Sejm" (meaning "gathering") is traced back to the King's Councils – "wiece" – which gained considerable authority during the time of Poland's fragmentation (1146-1295). The 1180 Sejm in Łęczyca (known as the 'First Polish parliament') was the most notable of these councils, in that for the first time in Poland's history it established laws constraining the power of the ruler. It forbade arbitrary sequestration of supplies in the countryside and takeover of bishopric lands after the death of a bishop. These early "Sejm"s were not a regular event, they convened at the King's behest.

Following the 1493 "Sejm" in Piotrków, it became a regularly convening body, to which indirect elections were held every two years. The bicameral system was also established; the "Sejm" then comprised two chambers: the "Senat" (Senate) of 81 bishops and other dignitaries; and the Chamber of Deputies, made up of 54 envoys elected by smaller local "sejmik" (assemblies of landed nobility) in each of the Kingdom's provinces. At the time, Poland's nobility, which accounted for around 10% of the state's population (then the highest amount in Europe), was becoming particularly influential, and with the eventual development of the Golden Liberty, the "Sejm"'s powers increased dramatically.

 Over time, the envoys in the lower chamber grew in number and power as they pressed the king for more privileges. The "Sejm" eventually became even more active in supporting the goals of the privileged classes when the King ordered that the landed nobility and their estates (peasants) be drafted into military service. 

The Union of Lublin in 1569, united the Kingdom of Poland and the Grand Duchy of Lithuania as one single state, the Polish–Lithuanian Commonwealth, and thus the "Sejm" was supplemented with new envoys from among the Lithuanian nobility. The Commonwealth ensured that the state of affairs surrounding the three-estates system continued, with the "Sejm", Senate and King forming the estates and supreme deliberating body of the state. In the first few decades of the 16th century, the Senate had established its precedence over the "Sejm"; however, from the mid-1500s onwards, the "Sejm" became a very powerful representative body of the "szlachta" ("middle nobility"). Its chambers reserved the final decisions in legislation, taxation, budget, and treasury matters (including military funding), foreign policy, and the confirment of nobility.

The 1573 Warsaw Confederation saw the nobles of the "Sejm" officially sanction and guarantee religious tolerance in Commonwealth territory, ensuring a refuge for those fleeing the ongoing Reformation and Counter-Reformation wars in Europe.

Until the end of the 16th century, unanimity was not required, and the majority-voting process was the most commonly used system for voting. Later, with the rise of the Polish magnates and their increasing power, the unanimity principle was re-introduced with the institution of the nobility's right of "liberum veto" (Latin: "I freely forbid"). Additionally, if the envoys were unable to reach a unanimous decision within six weeks (the time limit of a single session), deliberations were declared void and all previous acts passed by that "Sejm" were annulled. From the mid-17th century onward, any objection to a "Sejm" resolution, by either an envoy or a senator, automatically caused the rejection of other, previously approved resolutions. This was because all resolutions passed by a given session of the "Sejm" formed a whole resolution, and, as such, was published as the annual "constituent act" of the "Sejm", e.g. the ""Anno Domini" 1667" act. In the 16th century, no single person or small group dared to hold up proceedings, but, from the second half of the 17th century, the "liberum veto" was used to virtually paralyze the "Sejm", and brought the Commonwealth to the brink of collapse.

The "liberum veto" was abolished with the adoption of Poland's 3rd May Constitution in 1791, a piece of legislation which was passed as the "Government Act", and for which the "Sejm" required four years to propagate and adopt. The constitution's acceptance, and the possible long-term consequences it may have had, is arguably the reason for which the powers of Habsburg Austria, Russia and Prussia then decided to partition the Polish–Lithuanian Commonwealth, thus putting an end to over 300 years of Polish parliamentary continuity. It is estimated that between 1493 and 1793, a "Sejm" was held 240 times, the total debate-time sum of which was 44 years.

After the fall of the Duchy of Warsaw, which existed as a Napoleonic client state between 1807 and 1815, and its short-lived "Sejm" of the Duchy of Warsaw, the "Sejm" of Congress Poland was established in Congress Poland of the Russian Empire; it was composed of the king (the Russian emperor), the upper house (Senate), and the lower house (Chamber of Deputies). Overall, during the period from 1795 until the re-establishment of Poland's sovereignty in 1918, little power was actually held by any Polish legislative body and the occupying powers of Russia, Prussia (later united Germany) and Austria propagated legislation for their own respective formerly-Polish territories at a national level.

The Chamber of Deputies, despite its name, consisted not only of 77 envoys (sent by local assemblies) from the hereditary nobility, but also of 51 deputies, elected by the non-noble population. All deputies were covered by Parliamentary immunity, with each individual serving for a term of office of six years, with third of the deputies being elected every two years. Candidates for deputy had to be able to read and write, and have a certain amount of wealth. The legal voting age was 21, except for those citizens serving in the military, the personnel of which were not allowed to vote. Parliamentary sessions were initially convened every two years, and lasted for (at least) 30 days. However, after many clashes between liberal deputies and conservative government officials, sessions were later called only four times (1818, 1820, 1826, and 1830, with the last two sessions being secret). The "Sejm" had the right to call for votes on civil and administrative legal issues, and, with permission from the king, it could also vote on matters related to the fiscal policy and the military. It had the right to exercise control over government officials, and to file petitions. The 64-member Senate on the other hand, was composed of "voivodes" and "kasztelans" (both types of provincial governors), Russian envoys, diplomats or princes, and nine bishops. It acted as the Parliamentary Court, had the right to control "citizens' books", and had similar legislative rights as did the Chamber of Deputies.

In the Free City of Cracow (1815–1846), a unicameral Assembly of Representatives was established, and from 1827, a unicameral provincial "sejm" existed in the Grand Duchy of Poznań. Poles were elected to and represented the majority in both of these legislatures; however, they were largely powerless institutions and exercised only very limited power. After numerous failures in securing legislative sovereignty in the early 19th century, many Poles simply gave up trying to attain a degree of independence from their foreign master-states. In the Austrian partition, a relatively powerless "Sejm" of the Estates operated until the time of the Spring of Nations. After this, in the mid to late 19th century, only in autonomous Galicia (1861–1914) was there a unicameral and functional National "Sejm", the "Sejm" of the Land. It is recognised today as having played a major and overwhelming positive role in the development of Polish national institutions.

In the second half of the 19th century, Poles were able to become members of the parliaments of Austria, Prussia and Russia, where they formed Polish Clubs. Deputies of Polish nationality were elected to the Prussian "Landtag" from 1848, and then to the German Empire's "Reichstag" from 1871. Polish Deputies were members of the Austrian State Council (from 1867), and from 1906 were also elected to the Russian Imperial State "Duma" (lower chamber) and to the State Council (upper chamber).

After the First World War and re-establishment of Polish independence, the convocation of parliament, under the democratic electoral law of 1918, became an enduring symbol of the new state's wish to demonstrate and establish continuity with the 300-year Polish parliamentary traditions established before the time of the partitions. Maciej Rataj emphatically paid tribute to this with the phrase: "There is Poland there, and so is the "Sejm"".

During the interwar period of Poland's independence, the first Legislative "Sejm" of 1919, a Constituent Assembly, passed the Small Constitution of 1919, which introduced a parliamentary republic and proclaimed the principle of the "Sejm"'s sovereignty. This was then strengthened, in 1921, by the March Constitution, one of the most democratic European constitutions enacted after the end of World War I. The constitution established a political system which was based on Montesquieu's doctrine of separation of powers, and which restored the bicameral "Sejm" consisting of a chamber of deputies (to which alone the name of ""Sejm"" was from then on applied) and the Senate. In 1919, Roza Pomerantz-Meltzer, a member of the Zionist party, became the first woman ever elected to the "Sejm".

The legal content of the March Constitution allowed for "Sejm" supremacy in the system of state institutions at the expense of the executive powers, thus creating a parliamentary republic out of the Polish state. An attempt to strengthen executive powers in 1926 (through the August Amendment) proved too limited and largely failed in helping avoid legislative grid-lock which had ensued as a result of too-great parliamentary power in a state which had numerous diametrically-opposed political parties sitting in its legislature. In 1935, the parliamentary republic was weakened further when, by way of, Józef Piłsudski's May Coup, the president was forced to sign the April Constitution of 1935, an act through which the head of state assumed the dominant position in legislating for the state and the Senate increased its power at the expense of the "Sejm".

On 2 September 1939, the "Sejm" held its final pre-war session, during which it declared Poland's readiness to defend itself against invading German forces. On 2 November 1939, the President dissolved the "Sejm" and the Senate, which were then, according to plan, to resume their activity within two months after the end of the Second World War; this, however, never happened. During wartime, the National Council (1939–1945) was established to represent the legislature as part of the Polish Government in Exile. Meanwhile, in Nazi-occupied Poland, the Council of National Unity was set up; this body functioned from 1944 to 1945 as the parliament of the Polish Underground State. With the cessation of hostilities in 1945, and subsequent rise to power of the Communist-backed Provisional Government of National Unity, the Second Polish Republic legally ceased to exist.

The "Sejm" in the Polish People's Republic had 460 deputies throughout most of its history. At first, this number was declared to represent one deputy per 60,000 citizens (425 were elected in 1952), but, in 1960, as the population grew, the declaration was changed: The constitution then stated that the deputies were representative "of" the people and could be recalled "by" the people, but this article was never used, and, instead of the "five-point electoral law", a non-proportional, "four-point" version was used. Legislation was passed with majority voting.

Under the 1952 Constitution, the Sejm was defined as "the highest organ of State authority" in Poland, as well as "the highest spokesman of the will of the people in town and country." On paper, it was vested with great lawmaking and oversight powers. For instance, it was empowered with control over "the functioning of other organs of State authority and administration," and ministers were required to answer questions posed by deputies within seven days. In practice, it did little more than rubber-stamp decisions already made by the Communist Polish United Workers Party and its executive bodies. This was standard practice in nearly all Communist regimes due to the principle of democratic centralism.

The "Sejm" voted on the budget and on the periodic national plans that were a fixture of communist economies. The "Sejm" deliberated in sessions that were ordered to convene by the State Council.

The "Sejm" also chose a "Prezydium" ("presiding body") from among its members. The "Prezydium" was headed by the speaker, or Marshal, who was always a member of the United People's Party. In its preliminary session, the "Sejm" also nominated the Prime Minister, the Council of Ministers of Poland, and members of the State Council. It also chose many other government officials, including the head of the Supreme Chamber of Control and members of the State Tribunal and the Constitutional Tribunal, as well as the Ombudsman (the last three bodies of which were created in the 1980s).

When the Sejm was not in session, the State Council had the power to issue decrees that had the force of law. However, those decrees had to be approved by the Sejm at its next session. In practice, the principles of democratic centralism meant that such approval was only a formality.

The Senate of Poland was abolished by the Polish people's referendum in 1946, after which the "Sejm" became the sole legislative body in Poland. Even though the "Sejm" was largely subservient to the Communist party, one deputy, Romuald Bukowski (an independent) voted against the imposition of martial law in 1982.

After the end of communism in 1989, the Senate was reinstated as the second house of a bicameral national assembly, while the "Sejm" remained the first house. The "Sejm" is now composed of 460 deputies elected by proportional representation every four years.

Between 7 and 19 deputies are elected from each constituency using the d'Hondt method (with one exception, in 2001, when the Sainte-Laguë method was used), their number being proportional to their constituency's population. Additionally, a threshold is used, so that candidates are chosen only from parties that gained at least 5% of the nationwide vote (candidates from ethnic-minority parties are exempt from this threshold).






</doc>
<doc id="28168" url="https://en.wikipedia.org/wiki?curid=28168" title="Stock exchange">
Stock exchange

A stock exchange, securities exchange, or bourse is a facility where stockbrokers and traders can buy and sell securities, such as shares of stock and bonds and other financial instruments. Stock exchanges may also provide facilities for the issue and redemption of such securities and instruments and capital events including the payment of income and dividends. Securities traded on a stock exchange include stock issued by listed companies, unit trusts, derivatives, pooled investment products and bonds. Stock exchanges often function as "continuous auction" markets with buyers and sellers consummating transactions via open outcry at a central location such as the floor of the exchange or by using an electronic trading platform.

To be able to trade a security on a certain stock exchange, the security must be listed there. Usually, there is a central location at least for record keeping, but trade is increasingly less linked to a physical place, as modern markets use electronic communication networks, which give them advantages of increased speed and reduced cost of transactions. Trade on an exchange is restricted to brokers who are members of the exchange. In recent years, various other trading venues, such as electronic communication networks, alternative trading systems and "dark pools" have taken much of the trading activity away from traditional stock exchanges.

Initial public offerings of stocks and bonds to investors is done in the primary market and subsequent trading is done in the secondary market. A stock exchange is often the most important component of a stock market. Supply and demand in stock markets are driven by various factors that, as in all free markets, affect the price of stocks (see stock valuation).

There is usually no obligation for stock to be issued through the stock exchange itself, nor must stock be subsequently traded on an exchange. Such trading may be "off exchange" or over-the-counter. This is the usual way that derivatives and bonds are traded. Increasingly, stock exchanges are part of a global securities market. Stock exchanges also serve an economic function in providing liquidity to shareholders in providing an efficient means of disposing of shares.

There is little consensus among scholars as to when corporate stock was first traded. Some see the key event as the Dutch East India Company's founding in 1602, while others point to earlier developments (Bruges, Antwerp in 1531 and in Lyon in 1548). The first book in history of securities exchange, the Confusion of Confusions, was written by the Dutch-Jewish trader Joseph de la Vega and the Amsterdam Stock Exchange is often considered the oldest “modern” securities market in the world.. On the other hand, economist Ulrike Malmendier of the University of California at Berkeley argues that a share market existed as far back as ancient Rome, that derives from Etruscan "Argentari". In the Roman Republic, which existed for centuries before the Empire was founded, there were "societates publicanorum", organizations of contractors or leaseholders who performed temple-building and other services for the government. One such service was the feeding of geese on the Capitoline Hill as a reward to the birds after their honking warned of a Gallic invasion in 390 B.C. Participants in such organizations had "partes" or shares, a concept mentioned various times by the statesman and orator Cicero. In one speech, Cicero mentions "shares that had a very high price at the time". Such evidence, in Malmendier's view, suggests the instruments were tradable, with fluctuating values based on an organization's success. The "societas" declined into obscurity in the time of the emperors, as most of their services were taken over by direct agents of the state.

Tradable bonds as a commonly used type of security were a more recent innovation, spearheaded by the Italian city-states of the late medieval and early Renaissance periods.

While the Italian city-states produced the first transferable government bonds, they did not develop the other ingredient necessary to produce a fully-fledged capital market: the stock market in its modern sense. In the early 1600s the Dutch East India Company (VOC) became the first company in history to issue bonds and shares of stock to the general public. As Edward Stringham (2015) notes, "companies with transferable shares date back to classical Rome, but these were usually not enduring endeavors and no considerable secondary market existed (Neal, 1997, p. 61)." The VOC, formed to build up the spice trade, operated as a colonial ruler in what is now Indonesia and beyond, a purview that included conducting military operations against the wishes of the exploited natives and of competing colonial powers. Control of the company was held tightly by its directors, with ordinary shareholders not having much influence on management or even access to the company's accounting statements.

However, shareholders were rewarded well for their investment. The company paid an average dividend of over 16% per year from 1602 to 1650. Financial innovation in Amsterdam took many forms. In 1609, investors led by Isaac Le Maire formed history's first bear market syndicate, but their coordinated trading had only a modest impact in driving down share prices, which tended to remain robust throughout the 17th century. By the 1620s, the company was expanding its securities issuance with the first use of corporate bonds.

Joseph de la Vega, also known as Joseph Penso de la Vega and by other variations of his name, was an Amsterdam trader from a Spanish Jewish family and a prolific writer as well as a successful businessman in 17th-century Amsterdam. His 1688 book "Confusion of Confusions" explained the workings of the city's stock market. It was the earliest book about stock trading and inner workings of a stock market, taking the form of a dialogue between a merchant, a shareholder and a philosopher, the book described a market that was sophisticated but also prone to excesses, and de la Vega offered advice to his readers on such topics as the unpredictability of market shifts and the importance of patience in investment.
In England, King William III sought to modernize the kingdom's finances to pay for its wars, and thus the first government bonds were issued in 1693 and the Bank of England was set up the following year. Soon thereafter, English joint-stock companies began going public. 

London's first stockbrokers, however, were barred from the old commercial center known as the Royal Exchange, reportedly because of their rude manners. Instead, the new trade was conducted from coffee houses along Exchange Alley. By 1698, a broker named John Castaing, operating out of Jonathan's Coffee House, was posting regular lists of stock and commodity prices. Those lists mark the beginning of the London Stock Exchange.

One of history's greatest financial bubbles occurred around 1720. At the center of it were the South Sea Company, set up in 1711 to conduct English trade with South America, and the Mississippi Company, focused on commerce with France's Louisiana colony and touted by transplanted Scottish financier John Law, who was acting in effect as France's central banker. Investors snapped up shares in both, and whatever else was available. In 1720, at the height of the mania, there was even an offering of "a company for carrying out an undertaking of great advantage, but nobody to know what it is".

By the end of that same year, share prices had started collapsing, as it became clear that expectations of imminent wealth from the Americas were overblown. In London, Parliament passed the Bubble Act, which stated that only royally chartered companies could issue public shares. In Paris, Law was stripped of office and fled the country. Stock trading was more limited and subdued in subsequent decades. Yet the market survived, and by the 1790s shares were being traded in the young United States. On May 17, 1792, the New York Stock Exchange opened under a platanus occidentalis (buttonwood tree) in New York City, as 24 stockbrokers signed the Buttonwood Agreement, agreeing to trade five securities under that buttonwood tree.

Stock exchanges have multiple roles in the economy. This may include the following:

Besides the borrowing capacity provided to an individual or firm by the banking system, in the form of credit or a loan, a stock exchange provides companies with the facility to raise capital for expansion through selling shares to the investing public.

Capital intensive companies, particularly high tech companies, always need to raise high volumes of capital in their early stages. For this reason, the public market provided by the stock exchanges has been one of the most important funding sources for many capital intensive startups. In the 1990s and early 2000s, hi-tech listed companies experienced a boom and bust in the world's major stock exchanges. Since then, it has been much more demanding for the high-tech entrepreneur to take his/her company public, unless either the company is already generating sales and earnings, or the company has demonstrated credibility and potential from successful outcomes: clinical trials, market research, patent registrations, etc. This is quite different from the situation of the 1990s to early-2000s period, when a number of companies (particularly Internet boom and biotechnology companies) went public in the most prominent stock exchanges around the world in the total absence of sales, earnings, or any type of well-documented promising outcome. Though it's not as common, it still happens that highly speculative and financially unpredictable hi-tech startups are listed for the first time in a major stock exchange. Additionally, there are smaller, specialized entry markets for these kind of companies with stock indexes tracking their performance (examples include the Alternext, CAC Small, SDAX, TecDAX).

Companies have also raised significant amounts of capital through R&D limited partnerships. Tax law changes that were enacted in 1987 in the United States changed the tax deductibility of investments in R&D limited partnerships. In order for a partnership to be of interest to investors today, the cash on cash return must be high enough to entice investors.

A general source of capital for startup companies has been venture capital. This source remains largely available today, but the maximum statistical amount that the venture company firms in aggregate will invest in any one company is not limitless (it was approximately $15 million in 2001 for a biotechnology company).

Another alternative source of cash for a private company is a corporate partner, usually an established multinational company, which provides capital for the smaller company in return for marketing rights, patent rights, or equity. Corporate partnerships have been used successfully in a large number of cases.

When people draw their savings and invest in shares (through an initial public offering or the seasoned equity offering of an already listed company), it usually leads to rational allocation of resources because funds, which could have been consumed, or kept in idle deposits with banks, are mobilized and redirected to help companies' management boards finance their organizations. This may promote business activity with benefits for several economic sectors such as agriculture, commerce and industry, resulting in stronger economic growth and higher productivity levels of firms.

Companies view acquisitions as an opportunity to expand product lines, increase distribution channels, hedge against volatility, increase their market share, or acquire other necessary business assets. A takeover bid or mergers and acquisitions through the stock market is one of the simplest and most common ways for a company to grow by acquisition or fusion.

Both casual and professional stock investors, as large as institutional investors or as small as an ordinary middle-class family, through dividends and stock price increases that may result in capital gains, share in the wealth of profitable businesses. Unprofitable and troubled businesses may result in capital losses for shareholders.

By having a wide and varied scope of owners, companies generally tend to improve management standards and efficiency to satisfy the demands of these shareholders and the more stringent rules for public corporations imposed by public stock exchanges and the government. This improvement can be attributed in some cases to the price mechanism exerted through shares of stock, wherein the price of the stock falls when management is considered poor (making the firm vulnerable to a takeover by new management) or rises when management is doing well (making the firm less vulnerable to a takeover). In addition, publicly listed shares are subject to greater transparency so that investors can make informed decisions about a purchase. Consequently, it is alleged that public companies (companies that are owned by shareholders who are members of the general public and trade shares on public exchanges) tend to have better management records than privately held companies (those companies where shares are not publicly traded, often owned by the company founders, their families and heirs, or otherwise by a small group of investors).

Despite this claim, some well-documented cases are known where it is alleged that there has been considerable slippage in corporate governance on the part of some public companies, particularly in the cases of accounting scandals. The policies that led to the dot-com bubble in the late 1990s and the subprime mortgage crisis in 2007–08 are also examples of corporate mismanagement. The mismanagement of companies such as Pets.com (2000), Enron (2001), One.Tel (2001), Sunbeam Products (2001), Webvan (2001), Adelphia Communications Corporation (2002), MCI WorldCom (2002), Parmalat (2003), American International Group (2008), Bear Stearns (2008), Lehman Brothers (2008), General Motors (2009) and Satyam Computer Services (2009) all received plenty of media attention.

Many banks and companies worldwide utilize securities identification numbers (ISIN) to identify, uniquely, their stocks, bonds and other securities. Adding an ISIN code helps to distinctly identify securities and the ISIN system is used worldwide by funds, companies, and governments.

However, when poor financial, ethical or managerial records become public, stock investors tend to lose money as the stock and the company tend to lose value. In the stock exchanges, shareholders of underperforming firms are often penalized by significant share price decline, and they tend as well to dismiss incompetent management teams.

As opposed to other businesses that require huge capital outlay, investing in shares is open to both the large and small stock investors as minimum investment amounts are minimal. Therefore, the stock exchange provides the opportunity for small investors to own shares of the same companies as large investors.

Governments at various levels may decide to borrow money to finance infrastructure projects such as sewage and water treatment works or housing estates by selling another category of securities known as bonds. These bonds can be raised through the stock exchange whereby members of the public buy them, thus loaning money to the government. The issuance of such bonds can obviate, in the short term, direct taxation of citizens to finance development—though by securing such bonds with the full faith and credit of the government instead of with collateral, the government must eventually tax citizens or otherwise raise additional funds to make any regular coupon payments and refund the principal when the bonds mature.

At the stock exchange, share prices rise and fall depending, largely, on economic forces. Share prices tend to rise or remain stable when companies and the economy in general show signs of stability and growth. A recession, depression, or financial crisis could eventually lead to a stock market crash. Therefore, the movement of share prices and in general of the stock indexes can be an indicator of the general trend in the economy.

Each stock exchange imposes its own listing requirements upon companies that want to be listed on that exchange. Such conditions may include minimum number of shares outstanding, minimum market capitalization, and minimum annual income.

The listing requirements imposed by some stock exchanges include:

Stock exchanges originated as mutual organizations, owned by its member stockbrokers. However, the major stock exchanges have "demutualized", where the members sell their shares in an initial public offering. In this way the mutual organization becomes a corporation, with shares that are listed on a stock exchange. Examples are Australian Securities Exchange (1998), Euronext (merged with New York Stock Exchange), NASDAQ (2002), Bursa Malaysia (2004), the New York Stock Exchange (2005), Bolsas y Mercados Españoles, and the São Paulo Stock Exchange (2007).

The Shenzhen Stock Exchange and Shanghai Stock Exchange can be characterized as quasi-state institutions insofar as they were created by government bodies in China and their leading personnel are directly appointed by the China Securities Regulatory Commission.

Another example is Tashkent Stock Exchange established in 1994, three years after the collapse of the Soviet Union, mainly state-owned but has a form of a public corporation (joint-stock company). Korea Exchange (KRX) owns 25% less one share of the Tashkent Stock Exchange.

In 2018, there were 15 licensed stock exchanges in the United States, of which 13 actively traded securities. All of these exchanges were owned by three publicly traded multinational companies, Intercontinental Exchange, Nasdaq, Inc., and Cboe Global Markets, except one, IEX. In 2019, a group of financial corporations announced plans to open a members owned exchange, MEMX, an ownership structure similar to the mutual organizations of earlier exchanges.

In the 19th century, exchanges were opened to trade forward contracts on commodities. Exchange traded forward contracts are called futures contracts. These "commodity markets" later started offering future contracts on other products, such as interest rates and shares, as well as options contracts. They are now generally known as futures exchanges.


Lists:



</doc>
<doc id="28170" url="https://en.wikipedia.org/wiki?curid=28170" title="Son of God">
Son of God

Historically, many rulers have assumed titles such as son of God, son of a god or son of heaven.

The term "son of God" is used in the Hebrew Bible as another way of referring to humans with special relationships with God. In Exodus, the nation of Israel is called God's "Firstborn son". In Psalms, David is called "son of God", even commanded to proclaim that he is God's "begotten son" on the day he was made king. Solomon is also called "son of God". Angels, just and pious men, and the kings of Israel are all called "sons of God."

In the New Testament of the Christian Bible, "Son of God" is applied to Jesus on many occasions. Jesus is declared to be the Son of God on two separate occasions by a voice speaking from Heaven. Jesus is explicitly and implicitly described as the Son of God by himself and by various individuals who appear in the New Testament. Jesus is called "son of God," while followers of Jesus are called, "sons of God". As applied to Jesus, the term is a reference to his role as the Messiah, the King chosen by God. The contexts and ways in which Jesus' title, Son of God, means something more than or other than Messiah remain the subject of ongoing scholarly study and discussion.

The term "Son of God" should not be confused with the term "God the Son" (), the second Person of the Trinity in Christian theology. The doctrine of the Trinity identifies Jesus as God the Son, identical in essence but distinct in person with regard to God the Father and God the Holy Spirit (the first and third Persons of the Trinity). Nontrinitarian Christians accept the application to Jesus of the term "Son of God", which is found in the New Testament.

Throughout history, emperors and rulers ranging from the Western Zhou dynasty (c. 1000 BC) in China to Alexander the Great (c. 360 BC) to the Emperor of Japan (c. 600 AD) have assumed titles that reflect a filial relationship with deities.

The title "Son of Heaven" i.e. 天子 (from 天 meaning sky/heaven/god and 子 meaning child) was first used in the Western Zhou dynasty (c. 1000 BC). It is mentioned in the Shijing book of songs, and reflected the Zhou belief that as Son of Heaven (and as its delegate) the Emperor of China was responsible for the well being of the whole world by the Mandate of Heaven. This title may also be translated as "son of God" given that the word "Ten" or "Tien" in Chinese may either mean sky or god. The Emperor of Japan was also called the Son of Heaven (天子 "tenshi") starting in the early 7th century.

Among the Eurasian nomads, there was also a widespread use of "Son of God/Son of Heaven" for instance, in the third century BC, the ruler was called Chanyü and similar titles were used as late as the 13th century by Genghis Khan.

Examples of kings being considered the son of god are found throughout the Ancient Near East. Egypt in particular developed a long lasting tradition. Egyptian pharaohs are known to have been referred to as the son of a particular god and their begetting in some cases is even given in sexually explicit detail. Egyptian pharaohs did not have full parity with their divine fathers but rather were subordinate. Nevertheless, in the first four dynasties, the pharaoh was considered to be the embodiment of a god. Thus, Egypt was ruled by direct theocracy, wherein "God himself is recognized as the head" of the state. During the later Amarna Period, Akhenaten reduced the Pharaoh's role to one of coregent, where the Pharaoh and God ruled as father and son. Akhenaten also took on the role of the priest of god, eliminating representation on his behalf by others. Later still, the closest Egypt came to the Jewish variant of theocracy was during the reign of Herihor. He took on the role of ruler not as a god but rather as a high-priest and king.
Jewish kings are also known to have been referred to as "son of the ". The Jewish variant of theocracy can be thought of as a representative theocracy where the king is viewed as God's surrogate on earth. Jewish kings thus, had less of a direct connection to god than pharaohs. Unlike pharaohs, Jewish kings rarely acted as priests, nor were prayers addressed directly to them. Rather, prayers concerning the king are addressed directly to god. The Jewish philosopher Philo is known to have likened God to a supreme king, rather than likening Jewish kings to gods.

Based on the Bible, several kings of Damascus took the title son of Hadad. From the archaeological record a stela erected by Bar-Rakib for his father Panammuwa II contains similar language. The son of Panammuwa II a king of Sam'al referred to himself as a son of Rakib. Rakib-El is a god who appears in Phoenician and Aramaic inscriptions. Panammuwa II died unexpectedly while in Damascus. However, his son the king Bar-Rakib was not a native of Damascus but rather the ruler of Sam'al it is unknown if other rules of Sam'al used similar language.

In Greek mythology, Heracles (son of Zeus) and many other figures were considered to be sons of gods through union with mortal women. From around 360 BC onwards Alexander the Great may have implied he was a demigod by using the title "Son of Ammon–Zeus".
In 42 BC, Julius Caesar was formally deified as "the divine Julius" ("divus Iulius") after his assassination. His adopted son, Octavian (better known as Augustus, a title given to him 15 years later, in 27 BC) thus became known as "divi Iuli filius" (son of the divine Julius) or simply "divi filius" (son of the god). As a daring and unprecedented move, Augustus used this title to advance his political position in the Second Triumvirate, finally overcoming all rivals for power within the Roman state.

The word applied to Julius Caesar as deified was "divus", not the distinct word "deus". Thus Augustus called himself "Divi filius", and not "Dei filius". The line between been god and god-like was at times less than clear to the population at large, and Augustus seems to have been aware of the necessity of keeping the ambiguity. As a purely semantic mechanism, and to maintain ambiguity, the court of Augustus sustained the concept that any worship given to an emperor was paid to the "position of emperor" rather than the person of the emperor. However, the subtle semantic distinction was lost outside Rome, where Augustus began to be worshiped as a deity. The inscription DF thus came to be used for Augustus, at times unclear which meaning was intended. The assumption of the title "Divi filius" by Augustus meshed with a larger campaign by him to exercise the power of his image. Official portraits of Augustus made even towards the end of his life continued to portray him as a handsome youth, implying that miraculously, he never aged. Given that few people had ever seen the emperor, these images sent a distinct message.

Later, Tiberius (emperor from 14–37 AD) came to be accepted as the son of "divus Augustus" and Hadrian as the son of "divus Trajan". By the end of the 1st century, the emperor Domitian was being called "dominus et deus" (i.e. "master and god").

Outside the Roman Empire, the 2nd-century Kushan King Kanishka I used the title "devaputra" meaning "son of God".

In the writings of the Bahá'í Faith, the term "Son of God" is applied to Jesus, but does not indicate a literal physical relationship between Jesus and God, but is symbolic and is used to indicate the very strong spiritual relationship between Jesus and God and the source of his authority. Shoghi Effendi, the head of the Bahá'í Faith in the first half of the 20th century, also noted that the term does not indicate that the station of Jesus is superior to other prophets and messengers that Bahá'ís name Manifestations of God, including Buddha, Muhammad and Baha'u'llah among others. Shoghi Effendi notes that, since all Manifestations of God share the same intimate relationship with God and reflect the same light, the term Sonship can in a sense be attributable to all the Manifestations.

In Christianity, the title "Son of God" refers to the status of Jesus as the divine son of God the Father. It derives from several uses in the New Testament and early Christian theology.

In Islam, Jesus is known as "Īsā ibn Maryam" (), and is understood to be a prophet and messenger of God (Allah) and "al-Masih", the Arabic term for Messiah (Christ), sent to guide the Children of Israel ("banī isrā'īl" in Arabic) with a new revelation, the "al-Injīl" (Arabic for "the gospel").

Islam rejects any kinship between God and any other being, including a son. Thus, rejecting the belief that Jesus is the begotten son of God (Allah), God (Allah) himself or another god. As in Christianity, Islam believes Jesus had no earthly father. In Islam Jesus is believed to be born due to the command of God (Allah) "be". God (Allah) ordered the angel Jibrīl (Gabriel) to "blow" the soul of Jesus into Mary and so she gave birth to Jesus. Islamic scholars debate whether or not, the title "Son of God" might apply to Jesus in an adoptive rather than generative sense, just like Abraham was taken as a friend of God.

Although references to "sons of God", "son of God" and "son of the " are occasionally found in Jewish literature, they never refer to physical descent from God. There are two instances where Jewish kings are figuratively referred to as a god. The king is likened to the supreme king God. These terms are often used in the general sense in which the Jewish people were referred to as "children of the your God".

When used by the rabbis, the term referred to Israel or to human beings in general, and not as a reference to the Jewish mashiach. In Judaism the term "mashiach" has a broader meaning and usage and can refer to a wide range of people and objects, not necessarily related to the Jewish eschaton.

Gabriel's Revelation, also called the Vision of Gabriel or the Jeselsohn Stone, is a three-foot-tall (one metre) stone tablet with 87 lines of Hebrew text written in ink, containing a collection of short prophecies written in the first person and dated to the late 1st century BC. It is a tablet described as a "Dead Sea scroll in stone".

The text seems to talk about a messianic figure from Ephraim who broke evil before righteousness by three days. Later the text talks about a “prince of princes" a leader of Israel who was killed by the evil king and not properly buried. The evil king was then miraculously defeated. The text seems to refer to Jeremiah Chapter 31. The choice of Ephraim as the lineage of the messianic figure described in the text seems to draw on passages in Jeremiah, Zechariah and Hosea. This leader was referred to as a son of God.

The text seems to be based on a Jewish revolt recorded by Josephus dating from 4 BC. Based on its dating the text seems to refer to Simon of Peraea, one of the three leaders of this revolt.

In some versions of Deuteronomy the Dead Sea Scrolls refer to the sons of God rather than the sons of Israel, probably in reference to angels. The Septuagint reads similarly.

4Q174 is a midrashic text in which God refers to the Davidic messiah as his son.

4Q246 refers to a figure who will be called the son of God and son of the Most High. It is debated if this figure represents the royal messiah, a future evil gentile king or something else.

In 11Q13 Melchizedek is referred to as god the divine judge. Melchizedek in the bible was the king of Salem. At least some in the Qumran community seemed to think that at the end of days Melchizedek would reign as their king. The passage is based on Psalm 82.

In both Joseph and Aseneth and the related text The Story of Asenath, Joseph is referred to as the son of God. In the Prayer of Joseph both Jacob and the angel are referred to as angels and the sons of God.

This style of naming is also used for some rabbis in the Talmud.




</doc>
<doc id="28171" url="https://en.wikipedia.org/wiki?curid=28171" title="SA">
SA

Sa, SA, S.A. or s.a. may refer to:













</doc>
<doc id="28172" url="https://en.wikipedia.org/wiki?curid=28172" title="Saint Boniface">
Saint Boniface

Boniface (; 675 – 5 June 754 AD), born Winfrid (also spelled Winifred, Wynfrith, Winfrith or Wynfryth) in the Devon town of Crediton in Anglo-Saxon England, was a leading figure in the Anglo-Saxon mission to the Germanic parts of the Frankish Empire during the 8th century. He organised significant foundations of the church in Germany and was made archbishop of Mainz by Pope Gregory III. He was martyred in Frisia in 754, along with 52 others, and his remains were returned to Fulda, where they rest in a sarcophagus which became a site of pilgrimage. Boniface's life and death as well as his work became widely known, there being a wealth of material available—a number of "vitae", especially the near-contemporary "Vita Bonifatii auctore Willibaldi", legal documents, possibly some sermons, and above all his correspondence. He is venerated as a saint in the Christian church and became the patron saint of Germania, known as the "Apostle of the Germans".

Norman F. Cantor notes the three roles Boniface played that made him "one of the truly outstanding creators of the first Europe, as the apostle of Germania, the reformer of the Frankish church, and the chief fomentor of the alliance between the papacy and the Carolingian family." Through his efforts to reorganize and regulate the church of the Franks, he helped shape the Latin Church in Europe, and many of the dioceses he proposed remain today. After his martyrdom, he was quickly hailed as a saint in Fulda and other areas in Germania and in England. He is still venerated strongly today by German Catholics. Boniface is celebrated as a missionary; he is regarded as a unifier of Europe, and he is regarded by German Catholics as a national figure. In 2019 Devon County Council with the support of the Anglican and Catholic churches in Exeter and Plymouth, officially recognised St Boniface as the Patron Saint of Devon.

The earliest Bonifacian "vita", Willibald's, does not mention his place of birth but says that at an early age he attended a monastery ruled by Abbot Wulfhard in "escancastre", or "Examchester", which seems to denote Exeter, and may have been one of many "monasteriola" built by local landowners and churchmen; nothing else is known of it outside the Bonifacian "vitae". This monastery is believed to have occupied the site of the Church of St Mary Major in the City of Exeter, demolished in 1971, next to which was later built Exeter Cathedral. Later tradition places his birth at Crediton, but the earliest mention of Crediton in connection to Boniface is from the early fourteenth century, in John Grandisson's "Legenda Sanctorum: The Proper Lessons for Saints' Days according to the use of Exeter". In one of his letters Boniface mentions he was "born and reared...[in] the synod of London", but he may have been speaking metaphorically.

According to the "vitae", Winfrid was of a respected and prosperous family. Against his father's wishes he devoted himself at an early age to the monastic life. He received further theological training in the Benedictine monastery and minster of Nhutscelle (Nursling), not far from Winchester, which under the direction of abbot Winbert had grown into an industrious centre of learning in the tradition of Aldhelm. Winfrid taught in the abbey school and at the age of 30 became a priest; in this time, he wrote a Latin grammar, the "Ars Grammatica", besides a treatise on verse and some Aldhelm-inspired riddles. While little is known about Nursling outside of Boniface's "vitae", it seems clear that the library there was significant. In order to supply Boniface with the materials he needed, it would have contained works by Donatus, Priscian, Isidore, and many others. Around 716, when his abbot Wynberth of Nursling died, he was invited (or expected) to assume his position—it is possible that they were related, and the practice of hereditary right among the early Anglo-Saxons would affirm this. Winfrid, however, declined the position and in 716 set out on a missionary expedition to Frisia.

Boniface first left for the continent in 716. He traveled to Utrecht, where Willibrord, the "Apostle of the Frisians," had been working since the 690s. He spent a year with Willibrord, preaching in the countryside, but their efforts were frustrated by the war then being carried on between Charles Martel and Radbod, King of the Frisians. Willibrord fled to the abbey he had founded in Echternach (in modern-day Luxembourg) while Boniface returned to Nursling.

Boniface returned to the continent the next year and went straight to Rome, where Pope Gregory II renamed him "Boniface", after the (legendary) fourth-century martyr Boniface of Tarsus, and appointed him missionary bishop for Germania—he became a bishop without a diocese for an area that lacked any church organization. He would never return to England, though he remained in correspondence with his countrymen and kinfolk throughout his life.

According to the "vitae" Boniface felled the Donar Oak, Latinized by Willibald as "Jupiter's oak," near the present-day town of Fritzlar in northern Hesse. According to his early biographer Willibald, Boniface started to chop the oak down, when suddenly a great wind, as if by miracle, blew the ancient oak over. When the god did not strike him down, the people were amazed and converted to Christianity. He built a chapel dedicated to Saint Peter from its wood at the site—the chapel was the beginning of the monastery in Fritzlar. This account from the "vita" is stylized to portray Boniface as a singular character who alone acts to root out paganism. Lutz von Padberg and others point out that what the "vitae" leave out is that the action was most likely well-prepared and widely publicized in advance for maximum effect, and that Boniface had little reason to fear for his personal safety since the Frankish fortified settlement of Büraburg was nearby. According to Willibald, Boniface later had a church with an attached monastery built in Fritzlar, on the site of the previously built chapel, according to tradition.

The support of the Frankish mayors of the palace (maior domos), and later the early Pippinid and Carolingian rulers, was essential for Boniface's work. Boniface had been under the protection of Charles Martel from 723 on. The Christian Frankish leaders desired to defeat their rival power, the pagan Saxons, and to incorporate the Saxon lands into their own growing empire. Boniface's campaign of destruction of indigenous Germanic pagan sites may have benefited the Franks in their campaign against the Saxons.

In 732, Boniface traveled again to Rome to report, and Pope Gregory III conferred upon him the pallium as archbishop with jurisdiction over what is now Germany. Boniface again set out for the German lands and continued his mission, but also used his authority to work on the relations between the papacy and the Frankish church. Rome wanted more control over that church, which it felt was much too independent and which, in the eyes of Boniface, was subject to worldly corruption. Charles Martel, after having defeated the forces of the Umayyad Caliphate during the Battle of Tours (732), had rewarded many churches and monasteries with lands, but typically his supporters who held church offices were allowed to benefit from those possessions. Boniface would have to wait until the 740s before he could try to address this situation, in which Frankish church officials were essentially sinecures, and the church itself paid little heed to Rome. During his third visit to Rome in 737–38, he was made papal legate for Germany.

After Boniface's third trip to Rome, Charles Martel established four dioceses in Bavaria (Salzburg, Regensburg, Freising, and Passau) and gave them to Boniface as archbishop and metropolitan over all Germany east of the Rhine. In 745, he was granted Mainz as metropolitan see. In 742, one of his disciples, Sturm (also known as Sturmi, or Sturmius), founded the abbey of Fulda not far from Boniface's earlier missionary outpost at Fritzlar. Although Sturm was the founding abbot of Fulda, Boniface was very involved in the foundation. The initial grant for the abbey was signed by Carloman, the son of Charles Martel, and a supporter of Boniface's reform efforts in the Frankish church. Boniface himself explained to his old friend, Daniel of Winchester, that without the protection of Charles Martel he could "neither administer his church, defend his clergy, nor prevent idolatry".

According to German historian Gunther Wolf, the high point of Boniface's career was the Concilium Germanicum, organized by Carloman in an unknown location in April 743. Although Boniface was not able to safeguard the church from property seizures by the local nobility, he did achieve one goal, the adoption of stricter guidelines for the Frankish clergy, who often hailed directly from the nobility. After Carloman's resignation in 747 he maintained a sometimes turbulent relationship with the king of the Franks, Pepin; the claim that he would have crowned Pepin at Soissons in 751 is now generally discredited.

Boniface balanced this support and attempted to maintain some independence, however, by attaining the support of the papacy and of the Agilolfing rulers of Bavaria. In Frankish, Hessian, and Thuringian territory, he established the dioceses of Würzburg and Erfurt. By appointing his own followers as bishops, he was able to retain some independence from the Carolingians, who most likely were content to give him leeway as long as Christianity was imposed on the Saxons and other Germanic tribes.

According to the "vitae", Boniface had never relinquished his hope of converting the Frisians, and in 754 he set out with a retinue for Frisia. He baptized a great number and summoned a general meeting for confirmation at a place not far from Dokkum, between Franeker and Groningen. Instead of his converts, however, a group of armed robbers appeared who slew the aged archbishop. The "vitae" mention that Boniface persuaded his (armed) comrades to lay down their arms: "Cease fighting. Lay down your arms, for we are told in Scripture not to render evil for good but to overcome evil by good."

Having killed Boniface and his company, the Frisian bandits ransacked their possessions but found that the company's luggage did not contain the riches they had hoped for: "they broke open the chests containing the books and found, to their dismay, that they held manuscripts instead of gold vessels, pages of sacred texts instead of silver plates." They attempted to destroy these books, the earliest "vita" already says, and this account underlies the status of the Ragyndrudis Codex, now held as a Bonifacian relic in Fulda, and supposedly one of three books found on the field by the Christians who inspected it afterward. Of those three books, the Ragyndrudis Codex shows incisions that could have been made by sword or axe; its story appears confirmed in the Utrecht hagiography, the "Vita altera", which reports that an eye-witness saw that the saint at the moment of death held up a gospel as spiritual protection. The story was later repeated by Otloh's "vita"; at that time, the Ragyndrudis Codex seems to have been firmly connected to the martyrdom.

Boniface's remains were moved from the Frisian countryside to Utrecht, and then to Mainz, where sources contradict each other regarding the behavior of Lullus, Boniface's successor as archbishop of Mainz. According to Willibald's "vita" Lullus allowed the body to be moved to Fulda, while the (later) "Vita Sturmi", a hagiography of Sturm by Eigil of Fulda, Lullus attempted to block the move and keep the body in Mainz.

His remains were eventually buried in the abbey church of Fulda after resting for some time in Utrecht, and they are entombed within a shrine beneath the high altar of Fulda Cathedral, previously the abbey church.

Veneration of Boniface in Fulda began immediately after his death; his grave was equipped with a decorative tomb around ten years after his burial, and the grave and relics became the center of the abbey. Fulda monks prayed for newly elected abbots at the grave site before greeting them, and every Monday the saint was remembered in prayer, the monks prostrating themselves and reciting Psalm 50. After the abbey church was rebuilt to become the Ratgar Basilica (dedicated 791), Boniface's remains were translated to a new grave: since the church had been enlarged, his grave, originally in the west, was now in the middle; his relics were moved to a new apse in 819. From then on Boniface, as patron of the abbey, was regarded as both spiritual intercessor for the monks and legal owner of the abbey and its possessions, and all donations to the abbey were done in his name. He was honored on the date of his martyrdom, 5 June (with a mass written by Alcuin), and (around the year 1000) with a mass dedicated to his appointment as bishop, on 1 December.

Willibald's "vita" describes how a visitor on horseback come to the site of the martyrdom, and a hoof of his horse got stuck in the mire. When it was pulled loose, a well sprang up. By the time of the "Vita altera Bonifatii" (9th century), there was a church on the site, and the well had become a "fountain of sweet water" used to sanctify people. The "Vita Liudgeri", a hagiographical account of the work of Ludger, describes how Ludger himself had built the church, sharing duties with two other priests. According to James Palmer, the well was of great importance since the saint's body was hundreds of miles away; the physicality of the well allowed for an ongoing connection with the saint. In addition, Boniface signified Dokkum's and Frisia's "connect[ion] to the rest of (Frankish) Christendom".

Saint Boniface's feast day is celebrated on 5 June in the Roman Catholic Church, the Lutheran Church, the Anglican Communion and the Eastern Orthodox Church.

A famous statue of Saint Boniface stands on the grounds of Mainz Cathedral, seat of the archbishop of Mainz. A more modern rendition stands facing St. Peter's Church of Fritzlar.

The UK National Shrine is located at the Catholic church at Crediton, Devon, which has a bas-relief of the felling of Thor's Oak, by sculptor Kenneth Carter. The sculpture was unveiled by Princess Margaret in his native Crediton, located in Newcombes Meadow Park. There is also a series of paintings there by Timothy Moore. There are quite a few churches dedicated to St. Boniface in the United Kingdom: Bunbury, Cheshire; Chandler's Ford and Southampton Hampshire; Adler Street, London; Papa Westray, Orkney; St Budeaux, Plymouth (now demolished); Bonchurch, Isle of Wight; Cullompton, Devon.

Bishop George Errington founded St Boniface's Catholic College, Plymouth in 1856. The school celebrates Saint Boniface on 5 June each year.

In 1818, Father Norbert Provencher founded a mission on the east bank of the Red River in what was then Rupert's Land, building a log church and naming it after St. Boniface. The log church was consecrated as Saint Boniface Cathedral after Provencher was himself consecrated as a bishop and the diocese was formed. The community that grew around the cathedral eventually became the city of Saint Boniface, which merged into the city of Winnipeg in 1971. In 1844, four Grey Nuns arrived by canoe in Manitoba, and in 1871, built Western Canada's first hospital: St. Boniface Hospital, where the Assiniboine and Red Rivers meet. Today, St. Boniface Hospital is the second-largest hospital in Manitoba.

Some traditions credit Saint Boniface with the invention of the Christmas tree. The "vitae" mention nothing of the sort. However, it is mentioned on a BBC-Devon website, in an account which places Geismar in Bavaria, and in a number of educational books, including "St. Boniface and the Little Fir Tree", "The Brightest Star of All: Christmas Stories for the Family", "The American normal readers". and a short story by Henry van Dyke, "The First Christmas Tree".

The earliest "Life" of Boniface was written by a certain Willibald, an Anglo-Saxon priest who came to Mainz after Boniface's death, around 765. Willibald's biography was widely dispersed; Levison lists some forty manuscripts. According to his lemma, a group of four manuscripts including Codex Monacensis 1086 are copies directly from the original.

Listed second in Levison's edition is the entry from a late ninth-century Fulda document: Boniface's status as a martyr is attested by his inclusion in the "Fulda Martyrology" which also lists, for instance, the date (1 November) of his translation in 819, when the Fulda Cathedral had been rebuilt. A "Vita Bonifacii" was written in Fulda in the ninth century, possibly by Candidus of Fulda, but is now lost.

The next "vita", chronologically, is the "Vita altera Bonifatii auctore Radbodo", which originates in the Bishopric of Utrecht, and was probably revised by Radboud of Utrecht (899–917). Mainly agreeing with Willibald, it adds an eye-witness who presumably saw the martyrdom at Dokkum. The "Vita tertia Bonifatii" likewise originates in Utrecht. It is dated between 917 (Radboud's death) and 1075, the year Adam of Bremen wrote his "Gesta Hammaburgensis ecclesiae pontificum", which used the "Vita tertia".

A later "vita", written by Otloh of St. Emmeram (1062–1066), is based on Willibald's and a number of other "vitae" as well as the correspondence, and also includes information from local traditions.

Boniface engaged in regular correspondence with fellow churchmen all over Western Europe, including the three popes he worked with, and with some of his kinsmen back in England. Many of these letters contain questions about church reform and liturgical or doctrinal matters. In most cases, what remains is one half of the conversation, either the question or the answer. The correspondence as a whole gives evidence of Boniface's widespread connections; some of the letters also prove an intimate relationship especially with female correspondents.

There are 150 letters in what is generally called the Bonifatian correspondence, though not all them are by Boniface or addressed to him. They were assembled by order of archbishop Lullus, Boniface's successor in Mainz, and were initially organized into two parts, a section containing the papal correspondence and another with his private letters. They were reorganized in the eighth century, in a roughly chronological ordering. Otloh of St. Emmeram, who worked on a new "vita" of Boniface in the eleventh century, is credited with compiling the complete correspondence as we have it.

The correspondence was edited and published already in the seventeenth century, by Nicolaus Serarius. Stephan Alexander Würdtwein's 1789 edition, "Epistolae S. Bonifacii Archiepiscopi Magontini", was the basis for a number of (partial) translations in the nineteenth century. The first version to be published by Monumenta Germaniae Historica (MGH) was the edition by Ernst Dümmler (1892); the most authoritative version until today is Michael Tangl's 1912 "Die Briefe des Heiligen Bonifatius, Nach der Ausgabe in den Monumenta Germaniae Historica", published by MGH in 1916. This edition is the basis of Ephraim Emerton's selection and translation in English, "The Letters of Saint Boniface", first published in New York in 1940; it was republished most recently with a new introduction by Thomas F.X. Noble in 2000.

Included among his letters and dated to 716 is one to Abbess Edburga of Minster-in-Thanet containing the "Vision of the Monk of Wenlock". This otherworld vision describes how a violently ill monk is freed from his body and guided by angels to a place of judgment, where angels and devils fight over his soul as his sins and virtues come alive to accuse and defend him. He sees a hell of purgation full of pits vomiting flames. There is a bridge over a pitch-black boiling river. Souls either fall from it or safely reach the other side cleansed of their sins. This monk even sees some of his contemporary monks and is told to warn them to repent before they die. This vision bears signs of influence by the Apocalypse of Paul, the visions from the "Dialogues" of Gregory the Great, and the visions recorded by Bede.

Some fifteen preserved sermons are traditionally associated with Boniface, but that they were actually his is not generally accepted.

Early in his career, before he left for the continent, Boniface wrote the "Ars Bonifacii", a grammatical treatise presumably for his students in Nursling. Helmut Gneuss reports that one manuscript copy of the treatise originates from (the south of) England, mid-eighth century; it is now held in Marburg, in the Hessisches Staatsarchiv. He also wrote a treatise on verse, the "Caesurae uersuum", and a collection of twenty acrostic riddles, the "Enigmata", influenced greatly by Aldhelm and containing many references to works of Vergil (the "Aeneid", the "Georgics", and the "Eclogues"). The riddles fall into two sequences of ten poems. The first, "De virtutibus" ('on the virtues'), comprises: 1. "de ueritate"/truth; 2. "de fide catholica"/the Catholic faith; 3. "de spe"/hope; 4. "de misericordia"/compassion; 5. "de caritate"/love; 6. "de iustitia"/justice; 7. "de patientia"/patience; 8. "de pace uera, cristiana"/true, Christian peace; 9. "de humilitate cristiania"/Christian humility; 10. "de uirginitate"/virginity. The second sequence, "De vitiis" ('on the vices'), comprises: 1. "de neglegentia"/carelessness; 2. "de iracundia"/hot temper; 3. "de cupiditate"/greed; 4. "de superbia"/pride; 5. "de crapula"/intemperance; 6. "de ebrietate"/drunkenness; 7. "de luxoria"/fornication; 8. "de inuidia"/envy; 9. "de ignorantia"/ignorance; 10. "de uana gloria"/vainglory.

Three octosyllabic poems written in clearly Aldhelmian fashion (according to Andy Orchard) are preserved in his correspondence, all composed before he left for the continent.

A letter by Boniface charging Aldebert and Clement with heresy is preserved in the records of the Roman Council of 745 that condemned the two. Boniface had an interest in the Irish canon law collection known as "Collectio canonum Hibernensis", and a late 8th/early 9th-century manuscript in Würzburg contains, besides a selection from the "Hibernensis", a list of rubrics that mention the heresies of Clemens and Aldebert. The relevant folios containing these rubrics were most likely copied in Mainz, Würzburg, or Fulda—all places associated with Boniface. Michael Glatthaar suggested that the rubrics should be seen as Boniface's contribution to the agenda for a synod.

Boniface's death (and birth) has given rise to a number of noteworthy celebrations. The dates for some of these celebrations have undergone some changes: in 1805, 1855, and 1905 (and in England in 1955) anniversaries were calculated with Boniface's death dated in 755, according to the "Mainz tradition"; in Mainz, Michael Tangl's dating of the martyrdom in 754 was not accepted until after 1955. Celebrations in Germany centered on Fulda and Mainz, in the Netherlands on Dokkum and Utrecht, and in England on Crediton and Exeter.

The first German celebration on a fairly large scale was held in 1805 (the 1,050th anniversary of his death), followed by a similar celebration in a number of towns in 1855; both of these were predominantly Catholic affairs emphasizing the role of Boniface in German history. But if the celebrations were mostly Catholic, in the first part of the 19th century the respect for Boniface in general was an ecumenical affair, with both Protestants and Catholics praising Boniface as a founder of the German nation, in response to the German nationalism that arose after the Napoleonic era came to an end. The second part of the 19th century saw increased tension between Catholics and Protestants; for the latter, Martin Luther had become the model German, the founder of the modern nation, and he and Boniface were in direct competition for the honor. In 1905, when strife between Catholic and Protestant factions had eased (one Protestant church published a celebratory pamphlet, Gerhard Ficker's "Bonifatius, der "Apostel der Deutschen""), there were modest celebrations and a publication for the occasion on historical aspects of Boniface and his work, the 1905 "Festgabe" by Gregor Richter and Carl Scherer. In all, the content of these early celebrations showed evidence of the continuing question about the meaning of Boniface for Germany, though the importance of Boniface in cities associated with him was without question.
In 1954, celebrations were widespread in England, Germany, and the Netherlands, and a number of these celebrations were international affairs. Especially in Germany, these celebrations had a distinctly political note to them and often stressed Boniface as a kind of founder of Europe, such as when Konrad Adenauer, the (Catholic) German chancellor, addressed a crowd of 60,000 in Fulda, celebrating the feast day of the saint in a European context: "Das, was wir in Europa gemeinsam haben, [ist] gemeinsamen Ursprungs" ("What we have in common in Europe comes from the same source").

When Pope John Paul II visited Germany in November 1980, he spent two days in Fulda (17 and 18 November). He celebrated Mass in Fulda Cathedral with 30,000 gathered on the square in front of the building, and met with the German Bishops' Conference (held in Fulda since 1867). The pope next celebrated mass outside the cathedral, in front of an estimated crowd of 100,000, and hailed the importance of Boniface for German Christianity: "Der heilige Bonifatius, Bischof und Märtyrer, "bedeutet" den 'Anfang' des Evangeliums und der Kirche in Eurem Land" ("The holy Boniface, bishop and martyr, "signifies" the beginning of the gospel and the church in your country"). A photograph of the pope praying at Boniface's grave became the centerpiece of a prayer card distributed from the cathedral.

In 2004, anniversary celebrations were held throughout Northwestern Germany and Utrecht, and Fulda and Mainz—generating a great amount of academic and popular interest. The event occasioned a number of scholarly studies, esp. biographies (for instance, by Auke Jelsma in Dutch, Lutz von Padberg in German, and Klaas Bruinsma in Frisian), and a fictional completion of the Boniface correspondence (Lutterbach, "Mit Axt und Evangelium"). A German musical proved a great commercial success, and in the Netherlands an opera was staged.

There is an extensive body of literature on the saint and his work. At the time of the various anniversaries, edited collections were published containing essays by some of the best-known scholars of the time, such as the 1954 collection "Sankt Bonifatius: Gedenkgabe zum Zwölfhundertsten Todestag" and the 2004 collection "Bonifatius—Vom Angelsächsischen Missionar zum Apostel der Deutschen". In the modern era, published a number of biographies and articles on the saint focusing on his missionary praxis and his relics. The most authoritative biography remains Theodor Schieffer's "Winfrid-Bonifatius und die Christliche Grundlegung Europas" (1954).




 


</doc>
<doc id="28174" url="https://en.wikipedia.org/wiki?curid=28174" title="Data storage">
Data storage

Data storage is the recording (storing) of information (data) in a storage medium. DNA and RNA, handwriting, phonographic recording, magnetic tape, and optical discs are all examples of storage media. Recording is accomplished by virtually any form of energy. Electronic data storage requires electrical power to store and retrieve data. 

Data storage in a digital, machine-readable medium is sometimes called "digital data". Computer data storage is one of the core functions of a general purpose computer. Electronic documents can be stored in much less space than paper documents. Barcodes and magnetic ink character recognition (MICR) are two ways of recording machine-readable data on paper.

A recording medium is a physical material that holds information. Newly created information is distributed and can be stored in four storage media–print, film, magnetic, and optical–and seen or heard in four information flows–telephone, radio and TV, and the Internet as well as being observed directly. Digital information is stored on electronic media in many different recording formats.

With electronic media, the data and the recording media are sometimes referred to as "software" despite the more common use of the word to describe computer software. With (traditional art) static media, art materials such as crayons may be considered both equipment and medium as the wax, charcoal or chalk material from the equipment becomes part of the surface of the medium.

Some recording media may be temporary either by design or by nature. Volatile organic compounds may be used to preserve the environment or to purposely make data expire over time. Data such as smoke signals or skywriting are temporary by nature. Depending on the volatility, a gas (e.g. atmosphere, smoke) or a liquid surface such as a lake would be considered a temporary recording medium if at all.

A 2003 UC Berkeley report estimated that about five exabytes of new information were produced in 2002, and that 92% of this data was stored on hard disk drives. This was about twice the data produced in 2000. The amount of data transmitted over telecommunication systems in 2002 was nearly 18 exabytes—three and a half times more than was recorded on non-volatile storage. Telephone calls constituted 98% of the telecommunicated information in 2002. The researchers' highest estimate for the growth rate of newly stored information (uncompressed) was more than 30% per year.

It has been estimated that the year 2002 was the beginning of the digital age for information storage: an age in which more information is stored on digital storage devices than on analog storage devices. In 1986, approximately 1% of the world's capacity to store information was in digital format; this grew to 3% by 1993, to 25% by 2000, and to 97% by 2007. These figures correspond to less than three compressed exabytes in 1986, and 295 compressed exabytes in 2007. The quantity of digital storage doubled roughly every three years.

In a more limited study, the International Data Corporation estimated that the total amount of digital data in 2007 was 281 exabytes, and that the total amount of digital data produced exceeded the global storage capacity for the first time.

A study published in 2011 estimated that the world's technological capacity to store information in analog and digital devices grew from less than three (optimally compressed) exabytes in 1986, to 295 (optimally compressed) exabytes in 2007, and doubles roughly every three years.



</doc>
<doc id="28175" url="https://en.wikipedia.org/wiki?curid=28175" title="Sinn Féin">
Sinn Féin

Sinn Féin ( , ; ) is a democratic socialist, Irish republican political party active in both the Republic of Ireland and Northern Ireland.

The original Sinn Féin organisation was founded in 1905 by Arthur Griffith, but has split substantially on a number of occasions since then, notably giving rise to the two traditionally dominant parties of southern Irish politics—Fianna Fáil and Fine Gael—in the aftermath of the Irish Civil War. The party took its current form in 1970 after another split (with the other faction eventually becoming the Workers' Party of Ireland). It has historically been associated with the Provisional Irish Republican Army (IRA). Mary Lou McDonald became party president in February 2018.

Sinn Féin is one of the two largest parties in the Northern Ireland Assembly, winning just one seat less than the Democratic Unionist Party (DUP) at the 2017 Northern Ireland Assembly election. In that assembly it is the largest Irish nationalist party, and it holds four ministerial posts in the power-sharing Northern Ireland Executive . In the UK House of Commons, Sinn Féin holds seven of Northern Ireland's 18 seats, making it the second-largest bloc after the DUP; there it follows a policy of abstentionism, refusing to sit in parliament or vote on bills. In the Oireachtas (the parliament of the Republic of Ireland), Sinn Féin currently sits as the main opposition and the second largest party having won the largest share of first-preference votes at the 2020 Irish general election.

The phrase "Sinn Féin" is Irish for "Ourselves" or "We Ourselves", although it is frequently mistranslated as "ourselves alone" (from ""Sinn Féin Amháin"", an early-20th-century slogan. See also Sinn Féin (slogan)). The name is an assertion of Irish national sovereignty and self-determination; i.e., the Irish people governing themselves, rather than being part of a political union with Great Britain (England, Scotland and Wales) under the Westminster Parliament.

A split in January 1970, mirroring a split in the IRA, led to the emergence of two groups calling themselves Sinn Féin. One, under the continued leadership of Tomás Mac Giolla, became known as "Sinn Féin (Gardiner Place)", or "Official Sinn Féin"; the other, led by Ruairí Ó Brádaigh, became known as "Sinn Féin (Kevin Street)", or "Provisional Sinn Féin". As the "Officials" dropped all mention of Sinn Féin from their name in 1982–instead calling themselves the Workers' Party of Ireland–the term "Provisional Sinn Féin" has fallen out of use, and the party is now known simply as "Sinn Féin".

Sinn Féin members have been referred to colloquially as "Shinners", a term intended as a pejorative.

Sinn Féin was founded on 28 November 1905, when, at the first annual Convention of the National Council, Arthur Griffith outlined the Sinn Féin policy, "to establish in Ireland's capital a national legislature endowed with the moral authority of the Irish nation". The party contested the 1908 North Leitrim by-election, where it secured 27% of the vote. Thereafter, both support and membership fell. At the 1910 "Ard Fheis" (party conference) the attendance was poor, and there was difficulty finding members willing to take seats on the executive.

In 1914, Sinn Féin members, including Griffith, joined the anti-Redmond Irish Volunteers, which was referred to by Redmondites and others as the "Sinn Féin Volunteers". Although Griffith himself did not take part in the Easter Rising of 1916, many Sinn Féin members did, as they were also members of both the Volunteers and the Irish Republican Brotherhood. Government and newspapers dubbed the Rising "the Sinn Féin Rising". After the Rising, republicans came together under the banner of Sinn Féin, and at the 1917 "Ard Fheis" the party committed itself for the first time to the establishment of an Irish Republic. In the 1918 general election, Sinn Féin won 73 of Ireland's 105 seats, and in January 1919, its MPs assembled in Dublin and proclaimed themselves Dáil Éireann, the parliament of Ireland. The party supported the Irish Republican Army during the War of Independence, and members of the Dáil government negotiated the Anglo-Irish Treaty with the British government in 1921. In the Dáil debates that followed, the party divided on the Treaty. Anti-Treaty members led by Éamon de Valera walked out, and pro- and anti-Treaty members took opposite sides in the ensuing Civil War.

Pro-Treaty Dáil deputies and other Treaty supporters formed a new party, Cumann na nGaedheal, on 27 April 1923 at a meeting in Dublin, where delegates agreed on a constitution and political programme. Cumann na nGaedheal went on to govern the new Irish Free State for nine years. (It merged with two other organisations to form Fine Gael in 1933.) Anti-Treaty Sinn Féin members continued to boycott the Dáil. At a special "Ard Fheis" in March 1926, de Valera proposed that elected members be allowed to take their seats in the Dáil if and when the controversial Oath of Allegiance was removed. When his motion was defeated, de Valera resigned from Sinn Féin; on 16 May 1926, he founded his own party, Fianna Fáil, which was dedicated to republicanising the Free State from within its political structures. He took most Sinn Féin Teachtaí Dála (TDs) with him. De Valera's resignation meant also the loss of financial support from America. The rump Sinn Féin party could field no more than fifteen candidates, and won only six seats in the June 1927 general election, a level of support not seen since before 1916. Vice-President and "de facto" leader Mary MacSwiney announced that the party simply did not have the funds to contest the second election called that year, declaring "no true Irish citizen can vote for any of the other parties". Fianna Fáil came to power at the 1932 general election (to begin what would be an unbroken 16-year spell in government) and went on to long dominate politics in the independent Irish state.

An attempt in the 1940s to access funds that had been put in the care of the High Court led to the Sinn Féin Funds case, which the party lost and in which the judge ruled that it was not the legal successor to the Sinn Féin of 1917. At the 1955 United Kingdom general election, two Sinn Féin candidates were elected to Westminster, but the party's vote decreased at the following election in 1959, during the IRA's Border Campaign. Through the 1960s, some leading figures in the movement, such as Cathal Goulding, Sean Garland, Liam McMillen, Tomas MacGiolla, moved steadily to the left, even to Marxism, as a result of their own reading and thinking and contacts with the Irish and international left. This angered more traditional republicans, who wanted to stick to the national question and armed struggle. The Garland Commission was set up in 1967, to investigate the possibility of ending abstentionism. Its report angered the already disaffected traditional republican element within the party, notably Seán Mac Stíofáin and Ruairí Ó Brádaigh, who viewed such a policy as treason against the Irish Republic.

The Sinn Féin party split in two at the beginning of 1970. At the party's "Ard Fheis" on 11 January the proposal to end abstentionism and take seats, if elected, in the Dáil, the Parliament of Northern Ireland and the Parliament of the United Kingdom was put before the members. A similar motion had been adopted at an IRA convention the previous month, leading to the formation of a Provisional Army Council by Mac Stíofáin and other members opposed to the leadership. When the motion was put to the "Ard Fheis", it failed to achieve the necessary two-thirds majority. The Executive attempted to circumvent this by introducing a motion in support of IRA policy, at which point the dissenting delegates walked out of the meeting. These members reconvened at another place, appointed a Caretaker Executive and pledged allegiance to the Provisional Army Council. The Caretaker Executive declared itself opposed to the ending of abstentionism, the drift towards "extreme forms of socialism", the failure of the leadership to defend the nationalist people of Belfast during the 1969 Northern Ireland riots, and the expulsion of traditional republicans by the leadership during the 1960s.

At its October 1970 "Ard Fheis", delegates were informed that an IRA convention had been held and had regularised its structure, bringing to an end the 'provisional' period. By then, however, the label "Provisional" or "Provo" was already being applied to them by the media. The opposing, anti-abstentionist party became known as "Official Sinn Féin". It changed its name in 1977 to "Sinn Féin – The Workers' Party", and in 1982 to "The Workers' Party".

Because the "Provisionals" were committed to military rather than political action, Sinn Féin's initial membership was largely confined, in Danny Morrison's words, to men "over military age or women". A Sinn Féin organiser of the time in Belfast described the party's role as "agitation and publicity". New "cumainn" (branches) were established in Belfast, and a new newspaper, "Republican News", was published. Sinn Féin took off as a protest movement after the introduction of internment in August 1971, organising marches and pickets. The party launched its platform, "Éire Nua" ("a New Ireland") at the 1971 "Ard Fheis". In general, however, the party lacked a distinct political philosophy. In the words of Brian Feeney, "Ó Brádaigh would use Sinn Féin "ard fheiseanna" (party conferences) to announce republican policy, which was, in effect, IRA policy, namely that Britain should leave the North or the 'war' would continue". Sinn Féin was given a concrete presence in the community when the IRA declared a ceasefire in 1975. 'Incident centres' were set up to communicate potential confrontations to the British authorities. They were manned by Sinn Féin, which had been legalised the previous year by Merlyn Rees, Secretary of State for Northern Ireland.

Political status for prisoners became an issue after the ending of the truce. Rees released the last of the internees but introduced the Diplock courts, and ended 'Special Category Status' for all prisoners convicted after 1 March 1976. This led first to the blanket protest, and then to the dirty protest. Around the same time, Gerry Adams began writing for "Republican News", calling for Sinn Féin to become more involved politically. Over the next few years, Adams and those aligned with him would extend their influence throughout the republican movement and slowly marginalise Ó Brádaigh, part of a general trend of power in both Sinn Féin and the IRA shifting north. In particular, Ó Brádaigh's part in the 1975 IRA ceasefire had damaged his reputation in the eyes of northern republicans.

The prisoners' protest climaxed with the 1981 hunger strike, during which striker Bobby Sands was elected Member of Parliament for Fermanagh and South Tyrone as an Anti H-Block candidate. After his death on hunger strike, his seat was held, with an increased vote, by his election agent, Owen Carron. Two other Anti H-Block candidates were elected to Dáil Éireann in the general election in the Republic. These successes convinced republicans that they should contest every election. Danny Morrison expressed the mood at the 1981 "Ard Fheis" when he said:
This was the origin of what became known as the Armalite and ballot box strategy. "Éire Nua" was dropped in 1982, and the following year Ó Brádaigh stepped down as leader, and was replaced by Adams.

Under Adams' leadership electoral politics became increasingly important. In 1983 Alex Maskey was elected to Belfast City Council, the first Sinn Féin member to sit on that body. Sinn Féin polled over 100,000 votes in the Westminster elections that year, and Adams won the West Belfast seat that had been held by the Social Democratic and Labour Party (SDLP). By 1985 it had fifty-nine seats on seventeen of the twenty-six Northern Ireland councils, including seven on Belfast City Council.

The party began a reappraisal of the policy of abstention from the Dáil. At the 1983 "Ard Fheis" the constitution was amended to remove the ban on the discussion of abstentionism to allow Sinn Féin to run a candidate in the forthcoming European elections. However, in his address, Adams said, "We are an abstentionist party. It is not my intention to advocate change in this situation." A motion to permit entry into the Dáil was allowed at the 1985 "Ard Fheis", but did not have the active support of the leadership, and it failed narrowly. By October of the following year an IRA Convention had indicated its support for elected Sinn Féin TDs taking their seats. Thus, when the motion to end abstention was put to the "Ard Fheis" on 1 November 1986, it was clear that there would not be a split in the IRA as there had been in 1970. The motion was passed with a two-thirds majority. Ó Brádaigh and about twenty other delegates walked out, and met in a Dublin hotel with hundreds of supporters to re-organise as Republican Sinn Féin.

Tentative negotiations between Sinn Féin and the British government led to more substantive discussions with the SDLP in the 1990s. Multi-party negotiations began in 1994 in Northern Ireland, without Sinn Féin. The Provisional IRA declared a ceasefire in the autumn of 1994. Sinn Féin then joined the talks, but the Conservative government under John Major soon came to depend on unionist votes to remain in power. It suspended Sinn Féin from the talks, and began to insist that the IRA decommission all of their weapons before Sinn Féin be re-admitted to the talks; this led to the IRA calling off its ceasefire. The new Labour government of Tony Blair was not reliant on unionist votes and re-admitted Sinn Féin, leading to another, permanent, ceasefire.

The talks led to the Good Friday Agreement of 10 April 1998, which set up an inclusive devolved government in Northern Ireland, and altered the Dublin government's constitutional claim to the whole island in Articles 2 and 3 of the Constitution of Ireland. Republicans opposed to the direction taken by Sinn Féin in the peace process formed the 32 County Sovereignty Movement in the late 1990s.

The party expelled Denis Donaldson, a party official, in December 2005, with him stating publicly that he had been in the employ of the British government as an agent since the 1980s. Donaldson told reporters that the British security agencies who employed him were behind the collapse of the Assembly and set up Sinn Féin to take the blame for it, a claim disputed by the British Government. Donaldson was found fatally shot in his home in County Donegal on 4 April 2006, and a murder inquiry was launched. In April 2009, the Real IRA released a statement taking responsibility for the killing.

When Sinn Féin and the Democratic Unionist Party (DUP) became the largest parties, by the terms of the Good Friday Agreement no deal could be made without the support of both parties. They nearly reached a deal in November 2004, but the DUP insisted on photographic and/or video evidence that decommissioning had been carried out, which was unacceptable to Sinn Féin.

On 2 September 2006, Martin McGuinness publicly stated that Sinn Féin would refuse to participate in a shadow assembly at Stormont, asserting that his party would only take part in negotiations that were aimed at restoring a power-sharing government. This development followed a decision on the part of members of Sinn Féin to refrain from participating in debates since the Assembly's recall the previous May. The relevant parties to these talks were given a deadline of 24 November 2006 to decide upon whether or not they would ultimately form the executive.

The 86-year Sinn Féin boycott of policing in Northern Ireland ended on 28 January 2007, when the "Ard Fheis" voted overwhelmingly to support the Police Service of Northern Ireland (PSNI). Sinn Féin members began to sit on Policing Boards and join District Policing Partnerships. There was opposition to this decision within Sinn Féin, and some members left, including elected representatives. The most well-known opponent was former IRA prisoner Gerry McGeough, who stood in the 2007 Assembly election against Sinn Féin in the constituency of Fermanagh and South Tyrone, as an Independent Republican. He polled 1.8% of the vote. Others who opposed this development left to found the Republican Network for Unity.

Immediately after the June 2017 UK general election, where the Conservatives won 49% of seats but not an overall majority, so that non-mainstream parties could have significant influence, Gerry Adams announced for Sinn Féin that their elected MPs would continue the policy of not swearing allegiance to the Queen, as would be required for them to take their seats in the Westminster Parliament.

In 2017 and 2018 there were allegations of bullying within the party, leading to a number of resignations and expulsions of elected members.

At the "Ard Fheis" on 18 November 2017, Gerry Adams announced he would stand down as president of Sinn Féin in 2018, and would not stand for re-election as TD for Louth.

On 10 February 2018, Mary Lou McDonald was announced as the new president of Sinn Féin at a special Ard Fheis in Dublin. Michelle O'Neill was also elected as Vice President of the party. McDonald has made clear that as President of Sinn Féin, her ambition is to be in government north and south – and is willing to work in coalition as the major or minor party of government in the southern jurisdiction, a shift in policy compared to Adams ambition to govern as a minority government in the Oireachtas.

Sinn Féin are opposed to Northern Ireland leaving the European Union together with the rest of the United Kingdom, with Martin McGuinness suggesting a referendum on the reunification of Ireland immediately after the 2016 United Kingdom European Union membership referendum results were announced, a stance later reiterated by Mary Lou McDonald as a way of resolving the border issues raised by Brexit.

In the 2020 Irish general election, Sinn Féin received greatest number of first preference votes nationally (and in party's history). Although the party tried to form a coalition with the Labour Party and Social Democrats, a deal could not be agreed before Fianna Fáil, Fine Gael and the Green Party formed a coalition of their own in June 2020.

Sinn Féin is the largest Irish republican political party, and was historically associated with the IRA, while also having been associated with the Provisional IRA in the party's modern incarnation. The Irish government alleged that senior members of Sinn Féin have held posts on the IRA Army Council. However, the SF leadership has denied these claims. 

A republican document of the early 1980s stated: "Both Sinn Féin and the IRA play different but converging roles in the war of national liberation. The Irish Republican Army wages an armed campaign... Sinn Féin maintains the propaganda war and is the public and political voice of the movement". Robert White states at that time Sinn Fein was the junior partner in the relationship with the IRA, and they were separate organisations despite there being some overlapping membership.

The British government stated in 2005 that "we had always said all the way through we believed that Sinn Féin and the IRA were inextricably linked and that had obvious implications at leadership level".

The Northern Bank robbery of £26.5 million in Belfast in December 2004 further delayed a political deal in Northern Ireland. The IRA were widely blamed for the robbery although Sinn Féin denied this and stated that party officials had not known of the robbery nor sanctioned it. Because of the timing of the robbery, it is considered that the plans for the robbery must have been laid whilst Sinn Féin was engaged in talks about a possible peace settlement. This undermined confidence among unionists about the sincerity of republicans towards reaching agreement. In the aftermath of the row over the robbery, a further controversy erupted when, on RTÉ's "Questions and Answers" programme, the chairman of Sinn Féin, Mitchel McLaughlin, insisted that the IRA's controversial killing of a mother of ten young children, Jean McConville, in the early 1970s though "wrong", was not a crime, as it had taken place in the context of the political conflict. Politicians from the Republic, along with the Irish media, strongly attacked McLaughlin's comments.

On 10 February 2005, the government-appointed Independent Monitoring Commission reported that it firmly supported the PSNI and Garda Síochána assessments that the IRA was responsible for the Northern Bank robbery and that certain senior members of Sinn Féin were also senior members of the IRA and would have had knowledge of and given approval to the carrying out of the robbery. Sinn Féin has argued that the IMC is not independent, and that the inclusion of former Alliance Party leader John Alderdice and a British security head was proof of this. The IMC recommended further financial sanctions against Sinn Féin members of the Northern Ireland Assembly. The British government responded by saying it would ask MPs to vote to withdraw the parliamentary allowances of the four Sinn Féin MPs elected in 2001.

Gerry Adams responded to the IMC report by challenging the Irish government to have him arrested for IRA membership—a crime in both jurisdictions—and for conspiracy.

On 20 February 2005, Irish Minister for Justice, Equality and Law Reform Michael McDowell publicly accused three of the Sinn Féin leadership, Gerry Adams, Martin McGuinness and Martin Ferris (TD for Kerry North) of being on the seven-man IRA Army Council; they later denied this.

On 27 February 2005, a demonstration against the murder of Robert McCartney on 30 January 2005 was held in east Belfast. Alex Maskey, a former Sinn Féin Lord Mayor of Belfast, was told by relatives of McCartney to "hand over the 12" IRA members involved. The McCartney family, although formerly Sinn Féin voters themselves, urged witnesses to the crime to contact the PSNI. Three IRA men were expelled from the organisation, and a man was charged with McCartney's murder.

Irish Taoiseach Bertie Ahern subsequently called Sinn Féin and the IRA "both sides of the same coin". In February 2005 Dáil Éireann passed a motion condemning the party's alleged involvement in illegal activity. The Bush Administration did not invite Sinn Féin or any other Northern Irish political party to the annual St Patrick’s Day celebrations at the White House, choosing instead to invite the family of Robert McCartney. Senator Ted Kennedy, a regular sponsor of Gerry Adams' visits to the US during the peace process, also refused to meet Adams and hosted the McCartney family instead.

On 10 March 2005, the House of Commons in London passed without significant opposition a motion, introduced by the British government, to withdraw the allowances of the four Sinn Féin MPs for one year, in response to the Northern Bank Robbery. This measure cost the party approximately £400,000. However, the debate prior to the vote mainly surrounded the more recent events connected with the murder of Robert McCartney. Conservatives and unionists put down amendments to have the Sinn Féin MPs evicted from their offices at the House of Commons but these were defeated.

In March 2005, Mitchell Reiss, the United States Special Envoy for Northern Ireland, condemned the party's links to the IRA, saying "it is hard to understand how a European country in the year 2005 can have a private army associated with a political party".

The October 2015 Assessment on Paramilitary Groups in Northern Ireland concluded that the Provisional IRA still existed "in a much reduced form", and that some IRA members believed its Army Council oversaw both the IRA and Sinn Féin, although it believed that the leadership "remains committed to the peace process and its aim of achieving a united Ireland by political means".

Most of the party's policies are intended to be implemented on an "all-Ireland" basis which further emphasises their central aim of creating a united Ireland.

Sinn Féin is a democratic socialist and left-wing party. In the European Parliament, the party aligns itself with the European United Left–Nordic Green Left (GUE/NGL) parliamentary group. The party pledges support for minority rights, migrants' rights, and eradicating poverty. Although it is not in favour of the extension of legalised abortion (British 1967 Act) to Northern Ireland, Sinn Féin state they are opposed to the attitudes in society which "pressurise women" to have abortions and "criminalise" women who make this decision. The party does state that in cases of incest, rape, sexual abuse, "fatal foetal abnormalities", or when a woman's life and health are at risk or in danger, the final decision must rest with the woman. In the 2018 Irish abortion referendum, the party campaigned for a 'Yes' vote, but remained opposed to abortions up to 12 weeks. Categorised as "populist socialist" in literature, in 2014 leading party strategist and ideologue Eoin Ó Broin described Sinn Féin's entire political project as unashamedly populist.

Sinn Féin has been considered to be Eurosceptic. The party campaigned for a "No" vote in the Irish referendum on joining the European Economic Community in 1972. Sinn Féin was on the same side of the debate as the DUP and most of the UUP in that they wanted to pull out when UK had its referendum in 1975. The party was critical of the supposed need for an EU constitution as proposed in 2002, and urged a "No" vote in the 2008 referendum on the Lisbon Treaty, although Mary Lou McDonald said that there was "no contradiction in being pro-Europe, but anti-treaty". In its manifesto for the 2015 UK general election, Sinn Féin pledged that the party would campaign for the UK to stay within the European Union (EU), with Martin McGuinness saying that an exit "would be absolutely economically disastrous". Gerry Adams said that, if there were to be a referendum on the question, there ought to be a separate and binding referendum for Northern Ireland. Its policy of a "Europe of Equals", and its critical engagement after 2001, together with its engagement with the European Parliament, marks a change from the party's previous opposition to the EU. The party expresses, on one hand, "support for Europe-wide measures that promote and enhance human rights, equality and the all-Ireland agenda", and on the other a "principled opposition" to a European superstate. This has led political commentators to define the party as soft Eurosceptic since the 21st century.

Sinn Féin's main political goal is a united Ireland. Other key policies from their most recent election manifesto are listed below:



Sinn Féin has longstanding fraternal ties with the African National Congress and was described by Nelson Mandela as an 'old friend and ally in the anti-apartheid struggle'. Sinn Féin supports the creation of a "Minister for Europe", the Palestinians in the Israeli–Palestinian conflict the independence of Catalonia from Spain, and the right to self-determination regarding independence of the Basque Country from Spain and France. Sinn Féin opposes the United States embargo against Cuba and has called for a normalization of relations between the two countries. In 2016, the Sinn Féin party president, Gerry Adams was invited by the Cuban government to attend the state funeral of Fidel Castro whom Adams described as a 'freedom fighter' and a 'friend of Ireland's struggle'.

Sinn Féin support a policy of "critical engagement with the EU", and have a "principled opposition" to a European superstate. It opposes an EU constitution because it would reduce the sovereignty of the member-states. It also criticises the EU on grounds of neoliberalism. Sinn Féin MEP Matt Carthy says that the "European Union must become a cooperative union of nation states committed to working together on issues such as climate change, migration, trade, and using our common strengths to improve the lives of citizens. If it does not, EU disintegration becomes a real possibility." The party did however support continued UK membership of the European Union in the UK's 2016 EU referendum.

Sinn Féin is organised throughout Ireland, and membership is open to all Irish residents over the age of 16. The party is organised hierarchically into "cumainn" (branches), "comhairle ceantair" (district executives), and "cúigí" (regional executives). At national level, the "Coiste Seasta" (Standing Committee) oversees the day-to-day running of Sinn Féin. It is an eight-member body nominated by the Sinn Féin "Ard Chomhairle" (National Executive) and also includes the chairperson of each "cúige". The Sinn Féin "Ard Chomhairle" meets at least once a month. It directs the overall implementation of Sinn Féin policy and activities of the party.

The "Ard Chomhairle" also oversees the operation of various departments of Sinn Féin, viz Administration, Finance, National Organiser, Campaigns, Sinn Féin Republican Youth, Women's Forum, Culture, Publicity and International Affairs. It is made up of the following: Officer Board and nine other members, all of whom are elected by delegates to the "Ard Fheis", fifteen representing the five "Cúige" regions (three delegates each). The "Ard Chomhairle" can co-opt eight members for specific posts and additional members can be co-opted, if necessary, to ensure that at least thirty per cent of "Ard Chomhairle" members are women.

The Ardfheis (national delegate conference) is the ultimate policy-making body of the party, where delegates, directly elected by members of "cumainn", can decide on and implement policy. It is held at least once a year, but a special "Ard Fheis" can be called by the "Ard Chomhairle" or the membership under special circumstances.

Sinn Féin returned to Northern Ireland elections at the 1982 Assembly elections, winning five seats with 64,191 votes (10.1%). The party narrowly missed winning additional seats in Belfast North and Fermanagh and South Tyrone. In the 1983 UK general election eight months later, Sinn Féin increased its support, breaking the six-figure vote barrier in Northern Ireland for the first time by polling 102,701 votes (13.4%). Gerry Adams won the Belfast West constituency, and Danny Morrison fell only 78 votes short of victory in Mid Ulster.

The 1984 European elections proved to be a disappointment, with Sinn Féin's candidate Danny Morrison polling 91,476 (13.3%) and falling well behind the SDLP candidate John Hume.

By the beginning of 1985, Sinn Féin had won its first representation on local councils, owing to three by-election wins in Omagh (Seamus Kerr, May 1983) and Belfast (Alex Maskey in June 1983 and Sean McKnight in March 1984). Three sitting councillors also defected to Sinn Féin in Dungannon, Fermanagh and Derry (the last defecting from the SDLP). Sinn Féin succeeded in winning 59 seats in the 1985 local government elections, after it had predicted winning only 40 seats. However, the results continued to show a decline from the peak of 1983, as the party won 75,686 votes (11.8%). The party failed to gain any seats in the 1986 by-elections caused by the resignation of unionist MPs in protest at the Anglo-Irish Agreement. While this was partly due to an electoral pact between unionist candidates, the SF vote fell in the four constituencies they contested.

In the 1987 general election, Gerry Adams held his Belfast West seat, but the party failed to make breakthroughs elsewhere and overall polled 83,389 votes (11.4%). The same year saw the party contest the Dáil election in the Republic of Ireland; however, it failed to win any seats and polled less than 2%.

The 1989 local government elections saw a drop in support for Sinn Féin. Defending 58 seats (the 59 won in 1985, plus two 1987 by-election gains in West Belfast, minus three councillors who had defected to Republican Sinn Féin in 1986), the party lost 15 seats. In the aftermath of the election, Mitchell McLaughlin admitted that recent IRA activity had affected the Sinn Féin vote.

In the 1989 European election, Danny Morrison again failed to win a seat, polling at 48,914 votes (9%).

The nadir for SF in this period came in 1992, with Gerry Adams losing his Belfast West seat to the SDLP, and the SF vote falling in the other constituencies that they had contested relative to 1987.

In the 1997 UK general election, Adams regained Belfast West. Martin McGuinness also won a seat in Mid Ulster. In the Irish general election the same year the party won its first seat since 1957, with Caoimhghín Ó Caoláin gaining a seat in the Cavan–Monaghan constituency. In the Irish local elections of 1999 the party increased its number of councillors from 7 to 23.

The party overtook its nationalist rival, the Social Democratic and Labour Party, as the largest nationalist party in the local elections and UK general election of 2001, winning four Westminster seats to the SDLP's three. The party continues to subscribe, however, to an abstentionist policy towards the Westminster British parliament, on account of opposing that parliament's jurisdiction in Northern Ireland, as well as its oath to the Queen.
Sinn Féin increased its share of the nationalist vote in the 2003, 2007, and 2011 Assembly elections, with Martin McGuinness, former Minister for Education, taking the post of deputy First Minister in the Northern Ireland power-sharing Executive Committee. The party has three ministers in the Executive Committee.

In the 2010 general election, the party retained its five seats, and for the first time topped the poll at a Westminster election in Northern Ireland, winning 25.5% of the vote. All Sinn Féin MPs increased their share of the vote and with the exception of Fermanagh and South Tyrone, increased their majorities. In Fermanagh and South Tyrone, Unionist parties agreed a joint candidate, this resulted in the closest contest of the election, with Sinn Féin MP Michelle Gildernew holding her seat by 4 votes after 3 recounts and an election petition challenging the result.

Sinn Féin lost some ground in the 2016 Assembly election, dropping one seat to finish with 28, ten behind the DUP. In the snap election eight months later caused by the resignation of McGuinness as deputy First Minister, however, the party surged, winning 27.9% of the popular vote to 28.1% for the DUP, and 27 seats to the DUP's 28 in an Assembly reduced by 18 seats. The withdrawal of the DUP party whip from Jim Wells in May 2018 meant that Sinn Féin became the joint-largest party in the Assembly alongside the DUP, with 27 seats each.

The party had five TDs elected in the 2002 Irish general election, an increase of four from the previous election. At the general election in 2007 the party had expectations of substantial gains, with poll predictions that they would gain five to ten seats. However, the party lost one of its seats to Fine Gael. Seán Crowe, who had topped the poll in Dublin South-West fell to fifth place, with his first preference vote reduced from 20.28% to 12.16%.

On 26 November 2010, Pearse Doherty won a seat in the Donegal South-West by-election. It was the party's first by-election victory in the Republic of Ireland since 1925. After negotiations with the left-wing Independent TDs Finian McGrath and Maureen O'Sullivan, a Technical Group was formed in the Dáil to give its members more speaking time.

In the 2011 Irish general election the party made significant gains. All its sitting TDs were returned, with Seán Crowe regaining the seat he had lost in 2007 in Dublin South-West. In addition to winning long-targeted seats such as Dublin Central and Dublin North-West, the party gained unexpected seats in Cork East and Sligo–North Leitrim. It ultimately won 14 seats, the best performance at the time for the party's current incarnation. The party went on to win three seats in the Seanad election which followed their success at the general election. In the 2016 election it made further gains, finishing with 23 seats and overtaking the Labour Party as the third-largest party in the Dáil. It ran seven candidates in the Seanad election, all of whom were successful.

The party achieved their greatest contemporary result in the 2020 Irish general election, topping the first-preference votes with 24.5% and winning 37 seats. Due to poor results in the 2019 local elections and elections to the European Parliament, the party ran only 42 candidates and did not compete in Cork North-West. The party achieved unexpected success in the early counting, with 27 candidates being elected on the first count. Party leader Mary Lou McDonald called the result a "revolution" and announced she would pursue the formation of a government including Sinn Féin. Ultimately negotiations to form a new government led to Fianna Fáil, Fine Gael and the Green Party agreeing to enter a majority coalition government in June. Sinn Féin pledged to be a strong opposition to the new coalition.

Sinn Féin is represented on most county and city councils. It made large gains in the local elections of 2004, increasing its number of councillors from 21 to 54, and replacing the Progressive Democrats as the fourth-largest party in local government. At the local elections of June 2009, the party's vote fell by 0.95% to 7.34%, with no change in the number of seats. Losses in Dublin and urban areas were balanced by gains in areas such as Limerick, Wicklow, Cork, Tipperary and Kilkenny and the border counties . However, three of Sinn Féin's seven representatives on Dublin City Council resigned within six months of the June 2009 elections, one of them defecting to the Labour Party.

In the 2004 European Parliament election, Bairbre de Brún won Sinn Féin's first seat in the European Parliament, at the expense of the SDLP. She came in second behind Jim Allister of the DUP. In the 2009 election, de Brún was re-elected with 126,184 first preference votes, the only candidate to reach the quota on the first count. This was the first time since elections began in 1979 that the DUP failed to take the first seat, and was the first occasion Sinn Féin topped a poll in any Northern Ireland election.

Sinn Féin made a breakthrough in the Dublin constituency in 2004. The party's candidate, Mary Lou McDonald, was elected on the sixth count as one of four MEPs for Dublin. In the 2009 election, when Dublin's representation was reduced to three MEPs, she failed to hold her seat. In the South constituency their candidate, Councillor Toiréasa Ferris, managed to nearly double the number of first preference votes, lying third after the first count, but failed to get enough transfers to win a seat. In the 2014 election, Martina Anderson topped the poll in Northern Ireland, as did Lynn Boylan in Dublin. Liadh Ní Riada was elected in the South constituency, and Matt Carthy in Midlands–North-West. In the 2019 election, Carthy was re-elected, but Boylan and Ní Riada lost their seats. Anderson also held her Northern Ireland seat until early 2020 when her term was cut short by Brexit.




</doc>
<doc id="28176" url="https://en.wikipedia.org/wiki?curid=28176" title="Willis Tower">
Willis Tower

The Willis Tower (formerly and informally: Sears Tower, its name for 36 years) is a 110-story, skyscraper in Chicago. At completion in 1973, it surpassed the World Trade Center in New York City to become the tallest building in the world, a title that it held for nearly 25 years; it was also the tallest building in the Western Hemisphere for 41 years, until the new One World Trade Center surpassed it in 2013. While it held the title of "Tallest Office Building" until 2013, it lost the title of "Tallest Man-Made Structure" after only 3 years. The CN Tower in Toronto, which serves as a communications tower, took over the title in 1976.

The Willis Tower is considered a seminal achievement for engineer Fazlur Rahman Khan. It is currently the third-tallest building in the United States and the Western hemisphere – and the 23rd-tallest in the world. Each year, more than one million people visit its observation deck, the highest in the United States, making it one of Chicago's most popular tourist destinations. The structure was renamed in 2009 by the Willis Group as a term of its lease.

, the building's largest tenant is United Airlines, which moved its corporate headquarters from 77 West Wacker Drive (then the United Building) in 2012, occupying around 20 floors. Other major tenants include the building's namesake Willis Towers Watson and law firms Schiff Hardin and Seyfarth Shaw. Morgan Stanley plans to move to the building in 2019 and become its fourth-largest tenant by 2020.

In 1969, Sears, Roebuck & Co. was the largest retailer in the world, with about 350,000 employees. Sears executives decided to consolidate the thousands of employees in offices distributed throughout the Chicago area into one building on the western edge of Chicago's Loop. Sears asked its outside counsel, Arnstein, Gluck, Weitzenfeld & Minow (now known as Arnstein & Lehr, LLP) to suggest a location. The firm consulted with local and federal authorities and the applicable law, then offered Sears two options: the Goose Island area northwest of downtown, and a two-block area bounded by Franklin Street on the east, Jackson Boulevard on the south, Wacker Drive on the west and Adams Street on the north, with Quincy Street running through the middle from east to west.

After selection of the latter site, permits to vacate Quincy Street were obtained. Attorneys from the Arnstein firm, headed by Andrew Adsit, began buying the properties parcel by parcel. Sears purchased 15 old buildings from 100 owners and paid $2.7 million to the City of Chicago for the portion of Quincy Street the project absorbed.

Sears, which needed of office space for its planned consolidation and predicted growth, commissioned architects Skidmore, Owings & Merrill (SOM). Their team of Colombian-Peruvian architect Bruce Graham and Bangladeshi-Pakistani structural engineer Fazlur Rahman Khan designed the building as nine square "tubes" (each essentially a separate building), clustered in a 3×3 matrix forming a square base with sides. All nine tubes would rise up to the 50th floor of the building, where the northwest and southeast tubes terminate. The northeast and southwest tubes reach the 66th floor; the north, east, and south tubes end at the 90th. The remaining west and center tubes reach 108 floors.

The Sears Tower was the first building to use this innovative design. It was both structurally efficient and economic: at 1,450 feet, it provided more space and rose higher than the Empire State Building and cost much less per unit area. The system would prove highly influential in skyscraper construction and has been used in most supertall buildings since, including the world's current tallest building, the Burj Khalifa. To honor Khan's contributions, the Structural Engineers Association of Illinois commissioned a sculpture of him for the lobby of the Willis Tower.
Sears decided to focus their initial occupancy on housing their merchandise group, renting out the remaining space to other tenants until needed. The latter floor areas had to be designed to a smaller footprint with a high window-space to floor-space ratio to be attractive to prospective lessees. Smaller floorplates required a taller structure to yield sufficient square footage. Skidmore architects proposed a tower with large, floors in the lower part of the building with gradually tapered floorplates in a series of setbacks, which would give the tower its distinctive look.

As Sears continued to offer optimistic projections for growth, the tower's proposed floor count increased rapidly into the low hundreds, surpassing the height of New York's unfinished World Trade Center to become the world's tallest building. The height was restricted by a limit imposed by the Federal Aviation Administration (FAA) to protect air traffic. The financing of the tower was provided by Sears. It was topped with two antennas for television and radio broadcasting. Sears and the City of Chicago approved the design and the first steel was put in place in April 1971. The structure was completed in May 1973. The construction cost about US$150 million, equivalent to $ million in 2020 dollars. By comparison, Taipei 101, built in 2004, cost the equivalent of US$2.21 billion in 2018 dollars.

Black bands appear on the tower around the 29th–32nd, 64th–65th, 88th–89th, and 104th–108th floors. These elements are louvres to ventilate the building's environmental support systems and obscure its belted trusses. Even though regulations did not require a fire sprinkler system, the building was equipped with one from the beginning. There are around 40,000 sprinkler heads in the building, installed at a cost of $4 million.

In February 1982, two television antennas were added to the structure, increasing its total height to . The western antenna was later extended, bringing the overall height to on June 5, 2000, to improve reception of local NBC station WMAQ-TV.

As the construction of the building neared the 50th floor, lawsuits for an injunction were filed seeking to stop the building from exceeding 67 floors. The suits alleged that above that point television reception would deteriorate and cause property values to plummet. The first suit was filed by the state attorney in neighboring Lake County on March 17, 1972. A second suit was filed on March 28 in Cook County Circuit Court by the villages of Skokie, Northbrook, and Deerfield, Illinois.

Sears filed motions to dismiss the Lake and Cook County lawsuits and on May 17, 1972, Judge LaVerne Dickson, Chief of the Lake County Circuit Court, dismissed the suit, saying, "I find nothing that gives television viewers the right to reception without interference. They will have to find some other means of ensuring reception such as taller antennas." The Lake County state's attorney filed a notice of appeal to the Illinois Supreme Court, which ultimately decided in favor of Sears. In his decision on June 12, Judge Charles R. Barrett contended the plaintiffs did not have a right to undistorted television reception.

Meanwhile, the Illinois Citizens' Committee for Broadcasting requested the Federal Communications Commission (FCC) halt construction so the building would not interfere with television reception. On May 26, 1972, the Commission declined to take action on the grounds it did not have jurisdiction.

On June 30, 1972, the Illinois Supreme Court affirmed the previous rulings by Lake and Cook County Circuit Courts, by a letter order with a written opinion to follow. On September 8, 1972, the United States Court of Appeals for the Seventh Circuit upheld the FCC decision. The court's written opinion was filed on September 20, 1972. In affirming the lower court rulings, it held that "absent legislation to the contrary, defendant has a proprietary right to construct a building to its desired height and that completion of the project would not constitute a nuisance under the circumstances of this case." 

Sears' optimistic growth projections were not realized. Competition beyond its traditional rivals such as Montgomery Ward arose from emerging retail giants including Kmart, Kohl's, and Walmart. As a result of a surplus of office space that emerged in the 1980s, the tower did not draw as many tenants as projected and so stood half-vacant for a decade.

In 1984, Sears decided to improve the appeal of the lower floors of the tower to pedestrians. Their solution resulted in the addition of a new entryway dubbed by many as the "Lunchbox Entrance."

Sears looked into selling the Sears Tower in the late 1980s. In July 1990, with no potential buyer apparent, Sears took out a mortgage loan on the tower for $850 million from MetLife and AEW Capital Management, with Metlife as the holder of the mortgage note. This loan would mature in 2005.

In 1990, the law firm of Keck, Mahin & Cate decided to move into a development that would become 77 West Wacker Drive, rebuffing Sears' attempts to entice the firm to stay. Just two years later, Sears began a move of its own offices out of the building to a new campus in Hoffman Estates, Illinois, which was completed in 1995.

As the maturation of the mortgage approached, Sears renegotiated the loan in 1994. The negotiations resulted in an agreement where Sears would no longer be liable for the $850 million loan, although it would only nominally own the building, while AEW and Metlife effectively had total control. As part of the 1994 agreement, AEW and Metlife would be able to take official ownership of the building in 2003.

However, in 1997, Toronto-based TrizecHahn Corporation, (at the time the lessee of the CN Tower), purchased AEW's holdings in the building for $110 million, assuming $4 million in liabilities and a $734 million mortgage.

Trizec projected that the Sears Tower would quickly reach a value of $1 billion. These projections were not met, with the tower facing the same vacancy and other problems it saw under Sears, although Trizec made somewhat successful efforts to attract new tenants. Following the September 11 attacks, two of the largest tenants, Goldman Sachs and Merrill Lynch, immediately announced plans for vacating 300,000 ft of space. In 2003, Trizec sold its holdings of the tower to MetLife for $9 million.

In 2004, MetLife sold the building to a group of investors including New York-based Joseph Chetrit, Joseph Moinian, Lloyd Goldman, Joseph Cayre and Jeffrey Feil, and Skokie, Illinois-based American Landmark Properties. The quoted price was $840 million, with $825 million held in a mortgage.

In June 2006, seven men were arrested by the FBI and charged with plotting to destroy the tower. Deputy FBI Director John Pistole described their plot as "more aspirational than operational". The case went to court in October 2007. After three trials, five of the suspects were convicted and two acquitted. The alleged leader of the group, Narseal Batiste, was sentenced to 13½ years in prison. In response to the perceived threat of an attack, the building's largest tenant at this time, Ernst & Young, moved to North Wacker Drive in early 2009.

Since 2007, the owners had considered plans for the construction of a hotel on the north side of Jackson Boulevard, between Wacker Drive and Franklin Street, close to the entrance of the observation deck, above the tower's underground parking garage. According to the tower's owners, the second building was considered in the original design. The plan was eventually cancelled as city zoning did not permit construction of a such a tall building in that location.

In February 2009, the owners announced they were considering a plan to paint the structure silver, an idea that was later abandoned. It was hoped that a new, silver, paint-job would "rebrand" the building and highlight its advances in energy efficiency for an estimated cost of $50 million.

Although Sears' naming rights expired in 2003, the building continued to be called the Sears Tower for several years, despite multiple changes in ownership. In March 2009, London-based insurance broker Willis Group Holdings agreed to lease a portion of the building and obtained the naming rights. On July 16, 2009, the building was officially renamed Willis Tower. On August 13, 2012, United Airlines announced it would move its corporate headquarters from 77 West Wacker Drive to Willis Tower.

In 2015, the Blackstone Group purchased the tower for a reported $1.3 billion, the highest price ever paid for a U.S. property outside of New York City. In 2017, Blackstone announced a $500 million "facelift" for the property which would include the construction of a six-story commercial complex in the tower's plaza area.\

In May 2020, heavy rains caused three of the basement levels to flood, knocking out power to the building. This also resulted in many TV and radio stations going off the air.

The Willis Tower observation deck, called the Skydeck, opened on June 22, 1974. Located on the 103rd floor at an elevation of , it is the highest observation deck in the United States and one of Chicago's most famous tourist attractions. Tourists can experience how the building sways in wind and see far over the plains of Illinois and across Lake Michigan to Indiana, Michigan, and Wisconsin in clear conditions. Elevators reach the top in about 60 seconds, allowing occupants to feel the change in pressure as they ascend. The Skydeck competes with the John Hancock Center's observation floor a mile and a half away but reaching lower. Some 1.7 million tourists visit annually. A second observation deck on the 99th floor serves as a backup. The tourist entrance can be found on the south side of the building along Jackson Boulevard.

In January 2009, a major renovation of the Skydeck was begun, including the installation of retractable glass balconies which extend approximately from the facade of the 103rd floor, overlooking South Wacker Drive. The all-glass boxes, informally dubbed "The Ledge", allow visitors to see the street below. The boxes, which can accommodate , opened to the public on July 2, 2009. On May 29, 2014, the laminated glass flooring of one of the boxes shattered while visitors were inside but there were no injuries. The flooring on that same box shattered on June 12, 2019.

The Willis Tower remains the third tallest building in the Americas (after One World Trade Center and Central Park Tower) and the Western Hemisphere. With a pinnacle height of , it is the third-tallest freestanding structure in the Americas, shorter than Toronto's CN Tower. It is the eighth-tallest freestanding structure in the world by pinnacle height.

At tall, including decorative spires, the Petronas Twin Towers in Kuala Lumpur, Malaysia, controversially claimed to be the tallest building in the world in 1998. In the ensuing controversy, four categories of "tallest building" were created. Of these, Petronas was the tallest in the category of height to the top of architectural elements, meaning spires but not antennas.

Taipei 101 in Taiwan claimed the record in three of the four categories in 2004 to become recognized as the tallest building in the world. Taipei 101 surpassed the Petronas Twin Towers in spire height and the Sears Tower in roof height and highest occupied floor. The tower retained one record: its antenna exceeded Taipei 101's spire in height. In 2008, Shanghai World Financial Center claimed the records of tallest building by roof and highest occupied floor.

On August 12, 2007, the Burj Khalifa in Dubai was reported by its developers to have surpassed the tower in all height categories.

Upon completion, One World Trade Center in New York City surpassed the Willis Tower through its structural and pinnacle heights, but not by roof, observation deck elevation, or highest occupied floor.

Until 2000, the tower did not hold the record for being the tallest building by pinnacle height. From 1969 to 1978, this record was held by John Hancock Center, whose antenna reached a height of , taller than the Sears Tower's original height. One World Trade Center became taller by pinnacle height with the addition of a 359-foot (109.4-meter) antenna, bringing its total height to . In 1982, two antennas were installed which brought its total height to , making it taller than the John Hancock Center but not One World Trade Center. However, the extension of the tower's western antenna in June 2000 to allowed it to just barely claim the title of tallest building by pinnacle height.

On May 25, 1981, Dan Goodwin, wearing a homemade Spider-Man suit while using suction cups, camming devices, and sky hooks, and despite several attempts by the Chicago Fire Department to stop him, made the first successful outside ascent of the tower. Goodwin was arrested at the top after the seven-hour climb and was later charged with trespassing. Goodwin stated that the reason he made the climb was to call attention to shortcomings in high-rise rescue and firefighting techniques. After a lengthy interrogation by Chicago's District Attorney and Fire Commissioner, Goodwin was officially released from jail.

In August 1999, French urban climber Alain "Spiderman" Robert, using only his bare hands and bare feet, scaled the building's exterior glass and steel wall all the way to the top. A thick fog settled in near the end of his climb, making the last 20 stories of the building's glass and steel exterior slippery.

Annually, since 2009, the Willis Tower has hosted SkyRise Chicago, the world's tallest indoor stair climb, as a charity event benefiting Shirley Ryan AbilityLab, where participants can (legally) climb the Willis Tower's 103-story staircase.

Although Sears sold the tower in 1994 and had completely vacated it by 1995, the company retained the naming rights to the building through 2003. The new owners were rebuffed in renaming deals with CDW Corp in 2005 and the U.S. Olympic Committee in 2008. London-based insurance broker Willis Group Holdings Ltd leased more than of space on three floors in 2009. A Willis spokesman said the naming rights were obtained as part of the negotiations at no cost to Willis and the building was renamed Willis Tower on July 16, 2009.

The naming rights are valid for 15 years, so it is possible that the building's name could change again as soon as 2024. The "Chicago Tribune" joked that the building's new name reminded them of the oft-repeated "What you talkin' 'bout, Willis?" catchphrase from the American television sitcom "Diff'rent Strokes" and considered the name-change ill-advised in "a city with a deep appreciation of tradition and a healthy ego, where some Chicagoans still mourn the switch from Marshall Field's to Macy's". This feeling was confirmed in a July 16, 2009 CNN article in which some Chicago area residents expressed reluctance to accept the Willis Tower name, and in an article that appeared in the October 2010 issue of "Chicago" magazine that ranked the building among Chicago's 40 most important, the author pointedly refused to acknowledge the name change and referred to the building as the "Sears Tower". "Time" magazine called the name change one of the top 10 worst corporate name changes and pointed to negative press coverage by local news outlets and online petitions from angry residents. The naming rights issue continued into 2013, when Eric Zorn noted in the "Chicago Tribune" that "We're stubborn about such things. This month marked four years since the former Sears Tower was re-christened Willis Tower, and the new name has yet to stick."


Many broadcast station transmitters are located at the top of Willis Tower. Each list is ranked by height from the top down. Stations at the same height on the same mast indicate the use of a diplexer into the same shared antenna. Due to its extreme height, FM stations (all class B) are very limited in power output.

NOAA Weather Radio station KWO39 transmits off the tower at 162.550 MHz. Programmed by the National Weather Service Weather Forecast Office in Chicago, it is equipped with Specific Area Message Encoding (SAME), which sets off a siren on specially-programmed weather radios to alert of an impending hazard.

The building has appeared in numerous films and television shows set in Chicago such as "Ferris Bueller's Day Off", where Ferris and company visit the observation deck. "Late Night with Conan O'Brien" introduced a character called The Sears Tower Dressed In Sears Clothing when the show visited Chicago in 2006. The building is also featured in History Channel's "Life After People", in which it and other human-made landmarks suffer from neglect without humans around, collapsing two hundred years after people are gone. In an episode of the television series "Monk", Adrian Monk tries to conquer his fear of heights by imagining that he is on top of the tower. In an episode of "Kenan and Kel", Kenan Rockmore and Kel Kimble decide to climb to the top of the tower so Kenan can declare his love for a girl, but they end up getting stuck on a window washer's platform 110 stories up.

The Chicago Franchise frequently features locations where the tower is visible in the distance.

In the movie "", it is damaged by a tornado.

In the 1996 film "Michael", by Nora Ephron, the tower, then the tallest building in the world, is the last Earthly wonder the archangel Michael gets to see before vanishing from Earth.

In "1969", a Season 2 episode of the science-fiction series "Stargate SG-1", the SG-1 team accidentally travels back in time to the titular year. At one point the team travels through Chicago and the tower is shown (erroneously, since construction did not begin on the tower until two years later in 1971).

In the 2004 film "I, Robot", the tower is shown updated in the year 2035 with new triangular antennas. It is portrayed as having its height surpassed by the fictional USR (United States Robotics) Building.

In the 2008 film "The Dark Knight", it is part of Gotham City.

In the 2011 film "", it is featured in a number of scenes. The most notable one is when the N.E.S.T team tries to enter the city using V-22 Osprey helicopters. They use Willis Tower for cover before using wing suits to descend into the city streets.

In the 2013 film "Man of Steel", the tower is the location of the offices of the "Daily Planet".

In the 2014 film "Divergent", it is shown abandoned and decayed in a future Chicago.

In the 2015 film "Jupiter Ascending", the tower is featured prominently as the place where Caine and Jupiter await a spaceship to lift them off the planet.

In the 2018 film "Rampage", the Energyne corporation is headquartered in the building and uses the tower's antenna to broadcast an echolocation signal that would attract three mutated monsters. After the antenna is destroyed, the entire building collapses to the ground due to being heavily damaged by a giant mutated ape.

In the 2019 film "Captive State", which takes place a decade after an extraterrestrial race has taken over the Earth in an alien invasion, the aliens in Chicago have replaced the tower's antennas with their own jamming device to shut down all digital communication worldwide. The modified tower was also featured on one of the film's posters.

In Sufjan Stevens' 2005 album "Illinois", the tower is referenced in the track "Seer's Tower."

In the 1987 VHS video game Captain Power set in 2147, you can fly around the tower which is at that time called "The Tower of the Seer".



</doc>
<doc id="28177" url="https://en.wikipedia.org/wiki?curid=28177" title="Simony">
Simony

Simony () is the act of selling church offices and roles or sacred things. It is named after Simon Magus, who is described in the Acts of the Apostles as having offered two disciples of Jesus payment in exchange for their empowering him to impart the power of the Holy Spirit to anyone on whom he would place his hands. The term extends to other forms of trafficking for money in "spiritual things".

The appointment of ecclesiastical officials, such as bishops and abbots, by a secular authority came to be considered simoniacal and this became a key issue during the Investiture Controversy.

Although an offense against canon law, simony became widespread in the Catholic Church in the 9th and 10th centuries. In the canon law, the word bears a more extended meaning than in English law. "Simony according to the canonists", says John Ayliffe in his "Parergon",

In the "Corpus Juris Canonici", the "Decretum" and the Decretals of Gregory IX dealt with the subject. The offender whether "simoniacus" (the perpetrator of a simoniacal transaction) or "simoniace promotus" (the beneficiary of a simoniacal transaction), was liable to deprivation of his benefice and deposition from orders if a secular priest, or to confinement in a stricter monastery if a regular. No distinction seems to have been drawn between the sale of an immediate and of a reversionary interest. The innocent "simoniace promotus" was, apart from dispensation, liable to the same penalties as though he were guilty.

Certain matters were simoniacal by the canon law but would not be regarded as such in English law. So grave was the crime of simony considered that even infamous persons (deprived of citizens' rights due to conviction) could accuse another of it. English provincial and legatine constitutions continually assailed simony.

In 1494 a member of the Carmelite order, Adam of Genoa, was found murdered in his bed with twenty wounds after preaching against the practice of simony.

In the 14th century, Dante Alighieri depicted the punishment of many "clergymen, and popes and cardinals" in hell for being avaricious or miserly.

He also criticised certain popes and other simoniacs:

The Church of England struggled with the practice after its separation from Rome. For the purposes of English law, simony is defined by William Blackstone as "obtain[ing] orders, or a licence to preach, by money or corrupt practices" or, more narrowly, "the corrupt presentation of any one to an ecclesiastical benefice for gift or reward". While English law recognized simony as an offence, it treated it as merely an ecclesiastical matter, rather than a crime, for which the punishment was forfeiture of the office or any advantage from the offence and severance of any patronage relationship with the person who bestowed the office. Both Edward VI and Elizabeth I promulgated statutes against simony, in the latter case through the Simony Act 1588. The cases of Bishop of St. David's Thomas Watson in 1699 and of Dean of York William Cockburn in 1841 were particularly notable.

By the Benefices Act 1892, a person guilty of simony is guilty of an offence for which he may be proceeded against under the Clergy Discipline Act 1892. An innocent clerk is under no disability, as he might be by the canon law. Simony may be committed in three ways – in promotion to orders, in presentation to a benefice, and in resignation of a benefice. The common law (with which the canon law is incorporated, as far as it is not contrary to the common or statute law or the prerogative of the Crown) has been considerably modified by statute. Where no statute applies to the case, the doctrines of the canon law may still be of authority.

, simony remains an offence. An unlawfully bestowed office can be declared void by the Crown, and the offender can be disabled from making future appointments and fined up to £1000. Clergy are no longer required to make a declaration as to simony on ordination, but offences are now likely to be dealt with under the Clergy Discipline Measure 2003, r.8.



Attribution:



</doc>
<doc id="28178" url="https://en.wikipedia.org/wiki?curid=28178" title="September 26">
September 26





</doc>
<doc id="28179" url="https://en.wikipedia.org/wiki?curid=28179" title="Samaritans">
Samaritans

The Samaritans (; Samaritan Hebrew: , "" (, 'Guardians/Keepers/Watchers (of the Torah)'; , "al-Sāmiriyyūn") are an ethnoreligious group originating from the Israelites (or Hebrews) of the Ancient Near East.

Ancestrally, Samaritans claim descent from the tribe of Ephraim and tribe of Manasseh (two sons of Joseph) as well as from the Levites, who have links to ancient Samaria (now constituting the majority of the territory known as the West Bank) from the period of their entry into Canaan, while some Orthodox Jews suggest that it was from the beginning of the Babylonian captivity up to the Samaritan polity under the rule of Baba Rabba. Samaritans used to include descendants whose ancestry was ascribed to the Benjamin tribe, but this line became extinct in the 1960s. According to Samaritan tradition, the split between them and the Judean-led Southern Israelites began during the biblical time of the priest Eli when the Southern Israelites split off from the central Israelite tradition, as they perceive it.

In the Talmud, a central post-exilic religious text of Rabbinic Judaism, the Samaritans are called "Cuthites" or Cutheans (, "Kutim"), referring to the ancient city of Kutha, geographically located in what is today Iraq. Josephus's "Wars of the Jews" also refers to the Samaritans as the Cuthites. In the biblical account, however, Kuthah was one of several cities from which people were brought to Samaria, and they worshiped Nergal. Modern genetics partially support both the claims of the Samaritans and the account in the Hebrew Bible (and Talmud), suggesting that the genealogy of the Samaritans lies in some combination of these two accounts. This suggests that the Samaritans remained a genetically isolated population.

The Samaritans are adherents of Samaritanism, a religion closely related to Judaism. Samaritans believe that their worship, which is based on the Samaritan Pentateuch, is the true religion of the ancient Israelites from before the Babylonian captivity, preserved by those who remained in the Land of Israel, as opposed to Judaism, which they see as a related but altered and amended religion, brought back by those returning from the Babylonian Captivity. The Samaritans believe that Mount Gerizim was the original Holy Place of Israel from the time that Joshua conquered Canaan. The major issue between Jews and Samaritans has always been the location of the Chosen Place to worship God: The Temple Mount of Moriah in Jerusalem according to Judaism or Mount Gerizim according to Samaritanism.

Once a large community, the Samaritan population appears to have shrunk significantly in the wake of the bloody suppression of the Samaritan Revolts (mainly in 529 CE and 555 CE) against the Byzantine Empire. Conversion to Christianity under the Byzantines also reduced their numbers. Conversions to Islam took place as well, and by the mid–Middle Ages, Benjamin of Tudela estimated only around 1,900 Samaritans remained in Palestine and Syria.

The present-day population has been consistently divided between Qiryat Luza on Mount Gerizim and the city of Holon, just outside Tel Aviv. Most Samaritans in Holon and Qiryat Luza today speak Hebrew and Arabic. For liturgical purposes, Samaritan Hebrew, Samaritan Aramaic, and Arabic are used, all written with the Samaritan alphabet, a variant of the Paleo-Hebrew alphabet, which is distinct from the Jewish Hebrew alphabet which is a stylized form of the Imperial Aramaic script. Hebrew and later Aramaic were languages in use by the Jewish and Samaritan inhabitants of Judea (the name by which Israel was known during part of the Second Temple era) before the Roman exile.

Samaritans have a stand-alone religious status in Israel, and there are occasional conversions from Judaism to Samaritanism and vice versa due to marriages. While the Israeli Rabbinic authorities consider Samaritanism to be a branch of Judaism, the Chief Rabbinate of Israel requires Samaritans to officially go through a formal conversion to Judaism in order to be recognized as Halakhic Jews. One example is Israeli TV personality Sofi Tsedaka, who formally converted to Rabbinic Judaism at the age of 18. Samaritans with Israeli citizenship are obligated to undertake mandatory service in the Israel Defense Forces, while those with dual Israeli-Palestinian citizenship (living in Qiryat Luza) are generally exempted.

There is conflict over the etymology of the name for the Samaritans in Hebrew, stemming from the fact that they are referred to differently in different dialects of Hebrew. This has accompanied controversy over whether the Samaritans are named after the geographic area of Samaria (the northern part of what is now globally known as the West Bank), or whether the area received its name from the group. This distinction is controversial in part because different interpretations can be used to justify or deny claims of ancestry over this region, which has been deeply contested in modern times.

In Samaritan Hebrew, the Samaritans call themselves "Shamerim" (שַמֶרִים), which according to the Anchor Bible Dictionary, is derived from the Ancient Hebrew term meaning 'Guardians/Keepers/Watchers [of the Torah/Law]'.

Biblical Hebrew "Šomerim" () 'Guardians' (singular "Šomer") comes from the Hebrew Semitic root שמר, which means 'to watch, guard'.

Historically, Samaria was the key geographical concentration of the Samaritan community. Thus, it may suggest the region of Samaria is named after the Samaritans, rather than the Samaritans being named after the region. In Jewish tradition, however, it is sometimes claimed that Mount Samaria, meaning 'Watch Mountain', is actually named so because watchers used to watch from those mountains for approaching armies from Egypt in ancient times. In Modern Hebrew, the Samaritans are called , which would appear to simply mean 'inhabitants of Samaria'. This is a politically sensitive distinction.

That the etymology of the Samaritans' ethnonym in Samaritan Hebrew is derived from "Guardians/Keepers/Watchers [of the Law/Torah]", as opposed to Samaritans being named after the region of Samaria, has in history been supported by a number of Christian Church fathers, including Epiphanius of Salamis in the "Panarion", Jerome and Eusebius in the "Chronicon" and Origen in "The Commentary on Saint John's Gospel", and in some Talmudic commentary of Tanhuma on Genesis 31, and Pirke De-Rabbi Eliezer 38, p. 21.

According to Samaritan tradition, Mount Gerizim was the original Holy Place of the Israelites from the time that Joshua conquered Canaan and the tribes of Israel settled the land. The reference to Mount Gerizim derives from the biblical story of Moses ordering Joshua to take the Twelve Tribes of Israel to the mountains by Shekhem (Nablus) and place half of the tribes, six in number, on Mount Gerizim, the Mount of the Blessing, and the other half on Mount Ebal, the Mount of the Curse. The two mountains were used to symbolize the significance of the commandments and serve as a warning to whoever disobeyed them (Deut. 11:29; 27:12; Josh. 8:33).

Samaritans claim they are Israelite descendants of the Northern Israelite tribes of Ephraim and Manasseh, who survived the destruction of the Kingdom of Israel (Samaria) by the Assyrians in 722 BCE.

Samaritan historiography places the basic schism from the remaining part of Israel after the tribes of Israel conquered and returned to the land of Canaan, led by Joshua. In its account, after Joshua's death, Eli the priest left the Tabernacle which Moses erected in the desert and established on Mount Gerizim and built another one under his own rule in the hills of Shiloh.

Abu l-Fath, who in the 14th century wrote a major work of Samaritan history, comments on Samaritan origins as follows:

Further, the "Samaritan Chronicle Adler", or New Chronicle, believed to have been composed in the 18th century using earlier chronicles as sources states:

The emergence of the Samaritans as an ethnic and religious community distinct from other Levant peoples appears to have occurred at some point after the Assyrian conquest of the Israelite Kingdom of Israel in approximately 721 BCE. The records of Sargon II of Assyria indicate that he deported 27,290 inhabitants of the former kingdom.

Jewish tradition affirms the Assyrian deportations and replacement of the previous inhabitants by forced resettlement by other peoples but claims a different ethnic origin for the Samaritans. The Talmud accounts for a people called "Cuthim" on a number of occasions, mentioning their arrival by the hands of the Assyrians. According to 2 Kings and Josephus, the people of Israel were removed by the king of the Assyrians (Sargon II) to Halah, to Gozan on the Khabur River and to the towns of the Medes. The king of the Assyrians then brought people from Babylon, Cuthah, Avah, Emath, and Sepharvaim to place in Samaria. Because God sent lions among them to kill them, the king of the Assyrians sent one of the priests from Bethel to teach the new settlers about God's ordinances. The eventual result was that the new settlers worshiped both the God of the land and their own gods from the countries from which they came.

In the Chronicles, following Samaria's destruction, King Hezekiah is depicted as endeavouring to draw the Ephraimites and Manassites closer to Judah. Temple repairs at the time of Josiah were financed by money from all "the remnant of Israel" in Samaria, including from Manasseh, Ephraim, and Benjamin. Jeremiah likewise speaks of people from Shekhem, Shiloh, and Samaria who brought offerings of frankincense and grain to the House of YHWH. Chronicles makes no mention of an Assyrian resettlement. Yitzakh Magen argues that the version of Chronicles is perhaps closer to the historical truth and that the Assyrian settlement was unsuccessful, a notable Israelite population remained in Samaria, part of which, following the conquest of Judah, fled south and settled there as refugees.

A Midrash (Genesis Rabbah Sect. 94) relates about an encounter between Rabbi Meir and a Samaritan. The story that developed includes the following dialogue:

<poem>Rabbi Meir: What tribe are you from?
The Samaritan: From Joseph.
Rabbi Meir: No!
The Samaritan: From which one then?
Rabbi Meir: From Issachar.
The Samaritan: How do you figure?
Rabbi Meir: For it is written (Gen 46:13): The sons of Issachar: Tola, Puvah, Iob, and Shimron. These are the Samaritans (shamray).</poem>

Zertal dates the Assyrian onslaught at 721 BCE to 647 BCE and discusses three waves of imported settlers. He shows that Mesopotamian pottery in Samaritan territory cluster around the lands of Menasheh and that the type of pottery found was produced around 689 BCE. Some date their split with the Jews to the time of Nehemiah, Ezra, and the building of the Second Temple in Jerusalem after the Babylonian exile. Returning exiles considered the Samaritans to be non-Israelites and, thus, not fit for this religious work.

The "Encyclopaedia Judaica" (under "Samaritans") summarizes both past and present views on the Samaritans' origins. It says:

Furthermore, to this day the Samaritans claim descent from the tribe of Joseph:

The Dead Sea scroll 4Q372 hopes that the northern tribes will return to the land of Joseph. The current dwellers in the north are referred to as fools, an enemy people. However, they are not referred to as foreigners. It goes on to say that the Samaritans mocked Jerusalem and built a temple on a high place to provoke Israel.

The account of the Assyrian kings, which was among the archaeological discoveries in Babylon, differs from the Samaritan account, and confirms much of the Jewish biblical account but may differ in regard to the ethnicity of the foreigners settled in Samaria by the Assyrians. At one point, it is simply said that they were from Arabia, while at another, that they were brought from a number of countries conquered by Sargon II:

Also,

The narratives in Genesis about the rivalries among the twelve sons of Jacob are viewed by some as describing tensions between north and south. They were temporarily united in the United Monarchy, but after the death of Solomon, the kingdom split in two, the Kingdom of Israel with its last capital city Samaria and the Kingdom of Judah with its capital Jerusalem.

The Deuteronomistic history, written in Judah, portrayed Israel as a sinful kingdom, divinely punished for its idolatry and iniquity by being destroyed by the Assyrians in 720 BCE.

The tensions continued in the postexilic period. The Books of Kings are more inclusive than Ezra–Nehemiah since the ideal is of one Israel with twelve tribes, whereas the Books of Chronicles concentrate on the Kingdom of Judah and ignore the Kingdom of Israel (Samaria).

The Samaritans claimed that they were the true Israel who were descendants of the "Ten Lost Tribes" taken into Assyrian captivity. They had their own sacred precinct on Mount Gerizim and claimed that it was the original sanctuary. Moreover, they claimed that their version of the Pentateuch was the original and that the Jews had a falsified text produced by Ezra during the Babylonian exile.

Both Jewish and Samaritan religious leaders taught that it was wrong to have any contact with the opposite group, and neither was to enter the other's territories or even to speak to the other. During the New Testament period, the tensions were exploited by Roman authorities as they likewise had done between rival tribal factions elsewhere, and Josephus reports numerous violent confrontations between Jews and Samaritans throughout the first half of the first century.

According to historian Lawrence Schiffman, throughout the Persian Period, Judeans and Samaritans fought periodically with one another. The Samaritans were a blend of all kinds of people—made up of Israelites who were not exiled when the Northern Kingdom was destroyed in 722 BCE—of various different nationalities whom the Assyrians had resettled in the area. The Assyrians did this as an attempt to ensure that Israel’s national dream could not come true.

According to the Jewish version of events, when the Judean exile ended in 539 BCE and the exiles began returning home from Babylon, Samaritans found their former homeland of the north populated by other people who claimed the land as their own and Jerusalem, their former glorious capital, in ruins. The inhabitants worshiped the Pagan gods, but when the then-sparsely populated areas became infested with dangerous wild beasts, they appealed to the king of Assyria for Israelite priests to instruct them on how to worship the "God of that country." The result was a syncretistic religion, in which national groups worshiped the Israelite God, but they also served their own gods in accordance with the customs of the nations from which they had been brought.

According to Chronicles 36:22–23, the Persian emperor, Cyrus the Great (reigned 559–530 BCE), permitted the return of the exiles to their homeland and ordered the rebuilding of the Temple (Zion). The prophet Isaiah identified Cyrus as "the Lord's Messiah". The word "Messiah" refers to an anointed individual, such as a king or priest.

During the First Temple, it was possible for foreigners to help the Jewish people in an informal way until tension grew between the Samaritans and Judeans. This meant that foreigners could physically move into Judean land and abide by its laws and religion.

Ezra 4 says that the local inhabitants of the land offered to assist with the building of the new Temple during the time of Zerubbabel, but their offer was rejected. According to Ezra, this rejection precipitated a further interference not only with the rebuilding of the Temple but also with the reconstruction of Jerusalem. The issue surrounding the Samaritans offer to help rebuild the temple was a complicated one that took a while for the Judeans to think over. There had always been a division between the north and the south and this instance perfectly illustrates that. Following Solomon's death, sectionalism formed and inevitably led to the division of the kingdom. This division led to the Judeans rejecting the offer made by the Samaritans to centralise worship at the Temple.

The text is not clear on this matter, but one possibility is that these "people of the land" were thought of as Samaritans. We do know that Samaritan and Jewish alienation increased and that the Samaritans eventually built their own temple on Mount Gerizim, near Shechem.

The rebuilding of the Jewish Temple in Jerusalem took several decades. The project was first led by Sheshbazzar (ca. 538 BCE), later by Zerubbabel and Jeshua, and later still by Haggai and Zechariah (520–515 BCE). The work was completed in 515 BCE.

The term "Kuthim" applied by Jews to the Samaritans had clear pejorative connotations, implying that they were interlopers brought in from Kutha in Mesopotamia and rejecting their claim of descent from the ancient Tribes of Israel.

According to many scholars, archaeological excavations at Mount Gerizim indicate that a Samaritan temple was built there in the first half of the 5th century BCE. The date of the schism between Samaritans and Jews is unknown, but by the early 4th century BCE the communities seem to have had distinctive practices and communal separation.

Antiochus IV Epiphanes was on the throne of the Seleucid Empire from 175 to 163 BCE. His policy was to Hellenize his entire kingdom and standardize religious observance. According to 1 Maccabees 1:41-50 he proclaimed himself the incarnation of the Greek god Zeus and mandated death to anyone who refused to worship him. In the 2nd century BCE, a series of events led to a revolution by a faction of Judeans against Antiochus IV.

The universal peril led the Samaritans, eager for safety, to repudiate all connection and kinship with the Jews. The request was granted. This was put forth as the final breach between the two groups, being alleged at a much later date in the Christian Bible (John 4:9), "For Jews have no dealings with Samaritans"—or not "alleged" if the Greek sunchrasthai merely refers to not sharing utensils (NABRE).

Anderson notes that during the reign of Antiochus IV (175–164 BCE):

Josephus Book 12, Chapter 5 quotes the Samaritans as saying:

During the Hellenistic period, Samaria was largely divided between a Hellenizing faction based in Samaria (Sebastaea) and a pious faction in Shekhem and surrounding rural areas, led by the High Priest. Samaria was a largely autonomous state nominally dependent on the Seleucid Empire until around 113 BCE, when the Jewish Hasmonean ruler John Hyrcanus destroyed the Samaritan temple and devastated Samaria.

The Hellinized Samaritan Temple at Mount Gerizim was destroyed by John Hyrcanus in 113 BC, having existed about 200 years. Only a few stone remnants of it exist today.

Under the Roman Empire, Samaria became a part of the Herodian Kingdom, Herodian Tetrarchy and with deposition of the Herodian ethnarch Herod Achelaus in early 1st century CE, Samaria became a part of the province of Judaea.

Samaritans appear briefly in the Christian gospels, most notably in the account of the Samaritan woman at the well and the parable of the Good Samaritan. In the latter, it is only the Samaritan who helped the man stripped of clothing, beaten, and left on the road half dead, his Abrahamic covenantal circumcision implicitly evident. The priest and Levite walked past. But the Samaritan helped the naked man regardless of his nakedness (itself religiously offensive to the priest and Levite), his self-evident poverty, or to which Hebrew sect he belonged.

The Temple of Gerizim was rebuilt after the Bar Kokhba revolt against the Romans, around 136 CE. A building dated to the second century BCE, the Delos Synagogue, is commonly identified as a Samaritan synagogue, which would make it the oldest known Jewish or Samaritan synagogue. On the other hand, Matassa argues that, although there is evidence of Samaritans on Delos, there is no evidence the building was a synagogue.

Much of Samaritan liturgy was set by the high priest Baba Rabba in the 4th century.

There were some Samaritans in the Sasanian Empire, where they served in the army.

This period is considered as something of a golden age for the Samaritan community, the population thought to number up to a million.

According to Samaritan sources, Eastern Roman emperor Zeno (who ruled 474–491 and whom the sources call "Zait the King of Edom") persecuted the Samaritans. The Emperor went to Neapolis (Shechem), gathered the elders and asked them to convert; when they refused, Zeno had many Samaritans killed, and re-built the synagogue as a church. Zeno then took for himself Mount Gerizim, where the Samaritans worshiped God, and built several edifices, among them a tomb for his recently deceased son, on which he put a cross, so that the Samaritans, worshiping God, would prostrate in front of the tomb. Later, in 484, the Samaritans revolted. The rebels attacked Sichem, burned five churches built on Samaritan holy places and cut the finger of bishop Terebinthus, who was officiating the ceremony of Pentecost. They elected a Justa (or Justasa/Justasus) as their king and moved to Caesarea, where a noteworthy Samaritan community lived. Here several Christians were killed and the church of St. Sebastian was destroyed. Justa celebrated the victory with games in the circus. According to John Malalas, the "dux Palaestinae" Asclepiades, whose troops were reinforced by the Caesarea-based Arcadiani of Rheges, defeated Justa, killed him and sent his head to Zeno. According to Procopius, Terebinthus went to Zeno to ask for revenge; the Emperor personally went to Samaria to quell the rebellion.

Some modern historians believe that the order of the facts preserved by Samaritan sources should be inverted, as the persecution of Zeno was a consequence of the rebellion rather than its cause, and should have happened after 484, around 489. Zeno rebuilt the church of St. Procopius in Neapolis (Sichem) and the Samaritans were banned from Mount Gerizim, on whose top a signalling tower was built to alert in case of civil unrest.

Under a charismatic, messianic figure named Julianus ben Sabar (or ben Sahir), the Samaritans launched a war to create their own independent state in 529. With the help of the Ghassanids, Emperor Justinian I crushed the revolt; tens of thousands of Samaritans died or were enslaved. The Samaritan faith, which had previously enjoyed the status of "religio licita", was virtually outlawed thereafter by the Christian Byzantine Empire; from a population once at least in the hundreds of thousands, the Samaritan community dwindled to tens of thousands.
Though initially guaranteed religious freedom after the Muslim conquest of Palestine, Samaritan numbers dropped further as a result of massacres and conversions.

By the time of the Arab conquests, apart from Palestine, small dispersed communities of Samaritans were living also in Arab Egypt, Syria, and Iran. Like other non-Muslims in the empire, such as Jews, Samaritans were often considered to be People of the Book. Their minority status was protected by the Muslim rulers, and they had the right to practice their religion, but, as dhimmi, adult males had to pay the jizya or "protection tax". This however changed during late Abbasid period, with increasing persecution targeting the Samaritan community and considering them infidels which must convert to Islam. The tradition of men wearing a red tarboosh may go back to an order by the Abbasid Caliph al-Mutawakkil (847-861 CE) that required non-Muslims to be distinguished from Muslims.

During the Crusades, Samaritans, like the non-Latin Christian inhabitants of the Kingdom of Jerusalem, were second-class citizens, but they were tolerated and perhaps favored because they were docile and had been mentioned positively in the Christian New Testament.

While the majority of the Samaritan population in Damascus was massacred or converted during the reign of the Ottoman Pasha Mardam Beq in the early 17th century, the remainder of the Samaritan community there, in particular, the Danafi family, which is still influential today, moved back to Nablus in the 17th century.

The Nablus community endured because most of the surviving diaspora returned, and they have maintained a tiny presence there to this day. In 1624, the last Samaritan High Priest of the line of Eleazar son of Aaron died without issue, but according to Samaritan tradition, descendants of Aaron's other son, Ithamar, remained and took over the office.

By the late Ottoman period, the Samaritan community dwindled to its lowest. In 19th century, with pressure of conversion and persecution from the local rulers and occasional natural disasters, the community fell to just over 100 persons.

The situation of the Samaritan community improved significantly during the British Mandate of Palestine. At that time, they began to work in the public sector, like many other groups. The censuses of 1922 and 1931 recorded 163 and 182 Samaritans in Palestine, respectively. The majority of them lived in Nablus.

After the end of the British Mandate of Palestine and the subsequent establishment of the State of Israel, some of the Samaritans who were living in Jaffa emigrated to Samaria and lived in Nablus. By the late 1950s, around 100 Samaritans left the West Bank for Israel under an agreement with the Jordanian authorities in the West Bank. In 1954, Israeli President Yitzhak Ben-Zvi fostered a Samaritan enclave in Holon, Israel.

Until the 1990s, most of the Samaritans resided in the West Bank city of (Nablus) below Mount Gerizim. They relocated to the mountain itself near the Israeli settlement Har Brakha as a result of violence during the First Intifada (1987–1990). Consequently, all that is left of the Samaritan community in Nablus itself is an abandoned synagogue. The Israeli army maintains a presence in the area.

Demographic investigations of the Samaritan community were carried out in the 1960s. Detailed pedigrees of the last 13 generations show that the Samaritans comprise four lineages:

Recently several genetic studies on the Samaritan population were made using haplogroup comparisons as well as wide-genome genetic studies. Of the 12 Samaritan males used in the analysis, 10 (83%) had Y chromosomes belonging to haplogroup J, which includes three of the four Samaritan families. The Joshua-Marhiv family belongs to Haplogroup J-M267 (formerly "J1"), while the Danafi and Tsedakah families belong to haplogroup J-M172 (formerly "J2"), and can be further distinguished by the M67 SNP-- the derived allele of which has been found in the Danafi family-- and the PF5169 SNP found in the Tsedakah family. However the biggest and most important Samaritan family, the Cohen family (Tradition: Tribe of Levi), was found to belong to haplogroup E. This article predated the change of the classification of haplogroup E3b1-M78 to E3b1a-M78 and the further subdivision of E3b1a-M78 into 6 subclades based on the research of Cruciani, et al.

A 2004 article on the genetic ancestry of the Samaritans by Shen "et al." concluded from a sample comparing Samaritans to several Jewish populations, all currently living in Israel—representing the Beta Israel, Ashkenazi Jews, Iraqi Jews, Libyan Jews, Moroccan Jews, and Yemenite Jews, as well as Israeli Druze and Palestinians—that "the principal components analysis suggested a common ancestry of Samaritan and Jewish patrilineages. Most of the former may be traced back to a common ancestor in what is today identified as the paternally inherited Israelite high priesthood (Cohanim) with a common ancestor projected to the time of the Assyrian conquest of the kingdom of Israel."

Archaeologists Aharoni, et al., estimated that this "exile of peoples to and from Israel under the Assyrians" took place during ca. 734–712 BCE. The authors speculated that when the Assyrians conquered the Northern Kingdom of Israel, resulting in the exile of many of the Israelites, a subgroup of the Israelites that remained in the Land of Israel "married Assyrian and female exiles relocated from other conquered lands, which was a typical Assyrian policy to obliterate national identities." The study goes on to say that "Such a scenario could explain why Samaritan Y chromosome lineages cluster tightly with Jewish Y lineages, while their mitochondrial lineages are closest to Iraqi Jewish and Israeli Arab mtDNA sequences." Non-Jewish Iraqis were not sampled in this study; however, mitochondrial lineages of Jewish communities tend to correlate with their non-Jewish host populations, unlike paternal lineages which almost always correspond to Israelite lineages.

There were 1 million Samaritans in biblical times, but in recent times the numbers are smaller. There were 100 in 1786 and 141 in 1919, then 150 in 1967. This grew to 745 in 2011, 751 in 2012, 756 in 2013, 760 in 2014, 777 in 2015, 785 in 2016, 796 in 2017, 810 in 2018 and 820 in 2019.

Half reside in modern homes at Kiryat Luza on Mount Gerizim, which is sacred to them, and the rest in the city of Holon, just outside Tel Aviv. There are also four Samaritan families residing in Binyamina-Giv'at Ada, Matan, and Ashdod. 
As a small community physically divided between neighbors in a hostile region, Samaritans have been hesitant to overtly take sides in the Arab–Israeli conflict, fearing that doing so could lead to negative repercussions. While the Samaritan communities in both the West Bank's Nablus and Israeli Holon have assimilated to the surrounding respective cultures, Hebrew has become the primary domestic language for Samaritans. Samaritans who are Israeli citizens are drafted into the military, along with the Jewish citizens of Israel.

Relations of Samaritans with Jewish Israelis and Muslim and Christian Palestinians in neighboring areas have been mixed. Samaritans living in both Israel and in the West Bank enjoy Israeli citizenship. Samaritans in the Palestinian Authority-ruled territories are a minority in the midst of a Muslim majority. They had a reserved seat in the Palestinian Legislative Council in the election of 1996, but they no longer have one. Samaritans living in Samaria have been granted passports by both Israel and the Palestinian Authority.

One of the biggest problems facing the community today is the issue of continuity. With such a small population, divided into only four families (Cohen, Tsedakah, Danafi, and Marhiv, with the Matar family dying out in 1968) and a general refusal to accept converts, there has been a history of genetic disorders within the group due to the small gene pool. To counter this, the Samaritan community has recently agreed that men from the community marry non-Samaritan (primarily, Israeli Jewish) women, provided that the women agree to follow Samaritan religious practices. There is a six-month trial period before officially joining the Samaritan community to see whether this is a commitment that the woman would like to take. This often poses a problem for the women, who are typically less than eager to adopt the strict interpretation of biblical (Levitical) laws regarding menstruation, by which they must live in a separate dwelling during their periods and after childbirth. There have been a few instances of intermarriage. In addition, all marriages within the Samaritan community are first approved by a geneticist at Tel HaShomer Hospital, in order to prevent the spread of genetic disorders. In meetings arranged by "international marriage agencies", a small number of Ukrainian women have recently been allowed to marry into the community in an effort to expand the gene pool.

The Samaritan community in Israel also faces demographic challenges as some young people leave the community and convert to Judaism. A notable example is Israeli television presenter Sofi Tsedaka, who has made a documentary about her leaving the community at age 18.

The head of the community is the Samaritan High Priest, who is selected by age from the priestly family and resides on Mount Gerizim. The current high priest is Aabed-El ben Asher ben Matzliach who assumed the office in 2013.

Much of the local Palestinian population of Nablus is believed to be descended from Samaritans, who had converted to Islam. According to the historian Fayyad Altif, large numbers of Samaritans converted due to persecution under various Muslim rulers, and because the monotheistic nature of Islam made it easy for them to accept it. The Samaritans themselves describe the Ottoman period as the worst period in their modern history, as many Samaritan families were forced to convert to Islam during that time. Even today, certain Nabulsi family names such as Al-Amad, Al-Samri, Maslamani, Yaish, and Shaksheer among others, are associated with Samaritan ancestry.

For the Samaritans in particular, the passing of the al-Hakim Edict by the Fatimid Caliphate in 1021, under which all Jews and Christians in the Fatimid ruled southern Levant were ordered to either convert to Islam or leave, along with another notable forced conversion to Islam imposed at the hands of the rebel ibn Firāsa, would contribute to their rapid unprecedented decrease, and ultimately almost complete extinction as a separate religious community. As a result, they had decreased from nearly a million and a half in late Roman (Byzantine) times to 146 people by the end of the Ottoman Era.

In 1940, the future Israeli president and historian Yitzhak Ben-Zvi wrote an article in which he stated that two thirds of the residents of Nablus and the surrounding neighboring villages were of Samaritan origin. He mentioned the name of several Palestinian Muslim families as having Samaritan origins, including the Al-Amad, Al-Samri, Buwarda and Kasem families, who protected Samaritans from Muslim persecution in the 1850s. He further claimed that these families had written records testifying to their Samaritan ancestry, which were maintained by their priests and elders.

According to "The Economist", "most ethnic Samaritans are now pious Muslims."

The Samaritan religion is based on some of the same books used as the basis of Judaism but differs from the latter. Samaritan religious works include the Samaritan version of the Torah, the Memar Markah, the Samaritan liturgy, and Samaritan law codes and biblical commentaries. Many claim the Samaritans appear to have a text of the Torah as old as the Masoretic Text; scholars have various theories concerning the actual relationships between these three texts.

According to Samaritans, it was on Mount Gerizim that Abraham was commanded by God to offer Isaac, his son, as a sacrifice. In both narratives, God then causes the sacrifice to be interrupted, explaining that this was the ultimate test of Abraham's obedience, as a result of which all the world would receive blessing.

The Torah mentions the place where God chooses to establish His name (Deut 12:5), and Judaism takes this to refer to Jerusalem. However, the Samaritan text speaks of the place where God "has chosen" to establish His name, and Samaritans identify it as Mount Gerizim, making it the focus of their spiritual values.

The legitimacy of the Samaritan temple was attacked by Jewish scholars including Andronicus ben Meshullam.

In the Christian Bible, the Gospel of John relates an encounter between a Samaritan woman and Jesus in which she says that the mountain was the center of their worship. She poses the question to Jesus when she realizes that he is the Messiah. Jesus affirms the Jewish position, saying "You (that is, the Samaritans) worship what you do not know".


The Samaritans have retained an offshoot of the Ancient Hebrew script, a High Priesthood, the slaughtering and eating of lambs on Passover eve, and the celebration of the first month's beginning around springtime as the New Year. Yom Teru`ah (the biblical name for "Rosh Hashanah"), at the beginning of Tishrei, is not considered a New Year as it is in Rabbinic Judaism. The Samaritan Pentateuch differs from the Jewish Masoretic Text as well. Some differences are doctrinal: for example, the Samaritan Torah explicitly states that Mount Gerizim is "the place that God "has chosen"" to establish His name, as opposed to the Jewish Torah that refers to "the place that God "chooses"". Other differences are minor and seem more or less accidental.

Samaritans refer to themselves as "Benai Yisrael" ("Children of Israel") which is a term used by all Jewish denominations as a name for the Jewish people as a whole. They, however, do not refer to themselves as "Yehudim" (Jews), the standard Hebrew name for Jews.

The Talmudic attitude expressed in tractate Kutim is that they are to be treated as Jews in matters where their practice coincides with Rabbinic Judaism but as non-Jews where their practice differs. Some claim that since the 19th century, Rabbinic Judaism has regarded the Samaritans as a Jewish sect and the term "Samaritan Jews" has been used for them.

Samaritan law is not the same as Halakha (Rabbinic Jewish law). The Samaritans have several groups of religious texts, which correspond to Jewish Halakha. A few examples of such texts are:


Samaria or Samaritans are mentioned in the New Testament books of Matthew, Luke, John and Acts. The Gospel of Mark contains no mention of Samaritans or Samaria. The best known reference to the Samaritans is the Parable of the Good Samaritan, found in the Gospel of Luke. The following references are found:

The rest of the New Testament makes no specific mention of Samaria or Samaritans.

"The Samaritan News", a monthly magazine started in 1969, is written in Samaritan Aramaic, Hebrew, Arabic, and English and deals with current and historical issues with which the Samaritan community is concerned. The "Samaritan Update" is a bi-monthly e-newsletter for Samaritan Studies.

A documentary film was produced in 2018 entitled "How to Save a Tribe" presented by traveller/author Leon McCarron. It focuses on the population crisis among the Samaritans.



Samaritan view

Jewish view

Independent views

Books and other information

Photographic links

Video links



</doc>
<doc id="28180" url="https://en.wikipedia.org/wiki?curid=28180" title="Seneca Lake (New York)">
Seneca Lake (New York)

Seneca Lake is the largest of the glacial Finger Lakes of the U.S. state of New York, and the deepest lake entirely within the state. It is promoted as being the lake trout capital of the world, and is host of the National Lake Trout Derby. Because of its depth and relative ease of access, the US Navy uses Seneca Lake to perform test and evaluation of equipment ranging from single element transducers to complex sonar arrays and systems.
The lake takes its name from the Seneca nation of Native Americans. At the north end of Seneca Lake is the city of Geneva, New York, home of Hobart and William Smith Colleges and the New York State Agricultural Experiment Station, a division of Cornell University. At the south end of the lake is the village of Watkins Glen, New York, famed for auto racing and waterfalls.

Due to Seneca Lake's unique macroclimate it is home to over 50 wineries, many of them farm wineries and is the location of the Seneca Lake AVA. (See Seneca Lake wine trail).

At long, it is the second longest of the Finger Lakes and has the largest volume, estimated at , roughly half of the water in all the Finger Lakes. It has an average depth of , a maximum depth of , and a surface area of .

For comparison, Scotland's famous Loch Ness is long, wide, has a surface area of , an average depth of , a maximum depth of , and total volume of of water.

Seneca's two main inlets are Catharine Creek at the southern end and the Keuka Lake Outlet. Seneca Lake lets out into the Seneca River/ Cayuga-Seneca Canal, which joins Seneca and Cayuga Lakes at their northern ends.

It is fed by underground springs and replenished at a rate of 328,000 gallons (1240 m³) per minute. These springs keep the water moving in a circular motion, giving it little chance to freeze over. Because of Seneca Lake's great depth its temperature remains a near-constant . In summer the top warms to .

Seneca lake has a typical aquatic population for large deep lakes in the northeast, with coldwater fish such as lake trout and Atlantic salmon inhabiting the deeper waters, and warmwater fish such as smallmouth bass and yellow perch inhabiting the shallower areas. The lake is also home to a robust population of "sawbellies," the local term for alewife shad.

Seneca Lake was formed at least two million years ago by glacial carving of streams and valleys. Originally it was a part of a series of rivers that flowed northward. Around this time many continental glaciers moved into the area and started the Pleistocene glaciation also known as the Ice Age. It is presumed that the Finger Lakes were created by many advances and retreats of massive glaciers that were up to 2 miles wide.

Over 200 years ago, there were Iroquois villages on Seneca Lake's surrounding hillsides. During the American Revolutionary War, their villages, including Kanadaseaga ("Seneca Castle"), were wiped out during the 1779 Sullivan Expedition by Continental troops under order by General George Washington (in retaliation of the Wyoming Massacre ) to invade their homeland, destroy their dwellings and crops, and end their threat to the patriots. They destroyed nearly 50 Seneca and Cayuga villages. Today roadside signs trace Sullivan's route along the east side of Seneca Lake where the burning of villages and crops occurred.

After the war, the Iroquois were forced to cede their land when Britain was defeated. Their millions of acres were sold and some lands in this area were granted to veterans of the army in payment for their military service. A slow stream of European-American settlers began to arrive circa 1790. Initially the settlers were without a market nearby or a way to get their crops to market. The settlers' isolation ended in 1825 with the opening of the Erie Canal.

The canal linked the Finger Lakes Region to the outside world. Steamships, barges and ferries quickly became Seneca Lake's ambassadors of commerce and trade. The former, short Crooked Lake Canal linked Seneca Lake to Keuka Lake.

Numerous canal barges sank during operations and rest on the bottom of the lake. A collection of barges at the southwest end of the lake, near the village of Watkins Glen, is being preserved and made accessible for scuba diving by the Finger Lakes Underwater Preserve Association.

The lake is a popular fishing destination. Fish species in the lake include lake trout, rainbow trout, brown trout, landlocked salmon, largemouth bass, smallmouth bass, northern pike, pickerel, and yellow perch.

In July 1900, newspaper reports carried reports that on the evening of July 14, 1899, the steamboat Otetiani, carrying several dozen passengers, encountered a 25-foot-long sea monster with "two rows of sharp, white teeth." The steamer is said to have given chase to the creature and deliberately rammed it at full speed. The creature was struck by the ship's paddle wheel midway between head and tail, it spine broken. It raised its four-foot-long head, then gave a gasp as it died. The ship attempted to rope the monster and tow it back to shore, but it sank to the bottom of Seneca Lake. A report sometime later in the "Geneva Gazette" suggested that the incident was a hoax.

The painted rocks located at the southern end of the lake on the eastern cliff face depict an American flag, Tee-pee, and several Native Americans. The older paintings, located on the bottom of the cliff, were said to have been drawn in 1779 after the Senecas escaped men from John Sullivan's campaign. However, this account is questioned by historian Barbara Bell, arguing that it is unlikely that the Senecas would have returned to paint the paintings having just escaped from Sullivan's men. She suggests instead that these paintings may have been made much later, for tourists on Seneca Lake boat tours.

It is known that the more visible and prominent paintings of the Native Americans, American flag, and Tee-pee were added in 1929 during the Sullivan Sesquicentennial. There are two mistakes in these 1929 additions: firstly the Native Americans in the Seneca Region used longhouses and not Tee-pees, and secondly the flag is displayed pointing to the left which is never to be done on a horizontal surface.

Seneca Lake is also the site of strange and currently unexplained cannon-like booms and shakes that are heard and felt in the surrounding area. They are known locally as the Seneca Guns, Lake Drums, or Lake Guns, and these types of phenomena are known elsewhere as skyquakes. The term Lake Guns originated in the short story "The Lake Gun" by James Fenimore Cooper in 1851. There is no explanation that takes into account sounds the Iroquois heard before Cooper's time; it is possible sonic booms have been mistaken for natural sounds in modern days.

The east side of Seneca Lake was once home to a military training ground called Sampson Naval Base, primarily used during World War II. It became Sampson Air Force Base during the Korean War and was used for basic training. After Sampson AFB closed, the airfield remained as Seneca Army Airfield but was closed in 2000. The training grounds of Sampson have since been converted to a civilian picnic area called Sampson State Park.

There is still a Naval facility at Seneca Lake, the Naval Undersea Warfare Center (NUWC) Sonar test facility. A scale model of the sonar section of the nuclear submarine USS Seawolf (SSN 21) was tested during the development of this ship, which was launched in June, 1995.

There is a YSI EMM-2500 Buoy Platform located in the north end of Seneca Lake roughly in the center. Its coordinates are: latitude: 42°41'49.99"N, longitude: 76°55'29.93"W. The buoy has cellular modem communications and measures wind speed and direction, relative humidity, air temperature, barometric pressure, light intensity, and the water's depth and temperature, conductivity, turbidity, and chlorophyll-a levels.

The buoy was initially deployed in June 2006. The water depth where it is located is about .

Viticulture and winemaking in the area date back to the 19th century, with the foundation of the Seneca Lake Wine Company in 1866 marking the first major winery in the area. The modern era of wine production began in the 1970s with the establishment of several wineries and the passage of the New York Farm Winery Act of 1976. The region was established as an American Viticultural Area in 1988.

Seneca Lake Wine Trail hosts many events on and around the lake including the annual winter 'Deck the Halls' event, at which local wineries showcase their vintages.

The Elmira & Seneca Lake Railway opened for operation on June 19, 1900 from Horseheads, New York to Seneca Lake.



</doc>
<doc id="28181" url="https://en.wikipedia.org/wiki?curid=28181" title="Strait of Gibraltar">
Strait of Gibraltar

The Strait of Gibraltar (; , Archaic: Pillars of Hercules), also known as the Straits of Gibraltar, is a narrow strait that connects the Atlantic Ocean to the Mediterranean Sea and separates Gibraltar and Peninsular Spain in Europe from Morocco in Africa.

The two continents are separated by of ocean at the Strait's narrowest point. The Strait's depth ranges between which possibly interacted with the lower mean sea level of the last major glaciation 20,000 years ago when the level of the sea is believed to have been lower by . Ferries cross between the two continents every day in as little as 35 minutes. The Spanish side of the Strait is protected under El Estrecho Natural Park.

The name comes from the Rock of Gibraltar, which in turn originates from the Arabic (meaning "Tariq's Mount"), named after Tariq ibn Ziyad. It is also known as the Straits of Gibraltar, the Gut of Gibraltar (although this is mostly archaic), the STROG (STRait Of Gibraltar) in naval use, and (), "Gate of Morocco" or "Gate of the West".

In the Middle Ages, Muslims called it (), "the Passage", the Romans called it (Strait of Cadiz), and in the ancient world it was known as the "Pillars of Hercules" ().

On the northern side of the Strait are Spain and Gibraltar (a British overseas territory in the Iberian Peninsula), while on the southern side are Morocco and Ceuta (a Spanish autonomous city in northern Africa). Its boundaries were known in antiquity as the Pillars of Hercules.

Due to its location, the Strait is commonly used for illegal immigration from Africa to Europe.

The International Hydrographic Organization defines the limits of the Strait of Gibraltar as follows:

The seabed of the Strait is composed of synorogenic Betic-Rif clayey flysch covered by Pliocene and/or Quaternary calcareous sediments, sourced from thriving cold water coral communities. Exposed bedrock surfaces, coarse sediments and local sand dunes attest to the strong bottom current conditions at the present time.

Around 5.9 million years ago, the connection between the Mediterranean Sea and the Atlantic Ocean along the Betic and Rifan Corridor was progressively restricted until its total closure, effectively causing the salinity of the Mediterranean to rise periodically within the gypsum and salt deposition range, during what is known as the Messinian salinity crisis. In this water chemistry environment, dissolved mineral concentrations, temperature and stilled water currents combined and occurred regularly to precipitate many mineral salts in layers on the seabed. The resultant accumulation of various huge salt and mineral deposits about the Mediterranean basin are directly linked to this era. It is believed that this process took a short time, by geological standards, lasting between 500,000 and 600,000 years.

It is estimated that, were the Strait closed even at today's higher sea level, most water in the Mediterranean basin would evaporate within only a thousand years, as it is believed to have done then, and such an event would lay down mineral deposits like the salt deposits now found under the sea floor all over the Mediterranean.

After a lengthy period of restricted intermittent or no water exchange between the Atlantic Ocean and Mediterranean basin, approximately 5.33 million years ago, the Atlantic-Mediterranean connection was completely reestablished through the Strait of Gibraltar by the Zanclean flood, and has remained open ever since. The erosion produced by the incoming waters seems to be the main cause for the present depth of the Strait ( at the narrows, at the Camarinal Sill). The Strait is expected to close again as the African Plate moves northward relative to the Eurasian Plate, but on geological rather than human timescales.

The Strait has been identified as an Important Bird Area by BirdLife International because of the hundreds of thousands of seabirds which use it every year to migrate between the Mediterranean and the Atlantic, including significant numbers of Scopoli's and Balearic shearwaters, Audouin's and lesser black-backed gulls, razorbills, and Atlantic puffins.

A resident killer whale pod of some 36 individuals lives around the Strait, one of the few that are left in Western European waters. The pod may be facing extinction in the coming decades due to long term effects of PCB pollution.

Evidence of the first human habitation of the area by Neanderthals dates back to 125,000 years ago. It is believed that the Rock of Gibraltar may have been one of the last outposts of Neanderthal habitation in the world, with evidence of their presence there dating to as recently as 24,000 years ago. Archaeological evidence of Homo sapiens habitation of the area dates back years.

The relatively short distance between the two shores has served as a quick crossing point for various groups and civilizations throughout history, including Carthaginians campaigning against Rome, Romans travelling between the provinces of Hispania and Mauritania, Vandals raiding south from Germania through Western Rome and into North Africa in the 5th century, Moors and Berbers in the 8th–11th centuries, and Spain and Portugal in the 16th century.

Beginning in 1492, the Strait began to play a certain cultural role in acting as a barrier against cross-channel conquest and the flow of culture and language that would naturally follow such a conquest. In that year, the last Muslim government north of the Strait was overthrown by a Spanish force. Since that time, the Strait has come to foster the development of two very distinct and varied cultures on either side of it after sharing much the same culture for over 300 years from the 8th century to the early 13th century.

On the northern side, Christian-European culture has remained dominant since the expulsion of the last Muslim kingdom in 1492, along with the Romance Spanish language, while on the southern side, Muslim-Arabic/Mediterranean has been dominant since the spread of Islam into North Africa in the 700s, along with the Arabic language. For the last 500 years, religious and cultural intolerance, more than the small travel barrier that the Strait presents, has come to act as a powerful enforcing agent of the cultural separation that exists between these two groups.

The small British enclave of the city of Gibraltar presents a third cultural group found in the Strait. This enclave was first established in 1704 and has since been used by Britain to act as a surety for control of the sea lanes into and out of the Mediterranean.

Following the Spanish coup of July 1936 the Spanish Republican Navy tried to blockade the Strait of Gibraltar to hamper the transport of Army of Africa troops from Spanish Morocco to Peninsular Spain. On 5 August 1936 the so-called Convoy de la victoria was able to bring at least 2,500 men across the Strait, breaking the republican blockade.

The Strait is an important shipping route from the Mediterranean to the Atlantic. There are ferries that operate between Spain and Morocco across the Strait, as well as between Spain and Ceuta and Gibraltar to Tangier.

In December 2003, Spain and Morocco agreed to explore the construction of an undersea rail tunnel to connect their rail systems across the Strait. The gauge of the rail would be to match the proposed construction and conversion of significant parts of the existing broad gauge system to standard gauge. While the project remains in a planning phase, Spanish and Moroccan officials have met to discuss it as recently as 2012, and proposals predict it could be completed by 2025.

The Strait of Gibraltar links the Atlantic Ocean directly to the Mediterranean Sea. This direct linkage creates certain unique flow and wave patterns. These unique patterns are created due to the interaction of various regional and global evaporative forces, tidal forces, and wind forces.

Through the Strait, water generally flows more or less continually in both an eastward and a westward direction. A smaller amount of deeper saltier and therefore denser waters continually work their way westwards (the Mediterranean outflow), while a larger amount of surface waters with lower salinity and density continually work their way eastwards (the Mediterranean inflow). These general flow tendencies may be occasionally interrupted for brief periods by temporary tidal flows, depending on various lunar and solar alignments. Still, on the whole and over time, the balance of the water flow is eastwards, due to an evaporation rate within the Mediterranean basin higher than the combined inflow of all the rivers that empty into it. At the Strait's far western end is the Camarinal Sill, the Strait's shallowest point which limits mixing between the cold, less saline Atlantic water and the warm Mediterranean waters.

The Mediterranean waters are so much saltier than the Atlantic waters that they sink below the constantly incoming water and form a highly saline ("thermohaline", both warm and salty) layer of bottom water. This layer of bottom-water constantly works its way out into the Atlantic as the Mediterranean outflow. On the Atlantic side of the Strait, a density boundary separates the Mediterranean outflow waters from the rest at about depth. These waters flow out and down the continental slope, losing salinity, until they begin to mix and equilibrate more rapidly, much further out at a depth of about . The Mediterranean outflow water layer can be traced for thousands of kilometres west of the Strait, before completely losing its identity.
During the Second World War, German U-boats used the currents to pass into the Mediterranean Sea without detection, by maintaining silence with engines off. From September 1941 to May 1944 Germany managed to send 62 U-boats into the Mediterranean. All these boats had to navigate the British-controlled Strait of Gibraltar where nine U-boats were sunk while attempting passage and 10 more had to break off their run due to damage. No U-boats ever made it back into the Atlantic and all were either sunk in battle or scuttled by their own crews.

Internal waves (waves at the density boundary layer) are often produced by the Strait. Like traffic merging on a highway, the water flow is constricted in both directions because it must pass over the Camarinal Sill. When large tidal flows enter the Strait and the high tide relaxes, internal waves are generated at the Camarinal Sill and proceed eastwards. Even though the waves may occur down to great depths, occasionally the waves are almost imperceptible at the surface, at other times they can be seen clearly in satellite imagery. These "internal waves" continue to flow eastward and to refract around coastal features. They can sometimes be traced for as much as , and sometimes create interference patterns with refracted waves.

Except for its far eastern end, the Strait lies within the territorial waters of Spain and Morocco. The United Kingdom claims around Gibraltar on the northern side of the Strait, putting part of it inside British territorial waters. As this is less than the maximum, it means, according to the British claim, that part of the Strait lies in international waters. The ownership of Gibraltar and its territorial waters is disputed by Spain. Similarly, Morocco disputes Spanish sovereignty over Ceuta on the southern coast. There are several islets, such as the disputed Isla Perejil, that are claimed by both Morocco and Spain.

Under the United Nations Convention on the Law of the Sea, vessels passing through the strait do so under the regime of transit passage, rather than the more limited innocent passage allowed in most territorial waters.

Some studies have proposed the possibility of erecting tidal power generating stations within the Strait, to be powered from the predictable current at the Strait.

In the 1920s and 1930s, the Atlantropa project proposed damming the Strait to generate large amounts of electricity and lower the sea level of the Mediterranean by several hundreds of meters to create large new lands for settlement. This proposal would however have devastating effects on the local climate and ecology and would dramatically change the strength of the West African Monsoon.




</doc>
<doc id="28182" url="https://en.wikipedia.org/wiki?curid=28182" title="Social epistemology">
Social epistemology

Social epistemology refers to a broad set of approaches that can be taken in epistemology (the study of knowledge) that construes human knowledge as a collective achievement. Another way of characterizing social epistemology is as the evaluation of the social dimensions of knowledge or information. 

As a field of inquiry in analytic philosophy, social epistemology deals with questions about knowledge in social contexts, meaning those in which knowledge attributions cannot be explained by examining individuals in isolation from one another. The most common topics discussed in contemporary social epistemology are testimony (e.g. "When does a belief that x is true which resulted from being told 'x is true' constitute knowledge?"), peer disagreement (e.g. "When and how should I revise my beliefs in light of other people holding beliefs that contradict mine?"), and group epistemology (e.g. "What does it mean to attribute knowledge to groups rather than individuals, and when are such knowledge attributions appropriate?"). Social epistemology also examines the social justification of belief.

One of the enduring difficulties with defining "social epistemology" that arises is the attempt to determine what the word "knowledge" means in this context. There is also a challenge in arriving at a definition of "social" which satisfies academics from different disciplines. Social epistemologists may exist working in many of the disciplines of the humanities and social sciences, most commonly in philosophy and sociology. In addition to marking a distinct movement in traditional and analytic epistemology, social epistemology is associated with the interdisciplinary field of science and technology studies (STS).

The consideration of social dimensions of knowledge in relation to philosophy started in 380 B.C.E with Plato’s dialogue: Charmides. In it he questions the degree of certainty an unprofessional in a field can have towards a person’s claim to be a specialist in that same field. As the exploration of a dependence on authoritative figures constitutes a part of the study of social epistemology, it confirms the existence of the ideology in minds long before it was given its label. 

In 1936, Karl Mannheim turned Karl Marx‘s theory of ideology (which interpreted the “social” aspect in epistemology to be of a political or sociological nature) into an analysis of how the human society develops and functions in this respect.

The term “social epistemology” was first coined by the library scientists Margaret Egan and Jesse Shera in the 1950s. The term was used by Robert K. Merton in a 1972 article in the American Journal of Sociology and then by Steven Shapin in 1979. However, it was not until the 1980s that the current sense of “social epistemology” began to emerge. 

In the 1980s, here was a powerful growth of interest amongst philosophers in topics such as epistemic value of testimony, the nature and function of expertise, proper distribution of cognitive labor and resources among individuals in the communities and the status of group reasoning and knowledge.

In 1987, the philosophical journal ‘’Synthese‘’ published a special issue on social epistemology which included two authors that have since taken the branch of epistemology in two divergent directions: Alvin Goldman and Steve Fuller. Fuller founded a journal called ‘’Social Epistemology: A journal of knowledge, culture, and policy‘’ in 1987 and published his first book, ‘’Social Epistemology’’, in 1988. Goldman’s ‘’Knowledge in a Social World’’ came out in 1999. Goldman advocates for a type of epistemology which is sometimes called “veritistic epistemology” because of its large emphasis on truth. This type of epistemology is sometimes seen to side with “essentialism” as opposed to “multiculturalism”. But Goldman has argued that this association between veritistic epistemology and essentialism is not necessary. He describes Social Epistemology as knowledge derived from one’s interactions with another person, group or society. 

Goldman looks into one of the two strategies of the socialization of epistemology. This strategy includes the evaluation of social factors that impact knowledge formed on true belief. In contrast, Fuller takes preference for the second strategy that defines knowledge influenced by social factors as collectively accepted belief. The difference between the two can be simplified with exemplars e.g.: the first strategy means analyzing how your degree of wealth (a social factor) influences what information you determine to be valid whilst the second strategy occurs when an evaluation is done on wealth’s influence upon your knowledge acquired from the beliefs of the society in which you find yourself.

In 2012, on the occasion of the 25th anniversary of ‘’Social Epistemology’’, Fuller reflected upon the history and the prospects of the field, including the need for social epistemology to re-connect with the larger issues of knowledge production first identified by Charles Sanders Peirce as ‘’cognitive economy’’ and nowadays often pursued by library and information science. As for the “analytic social epistemology”, to which Goldman has been a significant contributor, Fuller concludes that it has “failed to make significant progress owing, in part, to a minimal understanding of actual knowledge practices, a minimised role for philosophers in ongoing inquiry, and a focus on maintaining the status quo of epistemology as a field.”

The basic view of knowledge that motivated the emergence of social epistemology as it is perceived today can be traced to the work of Thomas Kuhn and Michel Foucault, which gained acknowledgment at the end of the 1960s. Both brought historical concerns directly to bear on problems long associated with the philosophy of science. Perhaps the most notable issue here was the nature of truth, which both Kuhn and Foucault described as a relative and contingent notion. On this background, ongoing work in the sociology of scientific knowledge (SSK) and the history and philosophy of science (HPS) was able to assert its epistemological consequences, leading most notably to the establishment of the strong programme at the University of Edinburgh. In terms of the two strands of social epistemology, Fuller is more sensitive and receptive to this historical trajectory (if not always in agreement) than Goldman, whose “veritistic” social epistemology can be reasonably read as a systematic rejection of the more extreme claims associated with Kuhn and Foucault.

In the standard sense of the term today, social epistemology is a field within analytic philosophy. The field of social epistemology focuses on the social aspects of how knowledge is created and disseminated. What precisely these social aspects are, and whether they have beneficial or detrimental effects upon the possibilities to create, acquire and spread knowledge is a subject of continuous debate. The most common topics discussed in contemporary social epistemology are testimony (e.g. "When does a belief that x is true which resulted from being told 'x is true' constitute knowledge?"), peer disagreement (e.g. "When and how should I revise my beliefs in light of other people holding beliefs that contradict mine?", and group epistemology (e.g. "What does it mean to attribute knowledge to groups rather than individuals, and when are such knowledge attributions appropriate?").

Within the field, "the social" is approached in two complementary and not mutually exclusive ways: "the social" character of knowledge can either be approached through inquiries in "inter-individual" epistemic relations or through inquiries focusing on epistemic "communities". The inter-individual approach typically focuses on issues such as testimony, epistemic trust as a form of trust placed by one individual in another, epistemic dependence, epistemic authority, etc. The community approach typically focuses on issues such as community standards of justification, community procedures of critique, diversity, epistemic justice, and collective knowledge.

Social epistemology as a field within analytic philosophy has close ties to, and often overlaps with feminist epistemology and philosophy of science. While parts of the field engage in abstract, normative considerations of knowledge creation and dissemination, other parts of the field are "naturalized epistemology" in the sense that they draw on empirically gained insights---which could mean natural science research from, e.g., cognitive psychology, be that qualitative or quantitative social science research. (For the notion of "naturalized epistemology" see Willard Van Orman Quine.) And while parts of the field are concerned with analytic considerations of rather general character, case-based and domain-specific inquiries in, e.g., knowledge creation in collaborative scientific practice, knowledge exchange on online platforms or knowledge gained in learning institutions play an increasing role.

Important academic journals for social epistemology as a field within analytic philosophy are, e.g., "Episteme", "Hypatia", "Social Epistemology", and "Synthese". However, major works within this field are also published in journals that predominantly address philosophers of science and psychology or in interdisciplinary journals which focus on particular domains of inquiry (such as, e.g., "Ethics and Information Technology").


In both stages, both varieties of social epistemology remain largely "academic" or "theoretical" projects. Yet both emphasize the social significance of knowledge and therefore the cultural value of social epistemology itself. A range of journals publishing social epistemology welcome papers that include a policy dimension. 

More practical applications of social epistemology can be found in the areas of library science, academic publishing, guidelines for scientific authorship and collaboration, knowledge policy and debates over the role of the Internet in knowledge transmission and creation.

Social epistemology is still considered a relatively new addition to philosophy, with its problems and theories still fresh and in rapid movement. Of increasing importance is social epistemology developments within transdisciplinarity as manifested by media ecology.






</doc>
<doc id="28184" url="https://en.wikipedia.org/wiki?curid=28184" title="Sound card">
Sound card

A sound card (also known as an audio card) is an internal expansion card that provides input and output of audio signals to and from a computer under control of computer programs. The term "sound card" is also applied to external audio interfaces used for professional audio applications.

Sound functionality can also be integrated onto the motherboard, using components similar to those found on plug-in cards. The integrated sound system is often still referred to as a "sound card". Sound processing hardware is also present on modern video cards with HDMI to output sound along with the video using that connector; previously they used a S/PDIF connection to the motherboard or sound card.

Typical uses of sound cards or sound card functionality include providing the audio component for multimedia applications such as music composition, editing video or audio, presentation, education and entertainment (games) and video projection. Sound cards are also used for computer-based communication such as voice over IP and teleconferencing.

Sound cards use a digital-to-analog converter (DAC), which converts recorded or generated digital signal data into an analog format. The output signal is connected to an amplifier, headphones, or external device using standard interconnects, such as a TRS phone connector. 

A common external connector is the microphone connector. Input through a microphone connector can be used, for example, by speech recognition or voice over IP applications. Most sound cards have a line in connector for an analog input from a sound source that has higher voltage levels than a microphone. In either case, the sound card uses an analog-to-digital converter to digitize this signal. 

Some cards include a sound chip to support production of synthesized sounds, usually for real-time generation of music and sound effects using minimal data and CPU time.

The card may use direct memory access to transfer the samples to and from main memory, from where a recording and playback software may read and write it to the hard disk for storage, editing, or further processing.

An important sound card characteristic is polyphony, which refers to its ability to process and output multiple independent voices or sounds simultaneously. These distinct channels are seen as the number of audio outputs, which may correspond to a speaker configuration such as 2.0 (stereo), 2.1 (stereo and sub woofer), 5.1 (surround), or other configuration. Sometimes, the terms "voice" and "channel" are used interchangeably to indicate the degree of polyphony, not the output speaker configuration. For example, many older sound chips could accommodate three voices, but only one output audio channel (i.e., a single mono output), requiring all voices to be mixed together. Later cards, such as the AdLib sound card, had a 9-voice polyphony combined in 1 mono output channel.

Early PC sound cards had multiple FM synthesis voices (typically 9 or 16) which were used for MIDI music. The full capabilities of advanced cards are often not fully used; only one (mono) or two (stereo) voice(s) and channel(s) are usually dedicated to playback of digital sound samples, and playing back more than one digital sound sample usually requires a software downmix at a fixed sampling rate. Modern low-cost integrated sound cards (i.e., those built into motherboards) such as audio codecs like those meeting the AC'97 standard and even some lower-cost expansion sound cards still work this way. These devices may provide more than two sound output channels (typically 5.1 or 7.1 surround sound), but they usually have no actual hardware polyphony for either sound effects or MIDI reproduction these tasks are performed entirely in software. This is similar to the way inexpensive softmodems perform modem tasks in software rather than in hardware.

In the early days of wavetable synthesis, some sound card manufacturers advertised polyphony solely on the MIDI capabilities alone. In this case, typically, the card is only capable of two channels of digital sound and the polyphony specification solely applies to the number of MIDI instruments the sound card is capable of producing at once.

Modern sound cards may provide more flexible "audio accelerator" capabilities which can be used in support of higher levels of polyphony or other purposes such as hardware acceleration of 3D sound, positional audio and real-time DSP effects.

Connectors on the sound cards are color-coded as per the PC System Design Guide. They may also have symbols of arrows, holes and soundwaves that are associated with each jack position.

Sound cards for IBM PC compatible computers were very uncommon until 1988. For the majority IBM PC users, the internal PC speaker was the only way for early PC software to produce sound and music. The speaker hardware was typically limited to square waves. The resulting sound was generally described as "beeps and boops" which resulted in the common nickname "beeper". Several companies, most notably Access Software, developed techniques for digital sound reproduction over the PC speaker like RealSound. The resulting audio, while functional, suffered from heavily distorted output and low volume, and usually required all other processing to be stopped while sounds were played. Other home computers of the 1980s like the Commodore 64 included hardware support for digital sound playback and/or music synthesis, leaving the IBM PC at a disadvantage when it came to multimedia applications. Early sound cards for the IBM PC platform were not designed for gaming or multimedia applications, but rather on specific audio applications, such as music composition with the AdLib Personal Music System, IBM Music Feature Card, and Creative Music System, or on speech synthesis like Digispeech "DS201", Covox Speech Thing, and Street Electronics "Echo".

In 1988, a panel of computer-game CEOs stated at the Consumer Electronics Show that the PC's limited sound capability prevented it from becoming the leading home computer, that it needed a $49–79 sound card with better capability than current products, and that once such hardware was widely installed their companies would support it. Sierra On-Line, which had pioneered supporting EGA and VGA video, and 3 1/2" disks, promised that year to support the AdLib, IBM Music Feature, and Roland MT-32 sound cards in its games. A 1989 "Computer Gaming World" survey found that 18 of 25 game companies planned to support AdLib, six Roland and Covox, and seven Creative Music System/Game Blaster.

One of the first manufacturers of sound cards for the IBM PC was AdLib, which produced a card based on the Yamaha YM3812 sound chip, also known as the OPL2. The AdLib had two modes: A 9-voice mode where each voice could be fully programmed, and a less frequently used "percussion" mode with 3 regular voices producing 5 independent percussion-only voices for a total of 11. (The percussion mode was considered inflexible by most developers; it was used mostly by AdLib's own composition software.)

Creative Labs also marketed a sound card about the same time called the Creative Music System. Although the "C/MS " had twelve voices to AdLib's nine, and was a stereo card while the AdLib was mono, the basic technology behind it was based on the Philips SAA1099 chip which was essentially a square-wave generator. It sounded much like twelve simultaneous PC speakers would have except for each channel having amplitude control, and failed to sell well, even after Creative renamed it the Game Blaster a year later, and marketed it through RadioShack in the US. The Game Blaster retailed for under $100 and was compatible with many popular games, such as Silpheed.

A large change in the IBM PC compatible sound card market happened when Creative Labs introduced the Sound Blaster card. Recommended by Microsoft to developers creating software based on the Multimedia PC standard, the Sound Blaster cloned the AdLib and added a sound coprocessor for recording and play back of digital audio (likely to have been an Intel microcontroller relabeled by Creative). It was incorrectly called a "DSP" (to suggest it was a digital signal processor), a game port for adding a joystick, and capability to interface to MIDI equipment (using the game port and a special cable). With more features at nearly the same price, and compatibility as well, most buyers chose the Sound Blaster. It eventually outsold the AdLib and dominated the market.

Roland also made sound cards in the late 1980s, most of them being high quality "prosumer" cards, such as the MT-32 and LAPC-I. Roland cards often sold for hundreds of dollars, and sometimes over a thousand. Many games had music written for their cards, such as Silpheed and Police Quest II. The cards were often poor at sound effects such as laughs, but for music were by far the best sound cards available until the mid nineties. Some Roland cards, such as the SCC, and later versions of the MT-32 were made to be less expensive, but their quality was usually drastically poorer than the other Roland cards.

By 1992 one sound card vendor advertised that its product was "Sound Blaster, AdLib, Disney Sound Source and Covox Speech Thing Compatible!". Responding to readers complaining about an article on sound cards that unfavorably mentioned the Gravis Ultrasound, "Computer Gaming World" stated in January 1994 that "The de facto standard in the gaming world is Sound Blaster compatibility ... It would have been unfair to have recommended anything else". The magazine that year stated that "Wing Commander II" was "Probably the game responsible" for making it the standard card. The Sound Blaster line of cards, together with the first inexpensive CD-ROM drives and evolving video technology, ushered in a new era of multimedia computer applications that could play back CD audio, add recorded dialogue to video games, or even reproduce full motion video (albeit at much lower resolutions and quality in early days). The widespread decision to support the Sound Blaster design in multimedia and entertainment titles meant that future sound cards such as Media Vision's Pro Audio Spectrum and the Gravis Ultrasound had to be Sound Blaster compatible if they were to sell well. Until the early 2000s (by which the AC'97 audio standard became more widespread and eventually usurped the SoundBlaster as a standard due to its low cost and integration into many motherboards), Sound Blaster compatibility is a standard that many other sound cards still support to maintain compatibility with many games and applications released.

When game company Sierra On-Line opted to support add-on music hardware in addition to built-in hardware such as the PC speaker and built-in sound capabilities of the IBM PCjr and Tandy 1000, what could be done with sound and music on the IBM PC changed dramatically. Two of the companies Sierra partnered with were Roland and AdLib, opting to produce in-game music for King's Quest 4 that supported the MT-32 and AdLib Music Synthesizer. The MT-32 had superior output quality, due in part to its method of sound synthesis as well as built-in reverb. Since it was the most sophisticated synthesizer they supported, Sierra chose to use most of the MT-32's custom features and unconventional instrument patches, producing background sound effects (e.g., chirping birds, clopping horse hooves, etc.) before the Sound Blaster brought playing real audio clips to the PC entertainment world. Many game companies also supported the MT-32, but supported the Adlib card as an alternative because of the latter's higher market base. The adoption of the MT-32 led the way for the creation of the MPU-401/Roland Sound Canvas and General MIDI standards as the most common means of playing in-game music until the mid-1990s.

Early ISA bus sound cards were half-duplex, meaning they couldn't record and play digitized sound simultaneously, mostly due to inferior card hardware (e.g., DSPs). Later, ISA cards like the SoundBlaster AWE series and Plug-and-play Soundblaster clones eventually became full-duplex and supported simultaneous recording and playback, but at the expense of using up two IRQ and DMA channels instead of one, making them no different from having two half-duplex sound cards in terms of configuration. Towards the end of the ISA bus' life, ISA sound cards started taking advantage of IRQ sharing, thus reducing the IRQs needed to one, but still needed two DMA channels. Many Conventional PCI bus cards do not have these limitations and are mostly full-duplex. It should also be noted that many modern PCI bus cards also do not require free DMA channels to operate.

Also, throughout the years, sound cards have evolved in terms of digital audio sampling rate (starting from 8-bit , to 32-bit, that the latest solutions support). Along the way, some cards started offering 'wavetable' sample-based synthesis, which provides superior MIDI synthesis quality relative to the earlier OPL-based solutions, which uses FM-synthesis. Also, some higher end cards started having their own RAM and processor for user-definable sound samples and MIDI instruments as well as to offload audio processing from the CPU.

For years, sound cards had only one or two channels of digital sound (most notably the Sound Blaster series and their compatibles) with the exception of the E-MU card family, the Gravis GF-1 and AMD Interwave, which had hardware support for up to 32 independent channels of digital audio. Early games and MOD-players needing more channels than a card could support had to resort to mixing multiple channels in software. Even today, the tendency is still to mix multiple sound streams in software, except in products specifically intended for gamers or professional musicians, with a sensible difference in price from "software based" products. Also, in the early era of 'wavetable' sample-based synthesis, sound card companies would also sometimes boast about the card's polyphony capabilities in terms of MIDI synthesis. In this case polyphony solely refers to the count of MIDI notes the card is capable of synthesizing simultaneously at one given time and not the count of digital audio streams the card is capable of handling.

In regards to physical sound output, the number of physical sound channels has also increased. The first sound card solutions were mono. Stereo sound was introduced in the early 1980s, and quadraphonic sound came in 1989. This was shortly followed by 5.1 channel audio. The latest sound cards support up to audio channels in the 7.1 speaker setup.

Most new sound cards no longer have the audio loopback device commonly called "Stereo Mix"/"Wave out mix"/"Mono Mix"/"What U Hear" that was once very prevalent and that allows users to digitally record speaker output to the microphone input.

Lenovo and other manufacturers fail to implement the chipset feature in hardware, while other manufacturers disable the driver from supporting it. In some cases loopback can be reinstated with driver updates (as in the case of some Dell computers); alternatively software (Total Recorder or Virtual Audio Cable) can be purchased to enable the functionality. According to Microsoft, the functionality was hidden by default in Windows Vista (to reduce user confusion), but is still available, as long as the underlying sound card drivers and hardware support it. Ultimately, the user can connect the line out directly to the line in (analog hole).

In laptops, manufacturers have gradually moved from providing 3 separate jacks with TRS connectors - usually for line in, line out/headphone out and microphone in to just a single combo jack with TRRS connector that combines microphone in and line out.

Professional sound cards are special sound cards optimized for low-latency multichannel sound recording and playback, including studio-grade fidelity. Their drivers usually follow the Audio Stream Input Output protocol for use with professional sound engineering and music software, although ASIO drivers are also available for a range of consumer-grade sound cards.

Professional sound cards are usually described as "audio interfaces", and sometimes have the form of external rack-mountable units using USB, FireWire, or an optical interface, to offer sufficient data rates. The emphasis in these products is, in general, on multiple input and output connectors, direct hardware support for multiple input and output sound channels, as well as higher sampling rates and fidelity as compared to the usual consumer sound card. In that respect, their role and intended purpose is more similar to a specialized multi-channel data recorder and real-time audio mixer and processor, roles which are possible only to a limited degree with typical consumer sound cards.

On the other hand, certain features of consumer sound cards such as support for environmental audio extensions (EAX), optimization for hardware acceleration in video games, or real-time ambience effects are secondary, nonexistent or even undesirable in professional sound cards, and as such audio interfaces are not recommended for the typical home user.

The typical "consumer-grade" sound card is intended for generic home, office, and entertainment purposes with an emphasis on playback and casual use, rather than catering to the needs of audio professionals. In response to this, Steinberg (the creators of audio recording and sequencing software, Cubase and Nuendo) developed a protocol that specified the handling of multiple audio inputs and outputs.

In general, consumer grade sound cards impose several restrictions and inconveniences that would be unacceptable to an audio professional. One of a modern sound card's purposes is to provide an AD/DA converter (analog to digital/digital to analog). However, in professional applications, there is usually a need for enhanced recording (analog to digital) conversion capabilities.

One of the limitations of consumer sound cards is their comparatively large sampling latency; this is the time it takes for the AD Converter to complete conversion of a sound sample and transfer it to the computer's main memory.

Consumer sound cards are also limited in the "effective" sampling rates and bit depths they can actually manage (compare analog versus digital sound) and have lower numbers of less flexible input channels: professional studio recording use typically requires more than the two channels that consumer sound cards provide, and more accessible connectors, unlike the variable mixture of internal—and sometimes virtual—and external connectors found in consumer-grade sound cards.

In 1984, the first IBM PCjr had a rudimentary 3-voice sound synthesis chip (the SN76489) which was capable of generating three square-wave tones with variable amplitude, and a pseudo-white noise channel that could generate primitive percussion sounds. The Tandy 1000, initially a clone of the PCjr, duplicated this functionality, with the Tandy TL/SL/RL models adding digital sound recording and playback capabilities. Many games during the 1980s that supported the PCjr's video standard (described as "Tandy-compatible", "Tandy graphics", or "TGA") also supported PCjr/Tandy 1000 audio.

In the late 1990s many computer manufacturers began to replace plug-in sound cards with a "codec" chip (actually a combined audio AD/DA-converter) integrated into the motherboard. Many of these used Intel's AC'97 specification. Others used inexpensive ACR slot accessory cards.

From around 2001 many motherboards incorporated integrated "real" (non-codec) sound cards, usually in the form of a custom chipset providing something akin to full Sound Blaster compatibility, providing relatively high-quality sound.

However, these features were dropped when AC'97 was superseded by Intel's HD Audio standard, which was released in 2004, again specified the use of a codec chip, and slowly gained acceptance. As of 2011, most motherboards have returned to using a codec chip, albeit an HD Audio compatible one, and the requirement for Sound Blaster compatibility relegated to history.

Various non-IBM PC compatible computers, such as early home computers like the Commodore 64 (1982) and Amiga (1985), NEC's PC-88 and PC-98, Fujitsu's FM-7 and FM Towns, the MSX, Apple's Macintosh, and workstations from manufacturers like Sun, have had their own motherboard integrated sound devices. In some cases, most notably in those of the Macintosh, Amiga, C64, PC-98, MSX, FM-7, and FM towns, they provide very advanced capabilities (as of the time of manufacture), in others they are only minimal capabilities. Some of these platforms have also had sound cards designed for their bus architectures that cannot be used in a standard PC.

Several Japanese computer platforms, including the PC-88, PC-98, MSX, and FM-7, featured built-in FM synthesis sound from Yamaha by the mid-1980s. By 1989, the FM Towns computer platform featured built-in PCM sample-based sound and supported the CD-ROM format.

The custom sound chip on Amiga, named Paula, had four digital sound channels (2 for the left speaker and 2 for the right) with 8-bit resolution (although with patches, 14/15-bit was accomplishable at the cost of high CPU usage) for each channel and a 6-bit volume control per channel. Sound playback on Amiga was done by reading directly from the chip-RAM without using the main CPU.

Most arcade games have integrated sound chips, the most popular being the Yamaha OPL chip for BGM coupled with a variety of DACs for sampled audio and sound effects.

The earliest known sound card used by computers was the Gooch Synthetic Woodwind, a music device for PLATO terminals, and is widely hailed as the precursor to sound cards and MIDI. It was invented in 1972.

Certain early arcade machines made use of sound cards to achieve playback of complex audio waveforms and digital music, despite being already equipped with onboard audio. An example of a sound card used in arcade machines is the Digital Compression System card, used in games from Midway. For example, Mortal Kombat II on the Midway T Unit hardware. The T-Unit hardware already has an onboard YM2151 OPL chip coupled with an OKI 6295 DAC, but said game uses an added on DCS card instead. The card is also used in the arcade version of Midway and Aerosmith's Revolution X for complex looping BGM and speech playback (Revolution X used fully sampled songs from the band's album that transparently looped- an impressive feature at the time the game was released).

MSX computers, while equipped with built-in sound capabilities, also relied on sound cards to produce better quality audio. The card, known as Moonsound, uses a Yamaha OPL4 sound chip. Prior to the Moonsound, there were also sound cards called "MSX Music" and "MSX Audio", which uses OPL2 and OPL3 chipsets, for the system.

The Apple II series of computers, which did not have sound capabilities beyond a beep until the IIGS, could use plug-in sound cards from a variety of manufacturers. The first, in 1978, was ALF's Apple Music Synthesizer, with 3 voices; two or three cards could be used to create 6 or 9 voices in stereo. Later ALF created the Apple Music II, a 9-voice model. The most widely supported card, however, was the Mockingboard. Sweet Micro Systems sold the Mockingboard in various models. Early Mockingboard models ranged from 3 voices in mono, while some later designs had 6 voices in stereo. Some software supported use of two Mockingboard cards, which allowed 12-voice music and sound. A 12-voice, single card clone of the Mockingboard called the Phasor was made by Applied Engineering. In late 2005 a company called ReactiveMicro.com produced a 6-voice clone called the Mockingboard v1 and also had plans to clone the Phasor and produce a hybrid card user-selectable between Mockingboard and Phasor modes plus support both the SC-01 or SC-02 speech synthesizers.

The Sinclair ZX Spectrum that initially only had a beeper had some sound cards made for it. One example is the TurboSound. Other examples are the Fuller Box, Melodik for the Didaktik Gamma, AY-Magic et.c. The Zon X-81 for the ZX81 was also possible to use on the ZX Spectrum using an adapter.

Devices such as the Covox Speech Thing could be attached to the parallel port of an IBM PC and feed 6- or 8-bit PCM sample data to produce audio. Also, many types of professional sound cards (audio interfaces) have the form of an external FireWire or USB unit, usually for convenience and improved fidelity.

Sound cards using the PCMCIA Cardbus interface were available before laptop and notebook computers routinely had onboard sound. Cardbus audio may still be used if onboard sound quality is poor. When Cardbus interfaces were superseded by Expresscard on computers since about 2005, manufacturers followed. Most of these units are designed for mobile DJs, providing separate outputs to allow both playback and monitoring from one system, however some also target mobile gamers, providing high-end sound to gaming laptops who are usually well-equipped when it comes to graphics and processing power, but tend to have audio codecs that are no better than the ones found on regular laptops.

USB sound "cards" are external devices that plug into the computer via USB. They are often used in studios and on stage by electronic musicians including live PA performers and DJs. DJs who use DJ software typically use sound cards integrated into DJ controllers or specialized DJ sound cards. DJ sound cards sometimes have inputs with phono preamplifiers to allow turntables to be connected to the computer to control the software's playback of music files with timecode vinyl.

The USB specification defines a standard interface, the USB audio device class, allowing a single driver to work with the various USB sound devices and interfaces on the market. Mac OS X, Windows, and Linux support this standard. However, many USB sound cards do not conform to the standard and require proprietary drivers from the manufacturer.

Even cards meeting the older, slow, USB 1.1 specification are capable of high quality sound with a limited number of channels, or limited sampling frequency or bit depth, but USB 2.0 or later is more capable.

A USB audio interface may also describe a device allowing a computer which has a sound-card, yet lacks a standard audio socket, to be connected to an external device which requires such a socket, via its USB socket.

The main function of a sound card is to play audio, usually music, with varying formats (monophonic, stereophonic, various multiple speaker setups) and degrees of control. The source may be a CD or DVD, a file, streamed audio, or any external source connected to a sound card input.

Audio may be recorded. Sometimes sound card hardware and drivers do not support recording a source that is being played.

A card can also be used, in conjunction with software, to generate arbitrary waveforms, acting as an audio-frequency function generator. Free and commercial software is available for this purpose; there are also online services that generate audio files for any desired waveforms, playable through a sound card.

A card can be used, again in conjunction with free or commercial software, to analyse input waveforms. For example, a very-low-distortion sinewave oscillator can be used as input to equipment under test; the output is sent to a sound card's line input and run through Fourier transform software to find the amplitude of each harmonic of the added distortion. Alternatively, a less pure signal source may be used, with circuitry to subtract the input from the output, attenuated and phase-corrected; the result is distortion and noise only, which can be analysed.

There are programs which allow a sound card to be used as an audio-frequency oscilloscope.

For all measurement purposes a sound card must be chosen with good audio properties. It must itself contribute as little distortion and noise as possible, and attention must be paid to bandwidth and sampling. A typical integrated sound card, the Realtek ALC887, according to its data sheet has distortion of about 80 dB below the fundamental; cards are available with distortion better than -100 dB.

Sound cards with a sampling rate of 192 kHz can be used to synchronize the clock of the computer with a time signal transmitter working on frequencies below 96 kHz like DCF 77 with a special software and a coil at the entrance of the sound card, working as antenna , .

To use a sound card, the operating system (OS) typically requires a specific device driver, a low-level program that handles the data connections between the physical hardware and the operating system. Some operating systems include the drivers for many cards; for cards not so supported, drivers are supplied with the card, or available for download.





</doc>
<doc id="28186" url="https://en.wikipedia.org/wiki?curid=28186" title="Symmetry group">
Symmetry group

In group theory, the symmetry group of a geometric object is the group of all transformations under which the object is invariant, endowed with the group operation of composition. Such a transformation is an invertible mapping of the ambient space which takes the object to itself, and which preserves all the relevant structure of the object. A frequent notation for the symmetry group of an object "X" is "G" = Sym("X").

For an object in a metric space, its symmetries form a subgroup of the isometry group of the ambient space. This article mainly considers symmetry groups in Euclidean geometry, but the concept may also be studied for more general types of geometric structure.

We consider the "objects" possessing symmetry to be geometric figures, images, and patterns, such as a wallpaper pattern. For symmetry of physical objects, one may also take their physical composition as part of the pattern. (A pattern may be specified formally as a scalar field, a function of position with values in a set of colors or substances; as a vector field; or as a more general function on the object.) The group of isometries of space induces a group action on objects in it, and the symmetry group Sym("X") consists of those isometries which map "X" to itself (as well as mapping any further pattern to itself). We say "X" is "invariant" under such a mapping, and the mapping is a "symmetry" of "X".

The above is sometimes called the full symmetry group of "X" to emphasize that it includes orientation-reversing isometries (reflections, glide reflections and improper rotations), as long as those isometries map this particular "X" to itself. The subgroup of orientation-preserving symmetries (translations, rotations, and compositions of these) is called its proper symmetry group. An object is chiral when it has no orientation-reversing symmetries, so that its proper symmetry group is equal to its full symmetry group.

Any symmetry group whose elements have a common fixed point, which is true if the group is finite or the figure is bounded, can be represented as a subgroup of the orthogonal group O("n") by choosing the origin to be a fixed point. The proper symmetry group is then a subgroup of the special orthogonal group SO("n"), and is called the rotation group of the figure.

In a discrete symmetry group, the points symmetric to a given point do not accumulate toward a limit point. That is, every orbit of the group (the images of a given point under all group elements) forms a discrete set. All finite symmetry groups are discrete.

Discrete symmetry groups come in three types: (1) finite point groups, which include only rotations, reflections, inversions and rotoinversions – i.e., the finite subgroups of O("n"); (2) infinite lattice groups, which include only translations; and (3) infinite space groups containing elements of both previous types, and perhaps also extra transformations like screw displacements and glide reflections. There are also continuous symmetry groups (Lie groups), which contain rotations of arbitrarily small angles or translations of arbitrarily small distances. An example is O(3), the symmetry group of a sphere. Symmetry groups of Euclidean objects may be completely classified as the subgroups of the Euclidean group E("n") (the isometry group of R).

Two geometric figures have the same "symmetry type" when their symmetry groups are "conjugate" subgroups of the Euclidean group: that is, when the subgroups "H", "H" are related by for some "g" in E("n"). For example:


In the following sections, we only consider isometry groups whose orbits are topologically closed, including all discrete and continuous isometry groups. However, this excludes for example the 1D group of translations by a rational number; such a non-closed figure cannot be drawn with reasonable accuracy due to its arbitrarily fine detail.

The isometry groups in one dimension are:


See also symmetry groups in one dimension.

Up to conjugacy the discrete point groups in two-dimensional space are the following classes:


C is the trivial group containing only the identity operation, which occurs when the figure is asymmetric, for example the letter "F". C is the symmetry group of the letter "Z", C that of a triskelion, C of a swastika, and C, C, etc. are the symmetry groups of similar swastika-like figures with five, six, etc. arms instead of four.

D is the 2-element group containing the identity operation and a single reflection, which occurs when the figure has only a single axis of bilateral symmetry, for example the letter "A".

D, which is isomorphic to the Klein four-group, is the symmetry group of a non-equilateral rectangle. This figure has four symmetry operations: the identity operation, one twofold axis of rotation, and two nonequivalent mirror planes.

D, D etc. are the symmetry groups of the regular polygons.

Within each of these symmetry types, there are two degrees of freedom for the center of rotation, and in the case of the dihedral groups, one more for the positions of the mirrors.

The remaining isometry groups in two dimensions with a fixed point are:

Non-bounded figures may have isometry groups including translations; these are:

Up to conjugacy the set of three-dimensional point groups consists of 7 infinite series, and 7 other individual groups. In crystallography, only those point groups are considered which preserve some crystal lattice (so their rotations may only have order 1, 2, 3, 4, or 6). This crystallographic restriction of the infinite families of general point groups results in 32 crystallographic point groups (27 individual groups from the 7 series, and 5 of the 7 other individuals).

The continuous symmetry groups with a fixed point include those of:

For objects with scalar field patterns, the cylindrical symmetry implies vertical reflection symmetry as well. However, this is not true for vector field patterns: for example, in cylindrical coordinates with respect to some axis, the vector field 
formula_1 has cylindrical symmetry with respect to the axis whenever formula_2 and formula_3 have this symmetry (no dependence on formula_4); and it has reflectional symmetry only when formula_5.

For spherical symmetry, there is no such distinction: any patterned object has planes of reflection symmetry.

The continuous symmetry groups without a fixed point include those with a screw axis, such as an infinite helix. See also subgroups of the Euclidean group.

In wider contexts, a symmetry group may be any kind of transformation group, or automorphism group. Each type of mathematical structure has invertible mappings which preserve the structure. Conversely, specifying the symmetry group can define the structure, or at least clarify the meaning of geometric congruence or invariance; this is one way of looking at the Erlangen programme.

For example, objects in a hyperbolic non-Euclidean geometry have Fuchsian symmetry groups, which are the discrete subgroups of the isometry group of the hyperbolic plane, preserving hyperbolic rather than Euclidean distance. (Some are depicted in drawings of Escher.) Similarly, automorphism groups of finite geometries preserve families of point-sets (discrete subspaces) rather than Euclidean subspaces, distances, or inner products. Just as for Euclidean figures, objects in any geometric space have symmetry groups which are subgroups of the symmetries of the ambient space.

Another example of a symmetry group is that of a combinatorial graph: a graph symmetry is a permutation of the vertices which takes edges to edges. Any finitely presented group is the symmetry group of its Cayley graph; the free group is the symmetry group of an infinite tree graph.

Cayley's theorem states that any abstract group is a subgroup of the permutations of some set "X", and so can be considered as the symmetry group of "X" with some extra structure. In addition, many abstract features of the group (defined purely in terms of the group operation) can be interpreted in terms of symmetries.

For example, let "G" = Sym("X") be the finite symmetry group of a figure "X" in a Euclidean space, and let "H" ⊂ "G" be a subgroup. Then "H" can be interpreted as the symmetry group of "X", a "decorated" version of "X". Such a decoration may be constructed as follows. Add some patterns such as arrows or colors to "X" so as to break all symmetry, obtaining a figure "X" with Sym("X") = {1}, the trivial subgroup; that is, "gX" ≠ "X" for all non-trivial "g" ∈ "G". Now we get:

Normal subgroups may also be characterized in this framework. 
The symmetry group of the translation "gX" is the conjugate subgroup "gHg". Thus "H" is normal whenver:
that is, whenever the decoration of "X" may be drawn in any orientation, with respect to any side or feature of "X", and still yield the same symmetry group "gHg" = "H".

As an example, consider the dihedral group "G" = "D" = Sym("X"), where "X" is an equilateral triangle. We may decorate this with an arrow on one edge, obtaining an asymmetric figure "X". Letting τ ∈ "G" be the reflection of the arrowed edge, the composite figure "X" = "X" ∪ τ"X" has a bidirectional arrow on that edge, and its symmetry group is "H" = {1, τ}. This subgroup is not normal, since "gX" may have the bi-arrow on a different edge, giving a different reflection symmetry group.

However, letting H = {1, ρ, ρ} ⊂ "D" be the cyclic subgroup generated by a rotation, the decorated figure "X" consists of a 3-cycle of arrows with consistent orientation. Then "H" is normal, since drawing such a cycle with either orientation yields the same symmetry group "H".




</doc>
<doc id="28187" url="https://en.wikipedia.org/wiki?curid=28187" title="Singular they">
Singular they

Singular "they" is the use in English of the pronoun "they" or its inflected or derivative forms, "them", "their", "theirs", and "themselves" (or "themself", as an epicene (gender-neutral) singular pronoun. It typically occurs with an unspecified antecedent, as in sentences such as:

The singular "they" emerged by the 14th century, about a century after the plural "they". It has been commonly employed in everyday English ever since then and has gained currency in official contexts. Singular "they" has been criticised since the mid-18th century by prescriptive commentators who consider it an error. Its continued use in modern standard English has become more common and formally accepted with the move toward gender-neutral language, though many style guides continue to describe it as colloquial and less appropriate in formal writing.

In the early 21st century, use of singular "they" with known individuals emerged for people who do not identify as male or female, as in the following example:


"They" in this context was named "Word of the Year" for 2015 by the American Dialect Society, and for 2019 by Merriam-Webster. In 2020, the American Dialect Society also selected it as "Word of the Decade" for the 2010s.

The "singular "they"" permits a singular antecedent, but is used with the same (plural) verb forms as plural "they", and has the same inflected forms as plural "they" (i.e. "them", "their", and "theirs"), except that in the reflexive form, "themself" is sometimes used instead of "themselves".

"Themself" is attested from the 14th to 16th centuries. Its use has been increasing since the 1970s or 1980s, though it is sometimes still classified as "a minority form". In 2002, Payne and Huddleston, in "The Cambridge Grammar of the English Language", called its use in standard dialect "rare and acceptable only to a minority of speakers" but "likely to increase with the growing acceptance of "they" as a singular pronoun". It is useful when referring to a single person of indeterminate gender, where the plural form "themselves" might seem incongruous, as in:


The Canadian government recommends "themselves" as the reflexive form of singular "they" for use in Canadian federal legislative texts and advises against using "themself", but "themself" is also found:


"They" with a singular antecedent goes back to the Middle English of the 14th century (slightly younger than "they" with a plural antecedent, which was borrowed from Old Norse in the 13th century), and has remained in common use for centuries in spite of its proscription by traditional grammarians beginning in the mid 18th century.

Informal spoken English exhibits nearly universal use of the singular "they". An examination by Jürgen Gerner of the British National Corpus published in 1998 found that British speakers, regardless of social status, age, sex, or region, used the singular "they" overwhelmingly more often than the gender-neutral "he" or other options.

Singular "they" is found in the writings of many respected authors. Here are some examples, arranged chronologically:





Alongside "they", it was acceptable to use the pronoun "he" to refer to an indefinite person of any gender, as in the following:


Such usage is still occasionally found but has lost acceptability in most contexts, due to not being gender-neutral.

The earliest known explicit recommendation by a grammarian to use the generic "he" rather than "they" in formal English is Ann Fisher's mid-18th century "A New Grammar" assertion that "The "Masculine Person" answers to the "general Name", which comprehends both "Male" and "Female"; as, "any Person who knows what he says."" (Ann Fisher as quoted by Ostade)

Nineteenth-century grammarians insisted on "he" as a gender-neutral pronoun on the grounds of number agreement, while rejecting "he or she" as clumsy, and this was widely adopted: e.g. in 1850, the British Parliament passed an act which provided that, when used in acts of Parliament "words importing the masculine gender shall be deemed and taken to include females". Baskervill and Sewell mention the common use of the singular "they" in their "An English Grammar for the Use of High School, Academy and College Class" of 1895, but prefer the generic "he" on the basis of number agreement:

Baskervill gives a number of examples of recognized authors using the singular "they", including:


It has been argued that the real motivation for promoting the "generic" "he" was an androcentric world view, with the default sex of humans being male – and the default gender therefore being masculine. There is some evidence for this: Wilson wrote in 1560:

And Poole wrote in 1646:

In spite of continuous attempts on the part of educationalists to proscribe singular "they" in favour of "he", this advice was largely ignored; even writers of the period continued to use "they" (though the proscription may have been observed more by American writers). Use of the purportedly gender-neutral "he" remained acceptable until at least the 1960s, though some uses of "he" were later criticized as being awkward or silly, for instance when referring to:


"He" is still sometimes found in contemporary writing when referring to a generic or indeterminate antecedent. In some cases it is clear from the situation that the persons potentially referred to are likely to be male, as in:

In some cases the antecedent may refer to persons who are only "probably" male or to occupations traditionally thought of as male:
In other situations, the antecedent may refer to:

In 2010, Choy and Clark still recommend the use of generic "he" "in formal speech or writing":

In 2015, "Fowler's Dictionary of Modern English Usage" calls this "the now outmoded use of "he" to mean 'anyone, stating:
In 2016, "Garner's Modern English" calls the generic use of masculine pronouns "the traditional view, now widely assailed as sexist".

The earliest known attempt to create gender-neutral pronouns dates back to 1792, when Scottish economist James Anderson advocated for an indeterminate pronoun "ou".

In 1808, poet Samuel Taylor Coleridge suggested "it" and "which" as neutral pronouns for the word "Person":

In the second half of the 20th century, people expressed more widespread concern at the use of sexist and male-oriented language. This included criticism of the use of "man" as a generic term to include men and women and of the use of "he" to refer to any human, regardless of sex (social gender).

It was argued that "he" could not sensibly be used as a generic pronoun understood to include men and women. William Safire in his "On Language" column in "The New York Times" approved of the use of generic "he", mentioning the mnemonic phrase "the male embraces the female". C. Badendyck from Brooklyn wrote to the "New York Times" in a reply:

By 1980, the movement toward gender-neutral language had gained wide support, and many organizations, including most publishers, had issued guidelines on the use of gender-neutral language, but stopped short of recommending "they" to be third-person singular with a non-indeterminate, singular antecedent.

The use of masculine generic nouns and pronouns in written and spoken language has decreased since the 1970s.
In a corpus of spontaneous speech collected in Australia in the 1990s, singular "they" had become the most frequently used generic pronoun (rather than generic "he" or "he or she"). Similarly, a study from 2002 looking at a corpus of American and British newspapers showed a preference for "they" to be used as a singular epicene pronoun.

The increased use of singular "they" may owe in part to an increasing desire for gender-neutral language. A solution in formal writing has often been to write ""he or she"", or something similar, but this is often considered awkward or overly politically correct, particularly when used excessively. In 2016, the journal "American Speech" published a study by Darren K. LaScotte investigating the pronouns used by native English speakers in informal written responses to questions concerning a subject of unspecified gender, finding that 68% of study participants chose singular "they" to refer to such an antecedent. Some participants noted that they found constructions such as "he or she" inadequate as they do not include people who do not identify as either male or female.

"They" in this context was named Word of the Year for 2019 by Merriam-Webster and for 2015 by the American Dialect Society. On January 4, 2020, the American Dialect Society announced they had crowned "they", again in this context, Word of the Decade for the 2010s.

The singular antecedent can be a pronoun such as "someone", "anybody", or "everybody", or an interrogative pronoun such as "who":


Although the pronouns "everybody", "everyone", "nobody", and "no one" are singular in form and are used with a singular verb, these pronouns have an "implied plurality" that is somewhat similar to the implied plurality of collective or group nouns such as "crowd" or "team", and in some sentences where the antecedent is one of these "implied plural" pronouns, the word "they" cannot be replaced by generic "he", suggesting a "notional plural" rather than a "bound variable" interpretation . This is in contrast to sentences that involve multiple pairwise relationships and singular "they", such as:


There are examples where the antecedent pronoun (such as "everyone") may refer to a collective, with no necessary implication of pairwise relationships. These are examples of plural "they":

Which are apparent because they do not work with a generic "he" or "he or she":
In addition, for these "notional plural" cases, it would not be appropriate to use "themself" instead of "themselves" as in:

The singular antecedent can also be a noun such as "person", "patient", or "student":


Known individuals may be referred to as "they" if the individual's gender is unknown to the speaker.

A known individual may also be referred to as "they" if the individual is non-binary or genderqueer, regards male or female pronouns as inappropriate, and prefers "they" instead. Several social media applications permit account holders to choose to identify their gender using one of a variety of non-binary or genderqueer options, such as "gender fluid", "agender", or "bigender", and to designate a pronoun, including "they"/"them", which they wish to be used when referring to them. Though "singular "they"" has long been used with antecedents such as "everybody" or generic persons of unknown gender, this use, which may be chosen by an individual, is recent.

The singular "they" in the meaning "gender-neutral singular pronoun for a known person, as a non-binary identifier" was chosen by the American Dialect Society as their "Word of the Year" for 2015. In 2016, the American Dialect Society wrote:
The vote followed the previous year's approval of this use by "The Washington Post" style guide, when Bill Walsh, the "Post"s copy editor, said that the singular "they" is "the only sensible solution to English's lack of a gender-neutral third-person singular personal pronoun".

In 2019, the non-binary "they" was added to Merriam-Webster's dictionary.

The first non-binary main character on North American television appeared on the Showtime drama series "Billions" in 2017, with Asia Kate Dillon playing Taylor Mason. Both actor and character use singular they.

Though both generic "he" and generic "they" have long histories of use, and both are still used, both are also systematically avoided by particular groups.

Style guides that avoid expressing a preference for either approach sometimes recommend recasting a problem sentence, for instance replacing generic expressions with plurals to avoid the criticisms of either party.

The use of singular "they" may be more accepted in British English than in American English, or vice versa.

"Garner's Modern American Usage" (2nd ed., 2003) recommends cautious use of singular "they", and avoidance where possible because its use is stigmatized.
Garner suggests that use of singular "they" is more acceptable in British English:
and apparently regrets the resistance by the American language community:
He regards the trend toward using singular "they" with antecedents like "everybody", "anyone" and "somebody" as inevitable:

In the 14th edition (1993) of "The Chicago Manual of Style", the University of Chicago Press explicitly recommended using singular "they" and "their", noting a "revival" of this usage and citing "its venerable use by such writers as Addison, Austen, Chesterfield, Fielding, Ruskin, Scott, and Shakespeare."
From the 15th edition (2003), this was changed. In Chapter 5 of the 16th edition (2010), now written by Bryan A. Garner, the recommendations are:

and:

According to "The American Heritage Book of English Usage" and its usage panel of selected writers, journalism professors, linguists, and other experts, many Americans avoid use of "they" to refer to a singular antecedent out of respect for a "traditional" grammatical rule, despite use of singular "they" by modern writers of note and mainstream publications:

The 6th edition of the American Psychological Association's "Publication Manual" rejects most use of singular "they" and gives the following example as "incorrect" usage:

However, APA style endorses using "they" if it is someone's (for example, a non-binary person's) preferred pronoun.

The upcoming 7th edition of the APA Publication Manual, scheduled to be released in October 2019, will include guidelines on using singular "they" for bias free writing.

William Strunk Jr. & E. B. White, the original authors of "The Elements of Style", found use of "they" with a singular antecedent unacceptable and advised use of the singular pronoun ("he"). In the 3rd edition (1979), the recommendation was still:

The assessment, in 1979, was:

In the 4th edition (2000), use of singular "they" was still proscribed against, but use of generic "he" was no longer recommended.

Joseph M. Williams, who wrote a number of books on writing with "", discusses the advantages and disadvantages of various solutions when faced with the problem of referring to an antecedent such as "someone", "everyone", "no one" or a noun that does not indicate gender and suggests that this will continue to be a problem for some time. He "suspect[s] that eventually we will accept the plural "they" as a correct singular" but states that currently "formal usage requires a singular pronoun".

According to "The Little, Brown Handbook", most experts – and some teachers and employers – find use of singular "they" unacceptable:
It recommends using "he or she" or avoiding the problem by rewriting the sentence to use a plural or omit the pronoun.

The "Purdue Online Writing Lab" ("OWL") states that "grammar shifts and changes over time", that the use of singular "they" is acceptable, and that singular "they" as a replacement for "he" or "she" is more inclusive:

"The Washington Post"'s stylebook, as of 2015, recommends trying to "write around the problem, perhaps by changing singulars to plurals, before using the singular they as a last resort" and specifically permits use of "they" for a "gender-nonconforming person".

The "Associated Press Stylebook", as of 2017, recommends: "They/them/their is acceptable in limited cases as a singular and-or gender-neutral pronoun, when alternative wording is overly awkward or clumsy. However, rewording usually is possible and always is preferable."

In "The Handbook of Nonsexist Writing", Casey Miller and Kate Swift accept or recommend singular uses of "they" in cases where there is an element of semantic plurality expressed by a word such as "everyone" or where an indeterminate "person" is referred to, citing examples of such usage in formal speech. They also suggest rewriting sentences to use a plural "they", eliminating pronouns, or recasting sentences to use "one" or (for babies) "it".

In the first edition of "A Dictionary of Modern English Usage" (published in 1926) use of the generic "he" is recommended. It is stated that singular "they" is disapproved of by grammarians. Numerous examples of its use by eminent writers in the past are given, but it is stated that "few good modern writers would flout [grammarians] so conspicuously as Fielding and Thackeray", whose sentences are described as having an "old-fashioned sound".
The second edition, "Fowler's Modern English Usage" (edited by Sir Ernest Gowers and published in 1965) continues to recommend use of the generic "he"; use of the singular "they" is called "the popular solution", which "sets the literary man's teeth on edge". It is stated that singular "they" is disapproved of by grammarians but common in colloquial speech. Numerous examples of its use by eminent writers are given, but it is stated that "few good modern writers would flout [grammarians] so conspicuously as Fielding and Thackeray".
According to the third edition, "The New Fowler's Modern English Usage" (edited by Burchfield and published in 1996) singular "they" has not only been widely used by good writers for centuries, but is now generally accepted, except by some conservative grammarians, including the Fowler of 1926, who, it is argued, ignored the evidence:

"The Complete Plain Words" was originally written in 1948 by Ernest Gowers, a civil servant, in an attempt by the British civil service to improve "official English". A second edition, edited by Sir Bruce Fraser, was published in 1973. It refers to "they" or "them" as the "equivalent of a singular pronoun of common sex" as "common in speech and not unknown in serious writing " but "stigmatized by grammarians as usage grammatically indefensible. The book's advice for "official writers" (civil servants) is to avoid its use and not to be tempted by its "greater convenience", though "necessity may eventually force it into the category of accepted idiom".
A new edition of "Plain Words", revised and updated by Gowers's great granddaughter, Rebecca Gowers, was published in 2014.
It notes that singular "they" and "them" have become much more widespread since Gowers' original comments, but still finds it "safer" to treat a sentence like 'The reader may toss their book aside' as incorrect "in formal English", while rejecting even more strongly sentences like
"The Times Style and Usage Guide" (first published in 2003 by "The Times" of London) recommends avoiding sentences like
by using a plural construction:
"The Cambridge Guide to English Usage" (2004, Cambridge University Press) finds singular "they" "unremarkable":
It expresses several preferences.
"The Economist Style Guide" refers to the use of "they" in sentences like
as "scrambled syntax that people adopt because they cannot bring themselves to use a singular pronoun".
"New Hart's Rules" (Oxford University Press, 2012) is aimed at those engaged in copy editing, and the emphasis is on the formal elements of presentation including punctuation and typeface, rather than on linguistic style, although – like "The Chicago Manual of Style" – it makes occasional forays into matters of usage. It advises against use of the purportedly gender-neutral "he", and suggests cautious use of "they" where "he or she" presents problems.

The 2011 edition of the "New International Version Bible" uses singular "they" instead of the traditional "he" when translating pronouns that apply to both genders in the original Greek or Hebrew. This decision was based on research by a commission that studied modern English usage and determined that singular "they" ("them"/"their") was by far the most common way that English-language speakers and writers today refer back to singular antecedents such as "whoever", "anyone", "somebody", "a person", "no one", and the like."

The British edition of "The Handbook of Nonsexist Writing", modified in some respects from the original US edition to conform to differences in culture and vocabulary, preserved the same recommendations, allowing singular "they" with semantically plural terms like "everyone" and indeterminate ones like "person", but recommending a rewrite to avoid.

The Australian "Federation Press Style Guide for Use in Preparation of Book Manuscripts" recommends "gender-neutral language should be used", stating that use of "they" and "their" as singular pronouns is acceptable.

According to "A Comprehensive Grammar of the English Language" (1985):

"The Cambridge Grammar of the English Language" discusses the prescriptivist argument that "they" is a plural pronoun and that the use of "they" with a singular "antecedent" therefore violates the rule of agreement between antecedent and pronoun, but takes the view that "they", though "primarily" plural, can also be singular in a secondary "extended" sense, comparable to the purportedly extended sense of "he" to include female gender.

Use of singular "they" is stated to be "particularly common", even "stylistically neutral" with antecedents such as "everyone", "someone", and "no one", but more restricted when referring to common nouns as antecedents, as in

Use of the pronoun "themself" is described as being "rare" and "acceptable only to a minority of speakers", while use of the morphologically plural "themselves" is considered problematic when referring to "someone" rather than "everyone" (since only the latter implies a plural set).

There are also issues of grammatical acceptability when reflexive pronouns refer to singular noun phrases joined by "or", the following all being problematic:
On the motivation for using singular "they", "A Student's Introduction to English Grammar" states:

The alternative "he or she" can be "far too cumbersome", as in:

or even "flatly ungrammatical", as in

"Among younger speakers", use of singular "they" even with definite noun-phrase antecedents finds increasing acceptance, "sidestepping any presumption about the sex of the person referred to", as in:


One explanation given for some uses of "they" referring to a singular antecedent is "notional agreement", when the antecedent is seen as semantically plural:


In other words, in the Shakespeare quotation "a mother" is syntactically singular, but stands for all mothers; and in the Shaw quotation, "no man" is syntactically singular (demonstrated by taking the singular form "goes"), but is semantically plural ("all" go [to kill] not to be killed), hence idiomatically requiring "they". Such use, which goes back a long way, includes examples where the sex is known, as in the above examples.

Distributive constructions apply a "single" idea to "multiple" members of a group.
They are typically marked in English by words like "each", "every" and "any". The simplest examples are applied to groups of two, and use words like "either" and "or" – "Would you like tea or coffee?". Since distributive constructions apply an idea relevant to each individual in the group, rather than to the group as a whole, they are most often conceived of as singular, and a singular pronoun is used:


However, many languages, including English, show ambivalence in this regard. Because distribution also requires a group with more than one member, plural forms are sometimes used.

The singular "they", which uses the same verb form that plurals do, is typically used to refer to an indeterminate antecedent, for example:


In some sentences, typically those including words like "every" or "any", the morphologically singular antecedent does not refer to a single entity but is "anaphorically linked" to the associated pronoun to indicate a set of pairwise relationships, as in the sentence:


Linguists like Steven Pinker and Rodney Huddleston explain sentences like this (and others) in terms of bound variables, a term borrowed from logic. Pinker prefers the terms "quantifier" and "bound variable" to "antecedent" and " pronoun". He suggests that pronouns used as "variables" in this way are more appropriately regarded as homonyms of the equivalent referential pronouns.

The following shows different types of anaphoric reference, using various pronouns, including "they":


A study of whether "singular "they"" is more "difficult" to understand than gendered pronouns ("In Search of Gender Neutrality: Is Singular "They" a Cognitively Efficient Substitute for Generic "He"?" by Foertsch and Gernsbacher) found that "singular "they" is a cognitively efficient substitute for generic "he" or "she", particularly when the antecedent is nonreferential" (e.g. "anybody", "a nurse", or "a truck driver") rather than referring to a specific person (e.g. "a runner I knew" or "my nurse"). Clauses with singular "they" were read "just as quickly as clauses containing a gendered pronoun that matched the stereotype of the antecedent" (e.g. "she" for a nurse and "he" for a truck driver) and "much more quickly than clauses containing a gendered pronoun that went against the gender stereotype of the antecedent".

On the other hand, when the pronoun "they" was used to refer to known individuals ("referential antecedents, for which the gender was presumably known", e.g. "my nurse", "that truck driver", "a runner I knew"), reading was slowed when compared with use of a gendered pronoun consistent with the "stereotypic gender" (e.g. "he" for a specific truck driver).

The study concluded, that "the increased use of singular "they" is not problematic for the majority of readers".

The singular and plural use of "they" can be compared with the pronoun "you", which had been both a plural and polite singular, but by about 1700 replaced "thou" for singular referents. For "you", the singular reflexive pronoun ("yourself") is different from its plural reflexive pronoun ("yourselves"); with "they" one can hear either "themself" or "themselves" for the singular reflexive pronoun.

Singular "they" has also been compared to "royal we" (also termed "editorial we"), when a single person uses first-person plural in place of first-person singular pronouns. Similar to singular "you", its singular reflexive pronoun ("ourself") is different from the plural reflexive pronoun ("ourselves").

While the pronoun "it", which is used for inanimate objects, can be used for infants of unspecified gender, it tends to be dehumanizing, and is therefore more likely in a clinical context. In a more personal context, the use of "it" to refer to a person might indicate antipathy or other negative emotions.

"It" can also be used for non-human animals of unspecified sex, though "they" is common for pets and other domesticated animals of unspecified sex, especially when referred to by a proper name (e.g. "Rags", "Snuggles"). Normally, vertebrate birds and mammals with a known sex are referred to by their respective male or female pronoun ("he" and "she"; "him" and "her").

It is uncommon to use singular "they" instead of "it" for something other than a life form.


Sources of original examples





</doc>
<doc id="28189" url="https://en.wikipedia.org/wiki?curid=28189" title="Space Shuttle">
Space Shuttle

The Space Shuttle was a partially reusable low Earth orbital spacecraft system that was operated from 1981 to 2011 by the National Aeronautics and Space Administration (NASA) as part of the Space Shuttle program. Its official program name was Space Transportation System (STS), taken from a 1969 plan for a system of reusable spacecraft of which it was the only item funded for development. The first of four orbital test flights occurred in 1981, leading to operational flights beginning in 1982. Five complete Space Shuttle orbiter vehicles were built and flown on a total of 135 missions from 1981 to 2011, launched from the Kennedy Space Center (KSC) in Florida. Operational missions launched numerous satellites, Interplanetary probes, and the Hubble Space Telescope (HST); conducted science experiments in orbit; and participated in construction and servicing of the International Space Station. The Space Shuttle fleet's total mission time was 1322 days, 19 hours, 21 minutes and 23 seconds.

Space Shuttle components include the Orbiter Vehicle (OV) with three clustered Rocketdyne RS-25 main engines, a pair of recoverable solid rocket boosters (SRBs), and the expendable external tank (ET) containing liquid hydrogen and liquid oxygen. The Space Shuttle was launched vertically, like a conventional rocket, with the two SRBs operating in parallel with the orbiter's three main engines, which were fueled from the ET. The SRBs were jettisoned before the vehicle reached orbit, and the ET was jettisoned just before orbit insertion, which used the orbiter's two Orbital Maneuvering System (OMS) engines. At the conclusion of the mission, the orbiter fired its OMS to deorbit and reenter the atmosphere. The orbiter was protected during reentry by its thermal protection system tiles, and it glided as a spaceplane to a runway landing, usually to the Shuttle Landing Facility at KSC, Florida, or to Rogers Dry Lake in Edwards Air Force Base, California. If the landing occurred at Edwards, the orbiter was flown back to the KSC on the Shuttle Carrier Aircraft, a specially modified Boeing 747.

The first orbiter, "Enterprise", was built in 1976, used in Approach and Landing Tests and has no orbital capability. Four fully operational orbiters were initially built: "Columbia", "Challenger", "Discovery", and "Atlantis". Of these, two were lost in mission accidents: "Challenger" in 1986 and "Columbia" in 2003, with a total of fourteen astronauts killed. A fifth operational (and sixth in total) orbiter, "Endeavour", was built in 1991 to replace "Challenger". The Space Shuttle was retired from service upon the conclusion of "Atlantis"s final flight on July 21, 2011. The U.S. relied on the Russian Soyuz spacecraft to transport astronauts to the International Space Station from the last Shuttle flight until the first Commercial Crew Development launch on May 30, 2020.

During the 1950s, the United States Air Force proposed using a reusable piloted glider to perform military operations such as reconnaissance, satellite attack, and air-to-ground weapons employment. In the late 1950s, the Air Force began developing the partially reusable X-20 Dyna-Soar. The Air Force collaborated with NASA on the Dyna-Soar, and began training six pilots in June 1961. The rising costs of development and the prioritization of Project Gemini led to the cancellation of the Dyna-Soar program in December 1963. In addition to the Dyna-Soar, the Air Force had conducted a study in 1957 to test the feasibility of reusable boosters. This became the basis for the aerospaceplane, a fully reusable spacecraft that was never developed beyond the initial design phase in 1962–1963.

Beginning in the early 1950s, NASA and the Air Force collaborated on developing lifting bodies to test aircraft that primarily generated lift from their fuselages instead of wings, and tested the M2-F1, M2-F2, M2-F3, HL-10, X-24A, and the X-24B. The program tested aerodynamic characteristics that would later be incorporated in design of the Space Shuttle, including unpowered landing from a high altitude and speed.

In September 1966, NASA and the Air Force released a joint study concluding that a new vehicle was required to satisfy their respective future demands, and that a partially reusable system would be the most cost-effective solution. The head of the NASA Office of Manned Space Flight, George Mueller, announced the plan for a reusable shuttle on August 10, 1968. NASA issued a request for proposal (RFP) for designs of the Integrated Launch and Re-entry Vehicle (ILRV), which would later become the Space Shuttle. Rather than award a contract based upon initial proposals, NASA announced a phased approach for the Space Shuttle contracting and development; Phase A was a request for studies completed by competing aerospace companies, Phase B was a competition between two contractors for a specific contract, Phase C involved designing the details of the spacecraft components, and Phase D was the production of the spacecraft.

In December 1968, NASA created the Space Shuttle Task Group to determine the optimal design for a reusable spacecraft, and issued study contracts to General Dynamics, Lockheed, McDonnell Douglas, and North American Rockwell. In July 1969, the Space Shuttle Task Group issued a report that determined the Shuttle would support short-duration crewed missions and space station, as well as the capabilities to launch, service, and retrieve satellites. The report also created three classes of a future reusable shuttle: Class I would have a reusable orbiter mounted on expendable boosters, Class II would use multiple expendable rocket engines and a single propellant tank (stage-and-a-half), and Class III would have both a reusable orbiter and a reusable booster. In September 1969, the Space Task Group, under leadership of Vice President Spiro Agnew, issued a report calling for the development of a space shuttle to bring people and cargo to low Earth orbit (LEO), as well as a space tug for transfers between orbits and the Moon, and a reusable nuclear upper stage for deep space travel.

After the release of the Space Shuttle Task Group report, many aerospace engineers favored the Class III, fully reusable design because of perceived savings in hardware costs. Max Faget, a NASA engineer who had worked to design the Mercury capsule, patented a design for a two-stage fully recoverable system with a straight-winged orbiter mounted on a larger straight-winged booster. The Air Force Flight Dynamics Laboratory argued that a straight-wing design would not be able to withstand the high thermal and aerodynamic stresses during reentry, and would not provide the required cross-range capability. Additionally, the Air Force required a larger payload capacity than Faget's design allowed. In January 1971, NASA and Air Force leadership decided that a reusable delta-wing orbiter mounted on an expendable propellant tank would be the optimal design for the Space Shuttle.

After they established the need for a reusable, heavy-lift spacecraft, NASA and the Air Force determined the design requirements of their respective services. The Air Force expected to use the Space Shuttle to launch large satellites, and required it to be capable of lifting to an eastward LEO or into a polar orbit. The satellite designs also required that the Space Shuttle have a payload bay. NASA evaluated the F-1 and J-2 engines from the Saturn rockets, and determined that they were insufficient for the requirements of the Space Shuttle; in July 1971, it issued a contract to Rocketdyne to begin development on the RS-25 engine.

NASA reviewed 29 potential designs for the Space Shuttle, and determined that a design with two side boosters should be used, and the boosters should be reusable to reduce costs. NASA and the Air Force elected to use solid-propellant boosters because of the lower costs and the ease of refurbishing them for reuse after they landed in the ocean. In January 1972, President Richard Nixon approved the Shuttle, and NASA decided on its final design in March. That August, NASA awarded the contract to build the orbiter to North American Rockwell, the solid-rocket booster contract to Morton Thiokol, and the external tank contract to Martin Marietta.

On June 4, 1974, Rockwell began construction on the first orbiter, OV-101, which would later be named "Enterprise". "Enterprise" was designed as a test vehicle, and did not include engines or heat shielding. Construction was completed on September 17, 1976, and "Enterprise" was moved to the Edwards Air Force Base to begin testing. Rockwell constructed the Main Propulsion Test Article (MPTA)-098, which was a structural truss mounted to the ET with three RS-25 engines attached. It was tested at the National Space Technology Laboratory (NSTL) to ensure that the engines could safely run through the launch profile. Rockwell conducted mechanical and thermal stress tests on Structural Test Article (STA)-099 to determine the effects of aerodynamic and thermal stresses during launch and reentry.

The beginning of the development of the RS-25 Space Shuttle Main Engine was delayed for nine months while Pratt & Whitney challenged the contract that had been issued to Rocketdyne. The first engine was completed in March 1975, after issues with developing the first throttleable, reusable engine. During engine testing, the RS-25 experienced multiple nozzle failures, as well as broken turbine blades. Despite the problems during testing, NASA ordered the nine RS-25 engines needed for its three orbiters under construction in May 1978.

NASA experienced significant delays in the development of the Space Shuttle's thermal protection system. Previous NASA spacecraft had used ablative heat shields, but those could not be reused. NASA chose to use ceramic tiles for thermal protection, as the shuttle could then be constructed of lightweight aluminum, and the tiles could be individually replaced as needed. Construction began on "Columbia" on March 27, 1975, and it was delivered to the KSC on March 25, 1979. At the time of its arrival at the KSC, "Columbia" still had 6,000 of its 30,000 tiles remaining to be installed. However, many of the tiles that had been originally installed had to be replaced, requiring two years of installation before "Columbia" could fly.

On January 5, 1979, NASA commissioned a second orbiter. Later that month, Rockwell began converting STA-099 to OV-099, later named "Challenger". On January 29, 1979, NASA ordered two additional orbiters, OV-103 and OV-104, which were named "Discovery" and "Atlantis". Construction of OV-105, later named "Endeavour", began in February 1982, but NASA decided to limit the Space Shuttle fleet to four orbiters in 1983. After the loss of "Challenger", NASA resumed production of "Endeavour" in September 1987.

After it arrived at Edwards AFB, "Enterprise" underwent flight testing with the Shuttle Carrier Aircraft, a Boeing 747 that had been modified to carry the orbiter. In February 1977, "Enterprise" began the Approach and Landing Tests and underwent captive flights, where it remained attached to the Shuttle Carrier Aircraft for the duration of the flight. On August 12, 1977, "Enterprise" conducted its first glide test, where it detached from the Shuttle Carrier Aircraft and landed at Edwards AFB. After four additional flights, "Enterprise" was moved to the Marshall Space Flight Center (MSFC) on March 13, 1978. "Enterprise" underwent shake tests in the Mated Vertical Ground Vibration Test, where it was attached to an external tank and solid rocket boosters, and underwent vibrations to simulate the stresses of launch. In April 1979, "Enterprise" was taken to the KSC, where it was attached to an external tank and solid rocket boosters, and moved to LC-39. Once installed at the launch pad, the Space Shuttle was used to verify the proper positioning of launch complex hardware. "Enterprise" was taken back to California in August 1979, and later served in the development of the SLC-6 at Vandenberg AFB in 1984.

On November 24, 1980, "Columbia" was mated with its external tank and solid-rocket boosters, and was moved to LC-39 on December 29. The first Space Shuttle mission, STS-1, would be the first time NASA performed a crewed first-flight of a spacecraft. On April 12, 1981, the Space Shuttle launched for the first time, and was piloted by John Young and Robert Crippen. During the two-day mission, Young and Crippen tested equipment on board the shuttle, and found several of the ceramic tiles had fallen off the top side of the "Columbia". NASA coordinated with the Air Force to use satellites to image the underside of "Columbia", and determined there was no damage. "Columbia" reentered the atmosphere and landed at Edwards AFB on April 14.

NASA conducted three additional test flights with "Columbia" in 1981 and 1982. On July 4, 1982, STS-4, flown by Ken Mattingly and Henry Hartsfield, landed on a concrete runway at Edwards AFB. President Ronald Reagan and his wife Nancy met the crew, and delivered a speech. After STS-4, NASA declared its Space Transportation System (STS) operational.

The Space Shuttle was the first operational orbital spacecraft designed for reuse. Each Space Shuttle orbiter was designed for a projected lifespan of 100 launches or ten years of operational life, although this was later extended. At launch, it consisted of the orbiter, which contained the crew and payload, the external tank (ET), and the two solid rocket boosters (SRBs).

Responsibility for the Shuttle components was spread among multiple NASA field centers. The KSC was responsible for launch, landing and turnaround operations for equatorial orbits (the only orbit profile actually used in the program), the U.S. Air Force at the Vandenberg Air Force Base was responsible for launch, landing and turnaround operations for polar orbits (though this was never used), the Johnson Space Center (JSC) served as the central point for all Shuttle operations, the MSFC was responsible for the main engines, external tank, and solid rocket boosters, the John C. Stennis Space Center handled main engine testing, and the Goddard Space Flight Center managed the global tracking network.

The orbiter had design elements and capabilities of both a rocket and an aircraft to allow it to launch vertically and then land as a glider. Its three-part fuselage provided support for the crew compartment, cargo bay, flight surfaces, and engines. The rear of the orbiter contained the Space Shuttle Main Engines (SSME), which provided thrust during launch, as well as the Orbital Maneuvering System (OMS), which allowed the orbiter to achieve, alter, and exit its orbit once in space. Its double-delta wings were long, and were swept 81° at the inner leading edge and 45° at the outer leading edge. Each wing had an inboard and outboard elevon to provide flight control during reentry, along with a flap located between the wings, below the engines to control pitch. The orbiter's vertical stabilizer was swept backwards at 45°, and contained a rudder that could split to act as a speed brake. The vertical stabilizer also contained a two-part drag parachute system to slow the orbiter after landing. The orbiter used retractable landing gear with a nose landing gear and two main landing gear, each containing two tires. The main landing gear contained two brake assemblies each, and the nose landing gear contained an electro-hydraulic steering mechanism.

The Space Shuttle crew varied by mission. The test flights only had two members each, the commander and pilot, who were both qualified pilots that could fly and land the orbiter. The on-orbit operations, such as experiments, payload deployment, and EVAs, were conducted primarily by the mission specialists who were specifically trained for their intended missions and systems. Early in the Space Shuttle program, NASA flew with payload specialists, who were typically systems specialists who worked for the company paying for the payload's deployment or operations. The final payload specialist, Gregory B. Jarvis, flew on STS-51L, and future non-pilots were designated as mission specialists. An astronaut flew as a crewed spaceflight engineer on both STS-51C and STS-51J to serve as a military representative for a National Reconnaissance Office payload. A Space Shuttle crew typically had seven astronauts, with STS-61A flying with eight.

The crew compartment comprised three decks, and was the pressurized, habitable area on all Space Shuttle missions. The cockpit consisted of two seats for the commander and pilot, as well as an additional two to four seats for crew members. The mid-deck was located below the cockpit, and was where the galley and crew bunks were set up, as well as three or four crew member seats. The mid-deck contained the airlock, which could support two astronauts on an extravehicular activity (EVA), as well as access to pressurized research modules. An equipment bay was below the mid-deck, which stored environmental control and waste management systems.

On the first four Shuttle missions, astronauts wore modified U.S. Air Force high-altitude full-pressure suits, which included a full-pressure helmet during ascent and descent. From the fifth flight, STS-5, until the loss of "Challenger", the crew wore one-piece light blue nomex flight suits and partial-pressure helmets. After the "Challenger" disaster, the crew members wore the Launch Entry Suit (LES), a partial-pressure version of the high-altitude pressure suits with a helmet. In 1994, the LES was replaced by the full-pressure Advanced Crew Escape Suit (ACES), which improved the safety of the astronauts in an emergency situation. "Columbia" originally had modified SR-71 zero-zero ejection seats installed for the ALT and first four missions, but these were disabled after STS-4 and removed after STS-9.
The flight deck was the top level of the crew compartment, and contained the flight controls for the orbiter. The commander sat in the front left seat, and the pilot sat in the front right seat, with two to four additional seats set up for additional crew members. The instrument panels contained over 2,100 displays and controls, and the commander and pilot were both equipped with a heads-up display (HUD) and a Rotational Hand Controller (RHC) to gimbal the engines during powered flight and fly the orbiter during unpowered flight. Both seats also had rudder controls, to allow rudder movement in flight and nose-wheel steering on the ground. The orbiter vehicles were originally installed with the Multifunction CRT Display System (MCDS) to display and control flight information. The MCDS displayed the flight information at the commander and pilot seats, as well as at the aft seating location, and also controlled the data on the HUD. In 1998, "Atlantis" was upgraded with the Multifunction Electronic Display System (MEDS), which was a glass cockpit upgrade to the flight instruments that replaced the eight MCDS display units with 11 multifunction colored digital screens. MEDS was flown for the first time in May 2000 on STS-98, and the other orbiter vehicles were upgraded to it. The aft section of the flight decked contained windows looking into the payload bay, as well as an RHC to control the Remote Manipulator System during cargo operations. Additionally, the aft flight deck had monitors for a closed-circuit television to view the cargo bay.

The mid-deck contained the crew equipment storage, sleeping area, galley, medical equipment, and hygiene stations for the crew. The crew used modular lockers to store equipment that could be scaled depending on their needs, as well as permanently installed floor compartments. The mid-deck contained a port-side hatch that crew used for entry and exit while on Earth. Additionally, each orbiter was originally installed with an internal airlock in the mid-deck. The internal airlock was replaced with an external airlock in the payload bay on "Discovery", "Atlantis", and "Endeavour" to improve docking with Mir and the ISS, along with the Orbiter Docking System.

The orbiter was equipped with an avionics system to provide information and control during atmospheric flight. Its avionics suite contained three microwave scanning beam landing systems, three gyroscopes, three TACANs, three accelerometers, two radar altimeters, two barometric altimeters, three attitude indicators, two Mach indicators, and two ModeC transponders. During reentry, the crew deployed two air data probes once they were travelling slower than Mach 5. The orbiter had three inertial measuring units (IMU) that it used for guidance and navigation during all phases of flight. The orbiter contains two star trackers to align the IMUs while in orbit. The star trackers are deployed while in orbit, and can automatically or manually align on a star. In 1991, NASA began upgrading the inertial measurement units with an inertial navigation system (INS), which provided more accurate location information. In 1993, NASA flew a GPS receiver for the first time aboard STS-51. In 1997, Honeywell began developing an integrated GPS/INS to replace the IMU, INS, and TACAN systems, which first flew on STS-118 in August 2007

While in orbit, the crew primarily communicated using one of four S band radios, which provided both voice and data communications. Two of the Sband radios were phase modulation transceivers, and could transmit and receive information. The other two Sband radios were frequency modulation transmitters, and were used to transmit data to NASA. As Sband radios can operate only within their line of sight, NASA used the Tracking and Data Relay Satellite System and the Spacecraft Tracking and Data Acquisition Network ground stations to communicate with the orbiter throughout its orbit. Additionally, the orbiter deployed a high-bandwidth Kband radio out of the cargo bay, which could also utilized as a rendezvous radar. The orbiter was also equipped with two UHF radios for communications with air traffic control and astronauts conducting EVA.
The Space Shuttle's fly-by-wire control system was entirely reliant on its main computer, the Data Processing System (DPS). The DPS controlled the flight controls and thrusters on the orbiter, as well as the ET and SRBs during launch. The DPS consisted of five general purpose computers (GPC), two magnetic tape mass memory units (MMUs), and the associated sensors to monitors the Space Shuttle components. The original GPC used was the IBM AP-101B, which used a separate central processing unit (CPU) and input/output processor (IOP), and non-volatile solid-state memory. From 1991 to 1993, the orbiter vehicles were upgraded to the AP-101S, which improved the memory and processing capabilities, and reduced the volume and weight of the computers by combining the CPU and IOP into a single unit. Four of the GPCs were loaded with the Primary Avionics Software System (PASS), which was Space Shuttle-specific software that provided control through all phases of flight. During ascent, maneuvering, reentry, and landing, the four PASS GPCs functioned identically to produce quadruple redundancy, and would error check their results. In case of a software error that would cause erroneous reports from the four PASS GPCs, a fifth GPC ran the Backup Flight System, which used a different program and could control the Space Shuttle through ascent, orbit, and reentry, but could not support an entire mission. The five GPCs were separated in three separate bays within the mid-deck to provide redundancy in the event of a cooling fan failure. After achieving orbit, the crew would switch some of the GPCs functions from guidance, navigation, and control (GNC) to systems management (SM) and payload (PL) to support the operational mission. The Space Shuttle was not launched if its flight would run from December to January, as its flight software would have required the orbiter vehicle's computers to be reset at the year change. In 2007, NASA engineers devised a solution so Space Shuttle flights could cross the year-end boundary.

Space Shuttle missions typically brought a portable general support computer (PGSC) that could integrate with the orbiter vehicle's computers and communication suite, as well as monitor scientific and payload data. Early missions brought the Grid Compass, one of the first laptop computers, as the PGSC, but later missions brought Apple and Intel laptops.

The payload bay comprised most of the orbiter vehicle's fuselage, and provided the cargo-carrying space for the Space Shuttle's payloads. It was long and wide, and could accommodate cylindrical payloads up to in diameter. Two payload bay doors hinged on either side of the bay, and provided a relatively airtight seal to protect payloads from heating during launch and reentry. Payloads were secured in the payload bay to the attachment points on the longerons. The payload bay doors served an additional function as radiators for the orbiter vehicle's heat, and were opened upon reaching orbit for heat rejection.

The orbiter could be used in conjunction with a variety of add-on components depending on the mission. This included orbital laboratories, boosters for launching payloads farther into space, the Remote Manipulator System (RMS), and to extend the mission duration. To limit the fuel consumption while the orbiter was docked at the ISS, the Station-to-Shuttle Power Transfer System (SSPTS) was developed to convert and transfer station power to the orbiter. The SSPTS was first used on STS-118, and was installed on "Discovery" and "Endeavour".

The Remote Manipulator System (RMS), also known as Canadarm, was a mechanical arm attached to the cargo bay. It could be used to grasp and manipulate payloads, as well as serve as a mobile platform for astronauts conducting an EVA. The RMS was built by the Canadian company Spar Aerospace, and was controlled by an astronaut inside the orbiter's flight deck using their windows and closed-circuit television. The RMS allowed for six degrees of freedom, and had six joints located at three points along the arm. The original RMS could deploy or retrieve payloads up to , which was later improved to .

The Spacelab module was a European-funded pressurized laboratory that was carried within the payload bay and allowed for scientific research while in orbit. The Spacelab module contained two segments that were mounted in the aft end of the payload bay to maintain the center of gravity during flight. Astronauts entered the Spacelab module through a or tunnel that connected to the airlock. The Spacelab equipment was primarily stored in pallets, which provided storage for both experiments as well as computer and power equipment. Spacelab hardware was flown on 28 missions through 1999, and studied subjects including astronomy, microgravity, radar, and life sciences. Spacelab hardware also supported missions such as Hubble Space Telescope (HST) servicing and space station resupply. The Spacelab module was tested STS-2 and STS-3, and the first full mission was on STS-9.

Three RS-25 engines, also known as the Space Shuttle Main Engines (SSME), were mounted on the orbiter's aft fuselage in a triangular pattern. The engine nozzles could gimbal ±10.5° in pitch, and ±8.5° in yaw during ascent to change the direction of their thrust to steer the Shuttle. The titanium alloy reusable engines were independent from the orbiter vehicle, and would be removed and replaced in between flights. The RS-25 is a staged-combustion cycle cryogenic engine that used liquid oxygen and hydrogen, and had a higher chamber pressure than any previous liquid rocket. The original main combustion chamber operated at a maximum pressure of . The engine nozzle is tall and has an interior diameter of . The nozzle is cooled by 1,080 interior lines carrying liquid hydrogen, and is thermally protected by insulative and ablative material.

The RS-25 engines had several improvements to enhance reliability and power. During the development program, Rocketdyne determined that the engine was capable of safe reliable operation at 104% of the originally specified thrust. To keep the engine thrust values consistent with previous documentation and software, NASA kept the original specified thrust as 100%, but had the RS-25 operate at higher thrust. RS-25 upgrade versions were denoted as Block I and Block II. 109% thrust level was achieved with the Block II engines in 2001, which reduced the chamber pressure to , as it had a larger throat area. The normal maximum throttle was 104 percent, with 106% or 109% used for mission aborts.

The Orbital Maneuvering System (OMS) consisted of two aft-mounted AJ10-190 engines and the associated propellant tanks. The AJ10 engines used monomethylhydrazine (MMH) oxidized by dinitrogen tetroxide (NO). The pods carried a maximum of of MMH and of NO. The OMS engines were used after main engine cut-off (MECO) for orbital insertion. Throughout the flight, they were used for orbit changes, as well as the deorbit burn prior to reentry. Each OMS engine produced of thrust, and the entire system could provide of velocity change.

The orbiter was protected from heat during reentry by the thermal protection system (TPS), a thermal soaking protective layer around the orbiter. In contrast with previous US spacecraft, which had used ablative heat shields, the reusability of the orbiter required a multi-use heat shield. During reentry, the TPS experienced temperatures up to , but had to keep the orbiter vehicle's aluminum skin temperature below . The TPS primarily consisted of four types of tiles. The nose cone and leading edges of the wings experienced temperatures above , and were protected by reinforced carbon-carbon tiles (RCC). Thicker RCC tiles were developed and installed in 1998 to prevent damage from micrometeoroid and orbital debris, and were further improved after RCC damage caused in the "Columbia" disaster. Beginning with STS-114, the orbiter vehicles were equipped with the wing leading edge impact detection system to alert the crew to any potential damage. The entire underside of the orbiter vehicle, as well as the other hottest surfaces, were protected with high-temperature reusable surface insulation. Areas on the upper parts of the orbiter vehicle were coated in a white low-temperature reusable surface insulation, which provided protection for temperatures below . The payload bay doors and parts of the upper wing surfaces were coated in reusable felt surface insulation, as the temperature there remained below .

The Space Shuttle external tank (ET) carried the propellant for the Space Shuttle Main Engines, and connected the orbiter vehicle with the solid rocket boosters. The ET was tall and in diameter, and contained separate tanks for liquid oxygen (LOX) and liquid hydrogen (LH). The LOX tank was housed in the nose of the ET, and was tall. The LH comprised the bulk of the ET, and was tall. The orbiter vehicle was attached to the ET at two umbilical plates, which contained five propellant and two electrical umbilicals, and forward and aft structural attachments. The exterior of the ET was covered in orange spray-on foam to allow it to survive the heat of ascent.

The ET provided propellant to the Space Shuttle Main Engines from liftoff until main engine cutoff. The ET separated from the orbiter vehicle 18 seconds after engine cutoff, and could be triggered automatically or manually. At the time of separation, the orbiter vehicle retracted its umbilical plates, and the umbilical cords were sealed to prevent excess propellant from venting into the orbiter vehicle. After the bolts attached at the structural attachments were sheared, the ET separated from the orbiter vehicle. At the time of separation, gaseous oxygen was vented from the nose to cause the ET to tumble, ensuring that it would break up upon reentry. The ET was the only major component of the Space Shuttle system that was not reused, and it would travel along a ballistic trajectory into the Indian or Pacific Ocean.

For the first two missions, STS-1 and STS-2, the ET was covered in of white fire-retardant latex paint to provide protection against damage from ultraviolet radiation. Further research determined that the foam itself was sufficiently protected, and the ET was no longer covered in latex paint beginning on STS-3. A light-weight tank (LWT) was first flown on STS-6, which reduced tank weight by . The LWT's weight was reduced by removing components from the LH tank and reducing the thickness of some skin panels. In 1998, a super light-weight ET (SLWT) first flew on STS-91. The SLWT used the 2195 aluminum-lithium alloy, which was 40% stronger and 10% less dense than its predecessor, 2219 aluminum-lithium alloy. The SLWT weighed less than the LWT, which allowed the Space Shuttle to deliver heavy elements to ISS's high inclination orbit.

The Solid Rocket Boosters (SRB) provided 71.4% of the Space Shuttle's thrust during liftoff and ascent, and were the largest solid-propellant motors ever flown. Each SRB was tall and wide, weighed , and had a steel exterior approximately thick. The SRB's subcomponents were the solid-propellant motor, nose cone, and rocket nozzle. The solid-propellant motor comprised the majority of the SRB's structure. Its casing consisted of 11 steel sections which made up its four main segments. The nose cone housed the forward separation motors and the parachute systems that were used during recovery. The rocket nozzles could gimbal up to 8° to allow for in-flight adjustments.

The rocket motors were each filled with a total of solid rocket propellant, and joined together in the Vehicle Assembly Building (VAB) at KSC. In addition to providing thrust during the first stage of launch, the SRBs provided structural support for the orbiter vehicle and ET, as they were the only system that was connected to the mobile launcher platform (MLP). At the time of launch, the SRBs were armed at T-5 minutes, and could only be electrically ignited once the RS-25 engines had ignited and were without issue. They each provided of thrust, which was later improved to beginning on STS-8. After expending their fuel, the SRBs were jettisoned approximately two minutes after launch at an altitude of approximately . Following separation, they deployed drogue and main parachutes, landed in the ocean, and were recovered by the crews aboard the ships MV Freedom Star and MV Liberty Star. Once they were returned to Cape Canaveral, they were cleaned and disassembled. The rocket motor, igniter, and nozzle were then shipped to Thiokol to be refurbished and reused on subsequent flights.

The SRBs underwent several redesigns throughout the program's lifetime. STS-6 and STS-7 used SRBs that were lighter than the standard-weight cases due to walls that were thinner, but were determined to be too thin. Subsequent flights until STS-26 used cases that were thinner than the standard-weight cases, which saved . After the "Challenger" disaster as a result of an O-ring failing at low temperature, the SRBs were redesigned to provide a constant seal regardless of the ambient temperature.

The Space Shuttle's operations were supported by vehicles and infrastructure that facilitated its transportation, construction, and crew access. The crawler-transporters carried the MLP and the Space Shuttle from the VAB to the launch site. The Shuttle Carrier Aircraft (SCA) were two modified Boeing 747s that could carry an orbiter on its back. The original SCA (N905NA) was first flown in 1975, and was used for the ALT and ferrying the orbiter from Edwards AFB to the KSC on all missions prior to 1991. A second SCA (N911NA) was acquired in 1988, and was first used to transport "Endeavour" from the factory to the KSC. Following the retirement of the Space Shuttle, N905NA was put on display at the JSC, and N911NA was put on display at the Joe Davis Heritage Airpark in Palmdale, California. The Crew Transport Vehicle (CTV) was a modified airport jet bridge that was used to assist astronauts to egress from the orbiter after landing, where they would undergo their post-mission medical checkups. The Astrovan transported astronauts from the crew quarters in the Operations and Checkout Building to the launch pad on launch day. The NASA Railroad comprised three locomotives that transported SRB segments from the Florida East Coast Railway in Titusville to the KSC.

The Space Shuttle was prepared for launch primarily in the VAB at the KSC. The SRBs were assembled and attached to the external tank on the MLP. The orbiter vehicle was prepared at the Orbiter Processing Facility (OPF) and transferred to the VAB, where a crane was used to rotate it to the vertical orientation and mate it to the external tank. Once the entire stack was assembled, the MLP was carried for to Launch Complex 39 by one of the crawler-transporters. After the Space Shuttle arrived at one of the two launchpads, it would connect to the Fixed and Rotation Service Structures, which provided servicing capabilities, payload insertion, and crew transportation. The crew was transported to the launch pad at T−3hours and entered the orbiter vehicle, which was closed at T−2hours. LOX and LH were loaded into the external tank via umbilicals that attached to the orbiter vehicle, which began at T−5hours35minutes. At T−3hours45minutes, the LH fast-fill was complete, followed 15 minutes later by the LOX. Both tanks were slowly filled up until the launch as the oxygen and hydrogen evaporated.

The launch commit criteria considered precipitation, temperatures, cloud cover, lightning forecast, wind, and humidity. The Space Shuttle was not launched under conditions where it could have been struck by lightning, as its exhaust plume could have triggered lightning by providing a current path to ground after launch, which occurred on Apollo12. The NASA Anvil Rule for a Shuttle launch stated that an anvil cloud could not appear within a distance of . The Shuttle Launch Weather Officer monitored conditions until the final decision to scrub a launch was announced. In addition to the weather at the launch site, conditions had to be acceptable at one of the Transatlantic Abort Landing sites and the SRB recovery area.

The mission crew and the Launch Control Center (LCC) personnel completed systems checks throughout the countdown. Two built-in holds at T−20 minutes and T−9 minutes provided scheduled breaks to address any issues and additional preparation. After the built-in hold at T−9 minutes, the countdown was automatically controlled by the Ground Launch Sequencer (GLS) at the LCC, which stopped the countdown if it sensed a critical problem with any of the Space Shuttle's onboard systems. At T−3minutes45seconds, the engines began conducting gimbal tests, which were concluded at T−2minutes15seconds. The ground launch processing system handed off the control to the orbiter vehicle's GPCs at T−31seconds. At T−16seconds, the GPCs armed the SRBs, the sound suppression system (SPS) began to drench the MLP and SRB trenches with of water to protect the orbiter vehicle from damage by acoustical energy and rocket exhaust reflected from the flame trench and MLP during lift-off. At T−10seconds, hydrogen igniters were activated under each engine bell to quell the stagnant gas inside the cones before ignition. Failure to burn these gases could trip the onboard sensors and create the possibility of an overpressure and explosion of the vehicle during the firing phase. The LH prevalves were opened at T−9.5seconds in preparation for engine start.

Beginning at T−6.6seconds, the main engines were ignited sequentially at 120-millisecond intervals. All three RS-25 engines were required to reach 90% rated thrust by T−3seconds, otherwise the GPCs would initiate an RSLS abort. If all three engines indicated nominal performance by T−3seconds, they were commanded to gimbal to liftoff configuration and the command would be issued to arm the SRBs for ignition at T−0. Between T−6.6seconds and T−3seconds, while the RS-25 engines were firing but the SRBs were still bolted to the pad, the offset thrust caused the Space Shuttle to pitch down measured at the tip of the external tank; the 3-second delay allowed the stack to return to nearly vertical before SRB ignition. At T−0, the eight frangible nuts holding the SRBs to the pad were detonated, the final umbilicals were disconnected, the SSMEs were commanded to 100% throttle, and the SRBs were ignited. By T+0.23seconds, the SRBs built up enough thrust for liftoff to commence, and reached maximum chamber pressure by T+0.6seconds. At T−0, the JSC Mission Control Center assumed control of the flight from the LCC.

At T+4seconds, when the Space Shuttle reached an altitude of , the RS-25 engines were throttled up to 104.5%. At approximately T+7seconds, the Space Shuttle rolled to a heads-down orientation at an altitude of , which reduced aerodynamic stress and provided an improved communication and navigation orientation. Approximately 20−30seconds into ascent and an altitude of , the RS-25 engines were throttled down to 65−72% to reduce the maximum aerodynamic forces at Max Q. Additionally, the shape of the SRB propellant was designed to cause thrust to decrease at the time of Max Q. The GPCs could dynamically control the throttle of the RS-25 engines based upon the performance of the SRBs.

At approximately T+123seconds and an altitude of , pyrotechnic fasteners released the SRBs, which reached an apogee of before parachuting into the Atlantic Ocean. The Space Shuttle continued its ascent using only the RS-25 engines. On earlier missions the Space Shuttle remained in the heads-down orientation to maintain communications with the tracking station in Bermuda, but later missions, beginning with STS-87, rolled to a heads-up orientation at T+6minutes for communication with the tracking and data relay satellite constellation. The RS-25 engines were throttled at T+7minutes30seconds to limit vehicle acceleration to 3 "g". At 6seconds prior to main engine cutoff (MECO), which occurred at T+8minutes30seconds, the RS-25 engines were throttled down to 67%. The GPCs controlled ET separation, and dumped the remaining LOX and LH to prevent outgassing while in orbit. The ET continued on a ballistic trajectory and broke up during reentry, with some small pieces landing in the Indian or Pacific Ocean.

Early missions used two firings of the OMS to achieve orbit; the first firing raised the apogee while the second circularized the orbit. Missions after STS-38 used the RS-25 engines to achieve the optimal apogee, and used the OMS engines to circularize the orbit. The orbital altitude and inclination were mission-dependent, and the Space Shuttle's orbits varied from to .

The type of mission that the Space Shuttle was assigned to dictated the type of orbit that it entered. The initial design of the reusable Space Shuttle envisioned an increasingly cheap launch platform to deploy commercial and government satellites. Early missions routinely ferried satellites, which determined the type of orbit that the orbiter vehicle would enter. Following the "Challenger" disaster, many commercial payloads were moved to expendable commercial rockets, such as the Delta II. While later missions still launched commercial payloads, Space Shuttle assignments were routinely directed towards scientific payloads, such as the Hubble Space Telescope, Spacelab, and the Galileo spacecraft. Beginning with STS-74, the orbiter vehicle conducted dockings with the Mir space station. In its final decade of operation, the Space Shuttle was used for the construction of the International Space Station. Most missions involved staying in orbit several days to two weeks, although longer missions were possible with the Extended Duration Orbiter pallet. The 17 day 15 hour STS-80 mission was the longest Space Shuttle mission duration.

Approximately four hours prior to deorbit, the crew began preparing the orbiter vehicle for reentry by closing the payload doors, radiating excess heat, and retracting the Kuband antenna. The orbiter vehicle maneuvered to an upside down, tail first orientation and began a 2-4minute OMS burn approximately 20minutes before it reentered the atmosphere. The orbiter vehicle reoriented itself to a nose-forward position with a 40° angle-of-attack, and the forward RCS jets were emptied of fuel and disabled prior to reentry. The orbiter vehicle's reentry was defined as starting at an altitude , when it was traveling approximately Mach 25. The orbiter vehicle's reentry was controlled by the GPCs, which followed a preset angle-of-attack plan to prevent unsafe heating of the TPS. The GPCs also controlled the multiple aerobraking S-turns, using only the roll axis, to dissipate excess speed without changing the angle-of-attack.
The orbiter vehicle's aft RCS jets were disabled as it descended and its ailerons, elevators, and rudder became effective in the lower atmosphere. At an altitude of , the orbiter vehicle opened its speed brake on the vertical stabilizer. At 8minutes44seconds prior to landing, the crew deployed the air data probes, and began lowering the angle-of-attack to 36°. The orbiter's maximum glide ratio/lift-to-drag ratio varied considerably with speed, ranging from 1.3 at hypersonic speeds to 4.9 at subsonic speeds. The orbiter vehicle flew to one of the two Heading Alignment Cones, located away from each end of the runway's centerline, where it made its final turns to dissipate excess energy prior to its approach and landing. Once the orbiter vehicle was traveling subsonically, the crew took over manual control of the flight.

The approach and landing phase began when the orbiter vehicle was at an altitude of and traveling at . the orbiter vehicle followed either a -20° or -18° glideslope and descended at approximately . The speed brake was used to keep a continuous speed, and crew initiated a pre-flare maneuver to a -1.5° glideslope at an altitude of . The landing gear was deployed 10seconds prior to touchdown, when the orbiter was at an altitude of and traveling . A final flare maneuver reduced the orbiter vehicle's descent rate to , with touchdown occurring at , depending on the weight of the orbiter vehicle. After the landing gear touched down, the crew deployed a drag chute out of the vertical stabilizer, and began wheel braking when the orbiter vehicle was traveling slower than . After wheels stop, the crew deactivated the flight components and prepared to exit.

The primary Space Shuttle landing site was the Shuttle Landing Facility at KSC, where 78 of the 133 successful landings occurred. In the event of unfavorable landing conditions, the Shuttle could delay its landing or land at an alternate location. The primary alternate was Edwards AFB, which was used for 54 landings. STS-3 landed at the White Sands Space Harbor in New Mexico and required extensive post-processing after exposure to the gypsum-rich sand, some of which was found in "Columbia" debris after STS-107. Landings at alternate airfields required the Shuttle Carrier Aircraft to transport the orbiter back to Cape Canaveral.

In addition to the pre-planned landing airfields, there were 85 agreed-upon emergency landing sites to be used in different abort scenarios, with 58 located in other countries. The landing locations were chosen based upon political relationships, favorable weather, a runway at least long, and TACAN or DME equipment. Additionally, as the orbiter vehicle only had UHF radios, international sites with only VHF radios would have been unable to communicate directly with the crew. Facilities on the east coast of the US were planned for East Coast Abort Landings, while several sites in Europe and Africa were planned in the event of a Transoceanic Abort Landing. The facilities were prepared with equipment and personnel in the event of an emergency shuttle landing, but were never used.

After the landing, ground crews approached the orbiter to conduct safety checks. Teams wearing self-contained breathing gear tested for presence of hydrogen, hydrazine, monomethylhydrazine, nitrogen tetroxide, and ammonia to ensure the landing area was safe. Air conditioning and Freon lines were connected to cool the crew and equipment and dissipate excess heat from reentry. A flight surgeon boarded the orbiter and performed medical checks of the crew before they disembarked. Once the orbiter was secured, it was towed to the OPF to be inspected, repaired, and prepared for the next mission.

The Space Shuttle flew from April 12, 1981 until July 21, 2011. Throughout the program, the Space Shuttle had 135 missions, of which 133 returned safely. Throughout its lifetime, the Space Shuttle was used to conduct scientific research, deploy commercial, military, and scientific payloads, and was involved in the construction and operation of Mir and the ISS. During its tenure, the Space Shuttle served as the only U.S. vehicle to launch astronauts, of which there was no replacement until the launch of Crew Dragon Demo-2 on May 30, 2020.

The overall NASA budget of the Space Shuttle program has been estimated to be $221 billion (in 2012 dollars). The developers of the Space Shuttle advocated for reusability as a cost-saving measure, which resulted in higher development costs for presumed lower costs-per-launch. During the design of the Space Shuttle, the Phase B proposals were not as cheap as the initial Phase A estimates indicated; Space Shuttle program manager Robert Thompson acknowledged that reducing cost-per-pound was not the primary objective of the further design phases, as other technical requirements could not be met with the reduced costs. Development estimates made in 1972 projected a per-pound cost of payload as low as $1,109 (in 2012) per pound, but the actual payload costs, not to include the costs for the research and development of the Space Shuttle, were $37,207 (in 2012) per pound. Per-launch costs varied throughout the program, and were dependent on the rate of flights as well as research, development, and investigation proceedings throughout the Space Shuttle program. In 1982, NASA published an estimate of $260 million (in 2012) per flight, which was based on the prediction of 24 flights per year for a decade. The per-launch cost from 1995–2002, when the orbiters and ISS were not being constructed and there was no recovery work following a loss of crew, was $806 million. NASA published a study in 1999 that concluded that costs were $576 million (in 2012) if there were seven launches per year. In 2009, NASA determined that the cost of adding a single launch per year was $252 million (in 2012), which indicated that much of the Space Shuttle program costs are for year-round personnel and operations that continued regardless of the launch rate. Accounting for the entire Space Shuttle program budget, the per-launch cost was $1.642 billion (in 2012).

On January 28, 1986, STS-51-L disintegrated 73 seconds after launch, due to the failure of the right SRB, killing all seven astronauts on board "Challenger". The disaster was caused by low-temperature impairment of an O-ring, a mission-critical seal used between segments of the SRB casing. Failure of the O-ring allowed hot combustion gases to escape from between the booster sections and burn through the adjacent ET, leading to a sequence of events which caused the orbiter to disintegrate. Repeated warnings from design engineers voicing concerns about the lack of evidence of the O-rings' safety when the temperature was below 53 °F (12 °C) had been ignored by NASA managers.

On February 1, 2003, "Columbia" disintegrated during re-entry, killing all seven of the STS-107 crew, because of damage to the carbon-carbon leading edge of the wing caused during launch. Ground control engineers had made three separate requests for high-resolution images taken by the Department of Defense that would have provided an understanding of the extent of the damage, while NASA's chief TPS engineer requested that astronauts on board "Columbia" be allowed to leave the vehicle to inspect the damage. NASA managers intervened to stop the Department of Defense's imaging of the orbiter and refused the request for the spacewalk, and thus the feasibility of scenarios for astronaut repair or rescue by "Atlantis" were not considered by NASA management at the time.

The partial reusability of the Space Shuttle was one of the primary design requirements during its initial development. The technical decisions that dictated the orbiter's return and reuse reduced the per-launch payload capabilities with the intention of lowering the per-launch costs and resulting in a high-launch rate. The actual costs of a Space Shuttle launch were higher than initially predicted, and the Space Shuttle did not fly the intended 24 missions per year as initially predicted by NASA. The Space Shuttle was originally intended as a launch vehicle to deploy satellites, which it was primarily used for on the missions prior to the "Challenger" disaster. NASA's pricing, which was below cost, was lower than expendable launch vehicles; the intention was that the high volume of Space Shuttle missions would compensate for early financial losses. The improvement of expendable launch vehicles and the transition away from commercial payload on the Space Shuttle resulted in expendable launch vehicles becoming the primary deployment option for satellites.

The fatal "Challenger" and "Columbia" disasters demonstrated the safety risks of the Space Shuttle that could result in the loss of the crew. The spaceplane design of the orbiter limited the abort options, as the abort scenarios required the controlled flight of the orbiter to a runway or to allow the crew to egress individually, rather than the abort escape options on the Apollo and Soyuz space capsules. Early safety analyses advertised by NASA engineers and management predicted the chance of a catastrophic failure resulting in the death of the crew as ranging from 1 in 100 launches to as rare as 1 in 100,000. Following the loss of two Space Shuttle missions, the risks for the initial missions were reevaluated, and the chance of a catastrophic loss of the vehicle and crew was found to be as high as 1 in 9. NASA management was criticized after for accepting increased risk to the crew in exchange for higher mission rates. Both the "Challenger" and "Columbia" reports explained that NASA culture had failed to keep the crew safe by not objectively evaluating the potential risks of the missions.

The Space Shuttle retirement was announced in January 2004. President George W. Bush announced his Vision for Space Exploration, which called for the retirement of the Space Shuttle once it completed construction of the ISS. To ensure the ISS was properly assembled, the contributing partners determined the need for 16 remaining assembly missions in March 2006. One additional Hubble Space Telescope servicing mission was approved in October 2006. Originally, STS-134 was to be the final Space Shuttle mission. However, the "Columbia" disaster resulted in additional orbiters being prepared for launch on need in the event of a rescue mission. As "Atlantis" was prepared for the final launch-on-need mission, the decision was made in September 2010 that it would fly as STS-135 with a four-person crew that could remain at the ISS in the event of an emergency. STS-135 launched on July 8, 2011, and landed at the KSC on July 21, 2011, at 5:57a.m.EDT (09:57UTC). From then until the launch of Crew Dragon Demo-2 on May 30, 2020, the US launched its astronauts aboard Russian Soyuz spacecraft.

Following each orbiter's final flight, it was processed to make it safe for display. The OMS and RCS systems used presented the primary dangers due to their toxic hypergolic propellant, and most of their components were permanently removed to prevent any dangerous outgassing. "Atlantis" is on display at the Kennedy Space Center Visitor Complex, "Discovery" is at the Udvar-Hazy Center, "Endeavour" is on display at the California Science Center, and "Enterprise" is displayed at the Intrepid Sea-Air-Space Museum. Components from the orbiters were transferred to the US Air Force, ISS program, and Russian and Canadian governments. The engines were removed to be used on the Space Launch System, and spare RS-25 nozzles were attached for display purposes.

The Space Shuttle, and fictitious variants, have been featured in numerous movies. The plot of the 1979 James Bond film "Moonraker" featured a Space Shuttle that was stolen while loaned to the United Kingdom. The 1986 film "SpaceCamp" portrayed "Atlantis" accidentally launching into space with a group of U.S. Space Camp participants as its crew. The 2013 film "Gravity" features the fictional Space Shuttle "Explorer" during STS-157, whose crew are killed or left stranded after it is destroyed by a shower of high-speed orbital debris. The Space Shuttle has been featured as a Lego model. The Space Shuttle also appears in flight simulator and space flight simulator games such as "Microsoft Space Simulator", "Orbiter", and "Space Shuttle Mission 2007". The U.S. Postal Service has released several postage issues that depict the Space Shuttle. The first such stamps were issued in 1981, and are on display at the National Postal Museum.




</doc>
<doc id="28191" url="https://en.wikipedia.org/wiki?curid=28191" title="Snow">
Snow

Snow comprises individual ice crystals that grow while suspended in the atmosphere—usually within clouds—and then fall, accumulating on the ground where they undergo further changes. It consists of frozen crystalline water throughout its life cycle, starting when, under suitable conditions, the ice crystals form in the atmosphere, increase to millimeter size, precipitate and accumulate on surfaces, then metamorphose in place, and ultimately melt, slide or sublimate away. 

Snowstorms organize and develop by feeding on sources of atmospheric moisture and cold air. Snowflakes nucleate around particles in the atmosphere by attracting supercooled water droplets, which freeze in hexagonal-shaped crystals. Snowflakes take on a variety of shapes, basic among these are platelets, needles, columns and rime. As snow accumulates into a snowpack, it may blow into drifts. Over time, accumulated snow metamorphoses, by sintering, sublimation and freeze-thaw. Where the climate is cold enough for year-to-year accumulation, a glacier may form. Otherwise, snow typically melts seasonally, causing runoff into streams and rivers and recharging groundwater.

Major snow-prone areas include the polar regions, the northernmost half of the Northern Hemisphere and mountainous regions worldwide with sufficient moisture and cold temperatures. In the Southern Hemisphere, snow is confined primarily to mountainous areas, apart from Antarctica.

Snow affects such human activities as transportation: creating the need for keeping roadways, wings, and windows clear; agriculture: providing water to crops and safeguarding livestock; sports such as skiing, snowboarding, and snowmachine travel; and warfare. Snow affects ecosystems, as well, by providing an insulating layer during winter under which plants and animals are able to survive the cold.

Snow develops in clouds that themselves are part of a larger weather system. The physics of snow crystal development in clouds results from a complex set of variables that include moisture content and temperatures. The resulting shapes of the falling and fallen crystals can be classified into a number of basic shapes and combinations thereof. Occasionally, some plate-like, dendritic and stellar-shaped snowflakes can form under clear sky with a very cold temperature inversion present.

Snow clouds usually occur in the context of larger weather systems, the most important of which is the low-pressure area, which typically incorporate warm and cold fronts as part of their circulation. Two additional and locally productive sources of snow are lake-effect (also sea-effect) storms and elevation effects, especially in mountains.

Mid-latitude cyclones are low-pressure areas which are capable of producing anything from cloudiness and mild snow storms to heavy blizzards. During a hemisphere's fall, winter, and spring, the atmosphere over continents can be cold enough through the depth of the troposphere to cause snowfall. In the Northern Hemisphere, the northern side of the low-pressure area produces the most snow. For the southern mid-latitudes, the side of a cyclone that produces the most snow is the southern side.

A cold front, the leading edge of a cooler mass of air, can produce frontal snowsqualls—an intense frontal convective line (similar to a rainband), when temperature is near freezing at the surface. The strong convection that develops has enough moisture to produce whiteout conditions at places which line passes over as the wind causes intense blowing snow. This type of snowsquall generally lasts less than 30 minutes at any point along its path but the motion of the line can cover large distances. Frontal squalls may form a short distance ahead of the surface cold front or behind the cold front where there may be a deepening low-pressure system or a series of trough lines which act similar to a traditional cold frontal passage. In situations where squalls develop post-frontally it is not unusual to have two or three linear squall bands pass in rapid succession only separated by 25 miles (40 kilometers) with each passing the same point in roughly 30 minutes apart. In cases where there is a large amount of vertical growth and mixing the squall may develop embedded cumulonimbus clouds resulting in lightning and thunder which is dubbed thundersnow.

A warm front can produce snow for a period, as warm, moist air overrides below-freezing air and creates precipitation at the boundary. Often, snow transitions to rain in the warm sector behind the front.

Lake-effect snow is produced during cooler atmospheric conditions when a cold air mass moves across long expanses of warmer lake water, warming the lower layer of air which picks up water vapor from the lake, rises up through the colder air above, freezes and is deposited on the leeward (downwind) shores.

The same effect also occurs over bodies of salt water, when it is termed "ocean-effect" or "bay-effect snow". The effect is enhanced when the moving air mass is uplifted by the orographic influence of higher elevations on the downwind shores. This uplifting can produce narrow but very intense bands of precipitation, which deposit at a rate of many inches of snow each hour, often resulting in a large amount of total snowfall.

The areas affected by lake-effect snow are called snowbelts. These include areas east of the Great Lakes, the west coasts of northern Japan, the Kamchatka Peninsula in Russia, and areas near the Great Salt Lake, Black Sea, Caspian Sea, Baltic Sea, and parts of the northern Atlantic Ocean.

Orographic or relief snowfall is created when moist air is forced up the windward side of mountain ranges by the large-scale wind flow. The lifting of moist air up the side of a mountain range results in adiabatic cooling, and ultimately condensation and precipitation. Moisture is gradually removed from the air by this process, leaving drier and warmer air on the descending, or leeward, side. The resulting enhanced snowfall, along with the decrease in temperature with elevation, combine to increase snow depth and seasonal persistence of snowpack in snow-prone areas.

Mountain waves have also been found to help enhance precipitation amounts downwind of mountain ranges by enhancing the lift needed for condensation and precipitation.

A snowflake consists of roughly 10 water molecules, which are added to its core at different rates and in different patterns, depending on the changing temperature and humidity within the atmosphere that the snowflake falls through on its way to the ground. As a result, snowflakes vary among themselves, while following similar patterns.

Snow crystals form when tiny supercooled cloud droplets (about 10 μm in diameter) freeze. These droplets are able to remain liquid at temperatures lower than , because to freeze, a few molecules in the droplet need to get together by chance to form an arrangement similar to that in an ice lattice. Then the droplet freezes around this "nucleus". In warmer clouds an aerosol particle or "ice nucleus" must be present in (or in contact with) the droplet to act as a nucleus. Ice nuclei are very rare compared to that cloud condensation nuclei on which liquid droplets form. Clays, desert dust and biological particles can be nuclei. Artificial nuclei include particles of silver iodide and dry ice, and these are used to stimulate precipitation in cloud seeding.

Once a droplet has frozen, it grows in the supersaturated environment—one where air is saturated with respect to ice when the temperature is below the freezing point. The droplet then grows by diffusion of water molecules in the air (vapor) onto the ice crystal surface where they are collected. Because water droplets are so much more numerous than the ice crystals due to their sheer abundance, the crystals are able to grow to hundreds of micrometers or millimeters in size at the expense of the water droplets by the Wegener–Bergeron–Findeisen process. The corresponding depletion of water vapor causes the ice crystals to grow at the droplets' expense. These large crystals are an efficient source of precipitation, since they fall through the atmosphere due to their mass, and may collide and stick together in clusters, or aggregates. These aggregates are snowflakes, and are usually the type of ice particle that falls to the ground. Although the ice is clear, scattering of light by the crystal facets and hollows/imperfections mean that the crystals often appear white in color due to diffuse reflection of the whole spectrum of light by the small ice particles.

Micrography of thousands of snowflakes from 1885 onward, starting with Wilson Alwyn Bentley, revealed the wide diversity of snowflakes within a classifiable set of patterns. Closely matching snow crystals have been observed.

Ukichiro Nakaya developed a crystal morphology diagram, relating crystal shapes to the temperature and moisture conditions under which they formed, which is summarized in the following table.

Nakaya discovered that the shape is also a function of whether the prevalent moisture is above or below saturation. Forms below the saturation line trend more towards solid and compact. Crystals formed in supersaturated air trend more towards lacy, delicate and ornate. Many more complex growth patterns also form such as side-planes, bullet-rosettes and also planar types depending on the conditions and ice nuclei. If a crystal has started forming in a column growth regime, at around , and then falls into the warmer plate-like regime, then plate or dendritic crystals sprout at the end of the column, producing so called "capped columns".

Magono and Lee devised a classification of freshly formed snow crystals that includes 80 distinct shapes. They documented each with micrographs.

Snow accumulates from a series of snow events, punctuated by freezing and thawing, over areas that are cold enough to retain snow seasonally or perennially. Major snow-prone areas include the Arctic and Antarctic, the Northern Hemisphere, and alpine regions. The liquid equivalent of snowfall may be evaluated using a snow gauge or with a standard rain gauge, adjusted for winter by removal of a funnel and inner cylinder. Both types of gauges melt the accumulated snow and report the amount of water collected. At some automatic weather stations an ultrasonic snow depth sensor may be used to augment the precipitation gauge.

Snow flurry, snow shower, snow storm and blizzard describe snow events of progressively greater duration and intensity. A blizzard is a weather condition involving snow and has varying definitions in different parts of the world. In the United States, a blizzard occurs when two conditions are met for a period of three hours or more: A sustained wind or frequent gusts to , and sufficient snow in the air to reduce visibility to less than . In Canada and the United Kingdom, the criteria are similar. While heavy snowfall often occurs during blizzard conditions, falling snow is not a requirement, as blowing snow can create a ground blizzard.

Snowstorm intensity may be categorized by visibility and depth of accumulation. Snowfall's intensity is determined by visibility, as follows:

The "International Classification for Seasonal Snow on the Ground" defines "height of new snow" as the depth of freshly fallen snow, in centimeters as measured with a ruler, that accumulated on a snowboard during an observation period of 24 hours, or other observation interval. After the measurement, the snow is cleared from the board and the board is placed flush with the snow surface to provide an accurate measurement at the end of the next interval. Melting, compacting, blowing and drifting contribute to the difficulty of measuring snowfall.

Glaciers with their permanent snowpacks cover about 10% of the earth's surface, while seasonal snow covers about nine percent, mostly in the Northern Hemisphere, where seasonal snow covers about , according to a 1987 estimate. A 2007 estimate of snow cover over the Northern Hemisphere suggested that, on average, snow cover ranges from a minimum extent of each August to a maximum extent of each January or nearly half of the land surface in that hemisphere. A study of Northern Hemisphere snow cover extent for the period 1972–2006 suggests a reduction of over the 35-year period.

The following are world records regarding snowfall and snowflakes:

After deposition, snow progresses on one of two paths that determine its fate, either "ablation" (mostly by melting) or transitioning from firn (multi-year snow) into "glacier ice". During this transition, snow "is a highly porous, sintered material made up of a continuous ice structure and a continuously connected pore space, forming together the snow microstructure". Almost always near its melting temperature, a snowpack is continually transforming these properties in a process, known as "metamorphism", wherein all three phases of water may coexist, including liquid water partially filling the pore space. Starting as a powdery deposition, snow becomes more granular when it begins to compact under its own weight, be blown by the wind, sinter particles together and commence the cycle of melting and refreezing. Water vapor plays a role as it deposits ice crystals, known as hoar frost, during cold, still conditions.

Over the course of time, a snowpack may settle under its own weight until its density is approximately 30% of water. Increases in density above this initial compression occur primarily by melting and refreezing, caused by temperatures above freezing or by direct solar radiation. In colder climates, snow lies on the ground all winter. By late spring, snow densities typically reach a maximum of 50% of water. Snow that persists into summer evolves into névé, granular snow, which has been partially melted, refrozen and compacted. Névé has a minimum density of , which is roughly half of the density of liquid water.

Firn is snow that has persisted for multiple years and has been recrystallized into a substance denser than névé, yet less dense and hard than glacial ice. Firn resembles caked sugar and is very resistant to shovelling. Its density generally ranges from to , and it can often be found underneath the snow that accumulates at the head of a glacier. The minimum altitude that firn accumulates on a glacier is called the "firn limit", "firn line" or "snowline".

There are four main mechanisms for movement of deposited snow: "drifting" of unsintered snow, "avalanches" of accumulated snow on steep slopes, "snowmelt" during thaw conditions, and the "movement of glaciers" after snow has persisted for multiple years and metamorphosed into glacier ice.

When powdery, snow drifts with the wind from the location where it originally fell, forming deposits with a depth of several meters in isolated locations. After attaching to hillsides, blown snow can evolve into a snow slab, which is an avalanche hazard on steep slopes.

An avalanche (also called a snowslide or snowslip) is a rapid flow of snow down a sloping surface. Avalanches are typically triggered in a starting zone from a mechanical failure in the snowpack (slab avalanche) when the forces on the snow exceed its strength but sometimes only with gradually widening (loose snow avalanche). After initiation, avalanches usually accelerate rapidly and grow in mass and volume as they entrain more snow. If the avalanche moves fast enough some of the snow may mix with the air forming a powder snow avalanche, which is a type of gravity current. They occur in three major mechanisms:

Many rivers originating in mountainous or high-latitude regions receive a significant portion of their flow from snowmelt. This often makes the river's flow highly seasonal resulting in periodic flooding during the spring months and at least in dry mountainous regions like the mountain West of the US or most of Iran and Afghanistan, very low flow for the rest of the year. In contrast, if much of the melt is from glaciated or nearly glaciated areas, the melt continues through the warm season, with peak flows occurring in mid to late summer.

Glaciers form where the accumulation of snow and ice exceeds ablation. The area in which an alpine glacier forms is called a cirque (corrie or cwm), a typically armchair-shaped geological feature, which collects snow and where the snowpack compacts under the weight of successive layers of accumulating snow, forming névé. Further crushing of the individual snow crystals and reduction of entrapped air in the snow turns it into glacial ice. This glacial ice will fill the cirque until it overflows through a geological weakness or an escape route, such as the gap between two mountains. When the mass of snow and ice is sufficiently thick, it begins to move due to a combination of surface slope, gravity and pressure. On steeper slopes, this can occur with as little as 15 m (50 ft) of snow-ice.

Scientists study snow at a wide variety of scales that include the physics of chemical bonds and clouds; the distribution, accumulation, metamorphosis, and ablation of snowpacks; and the contribution of snowmelt to river hydraulics and ground hydrology. In doing so, they employ a variety of instruments to observe and measure the phenomena studied. Their findings contribute to knowledge applied by engineers, who adapt vehicles and structures to snow, by agronomists, who address the availability of snowmelt to agriculture, and those, who design equipment for sporting activities on snow. Scientists develop and others employ snow classification systems that describe its physical properties at scales ranging from the individual crystal to the aggregated snowpack. A sub-specialty is avalanches, which are of concern to engineers and outdoors sports people, alike.

Snow science addresses how snow forms, its distribution, and processes affecting how snowpacks change over time. Scientists improve storm forecasting, study global snow cover and its effect on climate, glaciers, and water supplies around the world. The study includes physical properties of the material as it changes, bulk properties of in-place snow packs, and the aggregate properties of regions with snow cover. In doing so, they employ on-the-ground physical measurement techniques to establish ground truth and remote sensing techniques to develop understanding of snow-related processes over large areas.

In the field snow scientists often excavate a snow pit within which to make basic measurements and observations. Observations can describe features caused by wind, water percolation, or snow unloading from trees. Water percolation into a snowpack can create flow fingers and ponding or flow along capillary barriers, which can refreeze into horizontal and vertical solid ice formations within the snowpack. Among the measurements of the properties of snowpacks that the "International Classification for Seasonal Snow on the Ground" includes are: snow height, snow water equivalent, snow strength, and extent of snow cover. Each has a designation with code and detailed description. The classification extends the prior classifications of Nakaya and his successors to related types of precipitation and are quoted in the following table:
"All are formed in cloud, except for rime, which forms on objects exposed to supercooled moisture."

It also has a more extensive classification of deposited snow than those that pertain to airborne snow. The categories include both natural and man-made snow types, descriptions of snow crystals as they metamorphose and melt, the development of hoar frost in the snow pack and the formation of ice therein. Each such layer of a snowpack differs from the adjacent layers by one or more characteristics that describe its microstructure or density, which together define the snow type, and other physical properties. Thus, at any one time, the type and state of the snow forming a layer have to be defined because its physical and mechanical properties depend on them. Physical properties include microstructure, grain size and shape, snow density, liquid water content, and temperature.

Remote sensing of snowpacks with satellites and other platforms typically includes multi-spectral collection of imagery. Multi-faceted interpretation of the data obtained allows inferences about what is observed. The science behind these remote observations has been verified with ground-truth studies of the actual conditions.

Satellite observations record a decrease in snow-covered areas since the 1960s, when satellite observations began. In some regions such as China, a trend of increasing snow cover was observed from 1978 to 2006. These changes are attributed to global climate change, which may lead to earlier melting and less coverage area. However, in some areas there may be an increase in snow depth because of higher temperatures for latitudes north of 40°. For the Northern Hemisphere as a whole the mean monthly snow-cover extent has been decreasing by 1.3% per decade.

The most frequently used methods to map and measure snow extent, snow depth and snow water equivalent employ multiple inputs on the visible–infrared spectrum to deduce the presence and properties of snow. The National Snow and Ice Data Center (NSIDC) uses the reflectance of visible and infrared radiation to calculate a normalized difference snow index, which is a ratio of radiation parameters that can distinguish between clouds and snow. Other researchers have developed decision trees, employing the available data to make more accurate assessments. One challenge to this assessment is where snow cover is patchy, for example during periods of accumulation or ablation and also in forested areas. Cloud cover inhibits optical sensing of surface reflectance, which has led to other methods for estimating ground conditions underneath clouds. For hydrological models, it is important to have continuous information about the snow cover. Passive microwave sensors are especially valuable for temporal and spatial continuity because they can map the surface beneath clouds and in darkness. When combined with reflective measurements, passive microwave sensing greatly extends the inferences possible about the snowpack.

Snow science often leads to predictive models that include snow deposition, snow melt, and snow hydrology—elements of the Earth's water cycle—which help describe global climate change.

Global climate change models (GCMs) incorporate snow as a factor in their calculations. Some important aspects of snow cover include its albedo (reflectivity of incident radiation, including light) and insulating qualities, which slow the rate of seasonal melting of sea ice. As of 2011, the melt phase of GCM snow models were thought to perform poorly in regions with complex factors that regulate snow melt, such as vegetation cover and terrain. These models typically derive snow water equivalent (SWE) in some manner from satellite observations of snow cover. The "International Classification for Seasonal Snow on the Ground" defines SWE as "the depth of water that would result if the mass of snow melted completely".

Given the importance of snowmelt to agriculture, hydrological runoff models that include snow in their predictions address the phases of accumulating snowpack, melting processes, and distribution of the meltwater through stream networks and into the groundwater. Key to describing the melting processes are solar heat flux, ambient temperature, wind, and precipitation. Initial snowmelt models used a degree-day approach that emphasized the temperature difference between the air and the snowpack to compute snow water equivalent, SWE. More recent models use an energy balance approach that take into account the following factors to compute "Q", the energy available for melt. This requires measurement of an array of snowpack and environmental factors to compute six heat flow mechanisms that contribute to "Q".

Snow affects human activity in four major areas, transportation, agriculture, structures, and sports. Most transportation modes are impeded by snow on the travel surface. Agriculture often relies on snow as a source of seasonal moisture. Structures may fail under snow loads. Humans find a wide variety of recreational activities in snowy landscapes.

Snow affects the rights of way of highways, airfields and railroads. They share a common tool for clearing snow, the snowplow. However, the application is different in each case—whereas roadways employ anti-icing chemicals to prevent bonding of ice, airfields may not; railroads rely on abrasives to enhance traction on tracks.

In the late 20th century, an estimated $2 billion was spent annually in North America on roadway winter maintenance, owing to snow and other winter weather events, according to a 1994 report by Kuemmel. The study surveyed the practices of jurisdictions within 44 US states and nine Canadian provinces. It assessed the policies, practices, and equipment used for winter maintenance. It found similar practices and progress to be prevalent in Europe.

The dominant effect of snow on vehicle contact with the road is diminished friction. This can be improved with the use of snow tires, which have a tread designed to compact snow in a manner that enhances traction. However, the key to maintaining a roadway that can accommodate traffic during and after a snow event is an effective anti-icing program that employs both chemicals and plowing. The FHWA "Manual of Practice for an Effective Anti-icing Program" emphasizes "anti-icing" procedures that prevent the bonding of snow and ice to the road. Key aspects of the practice include: understanding anti-icing in light of the level of service to be achieved on a given roadway, the climatic conditions to be encountered, and the different roles of deicing, anti-icing, and abrasive materials and applications, and employing anti-icing "toolboxes", one for operations, one for decision-making and another for personnel. The elements to the toolboxes are:
The manual offers matrices that address different types of snow and the rate of snowfall to tailor applications appropriately and efficiently.

Snow fences, constructed upwind of roadways control snow drifting by causing windblown, drifting snow to accumulate in a desired place. They are also used on railways. Additionally, farmers and ranchers use snow fences to create drifts in basins for a ready supply of water in the spring.

In order to keep airports open during winter storms, runways and taxiways require snow removal. Unlike roadways, where chloride chemical treatment is common to prevent snow from bonding to the pavement surface, such chemicals are typically banned from airports because of their strong corrosive effect on aluminum aircraft. Consequently, mechanical brushes are often used to complement the action of snow plows. Given the width of runways on airfields that handle large aircraft, vehicles with large plow blades, an echelon of plow vehicles or rotary snowplows are used to clear snow on runways and taxiways. Terminal aprons may require or more to be cleared.

Properly equipped aircraft are able to fly through snowstorms under Instrument flight rules. Prior to takeoff, during snowstorms they require deicing fluid to prevent accumulation and freezing of snow and other precipitation on wings and fuselages, which may compromise the safety of the aircraft and its occupants. In flight, aircraft rely on a variety of mechanisms to avoid rime and other types of icing in clouds, these include pulsing pneumatic boots, electro-thermal areas that generate heat, and fluid deicers that bleed onto the surface.

Railroads have traditionally employed two types of snow plows for clearing track, the wedge plow, which casts snow to both sides, and the rotary snowplow, which is suited for addressing heavy snowfall and casting snow far to one side or the other. Prior to the invention of the rotary snowplow ca. 1865, it required multiple locomotives to drive a wedge plow through deep snow. Subsequent to clearing the track with such plows, a "flanger" is used to clear snow from between the rails that are below the reach of the other types of plow. Where icing may affect the steel-to-steel contact of locomotive wheels on track, abrasives (typically sand) have been used to provide traction on steeper uphills.

Railroads employ snow sheds—structures that cover the track—to prevent the accumulation of heavy snow or avalanches to cover tracks in snowy mountainous areas, such as the Alps and the Rocky Mountains.


Snow can be compacted to form a snow road and be part of a winter road route for vehicles to access isolated communities or construction projects during the winter. Snow can also be used to provide the supporting structure and surface for a runway, as with the Phoenix Airfield in Antarctica. The snow-compacted runway is designed to withstand approximately 60 wheeled flights of heavy-lift military aircraft a year.

Snowfall can be beneficial to agriculture by serving as a thermal insulator, conserving the heat of the Earth and protecting crops from subfreezing weather. Some agricultural areas depend on an accumulation of snow during winter that will melt gradually in spring, providing water for crop growth, both directly and via runoff through streams and rivers, which supply irrigation canals. The following are examples of rivers that rely on meltwater from glaciers or seasonal snowpack as an important part of their flow on which irrigation depends: the Ganges, many of whose tributaries rise in the Himalayas and which provide much irrigation in northeast India, the Indus River, which rises in Tibet and provides irrigation water to Pakistan from rapidly retreating Tibetan glaciers, and the Colorado River, which receives much of its water from seasonal snowpack in the Rocky Mountains and provides irrigation water to some 4 million acres (1.6 million hectares).

Snow is an important consideration for loads on structures. To address these, European countries employ "Eurocode 1: Actions on structures - Part 1-3: General actions - Snow loads". In North America, ASCE "Minimum Design Loads for Buildings and Other Structures" gives guidance on snow loads. Both standards employ methods that translate maximum expected ground snow loads onto design loads for roofs.

Snow loads and icings are two principal issues for roofs. Snow loads are related to the climate in which a structure is sited. Icings are usually a result of the building or structure generating heat that melts the snow that is on it.

"Snow loads" – The "Minimum Design Loads for Buildings and Other Structures" gives guidance on how to translate the following factors into roof snow loads:
It gives tables for ground snow loads by region and a methodology for computing ground snow loads that may vary with elevation from nearby, measured values. The "Eurocode 1" uses similar methodologies, starting with ground snow loads that are tabulated for portions of Europe.

"Icings" – Roofs must also be designed to avoid ice dams, which result from meltwater running under the snow on the roof and freezing at the eave. Ice dams on roofs form when accumulated snow on a sloping roof melts and flows down the roof, under the insulating blanket of snow, until it reaches below freezing temperature air, typically at the eaves. When the meltwater reaches the freezing air, ice accumulates, forming a dam, and snow that melts later cannot drain properly through the dam. Ice dams may result in damaged building materials or in damage or injury when the ice dam falls off or from attempts to remove ice dams. The melting results from heat passing through the roof under the highly insulating layer of snow.

In areas with trees, utility distribution lines on poles are less susceptible to snow loads than they are subject to damage from trees falling on them, felled by heavy, wet snow. Elsewhere, snow can accrete on power lines as "sleeves" of rime ice. Engineers design for such loads, which are measured in kg/m (lb/ft) and power companies have forecasting systems that anticipate types of weather that may cause such accretions. Rime ice may be removed manually or by creating a sufficient short circuit in the affected segment of power lines to melt the accretions.

Snow figures into many winter sports and forms of recreation, including skiing and sledding. Common examples include cross-country skiing, Alpine skiing, snowboarding, snowshoeing, and snowmobiling. The design of the equipment used, typically relies on the bearing strength of snow, as with skis or snowboards and contends with the coefficient of friction of snow to allow sliding, often enhance by ski waxes.

Skiing is by far the largest form of winter recreation. As of 1994, of the estimated 65–75 million skiers worldwide, there were approximately 55 million who engaged in Alpine skiing, the rest engaged in cross-country skiing. Approximately 30 million skiers (of all kinds) were in Europe, 15 million in the US, and 14 million in Japan. As of 1996, there were reportedly 4,500 ski areas, operating 26,000 ski lifts and enjoying 390 million skier visits per year. The preponderant region for downhill skiing was Europe, followed by Japan and the US.

Increasingly, ski resorts are relying on snowmaking, the production of snow by forcing water and pressurized air through a snow gun on ski slopes. Snowmaking is mainly used to supplement natural snow at ski resorts. This allows them to improve the reliability of their snow cover and to extend their ski seasons from late autumn to early spring. The production of snow requires low temperatures. The threshold temperature for snowmaking increases as humidity decreases. Wet-bulb temperature is used as a metric since it takes air temperature and relative humidity into account. Snowmaking is a relatively expensive process in its energy consumption, thereby limiting its use.

Ski wax enhances the ability of a ski or other runner to slide over snow, which depends on both the properties of the snow and the ski to result in an optimum amount of lubrication from melting the snow by friction with the ski—too little and the ski interacts with solid snow crystals, too much and capillary attraction of meltwater retards the ski. Before a ski can slide, it must overcome the maximum value static friction. Kinetic (or dynamic) friction occurs when the ski is moving over the snow.

Snow affects warfare conducted in winter, alpine environments or at high latitudes. The main factors are "impaired visibility" for acquiring targets during falling snow, "enhanced visibility" of targets against snowy backgrounds for targeting, and mobility for both mechanized and infantry troops. Snowfall can severely inhibit the logistics of supplying troops, as well. Snow can also provide cover and fortification against small-arms fire. Noted winter warfare campaigns where snow and other factors affected the operations include:


Both plant and animal life endemic to snow-bound areas develop ways to adapt. Among the adaptive mechanisms for plants are dormancy, seasonal dieback, survival of seeds; and for animals are hibernation, insulation, anti-freeze chemistry, storing food, drawing on reserves from within the body, and clustering for mutual heat.

Snow interacts with vegetation in two principal ways, vegetation can influence the deposition and retention of snow and, conversely, the presence of snow can affect the distribution and growth of vegetation. Tree branches, especially of conifers intercept falling snow and prevent accumulation on the ground. Snow suspended in trees ablates more rapidly than that on the ground, owing to its greater exposure to sun and air movement. Trees and other plants can also promote snow retention on the ground, which would otherwise be blown elsewhere or melted by the sun. Snow affects vegetation in several ways, the presence of stored water can promote growth, yet the annual onset of growth is dependent on the departure of the snowpack for those plants that are buried beneath it. Furthermore, avalanches and erosion from snowmelt can scour terrain of vegetation.

Snow supports a wide variety of animals both on the surface and beneath. Many invertebrates thrive in snow, including spiders, wasps, beetles, snow scorpionflys and springtails. Such arthropods are typically active at temperatures down to . Invertebrates fall into two groups, regarding surviving subfreezing temperatures: freezing resistant and those that avoid freezing because they are freeze-sensitive. The first group may be cold hardy owing to the ability to produce antifreeze agents in their body fluids that allows survival of long exposure to sub-freezing conditions. Some organisms fast during the winter, which expels freezing-sensitive contents from their digestive tracts. The ability to survive the absence of oxygen in ice is an additional survival mechanism.

Small vertebrates are active beneath the snow. Among vertebrates, alpine salamanders are active in snow at temperatures as low as ; they burrow to the surface in springtime and lay their eggs in melt ponds. Among mammals, those that remain active are typically smaller than . Omnivores are more likely to enter a torpor or be hibernators, whereas herbivores are more likely to maintain food caches beneath the snow. Voles store up to of food and pikas up to . Voles also huddle in communal nests to benefit from one another's warmth. On the surface, wolves, coyotes, foxes, lynx, and weasels rely on these subsurface dwellers for food and often dive into the snowpack to find them.

Extraterrestrial "snow" includes water-based precipitation, but also precipitation of other compounds prevalent on other planets and moons in the Solar System. Examples are:

Lexicon
Notable snow events

Recreation

Related concepts

Science and scientists
Snow structures



</doc>
<doc id="28195" url="https://en.wikipedia.org/wiki?curid=28195" title="Symbolics">
Symbolics

Symbolics is a defunct computer manufacturer Symbolics, Inc., and a privately held company that acquired the assets of the former company and continues to sell and maintain the Open Genera Lisp system and the Macsyma computer algebra system.

The symbolics.com domain was originally registered on March 15, 1985, making it the first .com-domain in the world. In August 2009, it was sold to napkin.com (formerly XF.com) Investments.

Symbolics, Inc. was a computer manufacturer headquartered in Cambridge, Massachusetts, and later in Concord, Massachusetts, with manufacturing facilities in Chatsworth, California (a suburban section of Los Angeles). Its first CEO, chairman, and founder was Russell Noftsker. Symbolics designed and manufactured a line of Lisp machines, single-user computers optimized to run the programming language Lisp. Symbolics also made significant advances in software technology, and offered one of the premier software development environments of the 1980s and 1990s, now sold commercially as Open Genera for Tru64 UNIX on the Hewlett-Packard (HP) Alpha. The Lisp Machine was the first commercially available "workstation", although that word had not yet been coined.

Symbolics was a spinoff from the MIT AI Lab, one of two companies to be founded by AI Lab staffers and associated hackers for the purpose of manufacturing Lisp machines. The other was Lisp Machines, Inc., although Symbolics attracted most of the hackers, and more funding.

Symbolics' initial product, the LM-2, introduced in 1981, was a repackaged version of the MIT CADR Lisp machine design. The operating system and software development environment, over 500,000 lines, was written in Lisp from the microcode up, based on MIT's Lisp Machine Lisp.

The software bundle was later renamed ZetaLisp, to distinguish the Symbolics' product from other vendors who had also licensed the MIT software. Symbolics' Zmacs text editor, a variant of Emacs, was implemented in a text-processing package named "ZWEI", an acronym for "Zwei was Eine initially", with "Eine" being an acronym for "Eine Is Not Emacs". Both are recursive acronyms and puns on the German words for "one" ("eins", "eine") and "two" ("zwei").

The Lisp Machine system software was then copyrighted by MIT, and was licensed to both Symbolics and LMI. Until 1981, Symbolics shared all its copyrighted enhancements to the source code with MIT and kept it on an MIT server. According to Richard Stallman, Symbolics engaged in a business tactic in which it forced MIT to make all Symbolics' copyrighted fixes and improvements to the Lisp Machine OS available only to Symbolics (and MIT but not to Symbolics competitors), and thereby choke off its competitor LMI, which at that time had insufficient resources to independently maintain or develop the OS and environment.

Symbolics felt that they no longer had sufficient control over their product. At that point, Symbolics began using their own copy of the software, located on their company servers, while Stallman says that Symbolics did that to prevent its Lisp improvements from flowing to Lisp Machines, Inc. From that base, Symbolics made extensive improvements to every part of the software, and continued to deliver almost all the source code to their customers (including MIT). However, the policy prohibited MIT staff from distributing the Symbolics version of the software to others. With the end of open collaboration came the end of the MIT hacker community. As a reaction to this, Stallman initiated the GNU project to make a new community. Eventually, Copyleft and the GNU General Public License would ensure that a hacker's software could remain free software. In this way, Symbolics played a key, albeit adversarial, role in instigating the free software movement.

In 1983, a year later than planned, Symbolics introduced the 3600 family of Lisp machines. Code-named the "L-machine" internally, the 3600 family was an innovative new design, inspired by the CADR architecture but sharing few of its implementation details. The main processor had a 36-bit word (divided up as 4 or 8 bits of tags, and 32 bits of data or 28 bits of memory address). Memory words were 44 bits, the additional 8 bits being used for error-correcting code (ECC). The instruction set was that of a stack machine. The 3600 architecture provided 4,096 hardware registers, of which half were used as a cache for the top of the control stack; the rest were used by the microcode and time-critical routines of the operating system and Lisp run-time environment. Hardware support was provided for virtual memory, which was common for machines in its class, and for garbage collection, which was unique.

The original 3600 processor was a microprogrammed design like the CADR, and was built on several large circuit boards from standard TTL integrated circuits, both features being common for commercial computers in its class at the time. Central processing unit (CPU) clock speed varied depending on which instruction was being executed, but was typically around 5 MHz. Many Lisp primitives could be executed in a single clock cycle. Disk input/output (I/O) was handled by multitasking at the microcode level. A 68000 processor (termed the "front-end processor", (FEP)) started the main computer up, and handled the slower peripherals during normal operation. An Ethernet interface was standard equipment, replacing the Chaosnet interface of the LM-2.

The 3600 was roughly the size of a household refrigerator. This was partly due to the size of the processor (the cards were widely spaced to allow wire-wrap prototype cards to fit without interference) and partly due to the size of disk drive technology in the early 1980s. At the 3600's introduction, the smallest disk that could support the ZetaLisp software was wide (most 3600s shipped with the 10½-inch Fujitsu Eagle). The 3670 and 3675 were slightly shorter in height, but were essentially the same machine packed a little tighter. The advent of , and later , disk drives that could hold hundreds of megabytes led to the introduction of the 3640 and 3645, which were roughly the size of a two-drawer file cabinet.

Later versions of the 3600 architecture were implemented on custom integrated circuits, reducing the five cards of the original processor design to two, at a large manufacturing cost savings and with performance slightly better than the old design. The 3650, first of the "G machines", as they were known within the company, was housed in a cabinet derived from the 3640s. Denser memory and smaller disk drives enabled the introduction of the 3620, about the size of a modern full-size tower PC. The 3630 was a "fat 3620" with room for more memory and video interface cards. The 3610 was a lower priced variant of the 3620, essentially identical in every way except that it was licensed for application deployment rather than general development.

The various models of the 3600 family were popular for artificial intelligence (AI) research and commercial applications throughout the 1980s. The AI commercialization boom of the 1980s led directly to Symbolics' success during the decade. Symbolics computers were widely believed to be the best platform available for developing AI software. The LM-2 used a Symbolics-branded version of the complex space-cadet keyboard, while later models used a simplified version (at right), known simply as the "". The Symbolics keyboard featured the many modifier keys used in Zmacs, notably Control/Meta/Super/Hyper in a block, but did not feature the complex symbol set of the space-cadet keyboard.

Also contributing to the 3600 series' success was a line of bit-mapped graphics color video interfaces, combined with extremely powerful animation software. Symbolics' Graphics Division, headquartered in Westwood, Los Angeles, California, near to the major Hollywood movie and television studios, made its S-Render and S-Paint software into industry leaders in the animation business.

Symbolics developed the first workstations able to process high-definition television (HDTV) quality video, which enjoyed a popular following in Japan. A 3600, with the standard black-and-white monitor, made a cameo appearance in the movie "Real Genius". The company was also referenced in Michael Crichton's novel "Jurassic Park".

Symbolics' Graphics Division was sold to Nichimen Trading Company in the early 1990s, and the S-Graphics software suite (S-Paint, S-Geometry, S-Dynamics, S-Render) ported to Franz Allegro Common Lisp on Silicon Graphics (SGI) and PC computers running Windows NT. Today it is sold as Mirai by Izware LLC, and continues to be used in major motion pictures (most famously in New Line Cinema's "The Lord of the Rings"), video games, and military simulations.
Symbolic's 3600-series computers were also used as the first front end "controller" computers for the Connection Machine massively parallel computers manufactured by Thinking Machines Corporation, another MIT spinoff based in Cambridge, Massachusetts. The Connection Machine ran a parallel variant of Lisp and, initially, was used primarily by the AI community, so the Symbolics Lisp machine was a particularly good fit as a front-end machine.

For a long time, the operating system didn't have a name, but was finally named "Genera" around 1984. The system included several advanced dialects of Lisp. Its heritage was Maclisp on the PDP-10, but it included more data types, and multiple-inheritance object-oriented programming features. This Lisp dialect was called Lisp Machine Lisp at MIT. Symbolics used the name ZetaLisp. Symbolics later wrote new software in "Symbolics Common Lisp", its version of the Common Lisp standard.

In the late 1980s (2 years later than planned), the Ivory family of single-chip Lisp Machine processors superseded the G-Machine 3650, 3620, and 3630 systems. The Ivory 390k transistor VLSI implementation designed in Symbolics Common Lisp using NS, a custom Symbolics Hardware Design Language (HDL), addressed a 40-bit word (8 bits tag, 32 bits data/address). Since it only addressed full words and not bytes or half-words, this allowed addressing of 4 Gigawords (GW) or 16 gigabytes (GB) of memory; the increase in address space reflected the growth of programs and data as semiconductor memory and disk space became cheaper. The Ivory processor had 8 bits of ECC attached to each word, so each word fetched from external memory to the chip was actually 48 bits wide. Each Ivory instruction was 18 bits wide and two instructions plus a 2-bit CDR code and 2-bit Data Type were in each instruction word fetched from memory. Fetching two instruction words at a time from memory enhanced the Ivory's performance. Unlike the 3600's microprogrammed architecture, the Ivory instruction set was still microcoded, but was stored in a 1200 × 180-bit ROM inside the Ivory chip. The initial Ivory processors were fabricated by VLSI Technology Inc in San Jose, California, on a 2 µm CMOS process, with later generations fabricated by Hewlett Packard in Corvallis, Oregon, on 1.25 µm and 1 µm CMOS processes. The Ivory had a stack architecture and operated a 4-stage pipeline: Fetch, Decode, Execute and Write Back. Ivory processors were marketed in stand-alone Lisp Machines (the XL400, XL1200, and XL1201), headless Lisp Machines (NXP1000), and on add-in cards for Sun Microsystems (UX400, UX1200) and Apple Macintosh (MacIvory I, II, III) computers. The Lisp Machines with Ivory processors operated at speeds that were between two and six times faster than a 3600 depending on the model and the revision of the Ivory chip.

The Ivory instruction set was later emulated in software for microprocessors implementing the 64-bit Alpha architecture. The "Virtual Lisp Machine" emulator, combined with the operating system and software development environment from the XL machines, is sold as Open Genera.

Sunstone was a processor similar to a reduced instruction set computer (RISC), that was to be released shortly after the Ivory. It was designed by Ron Lebel's group at the Symbolics Westwood office. However, the project was canceled the day it was supposed to tape out.

As quickly as the commercial AI boom of the mid-1980s had propelled Symbolics to success, the "AI Winter" of the late 1980s and early 1990s, combined with the slowdown of the Ronald Reagan administration's Strategic Defense Initiative, popularly termed "Star Wars", missile defense program, for which the "Defense Advanced Research Projects Agency" (DARPA) had invested heavily in AI solutions, severely damaged Symbolics. An internal war between Noftsker and the CEO the board had hired in 1986, Brian Sear, over whether to follow Sun's suggested lead and focus on selling their software, or to re-emphasize their superior hardware, and the ensuing lack of focus when both Noftsker and Sear were fired from the company caused sales to plummet. This, combined with some ill-advised real estate deals by company management during the boom years (they had entered into large long-term lease obligations in California), drove Symbolics into bankruptcy. Rapid evolution in mass market microprocessor technology (the "PC revolution"), advances in Lisp compiler technology, and the economics of manufacturing custom microprocessors severely diminished the commercial advantages of purpose-built Lisp machines. By 1995, the Lisp machine era had ended, and with it Symbolics' hopes for success.

Symbolics continued as an enterprise with very limited revenues, supported mainly by service contracts on the remaining MacIvory, UX-1200, UX-1201, and other machines still used by commercial customers. Symbolics also sold Virtual Lisp Machine (VLM) software for DEC, Compaq, and HP Alpha-based workstations (AlphaStation) and servers (AlphaServer), refurbished MacIvory IIs, and Symbolics keyboards.

In July 2005, Symbolics closed its Chatsworth, California, maintenance facility. The reclusive owner of the company, Andrew Topping, died that same year. The current legal status of Symbolics software is uncertain. An assortment of Symbolics hardware was still available for purchase . The United States Department of Defense (US DoD) is still paying Symbolics for regular maintenance work.

On March 15, 1985, symbolics.com became the first (and currently, since it is still registered, the oldest) registered .com domain of the Internet. The symbolics.com domain was purchased by XF.com in 2009.

Genera also featured the most extensive networking interoperability software seen to that point. A local area network system called Chaosnet had been invented for the Lisp Machine (predating the commercial availability of Ethernet). The Symbolics system supported Chaosnet, but also had one of the first TCP/IP implementations. It also supported DECnet and IBM's SNA network protocols. A Dialnet protocol used phone lines and modems. Genera would, using hints from its distributed "namespace" database (somewhat similar to Domain Name System (DNS), but more comprehensive, like parts of Xerox's Grapevine), automatically select the best protocol combination to use when connecting to network service. An application program (or a user command) would only specify the name of the host and the desired service. For example, a host name and a request for "Terminal Connection" might yield a connection over TCP/IP using the Telnet protocol (although there were many other possibilities). Likewise, requesting a file operation (such as a Copy File command) might pick NFS, FTP, NFILE (the Symbolics network file access protocol), or one of several others, and it might execute the request over TCP/IP, Chaosnet, or whatever other network was most suitable.

The most popular application program for the Symbolics Lisp Machine was the ICAD computer-aided engineering system. One of the first networked multi-player video games, a version of Spacewar, was developed for the Symbolics Lisp Machine in 1983. Electronic CAD software on the Symbolics Lisp Machine was used to develop the first implementation of the Hewlett-Packard Precision Architecture (PA-RISC).

Symbolics' research and development staff (first at MIT, and then later at the company) produced several major innovations in software technology:


The Symbolics Graphics Division (SGD, founded in 1982, sold to Nichimen Graphics in 1992) developed the S-Graphics software suite (S-Paint, S-Geometry, S-Dynamics, S-Render) for Symbolics Genera.

This software was also used to create a few computer animated movies and was used for some popular movies.




</doc>
<doc id="28198" url="https://en.wikipedia.org/wiki?curid=28198" title="Surfing">
Surfing

Surfing is a surface water pastime in which the wave rider, referred to as a surfer, rides on the forward part, or face, of a moving wave, which usually carries the surfer towards the shore. Waves suitable for surfing are primarily found in the ocean, but can also be found in lakes or rivers in the form of a standing wave or tidal bore. However, surfers can also utilize artificial waves such as those from boat wakes and the waves created in artificial wave pools.
The term "surfing" usually refers to the act of riding a wave using a board, regardless of the stance. There are several types of boards. The native peoples of the Pacific, for instance, surfed waves on alaia, paipo, and other such craft, and did so on their belly and knees. The modern-day definition of surfing, however, most often refers to a surfer riding a wave standing on a surfboard; this is also referred to as stand-up surfing.

Another prominent form of surfing is body boarding, when a surfer rides the wave on a bodyboard, either lying on their belly, drop knee (one foot and one knee on the board), or sometimes even standing up on a body board. Other types of surfing include knee boarding, surf matting (riding inflatable mats), and using foils. Body surfing, where the wave is surfed without a board, using the surfer's own body to catch and ride the wave, is very common and is considered by some to be the purest form of surfing. The closest form of body surfing using a board is a handboard which normally has one strap over it to fit one hand in.

Three major subdivisions within stand-up surfing are stand-up paddling, long boarding and short boarding with several major differences including the board design and length, the riding style, and the kind of wave that is ridden.

In tow-in surfing (most often, but not exclusively, associated with big wave surfing), a motorized water vehicle such as a personal watercraft, tows the surfer into the wave front helping the surfer match a large wave's speed, which is generally a higher speed than a self-propelled surfer can produce. Surfing-related sports such as paddle boarding and sea kayaking do not require waves, and other derivative sports such as kite surfing and windsurfing rely primarily on wind for power, yet all of these platforms may also be used to ride waves. Recently with the use of V-drive boats, Wakesurfing, in which one surfs on the wake of a boat, has emerged. The Guinness Book of World Records recognized a wave ride by Garrett McNamara at Nazaré, Portugal as the largest wave ever surfed.

For hundreds of years, surfing was a central part of Polynesian culture. Surfing may have been observed by British explorers at Tahiti in 1767. Samuel Wallis and the crew members of were the first Britons to visit the island in June of that year. Another candidate is the botanist Joseph Banks being part of the first voyage of James Cook on , who arrived on Tahiti on 10 April 1769. Lieutenant James King was the first person to write about the art of Hawaiian Surfing, when he was completing the journals of Captain James Cook upon Cook's death in 1779.

When Mark Twain visited Hawaii in 1866 he wrote,
In one place we came upon a large company of naked natives, of both sexes and all ages, amusing themselves with the national pastime of surf-bathing.

References to surf riding on planks and single canoe hulls are also verified for pre-contact Samoa, where surfing was called "fa'ase'e" or "se'egalu" (see Augustin Krämer, "The Samoa Islands"), and Tonga, far pre-dating the practice of surfing by Hawaiians and eastern Polynesians by over a thousand years.

In July 1885, three teenage Hawaiian princes took a break from their boarding school, St. Mathew's Hall in San Mateo, and came to cool off in Santa Cruz, California. There, David Kawānanakoa, Edward Keliʻiahonui and Jonah Kūhiō Kalanianaʻole surfed the mouth of the San Lorenzo River on custom-shaped redwood boards, according to surf historians Kim Stoner and Geoff Dunn. In 1890 the pioneer in agricultural education John Wrightson reputedly became the first British surfer when instructed by two Hawaiian students at his college.

George Freeth (8 November 1883 – 7 April 1919) is often credited as being the "Father of Modern Surfing". He is thought to have been the first modern surfer.

In 1907, the eclectic interests of the land baron Henry E. Huntington brought the ancient art of surfing to the California coast. While on vacation, Huntington had seen Hawaiian boys surfing the island waves. Looking for a way to entice visitors to the area of Redondo Beach, where he had heavily invested in real estate, he hired a young Hawaiian to ride surfboards. George Freeth decided to revive the art of surfing, but had little success with the huge hardwood boards that were popular at that time. When he cut them in half to make them more manageable, he created the original "Long board", which made him the talk of the islands. To the delight of visitors, Freeth exhibited his surfing skills twice a day in front of the Hotel Redondo. Another native Hawaiian, Duke Kahanamoku, spread surfing to both the U.S. and Australia, riding the waves after displaying the swimming prowess that won him Olympic gold medals in 1912 and 1920.

In 1975, a professional tour started. That year Margo Oberg became the first female professional surfer.

Swell is generated when the wind blows consistently over a large area of open water, called the wind's fetch. The size of a swell is determined by the strength of the wind and the length of its fetch and duration. Because of this, the surf tends to be larger and more prevalent on coastlines exposed to large expanses of ocean traversed by intense low pressure systems.

Local wind conditions affect wave quality since the surface of a wave can become choppy in blustery conditions. Ideal conditions include a light to moderate "offshore" wind, because it blows into the front of the wave, making it a "barrel" or "tube" wave. Waves are Left handed and Right Handed depending upon the breaking formation of the wave.

Waves are generally recognized by the surfaces over which they break. For example, there are beach breaks, reef breaks and point breaks.

The most important influence on wave shape is the topography of the seabed directly behind and immediately beneath the breaking wave. The contours of the reef or bar front become stretched by diffraction. Each break is different since each location's underwater topography is unique. At beach breaks, sandbanks change shape from week to week. Surf forecasting is aided by advances in information technology. Mathematical modeling graphically depicts the size and direction of swells around the globe.

Swell regularity varies across the globe and throughout the year. During winter, heavy swells are generated in the mid-latitudes, when the North and South polar fronts shift toward the Equator. The predominantly Westerly winds generate swells that advance Eastward, so waves tend to be largest on West coasts during winter months. However, an endless train of mid-latitude cyclones cause the isobars to become undulated, redirecting swells at regular intervals toward the tropics.

East coasts also receive heavy winter swells when low-pressure cells form in the sub-tropics, where slow moving highs inhibit their movement. These lows produce a shorter fetch than polar fronts, however, they can still generate heavy swells since their slower movement increases the duration of a particular wind direction. The variables of fetch and duration both influence how long wind acts over a wave as it travels since a wave reaching the end of a fetch behaves as if the wind died.

During summer, heavy swells are generated when cyclones form in the tropics. Tropical cyclones form over warm seas, so their occurrence is influenced by El Niño & La Niña cycles. Their movements are unpredictable.

Surf travel and some surf camps offer surfers access to remote, tropical locations, where tradewinds ensure offshore conditions. Since winter swells are generated by mid-latitude cyclones, their regularity coincides with the passage of these lows. Swells arrive in pulses, each lasting for a couple of days, with a few days between each swell.

The availability of free model data from the NOAA has allowed the creation of several surf forecasting websites.

Tube shape is defined by length to width ratio. A perfectly cylindrical vortex has a ratio of 1:1. Other forms include:

Tube speed is defined by angle of peel line.

The value of good surf in attracting surf tourism has prompted the construction of artificial reefs and sand bars. Artificial surfing reefs can be built with durable sandbags or concrete, and resemble a submerged breakwater. These artificial reefs not only provide a surfing location, but also dissipate wave energy and shelter the coastline from erosion. Ships such as Seli 1 that have accidentally stranded on sandy bottoms, can create sandbanks that give rise to good waves.

An artificial reef known as Chevron Reef was constructed in El Segundo, California in hopes of creating a new surfing area. However, the reef failed to produce any quality waves and was removed in 2008. In Kovalam, South West India, an artificial reef has, however, successfully provided the local community with a quality lefthander, stabilized coastal soil erosion, and provided good habitat for marine life. ASR Ltd., a New Zealand-based company, constructed the Kovalam reef and is working on another reef in Boscombe, England.

Even with artificial reefs in place, a tourist's vacation time may coincide with a "flat spell", when no waves are available. Completely artificial Wave pools aim to solve that problem by controlling all the elements that go into creating perfect surf, however there are only a handful of wave pools that can simulate good surfing waves, owing primarily to construction and operation costs and potential liability. Most wave pools generate waves that are too small and lack the power necessary to surf. The Seagaia Ocean Dome, located in Miyazaki, Japan, was an example of a surfable wave pool. Able to generate waves with up to faces, the specialized pump held water in 20 vertical tanks positioned along the back edge of the pool. This allowed the waves to be directed as they approach the artificial sea floor. Lefts, Rights, and A-frames could be directed from this pump design providing for rippable surf and barrel rides. The Ocean Dome cost about $2 billion to build and was expensive to maintain. The Ocean Dome was closed in 2007. In England, construction is nearing completion on the Wave, situated near Bristol, which will enable people unable to get to the coast to enjoy the waves in a controlled environment, set in the heart of nature.

There are two main types of artificial waves that exist today. One being artificial or stationary waves which simulate a moving, breaking wave by pumping a layer of water against a smooth structure mimicking the shape of a breaking wave. Because of the velocity of the rushing water the wave and the surfer can remain stationary while the water rushes by under the surfboard. Artificial waves of this kind provide the opportunity to try surfing and learn its basics in a moderately small and controlled environment near or far from locations with natural surf.

Another artificial wave can be made through use of a wave pool. These wave pools strive to make a wave that replicates a real ocean wave more than the stationary wave does. In 2018, the first professional surfing tournament in a wave pool was held.

Surfers represent a diverse culture based on riding the waves. Some people practice surfing as a recreational activity while others make it the central focus of their lives. Surfing culture is most dominant in Hawaii and California because these two states offer the best surfing conditions. However, waves can be found wherever there is coastline, and a tight-knit yet far-reaching subculture of surfers has emerged throughout America. Some historical markers of the culture included the woodie, the station wagon used to carry surfers' boards, as well as boardshorts, the long swim shorts typically worn while surfing. Surfers also wear wetsuits in colder regions.

The sport is also a significant part of Australia's eastern coast sub-cultural life, especially in New South Wales, where the weather and water conditions are most favourable for surfing.

During the 1960s, as surfing caught on in California, its popularity spread through American pop culture. Several teen movies, starting with the Gidget series in 1959, transformed surfing into a dream life for American youth. Later movies, including Beach Party (1963), Ride the Wild Surf (1964), and Beach Blanket Bingo (1965) promoted the California dream of sun and surf. Surf culture also fueled the early records of the Beach Boys.

The sport of surfing now represents a multibillion-dollar industry especially in clothing and fashion markets. The World Surf League (WSL) runs the championship tour, hosting top competitors in some of the best surf spots around the globe. A small number of people make a career out of surfing by receiving corporate sponsorships and performing for photographers and videographers in far-flung destinations; they are typically referred to as freesurfers. Sixty-six surfboarders on a long surfboard set a record in Huntington Beach, California for most people on a surfboard at one time. Dale Webster consecutively surfed for 14,641 days, making it his main life focus.

When the waves were flat, surfers persevered with sidewalk surfing, which is now called skateboarding. Sidewalk surfing has a similar feel to surfing and requires only a paved road or sidewalk. To create the feel of the wave, surfers even sneak into empty backyard swimming pools to ride in, known as pool skating. Eventually, surfing made its way to the slopes with the invention of the Snurfer, later credited as the first snowboard. Many other board sports have been invented over the years, but all can trace their heritage back to surfing.

Many surfers claim to have a spiritual connection with the ocean, describing surfing, the surfing experience, both in and out of the water, as a type of spiritual experience or a religion.

Standup surfing begins when the surfer paddles toward shore in an attempt to match the speed of the wave (the same applies whether the surfer is standup paddling, bodysurfing, boogie-boarding or using some other type of watercraft, such as a waveski or kayak). Once the wave begins to carry the surfer forward, the surfer stands up and proceeds to ride the wave. The basic idea is to position the surfboard so it is just ahead of the breaking part (whitewash) of the wave. A common problem for beginners is being able to catch the wave at all.

Surfers' skills are tested by their ability to control their board in difficult conditions, riding challenging waves, and executing maneuvers such as strong turns and cutbacks (turning board back to the breaking wave) and "carving" (a series of strong back-to-back maneuvers). More advanced skills include the "floater" (riding on top of the breaking curl of the wave), and "off the lip" (banking off the breaking wave). A newer addition to surfing is the progression of the "air" whereby a surfer propels off the wave entirely up into the air, and then successfully lands the board back on the wave.

The tube ride is considered to be the ultimate maneuver in surfing. As a wave breaks, if the conditions are ideal, the wave will break in an orderly line from the middle to the shoulder, enabling the experienced surfer to position themselves inside the wave as it is breaking. This is known as a tube ride. Viewed from the shore, the tube rider may disappear from view as the wave breaks over the rider's head. The longer the surfer remains in the tube, the more successful the ride. This is referred to as getting tubed, barrelled, shacked or pitted. Some of the world's best known waves for tube riding include Pipeline on the North shore of Oahu, Teahupoo in Tahiti and G-Land in Java. Other names for the tube include "the barrel", and "the pit".

Hanging ten and hanging five are moves usually specific to long boarding. Hanging Ten refers to having both feet on the front end of the board with all of the surfer's toes off the edge, also known as nose-riding. Hanging Five is having just one foot near the front, with five toes off the edge.

Cutback: Generating speed down the line and then turning back to reverse direction.

Floater: Suspending the board atop the wave. Very popular on small waves.

Top-Turn: Turn off the top of the wave. Sometimes used to generate speed and sometimes to shoot spray.

Bottom Turn: A turn at the bottom or mid face of the wave, this manuever is used to set up other manuevers such as the top turn, cutback and even aerials. 

Airs/Aerials: These maneuvers have been becoming more and more prevalent in the sport in both competition and free surfing. An air is when the surfer can achieve enough speed and approach a certain type of section of a wave that is supposed to act as a ramp and launch the surfer above the lip line of the wave, “catching air”, and landing either in the transition of the wave or the whitewash when hitting a close-out section.

Airs can either be straight airs or rotational airs. Straight airs have minimal rotation if any, but definitely no more rotation than 90 degrees. Rotational airs require a rotation of 90 degrees or more depending on the level of the surfer.

Types of rotations:


The Glossary of surfing includes some of the extensive vocabulary used to describe various aspects of the sport of surfing as described in literature on the subject. In some cases terms have spread to a wider cultural use. These terms were originally coined by people who were directly involved in the sport of surfing.

Many popular surfing destinations have surf schools and surf camps that offer lessons. Surf camps for beginners and intermediates are multi-day lessons that focus on surfing fundamentals. They are designed to take new surfers and help them become proficient riders. All-inclusive surf camps offer overnight accommodations, meals, lessons and surfboards. Most surf lessons begin with instruction and a safety briefing on land, followed by instructors helping students into waves on longboards or "softboards". The softboard is considered the ideal surfboard for learning, due to the fact it is safer, and has more paddling speed and stability than shorter boards. Funboards are also a popular shape for beginners as they combine the volume and stability of the longboard with the manageable size of a smaller surfboard. New and inexperienced surfers typically learn to catch waves on softboards around the funboard size. Due to the softness of the surfboard the chance of getting injured is substantially minimized.

Typical surfing instruction is best performed one-on-one, but can also be done in a group setting. The most popular surf locations offer perfect surfing conditions for beginners, as well as challenging breaks for advanced students. The ideal conditions for learning would be small waves that crumble and break softly, as opposed to the steep, fast-peeling waves desired by more experienced surfers. When available, a sandy seabed is generally safer.

Surfing can be broken into several skills: Paddling strength, Positioning to catch the wave, timing, and balance. Paddling out requires strength, but also the mastery of techniques to break through oncoming waves ("duck diving", "eskimo roll"). Take-off positioning requires experience at predicting the wave set and where they will break. The surfer must pop up quickly as soon as the wave starts pushing the board forward. Preferred positioning on the wave is determined by experience at reading wave features including where the wave is breaking. Balance plays a crucial role in standing on a surfboard. Thus, balance training exercises are a good preparation. Practicing with a Balance board or swing boarding helps novices master the art.

The repetitive cycle of paddling, popping up, and balancing requires stamina, explosivity, and near-constant core stabilization. Having a proper warm up routine can help prevent injuries.

Surfing can be done on various equipment, including surfboards, longboards, stand up paddle boards (SUPs), bodyboards, wave skis, skimboards, kneeboards, surf mats and macca's trays. Surfboards were originally made of solid wood and were large and heavy (often up to long and having a mass of ). Lighter balsa wood surfboards (first made in the late 1940s and early 1950s) were a significant improvement, not only in portability, but also in increasing maneuverability.

Most modern surfboards are made of fiberglass foam (PU), with one or more wooden strips or "stringers", fiberglass cloth, and polyester resin (PE). An emerging board material is epoxy resin and Expanded Polystyrene foam (EPS) which is stronger and lighter than traditional PU/PE construction. Even newer designs incorporate materials such as carbon fiber and variable-flex composites in conjunction with fiberglass and epoxy or polyester resins. Since epoxy/EPS surfboards are generally lighter, they will float better than a traditional PU/PE board of similar size, shape and thickness. This makes them easier to paddle and faster in the water. However, a common complaint of EPS boards is that they do not provide as much feedback as a traditional PU/PE board. For this reason, many advanced surfers prefer that their surfboards be made from traditional materials.

Other equipment includes a leash (to stop the board from drifting away after a wipeout, and to prevent it from hitting other surfers), surf wax, traction pads (to keep a surfer's feet from slipping off the deck of the board), and fins (also known as "skegs") which can either be permanently attached ("glassed-on") or interchangeable. Sportswear designed or particularly suitable for surfing may be sold as "boardwear" (the term is also used in snowboarding). In warmer climates, swimsuits, surf trunks or boardshorts are worn, and occasionally rash guards; in cold water surfers can opt to wear wetsuits, boots, hoods, and gloves to protect them against lower water temperatures. A newer introduction is a rash vest with a thin layer of titanium to provide maximum warmth without compromising mobility. In recent years, there have been advancements in technology that have allowed surfers to pursue even bigger waves with added elements of safety. Big wave surfers are now experimenting with inflatable vests or colored dye packs to help decrease their odds of drowning.

There are many different surfboard sizes, shapes, and designs in use today. Modern longboards, generally in length, are reminiscent of the earliest surfboards, but now benefit from modern innovations in surfboard shaping and fin design. Competitive longboard surfers need to be competent at traditional "walking" manoeuvres, as well as the short-radius turns normally associated with shortboard surfing. The modern shortboard began life in the late 1960s and has evolved into today's common "thruster" style, defined by its three fins, usually around in length. The thruster was invented by Australian shaper Simon Anderson.

Midsize boards, often called funboards, provide more maneuverability than a longboard, with more flotation than a shortboard. While many surfers find that funboards live up to their name, providing the best of both surfing modes, others are critical.

There are also various niche styles, such as the "Egg", a longboard-style short board targeted for people who want to ride a shortboard but need more paddle power. The "Fish", a board which is typically shorter, flatter, and wider than a normal shortboard, often with a split tail (known as a "swallow tail"). The Fish often has two or four fins and is specifically designed for surfing smaller waves. For big waves there is the "Gun", a long, thick board with a pointed nose and tail (known as a pin tail) specifically designed for big waves.

The physics of surfing involves the physical oceanographic properties of wave creation in the surf zone, the characteristics of the surfboard, and the surfer's interaction with the water and the board.

Ocean waves are defined as a collection of dislocated water parcels that undergo a cycle of being forced past their normal position and being restored back to their normal position. Wind caused ripples and eddies form waves that gradually gain speed and distance (fetch). Waves increase in energy and speed, and then become longer and stronger. The fully developed sea has the strongest wave action that experiences storms lasting 10-hours and creates 15 meter wave heights in the open ocean.

The waves created in the open ocean are classified as deep-water waves. Deep-water waves have no bottom interaction and the orbits of these water molecules are circular; their wavelength is short relative to water depth and the velocity decays before the reaching the bottom of the water basin. Deep waves have depths greater than ½ their wavelengths. Wind forces waves to break in the deep sea.

Deep-water waves travel to shore and become shallow water waves. Shallow water waves have depths less than ½ of their wavelength. Shallow wave's wavelengths are long relative to water depth and have elliptical orbitals. The wave velocity effects the entire water basin. The water interacts with the bottom as it approaches shore and has a drag interaction. The drag interaction pulls on the bottom of the wave, causes refraction, increases the height, decreases the celerity (or the speed of the wave form), and the top (crest) falls over. This phenomenon happens because the velocity of the top of the wave is greater than the velocity of the bottom of the wave.

The surf zone is place of convergence of multiple waves types creating complex wave patterns. A wave suitable for surfing results from maximum speeds of 5 meters per second. This speed is relative because local onshore winds can cause waves to break. In the surf zone, shallow water waves are carried by global winds to the beach and interact with local winds to make surfing waves.

Different onshore and off-shore wind patterns in the surf zone create different types of waves. Onshore winds cause random wave breaking patterns and are more suitable for experienced surfers. Light offshore winds create smoother waves, while strong direct offshore winds cause plunging or large barrel waves. Barrel waves are large because the water depth is small when the wave breaks. Thus, the breaker intensity (or force) increases, and the wave speed and height increase. Off-shore winds produce non-surfable conditions by flattening a weak swell. Weak swell is made from surface gravity forces and has long wavelengths.

Surfing waves can be analyzed using the following parameters: breaking wave height, wave peel angle (α), wave breaking intensity, and wave section length. The breaking wave height has two measurements, the relative heights estimated by surfers and the exact measurements done by physical oceanographers. Measurements done by surfers were 1.36 to 2.58 times higher than the measurements done by scientists. The scientifically concluded wave heights that are physically possible to surf are 1 to 20 meters.

The wave peel angle is one of the main constituents of a potential surfing wave. Wave peel angle measures the distance between the peel-line and the line tangent to the breaking crest line. This angle controls the speed of the wave crest. The speed of the wave is an addition of the propagation velocity vector (Vw) and peel velocity vector (Vp), which results in the overall velocity of the wave (Vs).

Wave breaking intensity measures the force of the wave as it breaks, spills, or plunges (a plunging wave is termed by surfers as a "barrel wave"). Wave section length is the distance between two breaking crests in a wave set. Wave section length can be hard to measure because local winds, non-linear wave interactions, island sheltering, and swell interactions can cause multifarious wave configurations in the surf zone.

The parameters breaking wave height, wave peel angle (α), and wave breaking intensity, and wave section length are important because they are standardized by past oceanographers who researched surfing; these parameters have been used to create a guide that matches the type of wave formed and the skill level of surfer.

Table 1 shows a relationship of smaller peel angles correlating with a higher skill level of surfer. Smaller wave peel angles increase the velocities of waves. A surfer must know how to react and paddle quickly to match the speed of the wave to catch it. Therefore, more experience is required to catch a low peel angle waves. More experienced surfers can handle longer section lengths, increased velocities, and higher wave heights. Different locations offer different types of surfing conditions for each skill level.

A surf break is an area with an obstruction or an object that causes a wave to break. Surf breaks entail multiple scale phenomena. Wave section creation has micro-scale factors of peel angle and wave breaking intensity. The micro-scale components influence wave height and variations on wave crests. The mesoscale components of surf breaks are the ramp, platform, wedge, or ledge that may be present at a surf break. Macro-scale processes are the global winds that initially produce offshore waves. Types of surf breaks are headlands (point break), beach break, river/estuary entrance bar, reef breaks, and ledge breaks.

A headland or point break interacts with the water by causing refraction around the point or headland. The point absorbs the high frequency waves and long period waves persist, which are easier to surf. Examples of locations that have headland or point break induced surf breaks are Dunedin (New Zealand), Raglan, Malibu (California), Rincon (California), and Kirra (Australia).

A beach break happens where waves break from offshore waves, and onshore sandbars and rips. Wave breaks happen successively at beach breaks. Example locations are Tairua and Aramoana Beach (New Zealand) and the Gold Coast (Australia).

A river or estuary entrance bar creates waves from the ebb tidal delta, sediment outflow, and tidal currents. An ideal estuary entrance bar exists in Whangamata Bar, New Zealand.

A reef break is conducive to surfing because large waves consistently break over the reef. The reef is usually made of coral, and because of this, many injuries occur while surfing reef breaks. However, the waves that are produced by reef breaks are some of the best in the world. Famous reef breaks are present in Padang Padang (Indonesia), Pipeline (Hawaii), Uluwatu (Bali), and Teahupo'o (Tahiti). When surfing a reef break, the depth of the water needs to be considered as surfboards have fins on the bottom of the board.

A ledge break is formed by steep rocks ledges that makes intense waves because the waves travel through deeper water then abruptly reach shallower water at the ledge. Shark Island, Australia is a location with a ledge break. Ledge breaks create difficult surfing conditions, sometimes only allowing body surfing as the only feasible way to confront the waves.

Jetties are added to bodies of water to regulate erosion, preserve navigation channels, and make harbors. Jetties are classified into four different types and have two main controlling variables: the type of delta and the size of the jetty.

The first classification is a type 1 jetty. This type of jetty is significantly longer than the surf zone width and the waves break at the shore end of the jetty. The effect of a Type 1 jetty is sediment accumulation in a wedge formation on the jetty. These waves are large and increase in size as they pass over the sediment wedge formation. An example of a Type 1 jetty is Mission Beach, San Diego, California. This 1000-meter jetty was installed in 1950 at the mouth of Mission Bay. The surf waves happen north of the jetty, are longer waves, and are powerful. The bathymetry of the sea bottom in Mission Bay has a wedge shape formation that causes the waves to refract as they become closer to the jetty. The waves converge constructively after they refract and increase the sizes of the waves.

A type 2 jetty occurs in an ebb tidal delta, a delta transitioning between high and low tide. This area has shallow water, refraction, and a distinctive seabed shapes that creates large wave heights.

An example of a type 2 jetty is called "The Poles" in Atlantic Beach, Florida. Atlantic Beach is known to have flat waves, with exceptions during major storms. However, "The Poles" has larger than normal waves due to a 500-meter jetty that was installed on the south side of the St. Johns. This jetty was built to make a deep channel in the river. It formed a delta at "The Poles". This is special area because the jetty increases wave size for surfing, when comparing pre-conditions and post-conditions of the southern St. Johns River mouth area.

The wave size at "The Poles" depends on the direction of the incoming water. When easterly waters (from 55°) interact with the jetty, they create waves larger than southern waters (from 100°). When southern waves (from 100°) move toward "The Poles", one of the waves breaks north of the southern jetty and the other breaks south of the jetty. This does not allow for merging to make larger waves. Easterly waves, from 55°, converge north of the jetty and unite to make bigger waves.

A type 3 jetty is in an ebb tidal area with an unchanging seabed that has naturally created waves. Examples of a Type 3 jetty occurs in “Southside” Tamarack, Carlsbad, California.

A type 4 jetty is one that no longer functions nor traps sediment. The waves are created from reefs in the surf zone. A type 4 jetty can be found in Tamarack, Carlsbad, California.

Rip currents are fast, narrow currents that are caused by onshore transport within the surf zone and the successive return of the water seaward. The wedge bathymetry makes a convenient and consistent rip current of 5–10 meters that brings the surfers to the “take off point” then out to the beach.

Oceanographers have two theories on rip current formation. The wave interaction model assumes that two edges of waves interact, create differing wave heights, and cause longshore transport of nearshore currents. The Boundary Interaction Model assumes that the topography of the sea bottom causes nearshore circulation and longshore transport; the result of both models is a rip current.

Rip currents can be extremely strong and narrow as they extend out of the surf zone into deeper water, reaching speeds from and up to , which is faster than any human can swim. The water in the jet is sediment rich, bubble rich, and moves rapidly. The rip head of the rip current has long shore movement. Rip currents are common on beaches with mild slopes that experience sizeable and frequent oceanic swell.

The vorticity and inertia of rip currents have been studied. From a model of the vorticity of a rip current done at Scripps Institute of Oceanography, it was found that a fast rip current extends away from shallow water, the vorticity of the current increases, and the width of the current decreases. This model also acknowledges that friction plays a role and waves are irregular in nature. From data from Sector-Scanning Doppler Sonar at Scripps Institute of Oceanography, it was found that rip currents in La Jolla, CA lasted several minutes, reoccurred one to four times per hour, and created a wedge with a 45° arch and a radius 200–400 meters.

A longer surfboard of causes more friction with the water; therefore, it will be slower than a smaller and lighter board with a length of . Longer boards are good for beginners who need help balancing. Smaller boards are good for more experienced surfers who want to have more control and maneuverability.

When practicing the sport of surfing, the surfer paddles out past the wave break to wait for a wave. When a surfable wave arrives, the surfer must paddle extremely fast to match the velocity of the wave so the wave can accelerate him or her.

When the surfer is at wave speed, the surfer must quickly pop up, stay low, and stay toward the front of the wave to become stable and prevent falling as the wave steepens. The acceleration is less toward the front than toward the back. The physics behind the surfing of the wave involves the horizontal acceleration force (F·sinθ) and the vertical force (F·cosθ=mg). Therefore, the surfer should lean forward to gain speed, and lean on the back foot to brake. Also, to increase the length of the ride of the wave, the surfer should travel parallel to the wave crest.


Surfing, like all water sports, carries the inherent risk of drowning. Anyone at any age can learn to surf, but should have at least intermediate swimming skills. Although the board assists a surfer in staying buoyant, it can become separated from the user. A leash, attached to the ankle or knee, can keep a board from being swept away, but does not keep a rider on the board or above water. In some cases, possibly including the drowning of professional surfer Mark Foo, a leash can even be a cause of drowning by snagging on a reef or other object and holding the surfer underwater. By keeping the surfboard close to the surfer during a wipeout, a leash also increases the chances that the board may strike the rider, which could knock him or her unconscious and lead to drowning. A fallen rider's board can become trapped in larger waves, and if the rider is attached by a leash, he or she can be dragged for long distances underwater. Surfers should be careful to remain in smaller surf until they have acquired the advanced skills and experience necessary to handle bigger waves and more challenging conditions. However, even world-class surfers have drowned in extremely challenging conditions.

Under the wrong set of conditions, anything that a surfer's body can come in contact with is a potential hazard, including sand bars, rocks, small ice, reefs, surfboards, and other surfers. Collisions with these objects can sometimes cause injuries such as cuts and scrapes and in rare instances, death.

A large number of injuries, up to 66%, are caused by collision with a surfboard (nose or fins). Fins can cause deep lacerations and cuts, as well as bruising. While these injuries can be minor, they can open the skin to infection from the sea; groups like Surfers Against Sewage campaign for cleaner waters to reduce the risk of infections. Local bugs and disease can be risk factors when surfing around the globe.

Falling off a surfboard or colliding with others is commonly referred to as a "wipeout".

Sea life can sometimes cause injuries and even fatalities. Animals such as sharks, stingrays, Weever fish, seals and jellyfish can sometimes present a danger. Warmer-water surfers often do the "stingray shuffle" as they walk out through the shallows, shuffling their feet in the sand to scare away stingrays that may be resting on the bottom.

Rip currents are water channels that flow away from the shore. Under the wrong circumstances these currents can endanger both experienced and inexperienced surfers. Since a rip current appears to be an area of flat water, tired or inexperienced swimmers or surfers may enter one and be carried out beyond the breaking waves. Although many rip currents are much smaller, the largest rip currents have a width of . The flow of water moving out towards the sea in a rip will be stronger than most swimmers, making swimming back to shore difficult, however, by paddling parallel to the shore, a surfer can easily exit a rip current. Alternatively, some surfers actually ride on a rip current because it is a fast and effortless way to get out beyond the zone of breaking waves.

The seabed can pose a risk for surfers. If a surfer falls while riding a wave, the wave tosses and tumbles the surfer around, often in a downwards direction. At reef breaks and beach breaks, surfers have been seriously injured and even killed, because of a violent collision with the sea bed, the water above which can sometimes be very shallow, especially at beach breaks or reef breaks during low tide. Cyclops, Western Australia, for example, is one of the biggest and thickest reef breaks in the world, with waves measuring up to high, but the reef below is only about below the surface of the water.

A January 2018 study by the University of Exeter called the "Beach Bum Survey" found surfers and bodyboarders to be three times as likely as non-surfers to harbor antibiotic-resistant "E. coli" and four times as likely to harbor other bacteria capable of easily becoming antibiotic resistant. The researchers attributed this to the fact that surfers swallow roughly ten times as much seawater as swimmers.

Surfers should use ear protection such as ear plugs to avoid surfer's ear, inflammation of the ear or other damage. Surfer's ear is where the bone near the ear canal grows after repeated exposure to cold water, making the ear canal narrower. The narrowed canal makes it harder for water to drain from the ear. This can result in pain, infection and sometimes ringing of the ear. If surfer's ear develops it does so after repeated surfing sessions. Yet, damage such as inflammation of the ear can occur after only surfing once. This can be caused by repeatedly falling off the surfboard into the water and having the cold water rush into the ears, which can exert a damaging amount of pressure. Those with sensitive ears should therefore wear ear protection, even if they are not planning to surf very often.

Ear plugs designed for surfers, swimmers and other water athletes are primarily made to keep water out of the ear, thereby letting a protective pocket of air stay inside the ear canal. They can also block cold air, dirt and bacteria. Many designs are made to let sound through, and either float and/or have a leash in case the plug accidentally gets bumped out.

Surfer's eye ("Pterygium (conjunctiva)") is a gradual tissue growth on the cornea of the eye which ultimately can lead to vision loss. The cause of the condition is unclear, but appears to be partly related to long term exposure to UV light, dust and wind exposure. Prevention may include wearing sunglasses and a hat if in an area with strong sunlight. Surfers and other water-sport athletes should therefore wear eye protection that blocks 100% of the UV rays from the water, as is often used by snow-sport athletes. Surf goggles often have a head strap and ventilation to avoid fogging

Users of contact lenses should take extra care, and may consider wearing surfing goggles. Some risks of exposing contact lenses to the elements that can cause eye damage or infections are sand or organisms in the sea water getting between the eye and contact lens, or that lenses might fold.

Surfer's myelopathy is a rare spinal cord injury causing paralysis of the lower extremities, caused by hyperextension of the back. This is due to one of the main blood vessels of the spine becoming kinked, depriving the spinal cord of oxygen. In some cases the paralysis is permanent. Although any activity where the back is arched can cause this condition (i.e. yoga, pilates, etc.), this rare phenomenon has most often been seen in those surfing for the first time. According to DPT Sergio Florian, some recommendations for preventing myelopathy is proper warm up, limiting the session length and sitting on the board while waiting for waves, rather than lying.




</doc>
<doc id="28202" url="https://en.wikipedia.org/wiki?curid=28202" title="September 24">
September 24





</doc>
<doc id="28203" url="https://en.wikipedia.org/wiki?curid=28203" title="September 25">
September 25





</doc>
<doc id="28204" url="https://en.wikipedia.org/wiki?curid=28204" title="September 29">
September 29





</doc>
<doc id="28207" url="https://en.wikipedia.org/wiki?curid=28207" title="SMS">
SMS

SMS (short message service) is a text messaging service component of most telephone, Internet, and mobile device systems. It uses standardized communication protocols to enable mobile devices to exchange short text messages. An intermediary service can facilitate a text-to-voice conversion to be sent to landlines.

SMS, as used on modern devices, originated from radio telegraphy in radio memo pagers that used standardized phone protocols. These were defined in 1985 as part of the Global System for Mobile Communications (GSM) series of standards.<ref name="GSM 28/85">GSM Doc 28/85 "Services and Facilities to be provided in the GSM System" rev2, June 1985</ref> The first test SMS message was sent in 1992 and it commercially rolled out to many cellular networks that decade. SMS became hugely popular worldwide as a way of text communication. By the end of 2010, SMS was the most widely used data application, with an estimated 3.5 billion active users, or about 80% of all mobile phone subscribers.

The protocols allowed users to send and receive messages of up to 160 characters (when entirely alpha-numeric) to and from GSM mobiles. Although most SMS messages are sent from one mobile phone to another, support for the service has expanded to include other mobile technologies, such as ANSI CDMA networks and Digital AMPS.

Mobile marketing, a type of direct marketing, uses SMS. According to a 2014 market research report the global SMS messaging business was estimated to be worth over US$100 billion, accounting for almost 50 percent of all the revenue generated by mobile messaging.

Adding text messaging functionality to mobile devices began in the early 1980s. The first action plan of the CEPT Group GSM was approved in December 1982, requesting that "The services and facilities offered in the public switched telephone networks and public data networks ... should be available in the mobile system." This plan included the exchange of text messages either directly between mobile stations, or transmitted via message handling systems in use at that time.

The SMS concept was developed in the Franco-German GSM cooperation in 1984 by Friedhelm Hillebrand and Bernard Ghillebaert. The GSM is optimized for telephony, since this was identified as its main application. The key idea for SMS was to use this telephone-optimized system, and to transport messages on the signalling paths needed to control the telephone traffic during periods when no signalling traffic existed. In this way, unused resources in the system could be used to transport messages at minimal cost. However, it was necessary to limit the length of the messages to 128 bytes (later improved to 160 seven-bit characters) so that the messages could fit into the existing signalling formats. Based on his personal observations and on analysis of the typical lengths of postcard and Telex messages, Hillebrand argued that 160 characters was sufficient for most brief communications.

SMS could be implemented in every mobile station by updating its software. Hence, a large base of SMS-capable terminals and networks existed when people began to use SMS. A new network element required was a specialized short message service centre, and enhancements were required to the radio capacity and network transport infrastructure to accommodate growing SMS traffic.

The technical development of SMS was a multinational collaboration supporting the framework of standards bodies. Through these organizations the technology was made freely available to the whole world.

The first proposal which initiated the development of SMS was made by a contribution of Germany and France in the GSM group meeting in February 1985 in Oslo. This proposal was further elaborated in GSM subgroup WP1 Services (Chairman Martine Alvernhe, France Telecom) based on a contribution from Germany. There were also initial discussions in the subgroup WP3 network aspects chaired by Jan Audestad (Telenor). The result was approved by the main GSM group in a June 1985 document which was distributed to industry. The input documents on SMS had been prepared by Friedhelm Hillebrand of Deutsche Telekom, with contributions from Bernard Ghillebaert of France Télécom. The definition that Friedhelm Hillebrand and Bernard Ghillebaert brought into GSM called for the provision of a message transmission service of alphanumeric messages to mobile users "with acknowledgement capabilities". The last three words transformed SMS into something much more useful than the electronic paging services used at the time that some in GSM might have had in mind.

SMS was considered in the main GSM group as a possible service for the new digital cellular system. In GSM document ""Services and Facilities to be provided in the GSM System,"" both mobile-originated and mobile-terminated short messages appear on the table of GSM teleservices.

The discussions on the GSM services were concluded in the recommendation GSM 02.03 ""TeleServices supported by a GSM PLMN."" Here a rudimentary description of the three services was given:


The material elaborated in GSM and its WP1 subgroup was handed over in Spring 1987 to a new GSM body called IDEG (the Implementation of Data and Telematic Services Experts Group), which had its kickoff in May 1987 under the chairmanship of Friedhelm Hillebrand (German Telecom). The technical standard known today was largely created by IDEG (later WP4) as the two recommendations GSM 03.40 (the two point-to-point services merged) and GSM 03.41 (cell broadcast).

WP4 created a Drafting Group Message Handling (DGMH), which was responsible for the specification of SMS. Finn Trosby of Telenor chaired the draft group through its first 3 years, in which the design of SMS was established. DGMH had five to eight participants, and Finn Trosby mentions as major contributors Kevin Holley, Eija Altonen, Didier Luizard and Alan Cox. The first action plan mentions for the first time the Technical Specification 03.40 "Technical Realisation of the Short Message Service". Responsible editor was Finn Trosby. The first and very rudimentary draft of the technical specification was completed in November 1987. However, drafts useful for the manufacturers followed at a later stage in the period. A comprehensive description of the work in this period is given in.

The work on the draft specification continued in the following few years, where Kevin Holley of Cellnet (now Telefónica O2 UK) played a leading role. Besides the completion of the main specification GSM 03.40, the detailed protocol specifications on the system interfaces also needed to be completed.

The Mobile Application Part (MAP) of the SS7 protocol included support for the transport of Short Messages through the Core Network from its inception. MAP Phase 2 expanded support for SMS by introducing a separate operation code for Mobile Terminated Short Message transport. Since Phase 2, there have been no changes to the Short Message operation packages in MAP, although other operation packages have been enhanced to support CAMEL SMS control.

From 3GPP Releases 99 and 4 onwards, CAMEL Phase 3 introduced the ability for the Intelligent Network (IN) to control aspects of the Mobile Originated Short Message Service, while CAMEL Phase 4, as part of 3GPP Release 5 and onwards, provides the IN with the ability to control the Mobile Terminated service. CAMEL allows the gsmSCP to block the submission (MO) or delivery (MT) of Short Messages, route messages to destinations other than that specified by the user, and perform real-time billing for the use of the service. Prior to standardized CAMEL control of the Short Message Service, IN control relied on switch vendor specific extensions to the Intelligent Network Application Part (INAP) of SS7.

The first SMS message was sent over the Vodafone GSM network in the United Kingdom on 3 December 1992, from Neil Papworth of Sema Group (now Mavenir Systems) using a personal computer to Richard Jarvis of Vodafone using an Orbitel 901 handset. The text of the message was "Merry Christmas."

The first commercial deployment of a short message service center (SMSC) was by Aldiscon part of Logica (now part of CGI) with Telia (now TeliaSonera) in Sweden in 1993, followed by Fleet Call (now Nextel) in the US, Telenor in Norway and BT Cellnet (now O2 UK) later in 1993. All first installations of SMS gateways were for network notifications sent to mobile phones, usually to inform of voice mail messages.

The first commercially sold SMS service was offered to consumers, as a person-to-person text messaging service by Radiolinja (now part of Elisa) in Finland in 1993. Most early GSM mobile phone handsets did not support the ability to send SMS text messages, and Nokia was the only handset manufacturer whose total GSM phone line in 1993 supported user-sending of SMS text messages. According to Matti Makkonen, an engineer at Nokia at the time, the Nokia 2010, which was released in January 1994, was the first mobile phone to support composing SMSes easily.

Initial growth was slow, with customers in 1995 sending on average only 0.4 messages per GSM customer per month. One factor in the slow takeup of SMS was that operators were slow to set up charging systems, especially for prepaid subscribers, and eliminate billing fraud which was possible by changing SMSC settings on individual handsets to use the SMSCs of other operators. Initially, networks in the UK only allowed customers to send messages to other users on the same network, limiting the usefulness of the service. This restriction was lifted in 1999.

Over time, this issue was eliminated by switch billing instead of billing at the SMSC and by new features within SMSCs to allow blocking of foreign mobile users sending messages through it. By the end of 2000, the average number of messages reached 35 per user per month, and on Christmas Day 2006, over 205 million messages were sent in the UK alone.

SMS was originally designed as part of GSM, but is now available on a wide range of networks, including 3G networks. However, not all text messaging systems use SMS, and some notable alternative implementations of the concept include J-Phone's "SkyMail" and NTT Docomo's "Short Mail", both in Japan. Email messaging from phones, as popularized by NTT Docomo's i-mode and the RIM BlackBerry, also typically uses standard mail protocols such as SMTP over TCP/IP.

, 6.1 trillion (6.1 × 10) SMS text messages were sent, which is an average of 193,000 SMS per second. SMS has become a large commercial industry, earning $114.6 billion globally in 2010. The global average price for an SMS message is US$0.11, while mobile networks charge each other interconnect fees of at least US$0.04 when connecting between different phone networks.

In 2015, the actual cost of sending an SMS in Australia was found to be $0.00016 per SMS.

In 2014, Caktus Group developed the world's first SMS-based voter registration system in Libya. So far, more than 1.5 million people have registered using that system, providing Libyan voters with unprecedented access to the democratic process.

While SMS is still a growing market, it is being increasingly challenged by Internet Protocol-based messaging services such as Apple's iMessage, Facebook Messenger, WhatsApp, Viber, WeChat (in China) and Line (in Japan), available on smart phones with data connections. It has been reported that over 97% of smart phone owners use alternative messaging services at least once a day. However, in the U.S. these Internet-based services have not caught on as much, and SMS continues to be highly popular there.

SMS enablement allows individuals to send an SMS message to a business phone number (traditional landline) and receive a SMS in return. Providing customers with the ability to text to a phone number allows organizations to offer new services that deliver value. Examples include chat bots, and text enabled customer service and call centers.

The "Short Message Service—Point to Point (SMS-PP)"—was originally defined in GSM recommendation 03.40, which is now maintained in 3GPP as TS 23.040. GSM 03.41 (now 3GPP TS 23.041) defines the "Short Message Service—Cell Broadcast (SMS-CB)", which allows messages (advertising, public information, etc.) to be broadcast to all mobile users in a specified geographical area.

Messages are sent to a short message service center (SMSC), which provides a "store and forward" mechanism. It attempts to send messages to the SMSC's recipients. If a recipient is not reachable, the SMSC queues the message for later retry. Some SMSCs also provide a "forward and forget" option where transmission is tried only once. Both mobile terminated (MT, for messages sent "to" a mobile handset) and mobile originating (MO, for those sent "from" the mobile handset) operations are supported. Message delivery is "best effort", so there are no guarantees that a message will actually be delivered to its recipient, but delay or complete loss of a message is uncommon, typically affecting less than 5 percent of messages. Some providers allow users to request delivery reports, either via the SMS settings of most modern phones, or by prefixing each message with *0# or *N#. However, the exact meaning of confirmations varies from reaching the network, to being queued for sending, to being sent, to receiving a confirmation of receipt from the target device, and users are often not informed of the specific type of success being reported.

SMS is a stateless communication protocol in which every SMS message is considered entirely independent of other messages. Enterprise applications using SMS as a communication channel for stateful dialogue (where an MO reply message is paired to a specific MT message) requires that session management be maintained external to the protocol.

Transmission of short messages between the SMSC and the handset is done whenever using the Mobile Application Part (MAP) of the SS7 protocol. Messages are sent with the MAP MO- and MT-ForwardSM operations, whose payload length is limited by the constraints of the signaling protocol to precisely 140 bytes (140 bytes * 8 bits / byte = 1120 bits). 

Short messages can be encoded using a variety of alphabets: the default GSM 7-bit alphabet, the 8-bit data alphabet, and the 16-bit UCS-2 alphabet. Depending on which alphabet the subscriber has configured in the handset, this leads to the maximum individual short message sizes of 160 7-bit characters, 140 8-bit characters, or 70 16-bit characters. GSM 7-bit alphabet support is mandatory for GSM handsets and network elements, but characters in languages such as Hindi, Arabic, Chinese, Korean, Japanese, or Cyrillic alphabet languages (e.g., Russian, Ukrainian, Serbian, Bulgarian, etc.) must be encoded using the 16-bit UCS-2 character encoding (see Unicode). Routing data and other metadata is additional to the payload size.

Larger content (concatenated SMS, multipart or segmented SMS, or "long SMS") can be sent using multiple messages, in which case each message will start with a User Data Header (UDH) containing segmentation information. Since UDH is part of the payload, the number of available characters per segment is lower: 153 for 7-bit encoding, 134 for 8-bit encoding and 67 for 16-bit encoding. The receiving handset is then responsible for reassembling the message and presenting it to the user as one long message. While the standard theoretically permits up to 255 segments, 10 segments is the practical maximum with some carriers, and long messages are often billed as equivalent to multiple SMS messages. Some providers have offered length-oriented pricing schemes for messages, although that type of pricing structure is rapidly disappearing.

SMS gateway providers facilitate SMS traffic between businesses and mobile subscribers, including SMS for enterprises, content delivery, and entertainment services involving SMS, e.g. TV voting. Considering SMS messaging performance and cost, as well as the level of messaging services, SMS gateway providers can be classified as aggregators or SS7 providers.

The aggregator model is based on multiple agreements with mobile carriers to exchange two-way SMS traffic into and out of the operator's SMSC, also known as "local termination model". Aggregators lack direct access into the SS7 protocol, which is the protocol where the SMS messages are exchanged. SMS messages are delivered to the operator's SMSC, but not the subscriber's handset; the SMSC takes care of further handling of the message through the SS7 network.

Another type of SMS gateway provider is based on SS7 connectivity to route SMS messages, also known as "international termination model". The advantage of this model is the ability to route data directly through SS7, which gives the provider total control and visibility of the complete path during SMS routing. This means SMS messages can be sent directly to and from recipients without having to go through the SMSCs of other mobile operators. Therefore, it is possible to avoid delays and message losses, offering full delivery guarantees of messages and optimized routing. This model is particularly efficient when used in mission-critical messaging and SMS used in corporate communications. Moreover, these SMS gateway providers are providing branded SMS services with masking but after misuse of these gateways most countries's Governments have taken serious steps to block these gateways.

Message Service Centers communicate with the Public Land Mobile Network (PLMN) or PSTN via Interworking and Gateway MSCs.

Subscriber-originated messages are transported from a handset to a service center, and may be destined for mobile users, subscribers on a fixed network, or Value-Added Service Providers (VASPs), also known as application-terminated. Subscriber-terminated messages are transported from the service center to the destination handset, and may originate from mobile users, from fixed network subscribers, or from other sources such as VASPs.

On some carriers nonsubscribers can send messages to a subscriber's phone using an Email-to-SMS gateway. Additionally, many carriers, including AT&T Mobility, T-Mobile USA, Sprint, and Verizon Wireless, offer the ability to do this through their respective web sites.

For example, an AT&T subscriber whose phone number was 555-555-5555 would receive e-mails addressed to 5555555555@txt.att.net as text messages. Subscribers can easily reply to these SMS messages, and the SMS reply is sent back to the original email address. Sending email to SMS is free for the sender, but the recipient is subject to the standard delivery charges. Only the first 160 characters of an email message can be delivered to a phone, and only 160 characters can be sent from a phone. However, longer messages may be broken up into multiple texts, depending upon the telephone service provider.

Text-enabled fixed-line handsets are required to receive messages in text format. However, messages can be delivered to nonenabled phones using text-to-speech conversion.

Short messages can send binary content such as ringtones or logos, as well as Over-the-air programming (OTA) or configuration data. Such uses are a vendor-specific extension of the GSM specification and there are multiple competing standards, although Nokia's Smart Messaging is common. An alternative way for sending such binary content is EMS messaging, which is standardized and not dependent on vendors.

SMS is used for M2M (Machine to Machine) communication. For instance, there is an LED display machine controlled by SMS, and some vehicle tracking companies use SMS for their data transport or telemetry needs. SMS usage for these purposes is slowly being superseded by GPRS services owing to their lower overall cost. GPRS is offered by smaller telco players as a route of sending SMS text to reduce the cost of SMS texting internationally.

Many mobile and satellite transceiver units support the sending and receiving of SMS using an extended version of the Hayes command set. The extensions were standardised as part of the GSM Standards and extended as part of the 3GPP standards process.

The connection between the terminal equipment and the transceiver can be realized with a serial cable (e.g., USB), a Bluetooth link, an infrared link, etc. Common AT commands include AT+CMGS (send message), AT+CMSS (send message from storage), AT+CMGL (list messages) and AT+CMGR (read message).

However, not all modern devices support receiving of messages if the message storage (for instance the device's internal memory) is not accessible using AT commands.

Short messages may be used normally to provide premium rate services to subscribers of a telephone network.

Mobile-terminated short messages can be used to deliver digital content such as news alerts, financial information, logos, and ring tones. The first premium-rate media content delivered via the SMS system was the world's first paid downloadable ringing tones, as commercially launched by Saunalahti (later Jippii Group, now part of Elisa Group), in 1998. Initially, only Nokia branded phones could handle them. By 2002 the ringtone business globally had exceeded $1 billion of service revenues, and nearly US$5 billion by 2008. Today, they are also used to pay smaller payments online—for example, for file-sharing services, in mobile application stores, or VIP section entrance. Outside the online world, one can buy a bus ticket or beverages from ATM, pay a parking ticket, order a store catalog or some goods (e.g., discount movie DVDs), make a donation to charity, and much more.

Premium-rated messages are also used in Donors Message Service to collect money for charities and foundations. DMS was first launched at April 1, 2004, and is very popular in the Czech Republic. For example, the Czech people sent over 1.5 million messages to help South Asia recover from the 2004 Indian Ocean earthquake and tsunami.

The Value-added service provider (VASP) providing the content submits the message to the mobile operator's SMSC(s) using an TCP/IP protocol such as the short message peer-to-peer protocol (SMPP) or the External Machine Interface (EMI). The SMSC delivers the text using the normal Mobile Terminated delivery procedure. The subscribers are charged extra for receiving this premium content; the revenue is typically divided between the mobile network operator and the VASP either through revenue share or a fixed transport fee. Submission to the SMSC is usually handled by a third party.

Mobile-originated short messages may also be used in a premium-rated manner for services such as televoting. In this case, the VASP providing the service obtains a short code from the telephone network operator, and subscribers send texts to that number. The payouts to the carriers vary by carrier; percentages paid are greatest on the lowest-priced premium SMS services. Most information providers should expect to pay about 45 percent of the cost of the premium SMS up front to the carrier. The submission of the text to the SMSC is identical to a standard MO Short Message submission, but once the text is at the SMSC, the Service Center (SC) identifies the Short Code as a premium service. The SC will then direct the content of the text message to the VASP, typically using an IP protocol such as SMPP or EMI. Subscribers are charged a premium for the sending of such messages, with the revenue typically shared between the network operator and the VASP. Short codes only work within one country, they are not international.

An alternative to inbound SMS is based on long numbers (international number format, such as "+44 762 480 5000"), which can be used in place of short codes for SMS reception in several applications, such as TV voting, product promotions and campaigns. Long numbers work internationally, allow businesses to use their own numbers, rather than short codes, which are usually shared across many brands. Additionally, long numbers are nonpremium inbound numbers.

Threaded SMS is a visual styling orientation of SMS message history that arranges messages to and from a contact in chronological order on a single screen. 

It was first invented by a developer working to implement the SMS client for the BlackBerry, who was looking to make use of the blank screen left below the message on a device with a larger screen capable of displaying far more than the usual 160 characters, and was inspired by threaded Reply conversations in email. 

Visually, this style of representation provides a back-and-forth chat-like history for each individual contact. Hierarchical-threading at the conversation-level (as typical in blogs and on-line messaging boards) is not widely supported by SMS messaging clients. This limitation is due to the fact that there is no session identifier or subject-line passed back and forth between sent and received messages in the header data (as specified by SMS protocol) from which the client device can properly thread an incoming message to a specific dialogue, or even to a specific message within a dialogue. 

Most smart phone text-messaging-clients are able to create some contextual threading of "group messages" which narrows the context of the thread around the common interests shared by group members. On the other hand, advanced enterprise messaging applications which push messages from a remote server often display a dynamically changing reply number (multiple numbers used by the same sender), which is used along with the sender's phone number to create session-tracking capabilities analogous to the functionality that cookies provide for web-browsing. As one pervasive example, this technique is used to extend the functionality of many Instant Messenger (IM) applications such that they are able to communicate over two-way dialogues with the much larger SMS user-base. In cases where multiple reply numbers are used by the enterprise server to maintain the dialogue, the visual conversation threading on the client may be separated into multiple threads.

While SMS reached its popularity as a person-to-person messaging, another type of SMS is growing fast: application-to-person (A2P) messaging. A2P is a type of SMS sent from a subscriber to an application or sent from an application to a subscriber. It is commonly used by businesses, such as banks, to send SMS messages from their systems to their customers.

In the US, carriers have traditionally preferred that A2P messages must be sent using a short code rather than a standard long code. However, recently multiple US carriers, including Verizon have announced plans to officially support A2P messages over long codes. In the United Kingdom A2P messages can be sent with a dynamic 11 character sender ID; however, short codes are used for OPTOUT commands. There are specialist companies such as MMG Mobile Marketing Group which provide these services to businesses and enterprises.

All commercial satellite phone networks except ACeS and OptusSat support SMS. While early Iridium handsets only support incoming SMS, later models can also send messages. The price per message varies for different networks. Unlike some mobile phone networks, there is no extra charge for sending international SMS or to send one to a different satellite phone network. SMS can sometimes be sent from areas where the signal is too poor to make a voice call.

Satellite phone networks usually have web-based or email-based SMS portals where one can send free SMS to phones on that particular network.

Unlike dedicated texting systems like the Simple Network Paging Protocol and Motorola's ReFLEX protocol, SMS message delivery is not guaranteed, and many implementations provide no mechanism through which a sender can determine whether an SMS message has been delivered in a timely manner. SMS messages are generally treated as lower-priority traffic than voice, and various studies have shown that around 1% to 5% of messages are lost entirely, even during normal operation conditions, and others may not be delivered until long after their relevance has passed. The use of SMS as an emergency notification service in particular has been questioned.

The Global Service for Mobile communications (GSM), with the greatest worldwide number of users, succumbs to several security vulnerabilities. In the GSM, only the airway traffic between the Mobile Station (MS) and the Base Transceiver Station (BTS) is optionally encrypted with a weak and broken stream cipher (A5/1 or A5/2). The authentication is unilateral and also vulnerable. There are also many other security vulnerabilities and shortcomings. Such vulnerabilities are inherent to SMS as one of the superior and well-tried services with a global availability in the GSM networks. SMS messaging has some extra security vulnerabilities due to its store-and-forward feature, and the problem of fake SMS that can be conducted via the Internet. When a user is roaming, SMS content passes through different networks, perhaps including the Internet, and is exposed to various vulnerabilities and attacks. Another concern arises when an adversary gets access to a phone and reads the previous unprotected messages.

In October 2005, researchers from Pennsylvania State University published an analysis of vulnerabilities in SMS-capable cellular networks. The researchers speculated that attackers might exploit the open functionality of these networks to disrupt them or cause them to fail, possibly on a nationwide scale.

The GSM industry has identified a number of potential fraud attacks on mobile operators that can be delivered via abuse of SMS messaging services. The most serious threat is SMS Spoofing, which occurs when a fraudster manipulates address information in order to impersonate a user that has roamed onto a foreign network and is submitting messages to the home network. Frequently, these messages are addressed to destinations outside the home network—with the home SMSC essentially being "hijacked" to send messages into other networks.

The only sure way of detecting and blocking spoofed messages is to screen incoming mobile-originated messages to verify that the sender is a valid subscriber and that the message is coming from a valid and correct location. This can be implemented by adding an intelligent routing function to the network that can query originating subscriber details from the home location register (HLR) before the message is submitted for delivery. This kind of intelligent routing function is beyond the capabilities of legacy messaging infrastructure.

In an effort to limit telemarketers who had taken to bombarding users with hordes of unsolicited messages, India introduced new regulations in September 2011, including a cap of 3,000 SMS messages per subscriber per month, or an average of 100 per subscriber per day. Due to representations received from some of the service providers and consumers, TRAI (Telecom Regulatory Authority of India) has raised this limit to 200 SMS messages per SIM per day in case of prepaid services, and up to 6,000 SMS messages per SIM per month in case of postpaid services with effect from 1 November 2011. However, it was ruled unconstitutional by the Delhi high court, but there are some limitations.

A Flash SMS is a type of SMS that appears directly on the main screen without user interaction and is not automatically stored in the inbox. It can be useful in emergencies, such as a fire alarm or cases of confidentiality, as in delivering one-time passwords.

In Germany in 2010 almost half a million "silent SMS" messages were sent by the federal police, customs and the secret service "Verfassungsschutz" (offices for protection of the constitution). These silent messages, also known as "silent TMS", "stealth SMS", "stealth ping" or "Short Message Type 0", are used to locate a person and thus to create a complete movement profile. They do not show up on a display, nor trigger any acoustical signal when received. Their primary purpose was to deliver special services of the network operator to any cell phone.



</doc>
<doc id="28208" url="https://en.wikipedia.org/wiki?curid=28208" title="Santa Monica, California">
Santa Monica, California

Santa Monica () is a beachfront city in western Los Angeles County, California, United States. Situated on Santa Monica Bay, it is bordered on three sides by different neighborhoods of the city of Los Angeles: Pacific Palisades to the north, Brentwood on the northeast, West Los Angeles on the east, Mar Vista on the southeast, and Venice on the south. The 2010 U.S. Census population was 89,736. Due to a favorable climate and close proximity to Los Angeles, Santa Monica became a famed resort town by the early 20th century attracting many celebrities, like Marion Davies, to build magnificent beach front homes on Roosevelt Highway (PCH). The city has experienced a boom since the late 1980s through the revitalization of its downtown core, significant job growth and increased tourism. Popular tourists sites include Pacific Park on the Santa Monica Pier and Palisades Park atop a bluff over the Pacific Ocean.

Santa Monica was inhabited by the Tongva people. Santa Monica was called Kecheek in the Tongva language. The first non-indigenous group to set foot in the area was the party of explorer Gaspar de Portolà, who camped near the present-day intersection of Barrington and Ohio Avenues on August 3, 1769. Named after the Christian saint Monica, there are two different accounts of how the city's name came to be. One says it was named in honor of the feast day of Saint Monica (mother of Saint Augustine), but her feast day is May 4. Another version says it was named by Juan Crespí on account of a pair of springs, the Kuruvungna Springs (Serra Springs), that were reminiscent of the tears Saint Monica shed over her son's early impiety.

Following the Mexican–American War, Mexico signed the Treaty of Guadalupe Hidalgo, which gave Mexicans and Californios living in state certain unalienable rights. US government sovereignty in California began on February 2, 1848.
In the 1870s, the Los Angeles and Independence Railroad connected Santa Monica with Los Angeles, and a wharf out into the bay. The first town hall was an 1873 brick building, later a beer hall, and now part of the Santa Monica Hostel. It is Santa Monica's oldest extant structure. By 1885, the town's first hotel was the Santa Monica Hotel.

Amusement piers became popular in the first decades of the 20th century and the extensive Pacific Electric Railroad brought people to the city's beaches from across the Greater Los Angeles Area.

Around the start of the 20th century, a growing population of Asian Americans lived in and around Santa Monica and Venice. A Japanese fishing village was near the Long Wharf while small numbers of Chinese lived or worked in Santa Monica and Venice. The two ethnic minorities were often viewed differently by White Americans who were often well-disposed towards the Japanese but condescending towards the Chinese. The Japanese village fishermen were an integral economic part of the Santa Monica Bay community.
Donald Wills Douglas, Sr. built a plant in 1922 at Clover Field (Santa Monica Airport) for the Douglas Aircraft Company. In 1924, four Douglas-built planes took off from Clover Field to attempt the first aerial circumnavigation of the world. Two planes returned after covering in 175 days, and were greeted on their return September 23, 1924, by a crowd of 200,000. The Douglas Company (later McDonnell Douglas) kept facilities in the city until the 1960s.

The Great Depression hit Santa Monica deeply. One report gives citywide employment in 1933 of just 1,000. Hotels and office building owners went bankrupt. In the 1930s, corruption infected Santa Monica (along with neighboring Los Angeles). The federal Works Project Administration helped build several buildings, most notably City Hall. The main Post Office and Barnum Hall (Santa Monica High School auditorium) were also among other WPA projects.

Douglas's business grew with the onset of World War II, employing as many as 44,000 people in 1943. To defend against air attack, set designers from the Warner Brothers Studios prepared elaborate camouflage that disguised the factory and airfield. The RAND Corporation began as a project of the Douglas Company in 1945, and spun off into an independent think tank on May 14, 1948. RAND acquired a 15-acre (61,000 m) campus across the street from the Civic Center and is still there today.

The completion of the Santa Monica Civic Auditorium in 1958 eliminated Belmar, the first African American community in the city, and the Santa Monica Freeway in 1966 decimated the Pico neighborhood that had been a leading African American enclave on the Westside.

Beach volleyball is believed to have been developed by Duke Kahanamoku in Santa Monica during the 1920s.

Santa Monica has two hospitals: Saint John's Health Center and Santa Monica-UCLA Medical Center. Its cemetery is Woodlawn Memorial.

Santa Monica has several local newspapers including "Santa Monica Daily Press", "Santa Monica Mirror", "Santa Monica Star", and "Santa Monica Observer".

The Santa Monica Looff Hippodrome (carousel) is a National Historic Landmark. It sits on the Santa Monica Pier, which was built in 1909. The La Monica Ballroom on the pier was once the largest ballroom in the US and the source for many New Year's Eve national network broadcasts.

The Santa Monica Civic Auditorium was an important music venue for several decades and hosted the Academy Awards in the 1960s. McCabe's Guitar Shop is a leading acoustic performance space as well as retail outlet. The "Santa Monica Playhouse" is a popular theater in the city.

Bergamot Station is a city-owned art gallery compound that includes the Santa Monica Museum of Art. The city is also home to the California Heritage Museum and the Angels Attic dollhouse and toy museum.

The New West Symphony is the resident orchestra of Barnum Hall. They are also resident orchestra of the Oxnard Performing Arts Center and the Thousand Oaks Civic Arts Plaza.

Santa Monica has three main shopping districts: Montana Avenue on the north side, the Downtown District in the city's core, and Main Street on the south end. Each has its own unique feel and personality. Montana Avenue is a stretch of luxury boutique stores, restaurants, and small offices that generally features more upscale shopping. The Main Street district offers an eclectic mix of clothing, restaurants, and other specialty retail.
The Downtown District is the home of the Third Street Promenade, a major outdoor pedestrian-only shopping district that stretches for three blocks between Wilshire Blvd. and Broadway (not the same Broadway in downtown and south Los Angeles). Third Street is closed to vehicles for those three blocks to allow people to stroll, congregate, shop and enjoy street performers.

Santa Monica Place, featuring Bloomingdale's and Nordstrom in a three-level outdoor environment, is at the Promenade's southern end. After a period of redevelopment, the mall reopened in the fall of 2010 as a modern shopping, entertainment and dining complex with more outdoor space.

Santa Monica hosts the annual Santa Monica Film Festival.

The city's oldest movie theater is the Majestic. Opened in 1912 and also known as the Mayfair Theatre, the theater, it has been closed since the 1994 Northridge earthquake. The Aero Theater (now operated by the American Cinematheque) and Criterion Theater were built in the 1930s and still show movies. The Santa Monica Promenade alone supports more than a dozen movie screens.

Palisades Park stretches out along the crumbling bluffs overlooking the Pacific and is a favorite walking area to view the ocean. It includes public art, a totem pole, camera obscura, benches, picnic areas, pétanque courts, and restrooms.

Tongva Park occupies 6 acres between Ocean Avenue and Main Street, just south of Colorado Avenue. The park includes an overlook, amphitheater, playground, garden, fountains, picnic areas, and restrooms.

The Santa Monica Stairs, a long, steep staircase that leads from north of San Vicente down into Santa Monica Canyon, is a popular spot for outdoor workouts. Some area residents have complained that the stairs have become too popular, and attract too many exercisers to the wealthy neighborhood of multimillion-dollar properties.
The city of Santa Monica rests on a mostly flat slope that angles down towards Ocean Avenue and towards the south. High bluffs separate the north side of the city from the beaches. Santa Monica borders the L.A. neighborhoods of Pacific Palisades to the north and Venice to the south. To the west, Santa Monica has the 3-mile coastline fronting the Santa Monica Bay, and to the east of the city borders are the Los Angeles communities of West Los Angeles and Brentwood.

Santa Monica has a coastal Mediterranean climate (Köppen "Csb"). Santa Monica enjoys an average of 310 days of sunshine a year. It is in USDA plant hardiness zone 10a. Because of its location, nestled on the vast and open Santa Monica Bay, morning fog is a common phenomenon in May, June, July and early August (caused by ocean temperature variations and currents). Like other inhabitants of the greater Los Angeles area, residents have a particular terminology for this phenomenon: the "May Gray", the "June Gloom" and even “Fogust”. Overcast skies are common during June mornings, but usually the strong sun burns the fog off by noon. In the late winter/early summer, daily fog is a phenomenon too. It happens suddenly and it may last some hours or past sunset time. Nonetheless, it will sometimes stay cloudy and cool all day during June, even as other parts of the Los Angeles area enjoy sunny skies and warmer temperatures. At times, the sun can be shining east of 20th Street, while the beach area is overcast. As a general rule, the beach temperature is from 5 to 10 degrees Fahrenheit (3 to 6 degrees Celsius) cooler than it is inland during summer days, and 5–10 degrees warmer during winter nights.

It is also in September highest temperatures tend to be reached. It is winter, however, when the hot, dry winds of the Santa Anas are most common. In contrast, temperatures exceeding 10 degrees below average are rare.

The rainy season is from late October through late March. Winter storms usually approach from the northwest and pass quickly through the Southland. There is very little rain during the rest of the year. Yearly rainfall totals are unpredictable as rainy years are occasionally followed by droughts. There has never been any snow or frost, but there has been hail.

Santa Monica usually enjoys cool breezes blowing in from the ocean, which tend to keep the air fresh and clean. Therefore, smog is less of a problem for Santa Monica than elsewhere around Los Angeles. However, in the autumn months of September through November, the Santa Ana winds will sometimes blow from the east, bringing smoggy and hot inland air to the beaches.

The city first proposed its Sustainable City Plan in 1992 and in 1994, was one of the first cities in the nation to formally adopt a comprehensive sustainability plan, setting waste reduction and water conservation policies for both public and private sector through its Office of Sustainability and the Environment. Eighty-two percent of the city's public works vehicles run on alternative fuels, including most of the municipal bus system, making it among the largest such fleets in the country. Santa Monica fleet vehicles and buses source their natural gas from Redeem, a Southern California-based supplier of renewable and sustainable natural gas obtained from non-fracked methane biogas generated from organic landfill waste.

Santa Monica adopted a Community Energy Independence Initiative, with a goal of achieving complete energy independence by 2020 (vs. California's already ambitious 33% renewables goal). The city exceeded that aspiration when, in February 2019, it switched over to electricity from the Clean Power Alliance, with a citywide default of 100% renewably sourced energy. That same year, the Santa Monica City Council adopted a Climate Action and Adaptation Plan aimed at achieving an 80% cut in carbon emissions by 2030, and reaching community-wide carbon neutrality by 2050 or sooner.

An urban runoff facility (SMURFF), the first of its kind in the US, catches and treats of water each week that would otherwise flow into the bay via storm-drains and sells it back to end-users within the city for reuse as gray-water, while bioswales throughout the city allow rainwater to percolate into and replenish the groundwater. The groundwater supply plays an important role in the city's Sustainable Water Master Plan, whereby Santa Monica has set a goal of attaining 100% water independence by 2020. The city has numerous programs designed to promote water conservation among residents, including a rebate for those who convert lawns to drought-tolerant gardens that require less water.

Santa Monica has also instituted a green building-code whereby merely constructing to code automatically renders a building equivalent to the US Green Building Council's LEED Silver standards. The city's Main Library is one of many LEED certified or LEED equivalent buildings in the city. It is built over a 200,000 gallon cistern that collects filtered stormwater from the roof. The water is used for landscape irrigation.

Since 2009, Santa Monica has been developing the Zero Waste Strategic Operations Plan by which the city will set a goal of diverting at least 95% of all waste away from landfills, and toward recycling and composting, by 2030. The plan includes a food waste composting program, which diverts 3 million pounds of restaurant food waste away from landfills annually. Currently, 77% of all solid waste produced citywide is diverted from landfills.

The city is also in the process of implementing a 5-year and 20 year Bike Action Plan with a goal of attaining 14 to 35% bicycle transportation mode share by 2030 through the installation of enhanced bicycle infrastructure throughout the city. Other environmentally focused initiatives include curbside recycling, curbside composting bins (in addition to trash, yard-waste, and recycle bins), farmers' markets, community gardens, garden-share, an urban forest initiative, a hazardous materials home-collection service, and a green business certification.

Santa Monica's population has grown from 417 in 1880 to 89,736 in 2010.

The 2010 United States Census reported Santa Monica had a population of 89,736. The population density was 10,662.6 people per square mile (4,116.9/km). The racial makeup of Santa Monica was 69,663 (77.6%) White (70.1% Non-Hispanic White), 3,526 (3.9%) African American, 338 (0.4%) Native American, 8,053 (9.0%) Asian, 124 (0.1%) Pacific Islander, 4,047 (4.5%) from other races, and 3,985 (4.4%) from two or more races. Hispanic or Latino of any race were 11,716 persons (13.1%), with Mexican Americans, Spanish Americans, and Argentine Americans making up 64.2%, 6.4%, and 4.7% of the Hispanic population respectively.

The Census reported 87,610 people (97.6% of the population) lived in households, 1,299 (1.4%) lived in non-institutionalized group quarters, and 827 (0.9%) were institutionalized.

There were 46,917 households, out of which 7,835 (16.7%) had children under the age of 18 living in them, 13,092 (27.9%) were opposite-sex married couples living together, 3,510 (7.5%) had a female householder with no husband present, 1,327 (2.8%) had a male householder with no wife present. There were 2,867 (6.1%) unmarried opposite-sex partnerships, and 416 (0.9%) same-sex married couples or partnerships. 22,716 households (48.4%) were made up of individuals, and 5,551 (11.8%) had someone living alone who was 65 years of age or older. The average household size was 1.87. There were 17,929 families (38.2% of all households); the average family size was 2.79.

The population was spread out, with 12,580 people (14.0%) under the age of 18, 6,442 people (7.2%) aged 18 to 24, 32,552 people (36.3%) aged 25 to 44, 24,746 people (27.6%) aged 45 to 64, and 13,416 people (15.0%) who were 65 years of age or older. The median age was 40.4 years. For every 100 females, there were 93.2 males. For every 100 females age 18 and over, there were 91.2 males.

There were 50,912 housing units at an average density of 6,049.5 per square mile (2,335.7/km), of which 13,315 (28.4%) were owner-occupied, and 33,602 (71.6%) were occupied by renters. The homeowner vacancy rate was 1.1%; the rental vacancy rate was 5.1%. 30,067 people (33.5% of the population) lived in owner-occupied housing units and 57,543 people (64.1%) lived in rental housing units.

According to the 2010 United States Census, Santa Monica had a median household income of $73,649, with 11.2% of the population living below the federal poverty line.

As of the census of 2000, there were 84,084 people, 44,497 households, and 16,775 families in the city. The population density was 10,178.7 inhabitants per square mile (3,930.4/km). There were 47,863 housing units at an average density of 5,794.0 per square mile (2,237.3/km). The racial makeup of the city was 78.29% White, 7.25% Asian, 3.78% African American, 0.47% Native American, 0.10% Pacific Islander, 5.97% from other races, and 4.13% from two or more races. 13.44% of the population were Hispanic or Latino of any race.
There were 44,497 households, out of which 15.8% had children under the age of 18, 27.5% were married couples living together, 7.5% had a female householder with no husband present, and 62.3% were non-families. 51.2% of all households were made up of individuals, and 10.6% had someone living alone who was 65 years of age or older. The average household size was 1.83 and the average family size was 2.80.

The city of Santa Monica is consistently among the most educated cities in the United States, with 23.8 percent of all residents holding graduate degrees.

The population was diverse in age, with 14.6% under 18, 6.1% from 18 to 24, 40.1% from 25 to 44, 24.8% from 45 to 64, and 14.4% 65 years or older. The median age was 39 years. For every 100 females, there were 93.0 males. For every 100 females age 18 and over, there were 91.3 males.

According to a 2009 estimate, the median income for a household in the city was $71,095, and the median income for a family was $109,410. Males had a median income of $55,689 versus $42,948 for females. The per capita income for the city was $42,874. 10.4% of the population and 5.4% of families were below the poverty line. Out of the total population, 9.9% of those under the age of 18 and 10.2% of those 65 and older were living below the poverty line.

In 2006, crime in Santa Monica affected 4.41% of the population, slightly lower than the national average crime rate that year of 4.48%. The majority of this was property crime, which affected 3.74% of Santa Monica's population in 2006; this was higher than the rates for Los Angeles County (2.76%) and California (3.17%), but lower than the national average (3.91%). These per-capita crime rates are computed based on Santa Monica's full-time population of about 85,000. However, the Santa Monica Police Department has suggested the actual per-capita crime rate is much lower, as tourists, workers, and beachgoers can increase the city's daytime population to between 250,000 and 450,000 people.

Violent crimes affected 0.67% of the population in Santa Monica in 2006,
in line with Los Angeles County (0.65%),
but higher than the averages for California (0.53%)
and the nation (0.55%).

Hate crime has typically been minimal in Santa Monica, with only one reported incident in 2007.
However, the city experienced a spike of anti-Islamic hate crime in 2001, following the attacks of September 11. Hate crime levels returned to their minimal 2000 levels by 2002.

In 2006, Santa Monica voters passed "Measure Y" with a 65% majority, which moved the issuance of citations for marijuana smoking to the bottom of the police priority list. A 2009 study by the Santa Monica Daily Press showed since the law took effect in 2007, the Santa Monica Police had "not issued any citations for offenses involving the adult, personal use of marijuana inside private residences."

The Pico neighborhood of Santa Monica (south of the Santa Monica Freeway) experiences some gang activity. The city estimates there are about 50 gang members based in Santa Monica, although some community organizers dispute this claim. Gang activity has been prevalent for decades in the Pico neighborhood.

In October 1998, alleged Culver City 13 gang member Omar Sevilla, 21, of Culver City was killed. A couple of hours after the shooting of Sevilla, German tourist Horst Fietze was killed. Several days later Juan Martin Campos, age 23, a Santa Monica city employee, was shot and killed. Police believe this was a retaliatory killing in response to the death of Omar Sevilla. Less than twenty-four hours later, Javier Cruz was wounded in a drive-by shooting outside his home on 17th and Michigan.

In 1999, there was a double homicide in the Westside Clothing store on Lincoln Boulevard. During the incident, Culver City gang members David "Puppet" Robles and Jesse "Psycho" Garcia entered the store masked and began opening fire, killing Anthony and Michael Juarez. They then ran outside to a getaway vehicle driven by a third Culver City gang member, who is now also in custody. The clothing store was believed to be a local hang out for Santa Monica gang members. The dead included two men from Northern California who had merely been visiting the store's owner, their cousin, to see if they could open a similar store in their area. Police say the incident was in retaliation for a shooting committed by the Santa Monica 13 gang days before the Juarez brothers were gunned down.

Aside from the rivalry with the Culver City gang, gang members also feud with the Venice and West Los Angeles gangs. The main rivals in these regions include Venice 13, Graveyard Gangster Crips, Hell's Bandidos and Venice Shoreline Crips gangs in the Oakwood area of Venice, California.

The Santa Monica-Malibu Unified School District provides public education at the elementary and secondary levels. In addition to the traditional model of early education school houses, SMASH (Santa Monica Alternative School House) is "a K-8 public school of choice with team teachers and multi-aged classrooms".

The district maintains eight public elementary schools in Santa Monica:

The district maintains three public middle schools in Santa Monica: John Adams Middle School, Lincoln Middle School and SMASH.

The district maintains three high schools in Santa Monica: Olympic High School, Malibu High School and Santa Monica High School.

Private schools in the city include:

Asahi Gakuen, a weekend Japanese supplementary school system, operates its Santa Monica campus (サンタモニカ校･高等部 "Santamonika-kō kōtōbu") at Webster Middle in the Sawtelle neighborhood of Los Angeles. All high school classes in the Asahi Gakuen system are held at the Santa Monica campus. As of 1986, students take buses from as far away as Orange County to go to the high school classes of the Santa Monica campus.

Santa Monica College is a community college founded in 1929. Many SMC graduates transfer to the University of California system. It occupies 35 acres (14 hectares) and enrolls 30,000 students annually. The Frederick S. Pardee RAND Graduate School, associated with the RAND Corporation, is the U.S.'s largest producer of public policy PhDs. The Art Institute of California – Los Angeles is also in Santa Monica near the Santa Monica Airport.
Universities and colleges within a radius from Santa Monica include Santa Monica College, Antioch University Los Angeles, Loyola Marymount University, Mount St. Mary's University, Pepperdine University, California State University, Northridge, California State University, Los Angeles, UCLA, USC, West Los Angeles College, California Institute of Technology (Caltech), Occidental College (Oxy), Los Angeles City College, Los Angeles Southwest College, Los Angeles Valley College, and Emperor's College of Traditional Oriental Medicine.

The Santa Monica Public Library consists of a Main Library in the downtown area, plus four neighborhood branches: Fairview, Montana Avenue, Ocean Park, and Pico Boulevard.

Santa Monica has a bike action plan and launched a bicycle sharing system in November 2015. The city is traversed by the Marvin Braude Bike Trail. Santa Monica has received the Bicycle Friendly Community Award (Bronze in 2009, Silver in 2013) by the League of American Bicyclists. Local bicycle advocacy organizations include Santa Monica Spoke, a local chapter of the Los Angeles County Bicycle Coalition. Santa Monica is thought to be one of the leaders for bicycle infrastructure and programming in Los Angeles County although cycling infrastructure in Los Angeles County in general remains very poor compared to other major cities.

In terms of number of bicycle accidents, Santa Monica ranks as one of the worst (#2) out of 102 California cities with population 50,000–100,000, a ranking consistent with the city's composite ranking.
In 2007 and 2008, local police cracked down on Santa Monica Critical Mass rides that had become controversial, putting a damper on the tradition.

In August 2018, Santa Monica issued permits to Bird, Lime, Lyft, and Jump Bikes to operate dockless scooter-sharing systems in the city.

The Santa Monica Freeway (Interstate 10) begins in Santa Monica near the Pacific Ocean and heads east. The Santa Monica Freeway between Santa Monica and downtown Los Angeles has the distinction of being one of the busiest highways in all of North America. After traversing the Greater Los Angeles area, I-10 crosses seven more states, terminating at Jacksonville, Florida. In Santa Monica, there is a road sign designating this route as the Christopher Columbus Transcontinental Highway. State Route 2 (Santa Monica Boulevard) begins in Santa Monica, barely grazing State Route 1 at Lincoln Boulevard, and continues northeast across Los Angeles County, through the Angeles National Forest, crossing the San Gabriel Mountains as the Angeles Crest Highway, ending in Wrightwood. Santa Monica is also the western terminus of Historic U.S. Route 66. Close to the eastern boundary of Santa Monica, Sepulveda Boulevard reaches from Long Beach at the south, to the northern end of the San Fernando Valley. Just east of Santa Monica is Interstate 405, the San Diego Freeway, a major north–south route in Los Angeles and Orange counties.

The City of Santa Monica has purchased the first ZeroTruck all-electric medium-duty truck. The vehicle will be equipped with a Scelzi utility body, it is based on the Isuzu N series chassis, a UQM PowerPhase 100 advanced electric motor and is the only US built electric truck offered for sale in the United States in 2009.

The city of Santa Monica runs its own bus service, the Big Blue Bus, which also serves much of West Los Angeles and the University of California, Los Angeles (UCLA). A Big Blue Bus was featured prominently in the action movie "Speed".

The city of Santa Monica is also served by the Los Angeles County Metropolitan Transportation Authority's (Metro) bus lines. Metro also complements Big Blue service, as when Big Blue routes are not operational overnight, Metro buses make many Big Blue Bus stops, in addition to MTA stops.

Design and construction on the of the Expo Line from Culver City to Santa Monica started in September 2011, with service beginning on May 20, 2016. Santa Monica Metro stations include 26th Street/Bergamot, 17th Street/Santa Monica College, and Downtown Santa Monica. Travel time between the downtown Santa Monica and the downtown Los Angeles termini is approximately 47 minutes.

Historical aspects of the Expo line route are noteworthy. It uses the former Los Angeles region's electric interurban Pacific Electric Railway's right-of-way that ran from the Exposition Park area of Los Angeles to Santa Monica. This route was called the Santa Monica Air Line and provided electric-powered freight and passenger service between Los Angeles and Santa Monica beginning in the 1920s. Passenger service was discontinued in 1953, but diesel-powered freight deliveries to warehouses along the route continued until March 11, 1988. The abandonment of the line spurred future transportation considerations and concerns within the community, and the entire right-of-way was purchased from Southern Pacific by Los Angeles Metropolitan Transportation Authority. The line was built in 1875 as the steam-powered Los Angeles and Independence Railroad to bring mining ore to ships in Santa Monica harbor and as a passenger excursion train to the beach.

Since the mid-1980s, various proposals have been made to extend the Purple Line subway to Santa Monica under Wilshire Boulevard. There are no current plans to complete the "subway to the sea," an estimated $5 billion project.

The city owns and operates a general aviation airport, Santa Monica Airport, which has been the site of several important aviation achievements. Commercial flights are available for residents at LAX, a few miles south of Santa Monica.

Like other cities in Los Angeles County, Santa Monica is dependent upon the Port of Long Beach and the Port of Los Angeles for international ship cargo. In the 1890s, Santa Monica was once in competition with Wilmington, California, and San Pedro for recognition as the "Port of Los Angeles" (see History of Santa Monica, California).

Two major hospitals are within the Santa Monica city limits, UCLA Santa Monica Hospital and St. John's Hospital. There are four fire stations providing medical and fire response within the city staffed with 6 Paramedic Engines, 1 Truck company, 1 Hazardous Materials team and 1 Urban Search & Rescue team. Santa Monica Fire Department has its own Dispatch Center. Ambulance transportation is provided by McCormick Ambulance Services.

Law enforcement services is provided by the Santa Monica Police Department

The Los Angeles County Department of Health Services operates the Simms/Mann Health and Wellness Center in Santa Monica. The Department's West Area Health Office is in the Simms/Mann Center.

Santa Monica has a municipal wireless network which provides several free city Wi-Fi hotspots distributed around the City.

Santa Monica is governed by the Santa Monica City Council, a Council-Manager governing body with seven members elected at-large. The mayor is Kevin McKeown, and the Mayor Pro Tempore is Terry O'Day. The other five current council members are Sue Himmelrich, Ted Winterer, Ana Maria Jara, Gleam Davis, and Greg Morena.

In the California State Legislature, Santa Monica is in , and in .

In the United States House of Representatives, Santa Monica is in .

In recent years, Santa Monica has voted Democratic in presidential elections, with the Democrats winning over 70% of the vote in all five presidential elections since 2000. The Republican party by contrast, failed to reach 25% of the vote in any of those elections, with both John McCain in 2008, and Donald Trump in 2016 failing to reach 20% of the vote. The Libertarian Party has increased its share of the vote in each of the last four presidential elections. Earning over 2% in the 2012 and 2016 elections, after failing to reach one percent in any of the three prior elections.

Santa Monica is home to the headquarters of many notable businesses, such as Beachbody, Fatburger, Hulu, Illumination Entertainment, Lionsgate Films, Macerich, Miramax, the RAND Corporation, Saban Capital Group, The Recording Academy (which presents the annual Grammy Awards), TOMS Shoes, and Universal Music Group. Atlantic Aviation is at the Santa Monica Airport. The National Public Radio member station KCRW is at the Santa Monica College campus. VCA Animal Hospitals is just outside the eastern city limit.

A number of game development studios are based in Santa Monica, making it a major location for the industry. These include:

Recently, Santa Monica has emerged as the center of the Los Angeles region called Silicon Beach, and serves as the home of hundreds of venture capital funded startup companies.

Former Santa Monica businesses include Douglas Aircraft (now merged with Boeing), GeoCities (which in December 1996 was headquartered on the third floor of 1918 Main Street in Santa Monica), Metro-Goldwyn-Mayer, and MySpace (now headquartered in Beverly Hills).

According to the City's 2012–2013 Comprehensive Annual Financial Report, the top employers in the city are:

The men's and women's marathon ran through parts of Santa Monica during the 1984 Summer Olympics. The Santa Monica Track Club has many prominent track athletes, including many Olympic gold medalists. Santa Monica is the home to Southern California Aquatics, which was founded by Olympic swimmer Clay Evans and Bonnie Adair. Santa Monica is also home to the Santa Monica Rugby Club, a semi-professional team that competes in the Pacific Rugby Premiership, the highest-level rugby union club competition in the United States.

During the 2028 Summer Olympics. Santa Monica will host beach volleyball and surfing.

Hundreds of moving pictures have been shot or set in part within the city of Santa Monica.

One of the oldest exterior shots in Santa Monica is Buster Keaton's "Spite Marriage" (1929) which shows much of 2nd Street. The comedy "It's a Mad, Mad, Mad, Mad World" (1963) included several scenes shot in Santa Monica, including those along the California Incline, which led to the movie's treasure spot, "The Big W". The Sylvester Stallone film "Rocky III" (1982) shows Rocky Balboa and Apollo Creed training to fight Clubber Lang by running on the Santa Monica Beach, and Stallone's "Demolition Man" (1993) includes Santa Monica settings. In "Pee-wee's Big Adventure" (1985), the theft of Pee-wee's bike occurs on the Third Street Promenade. Henry Jaglom's indie "Someone to Love" (1987), the last film in which Orson Welles appeared, takes place in Santa Monica's venerable Mayfair Theatre. "Heathers" (1989) used Santa Monica's John Adams Middle School for many exterior shots. "The Truth About Cats & Dogs" (1996) is set entirely in Santa Monica, particularly the Palisades Park area, and features a radio station that resembles KCRW at Santa Monica College. "17 Again" (2009) was shot at Samohi. Other films that show significant exterior shots of Santa Monica include "Fletch" (1985), "Species" (1995), "Get Shorty" (1995), and "Ocean's Eleven" (2001). Richard Rossi's biopic "Aimee Semple McPherson" opens and closes at the beach in Santa Monica. "Iron Man" features the Santa Monica pier and surrounding communities as Tony Stark tests his experimental flight suit.

The documentary "Dogtown and Z-Boys" (2001) and the related dramatic film "Lords of Dogtown" (2005) are both about the influential skateboarding culture of Santa Monica's Ocean Park neighborhood in the 1970s.

The Santa Monica Pier is shown in many films, including "They Shoot Horses, Don't They?" (1969), "The Sting" (1973), "Ruthless People" (1986), "Beverly Hills Cop III" (1994), "Clean Slate" (1994), "Forrest Gump" (1994), "The Net" (1995), "Love Stinks" (1999), "Cellular" (2004), "" (2006), "Iron Man" (2008) and "" (2009).

The films "The Doors" (1991) and "Speed" (1994) featured vehicles from Santa Monica's Big Blue Bus line, relative to the eras depicted in the films.

The city of Santa Monica (and in particular the Santa Monica Airport) was featured in Roland Emmerich's disaster film "2012" (2009). A magnitude 10.9 earthquake destroys the airport and the surrounding area as a group of survivors escape in a personal plane. The Santa Monica Pier and the whole city sinks into the Pacific Ocean after the earthquake.

A number of television series have been set in Santa Monica, including "Baywatch", "Goliath", "Pacific Blue", "Private Practice", and "Three's Company". The Santa Monica pier is shown in the main theme of CBS series "". In "Buffy the Vampire Slayer", the main exterior set of the town of Sunnydale, including the infamous "sun sign", was in Santa Monica in a lot on Olympic Boulevard.

The main character from Edgar Rice Burroughs' fantasy novel, "The Land That Time Forgot" (serialized in 1918 and published in book form in 1924) was a shipbuilder from Santa Monica.

Horace McCoy's 1935 novel "They Shoot Horses, Don't They?" is set at a dance marathon held in a ballroom on the Santa Monica Pier.

Raymond Chandler's most famous character, private detective Philip Marlowe, frequently has a portion of his adventures in a place called "Bay City", which is modeled on Depression-era Santa Monica. In Marlowe's world, Bay City is "a wide-open town", where gambling and other crimes thrive due to a massively corrupt and ineffective police force.

In Gennifer Choldenko's historical fiction novel for young adults, "Al Capone Does My Shirts" (2006), the Flanagans move to Alcatraz from Santa Monica.

Tennessee Williams lived (while working at MGM Studios) in a hotel on Ocean Avenue in the 1940s. At that location he wrote the play "The Glass Menagerie" (that premiered in 1944). His short story, "" (1954), was set near Santa Monica Beach and mentions the clock visible in much of the city, high up on The Broadway Building, on Broadway near Second Street.



Santa Monica is featured in the video games:
"Driver" (1999), "" (2003), " Destroy All Humans! " (2004), "Grand Theft Auto San Andreas" (2004) as a fictional district – Santa Maria Beach, "" (2004), "L.A. Rush" (2005), "Tony Hawk's American Wasteland" (2005), "" (2008), "Cars Race-O-Rama" (2009) as a fictional city – Santa Carburera, "" (2013) as a fictional U.S. military base – Fort Santa Monica, "Grand Theft Auto V" (2013) as a fictional district – Del Perro, "The Crew" (2014), and "Need for Speed" (2015).





</doc>
<doc id="28209" url="https://en.wikipedia.org/wiki?curid=28209" title="Shot put">
Shot put

The shot put is a track and field event involving "putting" (pushing rather than throwing) a heavy spherical ball—the "shot"—as far as possible. The shot put competition for men has been a part of the modern Olympics since their revival in 1896, and women's competition began in 1948.

Homer mentions competitions of rock throwing by soldiers during the Siege of Troy but there is no record of any dead weights being thrown in Greek competitions. The first evidence for stone- or weight-throwing events were in the Scottish Highlands, and date back to approximately the first century. In the 16th century King Henry VIII was noted for his prowess in court competitions of weight and hammer throwing.

The first events resembling the modern shot put likely occurred in the Middle Ages when soldiers held competitions in which they hurled cannonballs. Shot put competitions were first recorded in early 19th century Scotland, and were a part of the British Amateur Championships beginning in 1866.

Competitors take their throw from inside a marked circle 2.135 m (7 ft) in diameter, with a stopboard about high at the front of the circle. The distance thrown is measured from the inside of the circumference of the circle to the nearest mark made on the ground by the falling shot, with distances rounded down to the nearest centimetre under IAAF and WMA rules.

The following rules (indoor and outdoor) must be adhered to for a legal throw:
Foul throws occur when an athlete:
At any time if the shot loses contact with the neck then it is technically an illegal put.

The following are either obsolete or non-existent, but commonly believed rules within professional competition:

Shot put competitions have been held at the modern Summer Olympic Games since their inception in 1896, and it is also included as an event in the World Athletics Championships.

Each of these competitions in the modern era have a set number of rounds of throws. Typically there are three qualification rounds to determine qualification for the final. There are then three preliminary rounds in the final with the top eight competitors receiving a further three throws. Each competitor in the final is credited with their longest throw, regardless of whether it was achieved in the preliminary or final three rounds. The competitor with the longest legal put is declared the winner.

In open competitions the men's shot weighs , and the women's shot weighs . Junior, school, and masters competitions often use different weights of shots, typically below the weights of those used in open competitions; the individual rules for each competition should be consulted in order to determine the correct weights to be used.

Two putting styles are in current general use by shot put competitors: the "glide" and the "spin". With all putting styles, the goal is to release the shot with maximum forward velocity at an angle of approximately forty-five degrees.

The origin of this technique glide dates to 1951, when Parry O'Brien from the United States invented a technique that involved the putter facing backwards, rotating 180 degrees across the circle, and then tossing the shot. Unlike spin this technique is a linear movement.

With this technique, a right-hand thrower would begin facing the rear of the circle. They would typically adopt a specific type of crouch, involving their bent right leg, in order to begin the throw from a more beneficial posture whilst also isometrically preloading their muscles. The positioning of their bodyweight over their bent leg, which pushes upwards with equal force, generates a preparatory isometric press. The force generated by this press will be channelled into the subsequent throw making it more powerful. To initiate the throw they kick to the front with the left leg, while pushing off forcefully with the right. As the thrower crosses the circle, the hips twist toward the front, the left arm is swung out then pulled back tight, followed by the shoulders, and they then strike in a putting motion with their right arm. The key is to move quickly across the circle with as little air under the feet as possible, hence the name 'glide'.

Also known as rotational technique. It was first practiced in Europe in the 1950s but did not receive much attention until the 1970s. In 1972 Aleksandr Baryshnikov set his first USSR record using a new putting style, the spin ("круговой мах" in Russian), invented by his coach Viktor Alexeyev. The spin involves rotating like a discus thrower and using rotational momentum for power. In 1976 Baryshnikov went on to set a world record of with his spin style, and was the first shot putter to cross the 22-meter mark.

With this technique, a right-hand thrower faces the rear, and begins to spin on the ball of the left foot. The thrower comes around and faces the front of the circle and drives the right foot into the center of the circle. Finally, the thrower reaches for the front of the circle with the left foot, twisting the hips and shoulders like in the glide, and puts the shot.

When the athlete executes the spin, the upper body is twisted hard to the right, so the imaginary lines created by the shoulders and hips are no longer parallel. This action builds up torque, and stretches the muscles, creating an involuntary elasticity in the muscles, providing extra power and momentum. When the athlete prepares to release, the left foot is firmly planted, causing the momentum and energy generated to be conserved, pushing the shot in an upward and outward direction.

Another purpose of the spin is to build up a high rotational speed, by swinging the right leg initially, then to bring all the limbs in tightly, similar to a figure skater bringing in their arms while spinning to increase their speed. Once this fast speed is achieved the shot is released, transferring the energy into the shot put.

Until 2016, a woman has never made an Olympic final (top 8) using the spin technique. The first woman to enter a final and win a medal at the Olympics was Anita Márton.

Currently, most top male shot putters use the spin. However the glide remains popular since the technique leads to greater consistency compared to the rotational technique. Almost all throwers start by using the glide. Tomasz Majewski notes that although most athletes use the spin, he and some other top shot putters achieved success using this classic method (for example he became first to defend the Olympic title in 56 years).

The world record by a male putter of by Randy Barnes was completed with the spin technique, while the second-best all-time put of by Ulf Timmermann was completed with the glide technique.

The decision to glide or spin may need to be decided on an individual basis, determined by the thrower's size and power. Short throwers may benefit from the spin and taller throwers may benefit from the glide, but many throwers do not follow this guideline.

The shot is made of different kinds of materials depending on its intended use. Materials used include sand, iron, cast iron, solid steel, stainless steel, brass, and synthetic materials like polyvinyl. Some metals are more dense than others making the size of the shot vary. For example, different materials are used to make indoor and outdoor shot - because damage to surroundings must be taken into account - so the latter are smaller. There are various size and weight standards for the implement that depend on the age and gender of the competitors as well as the national customs of the governing body.

The current world record holders are:
The current records held on each continent are:


Best women's throw using a spin technique is 19.87 by Anita Márton and Jillian Camarena-Williams.


The following athletes had their performance (inside 21.49 m) annulled due to doping offenses:





</doc>
<doc id="28211" url="https://en.wikipedia.org/wiki?curid=28211" title="Stan Kelly-Bootle">
Stan Kelly-Bootle

Stanley Bootle, known as Stan Kelly-Bootle (15 September 1929  – 16 April 2014), was a British author, academic, singer-songwriter and computer scientist. 

He took his stage name Stan Kelly (he was not known as Stan Kelly-Bootle in folk music circles) from the Irish folk song "Kelly, the boy from Killane". His best-known song is the "Liverpool Lullaby" or "The Mucky Kid" which was recorded in 1965 on the "Three City Four" LP and sung by Marian McKenzie. It was also sung by the Ian Campbell Folk Group on the "Contemporary Campbells" LP. It was later recorded by Judy Collins in 1966 for her album "In My Life". Cilla Black recorded it three years later as the B-side to her pop hit "Conversations". Kelly-Bootle achieved the first postgraduate degree in computer science in 1954, from the University of Cambridge.

Stan Kelly-Bootle was born Stanley Bootle in Liverpool, Lancashire, on 15 September 1929 and grew up in the Wavertree area of the city. His parents were Arthur Bootle and Ada Gallagher.

Kelly-Bootle was schooled at the Liverpool Institute. He spent 1948–1950 doing his national service in the British Army, achieving the rank of Sgt. Instructor in RADAR. He attended Downing College, Cambridge, graduating with a first class degree in Numerical Analysis and Automatic Computing in 1954, the first postgraduate degree in computer science.

In 1950, Kelly-Bootle helped found the St. Lawrence Folk Song Society at Cambridge University. As a folk singer-songwriter, he performed under the name Stan Kelly. He wrote some of his own tunes and also wrote lyrics set to traditional tunes. In the course of his musical career, he made over 200 radio and television appearances, and released several recordings, as well as having his songs recorded by others.

Solo releases include:

Other audio recordings include:

He started his computing career programming the pioneering EDSAC computer, designed and built at Cambridge University. He worked for IBM in the United States and the UK from 1955 to 1970. From 1970 to 1973, he worked as Manager for University Systems for Sperry-UNIVAC. He also lectured at the University of Warwick.

In 1973, Kelly-Bootle left Sperry-UNIVAC and became a freelance consultant, writer and programmer. He was known in the computer community for "The Devil's DP Dictionary" and its second edition, "The Computer Contradictionary" (1995), which he authored. These works are cynical lexicographies in the vein of Ambrose Bierce's "The Devil's Dictionary". Kelly-Bootle authored or co-authored several serious textbooks and tutorials on subjects such as the Motorola 68000 family of CPUs, programming languages including various C compilers, and the Unix operating system. He authored the "Devil's Advocate" column in "UNIX Review" from 1984 to 2000, and had columns in "Computer Language" ("Bit by Bit", 1989–1994), "OS/2 Magazine" ("End Notes", 1994–97) and "Software Development" ("Seamless Quanta", October 1995 – May 1997). He contributed columns and articles to several other computer industry magazines, as well.
Kelly-Bootle's articles for magazines such as "ACM Queue", "AI/Expert", and "UNIX Review" contain examples of word-play, criticism of silly marketing and usage (he refers often to the computer "laxicon"), and commentary on the industry in general. He wrote an online monthly column posted on the Internet. While most of his writing was oriented towards the computer industry, he wrote a few books relating to his other interests, including 

Stan Kelly-Bootle died on 16 April 2014, aged 84, in hospital in Oswestry, Shropshire.



</doc>
<doc id="28212" url="https://en.wikipedia.org/wiki?curid=28212" title="Skewness">
Skewness

In probability theory and statistics, skewness is a measure of the asymmetry of the probability distribution of a real-valued random variable about its mean. The skewness value can be positive, zero, negative, or undefined.

For a unimodal distribution, negative skew commonly indicates that the "tail" is on the left side of the distribution, and positive skew indicates that the tail is on the right. In cases where one tail is long but the other tail is fat, skewness does not obey a simple rule. For example, a zero value means that the tails on both sides of the mean balance out overall; this is the case for a symmetric distribution, but can also be true for an asymmetric distribution where one tail is long and thin, and the other is short but fat.

Consider the two distributions in the figure just below. Within each graph, the values on the right side of the distribution taper differently from the values on the left side. These tapering sides are called "tails", and they provide a visual means to determine which of the two kinds of skewness a distribution has:

Skewness in a data series may sometimes be observed not only graphically but by simple inspection of the values. For instance, consider the numeric sequence (49, 50, 51), whose values are evenly distributed around a central value of 50. We can transform this sequence into a negatively skewed distribution by adding a value far below the mean, which is probably a negative outlier, e.g. (40, 49, 50, 51). Therefore, the mean of the sequence becomes 47.5, and the median is 49.5. Based on the formula of nonparametric skew, defined as formula_1 the skew is negative. Similarly, we can make the sequence positively skewed by adding a value far above the mean, which is probably a positive outlier, e.g. (49, 50, 51, 60), where the mean is 52.5, and the median is 50.5.

As mentioned earlier, a unimodal distribution with zero value of skewness does not imply that this distribution is symmetric necessarily. However, a symmetric unimodal or multimodal distribution always has zero skewness.
The skewness is not directly related to the relationship between the mean and median: a distribution with negative skew can have its mean greater than or less than the median, and likewise for positive skew.

In the older notion of nonparametric skew, defined as formula_1 where formula_3 is the mean, formula_4 is the median, and formula_5 is the standard deviation, the skewness is defined in terms of this relationship: positive/right nonparametric skew means the mean is greater than (to the right of) the median, while negative/left nonparametric skew means the mean is less than (to the left of) the median. However, the modern definition of skewness and the traditional nonparametric definition do not always have the same sign: while they agree for some families of distributions, they differ in some of the cases, and conflating them is misleading.

If the distribution is symmetric, then the mean is equal to the median, and the distribution has zero skewness. If the distribution is both symmetric and unimodal, then the mean = median = mode. This is the case of a coin toss or the series 1,2,3,4... Note, however, that the converse is not true in general, i.e. zero skewness does not imply that the mean is equal to the median.

A 2005 journal article points out:Many textbooks teach a rule of thumb stating that the mean is right of the median under right skew, and left of the median under left skew. This rule fails with surprising frequency. It can fail in multimodal distributions, or in distributions where one tail is long but the other is heavy. Most commonly, though, the rule fails in discrete distributions where the areas to the left and right of the median are not equal. Such distributions not only contradict the textbook relationship between mean, median, and skew, they also contradict the textbook interpretation of the median.

For example, in the distribution of adult residents across US households, the skew is to the right. However, due to the majority of cases is less or equal to the mode, which is also the median, the mean sits in the heavier left tail. As a result, the rule of thumb that the mean is right of the median under right skew failed.

The skewness of a random variable "X" is the third standardized moment formula_6, defined as:

where "μ" is the mean, "σ" is the standard deviation, E is the expectation operator, "μ" is the third central moment, and "κ" are the "t"-th cumulants. It is sometimes referred to as Pearson's moment coefficient of skewness, or simply the moment coefficient of skewness, but should not be confused with Pearson's other skewness statistics (see below). The last equality expresses skewness in terms of the ratio of the third cumulant "κ" to the 1.5th power of the second cumulant "κ". This is analogous to the definition of kurtosis as the fourth cumulant normalized by the square of the second cumulant. 
The skewness is also sometimes denoted Skew["X"].

If "σ" is finite, "μ" is finite too and skewness can be expressed in terms of the non-central moment E["X"] by expanding the previous formula,
Skewness can be infinite, as when
where the third cumulants are infinite, or as when
where the third cumulant is undefined.

Examples of distributions with finite skewness include the following.


For a sample of "n" values, a natural method of moments estimator of the population skewness is

where formula_12 is the sample mean, "s" is the sample standard deviation, and the numerator "m" is the sample third central moment.

Another common definition of the "sample skewness" is

where formula_14 is the unique symmetric unbiased estimator of the third cumulant and formula_15 is the symmetric unbiased estimator of the second cumulant (i.e. the sample variance). This adjusted Fisher–Pearson standardized moment coefficient formula_16 is the version found in Excel and several statistical packages including Minitab, SAS and SPSS.

In general, the ratios formula_17 and formula_18 are both biased estimators of the population skewness formula_19; their expected values can even have the opposite sign from the true skewness. (For instance, a mixed distribution consisting of very thin Gaussians centred at −99, 0.5, and 2 with weights 0.01, 0.66, and 0.33 has a skewness of about −9.77, but in a sample of 3, formula_18 has an expected value of about 0.32, since usually all three samples are in the positive-valued part of the distribution, which is skewed the other way.) Nevertheless, formula_17 and formula_18 each have obviously the correct expected value of zero for any symmetric distribution with a finite third moment, including a normal distribution.

Under the assumption that the underlying random variable formula_23 is normally distributed, it can be shown that formula_24, i.e., its distribution converges to a normal distribution with mean 0 and variance 6. The variance of the skewness of a random sample of size "n" from a normal distribution is

An approximate alternative is 6/"n", but this is inaccurate for small samples.

In normal samples, formula_17 has the smaller variance of the two estimators, with

where in the denominator 

is the (biased) sample second central moment.

Skewness is a descriptive statistic that can be used in conjunction with the histogram and the normal quantile plot to characterize the data or distribution.

Skewness indicates the direction and relative magnitude of a distribution's deviation from the normal distribution.

With pronounced skewness, standard statistical inference procedures such as a confidence interval for a mean will be not only incorrect, in the sense that the true coverage level will differ from the nominal (e.g., 95%) level, but they will also result in unequal error probabilities on each side.

Skewness can be used to obtain approximate probabilities and quantiles of distributions (such as value at risk in finance) via the Cornish-Fisher expansion.

Many models assume normal distribution; i.e., data are symmetric about the mean. The normal distribution has a skewness of zero. But in reality, data points may not be perfectly symmetric. So, an understanding of the skewness of the dataset indicates whether deviations from the mean are going to be positive or negative.

D'Agostino's K-squared test is a goodness-of-fit normality test based on sample skewness and sample kurtosis.

Other measures of skewness have been used, including simpler calculations suggested by Karl Pearson (not to be confused with Pearson's moment coefficient of skewness, see above). These other measures are:

The Pearson mode skewness, or first skewness coefficient, is defined as

The Pearson median skewness, or second skewness coefficient, is defined as

Which is a simple multiple of the nonparametric skew.

Bowley's measure of skewness (from 1901), also called Yule's coefficient (from 1912) is defined as: 
When writing it as formula_30, it is easier to see that the numerator is difference between the average of the upper and lower quartiles (a measure of location) and the median (another measure of location), while the denominator is the semi-interquartile range (Q3-Q1)/2, which for symmetric distributions is the MAD measure of dispersion.

Other names for this measure are Galton's measure of skewness, the Yule–Kendall index and the quartile skewness ,

A more general formulation of a skewness function was described by Groeneveld, R. A. and Meeden, G. (1984):
where "F" is the cumulative distribution function. This leads to a corresponding overall measure of skewness defined as the supremum of this over the range 1/2 ≤ "u" < 1. Another measure can be obtained by integrating the numerator and denominator of this expression. The function "γ"("u") satisfies −1 ≤ "γ"("u") ≤ 1 and is well defined without requiring the existence of any moments of the distribution. Quantile-based skewness measures are at first glance easy to interpret, but they often show significantly larger sample variations, than moment-based methods. This means that often samples from a symmetric distribution (like the uniform distribution) have a large quantile-based skewness, just by chance.

Bowley's measure of skewness is γ("u") evaluated at "u" = 3/4. Kelley's measure of skewness uses "u" = 0.1.

Groeneveld & Meeden have suggested, as an alternative measure of skewness,

where "μ" is the mean, "ν" is the median, |...| is the absolute value, and "E"() is the expectation operator. This is closely related in form to Pearson's second skewness coefficient.

Use of L-moments in place of moments provides a measure of skewness known as the L-skewness.

A value of skewness equal to zero does not imply that the probability distribution is symmetric. Thus there is a need for another measure of asymmetry that has this property: such a measure was introduced in 2000. It is called distance skewness and denoted by dSkew. If "X" is a random variable taking values in the "d"-dimensional Euclidean space, "X" has finite expectation, "X" is an independent identically distributed copy of "X", and formula_33 denotes the norm in the Euclidean space, then a simple "measure of asymmetry" with respect to location parameter θ is
and dSkew("X") := 0 for "X" = θ (with probability 1). Distance skewness is always between 0 and 1, equals 0 if and only if "X" is diagonally symmetric with respect to θ ("X" and 2θ−"X" have the same probability distribution) and equals 1 if and only if X is a constant "c" (formula_35) with probability one. Thus there is a simple consistent statistical test of diagonal symmetry based on the sample distance skewness:

The medcouple is a scale-invariant robust measure of skewness, with a breakdown point of 25%. It is the median of the values of the kernel function
taken over all couples formula_38 such that formula_39, where formula_40 is the median of the sample formula_41. It can be seen as the median of all possible quantile skewness measures.




</doc>
<doc id="28215" url="https://en.wikipedia.org/wiki?curid=28215" title="Saint Columba (disambiguation)">
Saint Columba (disambiguation)

Saint Columba (521–597) was an Irish Christian saint who evangelized Scotland.

Saint Columba may also refer to:






</doc>
<doc id="28217" url="https://en.wikipedia.org/wiki?curid=28217" title="Serial Experiments Lain">
Serial Experiments Lain

Serial Experiments Lain (stylized as serial experiments lain) is a 1998 Japanese anime television series produced by Yasuyuki Ueda and animated by Triangle Staff. It was directed by Ryūtarō Nakamura and written by Chiaki J. Konaka, with original character designs by Yoshitoshi ABe. The series' 13 episodes were originally broadcast on TV Tokyo from July to September 1998, and explore themes such as reality, identity and communication through philosophy, computer history, cyberpunk literature and conspiracy theory.

The series focuses on Lain Iwakura, an adolescent middle school girl living in suburban Japan, and her introduction to the Wired, a global communications network which is similar to the Internet. Lain lives with her middle-class family, which consists of her inexpressive older sister Mika, her emotionally distant mother, and her computer-obsessed father; while Lain herself is somewhat awkward, introverted, and socially isolated from most of her school peers. However, the status-quo of her life becomes upturned by a series of bizarre incidents that start to take place after she learns that girls from her school have received an e-mail from a dead student, Chisa Yomoda, and she pulls out her old computer in order to check for the same message. Lain finds Chisa telling her that she is not dead, but has merely "abandoned her physical body and flesh" and is alive deep within the virtual reality-world of the Wired itself, where she has found the almighty and divine "God". From this point, Lain is caught up in a series of cryptic and surreal events that see her delving deeper into the mystery of the network in a narrative that explores themes of consciousness, perception, and the nature of reality.

The "Wired" is a virtual reality-world that contains and supports the very sum of "all" human communication and networks, created with the telegraph, televisions, and telephone services, and expanded with the Internet, cyberspace, and subsequent networks. The series assumes that the Wired could be linked to a system that enables unconscious communication between people and machines without physical interface. The storyline introduces such a system with the Schumann resonances, a property of the Earth's magnetic field that theoretically allows for unhindered long distance communications. If such a link were created, the network would become equivalent to Reality as the general consensus of all perceptions and knowledge. The increasingly thin invisible line between what is real and what is virtual/digital begins to slowly shatter.

Masami Eiri is introduced as the project director on Protocol Seven (the next-generation Internet protocol in the series' time-frame) for major computer company Tachibana General Laboratories. He had secretly included code of his very own creation to give himself absolute control of the Wired through the wireless system described above. He then "uploaded" his own brain, conscience, consciousness, memory, feelings, emotions – his very self – into the Wired and "died" a few days after, leaving only his physical, living body behind. These details are unveiled around the middle of the series, but this is the point where the story of "Serial Experiments Lain" begins. Masami later explains that Lain is the artifact by which the wall between the virtual and material worlds is to fall, and that he needs her to get to the Wired and "abandon the flesh", as he did, to achieve his plan. The series sees him trying to convince her through interventions, using the promise of unconditional love, romantic seduction and charm, and even, when all else fails, threats and force.

In the meantime, the anime follows a complex game of hide-and-seek between the "Knights of the Eastern Calculus", hackers whom Masami claims are "believers that enable him to be a God in the Wired", and Tachibana General Laboratories, who try to regain control of Protocol Seven. In the end, the viewer sees Lain realizing, after much introspection, that she has absolute control over everyone's mind and over reality itself. Her dialogue with different versions of herself shows how she feels shunned from the material world, and how she is afraid to live in the Wired, where she has the possibilities and responsibilities of an almighty goddess. The last scenes feature her erasing everything connected to herself from everyone's memories. She is last seen, unchanged, encountering her oldest and closest friend Alice once again, who is now married. Lain promises herself that she and Alice will surely meet again anytime as Lain can literally go and be anywhere she desires between both worlds.

"Serial Experiments Lain" was conceived, as a series, to be original to the point of it being considered "an enormous risk" by its producer Yasuyuki Ueda.

Producer Ueda had to answer repeated queries about a statement made in an "Animerica" interview. The controversial statement said "Lain" was "a sort of cultural war against American culture and the American sense of values we <nowiki>[Japan]</nowiki> adopted after World War II". He later explained in numerous interviews that he created "Lain" with a set of values he took as distinctly Japanese; he hoped Americans would not understand the series as the Japanese would. This would lead to a "war of ideas" over the meaning of the anime, hopefully culminating in new communication between the two cultures. When he discovered that the American audience held the same views on the series as the Japanese, he was disappointed.

The "Lain" franchise was originally conceived to connect across forms of media (anime, video games, manga). Producer Yasuyuki Ueda said in an interview, "the approach I took for this project was to communicate the essence of the work by the total sum of many media products". The scenario for the video game was written first, and the video game was produced at the same time as the anime series, though the series was released first. A dōjinshi titled "The Nightmare of Fabrication" was produced by Yoshitoshi ABe and released in Japanese in the artbook "Omnipresence in the Wired". Ueda and Konaka declared in an interview that the idea of a multimedia project was not unusual in Japan, as opposed to the contents of "Lain", and the way they are exposed.

The authors were asked in interviews if they had been influenced by "Neon Genesis Evangelion", in the themes and graphic design. This was strictly denied by writer Chiaki J. Konaka in an interview, arguing that he had not seen "Evangelion" until he finished the fourth episode of "Lain". Being primarily a horror movies writer, his stated influences are Godard (especially for using typography on screen), "The Exorcist", "Hell House", and Dan Curtis's "House of Dark Shadows". Alice's name, like the names of her two friends Julie and Reika, came from a previous production from Konaka, "Alice in Cyberland", which in turn was largely influenced by "Alice in Wonderland". As the series developed, Konaka was "surprised" by how close Alice's character became to the original "Wonderland" character.

Vannevar Bush (and memex), John C. Lilly, Timothy Leary and his eight-circuit model of consciousness, Ted Nelson and Project Xanadu are cited as precursors to the Wired. Douglas Rushkoff and his book "Cyberia" were originally to be cited as such, and in "Lain" Cyberia became the name of a nightclub populated with hackers and techno-punk teenagers. Likewise, the series' "deus ex machina" lies in the conjunction of the Schumann resonances and Jung's collective unconscious (the authors chose this term over Kabbalah and Akashic Record). Majestic 12 and the Roswell UFO incident are used as examples of how a hoax might still affect history, even after having been exposed as such, by creating sub-cultures. This links again to Vannevar Bush, the alleged "brains" of MJ12. Two of the literary references in "Lain" are quoted through Lain's father: he first logs onto a website with the password "Think Bule Count One Tow" ("Think Blue, Count Two" is an Instrumentality of Man story featuring virtual persons projected as real ones in people's minds); and his saying that "madeleines would be good with the tea" in the last episode makes "Lain" "perhaps the only cartoon to allude to Proust".

Yoshitoshi ABe confesses to have never read manga as a child, as it was "off-limits" in his household. His major influences are "nature and everything around him". Specifically speaking about Lain's character, ABe was inspired by Kenji Tsuruta, Akihiro Yamada, Range Murata and Yukinobu Hoshino. In a broader view, he has been influenced in his style and technique by Japanese artists Chinai-san and Tabuchi-san.

The character design of Lain was not ABe's sole responsibility. Her distinctive left forelock for instance was a demand from Yasuyuki Ueda. The goal was to produce asymmetry to reflect Lain's unstable and disconcerting nature. It was designed as a mystical symbol, as it is supposed to prevent voices and spirits from being heard by the left ear. The bear pajamas she wears were a demand from character animation director Takahiro Kishida. Though bears are a trademark of the Konaka brothers, Chiaki Konaka first opposed the idea. Director Nakamura then explained how the bear motif could be used as a shield for confrontations with her family. It is a key element of the design of the shy "real world" Lain (see "mental illness" under Themes). When she first goes to the Cyberia nightclub, she wears a bear hat for similar reasons. Retrospectively, Konaka said that Lain's pajamas became a major factor in drawing fans of "moe" characterization to the series, and remarked that "such items may also be important when making anime".

ABe's original design was generally more complicated than what finally appeared on screen. As an example, the X-shaped hairclip was to be an interlocking pattern of gold links. The links would open with a snap, or rotate around an axis until the moment the " X " became a " = ". This was not used as there is no scene where Lain takes her hairclip off.

"Serial Experiments Lain" is not a conventionally linear story, but "an alternative anime, with modern themes and realization". Themes range from theological to psychological and are dealt with in a number of ways: from classical dialogue to image-only introspection, passing by direct interrogation of imaginary characters.

Communication, in its wider sense, is one of the main themes of the series, not only as opposed to loneliness, but also as a subject in itself. Writer Konaka said he wanted to directly "communicate human feelings". Director Nakamura wanted to show the audience — and particularly viewers between 14 and 15—"the multidimensional wavelength of the existential self: the relationship between self and the world".

Loneliness, if only as representing a lack of communication, is recurrent through "Lain". Lain herself (according to Anime Jump) is "almost painfully introverted with no friends to speak of at school, a snotty, condescending sister, a strangely apathetic mother, and a father who seems to want to care but is just too damn busy to give her much of his time". Friendships turn on the first rumor; and the only insert song of the series is named "Kodoku no shigunaru", literally "signal of loneliness".

Mental illness, especially dissociative identity disorder, is a significant theme in "Lain": the main character is constantly confronted with alter-egos, to the point where writer Chiaki Konaka and Lain's voice actress Kaori Shimizu had to agree on subdividing the character's dialogues between three different orthographs. The three names designate distinct "versions" of Lain: the real-world, "childish" Lain has a shy attitude and bear pajamas. The "advanced" Lain, her Wired personality, is bold and questioning. Finally, the "evil" Lain is sly and devious, and does everything she can to harm Lain or the ones close to her. As a writing convention, the authors spelled their respective names in kanji, katakana, and roman characters (see picture).

Reality never has the pretense of objectivity in "Lain". Acceptations of the term are battling throughout the series, such as the "natural" reality, defined through normal dialogue between individuals; the material reality; and the tyrannic reality, enforced by one person onto the minds of others. A key debate to all interpretations of the series is to decide whether matter flows from thought, or the opposite. The production staff carefully avoided "the so-called God's Eye Viewpoint" to make clear the "limited field of vision" of the world of "Lain".

Theology plays its part in the development of the story too. "Lain" has been viewed as a questioning of the possibility of an infinite spirit in a finite body. From self-realization as a goddess to deicide, religion (the title of a layer) is an inherent part of "Lain" background.

"Lain" contains extensive references to Apple computers, as the brand was used at the time by most of the creative staff, such as writers, producers, and the graphical team. As an example, the title at the beginning of each episode is announced by the Apple computer speech synthesis program PlainTalk, using the voice ""Whisper"", e.g. codice_1. Tachibana Industries, the company that creates the NAVI computers, is a reference to Apple computers: "tachibana" means "Mandarin orange" in Japanese. NAVI is the abbreviation of Knowledge Navigator, and the HandiNAVI is based on the Apple Newton, one of the world's first PDAs. The NAVIs are seen to run "Copland OS Enterprise" (this reference to Copland was an initiative of Konaka, a declared Apple fan), and Lain's and Alice's NAVIs closely resembles the Twentieth Anniversary Macintosh and the iMac respectively. The HandiNAVI programming language, as seen on the seventh episode, is a dialect of Lisp. Notice that the Newton also used a Lisp dialect (NewtonScript). The program being typed by Lain can be found in the CMU AI repository; it is a simple implementation of Conway's Game of Life in Common Lisp.

During a series of disconnected images, an iMac and the Think Different advertising slogan appears for a short time, while the "Whisper" voice says it. This was an unsolicited insertion from the graphic team, also Mac-enthusiasts. Other subtle allusions can be found: "Close the world, Open the nExt" is the slogan for the "Serial Experiments Lain" video game. NeXT was the company that produced NeXTSTEP, which later evolved into Mac OS X after Apple bought NeXT. Another example is "To Be Continued." at the end of episodes 1–12, with a blue "B" and a red "e" on "Be": "this" "Be" is the original logo of Be Inc., a company founded by ex-Apple employees and NeXT's main competitor in its time.

"Serial Experiments Lain" was first aired on TV Tokyo on July 6, 1998 and concluded on September 28, 1998 with the thirteenth and final episode. The series consists of 13 episodes (referred to in the series as "Layers") of 24 minutes each, except for the sixth episode, "Kids" (23 minutes 14 seconds). In Japan, the episodes were released in LD, VHS, and DVD with a total of five volumes. A DVD compilation named ""Serial Experiments Lain DVD-BOX Яesurrection"" was released along with a promo DVD called ""LPR-309"" in 2000. As this box set is now discontinued, a rerelease was made in 2005 called ""Serial Experiments Lain TV-BOX"". A 4-volume DVD box set was released in the US by Pioneer/Geneon. A Blu-ray release of the anime was made in December 2009 called ""Serial Experiments Lain Blu-ray Box | RESTORE"". The anime series returned to US television on October 15, 2012 on the Funimation Channel.
The series' opening theme, "Duvet", was written and performed by Jasmine Rodgers and the British band Bôa. The ending theme, , was written and composed by Reichi Nakaido.

The anime series was licensed in North America by Pioneer Entertainment (later Geneon USA) on VHS, DVD and LaserDisc in 1999. However, the company closed its USA division in December 2007 and the series went out-of-print as a result. However, at Anime Expo 2010, North American distributor Funimation announced that it had obtained the license to the series and re-released it in 2012. It was also released in Singapore by Odex.


The first original soundtrack, "Serial Experiments Lain Soundtrack", features music by Reichi Nakaido: the ending theme and part of the television series' score, alongside other songs inspired by the series. The second, "Serial Experiments Lain Soundtrack: Cyberia Mix", features electronica songs inspired by the television series, including a remix of the opening theme "Duvet" by DJ Wasei. The third, "lain BOOTLEG", consists of the ambient score of the series across forty-five tracks. "BOOTLEG" also contains a second mixed-mode data and audio disc, containing a clock program and a game, as well as an extended version of the first disc – nearly double the length – across 57 tracks in 128 kbit/s MP3 format, and sound effects from the series in WAV format. Because the word "bootleg" appears in its title, it is easily confused with the Sonmay counterfeit edition of itself, which only contains the first disc in an edited format. All three soundtrack albums were released by Pioneer Records.

The series' opening theme, "Duvet", was written and performed in English by the British rock band Bôa. The band released the song as a single and as part of the EP "Tall Snake", which features both an acoustic version and DJ Wasei's remix from "Cyberia Mix".

On November 26, 1998, Pioneer LDC released a video game with the same name as the anime for the PlayStation. It was designed by Konaka and Yasuyuki, and made to be a "network simulator" in which the player would navigate to explore Lain's story. The creators themselves did not call it a game, but "Psycho-Stretch-Ware", and it has been described as being a kind of graphic novel: the gameplay is limited to unlocking pieces of information, and then reading/viewing/listening to them, with little or no puzzle needed to unlock. Lain distances itself even more from classical games by the random order in which information is collected. The aim of the authors was to let the player get the feeling that there are myriads of informations that they would have to sort through, and that they would have to do with less than what exists to understand. As with the anime, the creative team's main goal was to let the player "feel" Lain, and "to understand her problems, and to love her". A guidebook to the game called "Serial Experiments Lain Official Guide" () was released the same month by MediaWorks.

"Serial Experiments Lain" was first broadcast in Tokyo at 1:15 a.m. JST. The word "weird" appears almost systematically in English language reviews of the series, or the alternatives "bizarre", and "atypical", due mostly to the freedoms taken with the animation and its unusual science fiction themes, and due to its philosophical and psychological context. Critics responded positively to these thematic and stylistic characteristics, and it was awarded an Excellence Prize by the 1998 Japan Media Arts Festival for "its willingness to question the meaning of contemporary life" and the "extraordinarily philosophical and deep questions" it asks.

According to Christian Nutt from "Newtype USA", the main attraction to the series is its keen view on "the interlocking problems of identity and technology". Nutt saluted Abe's "crisp, clean character design" and the "perfect soundtrack" in his 2005 review of series, saying that ""Serial Experiments Lain" might not yet be considered a true classic, but it's a fascinating evolutionary leap that helped change the future of anime." "Anime Jump" gave it 4.5/5, and Anime on DVD gave it A+ on all criteria for volume 1 and 2, and a mix of A and A+ for volume 3 and 4.
"Lain" was subject to commentary in the literary and academic worlds. The "Asian Horror Encyclopedia" calls it "an outstanding psycho-horror anime about the psychic and spiritual influence of the Internet". It notes that the red spots present in all the shadows look like blood pools (see picture). It notes the death of a girl in a train accident is "a source of much ghost lore in the twentieth century", more so in Tokyo.

The "Anime Essentials" anthology by Gilles Poitras describes it as a "complex and somehow existential" anime that "pushed the envelope" of anime diversity in the 1990s, alongside the much better known "Neon Genesis Evangelion" and "Cowboy Bebop". Professor Susan J. Napier, in her 2003 reading to the American Philosophical Society called "The Problem of Existence in Japanese Animation" (published 2005), compared "Serial Experiments Lain" to "Ghost in the Shell" and Hayao Miyazaki's "Spirited Away". According to her, the main characters of the two other works cross barriers; they can cross back to our world, but Lain cannot. Napier asks whether there is something to which Lain should return, "between an empty 'real' and a dark 'virtual'". Mike Toole of Anime News Network named "Serial Experiments Lain" as one of the most important anime of the 1990s.

Unlike the anime, the video game drew little attention from the public. Criticized for its (lack of) gameplay, as well as for its "clunky interface", interminable dialogues, absence of music and very long loading times, it was nonetheless remarked for its (at the time) remarkable CG graphics, and its beautiful backgrounds.

Despite the positive feedback the television series had received, Anime Academy gave the series a 75%, partly due to the "lifeless" setting it had. Michael Poirier of "EX" magazine stated that the last three episodes fail to resolve the questions in other DVD volumes. Justin Sevakis of Anime News Network noted that the English dub was decent, but that the show relied so little on dialogue that it hardly mattered.





</doc>
<doc id="28219" url="https://en.wikipedia.org/wiki?curid=28219" title="Spontaneous emission">
Spontaneous emission

Spontaneous emission is the process in which a quantum mechanical system (such as a molecule, an atom or a subatomic particle) transits from an excited energy state to a lower energy state (e.g., its ground state) and emits a quantized amount of energy in the form of a photon. Spontaneous emission is ultimately responsible for most of the light we see all around us; it is so ubiquitous that there are many names given to what is essentially the same process. If atoms (or molecules) are excited by some means other than heating, the spontaneous emission is called luminescence. For example, fireflies are luminescent. And there are different forms of luminescence depending on how excited atoms are produced (electroluminescence, chemiluminescence etc.). If the excitation is affected by the absorption of radiation the spontaneous emission is called fluorescence. Sometimes molecules have a metastable level and continue to fluoresce long after the exciting radiation is turned off; this is called phosphorescence. Figurines that glow in the dark are phosphorescent. Lasers start via spontaneous emission, then during continuous operation work by stimulated emission.

Spontaneous emission cannot be explained by classical electromagnetic theory and is fundamentally a quantum process. The first person to derive the rate of spontaneous emission accurately from first principles was Dirac in his quantum theory of radiation, the precursor to the theory which he later called quantum electrodynamics. Contemporary physicists, when asked to give a physical explanation for spontaneous emission, generally invoke the zero-point energy of the electromagnetic field. In 1963, the Jaynes–Cummings model was developed describing the system of a two-level atom interacting with a quantized field mode (i.e. the vacuum) within an optical cavity. It gave the nonintuitive prediction that the rate of spontaneous emission could be controlled depending on the boundary conditions of the surrounding vacuum field. These experiments gave rise to cavity quantum electrodynamics (CQED), the study of effects of mirrors and cavities on radiative corrections.

If a light source ('the atom') is in an excited state with energy formula_1, it may spontaneously decay to a lower lying level (e.g., the ground state) with energy formula_2, releasing the difference in energy between the two states as a photon. The photon will have angular frequency formula_3 and an energy formula_4:

where formula_6 is the reduced Planck constant. Note: formula_7, where formula_8 is the Planck constant and formula_9 is the linear frequency. The phase of the photon in spontaneous emission is random as is the direction in which the photon propagates. This is not true for stimulated emission. An energy level diagram illustrating the process of spontaneous emission is shown below:

If the number of light sources in the excited state at time formula_10 is given by formula_11, the rate at which formula_12 decays is:

where formula_14 is the rate of spontaneous emission. In the rate-equation formula_14 is a proportionality constant for this particular transition in this particular light source. The constant is referred to as the "Einstein A coefficient", and has units formula_16. 
The above equation can be solved to give:

where formula_18 is the initial number of light sources in the excited state, formula_10 is the time and formula_20 is the radiative decay rate of the transition. The number of excited states formula_12 thus decays exponentially with time, similar to radioactive decay. After one lifetime, the number of excited states decays to 36.8% of its original value (formula_22-time). The radiative decay rate formula_20 is inversely proportional to the lifetime formula_24:

Spontaneous transitions were not explainable within the framework of the Schrödinger equation, in which the electronic energy levels were quantized, but the electromagnetic field was not. Given that the eigenstates of an atom are properly diagonalized, the overlap of the wavefunctions between the excited state and the ground state of the atom is zero. Thus, in the absence of a quantized electromagnetic field, the excited state atom cannot decay to the ground state. In order to explain spontaneous transitions, quantum mechanics must be extended to a quantum field theory, wherein the electromagnetic field is quantized at every point in space. The quantum field theory of electrons and electromagnetic fields is known as quantum electrodynamics.

In quantum electrodynamics (or QED), the electromagnetic field has a ground state, the QED vacuum, which can mix with the excited stationary states of the atom. As a result of this interaction, the "stationary state" of the atom is no longer a true eigenstate of the combined system of the atom plus electromagnetic field. In particular, the electron transition from the excited state to the electronic ground state mixes with the transition of the electromagnetic field from the ground state to an excited state, a field state with one photon in it. Spontaneous emission in free space depends upon vacuum fluctuations to get started.

Although there is only one electronic transition from the excited state to ground state, there are many ways in which the electromagnetic field may go from the ground state to a one-photon state. That is, the electromagnetic field has infinitely more degrees of freedom, corresponding to the different directions in which the photon can be emitted. Equivalently, one might say that the phase space offered by the electromagnetic field is infinitely larger than that offered by the atom. This infinite degree of freedom for the emission of the photon results in the apparent irreversible decay, i.e., spontaneous emission.

In the presence of electromagnetic vacuum modes, the combined atom-vacuum system is explained by the superposition of the wavefunctions of the excited state atom with no photon and the ground state atom with a single emitted photon:

where formula_27 and formula_28 are the atomic excited state-electromagnetic vacuum wavefunction and its probability amplitude, formula_29 and formula_30 are the ground state atom with a single photon (of mode formula_31) wavefunction and its probability amplitude, formula_32 is the atomic transition frequency, and formula_33 is the frequency of the photon. The sum is over formula_34 and formula_35, which are the wavenumber and polarization of the emitted photon, respectively. As mentioned above, the emitted photon has a chance to be emitted with different wavenumbers and polarizations, and the resulting wavefunction is a superposition of these possibilities. To calculate the probability of the atom at the ground state (formula_36), one needs to solve the time evolution of the wavefunction with an appropriate Hamiltonian. To solve for the transition amplitude, one needs to average over (integrate over) all the vacuum modes, since one must consider the probabilities that the emitted photon occupies various parts of phase space equally. The "spontaneously" emitted photon has infinite different modes to propagate into, thus the probability of the atom re-absorbing the photon and returning to the original state is negligible, making the atomic decay practically irreversible. Such irreversible time evolution of the atom-vacuum system is responsible for the apparent spontaneous decay of an excited atom. If one were to keep track of all the vacuum modes, the combined atom-vacuum system would undergo unitary time evolution, making the decay process reversible. Cavity quantum electrodynamics is one such system where the vacuum modes are modified resulting in the reversible decay process, see also Quantum revival. The theory of the spontaneous emission under the QED framework was first calculated by Weisskopf and Wigner.

In spectroscopy one can frequently find that atoms or molecules in the excited states dissipate their energy in the absence of any external source of photons. This is not spontaneous emission, but is actually nonradiative relaxation of the atoms or molecules caused by the fluctuation of the surrounding molecules present inside the bulk.

The rate of spontaneous emission (i.e., the radiative rate) can be described by Fermi's golden rule. The rate of emission depends on two factors: an 'atomic part', which describes
the internal structure of the light source and a 'field part', which describes the density of electromagnetic modes of the environment. The atomic part describes the strength of a transition between two states in terms of transition moments. In a homogeneous medium, such as free space, the rate of spontaneous emission in the dipole approximation is given by:

where formula_3 is the emission frequency, formula_40 is the index of refraction, formula_41 is the transition dipole moment, formula_42 is the vacuum permittivity, formula_6 is the reduced Planck constant, formula_44 is the vacuum speed of light, and formula_45 is the fine structure constant. The expression formula_46 stands for the definition of the transition dipole moment formula_47 for dipole moment operator formula_48, where formula_49 is the elementary charge and formula_50 stands for position operator. (This approximation breaks down in the case of inner shell electrons in high-Z atoms.) The above equation clearly shows that the rate of spontaneous emission in free space increases proportionally to formula_51.

In contrast with atoms, which have a discrete emission spectrum, quantum dots can be tuned continuously by changing their size. This property has been used to check the formula_51-frequency dependence of the spontaneous emission rate as described by Fermi's golden rule.

In the rate-equation above, it is assumed that decay of the number of excited states formula_12 only occurs under emission of light. In this case one speaks of full radiative decay and this means that the quantum efficiency is 100%. Besides radiative decay, which occurs under the emission of light, there is a second decay mechanism; nonradiative decay. To determine the total decay rate formula_54, radiative and nonradiative rates should be summed:

where formula_54 is the total decay rate, formula_20 is the radiative decay rate and formula_58 the nonradiative decay rate. The quantum efficiency (QE) is defined as the fraction of emission processes in which emission of light is involved:

In nonradiative relaxation, the energy is released as phonons, more commonly known as heat. Nonradiative relaxation occurs when the energy difference between the levels is very small, and these typically occur on a much faster time scale than radiative transitions. For many materials (for instance, semiconductors), electrons move quickly from a high energy level to a meta-stable level via small nonradiative transitions and then make the final move down to the bottom level via an optical or radiative transition. This final transition is the transition over the bandgap in semiconductors. Large nonradiative transitions do not occur frequently because the crystal structure generally cannot support large vibrations without destroying bonds (which generally doesn't happen for relaxation). Meta-stable states form a very important feature that is exploited in the construction of lasers. Specifically, since electrons decay slowly from them, they can be deliberately piled up in this state without too much loss and then stimulated emission can be used to boost an optical signal.




</doc>
<doc id="28220" url="https://en.wikipedia.org/wiki?curid=28220" title="Nicolas Léonard Sadi Carnot">
Nicolas Léonard Sadi Carnot

"Sous-lieutenant" Nicolas Léonard Sadi Carnot (; 1 June 1796 – 24 August 1832) was a French mechanical engineer in the French Army, military scientist and physicist, often described as the "father of thermodynamics." Like Copernicus, he published only one book, the "Reflections on the Motive Power of Fire" (Paris, 1824), in which he expressed, at the age of 27 years, the first successful theory of the maximum efficiency of heat engines. In this work he laid the foundations of an entirely new discipline, thermodynamics. Carnot's work attracted little attention during his lifetime, but it was later used by Rudolf Clausius and Lord Kelvin to formalize the second law of thermodynamics and define the concept of entropy. His father used the suffix Sadi to name him because of his intense interest in the character of Saadi Shirazi, a well-known Iranian poet.

Nicolas Léonard Sadi Carnot was born in Paris into a family that was distinguished in both science and politics. He was the first son of Lazare Carnot, an eminent mathematician, military engineer and leader of the French Revolutionary Army. Lazare chose his son's third given name (by which he would always be known) after the Persian poet Sadi of Shiraz. Sadi was the elder brother of statesman Hippolyte Carnot and the uncle of Marie François Sadi Carnot, who would serve as President of France from 1887 to 1894.

At the age of 16, Sadi Carnot became a cadet in the École Polytechnique in Paris, where his classmates included Michel Chasles and Gaspard-Gustave Coriolis. The École Polytechnique was intended to train engineers for military service, but its professors included such eminent scientists as André-Marie Ampère, François Arago, Joseph Louis Gay-Lussac, Louis Jacques Thénard and Siméon Denis Poisson, and the school had become renowned for its mathematical instruction. After graduating in 1814, Sadi became an officer in the French army's corps of engineers. His father Lazare had served as Napoleon's minister of the interior during the "Hundred Days", and after Napoleon's final defeat in 1815 Lazare was forced into exile. Sadi's position in the army, under the restored Bourbon monarchy of Louis XVIII, became increasingly difficult.

Sadi Carnot was posted to different locations, he inspected fortifications, tracked plans and wrote many reports. It appears his recommendations were ignored and his career was stagnating. On 15 September 1818 he took a six-month leave to prepare for the entrance examination of Royal Corps of Staff and School of Application for the Service of the General Staff.

In 1819, Sadi transferred to the newly formed General Staff, in Paris. He remained on call for military duty, but from then on he dedicated most of his attention to private intellectual pursuits and received only two-thirds pay. Carnot befriended the scientist Nicolas Clément and attended lectures on physics and chemistry. He became interested in understanding the limitation to improving the performance of steam engines, which led him to the investigations that became his "Reflections on the Motive Power of Fire", published in 1824.

Carnot retired from the army in 1828, without a pension. He was interned in a private asylum in 1832 as suffering from "mania" and "general delirum", and he died of cholera shortly thereafter, aged 36, at the hospital in Ivry-sur-Seine.

When Carnot began working on his book, steam engines had achieved widely recognized economic and industrial importance, but there had been no real scientific study of them. Newcomen had invented the first piston-operated steam engine over a century before, in 1712; some 50 years after that, James Watt made his celebrated improvements, which were responsible for greatly increasing the efficiency and practicality of steam engines. Compound engines (engines with more than one stage of expansion) had already been invented, and there was even a crude form of internal-combustion engine, with which Carnot was familiar and which he described in some detail in his book. Although there existed some intuitive understanding of the workings of engines, scientific theory for their operation was almost nonexistent. In 1824 the principle of conservation of energy was still poorly developed and controversial, and an exact formulation of the first law of thermodynamics was still more than a decade away; the mechanical equivalence of heat would not be formulated for another two decades. The prevalent theory of heat was the caloric theory, which regarded heat as a sort of weightless and invisible fluid that flowed when out of equilibrium.

Engineers in Carnot's time had tried, by means such as highly pressurized steam and the use of fluids, to improve the efficiency of engines. In these early stages of engine development, the efficiency of a typical engine—the useful work it was able to do when a given quantity of fuel was burned—was only 3%.

Carnot wanted to answer two questions about the operation of heat engines: "Is the work available from a heat source potentially unbounded?" and "Can heat engines in principle be improved by replacing the steam with some other working fluid or gas?" He attempted to answer these in a memoir, published as a popular work in 1824 when he was only 27 years old. It was entitled "Réflexions sur la Puissance Motrice du Feu" ("Reflections on the Motive Power of Fire"). The book was plainly intended to cover a rather wide range of topics about heat engines in a rather popular fashion; equations were kept to a minimum and called for little more than simple algebra and arithmetic, except occasionally in the footnotes, where he indulged in a few arguments involving some calculus. He discussed the relative merits of air and steam as working fluids, the merits of various aspects of steam engine design, and even included some ideas of his own regarding possible practical improvements. The most important part of the book was devoted to an abstract presentation of an idealized engine that could be used to understand and clarify the fundamental principles that are generally applied to all heat engines, independent of their design.

Perhaps the most important contribution Carnot made to thermodynamics was his abstraction of the essential features of the steam engine, as they were known in his day, into a more general and idealized heat engine. This resulted in a model thermodynamic system upon which exact calculations could be made, and avoided the complications introduced by many of the crude features of the contemporary steam engine. By idealizing the engine, he could arrive at clear and indisputable answers to his original two questions.

He showed that the efficiency of this idealized engine is a function only of the two temperatures of the reservoirs between which it operates. He did not, however, give the exact form of the function, which was later shown to be (T−T)/T, where T is the absolute temperature of the hotter reservoir. (Note: This equation probably came from Kelvin.) No thermal engine operating any other cycle can be more efficient, given the same operating temperatures.

The Carnot cycle is the most efficient possible engine, not only because of the (trivial) absence of friction and other incidental wasteful processes; the main reason is that it assumes no conduction of heat between parts of the engine at different temperatures. Carnot knew that the conduction of heat between bodies at different temperatures is a wasteful and irreversible process, which must be eliminated if the heat engine is to achieve maximum efficiency.

Regarding the second point, he also was quite certain that the maximum efficiency attainable did not depend upon the exact nature of the working fluid. He stated this for emphasis as a general proposition:

For his "motive power of heat", we would today say "the efficiency of a reversible heat engine", and rather than "transfer of caloric" we would say "the reversible transfer of entropy ∆S" or "the reversible transfer of heat at a given temperature Q/T". He knew intuitively that his engine would have the maximum efficiency, but was unable to state what that efficiency would be.

He concluded:

and
In an idealized model, the caloric transported from a hot to a cold body by a frictionless heat engine that lacks of conductive heat flow, driven by a difference of temperature, yielding work, could also be used to transport the caloric back to the hot body by reversing the motion of the engine consuming the same amount of work, a concept subsequently known as thermodynamic reversibility. Carnot further postulated that no caloric is lost during the operation of his idealized engine. The process being completely reversible, executed by this kind of heat engine is the most efficient possible process. The assumption that heat conduction driven by a temperature difference cannot exist, so that no caloric is lost by the engine, guided him to design the Carnot-cycle to be operated by his idealized engine. The cycle is consequently composed of adiabatic processes where no heat/caloric ∆S = 0 flows and isothermal processes where heat is transferred ∆S > 0 but no temperature difference ∆T = 0 exist. The proof of the existence of a maximum efficiency for heat engines is as follows:

As the cycle named after him doesn't waste caloric, the reversible engine has to use this cycle. Imagine now two large bodies, a hot and a cold one. He postulates now the existence of a heat machine with a greater efficiency. We couple now two idealized machine but of different efficiencies and connect them to the same hot and the same cold body. The first and less efficient one lets a constant amount of entropy ∆S = Q/T flow from hot to cold during each cycle, yielding an amount of work denoted W. If we use now this work to power the other more efficient machine, it would, using the amount of work W gained during each cycle by the first machine, make an amount of entropy ∆S' > ∆S flow from the cold to the hot body. The net effect is a flow of ∆S' − ∆S ≠ 0 of entropy from the cold to the hot body, while no net work is done. Consequently, the cold body is cooled down and the hot body rises in temperature. As the difference of temperature rises now the yielding of work by the first is greater in the successive cycles and due to the second engine difference in temperature of the two bodies stretches by each cycle even more. In the end this set of machines would be a perpetuum mobile that cannot exist. This proves that the assumption of the existence of a more efficient engine was wrong so that an heat engine that operates the Carnot cycle must be the most efficient one. This means that a frictionless heat engine that lacks of conductive heat flow driven by a difference of temperature shows maximum possible efficiency.

He concludes further that the choice of the working fluid, its density or the volume occupied by it cannot change this maximum efficiency. Using the equivalence of any working gas used in heat engines he deduced that the difference in the specific heat of a gas measured at constant pressure and at constant volume must be constant for all gases.
By comparing the operation of his hypothetical heat engines for two different volumes occupied by the same amount of working gas he correctly deduces the relation between entropy and volume for an isothermal process:

formula_1

Carnot's book received very little attention from his contemporaries. The only reference to it within a few years after its publication was in a review in the periodical "Revue Encyclopédique", which was a journal that covered a wide range of topics in literature. The impact of the work had only become apparent once it was modernized by Émile Clapeyron in 1834 and then further elaborated upon by Clausius and Kelvin, who together derived from it the concept of entropy and the second law of thermodynamics.

On Carnot's religious views, he was a Philosophical theist. As a deist, he believed in divine causality, stating that "what to an ignorant man is chance, cannot be chance to one better instructed," but he did not believe in divine punishment. He criticized established religion, though at the same time spoke in favor of "the belief in an all-powerful Being, who loves us and watches over us."

He was a reader of Blaise Pascal, Molière and Jean de La Fontaine.

Carnot died during a cholera epidemic in 1832, at the age of 36. 
Because of the contagious nature of cholera, many of Carnot's belongings and writings were buried together with him after his death. As a consequence, only a handful of his scientific writings survived.

After the publication of "Reflections on the Motive Power of Fire", the book quickly went out of print and for some time was very difficult to obtain. Kelvin, for one, had a difficult time getting a copy of Carnot's book. In 1890 an English translation of the book was published by R. H. Thurston; this version has been reprinted in recent decades by Dover and by Peter Smith, most recently by Dover in 2005. Some of Carnot's posthumous manuscripts have also been translated into English.

Carnot published his book in the heyday of steam engines. His theory explained why steam engines using superheated steam were better because of the higher temperature of the consequent hot reservoir. Carnot's theories and efforts did not immediately help improve the efficiency of steam engines; his theories only helped to explain why one existing practice was superior to others. It was only towards the end of the nineteenth century that Carnot's ideas, namely that a heat engine can be made more efficient if the temperature of its hot reservoir is increased, were put into practice. Carnot's book did, however, eventually have a real impact on the design of practical engines. Rudolf Diesel, for example, used Carnot's theories to design the diesel engine, in which the temperature of the hot reservoir is much higher than that of a steam engine, resulting in an engine which is more efficient.






</doc>
<doc id="28221" url="https://en.wikipedia.org/wiki?curid=28221" title="Suleiman I">
Suleiman I

Suleiman I may refer to:



</doc>
<doc id="28222" url="https://en.wikipedia.org/wiki?curid=28222" title="Sydney Opera House">
Sydney Opera House

The Sydney Opera House is a multi-venue performing arts centre at Sydney Harbour in Sydney, New South Wales, Australia. It is one of the 20th century's most famous and distinctive buildings.

Designed by Danish architect Jørn Utzon, but completed by an Australian architectural team headed up by Peter Hall, the building was formally opened on 20 October 1973 after a gestation beginning with Utzon's 1957 selection as winner of an international design competition. The Government of New South Wales, led by the premier, Joseph Cahill, authorised work to begin in 1958 with Utzon directing construction. The government's decision to build Utzon's design is often overshadowed by circumstances that followed, including cost and scheduling overruns as well as the architect's ultimate resignation.

The building and its surrounds occupy the whole of Bennelong Point on Sydney Harbour, between Sydney Cove and Farm Cove, adjacent to the Sydney central business district and the Royal Botanic Gardens, and close by the Sydney Harbour Bridge.

The building comprises multiple performance venues, which together host well over 1,500 performances annually, attended by more than 1.2 million people. Performances are presented by numerous performing artists, including three resident companies: Opera Australia, the Sydney Theatre Company and the Sydney Symphony Orchestra. As one of the most popular visitor attractions in Australia, the site is visited by more than eight million people annually, and approximately 350,000 visitors take a guided tour of the building each year. The building is managed by the Sydney Opera House Trust, an agency of the New South Wales State Government.

On 28 June 2007, the Sydney Opera House became a UNESCO World Heritage Site, having been listed on the (now defunct) Register of the National Estate since 1980, the National Trust of Australia register since 1983, the City of Sydney Heritage Inventory since 2000, the New South Wales State Heritage Register since 2003, and the Australian National Heritage List since 2005. Furthermore, the Opera House was a finalist in the "New7Wonders of the World" campaign list.

The facility features a modern expressionist design, with a series of large precast concrete "shells", each composed of sections of a sphere of radius, forming the roofs of the structure, set on a monumental podium. The building covers of land and is long and wide at its widest point. It is supported on 588 concrete piers sunk as much as below sea level. The highest roof point is 67 metres above sea-level which is the same height as that of a 22-storey building. The roof is made of 2,194 pre-cast concrete sections, which weigh up to 15 tonnes each.

Although the roof structures are commonly referred to as "shells" (as in this article), they are precast concrete panels supported by precast concrete ribs, not shells in a strictly structural sense. Though the shells appear uniformly white from a distance, they actually feature a subtle chevron pattern composed of 1,056,006 tiles in two colours: glossy white and matte cream. The tiles were manufactured by the Swedish company Höganäs AB which generally produced stoneware tiles for the paper-mill industry.

Apart from the tile of the shells and the glass curtain walls of the foyer spaces, the building's exterior is largely clad with aggregate panels composed of pink granite quarried at Tarana. Significant interior surface treatments also include off-form concrete, Australian white birch plywood supplied from Wauchope in northern New South Wales, and brush box glulam.

Of the two larger spaces, the Concert Hall is in the western group of shells, the Joan Sutherland Theatre in the eastern group. The scale of the shells was chosen to reflect the internal height requirements, with low entrance spaces, rising over the seating areas up to the high stage towers. The smaller venues (the Drama Theatre, the Playhouse and the Studio) are within the podium, beneath the Concert Hall. A smaller group of shells set to the western side of the Monumental Steps houses the Bennelong Restaurant. The podium is surrounded by substantial open public spaces, and the large stone-paved forecourt area with the adjacent monumental steps is regularly used as a performance space.

The Sydney Opera House includes a number of performance venues:

Other areas (for example the northern and western foyers) are also used for performances on an occasional basis. Venues are also used for conferences, ceremonies and social functions.

The building also houses a recording studio, cafes, restaurants, bars and retail outlets. Guided tours are available, including a frequent tour of the front-of-house spaces, and a daily backstage tour that takes visitors backstage to see areas normally reserved for performers and crew members.

Planning began in the late 1940s when Eugene Goossens, the Director of the NSW State Conservatorium of Music, lobbied for a suitable venue for large theatrical productions. The normal venue for such productions, the Sydney Town Hall, was not considered large enough. By 1954, Goossens succeeded in gaining the support of NSW Premier Joseph Cahill, who called for designs for a dedicated opera house. It was also Goossens who insisted that Bennelong Point be the site: Cahill had wanted it to be on or near Wynyard Railway Station in the northwest of the CBD.

An international design competition was launched by Cahill on 13 September 1955 and received 233 entries, representing architects from 32 countries. The criteria specified a large hall seating 3,000 and a small hall for 1,200 people, each to be designed for different uses, including full-scale operas, orchestral and choral concerts, mass meetings, lectures, ballet performances, and other presentations.

The winner, announced in 1957, was Jørn Utzon, a Danish architect. According to legend the Utzon design was rescued by noted Finnish-American architect Eero Saarinen from a final cut of 30 "rejects". The runner-up was a Philadelphia-based team assembled by Robert Geddes and George Qualls, both teaching at the University of Pennsylvania School of Design. They brought together a band of Penn faculty and friends from Philadelphia architectural offices, including Melvin Brecher, Warren Cunningham, Joseph Marzella, Walter Wiseman, and Leon Loschetter. Geddes, Brecher, Qualls, and Cunningham went on to found the firm GBQC Architects. The grand prize was 5,000 Australian pounds. Utzon visited Sydney in 1957 to help supervise the project. His office moved to Palm Beach, Sydney in February 1963.

Utzon received the Pritzker Architecture Prize, architecture's highest honour, in 2003. The Pritzker Prize citation read:
The Fort Macquarie Tram Depot, occupying the site at the time of these plans, was demolished in 1958 and construction began in March 1959. It was built in three stages: stage I (1959–1963) consisted of building the upper podium; stage II (1963–1967) the construction of the outer shells; stage III (1967–1973) interior design and construction.

Stage I commenced on 2 March 1959 with the construction firm Civil & Civic, monitored by the engineers Ove Arup and Partners. The government had pushed for work to begin early, fearing that funding, or public opinion, might turn against them. However, Utzon had still not completed the final designs. Major structural issues still remained unresolved. By 23 January 1961, work was running 47 weeks behind, mainly because of unexpected difficulties (inclement weather, unexpected difficulty diverting stormwater, construction beginning before proper construction drawings had been prepared, changes of original contract documents). Work on the podium was finally completed in February 1963. The forced early start led to significant later problems, not least of which was the fact that the podium columns were not strong enough to support the roof structure, and had to be re-built.

The shells of the competition entry were originally of undefined geometry, but, early in the design process, the "shells" were perceived as a series of parabolas supported by precast concrete ribs. However, engineers Ove Arup and Partners were unable to find an acceptable solution to constructing them. The formwork for using "in-situ" concrete would have been prohibitively expensive, and, because there was no repetition in any of the roof forms, the construction of precast concrete for each individual section would possibly have been even more expensive.

From 1957 to 1963, the design team went through at least 12 iterations of the form of the shells trying to find an economically acceptable form (including schemes with parabolas, circular ribs and ellipsoids) before a workable solution was completed. The design work on the shells involved one of the earliest uses of computers in structural analysis, to understand the complex forces to which the shells would be subjected. The computer system was also used in the assembly of the arches. The pins in the arches were surveyed at the end of each day, and the information was entered into the computer so the next arch could be properly placed the following day. In mid-1961, the design team found a solution to the problem: the shells all being created as sections from a sphere. This solution allows arches of varying length to be cast in a common mould, and a number of arch segments of common length to be placed adjacent to one another, to form a spherical section. With whom exactly this solution originated has been the subject of some controversy. It was originally credited to Utzon. Ove Arup's letter to Ashworth, a member of the Sydney Opera House Executive Committee, states: "Utzon came up with an idea of making all the shells of uniform curvature throughout in both directions." Peter Jones, the author of Ove Arup's biography, states that "the architect and his supporters alike claimed to recall the precise "eureka" moment ... ; the engineers and some of their associates, with equal conviction, recall discussion in both central London and at Ove's house."

He goes on to claim that "the existing evidence shows that Arup's canvassed several possibilities for the geometry of the shells, from parabolas to ellipsoids and spheres." Yuzo Mikami, a member of the design team, presents an opposite view in his book on the project, "Utzon's Sphere". It is unlikely that the truth will ever be categorically known, but there is a clear consensus that the design team worked very well indeed for the first part of the project and that Utzon, Arup, and Ronald Jenkins (partner of Ove Arup and Partners responsible for the Opera House project) all played a very significant part in the design development.

As Peter Murray states in "The Saga of the Sydney Opera House":

The design of the roof was tested on scale models in wind tunnels at University of Southampton and later NPL in order to establish the wind-pressure distribution around the roof shape in very high winds, which helped in the design of the roof tiles and their fixtures.

The shells were constructed by Hornibrook Group Pty Ltd, who were also responsible for construction in Stage III. Hornibrook manufactured the 2400 precast ribs and 4000 roof panels in an on-site factory and also developed the construction processes. The achievement of this solution avoided the need for expensive formwork construction by allowing the use of precast units and it also allowed the roof tiles to be prefabricated in sheets on the ground, instead of being stuck on individually at height.

The tiles themselves were manufactured by the Swedish company Höganäs Keramik. It took three years of development to produce the effect Utzon wanted in what became known as the Sydney Tile, 120mm square. It is made from clay with a small percentage of crushed stone.

Ove Arup and Partners' site engineer supervised the construction of the shells, which used an innovative adjustable steel-trussed "erection arch" (developed by Hornibrook's engineer Joe Bertony) to support the different roofs before completion. On 6 April 1962, it was estimated that the Opera House would be completed between August 1964 and March 1965.

Stage III, the interiors, started with Utzon moving his entire office to Sydney in February 1963. However, there was a change of government in 1965, and the new Robert Askin government declared the project under the jurisdiction of the Ministry of Public Works. Due to the Ministry's criticism of the project's costs and time, along with their impression of Utzon's designs being impractical, this ultimately led to his resignation in 1966 (see below).

The cost of the project so far, even in October 1966, was still only A$22.9 million, less than a quarter of the final $102 million cost. However, the projected costs for the design were at this stage much more significant.

The second stage of construction was progressing toward completion when Utzon resigned. His position was principally taken over by Peter Hall, who became largely responsible for the interior design. Other persons appointed that same year to replace Utzon were E. H. Farmer as government architect, D. S. Littlemore and Lionel Todd.

Following Utzon's resignation, the acoustic advisor, Lothar Cremer, confirmed to the Sydney Opera House Executive Committee (SOHEC) that Utzon's original acoustic design allowed for only 2,000 seats in the main hall and further stated that increasing the number of seats to 3,000 as specified in the brief would be disastrous for the acoustics. According to Peter Jones, the stage designer, Martin Carr, criticised the "shape, height and width of the stage, the physical facilities for artists, the location of the dressing rooms, the widths of doors and lifts, and the location of lighting switchboards."


The Opera House was formally completed in 1973, having cost $102 million. H.R. "Sam" Hoare, the Hornibrook director in charge of the project, provided the following approximations in 1973:
Stage I: podium Civil & Civic Pty Ltd approximately $5.5m.
Stage II: roof shells M.R. Hornibrook (NSW) Pty Ltd approximately $12.5m.
Stage III: completion The Hornibrook Group $56.5m.
Separate contracts: stage equipment, stage lighting and organ $9.0m. Fees and other costs: $16.5m.

The original cost and scheduling estimates in 1957 projected a cost of £3,500,000 ($7 million) and completion date of 26 January 1963 (Australia Day). In reality, the project was completed ten years late and 1,357% over budget in real terms.

In 1972, a construction worker was fired, leading the BLF affiliated workers to demand his rehiring and a 25% wage increase. In response to this, all the workers were fired, and in revenge the workers broke into the construction site with a crowbar and brought their own toolboxes. Workers' control was applied to the site for 5 weeks as the construction workers worked 35 hours a week with improved morale, more efficient organization and fewer people skipping work. The workers agreed to end their work-in when management agreed to give them a 25% wage increase, the right to elect their foremen, four weeks annual leave and a large payment for their troubles.

Before the Sydney Opera House competition, Jørn Utzon had won seven of the 18 competitions he had entered but had never seen any of his designs built. Utzon's submitted concept for the Sydney Opera House was almost universally admired and considered groundbreaking. The Assessors Report of January 1957, stated:

For the first stage, Utzon worked successfully with the rest of the design team and the client, but, as the project progressed, the Cahill government insisted on progressive revisions. They also did not fully appreciate the costs or work involved in design and construction. Tensions between the client and the design team grew further when an early start to construction was demanded despite an incomplete design. This resulted in a continuing series of delays and setbacks while various technical engineering issues were being refined. The building was unique, and the problems with the design issues and cost increases were exacerbated by commencement of work before the completion of the final plans.

After the 1965 election of the Liberal Party, with Robert Askin becoming Premier of New South Wales, the relationship of client, architect, engineers and contractors became increasingly tense. Askin had been a "vocal critic of the project prior to gaining office." His new Minister for Public Works, Davis Hughes, was even less sympathetic. Elizabeth Farrelly, an Australian architecture critic, wrote that:
Differences ensued. One of the first was that Utzon believed the clients should receive information on all aspects of the design and construction through his practice, while the clients wanted a system (notably drawn in sketch form by Davis Hughes) where architect, contractors, and engineers each reported to the client directly and separately. This had great implications for procurement methods and cost control, with Utzon wishing to negotiate contracts with chosen suppliers (such as Ralph Symonds for the plywood interiors) and the New South Wales government insisting contracts be put out to tender.

Utzon was highly reluctant to respond to questions or criticism from the client's Sydney Opera House Executive Committee (SOHEC). However, he was greatly supported throughout by a member of the committee and one of the original competition judges, Harry Ingham Ashworth. Utzon was unwilling to compromise on some aspects of his designs that the clients wanted to change.

Utzon's ability was never in doubt, despite questions raised by Davis Hughes, who attempted to portray Utzon as an impractical dreamer. Ove Arup actually stated that Utzon was "probably the best of any I have come across in my long experience of working with architects" and: "The Opera House could become the world's foremost contemporary masterpiece if Utzon is given his head."
In October 1965, Utzon gave Hughes a schedule setting out the completion dates of parts of his work for stage III. Utzon was at this time working closely with Ralph Symonds, a manufacturer of plywood based in Sydney and highly regarded by many, despite an Arup engineer warning that Ralph Symonds's "knowledge of the design stresses of plywood, was extremely sketchy" and that the technical advice was "elementary to say the least and completely useless for our purposes." Australian architecture critic Elizabeth Farrelly has referred to Ove Arup's project engineer Michael Lewis as having "other agendas". In any case, Hughes shortly after withheld permission for the construction of plywood prototypes for the interiors, and the relationship between Utzon and the client never recovered. By February 1966, Utzon was owed more than $100,000 in fees. Hughes then withheld funding so that Utzon could not even pay his own staff. The government minutes record that following several threats of resignation, Utzon finally stated to Davis Hughes: "If you don't do it, I resign." Hughes replied: "I accept your resignation. Thank you very much. Goodbye."

Utzon left the project on 28 February 1966. He said that Hughes's refusal to pay him any fees and the lack of collaboration caused his resignation and later famously described the situation as "Malice in Blunderland". In March 1966, Hughes offered him a subordinate role as "design architect" under a panel of executive architects, without any supervisory powers over the House's construction, but Utzon rejected this. Utzon left the country never to return.

Following the resignation, there was great controversy about who was in the right and who was in the wrong. "The Sydney Morning Herald" initially opined: "No architect in the world has enjoyed greater freedom than Mr Utzon. Few clients have been more patient or more generous than the people and the Government of NSW. One would not like history to record that this partnership was brought to an end by a fit of temper on the one side or by a fit of meanness on the other." On 17 March 1966, the "Herald" offered the view that: "It was not his [Utzon's] fault that a succession of Governments and the Opera House Trust should so signally have failed to impose any control or order on the project ... his concept was so daring that he himself could solve its problems only step by step ... his insistence on perfection led him to alter his design as he went along."

The Sydney Opera House opened the way for the immensely complex geometries of some modern architecture. The design was one of the first examples of the use of computer-aided design to design complex shapes. The design techniques developed by Utzon and Arup for the Sydney Opera House have been further developed and are now used for architecture, such as works of Gehry and blobitecture, as well as most reinforced concrete structures. The design is also one of the first in the world to use araldite to glue the precast structural elements together and proved the concept for future use.

It was also a first in mechanical engineering. Another Danish firm, Steensen Varming, was responsible for designing the new air-conditioning plant, the largest in Australia at the time, supplying over of air per minute, using the innovative idea of harnessing the harbour water to create a water-cooled heat pump system that is still in operation today.

After the resignation of Utzon, the Minister for Public Works, Davis Hughes, and the Government Architect, Ted Farmer, organised a team to bring the Sydney Opera House to completion. The architectural work was divided between three appointees who became the Hall, Todd, Littlemore partnership. David Littlemore would manage construction supervision, Lionel Todd contract documentation, while the crucial role of design became the responsibility of Peter Hall.

Peter Hall (1931–1995) completed a combined arts and architecture degree at Sydney University. Upon graduation a travel scholarship enabled him to spend twelve months in Europe during which time he visited Utzon in Hellebæk. Returning to Sydney, Hall worked for the Government Architect, a branch of the NSW Public Works Department. While there he established himself as a talented design architect with a number of court and university buildings, including the Goldstein Hall at the University of New South Wales, which won the Sir John Sulman Medal in 1964.

Hall resigned from the Government Architects office in early 1966 to pursue his own practice. When approached to take on the design role, (after at least two prominent Sydney architects had declined), Hall spoke with Utzon by phone before accepting the position. Utzon reportedly told Hall: he (Hall) would not be able to finish the job and the Government would have to invite him back. Hall also sought the advice of others, including architect Don Gazzard who warned him acceptance would be a bad career move as the project would "never be his own". 

Hall agreed to accept the role on the condition there was no possibility of Utzon returning. Even so, his appointment did not go down well with many of his fellow architects who considered that no one but Utzon should complete the Sydney Opera House. Upon Utzon's dismissal, a rally of protest had marched to Bennelong Point. A petition was also circulated, including in the Government Architects office. Peter Hall was one of the many who had signed the petition that called for Utzon's reinstatement.

When Hall agreed to the design role and was appointed in April 1966, he imagined he would find the design and documentation for the Stage III well advanced. What he found was an enormous amount of work ahead of him with many aspects completely unresolved by Utzon in relation to seating capacity, acoustics and structure. In addition Hall found the project had proceeded for nine years without the development of a concise client brief. To bring himself up to speed, Hall investigated concert and opera venues overseas and engaged stage consultant Ben Schlange and acoustic consultant Wilhelm Jordan, while establishing his team. In consultation with all the potential building users, the first Review of Program was completed in January 1967. The most significant conclusion reached by Hall was that concert and opera were incompatible in the same hall. Although Utzon had sketched ideas using plywood for the great enclosing glass walls, their structural viability was unresolved when Hall took on the design role. With the ability to delegate tasks and effectively coordinate the work of consultants, Hall guided the project for over five years until the opening day in 1973.

A former Government Architect, Peter Webber, in his book "Peter Hall: the Phantom of the Opera House", concludes: when Utzon resigned no one was better qualified (than Hall) to rise to the challenge of completing the design of the Opera House.

The Sydney Opera House was formally opened by Queen Elizabeth II, Queen of Australia on 20 October 1973. A large crowd attended. Utzon was not invited to the ceremony, nor was his name mentioned. The opening was televised and included fireworks and a performance of Beethoven's Symphony No. 9.

During the construction phase, lunchtime performances were often arranged for the workers, with American vocalist Paul Robeson the first artist to perform, in 1960.

Various performances were presented prior to the official opening:

After the opening:

In the late 1990s, the Sydney Opera House Trust resumed communication with Utzon in an attempt to effect a reconciliation and to secure his involvement in future changes to the building. In 1999, he was appointed by the Trust as a design consultant for future work.
In 2004, the first interior space rebuilt to an Utzon design was opened, and renamed "The Utzon Room" in his honour. It contains an original Utzon tapestry (14.00 x 3.70 metres) called "Homage to Carl Philipp Emmanuel Bach". In April 2007, he proposed a major reconstruction of the Opera Theatre, as it was then known. Utzon died on 29 November 2008.

A state memorial service, attended by Utzon's son Jan and daughter Lin, celebrating his creative genius, was held in the Concert Hall on 25 March 2009 featuring performances, readings and recollections from prominent figures in the Australian performing arts scene.

Refurbished Western Foyer and Accessibility improvements were commissioned on 17 November 2009, the largest building project completed since Utzon was re-engaged in 1999. Designed by Utzon and his son Jan, the project provided improved ticketing, toilet and cloaking facilities. New escalators and a public lift enabled enhanced access for the disabled and families with prams. The prominent paralympian athlete Louise Sauvage was announced as the building's "accessibility ambassador" to advise on further improvements to aid people with disabilities.

On 29 March 2016, an original 1959 tapestry by Le Corbusier (2.18 x 3.55 metres), commissioned by Utzon to be hung in the Sydney Opera House and called "Les Dés Sont Jetés" (The Dice Are Cast), was finally unveiled "in situ" after being owned by the Utzon family and held at their home in Denmark for over 50 years. The tapestry was bought at auction by the Sydney Opera House in June 2015. It now hangs in the building's Western Foyer and is accessible to the public.

In the second half of 2017, the Joan Sutherland Theatre was closed to replace the stage machinery and for other works. The Concert Hall is scheduled for work in 2020–2021.

In 1993, Constantine Koukias was commissioned by the Sydney Opera House Trust in association with REM Theatre to compose "Icon", a large-scale music theatre piece for the 20th anniversary of the Sydney Opera House.

During the 2000 Summer Olympics, the venue served as the focal point for the triathlon events. The event had a swimming loop at Farm Cove, along with competitions in the neighbouring Royal Botanical Gardens for the cycling and running portions of the event.

Since 2013, a group of residents from the nearby Bennelong Apartments (better known as 'The Toaster'), calling themselves the Sydney Opera House Concerned Citizens Group, have been campaigning against Forecourt Concerts on the grounds that they exceed noise levels outlined in the development approval (DA). In February 2017 the NSW Department of Planning and the Environment handed down a $15,000 fine to the Sydney Opera House for breach of allowed noise levels at a concert held in November 2015. However, the DA was amended in 2016 to allow an increase in noise levels in the forecourt by 5 decibels. The residents opposing the concerts contend that a new DA should have been filed rather than an amendment.

The Sydney Opera House sails formed a graphic projection-screen in a lightshow mounted in connection with the International Fleet Review in Sydney Harbour on 5 October 2013.

On 31 December 2013, the venue's 40th anniversary year, a New Year firework display was mounted for the first time in a decade. The Sydney Opera House hosted an event, 'the biggest blind date' on Friday 21 February 2014 that won an historic Guinness World Record. The longest continuous serving employee was commemorated on 27 June 2018, for 50 years of service.

On 14 June 2019, a state memorial service for former Australian Prime Minister Bob Hawke was held at the Sydney Opera House.

On 5 October 2018 the Opera House chief executive Louise Herron clashed with Sydney radio commentator Alan Jones, who called for her sacking for refusing to allow Racing NSW to use the Opera House sails to advertise The Everest horse race. Within hours, NSW Premier Gladys Berejiklian overruled Herron. Two days later, Prime Minister Scott Morrison supported the decision, calling the Opera House "the biggest billboard Sydney has". The NSW Labor Party leader, Luke Foley, and senior federal Labor frontbencher Anthony Albanese had supported the proposal. The political view was not supported by significant public opinion, with a petition against the advertising collecting over 298,000 names by 9 October 2018. 235,000 printed petition documents were presented to the NSW Parliament in the morning. A survey conducted on 8 October by market research firm Micromex found that 81% of those surveyed were not supportive of the premier's direction.



The opera house, along with the harbour bridge, frequently features in establishing shots in film and television to represent Sydney and the Australian nation.


 This Wikipedia article contains material from "Sydney Opera House", listed on the "New South Wales State Heritage Register" published by the Government of New South Wales under CC-BY 3.0 AU licence (accessed on 3 September 2017).





</doc>
<doc id="28223" url="https://en.wikipedia.org/wiki?curid=28223" title="Selim II">
Selim II

Selim II (Ottoman Turkish: سليم ثانى "Selīm-i sānī", Turkish: "II.Selim"; 28 May 1524 – 15 December 1574), also known as "Sarı Selim" ("Selim the Blond") or "Sarhoş Selim" ("Selim the Drunk"), was the Sultan of the Ottoman Empire from 1566 until his death in 1574. He was a son of Suleiman the Magnificent and his wife Hurrem Sultan. Selim had been an unlikely candidate for the throne until his brother Mehmed died of smallpox, his half-brother Mustafa was strangled to death by the order of his father, his brother Cihangir died of grief at the news of this latter execution, and his brother Bayezid was killed on the order of his father after a rebellion against Selim.

Selim died on 15 December 1574 and was buried in Hagia Sophia.

Selim was born in Constantinople (Istanbul), on 30 May 1524 , during the reign of his father Suleiman the Magnificent. His mother was Hurrem Sultan, a slave and concubine who was born an Orthodox priest's daughter, and later was freed and became Suleiman's legal wife.

In 1545, at Konya, Selim married Nurbanu Sultan, whose background is disputed. It is said that she was originally named Cecelia Venier Baffo, or Rachel, or Kale Katenou. She was the mother of Murad III, Selim's successor. 

Hubbi Hatun, a famous poet of the sixteenth century, was a lady-in-waiting to him.

Selim II gained the throne after palace intrigue and fraternal dispute, succeeding as sultan on the 7th of September 1566. Selim's Grand Vizier, Mehmed Sokollu and wife, Nurbanu Sultan, a native of what is now Bosnia and Herzegovina, controlled much of state affairs, and two years after Selim's accession succeeded in concluding at Constantinople a treaty (17 February 1568) with the Habsburg Holy Roman Emperor, Maximilian II, whereby the Emperor agreed to pay an annual "present" of 30,000 ducats and granted the Ottomans authority in Moldavia and Walachia. Gazanfer Agha (d. 1602), a friend to Selim and to the writer Mustafa Ali, was castrated so he could serve in Selim's harem. (Gazanfer's younger brother Cafer was also castrated, but did not survive.)

A plan had been prepared in Constantinople for uniting the Volga and Don by a canal in order to counter Russian expansion toward the Ottomans' northern frontier. In the summer of 1569 a large force of Janissaries and cavalry were sent to lay siege to Astrakhan and begin the canal works, while an Ottoman fleet besieged Azov. However, a sortie from the Astrakhan garrison drove back the besiegers. A Russian relief army of 15,000 attacked and scattered the workmen and the Tatar force sent for their protection. The Ottoman fleet was then destroyed by a storm. Early in 1570 the ambassadors of Ivan IV of Russia concluded at Istanbul a treaty that restored friendly relations between the Sultan and the Tsar.

Expeditions in the Hejaz and Yemen were more successful, but the conquest of Cyprus in 1571, led to the naval defeat against Spain and Italian states in the Battle of Lepanto in the same year.

The Empire's shattered fleets were soon restored (in just six months, it consisted of about 150 galleys and eight galleasses), and the Ottomans maintained control of the eastern Mediterranean (1573). In August 1574, months before Selim's death, the Ottomans regained control of Tunis from Spain, which had captured it in 1572.

Selim is known for restoring to Mahidevran Sultan her status and her wealth. He also built the tomb of his eldest brother, Şehzade Mustafa, who was executed in 1553.

In the famine of 1573, due to severe cold the farmers of that time did not provide good for the people. Selim gave people food and vegetables in the food kitchen. In April 1574, a fire broke in the printing house of Topkapi Palace and burned many rooms including the cooks and maids were burned and the kitchen was also burned. A few days later the captain, the Janissary, Istanbul lord and Mimar Sinan determined the location and size of the new kitchens to come to the fire. Mimar Sinan Ağa cleaned the fire place of the "building of the official building (design)" of Üslüb-ı ahar. The construction of the new Matbalı-ı Amir, which is broader and longer than the previous one, was taken from Divan-ı Ali Square. 

He is introduced as a generous monarch who is fond of pleasure and entertainment in the sources of the period, who is fond of drink councils, enjoys the presence of scholars and poets around him, as well as musicians, wrestlers, connoisseurs such as connoisseurs, who do not want to break the hearts of anyone. However, it is stated that he did not appear much in public, that his father often went to Friday and he went public and he neglected this and spent time in the palace. 

Selim's first and only wife, Nurbanu Sultan, was a Venetian who was the mother of his successor Murad III and three of his daughters. As a Haseki Sultan she received 1,000 aspers a day, while lower-ranking concubines who were the mothers of princes received 40 aspers a day. Selim bestowed upon Nurbanu 110,000 ducats as a dowry, surpassing the 100,000 ducats that his father bestowed upon his mother Hürrem Sultan. According to a privy purse register cited by Leslie Pierce, Selim had four other women, and each of them was mother of a prince.


Selim had seven sons:


Selim had at least four daughters:




<BR>

[aged 50]


</doc>
<doc id="28224" url="https://en.wikipedia.org/wiki?curid=28224" title="Smith">
Smith

Smith may refer to:












</doc>
<doc id="28226" url="https://en.wikipedia.org/wiki?curid=28226" title="Show business">
Show business

Show business, sometimes shortened to show biz or showbiz (since 1945), is a vernacular term for all aspects of the entertainment industry. From the business side (including managers, agents, producers, and distributors), the term applies to the creative element (including artists, performers, writers, musicians, and technicians) and was in common usage throughout the 20th century, though the first known use in print dates from 1850. At that time and for several decades, it typically included an initial "the". By the latter part of the century, it had acquired a slightly arcane quality associated with the era of variety, but the term is still in active use. In modern entertainment industry, it is also associated with the fashion industry (creating trend and fashion) and acquiring intellectual property rights from the invested research in the entertainment business.

The global media and entertainment (M&E) market, including film, television shows and advertising, streaming media, music, broadcasting, radio, book publishing, video games, and ancillary services and products) was worth $1.72 trillion in 2015, $1.9 trillion in 2016, with extrapolations ranging to $2.14 trillion by 2020. About one third of the total ($735 billion in 2017) is made up by the U.S. entertainment industry, the largest M&E in the world.

The entertainment sector can be split up into the following subsectors:


The industry segment is covered by class "R" of the International Standard Industrial Classification: "Arts, entertainment and recreation".



</doc>
<doc id="28230" url="https://en.wikipedia.org/wiki?curid=28230" title="Speaker for the Dead">
Speaker for the Dead

Speaker for the Dead is a 1986 science fiction novel by American writer Orson Scott Card, an indirect sequel to the novel "Ender's Game". The book takes place around the year 5270, some 3,000 years after the events in "Ender's Game". However, because of relativistic space travel at near-light speed, Ender himself is only about 35 years old.

This is the first book to discuss the Starways Congress, a high standpoint legislation for the human space colonies. It is also the first to describe the Hundred Worlds, the planets with human colonies that are tightly intertwined by Ansible technology which enables instantaneous communication, even across light years of distance.

Like "Ender's Game", the book won the Nebula Award in 1986 and the Hugo Award in 1987. "Speaker for the Dead" was published in a slightly revised edition in 1991. It was followed by "Xenocide" and "Children of the Mind".

Some years after the xenocide of the Formic species (in "Ender's Game"), Ender Wiggin writes a book called "The Hive Queen", describing the life of the Formics as described to him by the dormant Formic Queen whom he secretly carries. As humanity uses light-speed travel to establish new colonies, Ender and his sister Valentine age slowly through relativistic travel. Ender's older brother, the now-aged Hegemon Peter Wiggin, recognizes Ender's writings in "The Hive Queen", and requests Ender write for him once he dies. Ender agrees and authors "The Hegemon". These two books, written under the pseudonym "Speaker for the Dead", launch a new religious movement of Speakers, who have authority to investigate and eulogize a person and their work after their death.

Three thousand years after the Formic xenocide, humans have spread across the Hundred Worlds, ruled by Starways Congress. A Brazilian Catholic human colony called Milagre is established on the planet Lusitania (1886 S.C.). The planet is home to a sentient species of symbiotic forest dwellers. The colonists (who primarily speak Portuguese) dub them "Pequeninos" (Little Ones) but they are often referred to as "piggies" due to their porcine snouts. Their society is matriarchal and gender-segregated, and their belief system centers around the trees of the forests. The Pequeninos prove to be of great interest to scientists. Since humans had destroyed the only sentient species they had encountered (the Formics), special care is taken to ensure no similar mistakes are made with the Pequeninos. The colony is fenced in, strictly regulated to limit contact with the Pequeninos to a handful of scientists, and forbidden to share human technology with them. Shortly after the colony's founding, many of the colonists die from the "Descolada" virus (Portuguese for "uncoiled"), which causes terrible pain, rampant cancerous growth of fungus and even extra limbs, decay of healthy tissue, and death. The xenobiologists Gusto and Cida von Hesse manage to create a treatment for the virus before succumbing to it themselves (1936 S.C.), leaving behind their young daughter Novinha.

"Further information:" List of Ender's Game characters

Eight years after the Descolada virus is cured, Xenologer Pipo and his thirteen-year-old son and apprentice Libo have developed a friendship with the Pequeninos. They allow Novinha to join their science team as the colony's only xenobiologist, after she manages to pass the test at age thirteen. After accidentally sharing information about human genders with a male Pequenino named Rooter, the scientists find Rooter's body eviscerated, a sapling planted within it, and guess this may be a torturous sacrificial ritual.

A few years later, Novinha discovers that every lifeform on Lusitania carries the "Descolada" virus which, though lethal to humans, appears to serve a beneficial purpose to native lifeforms. When Pipo learns of this, he suddenly has an insight, and before he tells the others, races off to talk to the Pequeninos. Hours later, Libo and Novinha find Pipo's body cut open just as Rooter's had been, but with no sapling planted. As Pipo's death appears unprovoked, the Pequeninos are now considered a threat by the Starways Congress and restrictions on studying them are tightened. Novinha, having fallen in love with Libo but fearing that he will find out from her files what led to Pipo's death, marries another colonist, Marcos Ribeira, so as to lock her files from being opened, under colony law. Emotionally distraught, she then makes a call for a Speaker for the Dead for Pipo.

Andrew "Ender" Wiggin, living innocuously on the planet Trondheim, responds to Novinha's call. Though she has traveled with him for thousands of years, his sister Valentine is now pregnant and settled. He travels on alone, save for an artificial intelligence named Jane who communicates with Ender through a jewel in his ear and appears to live in the ansible network that enables faster-than-light communications. After relativistic travel, Ender arrives at Lusitania 22 years later (1970 S.C.), finding that Novinha had canceled her request for a Speaker five days after sending it. In the intervening time, Libo had died in a similar manner to Pipo, and Marcos had succumbed to a chronic illness. Novinha's eldest children, Ela and Miro, have requested a Speaker for Libo and Marcos. Ender, gaining access to all of the appropriate files, learns of tension since Pipo's death: Novinha has turned away from xenobiology to study crop growth, which created a loveless relationship with Marcos; Miro has secretly worked with Ouanda to continue to study the Pequeninos, while sharing human technology and knowledge with them. Over the course of time, Miro and Ouanda have fallen in love. With Ender's arrival, Miro tells him that one of the Pequeninos, Human, has taken a great interest in Ender, and Ender becomes aware that Human can hear messages from the Formic Hive Queen. Ender and Jane discover that Marcos was infertile: all six of Novinha's children, including Miro, were fathered by Libo, Ender also learns what Pipo had seen in Novinha's data.

As word of Miro's and Ouanda's illegal sharing of human technology with the Pequeninos is reported to Congress, Ender secretly goes to meet with the Pequeninos. They know his true identity, and they implore him to help them be part of civilization, while the Formic Queen tells Ender that Lusitania would be an ideal place to restart the hive, as her race can help guide the Pequeninos. By the time Ender returns to the colony, Congress has ordered Miro and Ouanda to be sent off-planet for penal action and the colony to be disbanded. Ender delivers his eulogy for Marcos, revealing Novinha's infidelity. Miro, distraught to learn that he is Ouanda's half-brother, attempts to escape to the Pequeninos, but he suffers neurological damage as he tries to cross the electrified fence. Ender reveals to the colony what he discovered that Pipo had learned: that every life form on Lusitania is paired with another through the "Descolada" virus, so that the death of one births the other, and as in the case of the Pequeninos, who become trees when they die. The colony leaders recognize Ender's words, and they agree to rebel against Congress, severing their ansible connection and deactivating the fence, allowing Ender, Ouanda, and Ela to go with Human to speak to the Pequenino wives, to help establish a case to present to Congress.

The Pequenino wives help Ender corroborate the complex life cycle of the Pequeninos, affirming that the death ritual Pipo observed was to help create "fathertrees" who fertilize the Pequenino females to continue their race. The Pequeninos believed they were honoring Pipo, and later Libo, by helping them become fathertrees, but Ender explains that humans lack this "third life", and if the Pequeninos are to cohabitate with humans, they must respect this difference. To affirm their understanding, Ender is allowed to perform the ritual of giving Human "third life" as a fathertree, providing Ouanda with the confirmation needed to present to Congress.

Miro recovers from most of the physical damage from his encounter with the fence, but he is still paralyzed. Valentine and her family inform Ender they plan to help Lusitania with the revolt, and they are traveling from Trondheim to help; Ender has Miro meet them halfway. Novinha, having gained understanding into the death of Pipo and Libo, finally absolves herself of her guilt, and she and Ender marry. Ender plants the Hive Queen as per her request, and he writes his third book, a biography of the life of the Pequenino, Human.

At the Los Angeles Times Book Festival (April 20, 2013), Card stated why he does not want "Speaker for the Dead" made into a film: 
""Speaker for the Dead" is unfilmable," Card said in response to a question from the audience. "It consists of talking heads, interrupted by moments of excruciating and unwatchable violence. Now, I admit, there's plenty of unwatchable violence in film, but never attached to my name. "Speaker for the Dead", I don't want it to be filmed. I can't imagine it being filmed."

Card writes in his introduction to the 1991 edition that he has received letters from readers who have conducted "Speakings" at funerals.





</doc>
<doc id="28232" url="https://en.wikipedia.org/wiki?curid=28232" title="Star catalogue">
Star catalogue

A star catalogue (Commonwealth English) or star catalog (American English), is an astronomical catalogue that lists stars. In astronomy, many stars are referred to simply by catalogue numbers. There are a great many different star catalogues which have been produced for different purposes over the years, and this article covers only some of the more frequently quoted ones. Star catalogues were compiled by many different ancient people, including the Babylonians, Greeks, Chinese, Persians, and Arabs. They were sometimes accompanied by a star chart for illustration. Most modern catalogues are available in electronic format and can be freely downloaded from space agencies' data centres. The largest is being compiled from the Gaia (spacecraft) and thus far has over a billion stars.

Completeness and accuracy are described by the weakest limiting magnitude V (largest number) and the accuracy of the positions.

From their existing records, it is known that the ancient Egyptians recorded the names of only a few identifiable constellations and a list of thirty-six decans that were used as a star clock. The Egyptians called the circumpolar star "the star that cannot perish" and, although they made no known formal star catalogues, they nonetheless created extensive star charts of the night sky which adorn the coffins and ceilings of tomb chambers.

Although the ancient Sumerians were the first to record the names of constellations on clay tablets, the earliest known star catalogues were compiled by the ancient Babylonians of Mesopotamia in the late 2nd millennium BC, during the Kassite Period (c. 1531 BC to c. 1155 BC). They are better known by their Assyrian-era name 'Three Stars Each'. These star catalogues, written on clay tablets, listed thirty-six stars: twelve for "Anu" along the celestial equator, twelve for "Ea" south of that, and twelve for "Enlil" to the north. The Mul.Apin lists, dated to sometime before the Neo-Babylonian Empire (626–539 BC), are direct textual descendants of the "Three Stars Each" lists and their constellation patterns show similarities to those of later Greek civilization.

In Ancient Greece, the astronomer and mathematician Eudoxus laid down a full set of the classical constellations around 370 BC. His catalogue "Phaenomena", rewritten by Aratus of Soli between 275 and 250 BC as a didactic poem, became one of the most consulted astronomical texts in antiquity and beyond. It contained descriptions of the positions of the stars and the shapes of the constellations, and provided information on their relative times of rising and setting.

Approximately in the 3rd century BC, the Greek astronomers Timocharis of Alexandria and Aristillus created another star catalogue. Hipparchus (c. 190 – c. 120 BC) completed his star catalogue in 129 BC, which he compared to Timocharis' and discovered that the longitude of the stars had changed over time. This led him to determine the first value of the precession of the equinoxes. In the 2nd century, Ptolemy (c. 90 – c. 186 AD) of Roman Egypt published a star catalogue as part of his "Almagest", which listed 1,022 stars visible from Alexandria. Ptolemy's catalogue was based almost entirely on an earlier one by Hipparchus. It remained the standard star catalogue in the Western and Arab worlds for over eight centuries. The Islamic astronomer al-Sufi updated it in 964, and the star positions were redetermined by Ulugh Beg in 1437, but it was not fully superseded until the appearance of the thousand-star catalogue of Tycho Brahe in 1598.

Although the ancient Vedas of India specified how the ecliptic was to be divided into twenty-eight "nakshatra", Indian constellation patterns were ultimately borrowed from Greek ones sometime after Alexander's conquests in Asia in the 4th century BC.

The earliest known inscriptions for Chinese star names were written on oracle bones and date to the Shang Dynasty (c. 1600 – c. 1050 BC). Sources dating from the Zhou Dynasty (c. 1050 – 256 BC) which provide star names include the "Zuo Zhuan", the "Shi Jing", and the "Canon of Yao" (堯典) in the "Book of Documents". The "Lüshi Chunqiu" written by the Qin statesman Lü Buwei (d. 235 BC) provides most of the names for the twenty-eight mansions (i.e. asterisms across the ecliptic belt of the celestial sphere used for constructing the calendar). An earlier lacquerware chest found in the Tomb of Marquis Yi of Zeng (interred in 433 BC) contains a complete list of the names of the twenty-eight mansions. Star catalogues are traditionally attributed to Shi Shen and Gan De, two rather obscure Chinese astronomers who may have been active in the 4th century BC of the Warring States period (403–221 BC). The "Shi Shen astronomy" (石申天文, Shi Shen tienwen) is attributed to Shi Shen, and the "Astronomic star observation" (天文星占, Tianwen xingzhan) to Gan De.

It was not until the Han Dynasty (202 BC – 220 AD) that astronomers started to observe and record names for all the stars that were apparent (to the naked eye) in the night sky, not just those around the ecliptic. A star catalogue is featured in one of the chapters of the late 2nd-century-BC history work "Records of the Grand Historian" by Sima Qian (145–86 BC) and contains the "schools" of Shi Shen and Gan De's work (i.e. the different constellations they allegedly focused on for astrological purposes). Sima's catalogue—the "Book of Celestial Offices" (天官書 Tianguan shu)—includes some 90 constellations, the stars therein named after temples, ideas in philosophy, locations such as markets and shops, and different people such as farmers and soldiers. For his "Spiritual Constitution of the Universe" (靈憲, Ling Xian) of 120 AD, the astronomer Zhang Heng (78–139 AD) compiled a star catalogue comprising 124 constellations. Chinese constellation names were later adopted by the Koreans and Japanese.

A large number of star catalogues were published by Muslim astronomers in the medieval Islamic world. These were mainly "Zij" treatises, including Arzachel's "Tables of Toledo" (1087), the Maragheh observatory's "Zij-i Ilkhani" (1272) and Ulugh Beg's "Zij-i-Sultani" (1437). Other famous Arabic star catalogues include Alfraganus' "A compendium of the science of stars" (850) which corrected Ptolemy's "Almagest"; and Azophi's "Book of Fixed Stars" (964) which described observations of the stars, their positions, magnitudes, brightness and colour, drawings for each constellation, and the first descriptions of Andromeda Galaxy and the Large Magellanic Cloud. Many stars are still known by their Arabic names (see List of Arabic star names).

The "Motul Dictionary", compiled in the 16th century by an anonymous author (although attributed to Fray Antonio de Ciudad Real), contains a list of stars originally observed by the ancient Mayas. The Maya Paris Codex also contain symbols for different constellations which were represented by mythological beings.

Two systems introduced in historical catalogues remain in use to the present day. The first system comes from the German astronomer Johann Bayer's "Uranometria", published in 1603 and regarding bright stars. These are given a Greek letter followed by the genitive case of the constellation in which they are located; examples are Alpha Centauri or Gamma Cygni. The major problem with Bayer's naming system was the number of letters in the Greek alphabet (24). It was easy to run out of letters before running out of stars needing names, particularly for large constellations such as Argo Navis. Bayer extended his lists up to 67 stars by using lower-case Roman letters ("a" through "z") then upper-case ones ("A" through "Q"). Few of those designations have survived. It is worth mentioning, however, as it served as the starting point for variable star designations, which start with "R" through "Z", then "RR", "RS", "RT"..."RZ", "SS", "ST"..."ZZ" and beyond.

The second system comes from the English astronomer John Flamsteed's "Historia coelestis Britannica" (1725). It kept the genitive-of-the-constellation rule for the back end of his catalogue names, but used numbers instead of the Greek alphabet for the front half. Examples include 61 Cygni and 47 Ursae Majoris.

Bayer and Flamsteed covered only a few thousand stars between them. In theory, full-sky catalogues try to list every star in the sky. There are, however, billions of stars resolvable by telescopes, so this is an impossible goal; with this kind of catalog, an attempt is generally made to get every star brighter than a given magnitude.

Jérôme Lalande published the "Histoire Céleste Française" in 1801, which contained an extensive star catalog, among other things. The observations made were made from the Paris Observatory and so it describes mostly northern stars. This catalogue contained the positions and magnitudes of 47,390 stars, out to magnitude 9, and was the most complete catalogue up to that time. A significant reworking of this catalogue in 1846 added reference numbers to the stars that are used to refer to some of these stars to this day. The decent accuracy of this catalogue kept it in common use as a reference by observatories around the world throughout the 19th century.

The "Bonner Durchmusterung" ("German": Bonn sampling) and follow-ups were the most complete of the pre-photographic star catalogues.

The "Bonner Durchmusterung" itself was published by Friedrich Wilhelm Argelander, Adalbert Krüger, and Eduard Schönfeld between 1852 and 1859. It covered 320,000 stars in epoch 1855.0.

As it covered only the northern sky and some of the south (being compiled from the Bonn observatory), this was then supplemented by the "Südliche Durchmusterung " (SD), which covers stars between declinations −1 and −23 degrees
(1886, 120,000 stars). It was further supplemented by the "Cordoba Durchmusterung" (580,000 stars), which began to be compiled at Córdoba, Argentina in 1892 under the initiative of John M. Thome and covers declinations −22 to −90. Lastly, the "Cape Photographic Durchmusterung" (450,000 stars, 1896), compiled at the Cape, South Africa, covers declinations −18 to −90.

Astronomers preferentially use the HD designation (see next entry) of a star, as that catalogue also gives spectroscopic information, but as the Durchmusterungs cover more stars they occasionally fall back on the older designations when dealing with one not found in Draper. Unfortunately, a lot of catalogues cross-reference the Durchmusterungs without specifying which one is used in the zones of overlap, so some confusion often remains.

Star names from these catalogues include the initials of which of the four catalogues they are from (though the "Southern" follows the example of the "Bonner" and uses BD; CPD is often shortened to CP), followed by the angle of declination of the star (rounded towards zero, and thus ranging from +00 to +89 and −00 to −89), followed by an arbitrary number as there are always thousands of stars at each angle. Examples include BD+50°1725 or CD−45°13677.

The Henry Draper Catalogue was published in the period 1918–1924. It covers the whole sky down to about ninth or tenth magnitude, and is notable as the first large-scale attempt to catalogue spectral types of stars.
The catalogue was compiled by Annie Jump Cannon and her co-workers at Harvard College Observatory under the supervision of Edward Charles Pickering, and was named in honour of Henry Draper, whose widow donated the money required to finance it.

HD numbers are widely used today for stars which have no Bayer or Flamsteed designation. Stars numbered 1–225300 are from the original catalogue and are numbered in order of right ascension for the 1900.0 epoch. Stars in the range 225301–359083 are from the 1949 extension of the catalogue. The notation HDE can be used for stars in this extension, but they are usually denoted HD as the numbering ensures that there can be no ambiguity.

The "Catalogue astrographique" (Astrographic Catalogue) was part of the international "Carte du Ciel" programme designed to photograph and measure the positions of all stars brighter than magnitude 11.0. In total, over 4.6 million stars were observed, many as faint as 13th magnitude. This project was started in the late 19th century. The observations were made between 1891 and 1950. To observe the entire celestial sphere without burdening too many institutions, the sky was divided among 20 observatories, by declination zones. Each observatory exposed and measured the plates of its zone, using a standardized telescope (a "normal astrograph") so each plate photographed had a similar scale of approximately 60 arcsecs/mm. The U.S. Naval Observatory took over custody of the catalogue, now in its 2000.2 edition.

First published in 1930 as the "Yale Catalog of Bright Stars", this catalogue contained information on all stars brighter than visual magnitude 6.5 in the "Harvard Revised Photometry Catalogue". The list was revised in 1983 with the publication of a supplement that listed additional stars down to magnitude 7.1. The catalogue detailed each star's coordinates, proper motions, photometric data, spectral types, and other useful information.

The last printed version of the Bright Star Catalogue was the 4th revised edition, released in 1982. The 5th edition is in electronic form and is available online.

The Smithsonian Astrophysical Observatory catalogue was compiled in 1966 from various previous astrometric catalogues, and contains only the stars to about ninth magnitude for which accurate proper motions were known. There is considerable overlap with the Henry Draper catalogue, but any star lacking motion data is omitted. The epoch for the position measurements in the latest edition is J2000.0. The SAO catalogue contains this major piece of information not in Draper, the proper motion of the stars, so it is often used when that fact is of importance. The cross-references with the Draper and Durchmusterung catalogue numbers in the latest edition are also useful.

Names in the SAO catalogue start with the letters SAO, followed by a number. The numbers are assigned following 18 ten-degree bands in the sky, with stars sorted by right ascension within each band.

USNO-B1.0 is an all-sky catalogue created by research and operations astrophysicists at the U.S. Naval Observatory (as developed at the United States Naval Observatory Flagstaff Station), that presents positions, proper motions, magnitudes in various optical passbands, and star/galaxy estimators for 1,042,618,261 objects derived from 3,643,201,733 separate observations. The data was obtained from scans of 7,435 Schmidt plates taken for the various sky surveys during the last 50 years. USNO-B1.0 is believed to provide all-sky coverage, completeness down to V = 21, 0.2 arcsecond astrometric accuracy at J2000.0, 0.3 magnitude photometric accuracy in up to five colors, and 85% accuracy for distinguishing stars from non-stellar objects. USNO-B is now followed by NOMAD; both can be found on the Naval Observatory server. The Naval Observatory is currently working on B2 and C variants of the USNO catalogue series.

The "Guide Star Catalog" is an online catalogue of stars produced for the purpose of accurately positioning and identifying stars satisfactory for use as guide stars by the Hubble Space Telescope program. The first version of the catalogue was produced in the late 1980s by digitizing photographic plates and contained about 20 million stars, out to about magnitude 15. The latest version of this catalogue contains information for 945,592,683 stars, out to magnitude 21. The latest version continues to be used to accurately position the Hubble Space Telescope.

The PPM Star Catalogue (1991) is one of the best, both in the proper motion and star position till 1999. Not as precise as the Hipparcos catalogue but with many more stars. The PPM was built from BD, SAO, HD and more, with sophisticated algorithm and is an extension for the Fifth Fundamental Catalogue, "Catalogues of Fundamental Stars".

The Hipparcos catalogue was compiled from the data gathered by the European Space Agency's astrometric satellite "Hipparcos", which was operational from 1989 to 1993. The catalogue was published in June 1997 and contains 118,218 stars; an updated version with re-processed data was published in 2007. It is particularly notable for its parallax measurements, which are considerably more accurate than those produced by ground-based observations.

The Gaia catalogue is released in stages that will contain increasing amounts of information; the early releases will also miss some stars, especially fainter stars located in dense star fields. Data from every data release can be accessed at the "Gaia" archive. Gaia DR1, the first data release of the spacecraft "Gaia" mission, based on 14 months of observations made through September 2015, took place on 13 September 2016. The data release includes positions and magnitudes in a single photometric band for 1.1 billion stars using only "Gaia" data, positions, parallaxes and proper motions for more than 2 million stars based on a combination of "Gaia" and Tycho-2 data for those objects in both catalogues, light curves and characteristics for about 3000 variable stars, and positions and magnitudes for more than 2000 extragalactic sources used to define the celestial reference frame. The second data release (DR2), which occurred on 25 April 2018, is based on 22 months of observations made between 25 July 2014 and 23 May 2016. It includes positions, parallaxes and proper motions for about 1.3 billion stars and positions of an additional 300 million stars, red and blue photometric data for about 1.1 billion stars and single colour photometry for an additional 400 million stars, and median radial velocities for about 7 million stars between magnitude 4 and 13. It also contains data for over 14,000 selected Solar System objects. The full "Gaia" catalogue will be released in 2022.

Specialized catalogues make no effort to list all the stars in the sky, working instead to highlight a particular type of star, such as variables or nearby stars.

Aitken's double star catalogue (1932) lists 17,180 double stars north of declination −30 degrees.

Stephenson's General Catalogue of galactic Carbon stars is a catalogue of 7000+ carbon stars.

The Gliese (later Gliese-Jahreiß) catalogue attempts to list all star systems within of Earth ordered by right ascension (see the List of nearest stars). Later editions expanded the coverage to . Numbers in the range 1.0–915.0 (Gl numbers) are from the second edition, which was

The integers up to 915 represent systems which were in the first edition. Numbers with a decimal point were used to insert new star systems for the second edition without destroying the desired order (by right ascension). This catalogue is referred to as CNS2, although this name is never used in catalogue numbers.

Numbers in the range 9001–9850 (Wo numbers) are from the supplement

Numbers in the ranges 1000–1294 and 2001–2159 (GJ numbers) are from the supplement

The range 1000–1294 represents nearby stars, while 2001–2159 represents suspected nearby stars. In the literature, the GJ numbers are sometimes retroactively extended to the Gl numbers (since there is no overlap). For example, Gliese 436 can be interchangeably referred to as either Gl 436 or GJ 436.

Numbers in the range 3001–4388 are from

Although this version of the catalogue was termed "preliminary", it is still the current one , and is referred to as CNS3. It lists a total of 3,803 stars. Most of these stars already had GJ numbers, but there were also 1,388 which were not numbered. The need to give these 1,388 "some" name has resulted in them being numbered 3001–4388 (NN numbers, for "no name"), and data files of this catalogue now usually include these numbers. An example of a star which is often referred to by one of these unofficial GJ numbers is GJ 3021.

The General Catalogue of Trigonometric Parallaxes, first published in 1952 and later superseded by the New GCTP (now in its fourth edition), covers nearly 9,000 stars. Unlike the Gliese, it does not cut off at a given distance from the Sun; rather it attempts to catalogue all known measured parallaxes. It gives the co-ordinates in 1900 epoch, the secular variation, the proper motion, the weighted average absolute parallax and its standard error, the number of parallax observations, quality of interagreement of the different values, the visual magnitude and various cross-identifications with other catalogues. Auxiliary information, including UBV photometry, MK spectral types, data on the variability and binary nature of the stars, orbits when available, and miscellaneous information to aid in determining the reliability of the data are also listed.

A common way of detecting nearby stars is to look for relatively high proper motions. Several catalogues exist, of which we'll mention a few. The Ross and Wolf catalogues pioneered the domain:

Willem Jacob Luyten later produced a series of catalogues:

L – Luyten, Proper motion stars and White dwarfs
LFT – Luyten Five-Tenths catalogue

LHS – Luyten Half-Second catalogue

LTT – Luyten Two-Tenths catalogue

NLTT – New Luyten Two-Tenths catalogue

LPM – Luyten Proper-Motion catalogue

Around the same time period, Henry Lee Giclas worked on a similar series of catalogues:

The "ubvyβ Photoelectric Photometric Catalogue" is a compilation of previously published photometric data. Published in 1998, the catalogue includes 63,316 stars surveyed through 1996.

The Robertson's "Zodiacal Catalogue", collected by the astronomer James Robertson, is a catalogue of 3539 zodiacal stars brighter than 9th magnitude. It is mainly used for Star Occultations by the Moon.

Stars evolve and move over time, making catalogues evolving, impermanent databases at even the most rigorous levels of production. The USNO catalogues are the most current and widely used astrometric catalogues available at present, and include USNO products such as USNO-B (the successor to USNO-A), NOMAD, UCAC and others in production or narrowly released. Some users may see specialized catalogues (more recent versions of the above), tailored catalogues, interferometrically-produced cataloges, dynamic catalogues, and those with updated positions, motions, colors, and improved errors. Catalogue data is continually collected at the Naval Observatory dark-sky facility, NOFS; and the latest refined, updated catalogues are reduced and produced by NOFS and the USNO. See the USNO Catalog and Image Servers for more information and access.






</doc>
<doc id="28233" url="https://en.wikipedia.org/wiki?curid=28233" title="Stellar designations and names">
Stellar designations and names

In astronomy, stars have a variety of different stellar designations and names, including catalogue designations, current and historical proper names, and foreign language names.

Only a tiny minority of known stars have proper names; all others have only designations from various catalogues or lists, or no identifier at all. Hipparchus in the 2nd century BC enumerated about 850 naked-eye stars. Johann Bayer in 1603 listed about twice this number. Only in the 19th century did star catalogues list the naked-eye stars exhaustively. The Bright Star Catalogue, which is a star catalogue listing all stars of apparent magnitude 6.5 or brighter, or roughly every star visible to the naked eye from Earth, contains 9,096 stars. The most voluminous modern catalogues list on the order of a billion stars, out of an estimated total of 200 to 400 billion in the Milky Way.

Proper names may be historical, often transliterated from Arabic or Chinese names. Such transliterations can vary so there may be multiple spellings. A smaller number of names have been introduced since the Middle Ages, and a few in modern times as nicknames have come into popular use, for example "Sualocin" for α Delphini and "Navi" for γ Cassiopeiae.

The International Astronomical Union (IAU) has begun a process to select and formalise unique proper names for the brighter naked-eye stars and for other stars of popular interest. To the IAU, "name" refers to the (usually colloquial) term used for a star in everyday speech, while ""designation" is solely alphanumerical" and used almost exclusively in official catalogues and for professional astronomy. Many of the names and some of the designations in use today were inherited from the time before the IAU existed. Other designations are being added all the time. As of the start of 2019, the IAU had decided on a little over 300 proper names, mostly for the brighter naked-eye stars.

Several hundred of the brightest stars had traditional names, most of which derived from Arabic, but a few from Latin. There were a number of problems with these names, however:

In 2016, the IAU organized a Working Group on Star Names (WGSN) to catalog and standardize proper names for stars. The WGSN's first bulletin dated July 2016 included a table of 125 stars comprising the first two batches of names approved by the WGSN (on 30 June and 20 July 2016) together with names of stars (including four traditional star names: Ain, Edasich, Errai and Fomalhaut) reviewed and adopted by the IAU Executive Committee Working Group on Public Naming of Planets and Planetary Satellites during the 2015 NameExoWorlds campaign and recognized by the WGSN. Further batches of names were approved on 21 August, 12 September, 5 October and 6 November 2016. These were listed in a table of 102 stars included in the WGSN's second bulletin dated November 2016. The next additions were done on 1 February 2017 (13 new star names), 30 June 2017 (29), 5 September 2017 (41), 17 November 2017 (3) and 1 June 2018 (17). All 330 names are included in the current List of IAU-approved Star Names, last updated on 1 June 2018 (with a minor correction posted on 11 June 2018).

In practice, names are only universally used for the very brightest stars (Sirius, Arcturus, Vega, etc.) and for a small number of slightly less bright but "interesting" stars (Algol, Polaris, Mira, etc.). For other naked eye stars, the Bayer or Flamsteed designation is often preferred.

In addition to the traditional names, a small number of stars that are "interesting" can have modern English names. For instance, two second-magnitude stars, Alpha Pavonis and Epsilon Carinae, were assigned the proper names Peacock and Avior respectively in 1937 by Her Majesty's Nautical Almanac Office during the creation of "The Air Almanac", a navigational almanac for the Royal Air Force. Of the fifty-seven stars included in the new almanac, these two had no traditional names. The RAF insisted that all of the stars must have names, so new names were invented for them. These names have been approved by the IAU WGSN.

The book "" by R. H. Allen (1899) has had effects on star names:

A few stars are named for individuals. These are mostly names in common use that were taken up by the scientific community at some juncture. The first such case (discounting characters from mythology) was Cor Caroli (α CVn), named in the 17th century for Charles I of England. The remaining examples are mostly named after astronomers, the best known are probably Barnard's Star (which has the highest known proper motion of any star and is thus notable even though it is far too faint to be seen with the naked eye), Kapteyn's Star and recently Tabby's Star.

In July 2014 the IAU launched a process for giving proper names to exoplanets and their host stars. As a result, the IAU approved the names Cervantes for Mu Arae and Copernicus for 55 Cancri A.

In the absence of any better means of designating a star, catalogue designations are generally used. Many star catalogues are used for this purpose; see star catalogues.

The first modern schemes for designating stars systematically labelled them within their constellation.


Full-sky star catalogues detach the star designation from the star's constellation and aim at enumerating all stars with apparent magnitude greater than a given cut-off value.


Variable stars that do not have Bayer designations are assigned designations in a variable star scheme that superficially extends the Bayer scheme with uppercase Latin letters followed by constellation names, starting with single letters R to Z, and proceeding to pairs of letters. Such designations mark them as variable stars. Examples include R Cygni, RR Lyrae, and GN Andromedae. (Many variable stars also have designations in other catalogues.)

When a planet is detected around a star, the star is often given a name and number based on the name of the telescope or survey mission that discovered it and based on how many planets have already been discovered by that mission e.g. HAT-P-9, WASP-1, COROT-1, Kepler-4, TRAPPIST-1.

Star naming rights are not available for sale via the IAU. Rather, star names are selected on a non-commercial basis by a small number of international organizations of astronomers, scientists, and registration bodies, who assign names consisting usually of a Greek letter followed by the star's constellation name, or less frequently based on their ancient traditional name.

However, there are a number of non-scientific "star-naming" companies that offer to assign personalized names to stars within their own private catalogs. These names are used only within that company (and usually available for viewing on their web site), and are not recognized by the astronomical community, or by competing star-naming companies. A survey conducted by amateur astronomers discovered that just over half of consumers would still want to "name a star" with a non-scientific star-naming company even though they have been warned or informed such naming is not recognized by the astronomical community.




</doc>
<doc id="28235" url="https://en.wikipedia.org/wiki?curid=28235" title="Space Shuttle Challenger">
Space Shuttle Challenger

Space Shuttle "Challenger" (Orbiter Vehicle Designation: OV-099) was the second orbiter of NASA's Space Shuttle program to be put into service, after "Columbia". "Challenger" was built by Rockwell International's Space Transportation Systems Division, in Downey, California. Its maiden flight, STS-6, began on April 4, 1983. The orbiter was launched and landed nine times before disintegrating 73 seconds into its tenth mission, STS-51-L, on January 28, 1986, resulting in the deaths of all seven crew members including a civilian school teacher.

"Challenger" was the first of two orbiters that were destroyed in flight, the other being "Columbia" in 2003. The "Challenger" accident led to a two-and-a-half-year grounding of the shuttle fleet; flights resumed in 1988, with STS-26 flown by "Discovery". "Challenger" was replaced by "Endeavour", which was built from structural spares ordered by NASA in the construction contracts for "Discovery" and "Atlantis".

"Challenger" was named after HMS "Challenger", a British corvette that was the command ship for the "Challenger" Expedition, a pioneering global marine research expedition undertaken from 1872 through 1876. The Apollo 17 Lunar Module, which landed on the Moon in 1972, was also named "Challenger".

Because of the low production volume of orbiters, the Space Shuttle program decided to build a vehicle as a Structural Test Article, STA-099, that could later be converted to a flight vehicle. The contract for STA-099 was awarded to North American Rockwell on July 26, 1972, and construction was completed in February 1978. After STA-099's rollout, it was sent to a Lockheed test site in Palmdale, where it spent over 11 months in vibration tests designed to simulate entire shuttle flights, from launch to landing. To prevent damage during structural testing, qualification tests were performed to a safety factor of 1.2 times the design limit loads. The qualification tests were used to validate computational models, and compliance with the required 1.4 factor of safety was shown by analysis. STA-099 was essentially a complete airframe of a Space Shuttle orbiter, with only a mockup crew module installed and thermal insulation placed on its forward fuselage.
NASA planned to refit the prototype orbiter "Enterprise" (OV-101), used for flight testing, as the second operational orbiter; but "Enterprise" lacked most of the systems needed for flight, including a functional propulsion system, thermal insulation, a life support system, and most of the cockpit instrumentation. Modifying it for spaceflight was considered to be too difficult, expensive, and time-consuming. Since STA-099 was not as far along in the construction of its airframe, it would be easier to upgrade to a flight article. Because STA-099's qualification testing prevented damage, NASA found that rebuilding STA-099 as a flight worthy orbiter would be less expensive than refitting "Enterprise". Work on converting STA-099 to operational status began in January 1979, starting with the crew module (the pressurized portion of the vehicle), as the rest of the vehicle was still being used for testing by Lockheed. STA-099 returned to the Rockwell plant in November 1979, and the original, unfinished crew module was replaced with the newly constructed model. Major parts of STA-099, including the payload bay doors, body flap, wings, and vertical stabilizer, also had to be returned to their individual subcontractors for rework. By early 1981, most of these components had returned to Palmdale to be reinstalled. Work continued on the conversion until July 1982, when the new orbiter was rolled out as "Challenger".

"Challenger", as did the orbiters built after it, had fewer tiles in its Thermal Protection System than "Columbia", though it still made heavier use of the white LRSI tiles on the cabin and main fuselage than did the later orbiters. Most of the tiles on the payload bay doors, upper wing surfaces, and rear fuselage surfaces were replaced with DuPont white Nomex felt insulation. These modifications and an overall lighter structure allowed "Challenger" to carry 2,500 lb (1,100 kg) more payload than "Columbia." "Challenger's" fuselage and wings were also stronger than "Columbia's" despite being lighter. The hatch and vertical-stabilizer tile patterns were also different from those of the other orbiters. "Challenger" was also the first orbiter to have a head-up display system for use in the descent phase of a mission, and the first to feature Phase I main engines rated for 104% maximum thrust.

After its first flight in April 1983, "Challenger" flew on 85% of all Space Shuttle missions. Even when the orbiters "Discovery" and "Atlantis" joined the fleet, "Challenger" flew three missions a year from 1983 to 1985. "Challenger", along with "Discovery", was modified at Kennedy Space Center to be able to carry the Centaur-G upper stage in its payload bay. If flight STS-51-L had been successful, "Challenger"'s next mission would have been the deployment of the "Ulysses" probe with the Centaur to study the polar regions of the Sun.

"Challenger" flew the first American woman, African-American, Dutchman and Canadian into space; carried three Spacelab missions; and performed the first night launch and night landing of a Space Shuttle.

"Challenger" was also the first space shuttle to be destroyed in an accident during a mission. The collected debris of the vessel is currently buried in decommissioned missile silos at Launch Complex 31, Cape Canaveral Air Force Station. A section of the fuselage recovered from Space Shuttle "Challenger" can also be found at the "Forever Remembered" memorial at the Kennedy Space Center Visitor Complex in Florida. From time to time, further pieces of debris from the orbiter wash up on the Florida coast. When this happens, they are collected and transported to the silos for storage. Because of its early loss, "Challenger" was the only space shuttle that never wore the NASA "meatball" logo, and was never modified with the MEDS "glass cockpit". The tail was never fitted with a drag chute – it was fitted to the remaining orbiters in 1992. Also because of its early demise "Challenger" was also one of only two shuttles that never visited the Mir Space Station or the International Space Station – the other one being its sister ship "Columbia".





</doc>
<doc id="28236" url="https://en.wikipedia.org/wiki?curid=28236" title="Space Shuttle Enterprise">
Space Shuttle Enterprise

Space Shuttle "Enterprise" (Orbiter Vehicle Designation: OV-101) was the first orbiter of the Space Shuttle system. Rolled out on September 17, 1976, it was built for NASA as part of the Space Shuttle program to perform atmospheric test flights after being launched from a modified Boeing 747. It was constructed without engines or a functional heat shield. As a result, it was not capable of spaceflight.

Originally, "Enterprise" had been intended to be refitted for orbital flight to become the second space-rated orbiter in service. However, during the construction of , details of the final design changed, making it simpler and less costly to build around a body frame that had been built as a test article. Similarly, "Enterprise" was considered for refit to replace "Challenger" after the latter was destroyed, but was built from structural spares instead.

"Enterprise" was restored and placed on display in 2003 at the Smithsonian's new Steven F. Udvar-Hazy Center in Virginia. Following the retirement of the Space Shuttle fleet, replaced "Enterprise" at the Udvar-Hazy Center, and "Enterprise" was transferred to the Intrepid Sea, Air & Space Museum in New York City, where it has been on display since July 2012.

The design of "Enterprise" was not the same as that planned for , the first flight model; the aft fuselage was constructed differently, and it did not have the interfaces to mount OMS pods. A large number of subsystems—ranging from main engines to radar equipment—were not installed on "Enterprise", but the capacity to add them in the future was retained, as NASA originally intended to refit the orbiter for spaceflight at the conclusion of its testing. Instead of a thermal protection system, its surface was primarily covered with simulated tiles made from polyurethane foam. Fiberglass was used for the leading edge panels in place of the reinforced carbon–carbon ones of spaceflight-worthy orbiters. Only a few sample thermal tiles and some Nomex blankets were real. "Enterprise" used fuel cells to generate its electrical power, but these were not sufficient to power the orbiter for spaceflight.

"Enterprise" also lacked reaction control system thrusters and hydraulic mechanisms for the landing gear; the landing gear doors were simply opened through the use of explosive bolts and the gear dropped down solely by gravity. As it was only used for atmospheric testing, "Enterprise" featured a large nose probe mounted on its nose cap, common on test aircraft because the location provides the most accurate readings for the test instruments, being mounted out in front of the disturbed airflow.

"Enterprise" was equipped with Lockheed-manufactured zero-zero ejection seats like those its sister carried on its first four missions.

Construction began on "Enterprise" on June 4, 1974. Designated OV-101, it was originally planned to be named "Constitution" and unveiled on Constitution Day, September 17, 1976. Fans of asked US President Gerald Ford, through a letter-writing campaign, to name the orbiter after the television show's fictional starship, USS "Enterprise". White House advisors cited "hundreds of thousands of letters" from Trekkies, "one of the most dedicated constituencies in the country", as a reason for giving the shuttle the name. Although Ford did not publicly mention the campaign, the president said that he was "partial to the name" "Enterprise", and directed NASA officials to change the name.

In mid-1976 the orbiter was used for ground vibration tests, allowing engineers to compare data from an actual flight vehicle with theoretical models.

On September 17, 1976, "Enterprise" was rolled out of Rockwell's plant at Palmdale, California. In recognition of its fictional namesake, "Star Trek" creator Gene Roddenberry and most of the principal cast of the original series of "Star Trek" were on hand at the dedication ceremony.

On January 31, 1977, "Enterprise" was taken by road to Dryden Flight Research Center at Edwards Air Force Base to begin operational testing.

While at NASA Dryden "Enterprise" was used by NASA for a variety of ground and flight tests intended to validate aspects of the shuttle program. The initial nine-month testing period was referred to by the acronym ALT, for "Approach and Landing Test". These tests included a maiden "flight" on February 18, 1977, atop a Boeing 747 Shuttle Carrier Aircraft (SCA) to measure structural loads and ground handling and braking characteristics of the mated system. Ground tests of all orbiter subsystems were carried out to verify functionality prior to atmospheric flight.

The mated "Enterprise"/SCA combination was then subjected to five test flights with "Enterprise" uncrewed and unactivated. The purpose of these test flights was to measure the flight characteristics of the mated combination. These tests were followed with three test flights with "Enterprise" crewed to test the shuttle flight control systems.

On August 12, 1977, "Enterprise" flew on its own for the first time. "Enterprise" underwent four more free flights where the craft separated from the SCA and was landed under astronaut control. These tests verified the flight characteristics of the orbiter design and were carried out under several aerodynamic and weight configurations. The first three flights were flown with a tailcone placed at the end of "Enterprise" aft fuselage, which reduced drag and turbulence when mated to the SCA. The final two flights saw the tailcone removed and mockup main engines installed. On the fifth and final glider flight, pilot-induced oscillation problems were revealed, which had to be addressed before the first orbital launch occurred.

Following the conclusion of the ALT test flight program, on March 13, 1978, "Enterprise" was flown once again, but this time halfway across the country to NASA's Marshall Space Flight Center (MSFC) in Alabama for the Mated Vertical Ground Vibration Testing (MGVT). The orbiter was lifted up on a sling very similar to the one used at Kennedy Space Center and placed inside the Dynamic Test Stand building, and there mated to the Vertical Mate Ground Vibration Test tank (VMGVT-ET), which in turn was attached to a set of inert Solid Rocket Boosters (SRB) to form a complete shuttle launch stack, and marked the first time in the program's history that all Space Shuttle elements, an Orbiter, an External Tank (ET), and two SRBs, were mated together. During the course of the program, "Enterprise" and the rest of the launch stack would be exposed to a punishing series of vibration tests simulating as closely as possible those expected during various phases of launch, some tests with and others without the SRBs in place.

At the conclusion of this testing, "Enterprise" was due to be taken back to Palmdale for retrofitting as a fully spaceflight capable vehicle. Under this arrangement, "Enterprise" would be launched on its maiden spaceflight in July 1981 to launch a communications satellite and retrieve the Long Duration Exposure Facility, then planned for a 1980 release on the first operational orbiter, "Columbia". Afterwards, "Enterprise" would conduct two Spacelab missions. However, in the period between the rollout of "Enterprise" and the rollout of "Columbia", a number of significant design changes had taken place, particularly with regard to the weight of the fuselage and wings. This meant that retrofitting the prototype would have been a much more expensive process than previously realized, involving the dismantling of the orbiter and the return of various structural sections to subcontractors across the country. As a consequence, NASA made the decision to convert an incomplete Structural Test Article, numbered STA-099, which had been built to undergo a variety of stress tests, into a fully flight-worthy orbiter, which became .

Following the MGVT program and with the decision to not use "Enterprise" for orbital missions, it was ferried to Kennedy Space Center on April 10, 1979. By June 1979, it was again mated with an external tank and solid rocket boosters (known as a boilerplate configuration) and tested in a launch configuration at KSC Launch Complex 39A for a series of fit checks of the facilities there.
With the completion of critical testing, was returned to Rockwell's plant in Palmdale in October 1987 and was partially disassembled to allow certain components to be reused in other shuttles. After this period, "Enterprise" was returned to NASA's Dryden Flight Research Facility in September 1981. In 1983 and 1984, "Enterprise" underwent an international tour visiting France, West Germany, Italy, the United Kingdom, and Canada. "Enterprise" also visited California, Alabama, and Louisiana (while visiting the 1984 Louisiana World Exposition). It was also used to fit-check the never-used shuttle launch pad at Vandenberg AFB, California. On November 18, 1985, "Enterprise" was ferried to Washington, D.C., where it became property of the Smithsonian Institution and was stored in the National Air and Space Museum's hangar at Dulles International Airport.

After the "Challenger" disaster, NASA considered using "Enterprise" as a replacement. Refitting the shuttle with all of the necessary equipment for it to be used in space was considered, but NASA decided to use spares constructed at the same time as and to build .

In 2003 after the breakup of during re-entry, the "Columbia" Accident Investigation Board conducted tests at Southwest Research Institute, which used an air cannon to shoot foam blocks of similar size, mass and speed to that which struck "Columbia" at a test structure which mechanically replicated the orbiter wing leading edge. They removed a section of fiberglass leading edge from "Enterprise" wing to perform analysis of the material and attached it to the test structure, then shot a foam block at it. While the leading edge was not broken as a result of the test, which took place on May 29, 2003, the impact was enough to permanently deform a seal and leave a thin gap long. Since the strength of the reinforced carbon–carbon (RCC) on "Columbia" is "substantially weaker and less flexible" than the test section from "Enterprise", this result suggested that the RCC would have been shattered. A section of RCC leading edge from "Discovery" was tested on June 6, to determine the effects of the foam on a similarly aged leading edge, resulting in a crack on panel 6 and cracking on a "T"-shaped seal between panels 6 and 7. On July 7, using a leading edge from "Atlantis" and focused on panel 8 with refined parameters stemming from the "Columbia" accident investigation, a second test created a ragged hole approximately in the RCC structure. The tests clearly demonstrated that a foam impact of the type "Columbia" sustained could seriously breach the protective RCC panels on the wing leading edge.

The board determined that the probable cause of the accident was that the foam impact caused a breach of a reinforced carbon-carbon panel along the leading edge of "Columbia" left wing, allowing hot gases generated during re-entry to enter the wing and cause structural collapse. This caused "Columbia" to tumble out of control, breaking up with the loss of the entire crew.

From 1985 to 2003, "Enterprise" was stored at the Smithsonian's hangar at Washington Dulles International Airport before it was restored and moved to the Smithsonian's newly built National Air and Space Museum Steven F. Udvar-Hazy Center at Washington Dulles, where it was the centerpiece of the space collection. On April 12, 2011, NASA announced that , the most traveled orbiter in the fleet, would replace "Enterprise" in the Smithsonian's collection once the Shuttle fleet was retired, with ownership of "Enterprise" transferred to the Intrepid Sea, Air & Space Museum in New York City. On April 17, 2012, "Discovery" was transported by Shuttle Carrier Aircraft to Dulles from Kennedy Space Center, where it made several passes over the Washington D.C. metro area. After "Discovery" had been removed from the Shuttle Carrier Aircraft, both orbiters were displayed nose-to-nose outside the Steven F. Udvar-Hazy Center before "Enterprise" was made ready for its flight to New York.

On December 12, 2011, ownership of "Enterprise" was officially transferred to the Intrepid Sea, Air & Space Museum in New York City. In preparation for the anticipated relocation, engineers evaluated the vehicle in early 2010 and determined that it was safe to fly on the Shuttle Carrier Aircraft once again. At approximately 13:40 UTC on April 27, 2012, "Enterprise" took off from Dulles International Airport en route to a fly-by over the Hudson River, New York's JFK International Airport, the Statue of Liberty, the George Washington and Verrazano-Narrows Bridges, and several other landmarks in the city, in an approximately 45-minute "final tour". At 15:23 UTC, "Enterprise" touched down at JFK International Airport.

The mobile Mate-Demate Device and cranes were transported from Dulles to the ramp at JFK and the shuttle was removed from the SCA overnight on May 12, 2012, placed on a specially designed flat bed trailer and returned to Hangar 12. On June 3 a Weeks Marine barge took "Enterprise" to Jersey City. The Shuttle sustained cosmetic damage to a wingtip when a gust of wind blew the barge towards a piling. It was hoisted June 6 onto the Intrepid Museum in Manhattan.

"Enterprise" went on public display on July 19, 2012, at the Intrepid Museum's new Space Shuttle Pavilion, a temporary shelter consisting of a pressurized, air-supported fabric bubble constructed on the aft end of the carrier's flight deck.

On October 29, 2012, storm surges from Hurricane Sandy caused Pier 86, including the Intrepid Museum's visitor center, to flood, and knocked out the museum's electrical power and both backup generators. The loss of power caused the Space Shuttle Pavilion to deflate, and high winds from the hurricane caused the fabric of the Pavilion to tear and collapse around the orbiter. Minor damage was spotted on the vertical stabilizer of the orbiter, as a portion of the tail fin above the rudder/speedbrake had broken off. The broken section was recovered by museum staff. While the pavilion itself could not be replaced for some time in 2013, the museum erected scaffolding and sheeting around "Enterprise" to protect it from the environment.

By April 2013, the damage sustained to "Enterprise" vertical stabilizer had been fully repaired, and construction work on the structure for a new pavilion was under way. The pavilion and exhibit reopened on July 10, 2013.

"Enterprise" was listed on the National Register of Historic Places on March 13, 2013, reference number 13000071, in recognition of its role in the development of the Space Shuttle Program. The historic significance criteria are in space exploration, transportation, and engineering.




</doc>
<doc id="28237" url="https://en.wikipedia.org/wiki?curid=28237" title="Space Shuttle Columbia">
Space Shuttle Columbia

The Space Shuttle "Columbia" (Orbiter Vehicle Designation: OV-102) was the first space-rated orbiter in NASA's Space Shuttle fleet. It launched for the first time on mission STS-1 on April 12, 1981, the first flight of the Space Shuttle program. Serving for over 22 years, it completed 27 missions before disintegrating during re-entry near the end of its 28th mission, STS-107 on February 1, 2003, resulting in the deaths of all seven crew members.

Construction began on "Columbia" in 1975 at Rockwell International's (formerly North American Aviation/North American Rockwell) principal assembly facility in Palmdale, California, a suburb of Los Angeles. "Columbia" was named after the American sloop "Columbia Rediviva" which, from 1787 to 1793, under the command of Captain Robert Gray, explored the US Pacific Northwest and became the first American vessel to circumnavigate the globe. It is also named after the command module of Apollo 11, the first crewed landing on another celestial body. "Columbia" was also the female symbol of the United States. After construction, the orbiter arrived at Kennedy Space Center on March 25, 1979, to prepare for its first launch. "Columbia" was originally scheduled to lift off in late 1979, however the launch date was delayed by problems with both the RS-25 engine, as well as the thermal protection system (TPS). On March 19, 1981, during preparations for a ground test, workers were asphyxiated while working in Columbia's nitrogen-purged aft engine compartment, resulting in (variously reported) two or three fatalities.
The first flight of "Columbia" (STS-1) was commanded by John Young, a veteran from the Gemini and Apollo programs who was the ninth person to walk on the Moon in 1972, and piloted by Robert Crippen, a rookie astronaut originally selected to fly on the military's Manned Orbital Laboratory (MOL) spacecraft, but transferred to NASA after its cancellation, and served as a support crew member for the Skylab and Apollo-Soyuz missions.

"Columbia" spent 610 days in the Orbiter Processing Facility (OPF), another 35 days in the Vehicle Assembly Building (VAB), and 105 days on Pad 39A before finally lifting off. "Columbia" was successfully launched on April 12, 1981, the 20th anniversary of the first human spaceflight (Vostok 1), and returned on April 14, 1981, after orbiting the Earth 36 times, landing on the dry lakebed runway at Edwards Air Force Base in California. "Columbia" then undertook three further research missions to test its technical characteristics and performance. Its first operational mission, with a four-man crew, was STS-5, which launched on November 11, 1982. At this point "Columbia" was joined by "Challenger", which flew the next three shuttle missions, while "Columbia" underwent modifications for the first Spacelab mission.

In 1983, "Columbia", under the command of John Young on what was his sixth spaceflight, undertook its second operational mission (STS-9), in which the Spacelab science laboratory and a six-person crew was carried, including the first non-American astronaut on a space shuttle, Ulf Merbold. After the flight, "Columbia" spent 18 months at the Rockwell Palmdale facility beginning in January 1984, undergoing modifications that removed the Orbiter Flight Test hardware and bringing it up to similar specifications as those of its sister orbiters. At that time the shuttle fleet was expanded to include "Discovery" and "Atlantis".

"Columbia" returned to space on January 12, 1986, with the launch of STS-61-C. The mission's crew included Dr. Franklin Chang-Diaz, as well as the first sitting member of the House of Representatives to venture into space, Bill Nelson.

The next shuttle mission, STS-51-L, was undertaken by "Challenger". It was launched on January 28, 1986, ten days after STS-61-C had landed, and ended in disaster 73 seconds after launch. Prior to the accident, "Columbia" had been slated to be ferried to Vandenberg Air Force Base to conduct fueling tests and to perform a flight readiness firing at SLC-6 to validate the west coast launch site. In the aftermath NASA's shuttle timetable was disrupted, and the Vandenberg tests, which would have cost $60 million, were canceled. "Columbia" was not flown again until 1989 (on STS-28), after which it resumed normal service as part of the shuttle fleet.

STS-93, launched on July 23, 1999, was the first U.S. space mission with a female commander, Lt. Col. Eileen Collins. This mission deployed the Chandra X-ray Observatory.

"Columbia"'s final complete mission was STS-109, the fourth servicing mission for the Hubble Space Telescope. Its next mission, STS-107, culminated in the orbiter's loss when it disintegrated during reentry, killing all seven of its crew.

Consequently, President George W. Bush decided to retire the Shuttle orbiter fleet by 2010 in favor of the Constellation program and its crewed Orion spacecraft. The Constellation program was later cancelled with the NASA Authorization Act of 2010 signed by President Barack Obama on October 11.

As the second orbiter to be constructed, and the first able to fly into space, "Columbia" was roughly heavier than subsequent orbiters such as "Endeavour", which were of a slightly different design, and had benefited from advances in materials technology. In part, this was due to heavier wing and fuselage spars, the weight of early test instrumentation that remained fitted to the avionics suite, and an internal airlock that, originally fitted into the other orbiters, was later removed in favor of an external airlock to facilitate Shuttle/Mir and Shuttle/International Space Station dockings. Due to its weight, "Columbia" could not have used the planned Centaur-G booster (cancelled after the loss of "Challenger"). The retention of the internal airlock allowed NASA to use "Columbia" for the STS-109 Hubble Space Telescope servicing mission, along with the Spacehab double module used on STS-107. Due to "Columbia's" higher weight, it was less ideal for NASA to use it for missions to the International Space Station, though modifications were made to the Shuttle during its last refit in case the spacecraft was needed for such tasks.

Externally, "Columbia" was the first orbiter in the fleet whose surface was mostly covered with High & Low Temperature Reusable Surface Insulation (HRSI/LRSI) tiles as its main thermal protection system (TPS), with white silicone rubber-painted Nomex – known as Felt Reusable Surface Insulation (FRSI) blankets – in some areas on the wings, fuselage and payload bay doors. FRSI once covered almost 25% of the orbiter; the first upgrade resulted in its removal from many areas, and in later flights it was only used on the upper section of the payload bay doors and inboard sections of the upper wing surfaces. The upgrade also involved replacing many of the white LRSI tiles on the upper surfaces with Advanced Flexible Reusable Surface Insulation (AFRSI) blankets (also known as Fibrous Insulation Blankets, or FIBs) that had been used on "Discovery" and "Atlantis". Originally, "Columbia" had 32,000 tiles – the upgrade reduced this to 24,300. The AFRSI blankets consisted of layers of pure silica felt sandwiched between a layer of silica fabric on the outside and S-Glass fabric on the inside, stitched together using pure silica thread in a 1-inch grid, then coated with a high-purity silica coating. The blankets were semi-rigid and could be made as large as 30" by 30". Each blanket replaced as many as 25 tiles and was bonded directly to the orbiter. The direct application of the blankets to the orbiter resulted in weight reduction, improved durability, reduced fabrication and installation cost, and reduced installation schedule time. All of this work was performed during "Columbia's" first retrofitting and the post-"Challenger" stand-down.

Despite refinements to the orbiter's thermal protection system and other enhancements, "Columbia" would never weigh as little unloaded as the other orbiters in the fleet. The next-oldest shuttle, "Challenger", was also relatively heavy, although lighter than "Columbia".

Until its last refit, "Columbia" was the only operational orbiter with wing markings consisting of an American flag on the port (left) wing and the letters "USA" on the starboard (right) wing. "Challenger", "Discovery", "Atlantis" and "Endeavour" all, until 1998, bore markings consisting of the letters "USA" above an American flag on the left wing, and the pre-1998 NASA "worm" logo afore the respective orbiter's name on the right wing. ("Enterprise", the test vehicle which was the prototype for "Columbia", originally had the same wing markings as "Columbia" but with the letters "USA" on the right wing spaced closer together; "Enterprise"'s markings were modified to match "Challenger" in 1983.) The name of the orbiter was originally placed on the payload bay doors much like "Enterprise" but was placed on the crew cabin after the "Challenger" disaster so that the orbiter could be easily identified while in orbit. From its last refit to its destruction, "Columbia" bore markings identical to those of its operational sister orbiters – the NASA "meatball" logo on the left wing and the American flag afore the orbiter's name on the right; only "Columbia's" distinctive wing "chines" remained. These black areas on the upper surfaces of the shuttle's forward wing were added because, at first, shuttle designers did not know how reentry heating would affect the craft's upper wing surfaces. The "chines" allowed "Columbia" to be easily recognized at a distance, as opposed to the subsequent orbiters. The "chines" were added after "Columbia" arrived at KSC in 1979.

Another unique external feature, termed the "SILTS" pod (Shuttle Infrared Leeside Temperature Sensing), was located on the top of "Columbia's" vertical stabilizer, and was installed after STS-9 to acquire infrared and other thermal data. Though the pod's equipment was removed after initial tests, NASA decided to leave it in place, mainly to save costs, along with the agency's plans to use it for future experiments. The vertical stabilizer was later modified to incorporate the drag chute first used on "Endeavour" in 1992.

"Columbia" was also originally fitted with Lockheed-built ejection seats identical to those found on the SR-71 Blackbird. These were active for the four orbital test flights, but deactivated after STS-4, and removed entirely after STS-9. "Columbia" was also the only spaceworthy orbiter not delivered with head-up displays for the Commander and Pilot, although these were incorporated after STS-9. Like its sister ships, "Columbia" was eventually retrofitted with the new MEDS "glass cockpit" display and lightweight seats.

Had "Columbia" not been destroyed, it would have been fitted with the external airlock/docking adapter for STS-118, an International Space Station assembly mission, originally planned for November 2003. "Columbia" was scheduled for this mission due to "Discovery" being out of service for its Orbital Major Modification, and because the ISS assembly schedule could not be adhered to with only "Endeavour" and "Atlantis".

"Columbia"'s 'career' would have started to wind down after STS-118. It was to service the Hubble Space Telescope two more times between 2004 and 2005, but no more missions were planned for it again except for a mission designated STS-144 where it would retrieve the Hubble Space Telescope from orbit and bring it back to Earth. Following the "Columbia" accident, NASA flew the STS-125 mission using "Atlantis", combining the planned fourth and fifth servicing missions into one final mission to Hubble. Because of the retirement of the Space Shuttle fleet, the batteries and gyroscopes that keep the telescope pointed will eventually fail also because of the magnifier screen, which would result in its reentry and break-up in Earth's atmosphere. A "Soft Capture Docking Mechanism", based on the docking adapter that was to be used on the Orion spacecraft, was installed during the last servicing mission in anticipation of this event.

"Columbia" was also scheduled to launch the X-38 V-201 Crew Return Vehicle prototype as the next mission after STS-118, until the cancellation of the project in 2002.

"Columbia" flew 28 missions, gathering 300.74 days spent in space with 4,808 orbits and a total distance of up until STS-107.

Despite being in service during the Shuttle-Mir and International Space Station programs, "Columbia" did not fly any missions that visited a space station. The other three active orbiters at the time had visited both "Mir" and the ISS at least once. "Columbia" was not suited for high-inclination missions.

<nowiki>*</nowiki> Mission cancelled following suspension of shuttle flights following the "Challenger" disaster.

<nowiki>**</nowiki> Mission flown by "Endeavour" due to loss of "Columbia" on STS-107.

<nowiki>***</nowiki> Mission flown by "Discovery" due to loss of "Columbia" on STS-107.

"Columbia" was destroyed at about 09:00 EST on February 1, 2003, while re-entering the atmosphere after a 16-day scientific mission. The Columbia Accident Investigation Board determined that a hole was punctured in the leading edge on one of "Columbia's" wings, which was made of a carbon composite. The hole had formed when a piece of insulating foam from the external fuel tank peeled off during the launch 16 days earlier and struck the shuttle's left wing. During the intense heat of re-entry, hot gases penetrated the interior of the wing, likely compromising the hydraulic system and leading to control failure of the control surfaces. The resulting loss of control exposed minimally protected areas of the orbiter to full-entry heating and dynamic pressures that eventually led to vehicle break up.

The report delved deeply into the underlying organizational and cultural issues that the Board believed contributed to the accident. The report was highly critical of NASA's decision-making and risk-assessment processes. Further, the board determined that, unlike NASA's early claims, a rescue mission would have been possible using the Shuttle "Atlantis", which was essentially ready for launch, and might have saved the "Columbia" crewmembers. The nearly 84,000 pieces of collected debris of the vessel are stored in a 16th-floor office suite in the Vehicle Assembly Building at the Kennedy Space Center. The collection was opened to the media once and has since been open only to researchers. Unlike "Challenger", which had a replacement orbiter built, "Columbia" did not.

The seven crew members who died aboard this final mission were: Rick Husband, Commander; William C. McCool, Pilot; Michael P. Anderson, Payload Commander/Mission Specialist 3; David M. Brown, Mission Specialist 1; Kalpana Chawla, Mission Specialist 2; Laurel Clark, Mission Specialist 4; and Ilan Ramon, Payload Specialist 1.

The debris field encompassed hundreds of miles across Texas and into Louisiana and Arkansas. The nose cap and remains of all seven crew members were found in Sabine County, East Texas.

To honor those who lost their lives aboard the shuttle and during the recovery efforts, the Patricia Huffman Smith NASA Museum "Remembering Columbia" was opened in Hemphill, Sabine County, Texas. The museum tells the story of Space Shuttle "Columbia" explorations throughout all its missions, including the final STS-107. Its exhibits also show the efforts of local citizens during the recovery period of the "Columbia" shuttle debris and its crew's remains. An area is dedicated to each STS-107 crew member, and also to the Texas Forest Service helicopter pilot who died in the recovery effort. The museum houses many objects and artifacts from: NASA and its contractors; the families of the STS-107 crew; and other individuals. The crew's families contributed personal items of the crew members to be on permanent display. The museum features two interactive simulator displays that emulate activities of the shuttle and orbiter. The digital learning center and its classroom provide educational opportunities for all ages.

The Columbia Memorial Space Center is the U.S. national memorial for the Space Shuttle "Columbia"s seven crew members. It is located in Downey on the site of the Space Shuttle's origins and production, the former North American Aviation plant in Los Angeles County, California. The facility is also a hands-on learning center with interactive exhibits, workshops, and classes about space science, astronautics, and the Space Shuttle program's legacy — providing educational opportunities for all ages.

The Shuttle's final crew was honored in 2003 when the United States Board on Geographic Names approved the name Columbia Point for a mountain in Colorado's Sangre de Cristo Mountains, less than a half-mile from Challenger Point, a peak named after America's other lost Space Shuttle. The Columbia Hills on Mars were also named in honor of the crew, and a host of other memorials were dedicated in various forms.

The Columbia supercomputer at the NASA Advanced Supercomputing (NAS) Division located at Ames Research Center in California was named in honor of the crew lost in the 2003 disaster. Built as a joint effort between NASA and technical partners SGI and Intel in 2004, the supercomputer was used in scientific research of space, the Earth's climate, and aerodynamic design of space launch vehicles and aircraft. The first part of the system, built in 2003, was dedicated to STS-107 astronaut and engineer Kalpana Chawla, who prior to joining the Space Shuttle program worked at Ames Research Center.

A female bald eagle at the National Eagle Center in Wabasha, Minnesota is named in tribute to the victims of the disaster.

Guitarist Steve Morse of the rock band Deep Purple wrote the instrumental "Contact Lost" in response to the tragedy, recorded by Deep Purple and featured as the closing track on their 2003 album "Bananas". It was dedicated to the astronauts whose lives were lost in the disaster. Morse donated songwriting royalties to the families of lost astronauts. Astronaut and mission specialist engineer Kalpana Chawla, one of the victims of the accident, was a fan of Deep Purple and had exchanged e-mails with the band during the flight, making the tragedy even more personal for the group. She took three CDs into space with her, two of which were Deep Purple albums ("Machine Head" and "Purpendicular"). Both CDs survived the destruction of the shuttle and the 39-mile plunge.

The musical group Echo's Children included singer-songwriter Cat Faber's "Columbia" on their final album "From the Hazel Tree".

The Long Winters band's 2005 album "Ultimatum" features the song "The Commander Thinks Aloud", a tribute to the final "Columbia" crew.

The Eric Johnson instrumental "Columbia" from his 2005 album "Bloom" was written as a commemoration and tribute to the lives that were lost. Johnson said "I wanted to make it more of a positive message, a salute, a celebration rather than just concentrating on a few moments of tragedy, but instead the bigger picture of these brave people's lives."

The graphic novel "Orbiter" by Warren Ellis and Colleen Doran was dedicated to the "lives, memories and legacies of the seven astronauts lost on space shuttle "Columbia" during mission STS-107."

The Scottish band Runrig pays tribute to Clark on the 2016 album "The Story". The final track, "Somewhere", ends with a recording of her voice. Clark was a Runrig fan and had a wake up call with Runrig's "Running to the Light". She took "The Stamping Ground" CD into space with her. When the shuttle exploded the CD was found back on Earth, and was presented to the band by her family.





</doc>
<doc id="28238" url="https://en.wikipedia.org/wiki?curid=28238" title="Space Shuttle Discovery">
Space Shuttle Discovery

Space Shuttle "Discovery" (Orbiter Vehicle Designation: OV-103) is one of the orbiters from NASA's Space Shuttle program and the third of five fully operational orbiters to be built. Its first mission, STS-41-D, flew from August 30 to September 5, 1984. Over 27 years of service it launched and landed 39 times, gathering more spaceflights than any other spacecraft to date. Like other shuttles, the shuttle has three main components: the Space Shuttle orbiter, a central fuel tank, and two rocket boosters. Nearly 25,000 heat-resistant tiles cover the orbiter to protect it from high temperatures on re-entry.

"Discovery" became the third operational orbiter to enter service, preceded by "Columbia" and "Challenger". It embarked on its last mission, STS-133, on February 24, 2011 and touched down for the final time at Kennedy Space Center on March 9, having spent a cumulative total of almost a full year in space. "Discovery" performed both research and International Space Station (ISS) assembly missions, and also carried the Hubble Space Telescope into orbit. 
"Discovery" was the first operational shuttle to be retired, followed by "Endeavour" and then "Atlantis". The shuttle is now on display at the Steven F. Udvar-Hazy Center of the Smithsonian National Air and Space Museum.

The name "Discovery" was chosen to carry on a tradition based on ships of exploration, primarily , one of the ships commanded by Captain James Cook during his third and final major voyage from 1776 to 1779, and Henry Hudson's , which was used in 1610–1611 to explore Hudson Bay and search for a Northwest Passage. Other ships bearing the name have included of the 1875–1876 British Arctic Expedition to the North Pole and , which led the 1901–1904 "Discovery Expedition" to Antarctica.

"Discovery" launched the Hubble Space Telescope and conducted the second and third Hubble service missions. It also launched the "Ulysses" probe and three TDRS satellites. Twice "Discovery" was chosen as the "Return To Flight" Orbiter, first in 1988 after the loss of "Challenger" in 1986, and then again for the twin "Return To Flight" missions in July 2005 and July 2006 after the "Columbia" disaster in 2003. Project Mercury astronaut John Glenn, who was 77 at the time, flew with "Discovery" on STS-95 in 1998, making him the oldest person to go into space.

Had plans to launch United States Department of Defense payloads from Vandenberg Air Force Base gone ahead, "Discovery" would have become the dedicated US Air Force shuttle. Its first West Coast mission, STS-62-A, was scheduled for 1986, but canceled in the aftermath of "Challenger".

"Discovery" was retired after completing its final mission, STS-133 on March 9, 2011. The spacecraft is now on display in Virginia at the Steven F. Udvar-Hazy Center, an annex of the Smithsonian Institution's National Air and Space Museum.

"Discovery" weighed roughly 3600 kg (3.6t) less than "Columbia" when it was brought into service due to optimizations determined during the construction and testing of "Enterprise", "Columbia" and "Challenger". "Discovery" weighs heavier than "Atlantis" and heavier than "Endeavour".

Part of the "Discovery" weight optimizations included the greater use of quilted AFRSI blankets rather than the white LRSI tiles on the fuselage, and the use of graphite epoxy instead of aluminum for the payload bay doors and some of the wing spars and beams.

Upon its delivery to the Kennedy Space Center in 1983, "Discovery" was modified alongside "Challenger" to accommodate the liquid-fueled Centaur-G booster, which had been planned for use beginning in 1986 but was cancelled in the wake of the "Challenger" disaster.

Beginning in late 1995, the orbiter underwent a nine-month Orbiter Maintenance Down Period (OMDP) in Palmdale, California. This included outfitting the vehicle with a 5th set of cryogenic tanks and an external airlock to support missions to the International Space Station. As with all the orbiters, it could be attached to the top of specialized aircraft and did so in June 1996 when it returned to the Kennedy Space Center, and later in April 2012 when sent to the Udvar-Hazy Center, riding piggy-back on a modified Boeing 747.

After STS-105, "Discovery" became the first of the orbiter fleet to undergo Orbiter Major Modification (OMM) period at the Kennedy Space Center. Work began in September 2002 to prepare the vehicle for Return to Flight. The work included scheduled upgrades and additional safety modifications.

"Discovery" was decommissioned on March 9, 2011.

NASA offered "Discovery" to the Smithsonian Institution's National Air and Space Museum for public display and preservation, after a month-long decontamination process, as part of the national collection. "Discovery" replaced in the Smithsonian's display at the Steven F. Udvar-Hazy Center in Virginia. "Discovery" was transported to Washington Dulles International Airport on April 17, 2012, and was transferred to the Udvar-Hazy on April 19 where a welcome ceremony was held. Afterwards, at around 5:30 pm, "Discovery" was rolled to its "final wheels stop" in the Udvar Hazy Center.

By its last mission, "Discovery" had flown 149 million miles (238 million km) in 39 missions, completed 5,830 orbits, and spent 365 days in orbit over 27 years. "Discovery" flew more flights than any other Orbiter Shuttle, including four in 1985 alone. "Discovery" flew all three "return to flight" missions after the "Challenger" and "Columbia" disasters: STS-26 in 1988, STS-114 in 2005, and STS-121 in 2006. "Discovery" flew the ante-penultimate mission of the Space Shuttle program, STS-133, having launched on February 24, 2011. "Endeavour" flew STS-134 and "Atlantis" performed STS-135, NASA's last Space Shuttle mission. On February 24, 2011, Space Shuttle "Discovery" launched from Kennedy Space Center's Launch Complex 39-A to begin its final orbital flight.

The Flow Director was responsible for the overall preparation of the shuttle for launch and processing it after landing, and remained permanently assigned to head the spacecraft's ground crew while the astronaut flight crews changed for every mission. Each shuttle's Flow Director was supported by a Vehicle Manager for the same spacecraft. Space Shuttle "Discovery"'s Flow Directors were:




</doc>
<doc id="28239" url="https://en.wikipedia.org/wiki?curid=28239" title="Space Shuttle Atlantis">
Space Shuttle Atlantis

Space Shuttle "Atlantis" (Orbiter Vehicle Designation: OV‑104) is a Space Shuttle orbiter vehicle belonging to the National Aeronautics and Space Administration (NASA), the spaceflight and space exploration agency of the United States. Manufactured by the Rockwell International company in Southern California and delivered to the Kennedy Space Center in Eastern Florida in April 1985, "Atlantis" is the fourth operational and the second-to-last Space Shuttle built. Its maiden flight was STS-51-J from 3 to 7 October 1985.

"Atlantis" embarked on its 33rd and final mission, also the final mission of a space shuttle, STS-135, on 8 July 2011. STS-134 by "Endeavour" was expected to be the final flight before STS-135 was authorized in October 2010. STS-135 took advantage of the processing for the STS-335 Launch on Need mission that would have been necessary if STS-134's crew became stranded in orbit.
"Atlantis" landed for the final time at the Kennedy Space Center on 21 July 2011.

By the end of its final mission, "Atlantis" had orbited the Earth a total of 4,848 times, traveling nearly or more than 525 times the distance from the Earth to the Moon.

"Atlantis" is named after RV "Atlantis", a two-masted sailing ship that operated as the primary research vessel for the Woods Hole Oceanographic Institution from 1930 to 1966.


Space Shuttle "Atlantis" lifted off on its maiden voyage on 3 October 1985, on mission STS-51-J, the second dedicated Department of Defense flight. It flew one other mission, STS-61-B, the second night launch in the shuttle program, before the Space Shuttle "Challenger" disaster temporarily grounded the Shuttle fleet in 1986. Among the five Space Shuttles flown into space, "Atlantis" conducted a subsequent mission in the shortest time after the previous mission (turnaround time) when it launched in November 1985 on STS-61-B, only 50 days after its previous mission, STS-51-J in October 1985. "Atlantis" was then used for ten flights between 1988 and 1992. Two of these, both flown in 1989, deployed the planetary probes "Magellan" to Venus (on STS-30) and "Galileo" to Jupiter (on STS-34). With STS-30 "Atlantis" became the first Space Shuttle to launch an interplanetary probe.

During NASA's 27th Shuttle Launch of STS-27 during an operation to release the payload, which was eventually determined to be a Lacrosse Surveillance satellite, "Atlantis" lost part of its protective heat shield during lift off, which substantially damaged the underside of her right wing, damaging over 700 tiles, which caused the melting of aluminum plating during her reentry. Before return to Earth, the Commander Robert L. Gibson thought to himself "We are going to die." due to the extensive damage to her wing. Due to the secretive nature of the "Atlantis's" payload, the crew was forced to use a more secure encrypted transmission, which had more than likely been received at a low quality. NASA engineers thought the damage was just light and shadows, and as a result the crew was infuriated. During reentry, Guy Garder, the pilot for the mission, returned the Shuttle safely. Upon inspection the Shuttle's bottom right wing was seen to be severely damaged in critical areas. Ultimately, the same fate would eventually be the result that destroyed the Space Shuttle "Columbia" in 2003, due to tile failures, which resulted in the "Columbia" being ripped apart on reentry. Had "Atlantis" been destroyed during her mission in 1988, more than likely the second destruction of an Orbiter would have set NASA back at least two years, forced a redesign of the fuel tanks foam coverings and the fragile heat shield plating, or it would have forced NASA to close down the Shuttle Program 22 years before it actually ended.

During another mission, STS-37 flown in 1991, "Atlantis" deployed the Compton Gamma Ray Observatory. Beginning in 1995 with STS-71, "Atlantis" made seven straight flights to the former Russian space station Mir as part of the Shuttle-Mir Program. STS-71 marked a number of firsts in human spaceflight: 100th U.S. crewed space flight; first U.S. Shuttle-Russian Space Station Mir docking and joint on-orbit operations; and first on-orbit change-out of shuttle crew. When linked, "Atlantis" and "Mir" together formed the largest spacecraft in orbit at the time.

Shuttle "Atlantis" also delivered several vital components for the construction of the International Space Station (ISS). During the February 2001 mission STS-98 to the ISS, "Atlantis" delivered the Destiny Module, the primary operating facility for U.S. research payloads aboard the ISS. The five-hour 25-minute third spacewalk performed by astronauts Robert Curbeam and Thomas Jones during STS-98 marked NASA's 100th extra vehicular activity in space. The Quest Joint Airlock, was flown and installed to the ISS by "Atlantis" during the mission STS-104 in July 2001. The successful installation of the airlock gave on-board space station crews the ability to stage repair and maintenance spacewalks outside the ISS using U.S. EMU or Russian Orlan space suits. The first mission flown by "Atlantis" after the Space Shuttle "Columbia" disaster was STS-115, conducted during September 2006. The mission carried the P3/P4 truss segments and solar arrays to the ISS. On ISS assembly flight STS-122 in February 2008, "Atlantis" delivered the Columbus laboratory to the ISS. Columbus laboratory is the largest single contribution to the ISS made by the European Space Agency (ESA).

In May 2009 "Atlantis" flew a seven-member crew to the Hubble Space Telescope for its Servicing Mission 4, STS-125. The mission was a success, with the crew completing five spacewalks totalling 37 hours to install new cameras, batteries, a gyroscope and other components to the telescope.
This was the final mission not to the ISS.

The longest mission flown using "Atlantis" was STS-117 which lasted almost 14 days in June 2007. During STS-117, Atlantis' crew added a new starboard truss segment and solar array pair (the S3/S4 truss), folded the P6 array in preparation for its relocation and performed four spacewalks. "Atlantis" was not equipped to take advantage of the Station-to-Shuttle Power Transfer System so missions could not be extended by making use of power provided by ISS.

During the STS-129 post-flight interview on 16 November 2009, shuttle launch director Mike Leinbach said that "Atlantis" officially beat Space Shuttle "Discovery" for the record low amount of Interim Problem Reports, with a total of just 54 listed since returning from STS-125. He continued to add "It is due to the team and the hardware processing. They just did a great job. The record will probably never be broken again in the history of the Space Shuttle Program, so congratulations to them".

During the STS-132 post-launch interview on 14 May 2010, Shuttle launch director Mike Leinbach said that "Atlantis" beat its own previous record low amount of Interim Problem Reports, with a total of 46 listed between STS-129 and STS-132.

"Atlantis" went through two overhauls of scheduled Orbiter Maintenance Down Periods (OMDPs) during its operational history.

"Atlantis" arrived at Palmdale, California in October 1992 for OMDP-1. During that visit 165 modifications were made over the next 20 months. These included the installation of a drag chute, new plumbing lines to configure the orbiter for extended duration, improved nose wheel steering, more than 800 new heat tiles and blankets and new insulation for main landing gear and structural modifications to the airframe.

"Atlantis" after suffering severe damage to her right wing during take-off, was forced to undergo repair to her aluminum structure, and replacement to 700 of her tiles in 1988. The Shuttle was relaunched in 1989.

On 5 November 1997, "Atlantis" again arrived at Palmdale for OMDP-2 which was completed on 24 September 1998. The 130 modifications carried out during OMDP-2 included glass cockpit displays, replacement of TACAN navigation with GPS and ISS airlock and docking installation. Several weight reduction modifications were also performed on the orbiter including replacement of Advanced Flexible Reusable Surface Insulation (AFRSI) insulation blankets on upper surfaces with FRSI. Lightweight crew seats were installed and the Extended Duration Orbiter (EDO) package installed on OMDP-1 was removed to lighten "Atlantis" to better serve its prime mission of servicing the ISS.

During the stand down period post "Columbia" accident, "Atlantis" went through over 75 modifications to the orbiter ranging from very minor bolt change-outs to window change-outs and different fluid systems.

"Atlantis" was known among the Shuttle workforce as being more prone than the others in the fleet to problems that needed to be addressed while readying the vehicle for launch, leading to some nicknaming it "Britney".

NASA initially planned to withdraw "Atlantis" from service in 2008, as the orbiter would have been due to undergo its third scheduled OMDP; the timescale of the final retirement of the shuttle fleet was such that having the orbiter undergo this work was deemed uneconomical. It was planned that "Atlantis" would be kept in near-flight condition to be used as a spares source for "Discovery" and "Endeavour". However, with the significant planned flight schedule up to 2010, the decision was taken to extend the time between OMDPs, allowing "Atlantis" to be retained for operations. "Atlantis" was subsequently swapped for one flight of each "Discovery" and "Endeavour" in the flight manifest. "Atlantis" had completed what was meant to be its last flight, STS-132, prior to the end of the shuttle program, but the extension of the Shuttle program into 2011 led to "Atlantis" being selected for STS-135, the final Space Shuttle mission in July 2011.

"Atlantis" is currently displayed at the Kennedy Space Center Visitor Complex. NASA Administrator Charles Bolden announced the decision at an employee event held on 12 April 2011 to commemorate the 30th anniversary of the first shuttle flight: "First, here at the Kennedy Space Center where every shuttle mission and so many other historic human space flights have originated, we'll showcase my old friend, "Atlantis"."

The Visitor Complex displays "Atlantis" with payload bay doors opened mounted at an angle to give the appearance of being in orbit around the Earth. The 43.21-degree mount angle also pays tribute to the countdown that preceded every shuttle launch at KSC. A multi-story digital projection of Earth rotates behind the orbiter in a indoor facility. Ground breaking of the facility occurred in 2012.
The exhibit opened on 29 June 2013.

A total of 156 individuals flew with Space Shuttle "Atlantis" over the course of its 33 missions. Because the shuttle sometimes flew crew members arriving and departing Mir and the ISS, not all of them launched and landed on "Atlantis".

Astronaut Clayton Anderson, ESA astronaut Leopold Eyharts and Russian cosmonauts Nikolai Budarin and Anatoly Solovyev only launched on "Atlantis". Similarly, astronauts Daniel Tani and Sunita Williams, as well as cosmonauts Vladimir Dezhurov and Gennady Strekalov only landed with "Atlantis". Only 146 men and women both launched and landed aboard "Atlantis".

Some of those people flew with "Atlantis" more than once. Taking them into account, 203 total seats were filled over "Atlantis" 33 missions. Astronaut Jerry Ross holds the record for the most flights aboard "Atlantis" at five.

Astronaut Rodolfo Neri Vela who flew aboard "Atlantis" on STS-61-B mission in 1985 became the first and so far only Mexican to have traveled to space. ESA astronaut Dirk Frimout who flew on STS-45 as a payload specialist was the first Belgian in space. STS-46 mission specialist Claude Nicollier was the first astronaut from Switzerland. On the same flight, astronaut Franco Malerba became the first citizen of Italy to travel to space.

Astronaut Michael Massimino who flew on STS-125 mission became the first person to use Twitter in space in May 2009.

Having flown aboard "Atlantis" as part of the STS-132 crew in May 2010 and "Discovery" as part of the STS-133 crew in February/March 2011, Stephen Bowen became the first NASA astronaut to be launched on consecutive missions.

NASA announced in 2007 that 24 helium and nitrogen gas tanks in "Atlantis" were older than their designed lifetime. These composite overwrapped pressure vessels (COPV) were designed for a 10-year life and later cleared for an additional 10 years; they exceeded this life in 2005. NASA said it could not guarantee any longer that the vessels on "Atlantis" would not burst or explode under full pressure. Failure of these tanks could have damaged parts of the orbiter and even wound or kill ground personnel. An in-flight failure of a pressure vessel could have even resulted in the loss of the orbiter and its crew. NASA analyses originally assumed that the vessels would leak before they burst, but new tests showed that they could in fact burst before leaking.

Because the original vendor was no longer in business, and a new manufacturer could not be qualified before 2010, when the shuttles were scheduled to be retired, NASA decided to continue operations with the existing tanks. Therefore, to reduce the risk of failure and the cumulative effects of load, the vessels were maintained at 80 percent of the operating pressure as late in the launch countdown as possible, and the launch pad was cleared of all but essential personnel when pressure was increased to 100 percent. The new launch procedure was employed during some of the remaining launches of "Atlantis", but was resolved when the two COPVs deemed to have the highest risk of failure were replaced.

After the STS-125 mission, a work light knob was discovered jammed in the space between one of "Atlantis"s front interior windows and the Orbiter dashboard structure. The knob was believed to have entered the space during flight, when the pressurized Orbiter was expanded to its maximum size. Then, once back on Earth, the Orbiter contracted, jamming the knob in place. Leaving "as-is" was considered unsafe for flight, and some options for removal (including window replacement) would have included a 6-month delay of "Atlantis"s next mission (planned to be STS-129). Had the removal of the knob been unsuccessful, the worst-case scenario was that "Atlantis" could have been retired from the fleet, leaving "Discovery" and "Endeavour" to complete the manifest alone. On 29 June 2009, "Atlantis" was pressurized to (3 psi above ambient), which forced the Orbiter to expand slightly. The knob was then frozen with dry ice, and successfully removed. Small areas of damage to the window were discovered where the edges of the knob had been embedded into the pane. Subsequent investigation of the window damage discovered a maximum defect depth of approximately , less than the reportable depth threshold of and not serious enough to warrant the pane's replacement.





</doc>
<doc id="28240" url="https://en.wikipedia.org/wiki?curid=28240" title="Space Shuttle Endeavour">
Space Shuttle Endeavour

Space Shuttle "Endeavour" (Orbiter Vehicle Designation: OV-105) is a retired orbiter from NASA's Space Shuttle program and the fifth and final operational Shuttle built. It embarked on its first mission, STS-49, in May 1992 and its 25th and final mission, STS-134, in May 2011. STS-134 was expected to be the final mission of the Space Shuttle program, but with the authorization of STS-135, "Atlantis" became the last shuttle to fly.

The United States Congress approved the construction of "Endeavour" in 1987 to replace "Challenger", which was destroyed in 1986.

Structural spares built during the construction of "Discovery" and "Atlantis" were used in its assembly. NASA chose, on cost grounds, to build "Endeavour" from spares rather than refitting "Enterprise."

Following the loss of "Challenger", in 1987 NASA was authorized to begin the procurement process for a replacement orbiter. Again, a major refit of the prototype orbiter "Enterprise" was looked at and rejected on cost grounds, with instead the cache of structural spares that were produced as part of the construction of "Discovery" and "Atlantis" earmarked for assembly into the new orbiter. Assembly was completed in July 1990, and the new orbiter was rolled out in April 1991. As part of the process, NASA ran a national competition for schools to name the new orbiter - the criteria included a requirement that it be named after an exploratory or research vessel, with a name "easily understood in the context of space"; entries included an essay about the name, the story behind it and why it was appropriate for a NASA shuttle, and the project that supported the name. Amongst the entries, "Endeavour" was suggested by one-third of the participating schools, with President George H.W. Bush eventually selecting it on the advice of the NASA Administrator, Richard Truly. The national winners were Senatobia Middle School in Senatobia, Mississippi, in the elementary division and Tallulah Falls School in Tallulah Falls, Georgia, in the upper school division. They were honored at several ceremonies in Washington, D.C., including a White House ceremony where President Bush presented awards to each school. "Endeavour" was delivered by Rockwell International Space Transportation Systems Division in May 1991 and first launched a year later, in May 1992, on STS-49. Rockwell International claimed that it had made no profit on Space Shuttle "Endeavour", despite construction costing US$2.2 billion.

The orbiter is named after the British HMS "Endeavour", the ship which took Captain James Cook on his first voyage of discovery (1768–1771). This is why the name is spelled in the British English manner, rather than the American English ("Endeavor"). This has caused confusion, including when NASA itself misspelled a sign on the launch pad in 2007. The Space Shuttle carried a piece of the original wood from Cook's ship inside the cockpit. The name also honored "Endeavour", the command module of Apollo 15, which was also named for Cook's ship.

On May 30, 2020, Dragon 2 capsule C206 was named "Endeavour" during the Crew Dragon Demo-2 mission by astronauts Doug Hurley and Bob Behnken in honor of the shuttle, on which both astronauts took their first flights (STS-127 and STS-123 respectively).

On its first mission, it captured and redeployed the stranded "INTELSAT VI" communications satellite. The first African-American woman astronaut, Mae Jemison, was launched into space on the mission STS-47 on September 12, 1992.

"Endeavour" flew the first servicing mission STS-61 for the Hubble Space Telescope in 1993. In 1997 it was withdrawn from service for eight months for a retrofit, including installation of a new airlock. In December 1998, it delivered the Unity Module to the International Space Station.

"Endeavour"s last Orbiter Major Modification period began in December 2003 and ended on October 6, 2005. During this time, "Endeavour" received major hardware upgrades, including a new, multi-functional, electronic display system, often referred to as a glass cockpit, and an advanced GPS receiver, along with safety upgrades recommended by the "Columbia" Accident Investigation Board (CAIB) for the shuttle's return to flight following the loss of "Columbia" during reentry on 1 February 2003.

The STS-118 mission, "Endeavour"s first since the refit, included astronaut Barbara Morgan, formerly assigned to the Teacher in Space project, and later a member of the Astronaut Corps from 1998 to 2008, as part of the crew. Morgan was the backup for Christa McAuliffe who was on the ill-fated mission STS-51-L in 1986.

As it was constructed later than its elder sisters, "Endeavour" was built with new hardware designed to improve and expand orbiter capabilities. Most of this equipment was later incorporated into the other three orbiters during out-of-service major inspection and modification programs. ""Endeavour"s upgrades include:

Modifications resulting from a 2005–2006 refit of "Endeavour" included:

"Endeavour" flew its final mission, STS-134, to the International Space Station (ISS) in May 2011. After the conclusion of STS-134, "Endeavour" was formally decommissioned.

STS-134 was intended to launch in late 2010, but on July 1 NASA released a statement saying the "Endeavour" mission was rescheduled for February 27, 2011.

"The target dates were adjusted because critical payload hardware for STS-133 will not be ready in time to support the previously planned 16 September launch," NASA said in a statement. With the "Discovery" launch moving to November, "Endeavour" mission "cannot fly as planned, so the next available launch window is in February 2011," NASA said, adding that the launch dates were subject to change.

The launch was further postponed until April to avoid a scheduling conflict with a Russian supply vehicle heading for the International Space Station. STS-134 did not launch until 16 May at 08:56 EDT.

"Endeavour" landed at the Kennedy Space Center at 06:34 UTC on June 1, 2011, completing its final mission. It was the 25th night landing of a shuttle. Over its flight career, "Endeavour" flew 122,883,151 miles and spent 299 days in space. During "Endeavour's" last mission, the Russian spacecraft Soyuz TMA-20 departed from the ISS and paused at a distance of 200 meters. Italian astronaut Paolo Nespoli took a series of photographs and videos of the ISS with "Endeavour" docked. This was the second time a shuttle was photographed docked and the first time since 1996. Commander Mark Kelly was the last astronaut off "Endeavour" after the landing, and the crew stayed on the landing strip to sign autographs and pose for pictures.

STS-134 was the penultimate Space Shuttle mission; STS-135 was added to the schedule in January 2011, and in July "Atlantis" flew for the final time.

After more than twenty organizations submitted proposals to NASA for the display of an orbiter, NASA announced that "Endeavour" would go to the California Science Center in Los Angeles.

After low level flyovers above NASA and civic landmarks across the country and in California, it was delivered to Los Angeles International Airport (LAX) on September 21, 2012. The orbiter was slowly and carefully transported through the streets of Los Angeles and Inglewood three weeks later, from October 11–14 along La Tijera, Manchester, Crenshaw, and Martin Luther King, Jr. Boulevards to its final destination at the California Science Center in Exposition Park.

"Endeavour"s route on the city streets between LAX and Exposition Park was meticulously measured and each move was carefully choreographed. In multiple locations, there were only inches of clearance for the Shuttle's wide wings between telephone poles, apartment buildings and other structures. Many street light standards and traffic signals were temporarily removed as the Shuttle passed through. It was necessary to remove over 400 street trees as well, some of which were fairly old, creating a small controversy. However, the removed trees were replaced two-for-one by the Science Center, using part of the $200 million funding for the move.

The power had to be turned off and power carrying poles had to be removed temporarily as the orbiter crept along Manchester, to Prairie Avenue, then Crenshaw Boulevard. News crews lined the streets along the path with visible news personalities in the news trucks. Police escorts and other security personnel, among them including the LAPD, LASD, CHP, and NASA officials, controlled the large crowds gathered, with support from the LAFD and LACoFD to treat heat exhaustion victims as "Endeavour" made its way through the city. "Endeavour" was parked for a few hours at the Great Western Forum where it was available for viewing. The journey was famous for an unmodified Toyota Tundra pickup truck pulling the Space Shuttle across the Manchester Boulevard Bridge. The Space Shuttle was mainly carried by four self-propelled robotic dollies throughout the 12 mile journey. However, due to bridge weight restrictions, "Endeavour" was moved onto the dolly towed by the Tundra. After it had completely crossed the bridge, the Space Shuttle was returned to the robotic dollies. The footage was later used in a commercial for the 2013 Super Bowl. Having taken longer than expected, "Endeavour" finally reached the Science Center on October 14.

The exhibit was opened to the public on October 30, 2012 at the temporary Samuel Oschin Space Shuttle "Endeavour" Display Pavilion of the museum. A new addition to the Science Center, called the Samuel Oschin Air and Space Center, is under construction as "Endeavour"s permanent home. Before the opening, "Endeavour" will be mounted vertically with an external tank and a pair of solid rocket boosters in the Shuttle stack configuration. One payload door will be opened out to reveal a demonstration payload inside.

After its decommissioning, "Endeavour"s Canadarm (formally the 'Shuttle Remote Manipulator System') was removed in order to be sent to the Canadian Space Agency's John H. Chapman Space Centre in Longueuil, Quebec, a suburb of Montreal, where it was to be placed on display. In a Canadian poll on which science or aerospace museum should be selected to display the Canadarm, originally built by SPAR Aerospace, the Canadian Space Agency's headquarters placed third to last with only 35 out of 638 votes. "Endeavour"s Canadarm has since gone on permanent display at the Canada Aviation and Space Museum in Ottawa.

In August 2015 NASA engineers went to work on removing a few of the tanks from "Endeavour" for reuse as storage containers for potable water on the International Space Station.

Space Shuttle "Endeavour" is the namesake for SpaceX's Dragon 2 capsule 206, which first flew on Crew Dragon Demo-2 beginning May 30, 2020.

The Flow Director was responsible for the overall preparation of the Shuttle for launch and processing it after landing, and remained permanently assigned to head the spacecraft's ground crew while the astronaut flight crews changed for every mission. Each Shuttle's Flow Director was supported by a Vehicle Manager for the same spacecraft. Space Shuttle "Endeavour"s Flow Directors were:


"Endeavour" is currently housed in the Samuel Oschin Pavilion at the California Science Center in Exposition Park in South Los Angeles about two miles south of Downtown Los Angeles. A companion exhibit, ""Endeavour": The California Story", features images and artifacts that relate the Space Shuttle program to California, where the orbiters were originally constructed. It has been planned for a new facility to be built with "Endeavour" attached to an external fuel tank (the last mission-ready one in existence as all others were destroyed during launch) and the two solid rocket boosters (SRBs) and raised in an upright position, as if "Endeavour" were to make one more flight. "Endeavour" is on display at the museum, the SRBs are in storage, and the external tank ET-94 is on display: ET-94 is currently undergoing restoration after being used to analyze the foam on its sister tank, which was a factor in the failure of STS-107.

Following their May 30, 2020 launch on board the SpaceX Crew Dragon Demo-2 vehicle, the crew announced in orbit that they had named their spacecraft "Capsule Endeavour". Astronauts Bob Behnken and Doug Hurley said the name has a dual meaning: first, after the "incredible endeavor" put forth by SpaceX and NASA after the retirement of the space shuttle fleet in 2011; and second, because both Hurley and Behnken each flew their first flight aboard the shuttle Endeavour (Behnken on STS-123, Hurley on STS-127) and wanted to name this new spacecraft after the one that took each of them into space.





</doc>
<doc id="28242" url="https://en.wikipedia.org/wiki?curid=28242" title="Sports Car Club of America">
Sports Car Club of America

The Sports Car Club of America (SCCA) is a non-profit American automobile club and sanctioning body supporting road racing, rallying, and autocross in the United States. Formed in 1944, it runs many programs for both amateur and professional racers.

The SCCA traces its roots to the Automobile Racing Club of America (not to be confused with the current stock car series of the same name). ARCA was founded in 1933 by brothers Miles and Sam Collier, and dissolved in 1941 at the outbreak of World War II. The SCCA was formed in 1944 as an enthusiast group. The SCCA began sanctioning road racing in 1948 with the inaugural Watkins Glen Grand Prix. Cameron Argetsinger, an SCCA member and local enthusiast who would later become Director of Pro Racing and Executive Director of the SCCA, helped organize the event for the SCCA.

In 1951, the SCCA National Sports Car Championship was formed from existing marquee events around the nation, including Watkins Glen, Pebble Beach, and Elkhart Lake. Many early SCCA events were held on disused air force bases, organized with the help of Air Force General Curtis LeMay, a renowned enthusiast of sports car racing. LeMay loaned out facilities of Strategic Air Command bases for the SCCA's use; the SCCA relied heavily on these venues during the early and mid-1950s during the transition from street racing to permanent circuits.

By 1962, the SCCA was tasked with managing the U.S. World Sportscar Championship rounds at Daytona, Sebring, Bridgehampton and Watkins Glen. The club was also involved in the Formula 1 U.S. Grand Prix. SCCA Executive Director John Bishop helped to create the United States Road Racing Championship series for Group 7 sports cars to recover races that had been taken by rival USAC Road Racing Championship. Bishop was also instrumental in founding the SCCA Trans-Am Series and the SCCA/CASC Can-Am series. In 1969, tension and infighting over Pro Racing's autonomy caused Bishop to resign and help form the International Motor Sports Association.

The SCCA dropped its amateur policy in 1962 and began sanctioning professional racing. In 1963, the United States Road Racing Championship was formed. In 1966 the Canadian-American Challenge Cup (Can-Am) was created for Group 7 open-top sportscars. The Trans-Am Series for pony cars also began in 1966. Today, Trans-Am uses GT-1 class regulations, giving amateur drivers a chance to race professionally. A professional series for open-wheel racing cars was introduced in 1967 as the SCCA Grand Prix Championship. This series was then held under various names through to the 1976 SCCA/USAC Formula 5000 Championship.

Current SCCA-sanctioned series include Trans Am, the Pirelli World Challenge for GT and touring cars, the Global MX-5 Cup, F2000 Championship Series, F1600 Championship Series and the Atlantic Championship Series. SCCA Pro Racing has also sanctioned professional series for some amateur classes such as Spec Racer Ford Pro and Formula Enterprises Pro. SCCA Pro Racing also sanctioned the Volkswagen Jetta TDI Cup during its time.

The Club Racing program is a road racing division where drivers race on either dedicated race tracks or on temporary street circuits. Competitors require either a regional or a national racing license. Both modified production cars (ranging from lightly modified cars with only extra safety equipment to heavily modified cars that retain only the basic shape of the original vehicle) and designed-from-scratch "formula" and "sports racer" cars can be used in Club Racing. Most of the participants in the Club Racing program are unpaid amateurs, but some go on to professional racing careers. The club is also the source for race workers in all specialties.

The annual national championship for Club Racing is called the SCCA National Championship Runoffs and has been held at Riverside International Raceway (1964, 1966, 1968), Daytona International Speedway (1965, 1967, 1969, 2015), Road Atlanta (1970–1993), Mid-Ohio Sports Car Course (1994–2005, 2016), Heartland Park Topeka (2006–2008), Road America (2009-2013, 2020), Mazda Raceway Laguna Seca (2014), and Indianapolis Motor Speedway (2017). In 2018, the Runoffs will go back west to Sonoma Raceway. In 2019, the race will be held at Virginia International Raceway a track where the race has never been held. It was announced on June 15, 2018 that the Runoffs would go back to Road America in the year 2020. On May 25th 2019, the weekend of the 2019 Indianapolis 500, SCCA announced they will be returning to Indianapolis Motor Speedway in 2021. The current SCCA record holder is Jerry Hansen, (former owner of Brainerd International Raceway), with twenty-seven national championships.

The seven national classes of the formula group are Formula Atlantic (FA), Formula Continental (FC), Formula SCCA (FE), Formula F (FF), Formula Vee (FV), Formula X (FX), and Formula 500 (F500).

The autocross program is branded as "Solo". Up to four cars at a time run on a course laid out with traffic cones on a large paved surface, such as a parking lot or airport runway, without interfering with one another.

Competitions are held at the regional, divisional, and national levels. A national champion in each class is determined at the national championship (usually referred to as "Nationals") held in September. In 2009, Solo Nationals moved to the Lincoln Airpark in Lincoln, Nebraska. Individual national-level events called "Championship Tours" and "Match Tours" are held throughout the racing season. The SCCA also holds national-level events in an alternate format called "ProSolo". In ProSolo, two cars compete at the same time on mirror-image courses with drag racing-style starts, complete with reaction and 60-foot times. Class winners and other qualifiers (based on time differential against the class winner) then compete in a handicapped elimination round called the "Challenge". Points are awarded in both class and Challenge competition, and an annual champion is crowned each September at the ProSolo Finale event in Lincoln, Nebraska.

The SCCA sanctions "RallyCross" events, similar to autocross, but on a non-paved course. SCCA ProRally was a national performance rally series similar to the World Rally Championship. At the end of the 2004 season SCCA dropped ProRally and ClubRally. A new organization, Rally America, picked up both series starting in 2005.

Road rallies are run on open, public roads. These are not races in the sense of speed, but of precision and navigation. The object is to drive on time, arriving at checkpoints with the proper amount of elapsed time from the previous checkpoint. Competitors do not know where the checkpoints are.

In recent years, the SCCA has expanded and re-organized some of the higher-speed events under the Time Trials banner. These include Performance Driving Experience ("PDX"), Club Trials, Track Trials, and Hill Climb events. PDX events are non-competition HPDE-type events and consist of driver-education and car control classroom learning combined with on-track instruction.

The SCCA is organized into six conferences, nine divisions and 115 regions, each organizing events in that area to make the events more accessible to people throughout the country. The number of divisions has increased since the SCCA's foundation. Northern Pacific and Southern Pacific started as a single Pacific Coast Division until dividing in 1966. Rocky Mountain Division is a relatively recent split. The Great Lakes Division was split from the Central Division at the end of 2006.




</doc>
<doc id="28244" url="https://en.wikipedia.org/wiki?curid=28244" title="Star network">
Star network

A star network is an implementation of a spoke–hub distribution paradigm in computer networks. In a star network, every host is connected to a central hub. In its simplest form, one central hub acts as a conduit to transmit messages. The star network is one of the most common computer network topologies.

The hub and hosts, and the transmission lines between them, form a graph with the topology of a star. Data on a star network passes through the hub before continuing to its destination. The hub manages and controls all functions of the network. It also acts as a repeater for the data flow. 

The star topology reduces the impact of a transmission line failure by independently connecting each host to the hub. Each host may thus communicate with all others by transmitting to, and receiving from, the hub. The failure of a transmission line linking any host to the hub will result in the isolation of that host from all others, but the rest of the network will be unaffected.

The star configuration is commonly used with twisted pair cable and optical fiber cable. However, it can also be used with coaxial cable.




</doc>
<doc id="28245" url="https://en.wikipedia.org/wiki?curid=28245" title="SQL Server">
SQL Server

SQL Server may refer to:


</doc>
